{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.13065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tars_M/0/1/0/all/0/1\">Maali Tars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tattar_A/0/1/0/all/0/1\">Andre T&#xe4;ttar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisel_M/0/1/0/all/0/1\">Mark Fi&#x161;el</a>",
          "description": "An effective method to improve extremely low-resource neural machine\ntranslation is multilingual training, which can be improved by leveraging\nmonolingual data to create synthetic bilingual corpora using the\nback-translation method. This work focuses on closely related languages from\nthe Uralic language family: from Estonian and Finnish geographical regions. We\nfind that multilingual learning and synthetic corpora increase the translation\nquality in every language pair for which we have data. We show that transfer\nlearning and fine-tuning are very effective for doing low-resource machine\ntranslation and achieve the best results. We collected new parallel data for\nV\\~oro, North and South Saami and present first results of neural machine\ntranslation for these languages.",
          "link": "http://arxiv.org/abs/2105.13065",
          "publishedOn": "2021-05-28T01:42:14.637Z",
          "wordCount": 539,
          "title": "Extremely low-resource machine translation for closely related languages. (arXiv:2105.13065v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chongming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Recommender systems exploit interaction history to estimate user preference,\nhaving been heavily used in a wide range of industry applications. However,\nstatic recommendation models are difficult to answer two important questions\nwell due to inherent shortcomings: (a) What exactly does a user like? (b) Why\ndoes a user like an item? The shortcomings are due to the way that static\nmodels learn user preference, i.e., without explicit instructions and active\nfeedback from users. The recent rise of conversational recommender systems\n(CRSs) changes this situation fundamentally. In a CRS, users and the system can\ndynamically communicate through natural language interactions, which provide\nunprecedented opportunities to explicitly obtain the exact preference of users.\n\nConsiderable efforts, spread across disparate settings and applications, have\nbeen put into developing CRSs. Existing models, technologies, and evaluation\nmethods for CRSs are far from mature. In this paper, we provide a systematic\nreview of the techniques used in current CRSs. We summarize the key challenges\nof developing CRSs in five directions: (1) Question-based user preference\nelicitation. (2) Multi-turn conversational recommendation strategies. (3)\nDialogue understanding and generation. (4) Exploitation-exploration trade-offs.\n(5) Evaluation and user simulation. These research directions involve multiple\nresearch fields like information retrieval (IR), natural language processing\n(NLP), and human-computer interaction (HCI). Based on these research\ndirections, we discuss some future challenges and opportunities. We provide a\nroad map for researchers from multiple communities to get started in this area.\nWe hope this survey can help to identify and address challenges in CRSs and\ninspire future research.",
          "link": "http://arxiv.org/abs/2101.09459",
          "publishedOn": "2021-05-28T01:42:14.603Z",
          "wordCount": 757,
          "title": "Advances and Challenges in Conversational Recommender Systems: A Survey. (arXiv:2101.09459v6 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>",
          "description": "kNN-MT, recently proposed by Khandelwal et al. (2020a), successfully combines\npre-trained neural machine translation (NMT) model with token-level\nk-nearest-neighbor (kNN) retrieval to improve the translation accuracy.\nHowever, the traditional kNN algorithm used in kNN-MT simply retrieves a same\nnumber of nearest neighbors for each target token, which may cause prediction\nerrors when the retrieved neighbors include noises. In this paper, we propose\nAdaptive kNN-MT to dynamically determine the number of k for each target token.\nWe achieve this by introducing a light-weight Meta-k Network, which can be\nefficiently trained with only a few training samples. On four benchmark machine\ntranslation datasets, we demonstrate that the proposed method is able to\neffectively filter out the noises in retrieval results and significantly\noutperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k\nNetwork learned on one domain could be directly applied to other domains and\nobtain consistent improvements, illustrating the generality of our method. Our\nimplementation is open-sourced at https://github.com/zhengxxn/adaptive-knn-mt.",
          "link": "http://arxiv.org/abs/2105.13022",
          "publishedOn": "2021-05-28T01:42:14.594Z",
          "wordCount": 595,
          "title": "Adaptive Nearest Neighbor Machine Translation. (arXiv:2105.13022v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.07790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stacey_J/0/1/0/all/0/1\">Joe Stacey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubossarsky_H/0/1/0/all/0/1\">Haim Dubossarsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>",
          "description": "Natural Language Inference (NLI) datasets contain annotation artefacts\nresulting in spurious correlations between the natural language utterances and\ntheir respective entailment classes. These artefacts are exploited by neural\nnetworks even when only considering the hypothesis and ignoring the premise,\nleading to unwanted biases. Belinkov et al. (2019b) proposed tackling this\nproblem via adversarial training, but this can lead to learned sentence\nrepresentations that still suffer from the same biases. We show that the bias\ncan be reduced in the sentence representations by using an ensemble of\nadversaries, encouraging the model to jointly decrease the accuracy of these\ndifferent adversaries while fitting the data. This approach produces more\nrobust NLI models, outperforming previous de-biasing efforts when generalised\nto 12 other datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In\naddition, we find that the optimal number of adversarial classifiers depends on\nthe dimensionality of the sentence representations, with larger sentence\nrepresentations being more difficult to de-bias while benefiting from using a\ngreater number of adversaries.",
          "link": "http://arxiv.org/abs/2004.07790",
          "publishedOn": "2021-05-28T01:42:14.571Z",
          "wordCount": 674,
          "title": "Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training. (arXiv:2004.07790v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Siqi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bingjin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenquan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingzhan Lin</a>",
          "description": "In this work, we explore the application of PLATO-2 on various dialogue\nsystems, including open-domain conversation, knowledge grounded dialogue, and\ntask-oriented conversation. PLATO-2 is initially designed as an open-domain\nchatbot, trained via two-stage curriculum learning. In the first stage, a\ncoarse-grained response generation model is learned to fit the simplified\none-to-one mapping relationship. This model is applied to the task-oriented\nconversation, given that the semantic mappings tend to be deterministic in task\ncompletion. In the second stage, another fine-grained generation model and an\nevaluation model are further learned for diverse response generation and\ncoherence estimation, respectively. With superior capability on capturing\none-to-many mapping, such models are suitable for the open-domain conversation\nand knowledge grounded dialogue. For the comprehensive evaluation of PLATO-2,\nwe have participated in multiple tasks of DSTC9, including interactive\nevaluation of open-domain conversation (Track3-task2), static evaluation of\nknowledge grounded dialogue (Track3-task1), and end-to-end task-oriented\nconversation (Track2-task1). PLATO-2 has obtained the 1st place in all three\ntasks, verifying its effectiveness as a unified framework for various dialogue\nsystems.",
          "link": "http://arxiv.org/abs/2105.02482",
          "publishedOn": "2021-05-28T01:42:14.551Z",
          "wordCount": 636,
          "title": "A Unified Pre-training Framework for Conversational AI. (arXiv:2105.02482v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_W/0/1/0/all/0/1\">Wenjie Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Keyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_S/0/1/0/all/0/1\">Simon J.D. Prince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanshuai Cao</a>",
          "description": "It is a common belief that training deep transformers from scratch requires\nlarge datasets. Consequently, for small datasets, people usually use shallow\nand simple additional layers on top of pre-trained models during fine-tuning.\nThis work shows that this does not always need to be the case: with proper\ninitialization and optimization, the benefits of very deep transformers can\ncarry over to challenging tasks with small datasets, including Text-to-SQL\nsemantic parsing and logical reading comprehension. In particular, we\nsuccessfully train $48$ layers of transformers, comprising $24$ fine-tuned\nlayers from pre-trained RoBERTa and $24$ relation-aware layers trained from\nscratch. With fewer training steps and no task-specific pre-training, we obtain\nthe state-of-the-art performance on the challenging cross-domain Text-to-SQL\nparsing benchmark Spider. We achieve this by deriving a novel Data-dependent\nTransformer Fixed-update initialization scheme (DT-Fixup), inspired by the\nprior T-Fixup work. Further error analysis shows that increasing depth can help\nimprove generalization on small datasets for hard cases that require reasoning\nand structural understanding.",
          "link": "http://arxiv.org/abs/2012.15355",
          "publishedOn": "2021-05-28T01:42:14.544Z",
          "wordCount": 645,
          "title": "Optimizing Deeper Transformers on Small Datasets. (arXiv:2012.15355v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voita_E/0/1/0/all/0/1\">Elena Voita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "In Neural Machine Translation (and, more generally, conditional language\nmodeling), the generation of a target token is influenced by two types of\ncontext: the source and the prefix of the target sequence. While many attempts\nto understand the internal workings of NMT models have been made, none of them\nexplicitly evaluates relative source and target contributions to a generation\ndecision. We argue that this relative contribution can be evaluated by adopting\na variant of Layerwise Relevance Propagation (LRP). Its underlying\n'conservation principle' makes relevance propagation unique: differently from\nother methods, it evaluates not an abstract quantity reflecting token\nimportance, but the proportion of each token's influence. We extend LRP to the\nTransformer and conduct an analysis of NMT models which explicitly evaluates\nthe source and target relative contributions to the generation process. We\nanalyze changes in these contributions when conditioning on different types of\nprefixes, when varying the training objective or the amount of training data,\nand during the training process. We find that models trained with more data\ntend to rely on source information more and to have more sharp token\ncontributions; the training process is non-monotonic with several stages of\ndifferent nature.",
          "link": "http://arxiv.org/abs/2010.10907",
          "publishedOn": "2021-05-28T01:42:14.535Z",
          "wordCount": 658,
          "title": "Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation. (arXiv:2010.10907v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Im_J/0/1/0/all/0/1\">Jinbae Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Moonki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hoyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsouk Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Sehee Chung</a>",
          "description": "Recently, opinion summarization, which is the generation of a summary from\nmultiple reviews, has been conducted in a self-supervised manner by considering\na sampled review as a pseudo summary. However, non-text data such as image and\nmetadata related to reviews have been considered less often. To use the\nabundant information contained in non-text data, we propose a self-supervised\nmultimodal opinion summarization framework called MultimodalSum. Our framework\nobtains a representation of each modality using a separate encoder for each\nmodality, and the text decoder generates a summary. To resolve the inherent\nheterogeneity of multimodal data, we propose a multimodal training pipeline. We\nfirst pretrain the text encoder--decoder based solely on text modality data.\nSubsequently, we pretrain the non-text modality encoders by considering the\npretrained text decoder as a pivot for the homogeneous representation of\nmultimodal data. Finally, to fuse multimodal representations, we train the\nentire framework in an end-to-end manner. We demonstrate the superiority of\nMultimodalSum by conducting experiments on Yelp and Amazon datasets.",
          "link": "http://arxiv.org/abs/2105.13135",
          "publishedOn": "2021-05-28T01:42:14.499Z",
          "wordCount": 593,
          "title": "Self-Supervised Multimodal Opinion Summarization. (arXiv:2105.13135v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stahlberg_F/0/1/0/all/0/1\">Felix Stahlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>",
          "description": "Synthetic data generation is widely known to boost the accuracy of neural\ngrammatical error correction (GEC) systems, but existing methods often lack\ndiversity or are too simplistic to generate the broad range of grammatical\nerrors made by human writers. In this work, we use error type tags from\nautomatic annotation tools such as ERRANT to guide synthetic data generation.\nWe compare several models that can produce an ungrammatical sentence given a\nclean sentence and an error type tag. We use these models to build a new, large\nsynthetic pre-training data set with error tag frequency distributions matching\na given development set. Our synthetic data set yields large and consistent\ngains, improving the state-of-the-art on the BEA-19 and CoNLL-14 test sets. We\nalso show that our approach is particularly effective in adapting a GEC system,\ntrained on mixed native and non-native English, to a native English test set,\neven surpassing real training data consisting of high-quality sentence pairs.",
          "link": "http://arxiv.org/abs/2105.13318",
          "publishedOn": "2021-05-28T01:42:14.491Z",
          "wordCount": 603,
          "title": "Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models. (arXiv:2105.13318v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertram Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1\">Zeerak Waseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margetts_H/0/1/0/all/0/1\">Helen Margetts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>",
          "description": "Detecting online hate is a difficult task that even state-of-the-art models\nstruggle with. Typically, hate speech detection models are evaluated by\nmeasuring their performance on held-out test data using metrics such as\naccuracy and F1 score. However, this approach makes it difficult to identify\nspecific model weak points. It also risks overestimating generalisable model\nperformance due to increasingly well-evidenced systematic gaps and biases in\nhate speech datasets. To enable more targeted diagnostic insights, we introduce\nHateCheck, a suite of functional tests for hate speech detection models. We\nspecify 29 model functionalities motivated by a review of previous research and\na series of interviews with civil society stakeholders. We craft test cases for\neach functionality and validate their quality through a structured annotation\nprocess. To illustrate HateCheck's utility, we test near-state-of-the-art\ntransformer models as well as two popular commercial models, revealing critical\nmodel weaknesses.",
          "link": "http://arxiv.org/abs/2012.15606",
          "publishedOn": "2021-05-28T01:42:14.483Z",
          "wordCount": 611,
          "title": "HateCheck: Functional Tests for Hate Speech Detection Models. (arXiv:2012.15606v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.07822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shining Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wanli Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenkun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xianglin Zuo</a>",
          "description": "Mining causality from text is a complex and crucial natural language\nunderstanding task corresponding to the human cognition. Existing studies at\nits solution can be grouped into two primary categories: feature engineering\nbased and neural model based methods. In this paper, we find that the former\nhas incomplete coverage and inherent errors but provide prior knowledge; while\nthe latter leverages context information but causal inference of which is\ninsufficiency. To handle the limitations, we propose a novel causality\ndetection model named MCDN to explicitly model causal reasoning process, and\nfurthermore, to exploit the advantages of both methods. Specifically, we adopt\nmulti-head self-attention to acquire semantic feature at word level and develop\nthe SCRN to infer causality at segment level. To the best of our knowledge,\nwith regards to the causality tasks, this is the first time that the Relation\nNetwork is applied. The experimental results show that: 1) the proposed\napproach performs prominent performance on causality detection; 2) further\nanalysis manifests the effectiveness and robustness of MCDN.",
          "link": "http://arxiv.org/abs/1908.07822",
          "publishedOn": "2021-05-28T01:42:14.461Z",
          "wordCount": 651,
          "title": "A Multi-level Neural Network for Implicit Causality Detection in Web Texts. (arXiv:1908.07822v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jiannan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>",
          "description": "An important aspect of developing dialogue systems is how to evaluate and\ncompare the performance of different systems. Existing automatic evaluation\nmetrics are based on turn-level quality evaluation and use average scores for\nsystem-level comparison. In this paper, we propose to measure the performance\nof a dialogue system by computing the distribution-wise distance between its\ngenerated conversations and real-world conversations. Specifically, two\ndistribution-wise metrics, FBD and PRD, are developed and evaluated.\nExperiments on several dialogue corpora show that our proposed metrics\ncorrelate better with human judgments than existing metrics.",
          "link": "http://arxiv.org/abs/2105.02573",
          "publishedOn": "2021-05-28T01:42:14.453Z",
          "wordCount": 552,
          "title": "Assessing Dialogue Systems with Distribution Distances. (arXiv:2105.02573v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>",
          "description": "Pre-trained language models such as BERT have exhibited remarkable\nperformances in many tasks in natural language understanding (NLU). The tokens\nin the models are usually fine-grained in the sense that for languages like\nEnglish they are words or sub-words and for languages like Chinese they are\ncharacters. In English, for example, there are multi-word expressions which\nform natural lexical units and thus the use of coarse-grained tokenization also\nappears to be reasonable. In fact, both fine-grained and coarse-grained\ntokenizations have advantages and disadvantages for learning of pre-trained\nlanguage models. In this paper, we propose a novel pre-trained language model,\nreferred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained\nand coarse-grained tokenizations. For English, AMBERT takes both the sequence\nof words (fine-grained tokens) and the sequence of phrases (coarse-grained\ntokens) as input after tokenization, employs one encoder for processing the\nsequence of words and the other encoder for processing the sequence of the\nphrases, utilizes shared parameters between the two encoders, and finally\ncreates a sequence of contextualized representations of the words and a\nsequence of contextualized representations of the phrases. Experiments have\nbeen conducted on benchmark datasets for Chinese and English, including CLUE,\nGLUE, SQuAD and RACE. The results show that AMBERT can outperform BERT in all\ncases, particularly the improvements are significant for Chinese. We also\ndevelop a method to improve the efficiency of AMBERT in inference, which still\nperforms better than BERT with the same computational cost as BERT.",
          "link": "http://arxiv.org/abs/2008.11869",
          "publishedOn": "2021-05-28T01:42:14.444Z",
          "wordCount": 755,
          "title": "AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization. (arXiv:2008.11869v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muthukumar_P/0/1/0/all/0/1\">Pratyush Muthukumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthukumar_K/0/1/0/all/0/1\">Karishma Muthukumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthirayan_D/0/1/0/all/0/1\">Deepan Muthirayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khargonekar_P/0/1/0/all/0/1\">Pramod Khargonekar</a>",
          "description": "Generative adversarial imitation learning (GAIL) is a model-free algorithm\nthat has been shown to provide strong results in imitating complex behaviors in\nhigh-dimensional environments. In this paper, we utilize the GAIL model for\ntext generation to develop empathy-based context-aware conversational AI. Our\nmodel uses an expert trajectory of empathetic prompt-response dialogues which\ncan accurately exhibit the correct empathetic emotion when generating a\nresponse. The Generator of the GAIL model uses the GPT-2 sequential pre-trained\nlanguage model trained on 117 million parameters from 40 GB of internet data.\nWe propose a novel application of an approach used in transfer learning to fine\ntune the GPT-2 model in order to generate concise, user-specific empathetic\nresponses validated against the Discriminator. Our novel GAIL model utilizes a\nsentiment analysis history-based reinforcement learning approach to\nempathetically respond to human interactions in a personalized manner. We find\nthat our model's response scores on various human-generated prompts collected\nfrom the Facebook Empathetic Dialogues dataset outperform baseline\ncounterparts. Moreover, our model improves upon various history-based\nconversational AI models developed recently, as our model's performance over a\nsustained conversation of 3 or more interactions outperform similar\nconversational AI models.",
          "link": "http://arxiv.org/abs/2105.13328",
          "publishedOn": "2021-05-28T01:42:14.433Z",
          "wordCount": 619,
          "title": "Generative Adversarial Imitation Learning for Empathy-based AI. (arXiv:2105.13328v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.02252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plachouras_V/0/1/0/all/0/1\">Vassilis Plachouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Challenging problems such as open-domain question answering, fact checking,\nslot filling and entity linking require access to large, external knowledge\nsources. While some models do well on individual tasks, developing general\nmodels is difficult as each task might require computationally expensive\nindexing of custom knowledge sources, in addition to dedicated infrastructure.\nTo catalyze research on models that condition on specific information in large\ntextual resources, we present a benchmark for knowledge-intensive language\ntasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia,\nreducing engineering turnaround through the re-use of components, as well as\naccelerating research into task-agnostic memory architectures. We test both\ntask-specific and general baselines, evaluating downstream performance in\naddition to the ability of the models to provide provenance. We find that a\nshared dense vector index coupled with a seq2seq model is a strong baseline,\noutperforming more tailor-made approaches for fact checking, open-domain\nquestion answering and dialogue, and yielding competitive results on entity\nlinking and slot filling, by generating disambiguated text. KILT data and code\nare available at https://github.com/facebookresearch/KILT.",
          "link": "http://arxiv.org/abs/2009.02252",
          "publishedOn": "2021-05-28T01:42:14.424Z",
          "wordCount": 689,
          "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks. (arXiv:2009.02252v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>",
          "description": "We propose to measure fine-grained domain relevance - the degree that a term\nis relevant to a broad (e.g., computer science) or narrow (e.g., deep learning)\ndomain. Such measurement is crucial for many downstream tasks in natural\nlanguage processing. To handle long-tail terms, we build a core-anchored\nsemantic graph, which uses core terms with rich description information to\nbridge the vast remaining fringe terms semantically. To support a fine-grained\ndomain without relying on a matching corpus for supervision, we develop\nhierarchical core-fringe learning, which learns core and fringe terms jointly\nin a semi-supervised manner contextualized in the hierarchy of the domain. To\nreduce expensive human efforts, we employ automatic annotation and hierarchical\npositive-unlabeled learning. Our approach applies to big or small domains,\ncovers head or tail terms, and requires little human effort. Extensive\nexperiments demonstrate that our methods outperform strong baselines and even\nsurpass professional human performance.",
          "link": "http://arxiv.org/abs/2105.13255",
          "publishedOn": "2021-05-28T01:42:14.403Z",
          "wordCount": 587,
          "title": "Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach. (arXiv:2105.13255v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>",
          "description": "Automatic machine translation is super efficient to produce translations yet\ntheir quality is not guaranteed. This technique report introduces TranSmart, a\npractical human-machine interactive translation system that is able to trade\noff translation quality and efficiency. Compared to existing publicly available\ninteractive translation systems, TranSmart supports three key features,\nword-level autocompletion, sentence-level autocompletion and translation\nmemory. By word-level and sentence-level autocompletion, TranSmart allows users\nto interactively translate words in their own manners rather than the strict\nmanner from left to right. In addition, TranSmart has the potential to avoid\nsimilar translation mistakes by using translated sentences in history as its\nmemory. This report presents major functions of TranSmart, algorithms for\nachieving these functions, how to use the TranSmart APIs, and evaluation\nresults of some key functions. TranSmart is publicly available at its homepage\n(https://transmart.qq.com).",
          "link": "http://arxiv.org/abs/2105.13072",
          "publishedOn": "2021-05-28T01:42:14.365Z",
          "wordCount": 567,
          "title": "TranSmart: A Practical Interactive Machine Translation System. (arXiv:2105.13072v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yinyu Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>",
          "description": "Background Knowledge graphs (KGs), especially medical knowledge graphs, are\noften significantly incomplete, so it necessitating a demand for medical\nknowledge graph completion (MedKGC). MedKGC can find new facts based on the\nexited knowledge in the KGs. The path-based knowledge reasoning algorithm is\none of the most important approaches to this task. This type of method has\nreceived great attention in recent years because of its high performance and\ninterpretability. In fact, traditional methods such as path ranking algorithm\n(PRA) take the paths between an entity pair as atomic features. However, the\nmedical KGs are very sparse, which makes it difficult to model effective\nsemantic representation for extremely sparse path features. The sparsity in the\nmedical KGs is mainly reflected in the long-tailed distribution of entities and\npaths. Previous methods merely consider the context structure in the paths of\nthe knowledge graph and ignore the textual semantics of the symbols in the\npath. Therefore, their performance cannot be further improved due to the two\naspects of entity sparseness and path sparseness. To address the above issues,\nthis paper proposes two novel path-based reasoning methods to solve the\nsparsity issues of entity and path respectively, which adopts the textual\nsemantic information of entities and paths for MedKGC. By using the pre-trained\nmodel BERT, combining the textual semantic representations of the entities and\nthe relationships, we model the task of symbolic reasoning in the medical KG as\na numerical computing issue in textual semantic representation.",
          "link": "http://arxiv.org/abs/2105.13074",
          "publishedOn": "2021-05-28T01:42:14.358Z",
          "wordCount": 680,
          "title": "Path-based knowledge reasoning with textual semantic information for medical knowledge graph completion. (arXiv:2105.13074v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "Finding codes given natural language query isb eneficial to the productivity\nof software developers. Future progress towards better semantic matching\nbetween query and code requires richer supervised training resources. To remedy\nthis, we introduce the CoSQA dataset.It includes 20,604 labels for pairs of\nnatural language queries and codes, each annotated by at least 3 human\nannotators. We further introduce a contrastive learning method dubbed CoCLR to\nenhance query-code matching, which works as a data augmenter to bring more\nartificially generated training instances. We show that evaluated on CodeXGLUE\nwith the same CodeBERT model, training on CoSQA improves the accuracy of code\nquestion answering by 5.1%, and incorporating CoCLR brings a further\nimprovement of 10.5%.",
          "link": "http://arxiv.org/abs/2105.13239",
          "publishedOn": "2021-05-28T01:42:14.322Z",
          "wordCount": 582,
          "title": "CoSQA: 20,000+ Web Queries for Code Search and Question Answering. (arXiv:2105.13239v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trott_S/0/1/0/all/0/1\">Sean Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin Bergen</a>",
          "description": "Most words are ambiguous--i.e., they convey distinct meanings in different\ncontexts--and even the meanings of unambiguous words are context-dependent.\nBoth phenomena present a challenge for NLP. Recently, the advent of\ncontextualized word embeddings has led to success on tasks involving lexical\nambiguity, such as Word Sense Disambiguation. However, there are few tasks that\ndirectly evaluate how well these contextualized embeddings accommodate the more\ncontinuous, dynamic nature of word meaning--particularly in a way that matches\nhuman intuitions. We introduce RAW-C, a dataset of graded, human relatedness\njudgments for 112 ambiguous words in context (with 672 sentence pairs total),\nas well as human estimates of sense dominance. The average inter-annotator\nagreement (assessed using a leave-one-annotator-out method) was 0.79. We then\nshow that a measure of cosine distance, computed using contextualized\nembeddings from BERT and ELMo, correlates with human judgments, but that cosine\ndistance also systematically underestimates how similar humans find uses of the\nsame sense of a word to be, and systematically overestimates how similar humans\nfind uses of different-sense homonyms. Finally, we propose a synthesis between\npsycholinguistic theories of the mental lexicon and computational models of\nlexical semantics.",
          "link": "http://arxiv.org/abs/2105.13266",
          "publishedOn": "2021-05-28T01:42:14.314Z",
          "wordCount": 620,
          "title": "RAW-C: Relatedness of Ambiguous Words--in Context (A New Lexical Resource for English). (arXiv:2105.13266v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dopierre_T/0/1/0/all/0/1\">Thomas Dopierre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gravier_C/0/1/0/all/0/1\">Christophe Gravier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logerais_W/0/1/0/all/0/1\">Wilfried Logerais</a>",
          "description": "Recent research considers few-shot intent detection as a meta-learning\nproblem: the model is learning to learn from a consecutive set of small tasks\nnamed episodes. In this work, we propose ProtAugment, a meta-learning algorithm\nfor short texts classification (the intent detection task). ProtAugment is a\nnovel extension of Prototypical Networks, that limits overfitting on the bias\nintroduced by the few-shots classification objective at each episode. It relies\non diverse paraphrasing: a conditional language model is first fine-tuned for\nparaphrasing, and diversity is later introduced at the decoding stage at each\nmeta-learning episode. The diverse paraphrasing is unsupervised as it is\napplied to unlabelled data, and then fueled to the Prototypical Network\ntraining objective as a consistency loss. ProtAugment is the state-of-the-art\nmethod for intent detection meta-learning, at no extra labeling efforts and\nwithout the need to fine-tune a conditional language model on a given\napplication domain.",
          "link": "http://arxiv.org/abs/2105.12995",
          "publishedOn": "2021-05-28T01:42:14.212Z",
          "wordCount": 594,
          "title": "ProtAugment: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning. (arXiv:2105.12995v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1\">J. M. de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rovelli_P/0/1/0/all/0/1\">P. Rovelli</a>",
          "description": "Internet and social media offer firms novel ways of managing their marketing\nstrategy and gain competitive advantage. The groups of users expressing\nthemselves on the Internet about a particular topic, product, or brand are\nfrequently called a virtual tribe or E-tribe. However, there are no automatic\ntools for identifying and studying the characteristics of these virtual tribes.\nTowards this aim, this paper presents Tribefinder, a system to reveal Twitter\nusers' tribal affiliations, by analyzing their tweets and language use. To show\nthe potential of this instrument, we provide an example considering three\nspecific tribal macro-categories: alternative realities, lifestyle, and\nrecreation. In addition, we discuss the different characteristics of each\nidentified tribe, in terms of use of language and social interaction metrics.\nTribefinder illustrates the importance of adopting a new lens for studying\nvirtual tribes, which is crucial for firms to properly design their marketing\nstrategy, and for scholars to extend prior marketing research.",
          "link": "http://arxiv.org/abs/2105.13036",
          "publishedOn": "2021-05-28T01:42:14.202Z",
          "wordCount": 622,
          "title": "Put your money where your mouth is: Using deep learning to identify consumer tribes from word usage. (arXiv:2105.13036v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>",
          "description": "Incorporating external knowledge into Named Entity Recognition (NER) systems\nhas been widely studied in the generic domain. In this paper, we focus on\nclinical domain where only limited data is accessible and interpretability is\nimportant. Recent advancement in technology and the acceleration of clinical\ntrials has resulted in the discovery of new drugs, procedures as well as\nmedical conditions. These factors motivate towards building robust zero-shot\nNER systems which can quickly adapt to new medical terminology. We propose an\nauxiliary gazetteer model and fuse it with an NER system, which results in\nbetter robustness and interpretability across different clinical datasets. Our\ngazetteer based fusion model is data efficient, achieving +1.7 micro-F1 gains\non the i2b2 dataset using 20% training data, and brings + 4.7 micro-F1 gains on\nnovel entity mentions never presented during training. Moreover, our fusion\nmodel is able to quickly adapt to new mentions in gazetteers without\nre-training and the gains from the proposed fusion model are transferable to\nrelated datasets.",
          "link": "http://arxiv.org/abs/2105.13225",
          "publishedOn": "2021-05-28T01:42:14.194Z",
          "wordCount": 596,
          "title": "Neural Entity Recognition with Gazetteer based Fusion. (arXiv:2105.13225v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zujie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Fan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>",
          "description": "Arguably, the visual perception of conversational agents to the physical\nworld is a key way for them to exhibit the human-like intelligence.\nImage-grounded conversation is thus proposed to address this challenge.\nExisting works focus on exploring the multimodal dialog models that ground the\nconversation on a given image. In this paper, we take a step further to study\nimage-grounded conversation under a fully open-ended setting where no paired\ndialog and image are assumed available. Specifically, we present Maria, a\nneural conversation agent powered by the visual world experiences which are\nretrieved from a large-scale image index. Maria consists of three flexible\ncomponents, i.e., text-to-image retriever, visual concept detector and\nvisual-knowledge-grounded response generator. The retriever aims to retrieve a\ncorrelated image to the dialog from an image index, while the visual concept\ndetector extracts rich visual knowledge from the image. Then, the response\ngenerator is grounded on the extracted visual knowledge and dialog context to\ngenerate the target response. Extensive experiments demonstrate Maria\noutperforms previous state-of-the-art methods on automatic metrics and human\nevaluation, and can generate informative responses that have some visual\ncommonsense of the physical world.",
          "link": "http://arxiv.org/abs/2105.13073",
          "publishedOn": "2021-05-28T01:42:14.185Z",
          "wordCount": 629,
          "title": "Maria: A Visual Experience Powered Conversational Agent. (arXiv:2105.13073v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fusheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Neural Machine Translation (NMT) models achieve state-of-the-art performance\non many translation benchmarks. As an active research field in NMT, knowledge\ndistillation is widely applied to enhance the model's performance by\ntransferring teacher model's knowledge on each training sample. However,\nprevious work rarely discusses the different impacts and connections among\nthese samples, which serve as the medium for transferring teacher knowledge. In\nthis paper, we design a novel protocol that can effectively analyze the\ndifferent impacts of samples by comparing various samples' partitions. Based on\nabove protocol, we conduct extensive experiments and find that the teacher's\nknowledge is not the more, the better. Knowledge over specific samples may even\nhurt the whole performance of knowledge distillation. Finally, to address these\nissues, we propose two simple yet effective strategies, i.e., batch-level and\nglobal-level selections, to pick suitable samples for distillation. We evaluate\nour approaches on two large-scale machine translation tasks, WMT'14\nEnglish->German and WMT'19 Chinese->English. Experimental results show that our\napproaches yield up to +1.28 and +0.89 BLEU points improvements over the\nTransformer baseline, respectively.",
          "link": "http://arxiv.org/abs/2105.12967",
          "publishedOn": "2021-05-28T01:42:14.177Z",
          "wordCount": 617,
          "title": "Selective Knowledge Distillation for Neural Machine Translation. (arXiv:2105.12967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viehmann_C/0/1/0/all/0/1\">Christina Viehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_M/0/1/0/all/0/1\">Marcus Maurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quiring_O/0/1/0/all/0/1\">Oliver Quiring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "This work investigates the use of interactively updated label suggestions to\nimprove upon the efficiency of gathering annotations on the task of opinion\nmining in German Covid-19 social media data. We develop guidelines to conduct a\ncontrolled annotation study with social science students and find that\nsuggestions from a model trained on a small, expert-annotated dataset already\nlead to a substantial improvement - in terms of inter-annotator agreement(+.14\nFleiss' $\\kappa$) and annotation quality - compared to students that do not\nreceive any label suggestions. We further find that label suggestions from\ninteractively trained models do not lead to an improvement over suggestions\nfrom a static model. Nonetheless, our analysis of suggestion bias shows that\nannotators remain capable of reflecting upon the suggested label in general.\nFinally, we confirm the quality of the annotated data in transfer learning\nexperiments between different annotator groups. To facilitate further research\nin opinion mining on social media data, we release our collected data\nconsisting of 200 expert and 2,785 student annotations.",
          "link": "http://arxiv.org/abs/2105.12980",
          "publishedOn": "2021-05-28T01:42:14.150Z",
          "wordCount": 653,
          "title": "Investigating label suggestions for opinion mining in German Covid-19 social media. (arXiv:2105.12980v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halterman_A/0/1/0/all/0/1\">Andrew Halterman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1\">Katherine A. Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_B/0/1/0/all/0/1\">Brendan O&#x27;Connor</a>",
          "description": "Automated event extraction in social science applications often requires\ncorpus-level evaluations: for example, aggregating text predictions across\nmetadata and unbiased estimates of recall. We combine corpus-level evaluation\nrequirements with a real-world, social science setting and introduce the\nIndiaPoliceEvents corpus--all 21,391 sentences from 1,257 English-language\nTimes of India articles about events in the state of Gujarat during March 2002.\nOur trained annotators read and label every document for mentions of police\nactivity events, allowing for unbiased recall evaluations. In contrast to other\ndatasets with structured event representations, we gather annotations by posing\nnatural questions, and evaluate off-the-shelf models for three different tasks:\nsentence classification, document ranking, and temporal aggregation of target\nevents. We present baseline results from zero-shot BERT-based models fine-tuned\non natural language inference and passage retrieval tasks. Our novel\ncorpus-level evaluations and annotation approach can guide creation of similar\nsocial-science-oriented resources in the future.",
          "link": "http://arxiv.org/abs/2105.12936",
          "publishedOn": "2021-05-28T01:42:14.136Z",
          "wordCount": 596,
          "title": "Corpus-Level Evaluation for Event QA: The IndiaPoliceEvents Corpus Covering the 2002 Gujarat Violence. (arXiv:2105.12936v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>",
          "description": "Query focused summarization (QFS) models aim to generate summaries from\nsource documents that can answer the given query. Most previous work on QFS\nonly considers the query relevance criterion when producing the summary.\nHowever, studying the effect of answer relevance in the summary generating\nprocess is also important. In this paper, we propose QFS-BART, a model that\nincorporates the explicit answer relevance of the source documents given the\nquery via a question answering model, to generate coherent and answer-related\nsummaries. Furthermore, our model can take advantage of large pre-trained\nmodels which improve the summarization performance significantly. Empirical\nresults on the Debatepedia dataset show that the proposed model achieves the\nnew state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2105.12969",
          "publishedOn": "2021-05-28T01:42:14.129Z",
          "wordCount": 551,
          "title": "Improve Query Focused Abstractive Summarization by Incorporating Answer Relevance. (arXiv:2105.12969v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weizhou Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>",
          "description": "The modeling of conversational context plays a vital role in emotion\nrecognition from conversation (ERC). In this paper, we put forward a novel idea\nof encoding the utterances with a directed acyclic graph (DAG) to better model\nthe intrinsic structure within a conversation, and design a directed acyclic\nneural network,~namely DAG-ERC, to implement this idea.~In an attempt to\ncombine the strengths of conventional graph-based neural models and\nrecurrence-based neural models,~DAG-ERC provides a more intuitive way to model\nthe information flow between long-distance conversation background and nearby\ncontext.~Extensive experiments are conducted on four ERC benchmarks with\nstate-of-the-art models employed as baselines for comparison.~The empirical\nresults demonstrate the superiority of this new model and confirm the\nmotivation of the directed acyclic graph architecture for ERC.",
          "link": "http://arxiv.org/abs/2105.12907",
          "publishedOn": "2021-05-28T01:42:14.122Z",
          "wordCount": 553,
          "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition. (arXiv:2105.12907v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>",
          "description": "While non-autoregressive (NAR) models are showing great promise for machine\ntranslation, their use is limited by their dependence on knowledge distillation\nfrom autoregressive models. To address this issue, we seek to understand why\ndistillation is so effective. Prior work suggests that distilled training data\nis less complex than manual translations. Based on experiments with the\nLevenshtein Transformer and the Mask-Predict NAR models on the WMT14\nGerman-English task, this paper shows that different types of complexity have\ndifferent impacts: while reducing lexical diversity and decreasing reordering\ncomplexity both help NAR learn better alignment between source and target, and\nthus improve translation quality, lexical diversity is the main reason why\ndistillation increases model confidence, which affects the calibration of\ndifferent NAR models differently.",
          "link": "http://arxiv.org/abs/2105.12900",
          "publishedOn": "2021-05-28T01:42:14.114Z",
          "wordCount": 564,
          "title": "How Does Distilled Data Complexity Impact the Quality and Confidence of Non-Autoregressive Machine Translation?. (arXiv:2105.12900v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1\">Q. Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. A. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tickoo_P/0/1/0/all/0/1\">P. Tickoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1\">T. Joshi</a>",
          "description": "In the information economy, individuals' work performance is closely\nassociated with their digital communication strategies. This study combines\nsocial network and semantic analysis to develop a method to identify top\nperformers based on email communication. By reviewing existing literature, we\nidentified the indicators that quantify email communication into measurable\ndimensions. To empirically examine the predictive power of the proposed\nindicators, we collected 2 million email archive of 578 executives in an\ninternational service company. Panel regression was employed to derive\ninterpretable association between email indicators and top performance. The\nresults suggest that top performers tend to assume central network positions\nand have high responsiveness to emails. In email contents, top performers use\nmore positive and complex language, with low emotionality, but rich in\ninfluential words that are probably reused by co-workers. To better explore the\npredictive power of the email indicators, we employed AdaBoost machine learning\nmodels, which achieved 83.56% accuracy in identifying top performers. With\ncluster analysis, we further find three categories of top performers,\n\"networkers\" with central network positions, \"influencers\" with influential\nideas and \"positivists\" with positive sentiments. The findings suggest that top\nperformers have distinctive email communication patterns, laying the foundation\nfor grounding email communication competence in theory. The proposed email\nanalysis method also provides a tool to evaluate the different types of\nindividual communication styles.",
          "link": "http://arxiv.org/abs/2105.13025",
          "publishedOn": "2021-05-28T01:42:14.018Z",
          "wordCount": 676,
          "title": "Finding top performers through email patterns analysis. (arXiv:2105.13025v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Cicero Nogueira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>",
          "description": "The performance of state-of-the-art neural rankers can deteriorate\nsubstantially when exposed to noisy inputs or applied to a new domain. In this\npaper, we present a novel method for fine-tuning neural rankers that can\nsignificantly improve their robustness to out-of-domain data and query\nperturbations. Specifically, a contrastive loss that compares data points in\nthe representation space is combined with the standard ranking loss during\nfine-tuning. We use relevance labels to denote similar/dissimilar pairs, which\nallows the model to learn the underlying matching semantics across different\nquery-document pairs and leads to improved robustness. In experiments with four\npassage ranking datasets, the proposed contrastive fine-tuning method obtains\nimprovements on robustness to query reformulations, noise perturbations, and\nzero-shot transfer for both BERT and BART based rankers. Additionally, our\nexperiments show that contrastive fine-tuning outperforms data augmentation for\nrobustifying neural rankers.",
          "link": "http://arxiv.org/abs/2105.12932",
          "publishedOn": "2021-05-28T01:42:14.002Z",
          "wordCount": 564,
          "title": "Contrastive Fine-tuning Improves Robustness for Neural Rankers. (arXiv:2105.12932v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sorathiya_N/0/1/0/all/0/1\">Nazib Sorathiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chuan-An Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Daniel Chen Daniel Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zin_S/0/1/0/all/0/1\">Scott Zin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">He Sarina Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sharon Xiaolei Huang</a>",
          "description": "Recently there has been a huge interest in dialog systems. This interest has\nalso been developed in the field of the medical domain where researchers are\nfocusing on building a dialog system in the medical domain. This research is\nfocused on the multi-turn dialog system trained on the multi-turn dialog data.\nIt is difficult to gather a huge amount of multi-turn conversational data in\nthe medical domain that is verified by professionals and can be trusted.\nHowever, there are several frequently asked questions (FAQs) or single-turn QA\npairs that have information that is verified by the experts and can be used to\nbuild a multi-turn dialog system.",
          "link": "http://arxiv.org/abs/2105.12887",
          "publishedOn": "2021-05-28T01:42:13.979Z",
          "wordCount": 547,
          "title": "Multi-turn Dialog System on Single-turn Data in Medical Domain. (arXiv:2105.12887v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>",
          "description": "Extensive work has argued in favour of paying crowd workers a wage that is at\nleast equivalent to the U.S. federal minimum wage. Meanwhile, research on\ncollecting high quality annotations suggests using a qualification that\nrequires workers to have previously completed a certain number of tasks. If\nmost requesters who pay fairly require workers to have completed a large number\nof tasks already then workers need to complete a substantial amount of poorly\npaid work before they can earn a fair wage. Through analysis of worker\ndiscussions and guidance for researchers, we estimate that workers spend\napproximately 2.25 months of full time effort on poorly paid tasks in order to\nget the qualifications needed for better paid tasks. We discuss alternatives to\nthis qualification and conduct a study of the correlation between\nqualifications and work quality on two NLP tasks. We find that it is possible\nto reduce the burden on workers while still collecting high quality data.",
          "link": "http://arxiv.org/abs/2105.12762",
          "publishedOn": "2021-05-28T01:42:13.970Z",
          "wordCount": 593,
          "title": "Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing. (arXiv:2105.12762v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>",
          "description": "In this paper, we introduce an event-driven trading strategy that predicts\nstock movements by detecting corporate events from news articles. Unlike\nexisting models that utilize textual features (e.g., bag-of-words) and\nsentiments to directly make stock predictions, we consider corporate events as\nthe driving force behind stock movements and aim to profit from the temporary\nstock mispricing that may occur when corporate events take place. The core of\nthe proposed strategy is a bi-level event detection model. The low-level event\ndetector identifies events' existences from each token, while the high-level\nevent detector incorporates the entire article's representation and the\nlow-level detected results to discover events at the article-level. We also\ndevelop an elaborately-annotated dataset EDT for corporate event detection and\nnews-based stock prediction benchmark. EDT includes 9721 news articles with\ntoken-level event labels as well as 303893 news articles with minute-level\ntimestamps and comprehensive stock price labels. Experiments on EDT indicate\nthat the proposed strategy outperforms all the baselines in winning rate,\nexcess returns over the market, and the average return on each transaction.",
          "link": "http://arxiv.org/abs/2105.12825",
          "publishedOn": "2021-05-28T01:42:13.947Z",
          "wordCount": 621,
          "title": "Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading. (arXiv:2105.12825v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_P/0/1/0/all/0/1\">Pranav Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lucas Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>",
          "description": "We study the problem of learning a named entity recognition (NER) model using\nnoisy la-bels from multiple weak supervision sources. Though cheaper than human\nannotators, weak sources usually yield incomplete, inaccurate, or contradictory\npredictions. To address such challenges, we propose a conditional hidden Markov\nmodel (CHMM). It inherits the hidden Markov model's ability to aggregating the\nlabels from weak sources through unsupervised learning. However, CHMM enhances\nthe hidden Markov model's flexibility and context representation capability by\npredicting token-wise transition and emission probabilities from the BERT\nembeddings of the input tokens. In addition, we refine CHMM's prediction with\nan alternate-training approach (CHMM-AlT). It fine-tunes a BERT-based NER model\nwith the labels inferred by CHMM, and this BERT-NER's output is regarded as an\nadditional weak source to train the CHMM in return. Evaluation on four datasets\nfrom various domains shows that our method is superior to the weakly\nsuper-vised baselines by a wide margin.",
          "link": "http://arxiv.org/abs/2105.12848",
          "publishedOn": "2021-05-28T01:42:13.876Z",
          "wordCount": 587,
          "title": "BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition. (arXiv:2105.12848v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perkins_H/0/1/0/all/0/1\">Hugh Perkins</a>",
          "description": "We propose a new dataset TexRel as a playground for the study of emergent\ncommunications, in particular for relations. By comparison with other relations\ndatasets, TexRel provides rapid training and experimentation, whilst being\nsufficiently large to avoid overfitting in the context of emergent\ncommunications. By comparison with using symbolic inputs, TexRel provides a\nmore realistic alternative whilst remaining efficient and fast to learn. We\ncompare the performance of TexRel with a related relations dataset Shapeworld.\nWe provide baseline performance results on TexRel for sender architectures,\nreceiver architectures and end-to-end architectures. We examine the effect of\nmultitask learning in the context of shapes, colors and relations on accuracy,\ntopological similarity and clustering precision. We investigate whether\nincreasing the size of the latent meaning space improves metrics of\ncompositionality. We carry out a case-study on using TexRel to reproduce the\nresults of an experiment in a recent paper that used symbolic inputs, but using\nour own non-symbolic inputs, from TexRel, instead.",
          "link": "http://arxiv.org/abs/2105.12804",
          "publishedOn": "2021-05-28T01:42:13.826Z",
          "wordCount": 587,
          "title": "TexRel: a Green Family of Datasets for Emergent Communications on Relations. (arXiv:2105.12804v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burke_L/0/1/0/all/0/1\">Lee Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pazdernik_K/0/1/0/all/0/1\">Karl Pazdernik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortin_D/0/1/0/all/0/1\">Daniel Fortin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1\">Benjamin Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goychayev_R/0/1/0/all/0/1\">Rustam Goychayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattingly_J/0/1/0/all/0/1\">John Mattingly</a>",
          "description": "Natural language processing (NLP) tasks (text classification, named entity\nrecognition, etc.) have seen revolutionary improvements over the last few\nyears. This is due to language models such as BERT that achieve deep knowledge\ntransfer by using a large pre-trained model, then fine-tuning the model on\nspecific tasks. The BERT architecture has shown even better performance on\ndomain-specific tasks when the model is pre-trained using domain-relevant\ntexts. Inspired by these recent advancements, we have developed NukeLM, a\nnuclear-domain language model pre-trained on 1.5 million abstracts from the\nU.S. Department of Energy Office of Scientific and Technical Information (OSTI)\ndatabase. This NukeLM model is then fine-tuned for the classification of\nresearch articles into either binary classes (related to the nuclear fuel cycle\n[NFC] or not) or multiple categories related to the subject of the article. We\nshow that continued pre-training of a BERT-style architecture prior to\nfine-tuning yields greater performance on both article classification tasks.\nThis information is critical for properly triaging manuscripts, a necessary\ntask for better understanding citation networks that publish in the nuclear\nspace, and for uncovering new areas of research in the nuclear (or\nnuclear-relevant) domains.",
          "link": "http://arxiv.org/abs/2105.12192",
          "publishedOn": "2021-05-27T01:32:29.896Z",
          "wordCount": 631,
          "title": "NukeLM: Pre-Trained and Fine-Tuned Language Models for the Nuclear and Energy Domains. (arXiv:2105.12192v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12172",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Junhyeong Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Heesoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaemin Jo</a>",
          "description": "We present IntelliCAT, an interactive translation interface with neural\nmodels that streamline the post-editing process on machine translation output.\nWe leverage two quality estimation (QE) models at different granularities:\nsentence-level QE, to predict the quality of each machine-translated sentence,\nand word-level QE, to locate the parts of the machine-translated sentence that\nneed correction. Additionally, we introduce a novel translation suggestion\nmodel conditioned on both the left and right contexts, providing alternatives\nfor specific words or phrases for correction. Finally, with word alignments,\nIntelliCAT automatically preserves the original document's styles in the\ntranslated document. The experimental results show that post-editing based on\nthe proposed QE and translation suggestions can significantly improve\ntranslation quality. Furthermore, a user study reveals that three features\nprovided in IntelliCAT significantly accelerate the post-editing task,\nachieving a 52.9\\% speedup in translation time compared to translating from\nscratch. The interface is publicly available at\nhttps://intellicat.beringlab.com/.",
          "link": "http://arxiv.org/abs/2105.12172",
          "publishedOn": "2021-05-27T01:32:29.684Z",
          "wordCount": 585,
          "title": "IntelliCAT: Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion. (arXiv:2105.12172v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12667",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_E/0/1/0/all/0/1\">Elizabeth Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>",
          "description": "Parsing spoken dialogue poses unique difficulties, including disfluencies and\nunmarked boundaries between sentence-like units. Previous work has shown that\nprosody can help with parsing disfluent speech (Tran et al. 2018), but has\nassumed that the input to the parser is already segmented into sentence-like\nunits (SUs), which isn't true in existing speech applications. We investigate\nhow prosody affects a parser that receives an entire dialogue turn as input (a\nturn-based model), instead of gold standard pre-segmented SUs (an SU-based\nmodel). In experiments on the English Switchboard corpus, we find that when\nusing transcripts alone, the turn-based model has trouble segmenting SUs,\nleading to worse parse performance than the SU-based model. However, prosody\ncan effectively replace gold standard SU boundaries: with prosody, the\nturn-based model performs as well as the SU-based model (90.79 vs. 90.65 F1\nscore, respectively), despite performing two tasks (SU segmentation and\nparsing) rather than one (parsing alone). Analysis shows that pitch and\nintensity features are the most important for this corpus, since they allow the\nmodel to correctly distinguish an SU boundary from a speech disfluency -- a\ndistinction that the model otherwise struggles to make.",
          "link": "http://arxiv.org/abs/2105.12667",
          "publishedOn": "2021-05-27T01:32:29.651Z",
          "wordCount": 608,
          "title": "Prosodic segmentation for parsing spoken dialogue. (arXiv:2105.12667v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Villegas_D/0/1/0/all/0/1\">Danae S&#xe1;nchez Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokaram_S/0/1/0/all/0/1\">Saeid Mokaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>",
          "description": "Online political advertising is a central aspect of modern election\ncampaigning for influencing public opinion. Computational analysis of political\nads is of utmost importance in political science to understand the\ncharacteristics of digital campaigning. It is also important in computational\nlinguistics to study features of political discourse and communication on a\nlarge scale. In this work, we present the first computational study on online\npolitical ads with the aim to (1) infer the political ideology of an ad\nsponsor; and (2) identify whether the sponsor is an official political party or\na third-party organization. We develop two new large datasets for the two tasks\nconsisting of ads from the U.S.. Evaluation results show that our approach that\ncombines textual and visual information from pre-trained neural models\noutperforms a state-of-the-art method for generic commercial ad classification.\nFinally, we provide an in-depth analysis of the limitations of our\nbest-performing models and linguistic analysis to study the characteristics of\npolitical ads discourse.",
          "link": "http://arxiv.org/abs/2105.04047",
          "publishedOn": "2021-05-27T01:32:29.301Z",
          "wordCount": 599,
          "title": "Analyzing Online Political Advertisements. (arXiv:2105.04047v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We propose Predict then Interpolate (PI), a simple algorithm for learning\ncorrelations that are stable across environments. The algorithm follows from\nthe intuition that when using a classifier trained on one environment to make\npredictions on examples from another environment, its mistakes are informative\nas to which correlations are unstable. In this work, we prove that by\ninterpolating the distributions of the correct predictions and the wrong\npredictions, we can uncover an oracle distribution where the unstable\ncorrelation vanishes. Since the oracle interpolation coefficients are not\naccessible, we use group distributionally robust optimization to minimize the\nworst-case risk across all such interpolations. We evaluate our method on both\ntext classification and image classification. Empirical results demonstrate\nthat our algorithm is able to learn robust classifiers (outperforms IRM by\n23.85% on synthetic environments and 12.41% on natural environments). Our code\nand data are available at https://github.com/YujiaBao/Predict-then-Interpolate.",
          "link": "http://arxiv.org/abs/2105.12628",
          "publishedOn": "2021-05-27T01:32:29.291Z",
          "wordCount": 591,
          "title": "Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers. (arXiv:2105.12628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.14004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wentao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>",
          "description": "Machine Reading Comprehension (MRC) is an important testbed for evaluating\nmodels' natural language understanding (NLU) ability. There has been rapid\nprogress in this area, with new models achieving impressive performance on\nvarious benchmarks. However, existing benchmarks only evaluate models on\nin-domain test sets without considering their robustness under test-time\nperturbations or adversarial attacks. To fill this important gap, we construct\nAdvRACE (Adversarial RACE), a new model-agnostic benchmark for evaluating the\nrobustness of MRC models under four different types of adversarial attacks,\nincluding our novel distractor extraction and generation attacks. We show that\nstate-of-the-art (SOTA) models are vulnerable to all of these attacks. We\nconclude that there is substantial room for building more robust MRC models and\nour benchmark can help motivate and measure progress in this area. We release\nour data and code at https://github.com/NoviScl/AdvRACE .",
          "link": "http://arxiv.org/abs/2004.14004",
          "publishedOn": "2021-05-27T01:32:29.161Z",
          "wordCount": 599,
          "title": "Benchmarking Robustness of Machine Reading Comprehension Models. (arXiv:2004.14004v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1\">Ryuichi Takanobu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Pre-trained Language Models (PLMs) have shown superior performance on various\ndownstream Natural Language Processing (NLP) tasks. However, conventional\npre-training objectives do not explicitly model relational facts in text, which\nare crucial for textual understanding. To address this issue, we propose a\nnovel contrastive learning framework ERICA to obtain a deep understanding of\nthe entities and their relations in text. Specifically, we define two novel\npre-training tasks to better understand entities and relations: (1) the entity\ndiscrimination task to distinguish which tail entity can be inferred by the\ngiven head entity and relation; (2) the relation discrimination task to\ndistinguish whether two relations are close or not semantically, which involves\ncomplex relational reasoning. Experimental results demonstrate that ERICA can\nimprove typical PLMs (BERT and RoBERTa) on several language understanding\ntasks, including relation extraction, entity typing and question answering,\nespecially under low-resource settings.",
          "link": "http://arxiv.org/abs/2012.15022",
          "publishedOn": "2021-05-27T01:32:29.156Z",
          "wordCount": 629,
          "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning. (arXiv:2012.15022v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng-Chieh Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jheng-Hong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "A vital step towards the widespread adoption of neural retrieval models is\ntheir resource efficiency throughout the training, indexing and query\nworkflows. The neural IR community made great advancements in training\neffective dual-encoder dense retrieval (DR) models recently. A dense text\nretrieval model uses a single vector representation per query and passage to\nscore a match, which enables low-latency first stage retrieval with a nearest\nneighbor search. Increasingly common, training approaches require enormous\ncompute power, as they either conduct negative passage sampling out of a\ncontinuously updating refreshing index or require very large batch sizes for\nin-batch negative sampling. Instead of relying on more compute capability, we\nintroduce an efficient topic-aware query and balanced margin sampling\ntechnique, called TAS-Balanced. We cluster queries once before training and\nsample queries out of a cluster per batch. We train our lightweight 6-layer DR\nmodel with a novel dual-teacher supervision that combines pairwise and in-batch\nnegative teachers. Our method is trainable on a single consumer-grade GPU in\nunder 48 hours (as opposed to a common configuration of 8x V100s). We show that\nour TAS-Balanced training method achieves state-of-the-art low-latency (64ms\nper query) results on two TREC Deep Learning Track query sets. Evaluated on\nNDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by\n11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces\nthe first dense retriever that outperforms every other method on recall at any\ncutoff on TREC-DL and allows more resource intensive re-ranking models to\noperate on fewer passages to improve results further.",
          "link": "http://arxiv.org/abs/2104.06967",
          "publishedOn": "2021-05-27T01:32:27.936Z",
          "wordCount": 730,
          "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling. (arXiv:2104.06967v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>",
          "description": "Proposal of large-scale datasets has facilitated research on deep neural\nmodels for news summarization. Deep learning can also be potentially useful for\nspoken dialogue summarization, which can benefit a range of real-life scenarios\nincluding customer service management and medication tracking. To this end, we\npropose DialogSum, a large-scale labeled dialogue summarization dataset. We\nconduct empirical analysis on DialogSum using state-of-the-art neural\nsummarizers. Experimental results show unique challenges in dialogue\nsummarization, such as spoken terms, special discourse structures, coreferences\nand ellipsis, pragmatics and social commonsense, which require specific\nrepresentation learning technologies to better deal with.",
          "link": "http://arxiv.org/abs/2105.06762",
          "publishedOn": "2021-05-27T01:32:27.923Z",
          "wordCount": 541,
          "title": "DialogSum: A Real-Life Scenario Dialogue Summarization Dataset. (arXiv:2105.06762v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+SM_A/0/1/0/all/0/1\">Akash SM</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph Weischedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>",
          "description": "With the recent advances of open-domain story generation, the lack of\nreliable automatic evaluation metrics becomes an increasingly imperative issue\nthat hinders the fast development of story generation. According to conducted\nresearches in this regard, learnable evaluation metrics have promised more\naccurate assessments by having higher correlations with human judgments. A\ncritical bottleneck of obtaining a reliable learnable evaluation metric is the\nlack of high-quality training data for classifiers to efficiently distinguish\nplausible and implausible machine-generated stories. Previous works relied on\n\\textit{heuristically manipulated} plausible examples to mimic possible system\ndrawbacks such as repetition, contradiction, or irrelevant content in the text\nlevel, which can be \\textit{unnatural} and \\textit{oversimplify} the\ncharacteristics of implausible machine-generated stories. We propose to tackle\nthese issues by generating a more comprehensive set of implausible stories\nusing {\\em plots}, which are structured representations of controllable factors\nused to generate stories. Since these plots are compact and structured, it is\neasier to manipulate them to generate text with targeted undesirable\nproperties, while at the same time maintain the grammatical correctness and\nnaturalness of the generated sentences. To improve the quality of generated\nimplausible stories, we further apply the adversarial filtering procedure\npresented by \\citet{zellers2018swag} to select a more nuanced set of\nimplausible texts. Experiments show that the evaluation metrics trained on our\ngenerated data result in more reliable automatic assessments that correlate\nremarkably better with human judgments compared to the baselines.",
          "link": "http://arxiv.org/abs/2104.05801",
          "publishedOn": "2021-05-27T01:32:27.697Z",
          "wordCount": 702,
          "title": "Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation. (arXiv:2104.05801v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Peter Wallis</a>",
          "description": "As Stefan Kopp and Nicole Kramer say in their recent paper[Frontiers in\nPsychology 12 (2021) 597], despite some very impressive demonstrations over the\nlast decade or so, we still don't know how how to make a computer have a half\ndecent conversation with a human. They argue that the capabilities required to\ndo this include incremental joint co-construction and mentalizing. Although\nagreeing whole heartedly with their statement of the problem, this paper argues\nfor a different approach to the solution based on the \"new\" AI of situated\naction.",
          "link": "http://arxiv.org/abs/2105.01949",
          "publishedOn": "2021-05-27T01:32:27.682Z",
          "wordCount": 546,
          "title": "Mind Reading at Work: Cooperation without common ground. (arXiv:2105.01949v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Touileb_S/0/1/0/all/0/1\">Samia Touileb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_J/0/1/0/all/0/1\">Jeremy Barnes</a>",
          "description": "Recent years have seen a rise in interest for cross-lingual transfer between\nlanguages with similar typology, and between languages of various scripts.\nHowever, the interplay between language similarity and difference in script on\ncross-lingual transfer is a less studied problem. We explore this interplay on\ncross-lingual transfer for two supervised tasks, namely part-of-speech tagging\nand sentiment analysis. We introduce a newly annotated corpus of Algerian\nuser-generated comments comprising parallel annotations of Algerian written in\nLatin, Arabic, and code-switched scripts, as well as annotations for sentiment\nand topic categories. We perform baseline experiments by fine-tuning\nmulti-lingual language models. We further explore the effect of script vs.\nlanguage similarity in cross-lingual transfer by fine-tuning multi-lingual\nmodels on languages which are a) typologically distinct, but use the same\nscript, b) typologically similar, but use a distinct script, or c) are\ntypologically similar and use the same script. We find there is a delicate\nrelationship between script and typology for part-of-speech, while sentiment\nanalysis is less sensitive.",
          "link": "http://arxiv.org/abs/2105.07400",
          "publishedOn": "2021-05-27T01:32:27.667Z",
          "wordCount": 620,
          "title": "The interplay between language similarity and script on a novel multi-layer Algerian dialect corpus. (arXiv:2105.07400v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1\">Raphael Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>",
          "description": "Car-focused navigation services are based on turns and distances of named\nstreets, whereas navigation instructions naturally used by humans are centered\naround physical objects called landmarks. We present a neural model that takes\nOpenStreetMap representations as input and learns to generate navigation\ninstructions that contain visible and salient landmarks from human natural\nlanguage instructions. Routes on the map are encoded in a location- and\nrotation-invariant graph representation that is decoded into natural language\ninstructions. Our work is based on a novel dataset of 7,672 crowd-sourced\ninstances that have been verified by human navigation in Street View. Our\nevaluation shows that the navigation instructions generated by our system have\nsimilar properties as human-generated instructions, and lead to successful\nhuman navigation in Street View.",
          "link": "http://arxiv.org/abs/2012.15329",
          "publishedOn": "2021-05-27T01:32:27.661Z",
          "wordCount": 593,
          "title": "Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem. (arXiv:2012.15329v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1\">Steven Schockaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>",
          "description": "Analogies play a central role in human commonsense reasoning. The ability to\nrecognize analogies such as \"eye is to seeing what ear is to hearing\",\nsometimes referred to as analogical proportions, shape how we structure\nknowledge and understand language. Surprisingly, however, the task of\nidentifying such analogies has not yet received much attention in the language\nmodel era. In this paper, we analyze the capabilities of transformer-based\nlanguage models on this unsupervised task, using benchmarks obtained from\neducational settings, as well as more commonly used datasets. We find that\noff-the-shelf language models can identify analogies to a certain extent, but\nstruggle with abstract and complex relations, and results are highly sensitive\nto model architecture and hyperparameters. Overall the best results were\nobtained with GPT-2 and RoBERTa, while configurations using BERT were not able\nto outperform word embedding models. Our results raise important questions for\nfuture work about how, and to what extent, pre-trained language models capture\nknowledge about abstract semantic relations.",
          "link": "http://arxiv.org/abs/2105.04949",
          "publishedOn": "2021-05-27T01:32:27.644Z",
          "wordCount": 628,
          "title": "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?. (arXiv:2105.04949v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>",
          "description": "A sememe is defined as the minimum semantic unit in linguistics. Sememe\nknowledge bases (SKBs), which comprise words annotated with sememes, enable\nsememes to be applied to natural language processing. So far a large body of\nresearch has showcased the unique advantages and effectiveness of SKBs in\nvarious tasks. However, most languages have no SKBs, and manual construction of\nSKBs is time-consuming and labor-intensive. To tackle this challenge, we\npropose a simple and fully automatic method of building an SKB via an existing\ndictionary. We use this method to build an English SKB and a French SKB, and\nconduct comprehensive evaluations from both intrinsic and extrinsic\nperspectives. Experimental results demonstrate that the automatically built\nEnglish SKB is even superior to HowNet, the most widely used SKB that takes\ndecades to build manually. And both the English and French SKBs can bring\nobvious performance enhancement in multiple downstream tasks. All the code and\ndata of this paper (except the copyrighted dictionaries) can be obtained at\nhttps://github.com/thunlp/DictSKB.",
          "link": "http://arxiv.org/abs/2105.12585",
          "publishedOn": "2021-05-27T01:32:27.627Z",
          "wordCount": 611,
          "title": "Automatic Construction of Sememe Knowledge Bases via Dictionaries. (arXiv:2105.12585v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1905.13322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goodrich_B/0/1/0/all/0/1\">Ben Goodrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1\">Vinay Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1\">Mohammad Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peter J Liu</a>",
          "description": "We propose a model-based metric to estimate the factual accuracy of generated\ntext that is complementary to typical scoring schemes like ROUGE\n(Recall-Oriented Understudy for Gisting Evaluation) and BLEU (Bilingual\nEvaluation Understudy). We introduce and release a new large-scale dataset\nbased on Wikipedia and Wikidata to train relation classifiers and end-to-end\nfact extraction models. The end-to-end models are shown to be able to extract\ncomplete sets of facts from datasets with full pages of text. We then analyse\nmultiple models that estimate factual accuracy on a Wikipedia text\nsummarization task, and show their efficacy compared to ROUGE and other\nmodel-free variants by conducting a human evaluation study.",
          "link": "http://arxiv.org/abs/1905.13322",
          "publishedOn": "2021-05-27T01:32:27.618Z",
          "wordCount": 584,
          "title": "Assessing The Factual Accuracy of Generated Text. (arXiv:1905.13322v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papantoniou_K/0/1/0/all/0/1\">Katerina Papantoniou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakos_P/0/1/0/all/0/1\">Panagiotis Papadakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patkos_T/0/1/0/all/0/1\">Theodore Patkos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flouris_G/0/1/0/all/0/1\">Giorgos Flouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plexousakis_D/0/1/0/all/0/1\">Dimitris Plexousakis</a>",
          "description": "Deception detection is a task with many applications both in direct physical\nand in computer-mediated communication. Our focus is on automatic deception\ndetection in text across cultures. We view culture through the prism of the\nindividualism/collectivism dimension and we approximate culture by using\ncountry as a proxy. Having as a starting point recent conclusions drawn from\nthe social psychology discipline, we explore if differences in the usage of\nspecific linguistic features of deception across cultures can be confirmed and\nattributed to norms in respect to the individualism/collectivism divide. We\nalso investigate if a universal feature set for cross-cultural text deception\ndetection tasks exists. We evaluate the predictive power of different feature\nsets and approaches. We create culture/language-aware classifiers by\nexperimenting with a wide range of n-gram features based on phonology,\nmorphology and syntax, other linguistic cues like word and phoneme counts,\npronouns use, etc., and token embeddings. We conducted our experiments over 11\ndatasets from 5 languages i.e., English, Dutch, Russian, Spanish and Romanian,\nfrom six countries (US, Belgium, India, Russia, Mexico and Romania), and we\napplied two classification methods i.e, logistic regression and fine-tuned BERT\nmodels. The results showed that our task is fairly complex and demanding. There\nare indications that some linguistic cues of deception have cultural origins,\nand are consistent in the context of diverse domains and dataset settings for\nthe same language. This is more evident for the usage of pronouns and the\nexpression of sentiment in deceptive language. The results of this work show\nthat the automatic deception detection across cultures and languages cannot be\nhandled in a unified manner, and that such approaches should be augmented with\nknowledge about cultural differences and the domains of interest.",
          "link": "http://arxiv.org/abs/2105.12530",
          "publishedOn": "2021-05-27T01:32:27.589Z",
          "wordCount": 732,
          "title": "Deception detection in text and its relation to the cultural dimension of individualism/collectivism. (arXiv:2105.12530v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Partanen_N/0/1/0/all/0/1\">Niko Partanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueter_J/0/1/0/all/0/1\">Jack Rueter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>",
          "description": "We train neural models for morphological analysis, generation and\nlemmatization for morphologically rich languages. We present a method for\nautomatically extracting substantially large amount of training data from FSTs\nfor 22 languages, out of which 17 are endangered. The neural models follow the\nsame tagset as the FSTs in order to make it possible to use them as fallback\nsystems together with the FSTs. The source code, models and datasets have been\nreleased on Zenodo.",
          "link": "http://arxiv.org/abs/2105.12428",
          "publishedOn": "2021-05-27T01:32:27.583Z",
          "wordCount": 522,
          "title": "Neural Morphology Dataset and Models for Multiple Languages, from the Large to the Endangered. (arXiv:2105.12428v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antonacci_G/0/1/0/all/0/1\">G. Antonacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_A/0/1/0/all/0/1\">A. Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. Gloor</a>",
          "description": "Purpose: The purpose of this paper is to identify the factors influencing the\ngrowth of healthcare virtual communities of practice (VCoPs) through a\nseven-year longitudinal study conducted using metrics from social-network and\nsemantic analysis. By studying online communication along the three dimensions\nof social interactions (connectivity, interactivity and language use), the\nauthors aim to provide VCoP managers with valuable insights to improve the\nsuccess of their communities. Design/methodology/approach: Communications over\na period of seven years (April 2008 to April 2015) and between 14,000 members\nof 16 different healthcare VCoPs coexisting on the same web platform were\nanalysed. Multilevel regression models were used to reveal the main\ndeterminants of community growth over time. Independent variables were derived\nfrom social network and semantic analysis measures. Findings: Results show that\nstructural and content-based variables predict the growth of the community.\nProgressively, more people will join a community if its structure is more\ncentralised, leaders are more dynamic (they rotate more) and the language used\nin the posts is less complex. Research limitations/implications: The available\ndata set included one Web platform and a limited number of control variables.\nTo consolidate the findings of the present study, the experiment should be\nreplicated on other healthcare VCoPs. Originality/value: The study provides\nuseful recommendations for setting up and nurturing the growth of professional\ncommunities, considering, at the same time, the interaction patterns among the\ncommunity members, the dynamic evolution of these interactions and the use of\nlanguage. New analytical tools are presented, together with the use of\ninnovative interaction metrics, that can significantly influence community\ngrowth, such as rotating leadership.",
          "link": "http://arxiv.org/abs/2105.12659",
          "publishedOn": "2021-05-27T01:32:27.577Z",
          "wordCount": 741,
          "title": "It is rotating leaders who build the swarm: social network determinants of growth for healthcare virtual communities of practice. (arXiv:2105.12659v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Automatic text summarization has enjoyed great progress over the last years.\nHowever, there is little research that investigates whether the current\nresearch focus adheres to users' needs. Importantly, these needs are dependent\non the envisioned target group of the generated summaries. One such important\ntarget group is formed by students, due to their usage of summaries in their\nstudy activities. For this reason, we investigate students' needs regarding\nautomatically generated summaries by means of a survey amongst university\nstudents and find that the current direction of the field does not fully align\nwith their needs. Motivated by our findings, we formulate three groups of\nimplications that together help us formulate a renewed perspective on future\nresearch on automatic summarization. First, the educational domain requires a\nbroader perspective on automatic summarization, beyond the approaches that are\ncurrently the standard. We illustrate how we can expand these approaches\nregarding the input material, the purpose of the summaries and their potential\nformat and we define requirements for datasets that can facilitate these\nresearch directions. Second, we propose a methodology to evaluate the\nusefulness of a summary based on the identified needs of a target group. Third,\nin more general terms, we hope that our survey will be reused to investigate\nthe needs of different user groups of automatically generated summaries to\nbroaden our perspective even further.",
          "link": "http://arxiv.org/abs/2012.07619",
          "publishedOn": "2021-05-27T01:32:27.571Z",
          "wordCount": 706,
          "title": "What Makes a Good Summary? Investigating the Focus of Automatic Summarization in an Educational Context. (arXiv:2012.07619v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1\">Hansi Hettiarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adedoyin_Olowe_M/0/1/0/all/0/1\">Mariam Adedoyin-Olowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhogal_J/0/1/0/all/0/1\">Jagdev Bhogal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaber_M/0/1/0/all/0/1\">Mohamed Medhat Gaber</a>",
          "description": "Social media is becoming a primary medium to discuss what is happening around\nthe world. Therefore, the data generated by social media platforms contain rich\ninformation which describes the ongoing events. Further, the timeliness\nassociated with these data is capable of facilitating immediate insights.\nHowever, considering the dynamic nature and high volume of data production in\nsocial media data streams, it is impractical to filter the events manually and\ntherefore, automated event detection mechanisms are invaluable to the\ncommunity. Apart from a few notable exceptions, most previous research on\nautomated event detection have focused only on statistical and syntactical\nfeatures in data and lacked the involvement of underlying semantics which are\nimportant for effective information retrieval from text since they represent\nthe connections between words and their meanings. In this paper, we propose a\nnovel method termed Embed2Detect for event detection in social media by\ncombining the characteristics in word embeddings and hierarchical agglomerative\nclustering. The adoption of word embeddings gives Embed2Detect the capability\nto incorporate powerful semantical features into event detection and overcome a\nmajor limitation inherent in previous approaches. We experimented our method on\ntwo recent real social media data sets which represent the sports and political\ndomain and also compared the results to several state-of-the-art methods. The\nobtained results show that Embed2Detect is capable of effective and efficient\nevent detection and it outperforms the recent event detection methods. For the\nsports data set, Embed2Detect achieved 27% higher F-measure than the\nbest-performed baseline and for the political data set, it was an increase of\n29%.",
          "link": "http://arxiv.org/abs/2006.05908",
          "publishedOn": "2021-05-27T01:32:27.565Z",
          "wordCount": 768,
          "title": "Embed2Detect: Temporally Clustered Embedded Words for Event Detection in Social Media. (arXiv:2006.05908v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suster_S/0/1/0/all/0/1\">Simon &#x160;uster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1\">Antonio Jimeno Yepes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_D/0/1/0/all/0/1\">David Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otmakhova_Y/0/1/0/all/0/1\">Yulia Otmakhova</a>",
          "description": "The COVID-19 pandemic has driven ever-greater demand for tools which enable\nefficient exploration of biomedical literature. Although semi-structured\ninformation resulting from concept recognition and detection of the defining\nelements of clinical trials (e.g. PICO criteria) has been commonly used to\nsupport literature search, the contributions of this abstraction remain poorly\nunderstood, especially in relation to text-based retrieval. In this study, we\ncompare the results retrieved by a standard search engine with those filtered\nusing clinically-relevant concepts and their relations. With analysis based on\nthe annotations from the TREC-COVID shared task, we obtain quantitative as well\nas qualitative insights into characteristics of relational and concept-based\nliterature exploration. Most importantly, we find that the relational concept\nselection filters the original retrieved collection in a way that decreases the\nproportion of unjudged documents and increases the precision, which means that\nthe user is likely to be exposed to a larger number of relevant documents.",
          "link": "http://arxiv.org/abs/2105.12261",
          "publishedOn": "2021-05-27T01:32:27.540Z",
          "wordCount": 643,
          "title": "Impact of detecting clinical trial elements in exploration of COVID-19 literature. (arXiv:2105.12261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>",
          "description": "Backdoor attacks are a kind of insidious security threat against machine\nlearning models. After being injected with a backdoor in training, the victim\nmodel will produce adversary-specified outputs on the inputs embedded with\npredesigned triggers but behave properly on normal inputs during inference. As\na sort of emergent attack, backdoor attacks in natural language processing\n(NLP) are investigated insufficiently. As far as we know, almost all existing\ntextual backdoor attack methods insert additional contents into normal samples\nas triggers, which causes the trigger-embedded samples to be detected and the\nbackdoor attacks to be blocked without much effort. In this paper, we propose\nto use syntactic structure as the trigger in textual backdoor attacks. We\nconduct extensive experiments to demonstrate that the syntactic trigger-based\nattack method can achieve comparable attack performance (almost 100\\% success\nrate) to the insertion-based methods but possesses much higher invisibility and\nstronger resistance to defenses. These results also reveal the significant\ninsidiousness and harmfulness of textual backdoor attacks. All the code and\ndata of this paper can be obtained at https://github.com/thunlp/HiddenKiller.",
          "link": "http://arxiv.org/abs/2105.12400",
          "publishedOn": "2021-05-27T01:32:27.532Z",
          "wordCount": 623,
          "title": "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger. (arXiv:2105.12400v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yong Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1\">Rong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haihong Tang</a>",
          "description": "Previous studies show effective of pre-trained language models for sentiment\nanalysis. However, most of these studies ignore the importance of sentimental\ninformation for pre-trained models.Therefore, we fully investigate the\nsentimental information for pre-trained models and enhance pre-trained language\nmodels with semantic graphs for sentiment analysis.In particular, we introduce\nSemantic Graphs based Pre-training(SGPT) using semantic graphs to obtain\nsynonym knowledge for aspect-sentiment pairs and similar aspect/sentiment\nterms.We then optimize the pre-trained language model with the semantic\ngraphs.Empirical studies on several downstream tasks show that proposed model\noutperforms strong pre-trained baselines. The results also show the\neffectiveness of proposed semantic graphs for pre-trained model.",
          "link": "http://arxiv.org/abs/2105.12305",
          "publishedOn": "2021-05-27T01:32:27.525Z",
          "wordCount": 544,
          "title": "SGPT: Semantic Graphs based Pre-training for Aspect-based Sentiment Analysis. (arXiv:2105.12305v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1\">Andrew Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkpen_D/0/1/0/all/0/1\">Diana Inkpen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonie_R/0/1/0/all/0/1\">R&#x103;zvan Andonie</a>",
          "description": "The introduction of Transformer neural networks has changed the landscape of\nNatural Language Processing (NLP) during the last years. So far, none of the\nvisualization systems has yet managed to examine all the facets of the\nTransformers. This gave us the motivation of the current work. We propose a new\nNLP Transformer context-sensitive visualization method that leverages existing\nNLP tools to find the most significant groups of tokens (words) that have the\ngreatest effect on the output, thus preserving some context from the original\ntext. First, we use a sentence-level dependency parser to highlight promising\nword groups. The dependency parser creates a tree of relationships between the\nwords in the sentence. Next, we systematically remove adjacent and non-adjacent\ntuples of \\emph{n} tokens from the input text, producing several new texts with\nthose tokens missing. The resulting texts are then passed to a pre-trained BERT\nmodel. The classification output is compared with that of the full text, and\nthe difference in the activation strength is recorded. The modified texts that\nproduce the largest difference in the target classification output neuron are\nselected, and the combination of removed words are then considered to be the\nmost influential on the model's output. Finally, the most influential word\ncombinations are visualized in a heatmap.",
          "link": "http://arxiv.org/abs/2105.12202",
          "publishedOn": "2021-05-27T01:32:27.510Z",
          "wordCount": 644,
          "title": "Context-Sensitive Visualization of Deep Learning Natural Language Processing Models. (arXiv:2105.12202v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Heng-Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zizhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>",
          "description": "Chinese Spell Checking (CSC) aims to detect and correct erroneous characters\nfor user-generated text in the Chinese language. Most of the Chinese spelling\nerrors are misused semantically, phonetically or graphically similar\ncharacters. Previous attempts noticed this phenomenon and try to use the\nsimilarity for this task. However, these methods use either heuristics or\nhandcrafted confusion sets to predict the correct character. In this paper, we\npropose a Chinese spell checker called ReaLiSe, by directly leveraging the\nmultimodal information of the Chinese characters. The ReaLiSe model tackles the\nCSC task by (1) capturing the semantic, phonetic and graphic information of the\ninput characters, and (2) selectively mixing the information in these\nmodalities to predict the correct output. Experiments on the SIGHAN benchmarks\nshow that the proposed model outperforms strong baselines by a large margin.",
          "link": "http://arxiv.org/abs/2105.12306",
          "publishedOn": "2021-05-27T01:32:27.487Z",
          "wordCount": 577,
          "title": "Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking. (arXiv:2105.12306v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Luyang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winestock_C/0/1/0/all/0/1\">Christopher Winestock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>",
          "description": "Medical entity retrieval is an integral component for understanding and\ncommunicating information across various health systems. Current approaches\ntend to work well on specific medical domains but generalize poorly to unseen\nsub-specialties. This is of increasing concern under a public health crisis as\nnew medical conditions and drug treatments come to light frequently. Zero-shot\nretrieval is challenging due to the high degree of ambiguity and variability in\nmedical corpora, making it difficult to build an accurate similarity measure\nbetween mentions and concepts. Medical knowledge graphs (KG), however, contain\nrich semantics including large numbers of synonyms as well as its curated\ngraphical structures. To take advantage of this valuable information, we\npropose a suite of learning tasks designed for training efficient zero-shot\nentity retrieval models. Without requiring any human annotation, our knowledge\ngraph enriched architecture significantly outperforms common zero-shot\nbenchmarks including BM25 and Clinical BERT with 7% to 30% higher recall across\nmultiple major medical ontologies, such as UMLS, SNOMED, and ICD-10.",
          "link": "http://arxiv.org/abs/2105.12682",
          "publishedOn": "2021-05-27T01:32:27.480Z",
          "wordCount": 596,
          "title": "Zero-shot Medical Entity Retrieval without Annotation: Learning From Rich Knowledge Graph Semantics. (arXiv:2105.12682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deruyttere_T/0/1/0/all/0/1\">Thierry Deruyttere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collell_G/0/1/0/all/0/1\">Guillem Collell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>",
          "description": "We propose a new spatial memory module and a spatial reasoner for the Visual\nGrounding (VG) task. The goal of this task is to find a certain object in an\nimage based on a given textual query. Our work focuses on integrating the\nregions of a Region Proposal Network (RPN) into a new multi-step reasoning\nmodel which we have named a Multimodal Spatial Region Reasoner (MSRR). The\nintroduced model uses the object regions from an RPN as initialization of a 2D\nspatial memory and then implements a multi-step reasoning process scoring each\nregion according to the query, hence why we call it a multimodal reasoner. We\nevaluate this new model on challenging datasets and our experiments show that\nour model that jointly reasons over the object regions of the image and words\nof the query largely improves accuracy compared to current state-of-the-art\nmodels.",
          "link": "http://arxiv.org/abs/2003.08717",
          "publishedOn": "2021-05-27T01:32:27.473Z",
          "wordCount": 626,
          "title": "Giving Commands to a Self-driving Car: A Multimodal Reasoner for Visual Grounding. (arXiv:2003.08717v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiachong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>",
          "description": "Current dialogue summarization systems usually encode the text with a number\nof general semantic features (e.g., keywords and topics) to gain more powerful\ndialogue modeling capabilities. However, these features are obtained via\nopen-domain toolkits that are dialog-agnostic or heavily relied on human\nannotations. In this paper, we show how DialoGPT, a pre-trained model for\nconversational response generation, can be developed as an unsupervised\ndialogue annotator, which takes advantage of dialogue background knowledge\nencoded in DialoGPT. We apply DialoGPT to label three types of features on two\ndialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non\npre-trained models as our summarizes. Experimental results show that our\nproposed method can obtain remarkable improvements on both datasets and\nachieves new state-of-the-art performance on the SAMSum dataset.",
          "link": "http://arxiv.org/abs/2105.12544",
          "publishedOn": "2021-05-27T01:32:27.466Z",
          "wordCount": 560,
          "title": "Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization. (arXiv:2105.12544v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Johnny Tian-Zheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>",
          "description": "Estimating the expected output quality of generation systems is central to\nNLG. This paper qualifies the notion that automatic metrics are not as good as\nhumans in estimating system-level quality. Statistically, humans are unbiased,\nhigh variance estimators, while metrics are biased, low variance estimators. We\ncompare these estimators by their error in pairwise prediction (which\ngeneration system is better?) using the bootstrap. Measuring this error is\ncomplicated: predictions are evaluated against noisy, human predicted labels\ninstead of the ground truth, and metric predictions fluctuate based on the test\nsets they were calculated on. By applying a bias-variance-noise decomposition,\nwe adjust this error to a noise-free, infinite test set setting. Our analysis\ncompares the adjusted error of metrics to humans and a derived, perfect\nsegment-level annotator, both of which are unbiased estimators dependent on the\nnumber of judgments collected. In MT, we identify two settings where metrics\noutperform humans due to a statistical advantage in variance: when the number\nof human judgments used is small, and when the quality difference between\ncompared systems is small. The data and code to reproduce our analyses are\navailable at https://github.com/johntzwei/metric-statistical-advantage .",
          "link": "http://arxiv.org/abs/2105.12437",
          "publishedOn": "2021-05-27T01:32:27.430Z",
          "wordCount": 620,
          "title": "The statistical advantage of automatic NLG metrics at the system level. (arXiv:2105.12437v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Quan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "Knowledge distillation has been proven to be effective in model acceleration\nand compression. It allows a small network to learn to generalize in the same\nway as a large network. Recent successes in pre-training suggest the\neffectiveness of transferring model parameters. Inspired by this, we\ninvestigate methods of model acceleration and compression in another line of\nresearch. We propose Weight Distillation to transfer the knowledge in the large\nnetwork parameters through a parameter generator. Our experiments on WMT16\nEn-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks show that weight\ndistillation can train a small network that is 1.88~2.94x faster than the large\nnetwork but with competitive performance. With the same sized small network,\nweight distillation can outperform knowledge distillation by 0.51~1.82 BLEU\npoints.",
          "link": "http://arxiv.org/abs/2009.09152",
          "publishedOn": "2021-05-27T01:32:27.423Z",
          "wordCount": 591,
          "title": "Weight Distillation: Transferring the Knowledge in Neural Network Parameters. (arXiv:2009.09152v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hailong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>",
          "description": "Great progress has been made in unsupervised bilingual lexicon induction\n(UBLI) by aligning the source and target word embeddings independently trained\non monolingual corpora. The common assumption of most UBLI models is that the\nembedding spaces of two languages are approximately isomorphic. Therefore the\nperformance is bound by the degree of isomorphism, especially on etymologically\nand typologically distant languages. To address this problem, we propose a\ntransformation-based method to increase the isomorphism. Embeddings of two\nlanguages are made to match with each other by rotating and scaling. The method\ndoes not require any form of supervision and can be applied to any language\npair. On a benchmark data set of bilingual lexicon induction, our approach can\nachieve competitive or superior performance compared to state-of-the-art\nmethods, with particularly strong results being found on distant languages.",
          "link": "http://arxiv.org/abs/2105.12297",
          "publishedOn": "2021-05-27T01:32:27.416Z",
          "wordCount": 562,
          "title": "Word Embedding Transformation for Robust Unsupervised Bilingual Lexicon Induction. (arXiv:2105.12297v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Ming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "In this work, we propose Masked Noun-Phrase Prediction (MNPP), a pre-training\nstrategy to tackle pronoun resolution in a fully unsupervised setting. Firstly,\nWe evaluate our pre-trained model on various pronoun resolution datasets\nwithout any finetuning. Our method outperforms all previous unsupervised\nmethods on all datasets by large margins. Secondly, we proceed to a few-shot\nsetting where we finetune our pre-trained model on WinoGrande-S and XS. Our\nmethod outperforms RoBERTa-large baseline with large margins, meanwhile,\nachieving a higher AUC score after further finetuning on the remaining three\nofficial splits of WinoGrande.",
          "link": "http://arxiv.org/abs/2105.12392",
          "publishedOn": "2021-05-27T01:32:27.399Z",
          "wordCount": 512,
          "title": "Unsupervised Pronoun Resolution via Masked Noun-Phrase Prediction. (arXiv:2105.12392v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chu-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaech_A/0/1/0/all/0/1\">Aaron Jaech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>",
          "description": "Standard autoregressive language models perform only polynomial-time\ncomputation to compute the probability of the next symbol. While this is\nattractive, it means they cannot model distributions whose next-symbol\nprobability is hard to compute. Indeed, they cannot even model them well enough\nto solve associated easy decision problems for which an engineer might want to\nconsult a language model. These limitations apply no matter how much\ncomputation and data are used to train the model, unless the model is given\naccess to oracle parameters that grow superpolynomially in sequence length.\n\nThus, simply training larger autoregressive language models is not a panacea\nfor NLP. Alternatives include energy-based models (which give up efficient\nsampling) and latent-variable autoregressive models (which give up efficient\nscoring of a given string). Both are powerful enough to escape the above\nlimitations.",
          "link": "http://arxiv.org/abs/2010.11939",
          "publishedOn": "2021-05-27T01:32:27.390Z",
          "wordCount": 601,
          "title": "Limitations of Autoregressive Models and Their Alternatives. (arXiv:2010.11939v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pritzen_J/0/1/0/all/0/1\">Julia Pritzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gref_M/0/1/0/all/0/1\">Michael Gref</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1\">Christoph Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuhlke_D/0/1/0/all/0/1\">Dietlind Z&#xfc;hlke</a>",
          "description": "Loanwords, such as Anglicisms, are a challenge in German speech recognition.\nDue to their irregular pronunciation compared to native German words,\nautomatically generated pronunciation dictionaries often include faulty phoneme\nsequences for Anglicisms. In this work, we propose a multitask\nsequence-to-sequence approach for grapheme-to-phoneme conversion to improve the\nphonetization of Anglicisms. We extended a grapheme-to-phoneme model with a\nclassifier to distinguish Anglicisms from native German words. With this\napproach, the model learns to generate pronunciations differently depending on\nthe classification result. We used our model to create supplementary Anglicism\npronunciation dictionaries that are added to an existing German speech\nrecognition model. Tested on a dedicated Anglicism evaluation set, we improved\nthe recognition of Anglicisms compared to a baseline model, reducing the word\nerror rate by 1 % and the Anglicism error rate by 3 %. We show that multitask\nlearning can help solving the challenge of loanwords in German speech\nrecognition.",
          "link": "http://arxiv.org/abs/2105.12708",
          "publishedOn": "2021-05-27T01:32:27.383Z",
          "wordCount": 597,
          "title": "Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition. (arXiv:2105.12708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ravi_A/0/1/0/all/0/1\">Akhilesh Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1\">Amit Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1\">Jainish Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dholakia_J/0/1/0/all/0/1\">Jatin Dholakia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Naman Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "The increasing use of dialogue agents makes it extremely desirable for them\nto understand and acknowledge the implied emotions to respond like humans with\nempathy. Chatbots using traditional techniques analyze emotions based on the\ncontext and meaning of the text and lack the understanding of emotions\nexpressed through face. Emojis representing facial expressions present a\npromising way to express emotions. However, none of the AI systems utilizes\nemojis for empathetic conversation generation. We propose, SentEmojiBot, based\non the SentEmoji dataset, to generate empathetic conversations with a\ncombination of emojis and text. Evaluation metrics show that the BERT-based\nmodel outperforms the vanilla transformer model. A user study indicates that\nthe dialogues generated by our model were understandable and adding emojis\nimproved empathetic traits in conversations by 9.8%",
          "link": "http://arxiv.org/abs/2105.12399",
          "publishedOn": "2021-05-27T01:32:27.374Z",
          "wordCount": 551,
          "title": "SentEmojiBot: Empathising Conversations Generation with Emojis. (arXiv:2105.12399v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1\">Tatsuya Hiraoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchiumi_K/0/1/0/all/0/1\">Kei Uchiumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keyaki_A/0/1/0/all/0/1\">Atsushi Keyaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>",
          "description": "Since traditional tokenizers are isolated from a downstream task and model,\nthey cannot output an appropriate tokenization depending on the task and model,\nalthough recent studies imply that the appropriate tokenization improves the\nperformance. In this paper, we propose a novel method to find an appropriate\ntokenization to a given downstream model by jointly optimizing a tokenizer and\nthe model. The proposed method has no restriction except for using loss values\ncomputed by the downstream model to train the tokenizer, and thus, we can apply\nthe proposed method to any NLP task. Moreover, the proposed method can be used\nto explore the appropriate tokenization for an already trained model as\npost-processing. Therefore, the proposed method is applicable to various\nsituations. We evaluated whether our method contributes to improving\nperformance on text classification in three languages and machine translation\nin eight language pairs. Experimental results show that our proposed method\nimproves the performance by determining appropriate tokenizations.",
          "link": "http://arxiv.org/abs/2105.12410",
          "publishedOn": "2021-05-27T01:32:27.362Z",
          "wordCount": 588,
          "title": "Joint Optimization of Tokenization and Downstream Model. (arXiv:2105.12410v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Junfu Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Despite existing pioneering works on sign language translation (SLT), there\nis a non-trivial obstacle, i.e., the limited quantity of parallel sign-text\ndata. To tackle this parallel data bottleneck, we propose a sign\nback-translation (SignBT) approach, which incorporates massive spoken language\ntexts into SLT training. With a text-to-gloss translation model, we first\nback-translate the monolingual text to its gloss sequence. Then, the paired\nsign sequence is generated by splicing pieces from an estimated gloss-to-sign\nbank at the feature level. Finally, the synthetic parallel data serves as a\nstrong supplement for the end-to-end training of the encoder-decoder SLT\nframework.\n\nTo promote the SLT research, we further contribute CSL-Daily, a large-scale\ncontinuous SLT dataset. It provides both spoken language translations and\ngloss-level annotations. The topic revolves around people's daily lives (e.g.,\ntravel, shopping, medical care), the most likely SLT application scenario.\nExtensive experimental results and analysis of SLT methods are reported on\nCSL-Daily. With the proposed sign back-translation method, we obtain a\nsubstantial improvement over previous state-of-the-art SLT methods.",
          "link": "http://arxiv.org/abs/2105.12397",
          "publishedOn": "2021-05-27T01:32:27.352Z",
          "wordCount": 622,
          "title": "Improving Sign Language Translation with Monolingual Data by Sign Back-Translation. (arXiv:2105.12397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loureiro_D/0/1/0/all/0/1\">Daniel Loureiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_A/0/1/0/all/0/1\">Al&#xed;pio M&#xe1;rio Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>",
          "description": "Distributional semantics based on neural approaches is a cornerstone of\nNatural Language Processing, with surprising connections to human meaning\nrepresentation as well. Recent Transformer-based Language Models have proven\ncapable of producing contextual word representations that reliably convey\nsense-specific information, simply as a product of self-supervision. Prior work\nhas shown that these contextual representations can be used to accurately\nrepresent large sense inventories as sense embeddings, to the extent that a\ndistance-based solution to Word Sense Disambiguation (WSD) tasks outperforms\nmodels trained specifically for the task. Still, there remains much to\nunderstand on how to use these Neural Language Models (NLMs) to produce sense\nembeddings that can better harness each NLM's meaning representation abilities.\nIn this work we introduce a more principled approach to leverage information\nfrom all layers of NLMs, informed by a probing analysis on 14 NLM variants. We\nalso emphasize the versatility of these sense embeddings in contrast to\ntask-specific models, applying them on several sense-related tasks, besides\nWSD, while demonstrating improved performance using our proposed approach over\nprior work focused on sense embeddings. Finally, we discuss unexpected findings\nregarding layer and model performance variations, and potential applications\nfor downstream tasks.",
          "link": "http://arxiv.org/abs/2105.12449",
          "publishedOn": "2021-05-27T01:32:27.328Z",
          "wordCount": 629,
          "title": "LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and Beyond. (arXiv:2105.12449v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Recently, token-level adaptive training has achieved promising improvement in\nmachine translation, where the cross-entropy loss function is adjusted by\nassigning different training weights to different tokens, in order to alleviate\nthe token imbalance problem. However, previous approaches only use static word\nfrequency information in the target language without considering the source\nlanguage, which is insufficient for bilingual tasks like machine translation.\nIn this paper, we propose a novel bilingual mutual information (BMI) based\nadaptive objective, which measures the learning difficulty for each target\ntoken from the perspective of bilingualism, and assigns an adaptive weight\naccordingly to improve token-level adaptive training. This method assigns\nlarger training weights to tokens with higher BMI, so that easy tokens are\nupdated with coarse granularity while difficult tokens are updated with fine\ngranularity. Experimental results on WMT14 English-to-German and WMT19\nChinese-to-English demonstrate the superiority of our approach compared with\nthe Transformer baseline and previous token-level adaptive training approaches.\nFurther analyses confirm that our method can improve the lexical diversity.",
          "link": "http://arxiv.org/abs/2105.12523",
          "publishedOn": "2021-05-27T01:32:27.318Z",
          "wordCount": 607,
          "title": "Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation. (arXiv:2105.12523v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yongcai Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>",
          "description": "The capacity of empathy is crucial to the success of open-domain dialog\nsystems. Due to its nature of multi-dimensionality, there are various factors\nthat relate to empathy expression, such as communication mechanism, dialog act\nand emotion. However, existing methods for empathetic response generation\nusually either consider only one empathy factor or ignore the hierarchical\nrelationships between different factors, leading to a weak ability of empathy\nmodeling. In this paper, we propose a multi-factor hierarchical framework,\nCoMAE, for empathetic response generation, which models the above three key\nfactors of empathy expression in a hierarchical way. We show experimentally\nthat our CoMAE-based model can generate more empathetic responses than previous\nmethods. We also highlight the importance of hierarchical modeling of different\nfactors through both the empirical analysis on a real-life corpus and the\nextensive experiments. Our codes and used data are available at\nhttps://github.com/chujiezheng/CoMAE.",
          "link": "http://arxiv.org/abs/2105.08316",
          "publishedOn": "2021-05-26T01:22:11.921Z",
          "wordCount": 600,
          "title": "CoMAE: A Multi-factor Hierarchical Framework for Empathetic Response Generation. (arXiv:2105.08316v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1\">Wen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhihua Tao</a>",
          "description": "Few-shot learning has been proposed and rapidly emerging as a viable means\nfor completing various tasks. Many few-shot models have been widely used for\nrelation learning tasks. However, each of these models has a shortage of\ncapturing a certain aspect of semantic features, for example, CNN on long-range\ndependencies part, Transformer on local features. It is difficult for a single\nmodel to adapt to various relation learning, which results in the high variance\nproblem. Ensemble strategy could be competitive on improving the accuracy of\nfew-shot relation extraction and mitigating high variance risks. This paper\nexplores an ensemble approach to reduce the variance and introduces fine-tuning\nand feature attention strategies to calibrate relation-level features. Results\non several few-shot relation learning tasks show that our model significantly\noutperforms the previous state-of-the-art models.",
          "link": "http://arxiv.org/abs/2105.11904",
          "publishedOn": "2021-05-26T01:22:10.594Z",
          "wordCount": 559,
          "title": "Ensemble Making Few-Shot Learning Stronger. (arXiv:2105.11904v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hickmann_M/0/1/0/all/0/1\">M. Lautaro Hickmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wurzberger_F/0/1/0/all/0/1\">Fabian Wurzberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoxhalli_M/0/1/0/all/0/1\">Megi Hoxhalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lochner_A/0/1/0/all/0/1\">Arne Lochner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tollich_J/0/1/0/all/0/1\">Jessica T&#xf6;llich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>",
          "description": "Modern multi-document summarization (MDS) methods are based on transformer\narchitectures. They generate state of the art summaries, but lack\nexplainability. We focus on graph-based transformer models for MDS as they\ngained recent popularity. We aim to improve the explainability of the\ngraph-based MDS by analyzing their attention weights. In a graph-based MDS such\nas GraphSum, vertices represent the textual units, while the edges form some\nsimilarity graph over the units. We compare GraphSum's performance utilizing\ndifferent textual units, i. e., sentences versus paragraphs, on two news\nbenchmark datasets, namely WikiSum and MultiNews. Our experiments show that\nparagraph-level representations provide the best summarization performance.\nThus, we subsequently focus oAnalysisn analyzing the paragraph-level attention\nweights of GraphSum's multi-heads and decoding layers in order to improve the\nexplainability of a transformer-based MDS model. As a reference metric, we\ncalculate the ROUGE scores between the input paragraphs and each sentence in\nthe generated summary, which indicate source origin information via text\nsimilarity. We observe a high correlation between the attention weights and\nthis reference metric, especially on the the later decoding layers of the\ntransformer architecture. Finally, we investigate if the generated summaries\nfollow a pattern of positional bias by extracting which paragraph provided the\nmost information for each generated summary. Our results show that there is a\nhigh correlation between the position in the summary and the source origin.",
          "link": "http://arxiv.org/abs/2105.11908",
          "publishedOn": "2021-05-26T01:22:10.481Z",
          "wordCount": 669,
          "title": "Analysis of GraphSum's Attention Weights to Improve the Explainability of Multi-Document Summarization. (arXiv:2105.11908v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1\">T. Spinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnitckaia_L/0/1/0/all/0/1\">L. Rudnitckaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">K. Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">F. Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">B. Gipp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">K. Donnay</a>",
          "description": "Many people consider news articles to be a reliable source of information on\ncurrent events. However, due to the range of factors influencing news agencies,\nsuch coverage may not always be impartial. Media bias, or slanted news\ncoverage, can have a substantial impact on public perception of events, and,\naccordingly, can potentially alter the beliefs and views of the public. The\nmain data gap in current research on media bias detection is a robust,\nrepresentative, and diverse dataset containing annotations of biased words and\nsentences. In particular, existing datasets do not control for the individual\nbackground of annotators, which may affect their assessment and, thus,\nrepresents critical information for contextualizing their annotations. In this\nposter, we present a matrix-based methodology to crowdsource such data using a\nself-developed annotation platform. We also present MBIC (Media Bias Including\nCharacteristics) - the first sample of 1,700 statements representing various\nmedia bias instances. The statements were reviewed by ten annotators each and\ncontain labels for media bias identification both on the word and sentence\nlevel. MBIC is the first available dataset about media bias reporting detailed\ninformation on annotator characteristics and their individual background. The\ncurrent dataset already significantly extends existing data in this domain\nproviding unique and more reliable insights into the perception of bias. In\nfuture, we will further extend it both with respect to the number of articles\nand annotators per article.",
          "link": "http://arxiv.org/abs/2105.11910",
          "publishedOn": "2021-05-26T01:22:10.188Z",
          "wordCount": 664,
          "title": "MBIC -- A Media Bias Annotation Dataset Including Annotator Characteristics. (arXiv:2105.11910v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>",
          "description": "Knowledge retrieval and reasoning are two key stages in multi-hop question\nanswering (QA) at web scale. Existing approaches suffer from low confidence\nwhen retrieving evidence facts to fill the knowledge gap and lack transparent\nreasoning process. In this paper, we propose a new framework to exploit more\nvalid facts while obtaining explainability for multi-hop QA by dynamically\nconstructing a semantic graph and reasoning over it. We employ Abstract Meaning\nRepresentation (AMR) as semantic graph representation. Our framework contains\nthree new ideas: (a) {\\tt AMR-SG}, an AMR-based Semantic Graph, constructed by\ncandidate fact AMRs to uncover any hop relations among question, answer and\nmultiple facts. (b) A novel path-based fact analytics approach exploiting {\\tt\nAMR-SG} to extract active facts from a large fact pool to answer questions. (c)\nA fact-level relation modeling leveraging graph convolution network (GCN) to\nguide the reasoning process. Results on two scientific multi-hop QA datasets\nshow that we can surpass recent approaches including those using additional\nknowledge graphs while maintaining high explainability on OpenBookQA and\nachieve a new state-of-the-art result on ARC-Challenge in a computationally\npracticable setting.",
          "link": "http://arxiv.org/abs/2105.11776",
          "publishedOn": "2021-05-26T01:22:10.158Z",
          "wordCount": 620,
          "title": "Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering. (arXiv:2105.11776v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barchiesi_M/0/1/0/all/0/1\">M. A. Barchiesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>",
          "description": "In order to face the complexity of business environments and detect\npriorities while triggering contingency strategies, we propose a new\nmethodological approach that combines text mining, social network and big data\nanalytics, with the assessment of stakeholders' attitudes towards company core\nvalues. This approach was applied in a case study where we considered the\nTwitter discourse about core values in Italy. We collected more than 94,000\ntweets related to the core values of the firms listed in Fortune's ranking of\nthe World's Most Admired Companies (2013-2017). For the Italian scenario, we\nfound three predominant core values orientations (Customers, Employees and\nExcellence) - which should be at the basis of any business strategy - and three\nlatent ones (Economic-Financial Growth, Citizenship and Social Responsibility),\nwhich need periodic attention. Our contribution is mostly methodological and\nextends the research on text mining and on online big data analytics applied in\ncomplex business contexts.",
          "link": "http://arxiv.org/abs/2105.12048",
          "publishedOn": "2021-05-26T01:22:10.023Z",
          "wordCount": 609,
          "title": "Big data and big values: When companies need to rethink themselves. (arXiv:2105.12048v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carrera_Casado_D/0/1/0/all/0/1\">David Carrera-Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>",
          "description": "It is well-known that, when sufficiently young children encounter a new word,\nthey tend to attach it to a meaning that does not have a word yet in their\nlexicon. In previous research, the strategy was shown to be optimal from an\ninformation theoretic standpoint. However, the information theoretic model\nemployed neither explains the weakening of that vocabulary learning bias in\nolder children or polylinguals nor reproduces Zipf's meaning-frequency law,\nnamely the non-linear relationship between the number of meanings of a word and\nits frequency. Here we consider a generalization of the model that is channeled\nto reproduce that law. The analysis of the new model reveals regions of the\nphase space where the bias disappears consistently with the weakening or loss\nof the bias in older children or polylinguals. In the deep learning era, the\nmodel is a transparent low-dimensional tool for future experimental research\nand illustrates the predictive power of a theoretical framework originally\ndesigned to shed light on the origins of Zipf's rank-frequency law.",
          "link": "http://arxiv.org/abs/2105.11519",
          "publishedOn": "2021-05-26T01:22:09.776Z",
          "wordCount": 609,
          "title": "The advent and fall of a vocabulary learning bias from communicative efficiency. (arXiv:2105.11519v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minshuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>",
          "description": "The Lottery Ticket Hypothesis suggests that an over-parametrized network\nconsists of \"lottery tickets\", and training a certain collection of them (i.e.,\na subnetwork) can match the performance of the full model. In this paper, we\nstudy such a collection of tickets, which is referred to as \"winning tickets\",\nin extremely over-parametrized models, e.g., pre-trained language models. We\nobserve that at certain compression ratios, generalization performance of the\nwinning tickets can not only match, but also exceed that of the full model. In\nparticular, we observe a phase transition phenomenon: As the compression ratio\nincreases, generalization performance of the winning tickets first improves\nthen deteriorates after a certain threshold. We refer to the tickets on the\nthreshold as \"super tickets\". We further show that the phase transition is task\nand model dependent -- as model size becomes larger and training data set\nbecomes smaller, the transition becomes more pronounced. Our experiments on the\nGLUE benchmark show that the super tickets improve single task fine-tuning by\n$0.9$ points on BERT-base and $1.0$ points on BERT-large, in terms of\ntask-average score. We also demonstrate that adaptively sharing the super\ntickets across tasks benefits multi-task learning.",
          "link": "http://arxiv.org/abs/2105.12002",
          "publishedOn": "2021-05-26T01:22:09.750Z",
          "wordCount": 650,
          "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization. (arXiv:2105.12002v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1\">Tedo Vrbanec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1\">Ana Mestrovic</a>",
          "description": "The article gives an overview of the plagiarism domain, with focus on\nacademic plagiarism. The article defines plagiarism, explains the origin of the\nterm, as well as plagiarism related terms. It identifies the extent of the\nplagiarism domain and then focuses on the plagiarism subdomain of text\ndocuments, for which it gives an overview of current classifications and\ntaxonomies and then proposes a more comprehensive classification according to\nseveral criteria: their origin and purpose, technical implementation,\nconsequence, complexity of detection and according to the number of linguistic\nsources. The article suggests the new classification of academic plagiarism,\ndescribes sorts and methods of plagiarism, types and categories, approaches and\nphases of plagiarism detection, the classification of methods and algorithms\nfor plagiarism detection. The title of the article explicitly targets the\nacademic community, but it is sufficiently general and interdisciplinary, so it\ncan be useful for many other professionals like software developers, linguists\nand librarians.",
          "link": "http://arxiv.org/abs/2105.12068",
          "publishedOn": "2021-05-26T01:22:09.723Z",
          "wordCount": 612,
          "title": "Taxonomy of academic plagiarism methods. (arXiv:2105.12068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Hongke Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_x/0/1/0/all/0/1\">xiaoqiang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yalong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>",
          "description": "Existing emotion-aware conversational models usually focus on controlling the\nresponse contents to align with a specific emotion class, whereas empathy is\nthe ability to understand and concern the feelings and experience of others.\nHence, it is critical to learn the causes that evoke the users' emotion for\nempathetic responding, a.k.a. emotion causes. To gather emotion causes in\nonline environments, we leverage counseling strategies and develop an\nempathetic chatbot to utilize the causal emotion information. On a real-world\nonline dataset, we verify the effectiveness of the proposed approach by\ncomparing our chatbot with several SOTA methods using automatic metrics,\nexpert-based human judgements as well as user-based online evaluation.",
          "link": "http://arxiv.org/abs/2105.11903",
          "publishedOn": "2021-05-26T01:22:09.707Z",
          "wordCount": 549,
          "title": "Towards an Online Empathetic Chatbot with Emotion Causes. (arXiv:2105.11903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongguang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Sentiment analysis (SA) is an important research area in cognitive\ncomputation-thus in-depth studies of patterns of sentiment analysis are\nnecessary. At present, rich resource data-based SA has been well developed,\nwhile the more challenging and practical multi-source unsupervised SA (i.e. a\ntarget domain SA by transferring from multiple source domains) is seldom\nstudied. The challenges behind this problem mainly locate in the lack of\nsupervision information, the semantic gaps among domains (i.e., domain shifts),\nand the loss of knowledge. However, existing methods either lack the\ndistinguishable capacity of the semantic gaps among domains or lose private\nknowledge. To alleviate these problems, we propose a two-stage domain\nadaptation framework. In the first stage, a multi-task methodology-based\nshared-private architecture is employed to explicitly model the domain common\nfeatures and the domain-specific features for the labeled source domains. In\nthe second stage, two elaborate mechanisms are embedded in the shared private\narchitecture to transfer knowledge from multiple source domains. The first\nmechanism is a selective domain adaptation (SDA) method, which transfers\nknowledge from the closest source domain. And the second mechanism is a\ntarget-oriented ensemble (TOE) method, in which knowledge is transferred\nthrough a well-designed ensemble method. Extensive experiment evaluations\nverify that the performance of the proposed framework outperforms unsupervised\nstate-of-the-art competitors. What can be concluded from the experiments is\nthat transferring from very different distributed source domains may degrade\nthe target-domain performance, and it is crucial to choose the proper source\ndomains to transfer from.",
          "link": "http://arxiv.org/abs/2105.11902",
          "publishedOn": "2021-05-26T01:22:09.694Z",
          "wordCount": 674,
          "title": "Unsupervised Sentiment Analysis by Transferring Multi-source Knowledge. (arXiv:2105.11902v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yueyuan Li</a>",
          "description": "For text-level discourse analysis, there are various discourse schemes but\nrelatively few labeled data, because discourse research is still immature and\nit is labor-intensive to annotate the inner logic of a text. In this paper, we\nattempt to unify multiple Chinese discourse corpora under different annotation\nschemes with discourse dependency framework by designing semi-automatic methods\nto convert them into dependency structures. We also implement several benchmark\ndependency parsers and research on how they can leverage the unified data to\nimprove performance.",
          "link": "http://arxiv.org/abs/2101.00167",
          "publishedOn": "2021-05-26T01:22:09.688Z",
          "wordCount": 538,
          "title": "Unifying Discourse Resources with Dependency Framework. (arXiv:2101.00167v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wrzalik_M/0/1/0/all/0/1\">Marco Wrzalik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krechel_D/0/1/0/all/0/1\">Dirk Krechel</a>",
          "description": "Many recent approaches towards neural information retrieval mitigate their\ncomputational costs by using a multi-stage ranking pipeline. In the first\nstage, a number of potentially relevant candidates are retrieved using an\nefficient retrieval model such as BM25. Although BM25 has proven decent\nperformance as a first-stage ranker, it tends to miss relevant passages. In\nthis context we propose CoRT, a simple neural first-stage ranking model that\nleverages contextual representations from pretrained language models such as\nBERT to complement term-based ranking functions while causing no significant\ndelay at query time. Using the MS MARCO dataset, we show that CoRT\nsignificantly increases the candidate recall by complementing BM25 with missing\ncandidates. Consequently, we find subsequent re-rankers achieve superior\nresults with less candidates. We further demonstrate that passage retrieval\nusing CoRT can be realized with surprisingly low latencies.",
          "link": "http://arxiv.org/abs/2010.10252",
          "publishedOn": "2021-05-26T01:22:09.682Z",
          "wordCount": 619,
          "title": "CoRT: Complementary Rankings from Transformers. (arXiv:2010.10252v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Searle_T/0/1/0/all/0/1\">Thomas Searle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1\">Zina Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1\">James Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1\">Richard JB Dobson</a>",
          "description": "The current mode of use of Electronic Health Record (EHR) elicits text\nredundancy. Clinicians often populate new documents by duplicating existing\nnotes, then updating accordingly. Data duplication can lead to a propagation of\nerrors, inconsistencies and misreporting of care. Therefore, quantifying\ninformation redundancy can play an essential role in evaluating innovations\nthat operate on clinical narratives.\n\nThis work is a quantitative examination of information redundancy in EHR\nnotes. We present and evaluate two strategies to measure redundancy: an\ninformation-theoretic approach and a lexicosyntactic and semantic model. We\nevaluate the measures by training large Transformer-based language models using\nclinical text from a large openly available US-based ICU dataset and a large\nmulti-site UK based Trust. By comparing the information-theoretic content of\nthe trained models with open-domain language models, the language models\ntrained using clinical text have shown ~1.5x to ~3x less efficient than\nopen-domain corpora. Manual evaluation shows a high correlation with\nlexicosyntactic and semantic redundancy, with averages ~43 to ~65%.",
          "link": "http://arxiv.org/abs/2105.11832",
          "publishedOn": "2021-05-26T01:22:09.659Z",
          "wordCount": 580,
          "title": "Estimating Redundancy in Clinical Text. (arXiv:2105.11832v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ide_T/0/1/0/all/0/1\">Tatsuya Ide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_D/0/1/0/all/0/1\">Daisuke Kawahara</a>",
          "description": "For a computer to naturally interact with a human, it needs to be human-like.\nIn this paper, we propose a neural response generation model with multi-task\nlearning of generation and classification, focusing on emotion. Our model based\non BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model,\nis trained to generate responses and recognize emotions simultaneously.\nFurthermore, we weight the losses for the tasks to control the update of\nparameters. Automatic evaluations and crowdsourced manual evaluations show that\nthe proposed model makes generated responses more emotionally aware.",
          "link": "http://arxiv.org/abs/2105.11696",
          "publishedOn": "2021-05-26T01:22:09.654Z",
          "wordCount": 522,
          "title": "Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation. (arXiv:2105.11696v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>",
          "description": "Existing pre-trained language models (PLMs) are often computationally\nexpensive in inference, making them impractical in various resource-limited\nreal-world applications. To address this issue, we propose a dynamic token\nreduction approach to accelerate PLMs' inference, named TR-BERT, which could\nflexibly adapt the layer number of each token in inference to avoid redundant\ncalculation. Specially, TR-BERT formulates the token reduction process as a\nmulti-step token selection problem and automatically learns the selection\nstrategy via reinforcement learning. The experimental results on several\ndownstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to\nsatisfy various performance demands. Moreover, TR-BERT can also achieve better\nperformance with less computation in a suite of long-text tasks since its\ntoken-level layer number adaption greatly accelerates the self-attention\noperation in PLMs. The source code and experiment details of this paper can be\nobtained from https://github.com/thunlp/TR-BERT.",
          "link": "http://arxiv.org/abs/2105.11618",
          "publishedOn": "2021-05-26T01:22:09.648Z",
          "wordCount": 574,
          "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference. (arXiv:2105.11618v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basto_C/0/1/0/all/0/1\">Carlos Basto</a>",
          "description": "A data-centric approach with Natural Language Processing (NLP) to predict\npersonality types based on the MBTI (an introspective self-assessment\nquestionnaire that indicates different psychological preferences about how\npeople perceive the world and make decisions) through systematic enrichment of\ntext representation, based on the domain of the area, under the generation of\nfeatures based on three types of analysis: sentimental, grammatical and\naspects. The experimentation had a robust baseline of stacked models, with\npremature optimization of hyperparameters through grid search, with gradual\nfeedback, for each of the four classifiers (dichotomies) of MBTI. The results\nshowed that attention to the data iteration loop focused on quality,\nexplanatory power and representativeness for the abstraction of more\nrelevant/important resources for the studied phenomenon made it possible to\nimprove the evaluation metrics results more quickly and less costly than\ncomplex models such as the LSTM or state of the art ones as BERT, as well as\nthe importance of these results by comparisons made from various perspectives.\nIn addition, the study demonstrated a broad spectrum for the evolution and\ndeepening of the task and possible approaches for a greater extension of the\nabstraction of personality types.",
          "link": "http://arxiv.org/abs/2105.11798",
          "publishedOn": "2021-05-26T01:22:09.639Z",
          "wordCount": 642,
          "title": "Extending the Abstraction of Personality Types based on MBTI with Machine Learning and Natural Language Processing. (arXiv:2105.11798v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weihong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qifang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhuoyao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1\">Qin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1\">Qiang Huo</a>",
          "description": "Recent grid-based document representations like BERTgrid allow the\nsimultaneous encoding of the textual and layout information of a document in a\n2D feature map so that state-of-the-art image segmentation and/or object\ndetection models can be straightforwardly leveraged to extract key information\nfrom documents. However, such methods have not achieved comparable performance\nto state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK\nyet. In this paper, we propose a new multi-modal backbone network by\nconcatenating a BERTgrid to an intermediate layer of a CNN model, where the\ninput of CNN is a document image and the BERTgrid is a grid of word embeddings,\nto generate a more powerful grid-based document representation, named\nViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal\nbackbone network are trained jointly. Our experimental results demonstrate that\nthis joint training strategy improves significantly the representation ability\nof ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction\napproach has achieved state-of-the-art performance on real-world datasets.",
          "link": "http://arxiv.org/abs/2105.11672",
          "publishedOn": "2021-05-26T01:22:09.605Z",
          "wordCount": 620,
          "title": "ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for Key Information Extraction from Documents. (arXiv:2105.11672v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1\">Rahul Aralikatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothe_S/0/1/0/all/0/1\">Sascha Rothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Ryan McDonald</a>",
          "description": "Professional summaries are written with document-level information, such as\nthe theme of the document, in mind. This is in contrast with most seq2seq\ndecoders which simultaneously learn to focus on salient content, while deciding\nwhat to generate, at each decoding step. With the motivation to narrow this\ngap, we introduce Focus Attention Mechanism, a simple yet effective method to\nencourage decoders to proactively generate tokens that are similar or topical\nto the input document. Further, we propose a Focus Sampling method to enable\ngeneration of diverse summaries, an area currently understudied in\nsummarization. When evaluated on the BBC extreme summarization task, two\nstate-of-the-art models augmented with Focus Attention generate summaries that\nare closer to the target and more faithful to their input documents,\noutperforming their vanilla counterparts on \\rouge and multiple faithfulness\nmeasures. We also empirically demonstrate that Focus Sampling is more effective\nin generating diverse and faithful summaries than top-$k$ or nucleus\nsampling-based decoding methods.",
          "link": "http://arxiv.org/abs/2105.11921",
          "publishedOn": "2021-05-26T01:22:09.584Z",
          "wordCount": 587,
          "title": "Focus Attention: Promoting Faithfulness and Diversity in Summarization. (arXiv:2105.11921v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alshomary_M/0/1/0/all/0/1\">Milad Alshomary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>",
          "description": "Text generation has received a lot of attention in computational\nargumentation research as of recent. A particularly challenging task is the\ngeneration of counter-arguments. So far, approaches primarily focus on\nrebutting a given conclusion, yet other ways to counter an argument exist. In\nthis work, we go beyond previous research by exploring argument undermining,\nthat is, countering an argument by attacking one of its premises. We\nhypothesize that identifying the argument's weak premises is key to effective\ncountering. Accordingly, we propose a pipeline approach that first assesses the\npremises' strength and then generates a counter-argument targeting the weak\nones. On the one hand, both manual and automatic evaluation proves the\nimportance of identifying weak premises in counter-argument generation. On the\nother hand, when considering correctness and content richness, human annotators\nfavored our approach over state-of-the-art counter-argument generation.",
          "link": "http://arxiv.org/abs/2105.11752",
          "publishedOn": "2021-05-26T01:22:09.565Z",
          "wordCount": 567,
          "title": "Argument Undermining: Counter-Argument Generation by Attacking Weak Premises. (arXiv:2105.11752v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan T&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-T&#xfc;r</a>",
          "description": "Interactive robots navigating photo-realistic environments face challenges\nunderlying vision-and-language navigation (VLN), but in addition, they need to\nbe trained to handle the dynamic nature of dialogue. However, research in\nCooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts\nwith a guide in natural language in order to reach a goal, treats the dialogue\nhistory as a VLN-style static instruction. In this paper, we present VISITRON,\na navigator better suited to the interactive regime inherent to CVDN by being\ntrained to: i) identify and associate object-level concepts and semantics\nbetween the environment and dialogue history, ii) identify when to interact vs.\nnavigate via imitation learning of a binary classification head. We perform\nextensive ablations with VISITRON to gain empirical insights and improve\nperformance on CVDN. VISITRON is competitive with models on the static CVDN\nleaderboard. We also propose a generalized interactive regime to fine-tune and\nevaluate VISITRON and future such models with pre-trained guides for\nadaptability.",
          "link": "http://arxiv.org/abs/2105.11589",
          "publishedOn": "2021-05-26T01:22:09.560Z",
          "wordCount": 617,
          "title": "VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator. (arXiv:2105.11589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Gaole He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Knowledge base question answering (KBQA) aims to answer a question over a\nknowledge base (KB). Recently, a large number of studies focus on semantically\nor syntactically complicated questions. In this paper, we elaborately summarize\nthe typical challenges and solutions for complex KBQA. We begin with\nintroducing the background about the KBQA task. Next, we present the two\nmainstream categories of methods for complex KBQA, namely semantic\nparsing-based (SP-based) methods and information retrieval-based (IR-based)\nmethods. We then review the advanced methods comprehensively from the\nperspective of the two categories. Specifically, we explicate their solutions\nto the typical challenges. Finally, we conclude and discuss some promising\ndirections for future research.",
          "link": "http://arxiv.org/abs/2105.11644",
          "publishedOn": "2021-05-26T01:22:09.553Z",
          "wordCount": 554,
          "title": "A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions. (arXiv:2105.11644v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulmizev_A/0/1/0/all/0/1\">Artur Kulmizev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>",
          "description": "In this work, we demonstrate that the contextualized word vectors derived\nfrom pretrained masked language model-based encoders share a common, perhaps\nundesirable pattern across layers. Namely, we find cases of persistent outlier\nneurons within BERT and RoBERTa's hidden state vectors that consistently bear\nthe smallest or largest values in said vectors. In an attempt to investigate\nthe source of this information, we introduce a neuron-level analysis method,\nwhich reveals that the outliers are closely related to information captured by\npositional embeddings. We also pre-train the RoBERTa-base models from scratch\nand find that the outliers disappear without using positional embeddings. These\noutliers, we find, are the major cause of anisotropy of encoders' raw vector\nspaces, and clipping them leads to increased similarity across vectors. We\ndemonstrate this in practice by showing that clipped vectors can more\naccurately distinguish word senses, as well as lead to better sentence\nembeddings when mean pooling. In three supervised tasks, we find that clipping\ndoes not affect the performance.",
          "link": "http://arxiv.org/abs/2011.04393",
          "publishedOn": "2021-05-26T01:22:09.538Z",
          "wordCount": 628,
          "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings. (arXiv:2011.04393v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbanek_J/0/1/0/all/0/1\">Jack Urbanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Margaret Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "We seek to create agents that both act and communicate with other agents in\npursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019) -- a\nlarge-scale crowd-sourced fantasy text-game -- with a dataset of quests. These\ncontain natural language motivations paired with in-game goals and human\ndemonstrations; completing a quest might require dialogue or actions (or both).\nWe introduce a reinforcement learning system that (1) incorporates large-scale\nlanguage modeling-based and commonsense reasoning-based pre-training to imbue\nthe agent with relevant priors; and (2) leverages a factorized action space of\naction commands and dialogue, balancing between the two. We conduct zero-shot\nevaluations using held-out human expert demonstrations, showing that our agents\nare able to act consistently and talk naturally with respect to their\nmotivations.",
          "link": "http://arxiv.org/abs/2010.00685",
          "publishedOn": "2021-05-26T01:22:09.520Z",
          "wordCount": 619,
          "title": "How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds. (arXiv:2010.00685v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sumers_T/0/1/0/all/0/1\">Theodore R. Sumers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1\">Mark K. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>",
          "description": "Speakers communicate to influence their partner's beliefs and shape their\nactions. Belief- and action-based objectives have been explored independently\nin recent computational models, but it has been challenging to explicitly\ncompare or integrate them. Indeed, we find that they are conflated in standard\nreferential communication tasks. To distinguish these accounts, we introduce a\nnew paradigm called signaling bandits, generalizing classic Lewis signaling\ngames to a multi-armed bandit setting where all targets in the context have\nsome relative value. We develop three speaker models: a belief-oriented speaker\nwith a purely informative objective; an action-oriented speaker with an\ninstrumental objective; and a combined speaker which integrates the two by\ninducing listener beliefs that generally lead to desirable actions. We then\npresent a series of simulations demonstrating that grounding production choices\nin future listener actions results in relevance effects and flexible uses of\nnonliteral language. More broadly, our findings suggest that language games\nbased on richer decision problems are a promising avenue for insight into\nrational communication.",
          "link": "http://arxiv.org/abs/2105.11950",
          "publishedOn": "2021-05-26T01:22:09.513Z",
          "wordCount": 611,
          "title": "Extending rational models of communication from beliefs to actions. (arXiv:2105.11950v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Han Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>",
          "description": "Cross-lingual speech adaptation aims to solve the problem of leveraging\nmultiple rich-resource languages to build models for a low-resource target\nlanguage. Since the low-resource language has limited training data, speech\nrecognition models can easily overfit. In this paper, we propose to use\nadapters to investigate the performance of multiple adapters for\nparameter-efficient cross-lingual speech adaptation. Based on our previous\nMetaAdapter that implicitly leverages adapters, we propose a novel algorithms\ncalled SimAdapter for explicitly learning knowledge from adapters. Our\nalgorithm leverages adapters which can be easily integrated into the\nTransformer structure.MetaAdapter leverages meta-learning to transfer the\ngeneral knowledge from training data to the test language. SimAdapter aims to\nlearn the similarities between the source and target languages during\nfine-tuning using the adapters. We conduct extensive experiments on\nfive-low-resource languages in Common Voice dataset. Results demonstrate that\nour MetaAdapter and SimAdapter methods can reduce WER by 2.98% and 2.55% with\nonly 2.5% and 15.5% of trainable parameters compared to the strong full-model\nfine-tuning baseline. Moreover, we also show that these two novel algorithms\ncan be integrated for better performance with up to 3.55% relative WER\nreduction.",
          "link": "http://arxiv.org/abs/2105.11905",
          "publishedOn": "2021-05-26T01:22:09.498Z",
          "wordCount": 626,
          "title": "Exploiting Adapters for Cross-lingual Low-resource Speech Recognition. (arXiv:2105.11905v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gothard_K/0/1/0/all/0/1\">Kelly Gothard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewhurst_D/0/1/0/all/0/1\">David Rushing Dewhurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minot_J/0/1/0/all/0/1\">Joshua R. Minot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1\">Jane Lydia Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>",
          "description": "Evolving out of a gender-neutral framing of an involuntary celibate identity,\nthe concept of `incels' has come to refer to an online community of men who\nbear antipathy towards themselves, women, and society-at-large for their\nperceived inability to find and maintain sexual relationships. By exploring\nincel language use on Reddit, a global online message board, we contextualize\nthe incel community's online expressions of misogyny and real-world acts of\nviolence perpetrated against women. After assembling around three million\ncomments from incel-themed Reddit channels, we analyze the temporal dynamics of\na data driven rank ordering of the glossary of phrases belonging to an emergent\nincel lexicon. Our study reveals the generation and normalization of an\nextensive coded misogynist vocabulary in service of the group's identity.",
          "link": "http://arxiv.org/abs/2105.12006",
          "publishedOn": "2021-05-26T01:22:09.490Z",
          "wordCount": 578,
          "title": "The incel lexicon: Deciphering the emergent cryptolect of a global misogynistic community. (arXiv:2105.12006v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "This paper explores the task of Difficulty-Controllable Question Generation\n(DCQG), which aims at generating questions with required difficulty levels.\nPrevious research on this task mainly defines the difficulty of a question as\nwhether it can be correctly answered by a Question Answering (QA) system,\nlacking interpretability and controllability. In our work, we redefine question\ndifficulty as the number of inference steps required to answer it and argue\nthat Question Generation (QG) systems should have stronger control over the\nlogic of generated questions. To this end, we propose a novel framework that\nprogressively increases question difficulty through step-by-step rewriting\nunder the guidance of an extracted reasoning chain. A dataset is automatically\nconstructed to facilitate the research, on which extensive experiments are\nconducted to test the performance of our method.",
          "link": "http://arxiv.org/abs/2105.11698",
          "publishedOn": "2021-05-26T01:22:09.463Z",
          "wordCount": 573,
          "title": "Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting. (arXiv:2105.11698v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsarapatsanis_D/0/1/0/all/0/1\">Dimitrios Tsarapatsanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>",
          "description": "Natural language processing (NLP) methods for analyzing legal text offer\nlegal scholars and practitioners a range of tools allowing to empirically\nanalyze law on a large scale. However, researchers seem to struggle when it\ncomes to identifying ethical limits to using NLP systems for acquiring genuine\ninsights both about the law and the systems' predictive capacity. In this paper\nwe set out a number of ways in which to think systematically about such issues.\nWe place emphasis on three crucial normative parameters which have, to the best\nof our knowledge, been underestimated by current debates: (a) the importance of\nacademic freedom, (b) the existence of a wide diversity of legal and ethical\nnorms domestically but even more so internationally and (c) the threat of\nmoralism in research related to computational law. For each of these three\nparameters we provide specific recommendations for the legal NLP community. Our\ndiscussion is structured around the study of a real-life scenario that has\nprompted recent debate in the legal NLP research community.",
          "link": "http://arxiv.org/abs/2105.02751",
          "publishedOn": "2021-05-26T01:22:09.456Z",
          "wordCount": 631,
          "title": "On the Ethical Limits of Natural Language Processing on Legal Text. (arXiv:2105.02751v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eric_M/0/1/0/all/0/1\">Mihail Eric</a>",
          "description": "A key challenge of dialog systems research is to effectively and efficiently\nadapt to new domains. A scalable paradigm for adaptation necessitates the\ndevelopment of generalizable models that perform well in few-shot settings. In\nthis paper, we focus on the intent classification problem which aims to\nidentify user intents given utterances addressed to the dialog system. We\npropose two approaches for improving the generalizability of utterance\nclassification models: (1) observers and (2) example-driven training. Prior\nwork has shown that BERT-like models tend to attribute a significant amount of\nattention to the [CLS] token, which we hypothesize results in diluted\nrepresentations. Observers are tokens that are not attended to, and are an\nalternative to the [CLS] token as a semantic representation of utterances.\nExample-driven training learns to classify utterances by comparing to examples,\nthereby using the underlying encoder as a sentence similarity model. These\nmethods are complementary; improving the representation through observers\nallows the example-driven model to better measure sentence similarities. When\ncombined, the proposed methods attain state-of-the-art results on three intent\nprediction datasets (\\textsc{banking77}, \\textsc{clinc150}, \\textsc{hwu64}) in\nboth the full data and few-shot (10 examples per intent) settings. Furthermore,\nwe demonstrate that the proposed approach can transfer to new intents and\nacross datasets without any additional training.",
          "link": "http://arxiv.org/abs/2010.08684",
          "publishedOn": "2021-05-26T01:22:09.432Z",
          "wordCount": 657,
          "title": "Example-Driven Intent Prediction with Observers. (arXiv:2010.08684v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Namysl_M/0/1/0/all/0/1\">Marcin Namysl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Joachim K&#xf6;hler</a>",
          "description": "Despite recent advances, standard sequence labeling systems often fail when\nprocessing noisy user-generated text or consuming the output of an Optical\nCharacter Recognition (OCR) process. In this paper, we improve the noise-aware\ntraining method by proposing an empirical error generation approach that\nemploys a sequence-to-sequence model trained to perform translation from\nerror-free to erroneous text. Using an OCR engine, we generated a large\nparallel text corpus for training and produced several real-world noisy\nsequence labeling benchmarks for evaluation. Moreover, to overcome the data\nsparsity problem that exacerbates in the case of imperfect textual input, we\nlearned noisy language model-based embeddings. Our approach outperformed the\nbaseline noise generation and error correction techniques on the erroneous\nsequence labeling data sets. To facilitate future research on robustness, we\nmake our code, embeddings, and data conversion scripts publicly available.",
          "link": "http://arxiv.org/abs/2105.11872",
          "publishedOn": "2021-05-26T01:22:09.409Z",
          "wordCount": 574,
          "title": "Empirical Error Modeling Improves Robustness of Noisy Neural Sequence Labeling. (arXiv:2105.11872v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scettri_G/0/1/0/all/0/1\">G. Scettri</a>",
          "description": "This study looks into employees' communication, offering novel metrics which\ncan help to predict a company's stock price. We studied the intranet forum of a\nlarge Italian company, exploring the interactions and the use of language of\nabout 8,000 employees. We built a network linking words included in the general\ndiscourse. In this network, we focused on the position of the node representing\nthe company brand. We found that a lower sentiment, a higher betweenness\ncentrality of the company brand, a denser word co-occurrence network and more\nequally distributed centrality scores of employees (lower group betweenness\ncentrality) are all significant predictors of higher stock prices. Our findings\noffers new metrics that can be helpful for scholars, company managers and\nprofessional investors and could be integrated into existing forecasting models\nto improve their accuracy. Lastly, we contribute to the research on word\nco-occurrence networks by extending their field of application.",
          "link": "http://arxiv.org/abs/2105.11780",
          "publishedOn": "2021-05-26T01:22:09.400Z",
          "wordCount": 621,
          "title": "Look inside. Predicting stock prices by analysing an enterprise intranet social network and using word co-occurrence networks. (arXiv:2105.11780v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziqiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Abstractive summarization for long-document or multi-document remains\nchallenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing\nlong-distance relations in text. In this paper, we present BASS, a novel\nframework for Boosting Abstractive Summarization based on a unified Semantic\ngraph, which aggregates co-referent phrases distributing across a long range of\ncontext and conveys rich relations between phrases. Further, a graph-based\nencoder-decoder model is proposed to improve both the document representation\nand summary generation process by leveraging the graph structure. Specifically,\nseveral graph augmentation methods are designed to encode both the explicit and\nimplicit relations in the text while the graph-propagation attention mechanism\nis developed in the decoder to select salient content into the summary.\nEmpirical results show that the proposed architecture brings substantial\nimprovements for both long-document and multi-document summarization tasks.",
          "link": "http://arxiv.org/abs/2105.12041",
          "publishedOn": "2021-05-26T01:22:09.391Z",
          "wordCount": 573,
          "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph. (arXiv:2105.12041v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuanmeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rumei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>",
          "description": "Learning high-quality sentence representations benefits a wide range of\nnatural language processing tasks. Though BERT-based pre-trained language\nmodels achieve high performance on many downstream tasks, the native derived\nsentence representations are proved to be collapsed and thus produce a poor\nperformance on the semantic textual similarity (STS) tasks. In this paper, we\npresent ConSERT, a Contrastive Framework for Self-Supervised Sentence\nRepresentation Transfer, that adopts contrastive learning to fine-tune BERT in\nan unsupervised and effective way. By making use of unlabeled texts, ConSERT\nsolves the collapse issue of BERT-derived sentence representations and make\nthem more applicable for downstream tasks. Experiments on STS datasets\ndemonstrate that ConSERT achieves an 8\\% relative improvement over the previous\nstate-of-the-art, even comparable to the supervised SBERT-NLI. And when further\nincorporating NLI supervision, we achieve new state-of-the-art performance on\nSTS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples\navailable, showing its robustness in data scarcity scenarios.",
          "link": "http://arxiv.org/abs/2105.11741",
          "publishedOn": "2021-05-26T01:22:09.382Z",
          "wordCount": 596,
          "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer. (arXiv:2105.11741v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yikun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guibing Guo</a>",
          "description": "SemEval task 4 aims to find a proper option from multiple candidates to\nresolve the task of machine reading comprehension. Most existing approaches\npropose to concat question and option together to form a context-aware model.\nHowever, we argue that straightforward concatenation can only provide a\ncoarse-grained context for the MRC task, ignoring the specific positions of the\noption relative to the question. In this paper, we propose a novel MRC model by\nfilling options into the question to produce a fine-grained context (defined as\nsummary) which can better reveal the relationship between option and question.\nWe conduct a series of experiments on the given dataset, and the results show\nthat our approach outperforms other counterparts to a large extent.",
          "link": "http://arxiv.org/abs/2105.12051",
          "publishedOn": "2021-05-26T01:22:09.373Z",
          "wordCount": 571,
          "title": "NEUer at SemEval-2021 Task 4: Complete Summary Representation by Filling Answers into Question for Matching Reading Comprehension. (arXiv:2105.12051v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yuejia Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Semantic embedding has been widely investigated for aligning knowledge graph\n(KG) entities. Current methods have explored and utilized the graph structure,\nthe entity names and attributes, but ignore the ontology (or ontological\nschema) which contains critical meta information such as classes and their\nmembership relationships with entities. In this paper, we propose an\nontology-guided entity alignment method named OntoEA, where both KGs and their\nontologies are jointly embedded, and the class hierarchy and the class\ndisjointness are utilized to avoid false mappings. Extensive experiments on\nseven public and industrial benchmarks have demonstrated the state-of-the-art\nperformance of OntoEA and the effectiveness of the ontologies.",
          "link": "http://arxiv.org/abs/2105.07688",
          "publishedOn": "2021-05-25T01:56:08.843Z",
          "wordCount": 566,
          "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding. (arXiv:2105.07688v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Meisin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1\">Lay-Ki Soon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siew_E/0/1/0/all/0/1\">Eu-Gene Siew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugianto_L/0/1/0/all/0/1\">Ly Fie Sugianto</a>",
          "description": "Commodity News contains a wealth of information such as sum-mary of the\nrecent commodity price movement and notable events that led tothe movement.\nThrough event extraction, useful information extracted fromcommodity news is\nextremely useful in mining for causal relation betweenevents and commodity\nprice movement, which can be used for commodity priceprediction. To facilitate\nthe future research, we introduce a new dataset withthe following information\nidentified and annotated: (i) entities (both nomi-nal and named), (ii) events\n(trigger words and argument roles), (iii) eventmetadata: modality, polarity and\nintensity and (iv) event-event relations.",
          "link": "http://arxiv.org/abs/2105.08214",
          "publishedOn": "2021-05-25T01:56:08.792Z",
          "wordCount": 550,
          "title": "The Commodities News Corpus: A Resource forUnderstanding Commodity News Better. (arXiv:2105.08214v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vries_W/0/1/0/all/0/1\">Wietse de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>",
          "description": "For many (minority) languages, the resources needed to train large models are\nnot available. We investigate the performance of zero-shot transfer learning\nwith as little data as possible, and the influence of language similarity in\nthis process. We retrain the lexical layers of four BERT-based models using\ndata from two low-resource target language varieties, while the Transformer\nlayers are independently fine-tuned on a POS-tagging task in the model's source\nlanguage. By combining the new lexical layers and fine-tuned Transformer\nlayers, we achieve high task performance for both target languages. With high\nlanguage similarity, 10MB of data appears sufficient to achieve substantial\nmonolingual transfer performance. Monolingual BERT-based models generally\nachieve higher downstream task performance after retraining the lexical layer\nthan multilingual BERT, even when the target language is included in the\nmultilingual model.",
          "link": "http://arxiv.org/abs/2105.02855",
          "publishedOn": "2021-05-25T01:56:08.760Z",
          "wordCount": 591,
          "title": "Adapting Monolingual Models: Data can be Scarce when Language Similarity is High. (arXiv:2105.02855v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeung_R/0/1/0/all/0/1\">Richie Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kartsaklis_D/0/1/0/all/0/1\">Dimitri Kartsaklis</a>",
          "description": "While the DisCoCat model (Coecke et al., 2010) has been proved a valuable\ntool for studying compositional aspects of language at the level of semantics,\nits strong dependency on pregroup grammars poses important restrictions: first,\nit prevents large-scale experimentation due to the absence of a pregroup\nparser; and second, it limits the expressibility of the model to context-free\ngrammars. In this paper we solve these problems by reformulating DisCoCat as a\npassage from Combinatory Categorial Grammar (CCG) to a category of semantics.\nWe start by showing that standard categorial grammars can be expressed as a\nbiclosed category, where all rules emerge as currying/uncurrying the identity;\nwe then proceed to model permutation-inducing rules by exploiting the symmetry\nof the compact closed category encoding the word meaning. We provide a proof of\nconcept for our method, converting \"Alice in Wonderland\" into DisCoCat form, a\ncorpus that we make available to the community.",
          "link": "http://arxiv.org/abs/2105.07720",
          "publishedOn": "2021-05-25T01:56:08.752Z",
          "wordCount": 615,
          "title": "A CCG-Based Version of the DisCoCat Framework. (arXiv:2105.07720v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chi-Yang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yun-Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>",
          "description": "Writing a coherent and engaging story is not easy. Creative writers use their\nknowledge and worldview to put disjointed elements together to form a coherent\nstoryline, and work and rework iteratively toward perfection. Automated visual\nstorytelling (VIST) models, however, make poor use of external knowledge and\niterative generation when attempting to create stories. This paper introduces\nPR-VIST, a framework that represents the input image sequence as a story graph\nin which it finds the best path to form a storyline. PR-VIST then takes this\npath and learns to generate the final story via an iterative training process.\nThis framework produces stories that are superior in terms of diversity,\ncoherence, and humanness, per both automatic and human evaluations. An ablation\nstudy shows that both plotting and reworking contribute to the model's\nsuperiority.",
          "link": "http://arxiv.org/abs/2105.06950",
          "publishedOn": "2021-05-25T01:56:08.513Z",
          "wordCount": 589,
          "title": "Plot and Rework: Modeling Storylines for Visual Storytelling. (arXiv:2105.06950v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sumeet S. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karayev_S/0/1/0/all/0/1\">Sergey Karayev</a>",
          "description": "We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.",
          "link": "http://arxiv.org/abs/2103.06450",
          "publishedOn": "2021-05-25T01:56:08.489Z",
          "wordCount": 629,
          "title": "Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_G/0/1/0/all/0/1\">Gia H. Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>",
          "description": "Spell check is a useful application which processes noisy human-generated\ntext. Spell check for Chinese poses unresolved problems due to the large number\nof characters, the sparse distribution of errors, and the dearth of resources\nwith sufficient coverage of heterogeneous and shifting error domains. For\nChinese spell check, filtering using confusion sets narrows the search space\nand makes finding corrections easier. However, most, if not all, confusion sets\nused to date are fixed and thus do not include new, shifting error domains. We\npropose a scalable adaptable filter that exploits hierarchical character\nembeddings to (1) obviate the need to handcraft confusion sets, and (2) resolve\nsparsity problems related to infrequent errors. Our approach compares favorably\nwith competitive baselines and obtains SOTA results on the 2014 and 2015\nChinese Spelling Check Bake-off datasets.",
          "link": "http://arxiv.org/abs/2008.12281",
          "publishedOn": "2021-05-25T01:56:08.470Z",
          "wordCount": 619,
          "title": "Domain-shift Conditioning using Adaptable Filtering via Hierarchical Embeddings for Robust Chinese Spell Check. (arXiv:2008.12281v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>",
          "description": "Question answering systems usually use keyword searches to retrieve potential\npassages related to a question, and then extract the answer from passages with\nthe machine reading comprehension methods. However, many questions tend to be\nunanswerable in the real world. In this case, it is significant and challenging\nhow the model determines when no answer is supported by the passage and\nabstains from answering. Most of the existing systems design a simple\nclassifier to determine answerability implicitly without explicitly modeling\nmutual interaction and relation between the question and passage, leading to\nthe poor performance for determining the unanswerable questions. To tackle this\nproblem, we propose a Multi-Step Co-Interactive Relation Network (MCR-Net) to\nexplicitly model the mutual interaction and locate key clues from coarse to\nfine by introducing a co-interactive relation module. The co-interactive\nrelation module contains a stack of interaction and fusion blocks to\ncontinuously integrate and fuse history-guided and current-query-guided clues\nin an explicit way. Experiments on the SQuAD 2.0 and DuReader datasets show\nthat our model achieves a remarkable improvement, outperforming the BERT-style\nbaselines in literature. Visualization analysis also verifies the importance of\nthe mutual interaction between the question and passage.",
          "link": "http://arxiv.org/abs/2103.04567",
          "publishedOn": "2021-05-25T01:56:08.464Z",
          "wordCount": 668,
          "title": "MCR-Net: A Multi-Step Co-Interactive Relation Network for Unanswerable Questions on Machine Reading Comprehension. (arXiv:2103.04567v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>",
          "description": "This paper presents our systems for the three Subtasks of SemEval Task4:\nReading Comprehension of Abstract Meaning (ReCAM). We explain the algorithms\nused to learn our models and the process of tuning the algorithms and selecting\nthe best model. Inspired by the similarity of the ReCAM task and the language\npre-training, we propose a simple yet effective technology, namely, negative\naugmentation with language model. Evaluation results demonstrate the\neffectiveness of our proposed approach. Our models achieve the 4th rank on both\nofficial test sets of Subtask 1 and Subtask 2 with an accuracy of 87.9% and an\naccuracy of 92.8%, respectively. We further conduct comprehensive model\nanalysis and observe interesting error cases, which may promote future\nresearches.",
          "link": "http://arxiv.org/abs/2102.12828",
          "publishedOn": "2021-05-25T01:56:08.456Z",
          "wordCount": 617,
          "title": "ZJUKLAB at SemEval-2021 Task 4: Negative Augmentation with Language Model for Reading Comprehension of Abstract Meaning. (arXiv:2102.12828v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Conversational Machine Reading (CMR) aims at answering questions in a\ncomplicated manner. Machine needs to answer questions through interactions with\nusers based on given rule document, user scenario and dialogue history, and ask\nquestions to clarify if necessary. In this paper, we propose a dialogue graph\nmodeling framework to improve the understanding and reasoning ability of\nmachine on CMR task. There are three types of graph in total. Specifically,\nDiscourse Graph is designed to learn explicitly and extract the discourse\nrelation among rule texts as well as the extra knowledge of scenario;\nDecoupling Graph is used for understanding local and contextualized connection\nwithin rule texts. And finally a global graph for fusing the information\ntogether and reply to the user with our final decision being either\n\"Yes/No/Irrelevant\" or to ask a follow-up question to clarify.",
          "link": "http://arxiv.org/abs/2012.14827",
          "publishedOn": "2021-05-25T01:56:08.449Z",
          "wordCount": 594,
          "title": "Dialogue Graph Modeling for Conversational Machine Reading. (arXiv:2012.14827v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiexi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1\">Ryuichi Takanobu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1\">Dazhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weiran Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>",
          "description": "Most language understanding models in task-oriented dialog systems are\ntrained on a small amount of annotated training data, and evaluated in a small\nset from the same distribution. However, these models can lead to system\nfailure or undesirable output when being exposed to natural language\nperturbation or variation in practice. In this paper, we conduct comprehensive\nevaluation and analysis with respect to the robustness of natural language\nunderstanding models, and introduce three important aspects related to language\nunderstanding in real-world dialog systems, namely, language variety, speech\ncharacteristics, and noise perturbation. We propose a model-agnostic toolkit\nLAUG to approximate natural language perturbations for testing the robustness\nissues in task-oriented dialog. Four data augmentation approaches covering the\nthree aspects are assembled in LAUG, which reveals critical robustness issues\nin state-of-the-art models. The augmented dataset through LAUG can be used to\nfacilitate future research on the robustness testing of language understanding\nin task-oriented dialog.",
          "link": "http://arxiv.org/abs/2012.15262",
          "publishedOn": "2021-05-25T01:56:08.441Z",
          "wordCount": 627,
          "title": "Robustness Testing of Language Understanding in Task-Oriented Dialog. (arXiv:2012.15262v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shiyao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yubin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>",
          "description": "Event detection tends to struggle when it needs to recognize novel event\ntypes with a few samples. The previous work attempts to solve this problem in\nthe identify-then-classify manner but ignores the trigger discrepancy between\nevent types, thus suffering from the error propagation. In this paper, we\npresent a novel unified model which converts the task to a few-shot tagging\nproblem with a double-part tagging scheme. To this end, we first propose the\nPrototypical Amortized Conditional Random Field (PA-CRF) to model the label\ndependency in the few-shot scenario, which approximates the transition scores\nbetween labels based on the label prototypes. Then Gaussian distribution is\nintroduced for modeling of the transition scores to alleviate the uncertain\nestimation resulting from insufficient data. Experimental results show that the\nunified models work better than existing identify-then-classify models and our\nPA-CRF further achieves the best results on the benchmark dataset FewEvent. Our\ncode and data are available at this http URL",
          "link": "http://arxiv.org/abs/2012.02353",
          "publishedOn": "2021-05-25T01:56:08.434Z",
          "wordCount": 620,
          "title": "Few-Shot Event Detection with Prototypical Amortized Conditional Random Field. (arXiv:2012.02353v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhewei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>",
          "description": "Slang is a common type of informal language, but its flexible nature and\npaucity of data resources present challenges for existing natural language\nsystems. We take an initial step toward machine generation of slang by\ndeveloping a framework that models the speaker's word choice in slang context.\nOur framework encodes novel slang meaning by relating the conventional and\nslang senses of a word while incorporating syntactic and contextual knowledge\nin slang usage. We construct the framework using a combination of probabilistic\ninference and neural contrastive learning. We perform rigorous evaluations on\nthree slang dictionaries and show that our approach not only outperforms\nstate-of-the-art language models, but also better predicts the historical\nemergence of slang word usages from 1960s to 2000s. We interpret the proposed\nmodels and find that the contrastively learned semantic space is sensitive to\nthe similarities between slang and conventional senses of words. Our work\ncreates opportunities for the automated generation and interpretation of\ninformal language.",
          "link": "http://arxiv.org/abs/2102.01826",
          "publishedOn": "2021-05-25T01:56:08.414Z",
          "wordCount": 630,
          "title": "A Computational Framework for Slang Generation. (arXiv:2102.01826v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Existing methods for vision-and-language learning typically require designing\ntask-specific architectures and objectives for each task. For example, a\nmulti-label answer classifier for visual question answering, a region scorer\nfor referring expression comprehension, and a language decoder for image\ncaptioning, etc. To alleviate these hassles, in this work, we propose a unified\nframework that learns different tasks in a single architecture with the same\nlanguage modeling objective, i.e., multimodal conditional text generation,\nwhere our models learn to generate labels in text based on the visual and\ntextual inputs. On 7 popular vision-and-language benchmarks, including visual\nquestion answering, referring expression comprehension, visual commonsense\nreasoning, most of which have been previously modeled as discriminative tasks,\nour generative approach (with a single unified architecture) reaches comparable\nperformance to recent task-specific state-of-the-art vision-and-language\nmodels. Moreover, our generative approach shows better generalization ability\non questions that have rare answers. Also, we show that our framework allows\nmulti-task learning in a single architecture with a single set of parameters,\nachieving similar performance to separately optimized single-task models. Our\ncode is publicly available at: https://github.com/j-min/VL-T5",
          "link": "http://arxiv.org/abs/2102.02779",
          "publishedOn": "2021-05-25T01:56:08.407Z",
          "wordCount": 654,
          "title": "Unifying Vision-and-Language Tasks via Text Generation. (arXiv:2102.02779v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Siyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Junyuan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Transformers are not suited for processing long documents, due to their\nquadratically increasing memory and time consumption. Simply truncating a long\ndocument or applying the sparse attention mechanism will incur the context\nfragmentation problem or lead to an inferior modeling capability against\ncomparable model sizes. In this paper, we propose ERNIE-Doc, a document-level\nlanguage pretraining model based on Recurrence Transformers. Two well-designed\ntechniques, namely the retrospective feed mechanism and the enhanced recurrence\nmechanism, enable ERNIE-Doc, which has a much longer effective context length,\nto capture the contextual information of a complete document. We pretrain\nERNIE-Doc to explicitly learn the relationships among segments with an\nadditional document-aware segment-reordering objective. Various experiments\nwere conducted on both English and Chinese document-level tasks. ERNIE-Doc\nimproved the state-of-the-art language modeling result of perplexity to 16.8 on\nWikiText-103. Moreover, it outperformed competitive pretraining models by a\nlarge margin on most language understanding tasks, such as text classification\nand question answering.",
          "link": "http://arxiv.org/abs/2012.15688",
          "publishedOn": "2021-05-25T01:56:08.400Z",
          "wordCount": 622,
          "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer. (arXiv:2012.15688v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1\">Letitia Parcalabescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>",
          "description": "We investigate the reasoning ability of pretrained vision and language (V&L)\nmodels in two tasks that require multimodal integration: (1) discriminating a\ncorrect image-sentence pair from an incorrect one, and (2) counting entities in\nan image. We evaluate three pretrained V&L models on these tasks: ViLBERT,\nViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results\nshow that models solve task (1) very well, as expected, since all models are\npretrained on task (1). However, none of the pretrained V&L models is able to\nadequately solve task (2), our counting probe, and they cannot generalise to\nout-of-distribution quantities. We propose a number of explanations for these\nfindings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of\ncatastrophic forgetting on task (1). Concerning our results on the counting\nprobe, we find evidence that all models are impacted by dataset bias, and also\nfail to individuate entities in the visual input. While a selling point of\npretrained V&L models is their ability to solve complex tasks, our findings\nsuggest that understanding their reasoning and grounding capabilities requires\nmore targeted investigations on specific phenomena.",
          "link": "http://arxiv.org/abs/2012.12352",
          "publishedOn": "2021-05-25T01:56:08.392Z",
          "wordCount": 688,
          "title": "Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks. (arXiv:2012.12352v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1\">Jakub N&#xe1;plava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1\">Jana Strakov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>",
          "description": "We present RobeCzech, a monolingual RoBERTa language representation model\ntrained on Czech data. RoBERTa is a robustly optimized Transformer-based\npretraining approach. We show that RobeCzech considerably outperforms\nequally-sized multilingual and Czech-trained contextualized language\nrepresentation models, surpasses current state of the art in all five evaluated\nNLP tasks and reaches state-of-theart results in four of them. The RobeCzech\nmodel is released publicly at https://hdl.handle.net/11234/1-3691 and\nhttps://huggingface.co/ufal/robeczech-base.",
          "link": "http://arxiv.org/abs/2105.11314",
          "publishedOn": "2021-05-25T01:56:08.382Z",
          "wordCount": 502,
          "title": "RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model. (arXiv:2105.11314v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vries_W/0/1/0/all/0/1\">Wietse de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>",
          "description": "Large generative language models have been very successful for English, but\nother languages lag behind, in part due to data and computational limitations.\nWe propose a method that may overcome these problems by adapting existing\npre-trained models to new languages. Specifically, we describe the adaptation\nof English GPT-2 to Italian and Dutch by retraining lexical embeddings without\ntuning the Transformer layers. As a result, we obtain lexical embeddings for\nItalian and Dutch that are aligned with the original English lexical\nembeddings. Additionally, we scale up complexity by transforming relearned\nlexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This\nmethod minimises the amount of training and prevents losing information during\nadaptation that was learned by GPT-2. English GPT-2 models with relearned\nlexical embeddings can generate realistic sentences in Italian and Dutch.\nThough on average these sentences are still identifiable as artificial by\nhumans, they are assessed on par with sentences generated by a GPT-2 model\nfully trained from scratch.",
          "link": "http://arxiv.org/abs/2012.05628",
          "publishedOn": "2021-05-25T01:56:08.362Z",
          "wordCount": 633,
          "title": "As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages. (arXiv:2012.05628v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>",
          "description": "NLP has a rich history of representing our prior understanding of language in\nthe form of graphs. Recent work on analyzing contextualized text\nrepresentations has focused on hand-designed probe models to understand how and\nto what extent do these representations encode a particular linguistic\nphenomenon. However, due to the inter-dependence of various phenomena and\nrandomness of training probe models, detecting how these representations encode\nthe rich information in these linguistic graphs remains a challenging problem.\nIn this paper, we propose a new information-theoretic probe, Bird's Eye, which\nis a fairly simple probe method for detecting if and how these representations\nencode the information in these linguistic graphs. Instead of using classifier\nperformance, our probe takes an information-theoretic view of probing and\nestimates the mutual information between the linguistic graph embedded in a\ncontinuous space and the contextualized word representations. Furthermore, we\nalso propose an approach to use our probe to investigate localized linguistic\ninformation in the linguistic graphs using perturbation analysis. We call this\nprobing setup Worm's Eye. Using these probes, we analyze BERT models on their\nability to encode a syntactic and a semantic graph structure, and find that\nthese models encode to some degree both syntactic as well as semantic\ninformation; albeit syntactic information to a greater extent.",
          "link": "http://arxiv.org/abs/2105.02629",
          "publishedOn": "2021-05-25T01:56:08.354Z",
          "wordCount": 687,
          "title": "Bird's Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach. (arXiv:2105.02629v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>",
          "description": "We translate a closed text that is known in advance into a severely low\nresource language by leveraging massive source parallelism. In other words,\ngiven a text in 124 source languages, we translate it into a severely low\nresource language using only ~1,000 lines of low resource data without any\nexternal help. Firstly, we propose a systematic method to rank and choose\nsource languages that are close to the low resource language. We call the\nlinguistic definition of language family Family of Origin (FAMO), and we call\nthe empirical definition of higher-ranked languages using our metrics Family of\nChoice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual\nOrder-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines\n(~3.5%) of low resource data. To translate named entities correctly, we build a\nmassive lexicon table for 2,939 Bible named entities in 124 source languages,\nand include many that occur once and covers more than 66 severely low resource\nlanguages. Moreover, we also build a novel method of combining translations\nfrom different source languages into one. Using English as a hypothetical low\nresource language, we get a +23.9 BLEU increase over a multilingual baseline,\nand a +10.3 BLEU increase over our asymmetric baseline in the Bible dataset. We\nget a 42.8 BLEU score for Portuguese-English translation on the medical EMEA\ndataset. We also have good results for a real severely low resource Mayan\nlanguage, Eastern Pokomchi.",
          "link": "http://arxiv.org/abs/2104.05848",
          "publishedOn": "2021-05-25T01:56:08.347Z",
          "wordCount": 768,
          "title": "Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation. (arXiv:2104.05848v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>",
          "description": "In open-domain question answering (QA), retrieve-and-read mechanism has the\ninherent benefit of interpretability and the easiness of adding, removing, or\nediting knowledge compared to the parametric approaches of closed-book QA\nmodels. However, it is also known to suffer from its large storage footprint\ndue to its document corpus and index. Here, we discuss several orthogonal\nstrategies to drastically reduce the footprint of a retrieve-and-read\nopen-domain QA system by up to 160x. Our results indicate that\nretrieve-and-read can be a viable option even in a highly constrained serving\nenvironment such as edge devices, as we show that it can achieve better\naccuracy than a purely parametric model with comparable docker-level system\nsize.",
          "link": "http://arxiv.org/abs/2104.07242",
          "publishedOn": "2021-05-25T01:56:08.340Z",
          "wordCount": 566,
          "title": "Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering. (arXiv:2104.07242v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corney_D/0/1/0/all/0/1\">David Corney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1\">Maram Hasanain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1\">Tamer Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_Cedeno_A/0/1/0/all/0/1\">Alberto Barr&#xf3;n-Cede&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papotti_P/0/1/0/all/0/1\">Paolo Papotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>",
          "description": "The reporting and the analysis of current events around the globe has\nexpanded from professional, editor-lead journalism all the way to citizen\njournalism. Nowadays, politicians and other key players enjoy direct access to\ntheir audiences through social media, bypassing the filters of official cables\nor traditional media. However, the multiple advantages of free speech and\ndirect communication are dimmed by the misuse of media to spread inaccurate or\nmisleading claims. These phenomena have led to the modern incarnation of the\nfact-checker -- a professional whose main aim is to examine claims using\navailable evidence and to assess their veracity. As in other text forensics\ntasks, the amount of information available makes the work of the fact-checker\nmore difficult. With this in mind, starting from the perspective of the\nprofessional fact-checker, we survey the available intelligent technologies\nthat can support the human expert in the different steps of her fact-checking\nendeavor. These include identifying claims worth fact-checking, detecting\nrelevant previously fact-checked claims, retrieving relevant evidence to\nfact-check a claim, and actually verifying a claim. In each case, we pay\nattention to the challenges in future work and the potential impact on\nreal-world fact-checking.",
          "link": "http://arxiv.org/abs/2103.07769",
          "publishedOn": "2021-05-25T01:56:08.333Z",
          "wordCount": 692,
          "title": "Automated Fact-Checking for Assisting Human Fact-Checkers. (arXiv:2103.07769v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zibo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_S/0/1/0/all/0/1\">Simon Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>",
          "description": "We study the learning of a matching model for dialogue response selection.\nMotivated by the recent finding that models trained with random negative\nsamples are not ideal in real-world scenarios, we propose a hierarchical\ncurriculum learning framework that trains the matching model in an\n\"easy-to-difficult\" scheme. Our learning framework consists of two\ncomplementary curricula: (1) corpus-level curriculum (CC); and (2)\ninstance-level curriculum (IC). In CC, the model gradually increases its\nability in finding the matching clues between the dialogue context and a\nresponse candidate. As for IC, it progressively strengthens the model's ability\nin identifying the mismatching information between the dialogue context and a\nresponse candidate. Empirical studies on three benchmark datasets with three\nstate-of-the-art matching models demonstrate that the proposed learning\nframework significantly improves the model performance across various\nevaluation metrics.",
          "link": "http://arxiv.org/abs/2012.14756",
          "publishedOn": "2021-05-25T01:56:08.292Z",
          "wordCount": 606,
          "title": "Dialogue Response Selection with Hierarchical Curriculum Learning. (arXiv:2012.14756v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nie_P/0/1/0/all/0/1\">Ping Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_A/0/1/0/all/0/1\">Arun Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>",
          "description": "Existing approaches for open-domain question answering (QA) are typically\ndesigned for questions that require either single-hop or multi-hop reasoning,\nwhich make strong assumptions of the complexity of questions to be answered.\nAlso, multi-step document retrieval often incurs higher number of relevant but\nnon-supporting documents, which dampens the downstream noise-sensitive reader\nmodule for answer extraction. To address these challenges, we propose a unified\nQA framework to answer any-hop open-domain questions, which iteratively\nretrieves, reranks and filters documents, and adaptively determines when to\nstop the retrieval process. To improve the retrieval accuracy, we propose a\ngraph-based reranking model that perform multi-document interaction as the core\nof our iterative reranking framework. Our method consistently achieves\nperformance comparable to or better than the state-of-the-art on both\nsingle-hop and multi-hop open-domain QA datasets, including Natural Questions\nOpen, SQuAD Open, and HotpotQA.",
          "link": "http://arxiv.org/abs/2009.07465",
          "publishedOn": "2021-05-25T01:56:08.285Z",
          "wordCount": 623,
          "title": "Answering Any-hop Open-domain Questions with Iterative Document Reranking. (arXiv:2009.07465v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11407",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_D/0/1/0/all/0/1\">Debanjali Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_M/0/1/0/all/0/1\">Mohnish Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rony_M/0/1/0/all/0/1\">Md Rashad Al Hasan Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>",
          "description": "In the last years, there have been significant developments in the area of\nQuestion Answering over Knowledge Graphs (KGQA). Despite all the notable\nadvancements, current KGQA datasets only provide the answers as the direct\noutput result of the formal query, rather than full sentences incorporating\nquestion context. For achieving coherent answers sentence with the question's\nvocabulary, template-based verbalization so are usually employed for a better\nrepresentation of answers, which in turn require extensive expert intervention.\nThus, making way for machine learning approaches; however, there is a scarcity\nof datasets that empower machine learning models in this area. Hence, we\nprovide the VANiLLa dataset which aims at reducing this gap by offering answers\nin natural language sentences. The answer sentences in this dataset are\nsyntactically and semantically closer to the question than to the triple fact.\nOur dataset consists of over 100k simple questions adapted from the CSQA and\nSimpleQuestionsWikidata datasets and generated using a semi-automatic\nframework. We also present results of training our dataset on multiple baseline\nmodels adapted from current state-of-the-art Natural Language Generation (NLG)\narchitectures. We believe that this dataset will allow researchers to focus on\nfinding suitable methodologies and architectures for answer verbalization.",
          "link": "http://arxiv.org/abs/2105.11407",
          "publishedOn": "2021-05-25T01:56:08.278Z",
          "wordCount": 628,
          "title": "VANiLLa : Verbalized Answers in Natural Language at Large Scale. (arXiv:2105.11407v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>",
          "description": "Pretrained language models (LMs) perform well on many tasks even when\nlearning from a few examples, but prior work uses many held-out examples to\ntune various aspects of learning, such as hyperparameters, training objectives,\nand natural language templates (\"prompts\"). Here, we evaluate the few-shot\nability of LMs when such held-out examples are unavailable, a setting we call\ntrue few-shot learning. We test two model selection criteria, cross-validation\nand minimum description length, for choosing LM prompts and hyperparameters in\nthe true few-shot setting. On average, both marginally outperform random\nselection and greatly underperform selection based on held-out examples.\nMoreover, selection criteria often prefer models that perform significantly\nworse than randomly-selected ones. We find similar results even when taking\ninto account our uncertainty in a model's true performance during selection, as\nwell as when varying the amount of computation and number of examples used for\nselection. Overall, our findings suggest that prior work significantly\noverestimated the true few-shot ability of LMs given the difficulty of few-shot\nmodel selection.",
          "link": "http://arxiv.org/abs/2105.11447",
          "publishedOn": "2021-05-25T01:56:08.272Z",
          "wordCount": 601,
          "title": "True Few-Shot Learning with Language Models. (arXiv:2105.11447v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.02725",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1\">Yuewen Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "This paper proposes an any-to-many location-relative, sequence-to-sequence\n(seq2seq), non-parallel voice conversion approach, which utilizes text\nsupervision during training. In this approach, we combine a bottle-neck feature\nextractor (BNE) with a seq2seq synthesis module. During the training stage, an\nencoder-decoder-based hybrid connectionist-temporal-classification-attention\n(CTC-attention) phoneme recognizer is trained, whose encoder has a bottle-neck\nlayer. A BNE is obtained from the phoneme recognizer and is utilized to extract\nspeaker-independent, dense and rich spoken content representations from\nspectral features. Then a multi-speaker location-relative attention based\nseq2seq synthesis model is trained to reconstruct spectral features from the\nbottle-neck features, conditioning on speaker representations for speaker\nidentity control in the generated speech. To mitigate the difficulties of using\nseq2seq models to align long sequences, we down-sample the input spectral\nfeature along the temporal dimension and equip the synthesis model with a\ndiscretized mixture of logistic (MoL) attention mechanism. Since the phoneme\nrecognizer is trained with large speech recognition data corpus, the proposed\napproach can conduct any-to-many voice conversion. Objective and subjective\nevaluations show that the proposed any-to-many approach has superior voice\nconversion performance in terms of both naturalness and speaker similarity.\nAblation studies are conducted to confirm the effectiveness of feature\nselection and model design strategies in the proposed approach. The proposed VC\napproach can readily be extended to support any-to-any VC (also known as\none/few-shot VC), and achieve high performance according to objective and\nsubjective evaluations.",
          "link": "http://arxiv.org/abs/2009.02725",
          "publishedOn": "2021-05-25T01:56:08.265Z",
          "wordCount": 712,
          "title": "Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence Modeling. (arXiv:2009.02725v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>",
          "description": "Pretrained language models (PLMs) perform poorly under adversarial attacks.\nTo improve the adversarial robustness, adversarial data augmentation (ADA) has\nbeen widely adopted to cover more search space of adversarial attacks by adding\ntextual adversarial examples during training. However, the number of\nadversarial examples for text augmentation is still extremely insufficient due\nto the exponentially large attack search space. In this work, we propose a\nsimple and effective method to cover a much larger proportion of the attack\nsearch space, called Adversarial and Mixup Data Augmentation (AMDA).\nSpecifically, AMDA linearly interpolates the representations of pairs of\ntraining samples to form new virtual samples, which are more abundant and\ndiverse than the discrete text adversarial examples in conventional ADA.\nMoreover, to fairly evaluate the robustness of different models, we adopt a\nchallenging evaluation setup, which generates a new set of adversarial examples\ntargeting each model. In text classification experiments of BERT and RoBERTa,\nAMDA achieves significant robustness gains under two strong adversarial attacks\nand alleviates the performance degradation of ADA on the clean data. Our code\nis released at: https://github.com/thunlp/MixADA .",
          "link": "http://arxiv.org/abs/2012.15699",
          "publishedOn": "2021-05-25T01:56:08.246Z",
          "wordCount": 657,
          "title": "Better Robustness by More Coverage: Adversarial Training with Mixup Augmentation for Robust Fine-tuning. (arXiv:2012.15699v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jia Tracy Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_M/0/1/0/all/0/1\">Michiharu Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prihar_E/0/1/0/all/0/1\">Ethan Prihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGrew_S/0/1/0/all/0/1\">Sean McGrew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>",
          "description": "Educational content labeled with proper knowledge components (KCs) are\nparticularly useful to teachers or content organizers. However, manually\nlabeling educational content is labor intensive and error-prone. To address\nthis challenge, prior research proposed machine learning based solutions to\nauto-label educational content with limited success. In this work, we\nsignificantly improve prior research by (1) expanding the input types to\ninclude KC descriptions, instructional video titles, and problem descriptions\n(i.e., three types of prediction task), (2) doubling the granularity of the\nprediction from 198 to 385 KC labels (i.e., more practical setting but much\nharder multinomial classification problem), (3) improving the prediction\naccuracies by 0.5-2.3% using Task-adaptive Pre-trained BERT, outperforming six\nbaselines, and (4) proposing a simple evaluation measure by which we can\nrecover 56-73% of mispredicted KC labels. All codes and data sets in the\nexperiments are available at:https://github.com/tbs17/TAPT-BERT",
          "link": "http://arxiv.org/abs/2105.11343",
          "publishedOn": "2021-05-25T01:56:08.240Z",
          "wordCount": 575,
          "title": "Classifying Math KCs via Task-Adaptive Pre-Trained BERT. (arXiv:2105.11343v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.04507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kehai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>",
          "description": "Unsupervised neural machine translation (UNMT) that relies solely on massive\nmonolingual corpora has achieved remarkable results in several translation\ntasks. However, in real-world scenarios, massive monolingual corpora do not\nexist for some extremely low-resource languages such as Estonian, and UNMT\nsystems usually perform poorly when there is not adequate training corpus for\none language. In this paper, we first define and analyze the unbalanced\ntraining data scenario for UNMT. Based on this scenario, we propose UNMT\nself-training mechanisms to train a robust UNMT system and improve its\nperformance in this case. Experimental results on several language pairs show\nthat the proposed methods substantially outperform conventional UNMT systems.",
          "link": "http://arxiv.org/abs/2004.04507",
          "publishedOn": "2021-05-25T01:56:08.232Z",
          "wordCount": 578,
          "title": "Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios. (arXiv:2004.04507v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1\">Da-Cheng Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Che Zheng</a>",
          "description": "The dot product self-attention is known to be central and indispensable to\nstate-of-the-art Transformer models. But is it really required? This paper\ninvestigates the true importance and contribution of the dot product-based\nself-attention mechanism on the performance of Transformer models. Via\nextensive experiments, we find that (1) random alignment matrices surprisingly\nperform quite competitively and (2) learning attention weights from token-token\n(query-key) interactions is useful but not that important after all. To this\nend, we propose \\textsc{Synthesizer}, a model that learns synthetic attention\nweights without token-token interactions. In our experiments, we first show\nthat simple Synthesizers achieve highly competitive performance when compared\nagainst vanilla Transformer models across a range of tasks, including machine\ntranslation, language modeling, text generation and GLUE/SuperGLUE benchmarks.\nWhen composed with dot product attention, we find that Synthesizers\nconsistently outperform Transformers. Moreover, we conduct additional\ncomparisons of Synthesizers against Dynamic Convolutions, showing that simple\nRandom Synthesizer is not only $60\\%$ faster but also improves perplexity by a\nrelative $3.5\\%$. Finally, we show that simple factorized Synthesizers can\noutperform Linformers on encoding only tasks.",
          "link": "http://arxiv.org/abs/2005.00743",
          "publishedOn": "2021-05-25T01:56:08.225Z",
          "wordCount": 657,
          "title": "Synthesizer: Rethinking Self-Attention in Transformer Models. (arXiv:2005.00743v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Kiran Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_O/0/1/0/all/0/1\">Owais Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1\">Ankan Mullick</a>",
          "description": "The presented report evaluates Contextualizing Hate Speech Classifiers with\nPost-hoc Explanation paper within the scope of ML Reproducibility Challenge\n2020. Our work focuses on both aspects constituting the paper: the method\nitself and the validity of the stated results. In the following sections, we\nhave described the paper, related works, algorithmic frameworks, our\nexperiments and evaluations.",
          "link": "http://arxiv.org/abs/2105.11412",
          "publishedOn": "2021-05-25T01:56:08.215Z",
          "wordCount": 489,
          "title": "Reproducibility Report: Contextualizing Hate Speech Classifiers with Post-hoc Explanation. (arXiv:2105.11412v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1\">Jakub N&#xe1;plava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1\">Jana Strakov&#xe1;</a>",
          "description": "We propose a new architecture for diacritics restoration based on\ncontextualized embeddings, namely BERT, and we evaluate it on 12 languages with\ndiacritics. Furthermore, we conduct a detailed error analysis on Czech, a\nmorphologically rich language with a high level of diacritization. Notably, we\nmanually annotate all mispredictions, showing that roughly 44% of them are\nactually not errors, but either plausible variants (19%), or the system\ncorrections of erroneous data (25%). Finally, we categorize the real errors in\ndetail. We release the code at\nhttps://github.com/ufal/bert-diacritics-restoration.",
          "link": "http://arxiv.org/abs/2105.11408",
          "publishedOn": "2021-05-25T01:56:08.194Z",
          "wordCount": 528,
          "title": "Diacritics Restoration using BERT with Analysis on Czech language. (arXiv:2105.11408v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lihu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1\">Ga&#xeb;l Varoquaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1\">Fabian M. Suchanek</a>",
          "description": "Biomedical entity linking aims to map biomedical mentions, such as diseases\nand drugs, to standard entities in a given knowledge base. The specific\nchallenge in this context is that the same biomedical entity can have a wide\nrange of names, including synonyms, morphological variations, and names with\ndifferent word orderings. Recently, BERT-based methods have advanced the\nstate-of-the-art by allowing for rich representations of word sequences.\nHowever, they often have hundreds of millions of parameters and require heavy\ncomputing resources, which limits their applications in resource-limited\nscenarios. Here, we propose a lightweight neural method for biomedical entity\nlinking, which needs just a fraction of the parameters of a BERT model and much\nless computing resources. Our method uses a simple alignment layer with\nattention mechanisms to capture the variations between mention and entity\nnames. Yet, we show that our model is competitive with previous work on\nstandard evaluation benchmarks.",
          "link": "http://arxiv.org/abs/2012.08844",
          "publishedOn": "2021-05-25T01:56:08.186Z",
          "wordCount": 600,
          "title": "A Lightweight Neural Model for Biomedical Entity Linking. (arXiv:2012.08844v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Arkadipta De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.",
          "link": "http://arxiv.org/abs/2105.11347",
          "publishedOn": "2021-05-25T01:56:08.178Z",
          "wordCount": 634,
          "title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.03327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pykl_S/0/1/0/all/0/1\">Srinivas Pykl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guptha_V/0/1/0/all/0/1\">Vineeth Guptha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_G/0/1/0/all/0/1\">Gitanjali Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news\nand rumors are rampant on social media. Believing in rumors can cause\nsignificant harm. This is further exacerbated at the time of a pandemic. To\ntackle this, we curate and release a manually annotated dataset of 10,700\nsocial media posts and articles of real and fake news on COVID-19. We benchmark\nthe annotated dataset with four machine learning baselines - Decision Tree,\nLogistic Regression, Gradient Boost, and Support Vector Machine (SVM). We\nobtain the best performance of 93.46% F1-score with SVM. The data and code is\navailable at: https://github.com/parthpatwa/covid19-fake-news-dectection",
          "link": "http://arxiv.org/abs/2011.03327",
          "publishedOn": "2021-05-25T01:56:08.170Z",
          "wordCount": 641,
          "title": "Fighting an Infodemic: COVID-19 Fake News Dataset. (arXiv:2011.03327v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhong-Qiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">DeLiang Wang</a>",
          "description": "We propose multi-microphone complex spectral mapping, a simple way of\napplying deep learning for time-varying non-linear beamforming, for speaker\nseparation in reverberant conditions. We aim at both speaker separation and\ndereverberation. Our study first investigates offline utterance-wise speaker\nseparation and then extends to block-online continuous speech separation (CSS).\nAssuming a fixed array geometry between training and testing, we train deep\nneural networks (DNN) to predict the real and imaginary (RI) components of\ntarget speech at a reference microphone from the RI components of multiple\nmicrophones. We then integrate multi-microphone complex spectral mapping with\nminimum variance distortionless response (MVDR) beamforming and post-filtering\nto further improve separation, and combine it with frame-level speaker counting\nfor block-online CSS. Although our system is trained on simulated room impulse\nresponses (RIR) based on a fixed number of microphones arranged in a given\ngeometry, it generalizes well to a real array with the same geometry.\nState-of-the-art separation performance is obtained on the simulated two-talker\nSMS-WSJ corpus and the real-recorded LibriCSS dataset.",
          "link": "http://arxiv.org/abs/2010.01703",
          "publishedOn": "2021-05-25T01:56:08.162Z",
          "wordCount": 649,
          "title": "Multi-microphone Complex Spectral Mapping for Utterance-wise and Continuous Speech Separation. (arXiv:2010.01703v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Tung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kui_W/0/1/0/all/0/1\">Wu Kui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aw_A/0/1/0/all/0/1\">Ai Ti Aw</a>",
          "description": "Recent unsupervised machine translation (UMT) systems usually employ three\nmain principles: initialization, language modeling and iterative\nback-translation, though they may apply them differently. Crucially, iterative\nback-translation and denoising auto-encoding for language modeling provide data\ndiversity to train the UMT systems. However, the gains from these\ndiversification processes has seemed to plateau. We introduce a novel component\nto the standard UMT framework called Cross-model Back-translated Distillation\n(CBD), that is aimed to induce another level of data diversification that\nexisting principles lack. CBD is applicable to all previous UMT approaches. In\nour experiments, CBD achieves the state of the art in the WMT'14\nEnglish-French, WMT'16 English-German and English-Romanian bilingual\nunsupervised translation tasks, with 38.2, 30.1, and 36.3 BLEU respectively. It\nalso yields 1.5-3.3 BLEU improvements in IWSLT English-French and\nEnglish-German tasks. Through extensive experimental analyses, we show that CBD\nis effective because it embraces data diversity while other similar variants do\nnot.",
          "link": "http://arxiv.org/abs/2006.02163",
          "publishedOn": "2021-05-25T01:56:08.143Z",
          "wordCount": 636,
          "title": "Cross-model Back-translated Distillation for Unsupervised Machine Translation. (arXiv:2006.02163v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_K/0/1/0/all/0/1\">Kasra Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beelen_K/0/1/0/all/0/1\">Kaspar Beelen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colavizza_G/0/1/0/all/0/1\">Giovanni Colavizza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardanuy_M/0/1/0/all/0/1\">Mariona Coll Ardanuy</a>",
          "description": "We present four types of neural language models trained on a large historical\ndataset of books in English, published between 1760-1900 and comprised of ~5.1\nbillion tokens. The language model architectures include static (word2vec and\nfastText) and contextualized models (BERT and Flair). For each architecture, we\ntrained a model instance using the whole dataset. Additionally, we trained\nseparate instances on text published before 1850 for the two static models, and\nfour instances considering different time slices for BERT. Our models have\nalready been used in various downstream tasks where they consistently improved\nperformance. In this paper, we describe how the models have been created and\noutline their reuse potential.",
          "link": "http://arxiv.org/abs/2105.11321",
          "publishedOn": "2021-05-25T01:56:08.134Z",
          "wordCount": 540,
          "title": "Neural Language Models for Nineteenth-Century English. (arXiv:2105.11321v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jensen_K/0/1/0/all/0/1\">Kristian N&#xf8;rgaard Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>",
          "description": "De-identification is the task of detecting privacy-related entities in text,\nsuch as person names, emails and contact data. It has been well-studied within\nthe medical domain. The need for de-identification technology is increasing, as\nprivacy-preserving data handling is in high demand in many domains. In this\npaper, we focus on job postings. We present JobStack, a new corpus for\nde-identification of personal data in job vacancies on Stackoverflow. We\nintroduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer\nmodels. To improve upon these baselines, we experiment with contextualized\nembeddings and distantly related auxiliary data via multi-task learning. Our\nresults show that auxiliary data improves de-identification performance.\nSurprisingly, vanilla BERT turned out to be more effective than a BERT model\ntrained on other portions of Stackoverflow.",
          "link": "http://arxiv.org/abs/2105.11223",
          "publishedOn": "2021-05-25T01:56:08.122Z",
          "wordCount": 559,
          "title": "De-identification of Privacy-related Entities in Job Postings. (arXiv:2105.11223v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>",
          "description": "Fine-tuned pre-trained language models (PLMs) have achieved awesome\nperformance on almost all NLP tasks. By using additional prompts to fine-tune\nPLMs, we can further stimulate the rich knowledge distributed in PLMs to better\nserve downstream task. Prompt tuning has achieved promising results on some\nfew-class classification tasks such as sentiment classification and natural\nlanguage inference. However, manually designing lots of language prompts is\ncumbersome and fallible. For those auto-generated prompts, it is also expensive\nand time-consuming to verify their effectiveness in non-few-shot scenarios.\nHence, it is challenging for prompt tuning to address many-class classification\ntasks. To this end, we propose prompt tuning with rules (PTR) for many-class\ntext classification, and apply logic rules to construct prompts with several\nsub-prompts. In this way, PTR is able to encode prior knowledge of each class\ninto prompt tuning. We conduct experiments on relation classification, a\ntypical many-class classification task, and the results on benchmarks show that\nPTR can significantly and consistently outperform existing state-of-the-art\nbaselines. This indicates that PTR is a promising approach to take advantage of\nPLMs for those complicated classification tasks.",
          "link": "http://arxiv.org/abs/2105.11259",
          "publishedOn": "2021-05-25T01:56:08.114Z",
          "wordCount": 608,
          "title": "PTR: Prompt Tuning with Rules for Text Classification. (arXiv:2105.11259v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Peter Wallis</a>",
          "description": "Virtual Personal Assistants like Siri have great potential but such\ndevelopments hit the fundamental problem of how to make computational devices\nthat understand human speech. Natural language understanding is one of the more\ndisappointing failures of AI research and it seems there is something we\ncomputer scientists don't get about the nature of language. Of course\nphilosophers and linguists think quite differently about language and this\npaper describes how we have taken ideas from other disciplines and implemented\nthem. The background to the work is to take seriously the notion of language as\naction and look at what people actually do with language using the techniques\nof Conversation Analysis. The observation has been that human communication is\n(behind the scenes) about the management of social relations as well as the\n(foregrounded) passing of information. To claim this is one thing but to\nimplement it requires a mechanism. The mechanism described here is based on the\nnotion of language being intentional - we think intentionally, talk about them\nand recognise them in others - and cooperative in that we are compelled to help\nout. The way we are compelled points to a solution to the ever present problem\nof keeping the human on topic. The approach has led to a recent success in\nwhich we significantly improve user satisfaction independent of task\ncompletion. Talk Markup Language (TalkML) is a draft alternative to VoiceXML\nthat, we propose, greatly simplifies the scripting of interaction by providing\ndefault behaviours for no input and not recognised speech events.",
          "link": "http://arxiv.org/abs/2105.11294",
          "publishedOn": "2021-05-25T01:56:08.103Z",
          "wordCount": 696,
          "title": "Introducing the Talk Markup Language (TalkML):Adding a little social intelligence to industrial speech interfaces. (arXiv:2105.11294v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1\">Payam Karisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>",
          "description": "We present an algorithm based on multi-layer transformers for identifying\nAdverse Drug Reactions (ADR) in social media data. Our model relies on the\nproperties of the problem and the characteristics of contextual word embeddings\nto extract two views from documents. Then a classifier is trained on each view\nto label a set of unlabeled documents to be used as an initializer for a new\nclassifier in the other view. Finally, the initialized classifier in each view\nis further trained using the initial training examples. We evaluated our model\nin the largest publicly available ADR dataset. The experiments testify that our\nmodel significantly outperforms the transformer-based models pretrained on\ndomain-specific data.",
          "link": "http://arxiv.org/abs/2105.11354",
          "publishedOn": "2021-05-25T01:56:08.089Z",
          "wordCount": 553,
          "title": "View Distillation with Unlabeled Data for Extracting Adverse Drug Effects from User-Generated Data. (arXiv:2105.11354v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_K/0/1/0/all/0/1\">Kristian N&#xf8;rgaard Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>",
          "description": "This paper introduces DaN+, a new multi-domain corpus and annotation\nguidelines for Danish nested named entities (NEs) and lexical normalization to\nsupport research on cross-lingual cross-domain learning for a less-resourced\nlanguage. We empirically assess three strategies to model the two-layer Named\nEntity Recognition (NER) task. We compare transfer capabilities from German\nversus in-language annotation from scratch. We examine language-specific versus\nmultilingual BERT, and study the effect of lexical normalization on NER. Our\nresults show that 1) the most robust strategy is multi-task learning which is\nrivaled by multi-label decoding, 2) BERT-based NER models are sensitive to\ndomain shifts, and 3) in-language BERT and lexical normalization are the most\nbeneficial on the least canonical data. Our results also show that an\nout-of-domain setup remains challenging, while performance on news plateaus\nquickly. This highlights the importance of cross-domain evaluation of\ncross-lingual transfer.",
          "link": "http://arxiv.org/abs/2105.11301",
          "publishedOn": "2021-05-25T01:56:08.068Z",
          "wordCount": 569,
          "title": "DaN+: Danish Nested Named Entities and Lexical Normalization. (arXiv:2105.11301v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>",
          "description": "Pre-trained Transformer-based neural language models, such as BERT, have\nachieved remarkable results on varieties of NLP tasks. Recent works have shown\nthat attention-based models can benefit from more focused attention over local\nregions. Most of them restrict the attention scope within a linear span, or\nconfine to certain tasks such as machine translation and question answering. In\nthis paper, we propose a syntax-aware local attention, where the attention\nscopes are restrained based on the distances in the syntactic structure. The\nproposed syntax-aware local attention can be integrated with pretrained\nlanguage models, such as BERT, to render the model to focus on syntactically\nrelevant words. We conduct experiments on various single-sentence benchmarks,\nincluding sentence classification and sequence labeling tasks. Experimental\nresults show consistent gains over BERT on all benchmark datasets. The\nextensive studies verify that our model achieves better performance owing to\nmore focused attention over syntactically relevant words.",
          "link": "http://arxiv.org/abs/2012.15150",
          "publishedOn": "2021-05-25T01:56:08.061Z",
          "wordCount": 604,
          "title": "Improving BERT with Syntax-aware Local Attention. (arXiv:2012.15150v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iezzi_D/0/1/0/all/0/1\">D. F. Iezzi</a>",
          "description": "According to Freud \"words were originally magic and to this day words have\nretained much of their ancient magical power\". By words, behaviors are\ntransformed and problems are solved. The way we use words reveals our\nintentions, goals and values. Novel tools for text analysis help understand the\nmagical power of words. This power is multiplied, if it is combined with the\nstudy of social networks, i.e. with the analysis of relationships among social\nunits. This special issue of the International Journal of Information\nManagement, entitled \"Combining Social Network Analysis and Text Mining: from\nTheory to Practice\", includes heterogeneous and innovative research at the\nnexus of text mining and social network analysis. It aims to enrich work at the\nintersection of these fields, which still lags behind in theoretical,\nempirical, and methodological foundations. The nine articles accepted for\ninclusion in this special issue all present methods and tools that have\nbusiness applications. They are summarized in this editorial introduction.",
          "link": "http://arxiv.org/abs/2105.11263",
          "publishedOn": "2021-05-25T01:56:08.051Z",
          "wordCount": 608,
          "title": "Editorial introduction: The power of words and networks. (arXiv:2105.11263v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bella_A/0/1/0/all/0/1\">A. La Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battistoni_E/0/1/0/all/0/1\">E. Battistoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellan_S/0/1/0/all/0/1\">S. Castellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francucci_M/0/1/0/all/0/1\">M. Francucci</a>",
          "description": "We propose a text classification tool based on support vector machines for\nthe assessment of organizational leadership styles, as appearing to Twitter\nusers. We collected Twitter data over 51 days, related to the first 30 Italian\norganizations in the 2015 ranking of Forbes Global 2000-out of which we\nselected the five with the most relevant volumes of tweets. We analyzed the\ncommunication of the company leaders, together with the dialogue among the\nstakeholders of each company, to understand the association with perceived\nleadership styles and dimensions. To assess leadership profiles, we referred to\nthe 10-factor model developed by Barchiesi and La Bella in 2007. We maintain\nthe distinctiveness of the approach we propose, as it allows a rapid assessment\nof the perceived leadership capabilities of an enterprise, as they emerge from\nits social media interactions. It can also be used to show how companies\nrespond and manage their communication when specific events take place, and to\nassess their stakeholder's reactions.",
          "link": "http://arxiv.org/abs/2105.11276",
          "publishedOn": "2021-05-25T01:56:08.043Z",
          "wordCount": 611,
          "title": "Assessing perceived organizational leadership styles through twitter text mining. (arXiv:2105.11276v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tianming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gaurav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Maozu Guo</a>",
          "description": "Label noise and long-tailed distributions are two major challenges in\ndistantly supervised relation extraction. Recent studies have shown great\nprogress on denoising, but pay little attention to the problem of long-tailed\nrelations. In this paper, we introduce constraint graphs to model the\ndependencies between relation labels. On top of that, we further propose a\nnovel constraint graph-based relation extraction framework(CGRE) to handle the\ntwo challenges simultaneously. CGRE employs graph convolution networks (GCNs)\nto propagate information from data-rich relation nodes to data-poor relation\nnodes, and thus boosts the representation learning of long-tailed relations. To\nfurther improve the noise immunity, a constraint-aware attention module is\ndesigned in CGRE to integrate the constraint information. Experimental results\non a widely-used benchmark dataset indicate that our approach achieves\nsignificant improvements over the previous methods for both denoising and\nlong-tailed relation extraction.",
          "link": "http://arxiv.org/abs/2105.11225",
          "publishedOn": "2021-05-25T01:56:08.035Z",
          "wordCount": 561,
          "title": "Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs. (arXiv:2105.11225v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>",
          "description": "Prior work has proved that Translation memory (TM) can boost the performance\nof Neural Machine Translation (NMT). In contrast to existing work that uses\nbilingual corpus as TM and employs source-side similarity search for memory\nretrieval, we propose a new framework that uses monolingual memory and performs\nlearnable memory retrieval in a cross-lingual manner. Our framework has unique\nadvantages. First, the cross-lingual memory retriever allows abundant\nmonolingual data to be TM. Second, the memory retriever and NMT model can be\njointly optimized for the ultimate translation goal. Experiments show that the\nproposed method obtains substantial improvements. Remarkably, it even\noutperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the\nability to leverage monolingual data, our model also demonstrates effectiveness\nin low-resource and domain adaptation scenarios.",
          "link": "http://arxiv.org/abs/2105.11269",
          "publishedOn": "2021-05-25T01:56:08.009Z",
          "wordCount": 557,
          "title": "Neural Machine Translation with Monolingual Translation Memory. (arXiv:2105.11269v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1\">Bin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>",
          "description": "Large pre-trained language models achieve state-of-the-art results when\nfine-tuned on downstream NLP tasks. However, they almost exclusively focus on\ntext-only representation, while neglecting cell-level layout information that\nis important for form image understanding. In this paper, we propose a new\npre-training approach, StructuralLM, to jointly leverage cell and layout\ninformation from scanned documents. Specifically, we pre-train StructuralLM\nwith two new designs to make the most of the interactions of cell and layout\ninformation: 1) each cell as a semantic unit; 2) classification of cell\npositions. The pre-trained StructuralLM achieves new state-of-the-art results\nin different types of downstream tasks, including form understanding (from\n78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and\ndocument image classification (from 94.43 to 96.08).",
          "link": "http://arxiv.org/abs/2105.11210",
          "publishedOn": "2021-05-25T01:56:08.000Z",
          "wordCount": 555,
          "title": "StructuralLM: Structural Pre-training for Form Understanding. (arXiv:2105.11210v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halterman_A/0/1/0/all/0/1\">Andrew Halterman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_B/0/1/0/all/0/1\">Benjamin J. Radford</a>",
          "description": "We propose a new task and dataset for a common problem in social science\nresearch: \"upsampling\" coarse document labels to fine-grained labels or spans.\nWe pose the problem in a question answering format, with the answers providing\nthe fine-grained labels. We provide a benchmark dataset and baselines on a\nsocially impactful task: identifying the exact crowd size at protests and\ndemonstrations in the United States given only order-of-magnitude information\nabout protest attendance, a very small sample of fine-grained examples, and\nEnglish-language news text. We evaluate several baseline models, including\nzero-shot results from rule-based and question-answering models, few-shot\nmodels fine-tuned on a small set of documents, and weakly supervised models\nusing a larger set of coarsely-labeled documents. We find that our rule-based\nmodel initially outperforms a zero-shot pre-trained transformer language model\nbut that further fine-tuning on a very small subset of 25 examples\nsubstantially improves out-of-sample performance. We also demonstrate a method\nfor fine-tuning the transformer span on only the coarse labels that performs\nsimilarly to our rule-based approach. This work will contribute to social\nscientists' ability to generate data to understand the causes and successes of\ncollective action.",
          "link": "http://arxiv.org/abs/2105.11260",
          "publishedOn": "2021-05-25T01:56:07.984Z",
          "wordCount": 617,
          "title": "Few-Shot Upsampling for Protest Size Detection. (arXiv:2105.11260v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PYKL_S/0/1/0/all/0/1\">Srinivas PYKL</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_P/0/1/0/all/0/1\">Prerana Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulabaigari_V/0/1/0/all/0/1\">Viswanath Pulabaigari</a>",
          "description": "Contending hate speech in social media is one of the most challenging social\nproblems of our time. There are various types of anti-social behavior in social\nmedia. Foremost of them is aggressive behavior, which is causing many social\nissues such as affecting the social lives and mental health of social media\nusers. In this paper, we propose an end-to-end ensemble-based architecture to\nautomatically identify and classify aggressive tweets. Tweets are classified\ninto three categories - Covertly Aggressive, Overtly Aggressive, and\nNon-Aggressive. The proposed architecture is an ensemble of smaller subnetworks\nthat are able to characterize the feature embeddings effectively. We\ndemonstrate qualitatively that each of the smaller subnetworks is able to learn\nunique features. Our best model is an ensemble of Capsule Networks and results\nin a 65.2% F1 score on the Facebook test set, which results in a performance\ngain of 0.95% over the TRAC-2018 winners. The code and the model weights are\npublicly available at\nhttps://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks.",
          "link": "http://arxiv.org/abs/2105.11219",
          "publishedOn": "2021-05-25T01:56:07.977Z",
          "wordCount": 600,
          "title": "Hater-O-Genius Aggression Classification using Capsule Networks. (arXiv:2105.11219v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hongru Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huaqing Li</a>",
          "description": "Human evaluation is becoming a necessity to test the performance of Chatbots.\nHowever, off-the-shelf settings suffer the severe reliability and replication\nissues partly because of the extremely high diversity of criteria. It is high\ntime to come up with standard criteria and exact definitions. To this end, we\nconduct a through investigation of 105 papers involving human evaluation for\nChatbots. Deriving from this, we propose five standard criteria along with\nprecise definitions.",
          "link": "http://arxiv.org/abs/2105.11197",
          "publishedOn": "2021-05-25T01:56:07.966Z",
          "wordCount": 497,
          "title": "Towards Standard Criteria for human evaluation of Chatbots: A Survey. (arXiv:2105.11197v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peiji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shixing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhisheng Wang</a>",
          "description": "Cross-lingual text classification aims at training a classifier on the source\nlanguage and transferring the knowledge to target languages, which is very\nuseful for low-resource languages. Recent multilingual pretrained language\nmodels (mPLM) achieve impressive results in cross-lingual classification tasks,\nbut rarely consider factors beyond semantic similarity, causing performance\ndegradation between some language pairs. In this paper we propose a simple yet\neffective method to incorporate heterogeneous information within and across\nlanguages for cross-lingual text classification using graph convolutional\nnetworks (GCN). In particular, we construct a heterogeneous graph by treating\ndocuments and words as nodes, and linking nodes with different relations, which\ninclude part-of-speech roles, semantic similarity, and document translations.\nExtensive experiments show that our graph-based method significantly\noutperforms state-of-the-art models on all tasks, and also achieves consistent\nperformance gain over baselines in low-resource settings where external tools\nlike translators are unavailable.",
          "link": "http://arxiv.org/abs/2105.11246",
          "publishedOn": "2021-05-25T01:56:07.957Z",
          "wordCount": 575,
          "title": "Cross-lingual Text Classification with Heterogeneous Graph Neural Network. (arXiv:2105.11246v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jieyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jiajie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>",
          "description": "Pre-trained language models have achieved human-level performance on many\nMachine Reading Comprehension (MRC) tasks, but it remains unclear whether these\nmodels truly understand language or answer questions by exploiting statistical\nbiases in datasets. Here, we demonstrate a simple yet effective method to\nattack MRC models and reveal the statistical biases in these models. We apply\nthe method to the RACE dataset, for which the answer to each MRC question is\nselected from 4 options. It is found that several pre-trained language models,\nincluding BERT, ALBERT, and RoBERTa, show consistent preference to some\noptions, even when these options are irrelevant to the question. When\ninterfered by these irrelevant options, the performance of MRC models can be\nreduced from human-level performance to the chance-level performance. Human\nreaders, however, are not clearly affected by these irrelevant options.\nFinally, we propose an augmented training method that can greatly reduce\nmodels' statistical biases.",
          "link": "http://arxiv.org/abs/2105.11136",
          "publishedOn": "2021-05-25T01:56:07.937Z",
          "wordCount": 581,
          "title": "Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models. (arXiv:2105.11136v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_C/0/1/0/all/0/1\">Christina Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetto_M/0/1/0/all/0/1\">Matthias Cetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handschuh_S/0/1/0/all/0/1\">Siegfried Handschuh</a>",
          "description": "We present a context-preserving text simplification (TS) approach that\nrecursively splits and rephrases complex English sentences into a semantic\nhierarchy of simplified sentences. Using a set of linguistically principled\ntransformation patterns, input sentences are converted into a hierarchical\nrepresentation in the form of core sentences and accompanying contexts that are\nlinked via rhetorical relations. Hence, as opposed to previously proposed\nsentence splitting approaches, which commonly do not take into account\ndiscourse-level aspects, our TS approach preserves the semantic relationship of\nthe decomposed constituents in the output. A comparative analysis with the\nannotations contained in the RST-DT shows that we are able to capture the\ncontextual hierarchy between the split sentences with a precision of 89% and\nreach an average precision of 69% for the classification of the rhetorical\nrelations that hold between them.",
          "link": "http://arxiv.org/abs/2105.11178",
          "publishedOn": "2021-05-25T01:56:07.832Z",
          "wordCount": 548,
          "title": "Context-Preserving Text Simplification. (arXiv:2105.11178v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miao_M/0/1/0/all/0/1\">Mengqi Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao-Hua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "The Neural Machine Translation (NMT) model is essentially a joint language\nmodel conditioned on both the source sentence and partial translation.\nTherefore, the NMT model naturally involves the mechanism of the Language Model\n(LM) that predicts the next token only based on partial translation. Despite\nits success, NMT still suffers from the hallucination problem, generating\nfluent but inadequate translations. The main reason is that NMT pays excessive\nattention to the partial translation while neglecting the source sentence to\nsome extent, namely overconfidence of the LM. Accordingly, we define the Margin\nbetween the NMT and the LM, calculated by subtracting the predicted probability\nof the LM from that of the NMT model for each token. The Margin is negatively\ncorrelated to the overconfidence degree of the LM. Based on the property, we\npropose a Margin-based Token-level Objective (MTO) and a Margin-based\nSentencelevel Objective (MSO) to maximize the Margin for preventing the LM from\nbeing overconfident. Experiments on WMT14 English-to-German, WMT19\nChinese-to-English, and WMT14 English-to-French translation tasks demonstrate\nthe effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements,\nrespectively, compared to the Transformer baseline. The human evaluation\nfurther verifies that our approaches improve translation adequacy as well as\nfluency.",
          "link": "http://arxiv.org/abs/2105.11098",
          "publishedOn": "2021-05-25T01:56:07.756Z",
          "wordCount": 653,
          "title": "Prevent the Language Model from being Overconfident in Neural Machine Translation. (arXiv:2105.11098v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohenecker_P/0/1/0/all/0/1\">Patrick Hohenecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "In this paper, we introduce the new task of controllable text edition, in\nwhich we take as input a long text, a question, and a target answer, and the\noutput is a minimally modified text, so that it fits the target answer. This\ntask is very important in many situations, such as changing some conditions,\nconsequences, or properties in a legal document, or changing some key\ninformation of an event in a news text. This is very challenging, as it is hard\nto obtain a parallel corpus for training, and we need to first find all text\npositions that should be changed and then decide how to change them. We\nconstructed the new dataset WikiBioCTE for this task based on the existing\ndataset WikiBio (originally created for table-to-text generation). We use\nWikiBioCTE for training, and manually labeled a test set for testing. We also\npropose novel evaluation metrics and a novel method for solving the new task.\nExperimental results on the test set show that our proposed method is a good\nfit for this novel NLP task.",
          "link": "http://arxiv.org/abs/2105.11018",
          "publishedOn": "2021-05-25T01:56:07.749Z",
          "wordCount": 616,
          "title": "Controlling Text Edition by Changing Answers of Specific Questions. (arXiv:2105.11018v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Pre-trained language models (PrLMs) have demonstrated superior performance\ndue to their strong ability to learn universal language representations from\nself-supervised pre-training. However, even with the help of the powerful\nPrLMs, it is still challenging to effectively capture task-related knowledge\nfrom dialogue texts which are enriched by correlations among speaker-aware\nutterances. In this work, we present SPIDER, Structural Pre-traIned DialoguE\nReader, to capture dialogue exclusive features. To simulate the dialogue-like\nfeatures, we propose two training objectives in addition to the original LM\nobjectives: 1) utterance order restoration, which predicts the order of the\npermuted utterances in dialogue context; 2) sentence backbone regularization,\nwhich regularizes the model to improve the factual correctness of summarized\nsubject-verb-object triplets. Experimental results on widely used dialogue\nbenchmarks verify the effectiveness of the newly introduced self-supervised\ntasks.",
          "link": "http://arxiv.org/abs/2105.10956",
          "publishedOn": "2021-05-25T01:56:07.740Z",
          "wordCount": 557,
          "title": "Structural Pre-training for Dialogue Comprehension. (arXiv:2105.10956v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yichao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yige Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>",
          "description": "Recently, the sequence-to-sequence models have made remarkable progress on\nthe task of keyphrase generation (KG) by concatenating multiple keyphrases in a\npredefined order as a target sequence during training. However, the keyphrases\nare inherently an unordered set rather than an ordered sequence. Imposing a\npredefined order will introduce wrong bias during training, which can highly\npenalize shifts in the order between keyphrases. In this work, we propose a new\ntraining paradigm One2Set without predefining an order to concatenate the\nkeyphrases. To fit this paradigm, we propose a novel model that utilizes a\nfixed set of learned control codes as conditions to generate a set of\nkeyphrases in parallel. To solve the problem that there is no correspondence\nbetween each prediction and target during training, we propose a $K$-step\ntarget assignment mechanism via bipartite matching, which greatly increases the\ndiversity and reduces the duplication ratio of generated keyphrases. The\nexperimental results on multiple benchmarks demonstrate that our approach\nsignificantly outperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.11134",
          "publishedOn": "2021-05-25T01:56:07.733Z",
          "wordCount": 596,
          "title": "One2Set: Generating Diverse Keyphrases as a Set. (arXiv:2105.11134v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>",
          "description": "Commonsense generation is a challenging task of generating a plausible\nsentence describing an everyday scenario using provided concepts. Its\nrequirement of reasoning over commonsense knowledge and compositional\ngeneralization ability even puzzles strong pre-trained language generation\nmodels. We propose a novel framework using retrieval methods to enhance both\nthe pre-training and fine-tuning for commonsense generation. We retrieve\nprototype sentence candidates by concept matching and use them as auxiliary\ninput. For fine-tuning, we further boost its performance with a trainable\nsentence retriever. We demonstrate experimentally on the large-scale CommonGen\nbenchmark that our approach achieves new state-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.11174",
          "publishedOn": "2021-05-25T01:56:07.702Z",
          "wordCount": 532,
          "title": "Retrieval Enhanced Model for Commonsense Generation. (arXiv:2105.11174v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valido_A/0/1/0/all/0/1\">Alberto Valido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingram_K/0/1/0/all/0/1\">Katherine M. Ingram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1\">Giulia Fanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1\">Suma Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espelage_D/0/1/0/all/0/1\">Dorothy L. Espelage</a>",
          "description": "Abusive language is a massive problem in online social platforms. Existing\nabusive language detection techniques are particularly ill-suited to comments\ncontaining heterogeneous abusive language patterns, i.e., both abusive and\nnon-abusive parts. This is due in part to the lack of datasets that explicitly\nannotate heterogeneity in abusive language. We tackle this challenge by\nproviding an annotated dataset of abusive language in over 11,000 comments from\nYouTube. We account for heterogeneity in this dataset by separately annotating\nboth the comment as a whole and the individual sentences that comprise each\ncomment. We then propose an algorithm that uses a supervised attention\nmechanism to detect and categorize abusive content using multi-task learning.\nWe empirically demonstrate the challenges of using traditional techniques on\nheterogeneous content and the comparative gains in performance of the proposed\napproach over state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.11119",
          "publishedOn": "2021-05-25T01:56:07.607Z",
          "wordCount": 590,
          "title": "Abusive Language Detection in Heterogeneous Contexts: Dataset Collection and the Role of Supervised Attention. (arXiv:2105.11119v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dakle_P/0/1/0/all/0/1\">Parag Pravin Dakle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moldovan_D/0/1/0/all/0/1\">Dan I. Moldovan</a>",
          "description": "We present the first large scale corpus for entity resolution in email\nconversations (CEREC). The corpus consists of 6001 email threads from the Enron\nEmail Corpus containing 36,448 email messages and 60,383 entity coreference\nchains. The annotation is carried out as a two-step process with minimal manual\neffort. Experiments are carried out for evaluating different features and\nperformance of four baselines on the created corpus. For the task of mention\nidentification and coreference resolution, a best performance of 60.08 F1 is\nreported, highlighting the room for improvement. An in-depth qualitative and\nquantitative error analysis is presented to understand the limitations of the\nbaselines considered.",
          "link": "http://arxiv.org/abs/2105.10606",
          "publishedOn": "2021-05-25T01:56:07.501Z",
          "wordCount": 550,
          "title": "CEREC: A Corpus for Entity Resolution in Email Conversations. (arXiv:2105.10606v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Binghui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1\">Christos Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>",
          "description": "Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.",
          "link": "http://arxiv.org/abs/2105.11115",
          "publishedOn": "2021-05-25T01:56:07.481Z",
          "wordCount": 611,
          "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages. (arXiv:2105.11115v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>",
          "description": "Despite rapid progress in the recent past, current speech recognition systems\nstill require labeled training data which limits this technology to a small\nfraction of the languages spoken around the globe. This paper describes\nwav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition\nmodels without any labeled data. We leverage self-supervised speech\nrepresentations to segment unlabeled audio and learn a mapping from these\nrepresentations to phonemes via adversarial training. The right representations\nare key to the success of our method. Compared to the best previous\nunsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT\nbenchmark from 26.1 to 11.3. On the larger English Librispeech benchmark,\nwav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the\nbest published systems trained on 960 hours of labeled data from only two years\nago. We also experiment on nine other languages, including low-resource\nlanguages such as Kyrgyz, Swahili and Tatar.",
          "link": "http://arxiv.org/abs/2105.11084",
          "publishedOn": "2021-05-25T01:56:07.323Z",
          "wordCount": 580,
          "title": "Unsupervised Speech Recognition. (arXiv:2105.11084v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>",
          "description": "The advances in pre-trained models (e.g., BERT, XLNET and etc) have largely\nrevolutionized the predictive performance of various modern natural language\nprocessing tasks. This allows corporations to provide machine learning as a\nservice (MLaaS) by encapsulating fine-tuned BERT-based models as commercial\nAPIs. However, previous works have discovered a series of vulnerabilities in\nBERT- based APIs. For example, BERT-based APIs are vulnerable to both model\nextraction attack and adversarial example transferrability attack. However, due\nto the high capacity of BERT-based APIs, the fine-tuned model is easy to be\noverlearned, what kind of information can be leaked from the extracted model\nremains unknown and is lacking. To bridge this gap, in this work, we first\npresent an effective model extraction attack, where the adversary can\npractically steal a BERT-based API (the target/victim model) by only querying a\nlimited number of queries. We further develop an effective attribute inference\nattack to expose the sensitive attribute of the training data used by the\nBERT-based APIs. Our extensive experiments on benchmark datasets under various\nrealistic settings demonstrate the potential vulnerabilities of BERT-based\nAPIs.",
          "link": "http://arxiv.org/abs/2105.10909",
          "publishedOn": "2021-05-25T01:56:07.260Z",
          "wordCount": 629,
          "title": "Killing Two Birds with One Stone: Stealing Model and Inferring Attribute from BERT-based APIs. (arXiv:2105.10909v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zogan_H/0/1/0/all/0/1\">Hamad Zogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_I/0/1/0/all/0/1\">Imran Razzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jameel_S/0/1/0/all/0/1\">Shoaib Jameel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guandong Xu</a>",
          "description": "Twitter is currently a popular online social media platform which allows\nusers to share their user-generated content. This publicly-generated user data\nis also crucial to healthcare technologies because the discovered patterns\nwould hugely benefit them in several ways. One of the applications is in\nautomatically discovering mental health problems, e.g., depression. Previous\nstudies to automatically detect a depressed user on online social media have\nlargely relied upon the user behaviour and their linguistic patterns including\nuser's social interactions. The downside is that these models are trained on\nseveral irrelevant content which might not be crucial towards detecting a\ndepressed user. Besides, these content have a negative impact on the overall\nefficiency and effectiveness of the model. To overcome the shortcomings in the\nexisting automatic depression detection methods, we propose a novel\ncomputational framework for automatic depression detection that initially\nselects relevant content through a hybrid extractive and abstractive\nsummarization strategy on the sequence of all user tweets leading to a more\nfine-grained and relevant content. The content then goes to our novel deep\nlearning framework comprising of a unified learning machinery comprising of\nConvolutional Neural Network (CNN) coupled with attention-enhanced Gated\nRecurrent Units (GRU) models leading to better empirical performance than\nexisting strong baselines.",
          "link": "http://arxiv.org/abs/2105.10878",
          "publishedOn": "2021-05-25T01:56:07.245Z",
          "wordCount": 651,
          "title": "DepressionNet: A Novel Summarization Boosted Deep Framework for Depression Detection on Social Media. (arXiv:2105.10878v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1\">Dustin Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Scientific document understanding is challenging as the data is highly domain\nspecific and diverse. However, datasets for tasks with scientific text require\nexpensive manual annotation and tend to be small and limited to only one or a\nfew fields. At the same time, scientific documents contain many potential\ntraining signals, such as citations, which can be used to build large labelled\ndatasets. Given this, we present an in-depth study of cite-worthiness detection\nin English, where a sentence is labelled for whether or not it cites an\nexternal source. To accomplish this, we introduce CiteWorth, a large,\ncontextualized, rigorously cleaned labelled dataset for cite-worthiness\ndetection built from a massive corpus of extracted plain-text scientific\ndocuments. We show that CiteWorth is high-quality, challenging, and suitable\nfor studying problems such as domain adaptation. Our best performing\ncite-worthiness detection model is a paragraph-level contextualized sentence\nlabelling model based on Longformer, exhibiting a 5 F1 point improvement over\nSciBERT which considers only individual sentences. Finally, we demonstrate that\nlanguage model fine-tuning with cite-worthiness as a secondary task leads to\nimproved performance on downstream scientific document understanding tasks.",
          "link": "http://arxiv.org/abs/2105.10912",
          "publishedOn": "2021-05-25T01:56:07.215Z",
          "wordCount": 625,
          "title": "CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding. (arXiv:2105.10912v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oksanen_J/0/1/0/all/0/1\">Joel Oksanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1\">Oana Cocarascu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>",
          "description": "Ontologies have proven beneficial in different settings that make use of\ntextual reviews. However, manually constructing ontologies is a laborious and\ntime-consuming process in need of automation. We propose a novel methodology\nfor automatically extracting ontologies, in the form of meronomies, from\nproduct reviews, using a very limited amount of hand-annotated training data.\nWe show that the ontologies generated by our method outperform hand-crafted\nontologies (WordNet) and ontologies extracted by existing methods (Text2Onto\nand COMET) in several, diverse settings. Specifically, our generated ontologies\noutperform the others when evaluated by human annotators as well as on an\nexisting Q&A dataset from Amazon. Moreover, our method is better able to\ngeneralise, in capturing knowledge about unseen products. Finally, we consider\na real-world setting, showing that our method is better able to determine\nrecommended products based on their reviews, in alternative to using Amazon's\nstandard score aggregations.",
          "link": "http://arxiv.org/abs/2105.10966",
          "publishedOn": "2021-05-25T01:56:07.153Z",
          "wordCount": 565,
          "title": "Automatic Product Ontology Extraction from Textual Reviews. (arXiv:2105.10966v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Tung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>",
          "description": "We introduce a novel top-down end-to-end formulation of document-level\ndiscourse parsing in the Rhetorical Structure Theory (RST) framework. In this\nformulation, we consider discourse parsing as a sequence of splitting decisions\nat token boundaries and use a seq2seq network to model the splitting decisions.\nOur framework facilitates discourse parsing from scratch without requiring\ndiscourse segmentation as a prerequisite; rather, it yields segmentation as\npart of the parsing process. Our unified parsing model adopts a beam search to\ndecode the best tree structure by searching through a space of high-scoring\ntrees. With extensive experiments on the standard English RST discourse\ntreebank, we demonstrate that our parser outperforms existing methods by a good\nmargin in both end-to-end parsing and parsing with gold segmentation. More\nimportantly, it does so without using any handcrafted features, making it\nfaster and easily adaptable to new languages and domains.",
          "link": "http://arxiv.org/abs/2105.10861",
          "publishedOn": "2021-05-25T01:56:07.132Z",
          "wordCount": 570,
          "title": "RST Parsing from Scratch. (arXiv:2105.10861v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10344",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Sanwal_U/0/1/0/all/0/1\">Usman Sanwal</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hoang_T/0/1/0/all/0/1\">Thai Son Hoang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Petre_L/0/1/0/all/0/1\">Luigia Petre</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Petre_I/0/1/0/all/0/1\">Ion Petre</a>",
          "description": "Biology offers many examples of large-scale, complex, concurrent systems:\nmany processes take place in parallel, compete on resources and influence each\nother's behavior. The scalable modeling of biological systems continues to be a\nvery active field of research. In this paper we introduce a new approach based\non Event-B, a state-based formal method with refinement as its central\ningredient, allowing us to check for model consistency step-by-step in an\nautomated way. Our approach based on functions leads to an elegant and concise\nmodeling method. We demonstrate this approach by constructing what is, to our\nknowledge, the largest ever built Event-B model, describing the ErbB signaling\npathway, a key evolutionary pathway with a significant role in development and\nin many types of cancer. The Event-B model for the ErbB pathway describes 1320\nmolecular reactions through 242 events.",
          "link": "http://arxiv.org/abs/2105.10344",
          "publishedOn": "2021-05-24T05:08:41.025Z",
          "wordCount": 566,
          "title": "Towards Scalable Modeling of Biology in Event-B. (arXiv:2105.10344v1 [q-bio.MN])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1\">Victor Veitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_D/0/1/0/all/0/1\">Dhanya Sridhar</a>",
          "description": "We consider the problem of using observational data to estimate the causal\neffects of linguistic properties. For example, does writing a complaint\npolitely lead to a faster response time? How much will a positive product\nreview increase sales? This paper addresses two technical challenges related to\nthe problem before developing a practical method. First, we formalize the\ncausal quantity of interest as the effect of a writer's intent, and establish\nthe assumptions necessary to identify this from observational data. Second, in\npractice, we only have access to noisy proxies for the linguistic properties of\ninterest -- e.g., predictions from classifiers and lexicons. We propose an\nestimator for this setting and prove that its bias is bounded when we perform\nan adjustment for the text. Based on these results, we introduce TextCause, an\nalgorithm for estimating causal effects of linguistic properties. The method\nleverages (1) distant supervision to improve the quality of noisy proxies, and\n(2) a pre-trained language model (BERT) to adjust for the text. We show that\nthe proposed method outperforms related approaches when estimating the effect\nof Amazon review sentiment on semi-simulated sales figures. Finally, we present\nan applied case study investigating the effects of complaint politeness on\nbureaucratic response times.",
          "link": "http://arxiv.org/abs/2010.12919",
          "publishedOn": "2021-05-24T05:08:40.908Z",
          "wordCount": 696,
          "title": "Causal Effects of Linguistic Properties. (arXiv:2010.12919v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1\">Matthew R. Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patki_S/0/1/0/all/0/1\">Siddharth Patki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniele_A/0/1/0/all/0/1\">Andrea F. Daniele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahnestock_E/0/1/0/all/0/1\">Ethan Fahnestock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duvallet_F/0/1/0/all/0/1\">Felix Duvallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemachandra_S/0/1/0/all/0/1\">Sachithra Hemachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stentz_A/0/1/0/all/0/1\">Anthony Stentz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1\">Nicholas Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_T/0/1/0/all/0/1\">Thomas M. Howard</a>",
          "description": "Contemporary approaches to perception, planning, estimation, and control have\nallowed robots to operate robustly as our remote surrogates in uncertain,\nunstructured environments. There is now an opportunity for robots to operate\nnot only in isolation, but also with and alongside humans in our complex\nenvironments. Natural language provides an efficient and flexible medium\nthrough which humans can communicate with collaborative robots. Through\nsignificant progress in statistical methods for natural language understanding,\nrobots are now able to interpret a diverse array of free-form navigation,\nmanipulation, and mobile manipulation commands. However, most contemporary\napproaches require a detailed prior spatial-semantic map of the robot's\nenvironment that models the space of possible referents of the utterance.\nConsequently, these methods fail when robots are deployed in new, previously\nunknown, or partially observed environments, particularly when mental models of\nthe environment differ between the human operator and the robot. This paper\nprovides a comprehensive description of a novel learning framework that allows\nfield and service robots to interpret and correctly execute natural language\ninstructions in a priori unknown, unstructured environments. Integral to our\napproach is its use of language as a \"sensor\" -- inferring spatial,\ntopological, and semantic information implicit in natural language utterances\nand then exploiting this information to learn a distribution over a latent\nenvironment model. We incorporate this distribution in a probabilistic language\ngrounding model and infer a distribution over a symbolic representation of the\nrobot's action space. We use imitation learning to identify a belief space\npolicy that reasons over the environment and behavior distributions. We\nevaluate our framework through a variety of different navigation and mobile\nmanipulation experiments.",
          "link": "http://arxiv.org/abs/2105.10396",
          "publishedOn": "2021-05-24T05:08:40.831Z",
          "wordCount": 722,
          "title": "Language Understanding for Field and Service Robots in a Priori Unknown Environments. (arXiv:2105.10396v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kvapilikova_I/0/1/0/all/0/1\">Ivana Kvapil&#x131;kova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1\">Gorka Labaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>",
          "description": "Existing models of multilingual sentence embeddings require large parallel\ndata resources which are not available for low-resource languages. We propose a\nnovel unsupervised method to derive multilingual sentence embeddings relying\nonly on monolingual data. We first produce a synthetic parallel corpus using\nunsupervised machine translation, and use it to fine-tune a pretrained\ncross-lingual masked language model (XLM) to derive the multilingual sentence\nrepresentations. The quality of the representations is evaluated on two\nparallel corpus mining tasks with improvements of up to 22 F1 points over\nvanilla XLM. In addition, we observe that a single synthetic bilingual corpus\nis able to improve results for other language pairs.",
          "link": "http://arxiv.org/abs/2105.10419",
          "publishedOn": "2021-05-24T05:08:40.774Z",
          "wordCount": 565,
          "title": "Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining. (arXiv:2105.10419v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ennen_P/0/1/0/all/0/1\">Philipp Ennen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozbay_A/0/1/0/all/0/1\">Ali Girayhan Ozbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Insalata_F/0/1/0/all/0/1\">Ferdinando Insalata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalali_S/0/1/0/all/0/1\">Sepehr Jalali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1\">Da-shan Shiu</a>",
          "description": "In a dialogue system pipeline, a natural language generation (NLG) unit\nconverts the dialogue direction and content to a corresponding natural language\nrealization. A recent trend for dialogue systems is to first pre-train on large\ndatasets and then fine-tune in a supervised manner using datasets annotated\nwith application-specific features. Though novel behaviours can be learned from\ncustom annotation, the required effort severely bounds the quantity of the\ntraining set, and the application-specific nature limits the reuse. In light of\nthe recent success of data-driven approaches, we propose the novel future\nbridging NLG (FBNLG) concept for dialogue systems and simulators. The critical\nstep is for an FBNLG to accept a future user or system utterance to bridge the\npresent context towards. Future bridging enables self supervised training over\nannotation-free datasets, decoupled the training of NLG from the rest of the\nsystem. An FBNLG, pre-trained with massive datasets, is expected to apply in\nclassical or new dialogue scenarios with minimal adaptation effort. We evaluate\na prototype FBNLG to show that future bridging can be a viable approach to a\nuniversal few-shot NLG for task-oriented and chit-chat dialogues.",
          "link": "http://arxiv.org/abs/2105.10267",
          "publishedOn": "2021-05-24T05:08:39.757Z",
          "wordCount": 638,
          "title": "Towards a Universal NLG for Dialogue Systems and Simulators with Future Bridging. (arXiv:2105.10267v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>",
          "description": "The problem of knowledge-based visual question answering involves answering\nquestions that require external knowledge in addition to the content of the\nimage. Such knowledge typically comes in a variety of forms, including visual,\ntextual, and commonsense knowledge. The use of more knowledge sources, however,\nalso increases the chance of retrieving more irrelevant or noisy facts, making\nit difficult to comprehend the facts and find the answer. To address this\nchallenge, we propose Multi-modal Answer Validation using External knowledge\n(MAVEx), where the idea is to validate a set of promising answer candidates\nbased on answer-specific knowledge retrieval. This is in contrast to existing\napproaches that search for the answer in a vast collection of often irrelevant\nfacts. Our approach aims to learn which knowledge source should be trusted for\neach answer candidate and how to validate the candidate using that source. We\nconsider a multi-modal setting, relying on both textual and visual knowledge\nresources, including images searched using Google, sentences from Wikipedia\narticles, and concepts from ConceptNet. Our experiments with OK-VQA, a\nchallenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2103.12248",
          "publishedOn": "2021-05-24T05:08:39.733Z",
          "wordCount": 642,
          "title": "Multi-Modal Answer Validation for Knowledge-Based VQA. (arXiv:2103.12248v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>",
          "description": "Although neural models have achieved competitive results in dialogue systems,\nthey have shown limited ability in representing core semantics, such as\nignoring important entities. To this end, we exploit Abstract Meaning\nRepresentation (AMR) to help dialogue modeling. Compared with the textual\ninput, AMR explicitly provides core semantic knowledge and reduces data\nsparsity. We develop an algorithm to construct dialogue-level AMR graphs from\nsentence-level AMRs and explore two ways to incorporate AMRs into dialogue\nsystems. Experimental results on both dialogue understanding and response\ngeneration tasks show the superiority of our model. To our knowledge, we are\nthe first to leverage a formal semantic representation into neural dialogue\nmodeling.",
          "link": "http://arxiv.org/abs/2105.10188",
          "publishedOn": "2021-05-24T05:08:39.722Z",
          "wordCount": 535,
          "title": "Semantic Representation for Dialogue Modeling. (arXiv:2105.10188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyusong Lee</a>",
          "description": "Text-to-image retrieval is an essential task in cross-modal information\nretrieval, i.e., retrieving relevant images from a large and unlabelled dataset\ngiven textual queries. In this paper, we propose VisualSparta, a novel\n(Visual-text Sparse Transformer Matching) model that shows significant\nimprovement in terms of both accuracy and efficiency. VisualSparta is capable\nof outperforming previous state-of-the-art scalable methods in MSCOCO and\nFlickr30K. We also show that it achieves substantial retrieving speed\nadvantages, i.e., for a 1 million image index, VisualSparta using CPU gets\n~391X speedup compared to CPU vector search and ~5.4X speedup compared to\nvector search with GPU acceleration. Experiments show that this speed advantage\neven gets bigger for larger datasets because VisualSparta can be efficiently\nimplemented as an inverted index. To the best of our knowledge, VisualSparta is\nthe first transformer-based text-to-image retrieval model that can achieve\nreal-time searching for large-scale datasets, with significant accuracy\nimprovement compared to previous state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2101.00265",
          "publishedOn": "2021-05-24T05:08:39.544Z",
          "wordCount": 629,
          "title": "VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words. (arXiv:2101.00265v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.03659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1\">John Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitski_O/0/1/0/all/0/1\">Osvald Nitski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bader_G/0/1/0/all/0/1\">Gary Bader</a>",
          "description": "Sentence embeddings are an important component of many natural language\nprocessing (NLP) systems. Like word embeddings, sentence embeddings are\ntypically learned on large text corpora and then transferred to various\ndownstream tasks, such as clustering and retrieval. Unlike word embeddings, the\nhighest performing solutions for learning sentence embeddings require labelled\ndata, limiting their usefulness to languages and domains where labelled data is\nabundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for\nUnsupervised Textual Representations. Inspired by recent advances in deep\nmetric learning (DML), we carefully design a self-supervised objective for\nlearning universal sentence embeddings that does not require labelled training\ndata. When used to extend the pretraining of transformer-based language models,\nour approach closes the performance gap between unsupervised and supervised\npretraining for universal sentence encoders. Importantly, our experiments\nsuggest that the quality of the learned embeddings scale with both the number\nof trainable parameters and the amount of unlabelled training data, making\nfurther improvements straightforward. Our code and pretrained models are\npublicly available and can be easily adapted to new domains or used to embed\nunseen text.",
          "link": "http://arxiv.org/abs/2006.03659",
          "publishedOn": "2021-05-24T05:08:39.454Z",
          "wordCount": 649,
          "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. (arXiv:2006.03659v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. A. Gloor</a>",
          "description": "This paper investigates the research question if senders of large amounts of\nirrelevant or unsolicited information - commonly called \"spammers\" - distort\nthe network structure of social networks. Two large social networks are\nanalyzed, the first extracted from the Twitter discourse about a big\ntelecommunication company, and the second obtained from three years of email\ncommunication of 200 managers working for a large multinational company. This\nwork compares network robustness and the stability of centrality and\ninteraction metrics, as well as the use of language, after removing spammers\nand the most and least connected nodes. The results show that spammers do not\nsignificantly alter the structure of the information-carrying network, for most\nof the social indicators. The authors additionally investigate the correlation\nbetween e-mail subject line and content by tracking language sentiment,\nemotionality, and complexity, addressing the cases where collecting email\nbodies is not permitted for privacy reasons. The findings extend the research\nabout robustness and stability of social networks metrics, after the\napplication of graph simplification strategies. The results have practical\nimplication for network analysts and for those company managers who rely on\nnetwork analytics (applied to company emails and social media data) to support\ntheir decision-making processes.",
          "link": "http://arxiv.org/abs/2105.10256",
          "publishedOn": "2021-05-24T05:08:39.442Z",
          "wordCount": 656,
          "title": "Measuring the impact of spammers on e-mail and Twitter networks. (arXiv:2105.10256v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chenhao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengsong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>",
          "description": "Distantly supervision automatically generates plenty of training samples for\nrelation extraction. However, it also incurs two major problems: noisy labels\nand imbalanced training data. Previous works focus more on reducing wrongly\nlabeled relations (false positives) while few explore the missing relations\nthat are caused by incompleteness of knowledge base (false negatives).\nFurthermore, the quantity of negative labels overwhelmingly surpasses the\npositive ones in previous problem formulations. In this paper, we first provide\na thorough analysis of the above challenges caused by negative data. Next, we\nformulate the problem of relation extraction into as a positive unlabeled\nlearning task to alleviate false negative problem. Thirdly, we propose a\npipeline approach, dubbed \\textsc{ReRe}, that performs sentence-level relation\ndetection then subject/object extraction to achieve sample-efficient training.\nExperimental results show that the proposed method consistently outperforms\nexisting approaches and remains excellent performance even learned with a large\nquantity of false positive samples.",
          "link": "http://arxiv.org/abs/2105.10158",
          "publishedOn": "2021-05-24T05:08:39.422Z",
          "wordCount": 583,
          "title": "Revisiting the Negative Data of Distantly Supervised Relation Extraction. (arXiv:2105.10158v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>",
          "description": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP",
          "link": "http://arxiv.org/abs/2105.03075",
          "publishedOn": "2021-05-24T05:08:39.407Z",
          "wordCount": 634,
          "title": "A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Divyansh Singh</a>",
          "description": "In recent times, we have seen an increased use of text chat for communication\non social networks and smartphones. This particularly involves the use of\nHindi-English code-mixed text which contains words which are not recognized in\nEnglish vocabulary. We have worked on detecting emotions in these mixed data\nand classify the sentences in human emotions which are angry, fear, happy or\nsad. We have used state of the art natural language processing models and\ncompared their performance on the dataset comprising sentences in this mixed\ndata. The dataset was collected and annotated from sources and then used to\ntrain the models.",
          "link": "http://arxiv.org/abs/2105.09226",
          "publishedOn": "2021-05-24T05:08:39.399Z",
          "wordCount": 538,
          "title": "Detection of Emotions in Hindi-English Code Mixed Text Data. (arXiv:2105.09226v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyusong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>",
          "description": "Although open-domain question answering (QA) draws great attention in recent\nyears, it requires large amounts of resources for building the full system and\nis often difficult to reproduce previous results due to complex configurations.\nIn this paper, we introduce SF-QA: simple and fair evaluation framework for\nopen-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,\nwhich makes the task itself easily accessible and reproducible to research\ngroups without enough computing resources. The proposed evaluation framework is\npublicly available and anyone can contribute to the code and evaluations.",
          "link": "http://arxiv.org/abs/2101.01910",
          "publishedOn": "2021-05-24T05:08:39.389Z",
          "wordCount": 555,
          "title": "SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering. (arXiv:2101.01910v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiping Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>",
          "description": "Personalized conversation models (PCMs) generate responses according to\nspeaker preferences. Existing personalized conversation tasks typically require\nmodels to extract speaker preferences from user descriptions or their\nconversation histories, which are scarce for newcomers and inactive users. In\nthis paper, we propose a few-shot personalized conversation task with an\nauxiliary social network. The task requires models to generate personalized\nresponses for a speaker given a few conversations from the speaker and a social\nnetwork. Existing methods are mainly designed to incorporate descriptions or\nconversation histories. Those methods can hardly model speakers with so few\nconversations or connections between speakers. To better cater for newcomers\nwith few resources, we propose a personalized conversation model (PCM) that\nlearns to adapt to new speakers as well as enabling new speakers to learn from\nresource-rich speakers. Particularly, based on a meta-learning based PCM, we\npropose a task aggregator (TA) to collect other speakers' information from the\nsocial network. The TA provides prior knowledge of the new speaker in its\nmeta-learning. Experimental results show our methods outperform all baselines\nin appropriateness, diversity, and consistency with speakers.",
          "link": "http://arxiv.org/abs/2105.10323",
          "publishedOn": "2021-05-24T05:08:39.325Z",
          "wordCount": 621,
          "title": "Learning from My Friends: Few-Shot Personalized Conversation Systems via Social Networks. (arXiv:2105.10323v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>",
          "description": "Persona can function as the prior knowledge for maintaining the consistency\nof dialogue systems. Most of previous studies adopted the self persona in\ndialogue whose response was about to be selected from a set of candidates or\ndirectly generated, but few have noticed the role of partner in dialogue. This\npaper makes an attempt to thoroughly explore the impact of utilizing personas\nthat describe either self or partner speakers on the task of response selection\nin retrieval-based chatbots. Four persona fusion strategies are designed, which\nassume personas interact with contexts or responses in different ways. These\nstrategies are implemented into three representative models for response\nselection, which are based on the Hierarchical Recurrent Encoder (HRE),\nInteractive Matching Network (IMN) and Bidirectional Encoder Representations\nfrom Transformers (BERT) respectively. Empirical studies on the Persona-Chat\ndataset show that the partner personas neglected in previous studies can\nimprove the accuracy of response selection in the IMN- and BERT-based models.\nBesides, our BERT-based model implemented with the context-response-aware\npersona fusion strategy outperforms previous methods by margins larger than\n2.7% on original personas and 4.6% on revised personas in terms of hits@1\n(top-1 accuracy), achieving a new state-of-the-art performance on the\nPersona-Chat dataset.",
          "link": "http://arxiv.org/abs/2105.09050",
          "publishedOn": "2021-05-24T05:08:39.305Z",
          "wordCount": 663,
          "title": "Partner Matters! An Empirical Study on Fusing Personas for Personalized Response Selection in Retrieval-Based Chatbots. (arXiv:2105.09050v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ambroszkiewicz_S/0/1/0/all/0/1\">Stanislaw Ambroszkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartyna_W/0/1/0/all/0/1\">Waldemar Bartyna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bylka_S/0/1/0/all/0/1\">Stanislaw Bylka</a>",
          "description": "Cloud Native Application CNApp (as a distributed system) is a collection of\nindependent components (micro-services) interacting via communication\nprotocols. This gives rise to present an abstract architecture of CNApp as\ndynamically re-configurable acyclic directed multi graph where vertices are\nmicroservices, and edges are the protocols. Generic mechanisms for such\nreconfigurations evidently correspond to higher-level functions (functionals).\nThis implies also internal abstract architecture of microservice as a\ncollection of event-triggered serverless functions (including functions\nimplementing the protocols) that are dynamically composed into event-dependent\ndata-flow graphs. Again, generic mechanisms for such compositions correspond to\ncalculus of functionals and relations.",
          "link": "http://arxiv.org/abs/2105.10362",
          "publishedOn": "2021-05-24T05:08:39.287Z",
          "wordCount": 543,
          "title": "Functionals in the Clouds: An abstract architecture of serverless Cloud-Native Apps. (arXiv:2105.10362v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Text generation has become one of the most important yet challenging tasks in\nnatural language processing (NLP). The resurgence of deep learning has greatly\nadvanced this field by neural generation models, especially the paradigm of\npretrained language models (PLMs). In this paper, we present an overview of the\nmajor advances achieved in the topic of PLMs for text generation. As the\npreliminaries, we present the general task definition and briefly describe the\nmainstream architectures of PLMs for text generation. As the core content, we\ndiscuss how to adapt existing PLMs to model different input data and satisfy\nspecial properties in the generated text. We further summarize several\nimportant fine-tuning strategies for text generation. Finally, we present\nseveral future directions and conclude this paper. Our survey aims to provide\ntext generation researchers a synthesis and pointer to related research.",
          "link": "http://arxiv.org/abs/2105.10311",
          "publishedOn": "2021-05-24T05:08:39.249Z",
          "wordCount": 577,
          "title": "Pretrained Language Models for Text Generation: A Survey. (arXiv:2105.10311v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tufano_M/0/1/0/all/0/1\">Michele Tufano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shao Kun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>",
          "description": "Automated unit test case generation tools facilitate test-driven development\nand support developers by suggesting tests intended to identify flaws in their\ncode. Existing approaches are usually guided by the test coverage criteria,\ngenerating synthetic test cases that are often difficult for developers to read\nor understand. In this paper we propose AthenaTest, an approach that aims to\ngenerate unit test cases by learning from real-world focal methods and\ndeveloper-written testcases. We formulate unit test case generation as a\nsequence-to-sequence learning task, adopting a two-step training procedure\nconsisting of denoising pretraining on a large unsupervised Java corpus, and\nsupervised finetuning for a downstream translation task of generating unit\ntests. We investigate the impact of natural language and source code\npretraining, as well as the focal context information surrounding the focal\nmethod. Both techniques provide improvements in terms of validation loss, with\npretraining yielding 25% relative improvement and focal context providing\nadditional 11.1% improvement. We also introduce Methods2Test, the largest\npublicly available supervised parallel corpus of unit test case methods and\ncorresponding focal methods in Java, which comprises 780K test cases mined from\n91K open-source repositories from GitHub. We evaluate AthenaTest on five\ndefects4j projects, generating 25K passing test cases covering 43.7% of the\nfocal methods with only 30 attempts. We execute the test cases, collect test\ncoverage information, and compare them with test cases generated by EvoSuite\nand GPT-3, finding that our approach outperforms GPT-3 and has comparable\ncoverage w.r.t. EvoSuite. Finally, we survey professional developers on their\npreference in terms of readability, understandability, and testing\neffectiveness of the generated tests, showing overwhelmingly preference towards\nAthenaTest.",
          "link": "http://arxiv.org/abs/2009.05617",
          "publishedOn": "2021-05-24T05:08:39.225Z",
          "wordCount": 731,
          "title": "Unit Test Case Generation with Transformers and Focal Context. (arXiv:2009.05617v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Logical reasoning, which is closely related to human cognition, is of vital\nimportance in human's understanding of texts. Recent years have witnessed\nincreasing attentions on machine's logical reasoning abilities. However,\nprevious studies commonly apply ad-hoc methods to model pre-defined relation\npatterns, such as linking named entities, which only considers global knowledge\ncomponents that are related to commonsense, without local perception of\ncomplete facts or events. Such methodology is obviously insufficient to deal\nwith complicated logical structures. Therefore, we argue that the natural logic\nunits would be the group of backbone constituents of the sentence such as the\nsubject-verb-object formed \"facts\", covering both global and local knowledge\npieces that are necessary as the basis for logical reasoning. Beyond building\nthe ad-hoc graphs, we propose a more general and convenient fact-driven\napproach to construct a supergraph on top of our newly defined fact units, and\nenhance the supergraph with further explicit guidance of local question and\noption interactions. Experiments on two challenging logical reasoning benchmark\ndatasets, ReClor and LogiQA, show that our proposed model, \\textsc{Focal\nReasoner}, outperforms the baseline models dramatically. It can also be\nsmoothly applied to other downstream tasks such as MuTual, a dialogue reasoning\ndataset, achieving competitive results.",
          "link": "http://arxiv.org/abs/2105.10334",
          "publishedOn": "2021-05-24T05:08:39.208Z",
          "wordCount": 617,
          "title": "Fact-driven Logical Reasoning. (arXiv:2105.10334v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Context-aware machine translation models are designed to leverage contextual\ninformation, but often fail to do so. As a result, they inaccurately\ndisambiguate pronouns and polysemous words that require context for resolution.\nIn this paper, we ask several questions: What contexts do human translators use\nto resolve ambiguous words? Are models paying large amounts of attention to the\nsame context? What if we explicitly train them to do so? To answer these\nquestions, we introduce SCAT (Supporting Context for Ambiguous Translations), a\nnew English-French dataset comprising supporting context words for 14K\ntranslations that professional translators found useful for pronoun\ndisambiguation. Using SCAT, we perform an in-depth analysis of the context used\nto disambiguate, examining positional and lexical characteristics of the\nsupporting words. Furthermore, we measure the degree of alignment between the\nmodel's attention scores and the supporting context from SCAT, and apply a\nguided attention strategy to encourage agreement between the two.",
          "link": "http://arxiv.org/abs/2105.06977",
          "publishedOn": "2021-05-24T05:08:39.198Z",
          "wordCount": 613,
          "title": "Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1\">Prahal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1\">Masoumeh Aminzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training.",
          "link": "http://arxiv.org/abs/2105.09996",
          "publishedOn": "2021-05-24T05:08:39.151Z",
          "wordCount": 565,
          "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1\">Andrew Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1\">Nga Than</a>",
          "description": "Topic models are widely used in studying social phenomena. We conduct a\ncomparative study examining state-of-the-art neural versus non-neural topic\nmodels, performing a rigorous quantitative and qualitative assessment on a\ndataset of tweets about the COVID-19 pandemic. Our results show that not only\ndo neural topic models outperform their classical counterparts on standard\nevaluation metrics, but they also produce more coherent topics, which are of\ngreat benefit when studying complex social problems. We also propose a novel\nregularization term for neural topic models, which is designed to address the\nwell-documented problem of mode collapse, and demonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2105.10165",
          "publishedOn": "2021-05-24T05:08:39.143Z",
          "wordCount": 605,
          "title": "Have you tried Neural Topic Models? Comparative Analysis of Neural and Non-Neural Topic Models with Application to COVID-19 Twitter Data. (arXiv:2105.10165v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1\">Atul Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1\">Anshul Nasery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "Recently, unsupervised parsing of syntactic trees has gained considerable\nattention. A prototypical approach to such unsupervised parsing employs\nreinforcement learning and auto-encoders. However, no mechanism ensures that\nthe learnt model leverages the well-understood language grammar. We propose an\napproach that utilizes very generic linguistic knowledge of the language\npresent in the form of syntactic rules, thus inducing better syntactic\nstructures. We introduce a novel formulation that takes advantage of the\nsyntactic grammar rules and is independent of the base system. We achieve new\nstate-of-the-art results on two benchmarks datasets, MNLI and WSJ. The source\ncode of the paper is available at https://github.com/anshuln/Diora_with_rules.",
          "link": "http://arxiv.org/abs/2105.10193",
          "publishedOn": "2021-05-24T05:08:39.000Z",
          "wordCount": 543,
          "title": "Rule Augmented Unsupervised Constituency Parsing. (arXiv:2105.10193v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Mao Nguyen Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Loi Duc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khiem Vinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "Machine reading comprehension (MRC) is a sub-field in natural language\nprocessing that aims to help computers understand unstructured texts and then\nanswer questions related to them. In practice, conversation is an essential way\nto communicate and transfer information. To help machines understand\nconversation texts, we present UIT-ViCoQA - a new corpus for conversational\nmachine reading comprehension in the Vietnamese language. This corpus consists\nof 10,000 questions with answers to over 2,000 conversations about health news\narticles. Then, we evaluate several baseline approaches for conversational\nmachine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1\nscore of 45.27%, which is 30.91 points behind human performance (76.18%),\nindicating that there is ample room for improvement.",
          "link": "http://arxiv.org/abs/2105.01542",
          "publishedOn": "2021-05-24T05:08:38.979Z",
          "wordCount": 586,
          "title": "Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1\">Adyasha Maharana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannan_D/0/1/0/all/0/1\">Darryl Hannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Story visualization is an under-explored task that falls at the intersection\nof many important research directions in both computer vision and natural\nlanguage processing. In this task, given a series of natural language captions\nwhich compose a story, an agent must generate a sequence of images that\ncorrespond to the captions. Prior work has introduced recurrent generative\nmodels which outperform text-to-image synthesis models on this task. However,\nthere is room for improvement of generated images in terms of visual quality,\ncoherence and relevance. We present a number of improvements to prior modeling\napproaches, including (1) the addition of a dual learning framework that\nutilizes video captioning to reinforce the semantic alignment between the story\nand generated images, (2) a copy-transform mechanism for\nsequentially-consistent story visualization, and (3) MART-based transformers to\nmodel complex interactions between frames. We present ablation studies to\ndemonstrate the effect of each of these techniques on the generative power of\nthe model for both individual images as well as the entire narrative.\nFurthermore, due to the complexity and generative nature of the task, standard\nevaluation metrics do not accurately reflect performance. Therefore, we also\nprovide an exploration of evaluation metrics for the model, focused on aspects\nof the generated frames such as the presence/quality of generated characters,\nthe relevance to captions, and the diversity of the generated images. We also\npresent correlation experiments of our proposed automated metrics with human\nevaluations. Code and data available at:\nhttps://github.com/adymaharana/StoryViz",
          "link": "http://arxiv.org/abs/2105.10026",
          "publishedOn": "2021-05-24T05:08:38.915Z",
          "wordCount": 686,
          "title": "Improving Generation and Evaluation of Visual Stories via Semantic Consistency. (arXiv:2105.10026v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kohli_H/0/1/0/all/0/1\">Harsh Kohli</a>",
          "description": "Modern transformer-based neural architectures yield impressive results in\nnearly every NLP task and Word Sense Disambiguation, the problem of discerning\nthe correct sense of a word in a given context, is no exception.\nState-of-the-art approaches in WSD today leverage lexical information along\nwith pre-trained embeddings from these models to achieve results comparable to\nhuman inter-annotator agreement on standard evaluation benchmarks. In the same\nvein, we experiment with several strategies to optimize bi-encoders for this\nspecific task and propose alternative methods of presenting lexical information\nto our model. Through our multi-stage pre-training and fine-tuning pipeline we\nfurther the state of the art in Word Sense Disambiguation.",
          "link": "http://arxiv.org/abs/2105.10146",
          "publishedOn": "2021-05-24T05:08:38.868Z",
          "wordCount": 540,
          "title": "Training Bi-Encoders for Word Sense Disambiguation. (arXiv:2105.10146v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jennifer C. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>",
          "description": "Probes are models devised to investigate the encoding of knowledge -- e.g.\nsyntactic structure -- in contextual representations. Probes are often designed\nfor simplicity, which has led to restrictions on probe design that may not\nallow for the full exploitation of the structure of encoded information; one\nsuch restriction is linearity. We examine the case of a structural probe\n(Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic\nstructure in contextual representations through learning only linear\ntransformations. By observing that the structural probe learns a metric, we are\nable to kernelize it and develop a novel non-linear variant with an identical\nnumber of parameters. We test on 6 languages and find that the radial-basis\nfunction (RBF) kernel, in conjunction with regularization, achieves a\nstatistically significant improvement over the baseline in all languages --\nimplying that at least part of the syntactic knowledge is encoded non-linearly.\nWe conclude by discussing how the RBF kernel resembles BERT's self-attention\nlayers and speculate that this resemblance leads to the RBF-based probe's\nstronger performance.",
          "link": "http://arxiv.org/abs/2105.10185",
          "publishedOn": "2021-05-24T05:08:38.860Z",
          "wordCount": 600,
          "title": "A Non-Linear Structural Probe. (arXiv:2105.10185v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kawintiranon_K/0/1/0/all/0/1\">Kornraphop Kawintiranon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaguang Liu</a>",
          "description": "General Data Protection Regulation (GDPR) becomes a standard law for data\nprotection in many countries. Currently, twelve countries adopt the regulation\nand establish their GDPR-like regulation. However, to evaluate the differences\nand similarities of these GDPR-like regulations is time-consuming and needs a\nlot of manual effort from legal experts. Moreover, GDPR-like regulations from\ndifferent countries are written in their languages leading to a more difficult\ntask since legal experts who know both languages are essential. In this paper,\nwe investigate a simple natural language processing (NLP) approach to tackle\nthe problem. We first extract chunks of information from GDPR-like documents\nand form structured data from natural language. Next, we use NLP methods to\ncompare documents to measure their similarity. Finally, we manually label a\nsmall set of data to evaluate our approach. The empirical result shows that the\nBERT model with cosine similarity outperforms other baselines. Our data and\ncode are publicly available.",
          "link": "http://arxiv.org/abs/2105.10117",
          "publishedOn": "2021-05-24T05:08:38.850Z",
          "wordCount": 600,
          "title": "Towards Automatic Comparison of Data Privacy Documents: A Preliminary Experiment on GDPR-like Laws. (arXiv:2105.10117v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1\">Nihal Potdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1\">Anderson R. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yiran Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>",
          "description": "End-to-end spoken language understanding (SLU) has recently attracted\nincreasing interest. Compared to the conventional tandem-based approach that\ncombines speech recognition and language understanding as separate modules, the\nnew approach extracts users' intentions directly from the speech signals,\nresulting in joint optimization and low latency. Such an approach, however, is\ntypically designed to process one intention at a time, which leads users to\ntake multiple rounds to fulfill their requirements while interacting with a\ndialogue system. In this paper, we propose a streaming end-to-end framework\nthat can process multiple intentions in an online and incremental way. The\nbackbone of our framework is a unidirectional RNN trained with the\nconnectionist temporal classification (CTC) criterion. By this design, an\nintention can be identified when sufficient evidence has been accumulated, and\nmultiple intentions can be identified sequentially. We evaluate our solution on\nthe Fluent Speech Commands (FSC) dataset and the intent detection accuracy is\nabout 97 % on all multi-intent settings. This result is comparable to the\nperformance of the state-of-the-art non-streaming models, but is achieved in an\nonline and incremental way. We also employ our model to a keyword spotting task\nusing the Google Speech Commands dataset and the results are also highly\npromising.",
          "link": "http://arxiv.org/abs/2105.10042",
          "publishedOn": "2021-05-24T05:08:38.840Z",
          "wordCount": 647,
          "title": "A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_B/0/1/0/all/0/1\">Boaz Shmueli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Soumya Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>",
          "description": "Datasets with induced emotion labels are scarce but of utmost importance for\nmany NLP tasks. We present a new, automated method for collecting texts along\nwith their induced reaction labels. The method exploits the online use of\nreaction GIFs, which capture complex affective states. We show how to augment\nthe data with induced emotion and induced sentiment labels. We use our method\nto create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K\ntweets. We provide baselines for three new tasks, including induced sentiment\nprediction and multilabel classification of induced emotions. Our method and\ndataset open new research opportunities in emotion detection and affective\ncomputing.",
          "link": "http://arxiv.org/abs/2105.09967",
          "publishedOn": "2021-05-24T05:08:38.828Z",
          "wordCount": 563,
          "title": "Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter. (arXiv:2105.09967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gidiotis_A/0/1/0/all/0/1\">Alexios Gidiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>",
          "description": "We propose a novel approach to summarization based on Bayesian deep learning.\nWe approximate Bayesian summary generation by first extending state-of-the-art\nsummarization models with Monte Carlo dropout and then using them to perform\nmultiple stochastic forward passes. This method allows us to improve\nsummarization performance by simply using the median of multiple stochastic\nsummaries. We show that our variational equivalents of BART and PEGASUS can\noutperform their deterministic counterparts on multiple benchmark datasets. In\naddition, we rely on Bayesian inference to measure the uncertainty of the model\nwhen generating summaries. Having a reliable uncertainty measure, we can\nimprove the experience of the end user by filtering out generated summaries of\nhigh uncertainty. Furthermore, our proposed metric could be used as a criterion\nfor selecting samples for annotation, and can be paired nicely with active\nlearning and human-in-the-loop approaches.",
          "link": "http://arxiv.org/abs/2105.10155",
          "publishedOn": "2021-05-24T05:08:38.791Z",
          "wordCount": 549,
          "title": "Uncertainty-Aware Abstractive Summarization. (arXiv:2105.10155v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huijun Liu</a>",
          "description": "Span-based joint extraction simultaneously conducts named entity recognition\n(NER) and relation extraction (RE) in text span form. Recent studies have shown\nthat token labels can convey crucial task-specific information and enrich token\nsemantics. However, as far as we know, due to completely abstain from sequence\ntagging mechanism, all prior span-based work fails to use token label\nin-formation. To solve this problem, we pro-pose Sequence Tagging enhanced\nSpan-based Network (STSN), a span-based joint extrac-tion network that is\nenhanced by token BIO label information derived from sequence tag-ging based\nNER. By stacking multiple atten-tion layers in depth, we design a deep neu-ral\narchitecture to build STSN, and each atten-tion layer consists of three basic\nattention units. The deep neural architecture first learns seman-tic\nrepresentations for token labels and span-based joint extraction, and then\nconstructs in-formation interactions between them, which also realizes\nbidirectional information interac-tions between span-based NER and RE.\nFur-thermore, we extend the BIO tagging scheme to make STSN can extract\noverlapping en-tity. Experiments on three benchmark datasets show that our\nmodel consistently outperforms previous optimal models by a large margin,\ncreating new state-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.10080",
          "publishedOn": "2021-05-24T05:08:38.783Z",
          "wordCount": 622,
          "title": "Boosting Span-based Joint Entity and Relation Extraction via Squence Tagging Mechanism. (arXiv:2105.10080v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bedi_M/0/1/0/all/0/1\">Manjot Bedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shivani Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Sarcasm detection and humor classification are inherently subtle problems,\nprimarily due to their dependence on the contextual and non-verbal information.\nFurthermore, existing studies in these two topics are usually constrained in\nnon-English languages such as Hindi, due to the unavailability of qualitative\nannotated datasets. In this work, we make two major contributions considering\nthe above limitations: (1) we develop a Hindi-English code-mixed dataset,\nMaSaC, for the multi-modal sarcasm detection and humor classification in\nconversational dialog, which to our knowledge is the first dataset of its kind;\n(2) we propose MSH-COMICS, a novel attention-rich neural architecture for the\nutterance classification. We learn efficient utterance representation utilizing\na hierarchical attention mechanism that attends to a small portion of the input\nsentence at a time. Further, we incorporate dialog-level contextual attention\nmechanism to leverage the dialog history for the multi-modal classification. We\nperform extensive experiments for both the tasks by varying multi-modal inputs\nand various submodules of MSH-COMICS. We also conduct comparative analysis\nagainst existing approaches. We observe that MSH-COMICS attains superior\nperformance over the existing models by > 1 F1-score point for the sarcasm\ndetection and 10 F1-score points in humor classification. We diagnose our model\nand perform thorough analysis of the results to understand the superiority and\npitfalls.",
          "link": "http://arxiv.org/abs/2105.09984",
          "publishedOn": "2021-05-24T05:08:38.766Z",
          "wordCount": 643,
          "title": "Multi-modal Sarcasm Detection and Humor Classification in Code-mixed Conversations. (arXiv:2105.09984v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rakshit_G/0/1/0/all/0/1\">Geetanjali Rakshit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1\">Jeffrey Flanigan</a>",
          "description": "In this work, we introduce ASQ, a tool to automatically mine questions and\nanswers from a sentence, using its Abstract Meaning Representation (AMR).\nPrevious work has made a case for using question-answer pairs to specify\npredicate-argument structure of a sentence using natural language, which does\nnot require linguistic expertise or training. This has resulted in the creation\nof datasets such as QA-SRL and QAMR, for both of which, the question-answer\npair annotations were crowdsourced. Our approach has the same end-goal, but is\nautomatic, making it faster and cost-effective, without compromising on the\nquality and validity of the question-answer pairs thus obtained. A qualitative\nevaluation of the output generated by ASQ from the AMR 2.0 data shows that the\nquestion-answer pairs are natural and valid, and demonstrate good coverage of\nthe content. We run ASQ on the sentences from the QAMR dataset, to observe that\nthe semantic roles in QAMR are also captured by ASQ.We intend to make this tool\nand the results publicly available for others to use and build upon.",
          "link": "http://arxiv.org/abs/2105.10023",
          "publishedOn": "2021-05-24T05:08:38.727Z",
          "wordCount": 590,
          "title": "ASQ: Automatically Generating Question-Answer Pairs using AMRs. (arXiv:2105.10023v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiyan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenming Xiao</a>",
          "description": "Lexicon information and pre-trained models, such as BERT, have been combined\nto explore Chinese sequence labelling tasks due to their respective strengths.\nHowever, existing methods solely fuse lexicon features via a shallow and random\ninitialized sequence layer and do not integrate them into the bottom layers of\nBERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese\nsequence labelling, which integrates external lexicon knowledge into BERT\nlayers directly by a Lexicon Adapter layer. Compared with the existing methods,\nour model facilitates deep lexicon knowledge fusion at the lower layers of\nBERT. Experiments on ten Chinese datasets of three tasks including Named Entity\nRecognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT\nachieves the state-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.07148",
          "publishedOn": "2021-05-23T06:08:17.061Z",
          "wordCount": 566,
          "title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter. (arXiv:2105.07148v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>",
          "description": "Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of available\nEnglish datasets by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in low-resource languages. We project\npredictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,\nSpanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in\nTRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in\nOffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513\nF1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our\napproach compares favourably to the best systems submitted to recent shared\ntasks on these three languages. Additionally, we report competitive performance\non Arabic, and Turkish using the training and development sets of OffensEval\n2020 shared task. The results for all languages confirm the robustness of\ncross-lingual contextual embeddings and transfer learning for this task.",
          "link": "http://arxiv.org/abs/2105.05996",
          "publishedOn": "2021-05-23T06:08:17.026Z",
          "wordCount": 695,
          "title": "Multilingual Offensive Language Identification for Low-resource Languages. (arXiv:2105.05996v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1\">Grusha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>",
          "description": "When language models process syntactically complex sentences, do they use\nabstract syntactic information present in these sentences in a manner that is\nconsistent with the grammar of English, or do they rely solely on a set of\nheuristics? We propose a method to tackle this question, AlterRep. For any\nlinguistic feature in the sentence, AlterRep allows us to generate\ncounterfactual representations by altering how this feature is encoded, while\nleaving all other aspects of the original representation intact. Then, by\nmeasuring the change in a models' word prediction with these counterfactual\nrepresentations in different sentences, we can draw causal conclusions about\nthe contexts in which the model uses the linguistic feature (if any). Applying\nthis method to study how BERT uses relative clause (RC) span information, we\nfound that BERT uses information about RC spans during agreement prediction\nusing the linguistically correct strategy. We also found that counterfactual\nrepresentations generated for a specific RC subtype influenced the number\nprediction in sentences with other RC subtypes, suggesting that information\nabout RC boundaries was encoded abstractly in BERT's representation.",
          "link": "http://arxiv.org/abs/2105.06965",
          "publishedOn": "2021-05-23T06:08:16.967Z",
          "wordCount": 645,
          "title": "Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. (arXiv:2105.06965v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>",
          "description": "This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge\nTransfer), a novel method for the automatic transfer of explanatory knowledge\nthrough neural encoding mechanisms. We demonstrate that N-XKT is able to\nimprove accuracy and generalization on science Question Answering (QA).\nSpecifically, by leveraging facts from background explanatory knowledge\ncorpora, the N-XKT model shows a clear improvement on zero-shot QA.\nFurthermore, we show that N-XKT can be fine-tuned on a target QA dataset,\nenabling faster convergence and more accurate results. A systematic analysis is\nconducted to quantitatively analyze the performance of the N-XKT model and the\nimpact of different categories of knowledge on the zero-shot generalization\ntask.",
          "link": "http://arxiv.org/abs/2105.05737",
          "publishedOn": "2021-05-23T06:08:16.959Z",
          "wordCount": 554,
          "title": "Encoding Explanatory Knowledge for Zero-shot Science Question Answering. (arXiv:2105.05737v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Ed Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the accuracy of popular NAR models adopted in neural machine\ntranslation by a large margin.",
          "link": "http://arxiv.org/abs/2105.03842",
          "publishedOn": "2021-05-23T06:08:16.945Z",
          "wordCount": 751,
          "title": "FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_R/0/1/0/all/0/1\">Rihao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>",
          "description": "Learning prerequisite chains is an essential task for efficiently acquiring\nknowledge in both known and unknown domains. For example, one may be an expert\nin the natural language processing (NLP) domain but want to determine the best\norder to learn new concepts in an unfamiliar Computer Vision domain (CV). Both\ndomains share some common concepts, such as machine learning basics and deep\nlearning models. In this paper, we propose unsupervised cross-domain concept\nprerequisite chain learning using an optimized variational graph autoencoder.\nOur model learns to transfer concept prerequisite relations from an\ninformation-rich domain (source domain) to an information-poor domain (target\ndomain), substantially surpassing other baseline models. Also, we expand an\nexisting dataset by introducing two new domains: CV and Bioinformatics (BIO).\nThe annotated data and resources, as well as the code, will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2105.03505",
          "publishedOn": "2021-05-23T06:08:16.936Z",
          "wordCount": 588,
          "title": "Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders. (arXiv:2105.03505v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_J/0/1/0/all/0/1\">Jun Araki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Haibo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Recent works have shown that language models (LM) capture different types of\nknowledge regarding facts or common sense. However, because no model is\nperfect, they still fail to provide appropriate answers in many cases. In this\npaper, we ask the question \"how can we know when language models know, with\nconfidence, the answer to a particular query?\" We examine this question from\nthe point of view of calibration, the property of a probabilistic model's\npredicted probabilities actually being well correlated with the probabilities\nof correctness. We examine three strong generative models -- T5, BART, and\nGPT-2 -- and study whether their probabilities on QA tasks are well calibrated,\nfinding the answer is a relatively emphatic no. We then examine methods to\ncalibrate such models to make their confidence scores correlate better with the\nlikelihood of correctness through fine-tuning, post-hoc probability\nmodification, or adjustment of the predicted outputs or inputs. Experiments on\na diverse range of datasets demonstrate the effectiveness of our methods. We\nalso perform analysis to study the strengths and limitations of these methods,\nshedding light on further improvements that may be made in methods for\ncalibrating LMs. We have released the code at\nhttps://github.com/jzbjyb/lm-calibration.",
          "link": "http://arxiv.org/abs/2012.00955",
          "publishedOn": "2021-05-23T06:08:16.916Z",
          "wordCount": 672,
          "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering. (arXiv:2012.00955v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Can Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guocheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Existed pre-training methods either focus on single-modal tasks or\nmulti-modal tasks, and cannot effectively adapt to each other. They can only\nutilize single-modal data (i.e. text or image) or limited multi-modal data\n(i.e. image-text pairs). In this work, we propose a unified-modal pre-training\narchitecture, namely UNIMO, which can effectively adapt to both single-modal\nand multi-modal understanding and generation tasks. Large scale of free text\ncorpus and image collections can be utilized to improve the capability of\nvisual and textual understanding, and cross-modal contrastive learning (CMCL)\nis leveraged to align the textual and visual information into a unified\nsemantic space over a corpus of image-text pairs. As the non-paired\nsingle-modal data is very rich, our model can utilize much larger scale of data\nto learn more generalizable representations. Moreover, the textual knowledge\nand visual knowledge can enhance each other in the unified semantic space. The\nexperimental results show that UNIMO significantly improves the performance of\nseveral single-modal and multi-modal downstream tasks. Our code and pre-trained\nmodels are public at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO",
          "link": "http://arxiv.org/abs/2012.15409",
          "publishedOn": "2021-05-23T06:08:16.907Z",
          "wordCount": 665,
          "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. (arXiv:2012.15409v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1\">Dheeraj Pailla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Himanshu Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1\">Aadilmehdi Sanchawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>",
          "description": "The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.",
          "link": "http://arxiv.org/abs/2104.05947",
          "publishedOn": "2021-05-23T06:08:16.899Z",
          "wordCount": 668,
          "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1\">Rishi Hazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_S/0/1/0/all/0/1\">Sonu Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1\">Sayambhu Sen</a>",
          "description": "Human language has been described as a system that makes \\textit{use of\nfinite means to express an unlimited array of thoughts}. Of particular interest\nis the aspect of compositionality, whereby, the meaning of a compound language\nexpression can be deduced from the meaning of its constituent parts. If\nartificial agents can develop compositional communication protocols akin to\nhuman language, they can be made to seamlessly generalize to unseen\ncombinations. However, the real question is, how do we induce compositionality\nin emergent communication? Studies have recognized the role of curiosity in\nenabling linguistic development in children. It is this same intrinsic urge\nthat drives us to master complex tasks with decreasing amounts of explicit\nreward. In this paper, we seek to use this intrinsic feedback in inducing a\nsystematic and unambiguous protolanguage in artificial agents. We show how\nthese rewards can be leveraged in training agents to induce compositionality in\nabsence of any external feedback. Additionally, we introduce gComm, an\nenvironment for investigating grounded language acquisition in 2D-grid\nenvironments. Using this, we demonstrate how compositionality can enable agents\nto not only interact with unseen objects but also transfer skills from one task\nto another in a zero-shot setting: \\textit{Can an agent, trained to `pull' and\n`push twice', `pull twice'?}.",
          "link": "http://arxiv.org/abs/2012.05011",
          "publishedOn": "2021-05-23T06:08:16.830Z",
          "wordCount": 674,
          "title": "Infinite use of finite means: Zero-Shot Generalization using Compositional Emergent Protocols. (arXiv:2012.05011v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resabal_J/0/1/0/all/0/1\">Jose Kristian Resabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">James Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1\">Dan John Velasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Transformers represent the state-of-the-art in Natural Language Processing\n(NLP) in recent years, proving effective even in tasks done in low-resource\nlanguages. While pretrained transformers for these languages can be made, it is\nchallenging to measure their true performance and capacity due to the lack of\nhard benchmark datasets, as well as the difficulty and cost of producing them.\nIn this paper, we present three contributions: First, we propose a methodology\nfor automatically producing Natural Language Inference (NLI) benchmark datasets\nfor low-resource languages using published news articles. Through this, we\ncreate and release NewsPH-NLI, the first sentence entailment benchmark dataset\nin the low-resource Filipino language. Second, we produce new pretrained\ntransformers based on the ELECTRA technique to further alleviate the resource\nscarcity in Filipino, benchmarking them on our dataset against other\ncommonly-used transfer learning techniques. Lastly, we perform analyses on\ntransfer learning techniques to shed light on their true performance when\noperating in low-data domains through the use of degradation tests.",
          "link": "http://arxiv.org/abs/2010.11574",
          "publishedOn": "2021-05-23T06:08:16.822Z",
          "wordCount": 649,
          "title": "Exploiting News Article Structure for Automatic Corpus Generation. (arXiv:2010.11574v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Akul Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1\">Ethan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puranik_S/0/1/0/all/0/1\">Samir Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Horace He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "While programming is one of the most broadly applicable skills in modern\nsociety, modern machine learning models still cannot code solutions to basic\nproblems. It can be difficult to accurately assess code generation performance,\nand there has been surprisingly little work on evaluating code generation in a\nway that is both flexible and rigorous. To meet this challenge, we introduce\nAPPS, a benchmark for code generation. Unlike prior work in more restricted\nsettings, our benchmark measures the ability of models to take an arbitrary\nnatural language specification and generate Python code fulfilling this\nspecification. Similar to how companies assess candidate software developers,\nwe then evaluate models by checking their generated code on test cases. Our\nbenchmark includes 10,000 problems, which range from having simple one-line\nsolutions to being substantial algorithmic challenges. We fine-tune large\nlanguage models on both GitHub and our training set, and we find that the\nprevalence of syntax errors is decreasing exponentially. Recent models such as\nGPT-Neo can pass approximately 15% of the test cases of introductory problems,\nso we find that machine learning models are beginning to learn how to code. As\nthe social significance of automatic code generation increases over the coming\nyears, our benchmark can provide an important measure for tracking\nadvancements.",
          "link": "http://arxiv.org/abs/2105.09938",
          "publishedOn": "2021-05-23T06:08:16.792Z",
          "wordCount": 662,
          "title": "Measuring Coding Challenge Competence With APPS. (arXiv:2105.09938v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2006.16362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cordonnier_J/0/1/0/all/0/1\">Jean-Baptiste Cordonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1\">Andreas Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Attention layers are widely used in natural language processing (NLP) and are\nbeginning to influence computer vision architectures. Training very large\ntransformer models allowed significant improvement in both fields, but once\ntrained, these networks show symptoms of over-parameterization. For instance,\nit is known that many attention heads can be pruned without impacting accuracy.\nThis work aims to enhance current understanding on how multiple heads interact.\nMotivated by the observation that attention heads learn redundant key/query\nprojections, we propose a collaborative multi-head attention layer that enables\nheads to learn shared projections. Our scheme decreases the number of\nparameters in an attention layer and can be used as a drop-in replacement in\nany transformer architecture. Our experiments confirm that sharing key/query\ndimensions can be exploited in language understanding, machine translation and\nvision. We also show that it is possible to re-parametrize a pre-trained\nmulti-head attention layer into our collaborative attention layer.\nCollaborative multi-head attention reduces the size of the key and query\nprojections by 4 for same accuracy and speed. Our code is public.",
          "link": "http://arxiv.org/abs/2006.16362",
          "publishedOn": "2021-05-23T06:08:16.782Z",
          "wordCount": 628,
          "title": "Multi-Head Attention: Collaborate Instead of Concatenate. (arXiv:2006.16362v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1\">George Michalopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaka_H/0/1/0/all/0/1\">Hussam Kaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Helen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have\nachieved state-of-the-art results in biomedical natural language processing\ntasks by focusing their pre-training process on domain-specific corpora.\nHowever, such models do not take into consideration expert domain knowledge.\n\nIn this work, we introduced UmlsBERT, a contextual embedding model that\nintegrates domain knowledge during the pre-training process via a novel\nknowledge augmentation strategy. More specifically, the augmentation on\nUmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was\nperformed in two ways: i) connecting words that have the same underlying\n`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to\ncreate clinically meaningful input embeddings. By applying these two\nstrategies, UmlsBERT can encode clinical domain knowledge into word embeddings\nand outperform existing domain-specific models on common named-entity\nrecognition (NER) and clinical natural language inference clinical NLP tasks.",
          "link": "http://arxiv.org/abs/2010.10391",
          "publishedOn": "2021-05-23T06:08:16.766Z",
          "wordCount": 642,
          "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus. (arXiv:2010.10391v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>",
          "description": "NLP research has attained high performances in abusive language detection as\na supervised classification task. While in research settings, training and test\ndatasets are usually obtained from similar data samples, in practice systems\nare often applied on data that are different from the training set in topic and\nclass distributions. Also, the ambiguity in class definitions inherited in this\ntask aggravates the discrepancies between source and target datasets. We\nexplore the topic bias and the task formulation bias in cross-dataset\ngeneralization. We show that the benign examples in the Wikipedia Detox dataset\nare biased towards platform-specific topics. We identify these examples using\nunsupervised topic modeling and manual inspection of topics' keywords. Removing\nthese topics increases cross-dataset generalization, without reducing in-domain\nclassification performance. For a robust dataset design, we suggest applying\ninexpensive unsupervised methods to inspect the collected data and downsize the\nnon-generalizable content before manually annotating for class labels.",
          "link": "http://arxiv.org/abs/2010.07414",
          "publishedOn": "2021-05-23T06:08:16.758Z",
          "wordCount": 628,
          "title": "On Cross-Dataset Generalization in Automatic Detection of Online Abuse. (arXiv:2010.07414v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1\">Hansi Hettiarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>",
          "description": "This paper presents the team BRUMS submission to SemEval-2020 Task 3: Graded\nWord Similarity in Context. The system utilises state-of-the-art contextualised\nword embeddings, which have some task-specific adaptations, including stacked\nembeddings and average embeddings. Overall, the approach achieves good\nevaluation scores across all the languages, while maintaining simplicity.\nFollowing the final rankings, our approach is ranked within the top 5 solutions\nof each language while preserving the 1st position of Finnish subtask 2.",
          "link": "http://arxiv.org/abs/2010.06269",
          "publishedOn": "2021-05-23T06:08:16.745Z",
          "wordCount": 556,
          "title": "BRUMS at SemEval-2020 Task 3: Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity. (arXiv:2010.06269v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1\">Sukhdeep S. Sodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1\">Ellie Ka-In Chio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1\">Ambarish Jash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apte_A/0/1/0/all/0/1\">Ajit Apte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeje_A/0/1/0/all/0/1\">Ayooluwakunmi Jeje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1\">Dima Kuzmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_H/0/1/0/all/0/1\">Harry Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Effrat_J/0/1/0/all/0/1\">Jon Effrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_T/0/1/0/all/0/1\">Tarush Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1\">Nitin Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sarvjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Senqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tameen Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wankhede_A/0/1/0/all/0/1\">Amol Wankhede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1\">Moustafa Alzantot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Allen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1\">Tushar Chandra</a>",
          "description": "As more and more online search queries come from voice, automatic speech\nrecognition becomes a key component to deliver relevant search results. Errors\nintroduced by automatic speech recognition (ASR) lead to irrelevant search\nresults returned to the user, thus causing user dissatisfaction. In this paper,\nwe introduce an approach, Mondegreen, to correct voice queries in text space\nwithout depending on audio signals, which may not always be available due to\nsystem constraints or privacy or bandwidth (for example, some ASR systems run\non-device) considerations. We focus on voice queries transcribed via several\nproprietary commercial ASR systems. These queries come from users making\ninternet, or online service search queries. We first present an analysis\nshowing how different the language distribution coming from user voice queries\nis from that in traditional text corpora used to train off-the-shelf ASR\nsystems. We then demonstrate that Mondegreen can achieve significant\nimprovements in increased user interaction by correcting user voice queries in\none of the largest search systems in Google. Finally, we see Mondegreen as\ncomplementing existing highly-optimized production ASR systems, which may not\nbe frequently retrained and thus lag behind due to vocabulary drifts.",
          "link": "http://arxiv.org/abs/2105.09930",
          "publishedOn": "2021-05-23T06:08:16.721Z",
          "wordCount": 678,
          "title": "Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries. (arXiv:2105.09930v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2005.01107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1\">Luis Enrico Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1\">Diane Kathryn Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Question generation (QG) is a natural language generation task where a model\nis trained to ask questions corresponding to some input text. Most recent\napproaches frame QG as a sequence-to-sequence problem and rely on additional\nfeatures and mechanisms to increase performance; however, these often increase\nmodel complexity, and can rely on auxiliary data unavailable in practical use.\nA single Transformer-based unidirectional language model leveraging transfer\nlearning can be used to produce high quality questions while disposing of\nadditional task-specific complexity. Our QG model, finetuned from GPT-2 Small,\noutperforms several paragraph-level QG baselines on the SQuAD dataset by 0.95\nMETEOR points. Human evaluators rated questions as easy to answer, relevant to\ntheir context paragraph, and corresponding well to natural human speech. Also\nintroduced is a new set of baseline scores on the RACE dataset, which has not\npreviously been used for QG tasks. Further experimentation with varying model\ncapacities and datasets with non-identification type questions is recommended\nin order to further verify the robustness of pretrained Transformer-based LMs\nas question generators.",
          "link": "http://arxiv.org/abs/2005.01107",
          "publishedOn": "2021-05-23T06:08:16.708Z",
          "wordCount": 646,
          "title": "Simplifying Paragraph-level Question Generation via Transformer Language Models. (arXiv:2005.01107v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scontras_G/0/1/0/all/0/1\">Gregory Scontras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>",
          "description": "Recent advances in computational cognitive science (i.e., simulation-based\nprobabilistic programs) have paved the way for significant progress in formal,\nimplementable models of pragmatics. Rather than describing a pragmatic\nreasoning process in prose, these models formalize and implement one, deriving\nboth qualitative and quantitative predictions of human behavior -- predictions\nthat consistently prove correct, demonstrating the viability and value of the\nframework. The current paper provides a practical introduction to and critical\nassessment of the Bayesian Rational Speech Act modeling framework, unpacking\ntheoretical foundations, exploring technological innovations, and drawing\nconnections to issues beyond current applications.",
          "link": "http://arxiv.org/abs/2105.09867",
          "publishedOn": "2021-05-23T06:08:16.699Z",
          "wordCount": 522,
          "title": "A practical introduction to the Rational Speech Act modeling framework. (arXiv:2105.09867v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$~GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.",
          "link": "http://arxiv.org/abs/2105.09858",
          "publishedOn": "2021-05-23T06:08:16.671Z",
          "wordCount": 646,
          "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7~GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.",
          "link": "http://arxiv.org/abs/2105.09856",
          "publishedOn": "2021-05-23T06:08:16.662Z",
          "wordCount": 635,
          "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1\">Bhaskar Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1\">Nick Craswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "An emerging recipe for achieving state-of-the-art effectiveness in neural\ndocument re-ranking involves utilizing large pre-trained language models -\ne.g., BERT - to evaluate all individual passages in the document and then\naggregating the outputs by pooling or additional Transformer layers. A major\ndrawback of this approach is high query latency due to the cost of evaluating\nevery passage in the document with BERT. To make matters worse, this high\ninference cost and latency varies based on the length of the document, with\nlonger documents requiring more time and computation. To address this\nchallenge, we adopt an intra-document cascading strategy, which prunes passages\nof a candidate document using a less expensive model, called ESM, before\nrunning a scoring model that is more expensive and effective, called ETM. We\nfound it best to train ESM (short for Efficient Student Model) via knowledge\ndistillation from the ETM (short for Effective Teacher Model) e.g., BERT. This\npruning allows us to only run the ETM model on a smaller set of passages whose\nsize does not vary by document length. Our experiments on the MS MARCO and TREC\nDeep Learning Track benchmarks suggest that the proposed Intra-Document\nCascaded Ranking Model (IDCM) leads to over 400% lower query latency by\nproviding essentially the same effectiveness as the state-of-the-art BERT-based\ndocument ranking models.",
          "link": "http://arxiv.org/abs/2105.09816",
          "publishedOn": "2021-05-23T06:08:16.636Z",
          "wordCount": 659,
          "title": "Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Junru Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnow_K/0/1/0/all/0/1\">Kevin Parnow</a>",
          "description": "Constituent and dependency parsing, the two classic forms of syntactic\nparsing, have been found to benefit from joint training and decoding under a\nuniform formalism, Head-driven Phrase Structure Grammar (HPSG). However,\ndecoding this unified grammar has a higher time complexity ($O(n^5)$) than\ndecoding either form individually ($O(n^3)$) since more factors have to be\nconsidered during decoding. We thus propose an improved head scorer that helps\nachieve a novel performance-preserved parser in $O$($n^3$) time complexity.\nFurthermore, on the basis of this proposed practical HPSG parser, we\ninvestigated the strengths of HPSG-based parsing and explored the general\nmethod of training an HPSG-based parser from only a constituent or dependency\nannotations in a multilingual scenario. We thus present a more effective, more\nin-depth, and general work on HPSG parsing.",
          "link": "http://arxiv.org/abs/2105.09835",
          "publishedOn": "2021-05-23T06:08:16.627Z",
          "wordCount": 552,
          "title": "Head-driven Phrase Structure Parsing in O($n^3$) Time Complexity. (arXiv:2105.09835v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jihyung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jangwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chisung Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junseong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yongsook Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Taehwan Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joohong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Juhyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Sungwon Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Younghoon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Inkwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Sangwoo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Myeonghwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Seongbo Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_S/0/1/0/all/0/1\">Seungwon Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunkyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jamin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Lucy Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jungwoo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho Alice Oh Jungwoo Ha Kyunghyun Cho</a>",
          "description": "We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE\nis a collection of 8 Korean natural language understanding (NLU) tasks,\nincluding Topic Classification, Semantic Textual Similarity, Natural Language\nInference, Named Entity Recognition, Relation Extraction, Dependency Parsing,\nMachine Reading Comprehension, and Dialogue State Tracking. We build all of the\ntasks from scratch from diverse source corpora while respecting copyrights, to\nensure accessibility for anyone without any restrictions. With ethical\nconsiderations in mind, we carefully design annotation protocols. Along with\nthe benchmark tasks and data, we provide suitable evaluation metrics and\nfine-tuning recipes for pretrained language models for each task. We\nfurthermore release the pretrained language models (PLM), KLUE-BERT and\nKLUE-RoBERTa, to help reproduce baseline models on KLUE and thereby facilitate\nfuture research. We make a few interesting observations from the preliminary\nexperiments using the proposed KLUE benchmark suite, already demonstrating the\nusefulness of this new benchmark suite. First, we find KLUE-RoBERTa-large\noutperforms other baselines, including multilingual PLMs and existing\nopen-source Korean PLMs. Second, we see minimal degradation in performance even\nwhen we replace personally identifiable information from the pretraining\ncorpus, suggesting that privacy and NLU capability are not at odds with each\nother. Lastly, we find that using BPE tokenization in combination with\nmorpheme-level pre-tokenization is effective in tasks involving morpheme-level\ntagging, detection and generation. In addition to accelerating Korean NLP\nresearch, our comprehensive documentation on creating KLUE will facilitate\ncreating similar resources for other languages in the future. KLUE is available\nat this https URL (https://klue-benchmark.com/).",
          "link": "http://arxiv.org/abs/2105.09680",
          "publishedOn": "2021-05-23T06:08:16.503Z",
          "wordCount": 735,
          "title": "KLUE: Korean Language Understanding Evaluation. (arXiv:2105.09680v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1\">Keyue Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuzhuo Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Distantly supervised (DS) relation extraction (RE) has attracted much\nattention in the past few years as it can utilize large-scale auto-labeled\ndata. However, its evaluation has long been a problem: previous works either\ntook costly and inconsistent methods to manually examine a small sample of\nmodel predictions, or directly test models on auto-labeled data -- which, by\nour check, produce as much as 53% wrong labels at the entity pair level in the\npopular NYT10 dataset. This problem has not only led to inaccurate evaluation,\nbut also made it hard to understand where we are and what's left to improve in\nthe research of DS-RE. To evaluate DS-RE models in a more credible way, we\nbuild manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,\nand thoroughly evaluate several competitive models, especially the latest\npre-trained ones. The experimental results show that the manual evaluation can\nindicate very different conclusions from automatic ones, especially some\nunexpected observations, e.g., pre-trained models can achieve dominating\nperformance while being more susceptible to false-positives compared to\nprevious methods. We hope that both our manual test sets and novel observations\ncan help advance future DS-RE research.",
          "link": "http://arxiv.org/abs/2105.09543",
          "publishedOn": "2021-05-23T06:08:16.493Z",
          "wordCount": 643,
          "title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction. (arXiv:2105.09543v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Extensive research on target-dependent sentiment classification (TSC) has led\nto strong classification performances in domains where authors tend to\nexplicitly express sentiment about specific entities or topics, such as in\nreviews or on social media. We investigate TSC in news articles, a much less\nresearched domain, despite the importance of news as an essential information\nsource in individual and societal decision making. This article introduces\nNewsTSC, a manually annotated dataset to explore TSC on news articles.\nInvestigating characteristics of sentiment in news and contrasting them to\npopular TSC domains, we find that sentiment in the news is expressed less\nexplicitly, is more dependent on context and readership, and requires a greater\ndegree of interpretation. In an extensive evaluation, we find that the state of\nthe art in TSC performs worse on news articles than on other domains (average\nrecall AvgRec = 69.8 on NewsTSC compared to AvgRev = [75.6, 82.2] on\nestablished TSC datasets). Reasons include incorrectly resolved relation of\ntarget and sentiment-bearing phrases and off-context dependence. As a major\nimprovement over previous news TSC, we find that BERT's natural language\nunderstanding capabilities capture the less explicit sentiment used in news\narticles.",
          "link": "http://arxiv.org/abs/2105.09660",
          "publishedOn": "2021-05-23T06:08:16.482Z",
          "wordCount": 621,
          "title": "Towards Target-dependent Sentiment Classification in News Articles. (arXiv:2105.09660v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Profitlich_H/0/1/0/all/0/1\">Hans-J&#xfc;rgen Profitlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>",
          "description": "We describe our work on information extraction in medical documents written\nin German, especially detecting negations using an architecture based on the\nUIMA pipeline. Based on our previous work on software modules to cover medical\nconcepts like diagnoses, examinations, etc. we employ a version of the NegEx\nregular expression algorithm with a large set of triggers as a baseline. We\nshow how a significantly smaller trigger set is sufficient to achieve similar\nresults, in order to reduce adaptation times to new text types. We elaborate on\nthe question whether dependency parsing (based on the Stanford CoreNLP model)\nis a good alternative and describe the potentials and shortcomings of both\napproaches.",
          "link": "http://arxiv.org/abs/2105.09702",
          "publishedOn": "2021-05-23T06:08:16.455Z",
          "wordCount": 571,
          "title": "A Case Study on Pros and Cons of Regular Expression Detection and Dependency Parsing for Negation Extraction from German Medical Documents. Technical Report. (arXiv:2105.09702v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aashish Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zesch_T/0/1/0/all/0/1\">Torsten Zesch</a>",
          "description": "When evaluating the performance of automatic speech recognition models,\nusually word error rate within a certain dataset is used. Special care must be\ntaken in understanding the dataset in order to report realistic performance\nnumbers. We argue that many performance numbers reported probably underestimate\nthe expected error rate. We conduct experiments controlling for selection bias,\ngender as well as overlap (between training and test data) in content, voices,\nand recording conditions. We find that content overlap has the biggest impact,\nbut other factors like gender also play a role.",
          "link": "http://arxiv.org/abs/2105.09742",
          "publishedOn": "2021-05-23T06:08:16.444Z",
          "wordCount": 532,
          "title": "Robustness of end-to-end Automatic Speech Recognition Models -- A Case Study using Mozilla DeepSpeech. (arXiv:2105.09742v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giacomelli_G/0/1/0/all/0/1\">G. Giacomelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saran_T/0/1/0/all/0/1\">T. Saran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grippa_F/0/1/0/all/0/1\">F. Grippa</a>",
          "description": "We investigate the impact of a novel method called \"virtual mirroring\" to\npromote employee self-reflection and impact customer satisfaction. The method\nis based on measuring communication patterns, through social network and\nsemantic analysis, and mirroring them back to the individual. Our goal is to\ndemonstrate that self-reflection can trigger a change in communication\nbehaviors, which lead to increased customer satisfaction. We illustrate and\ntest our approach analyzing e-mails of a large global services company by\ncomparing changes in customer satisfaction associated with team leaders exposed\nto virtual mirroring (the experimental group). We find an increase in customer\nsatisfaction in the experimental group and a decrease in the control group\n(team leaders not involved in the virtual mirroring process). With regard to\nthe individual communication indicators, we find that customer satisfaction is\nhigher when employees are more responsive, use a simpler language, are embedded\nin less centralized communication networks, and show more stable leadership\npatterns.",
          "link": "http://arxiv.org/abs/2105.09571",
          "publishedOn": "2021-05-23T06:08:16.435Z",
          "wordCount": 611,
          "title": "The impact of virtual mirroring on customer satisfaction. (arXiv:2105.09571v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1\">Magnus Sahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeuniaux_P/0/1/0/all/0/1\">Patrick Jeuniaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyllensten_A/0/1/0/all/0/1\">Amaru Cuba Gyllensten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miliani_M/0/1/0/all/0/1\">Martina Miliani</a>",
          "description": "Distributional semantics has deeply changed in the last decades. First,\npredict models stole the thunder from traditional count ones, and more recently\nboth of them were replaced in many NLP applications by contextualized vectors\nproduced by Transformer neural language models. Although an extensive body of\nresearch has been devoted to Distributional Semantic Model (DSM) evaluation, we\nstill lack a thorough comparison with respect to tested models, semantic tasks,\nand benchmark datasets. Moreover, previous work has mostly focused on\ntask-driven evaluation, instead of exploring the differences between the way\nmodels represent the lexical semantic space. In this paper, we perform a\ncomprehensive evaluation of type distributional vectors, either produced by\nstatic DSMs or obtained by averaging the contextualized vectors generated by\nBERT. First of all, we investigate the performance of embeddings in several\nsemantic tasks, carrying out an in-depth statistical analysis to identify the\nmajor factors influencing the behavior of DSMs. The results show that i.) the\nalleged superiority of predict based models is more apparent than real, and\nsurely not ubiquitous and ii.) static DSMs surpass contextualized\nrepresentations in most out-of-context semantic tasks and datasets.\nFurthermore, we borrow from cognitive neuroscience the methodology of\nRepresentational Similarity Analysis (RSA) to inspect the semantic spaces\ngenerated by distributional models. RSA reveals important differences related\nto the frequency and part-of-speech of lexical items.",
          "link": "http://arxiv.org/abs/2105.09825",
          "publishedOn": "2021-05-23T06:08:16.424Z",
          "wordCount": 663,
          "title": "A comprehensive comparative evaluation and analysis of Distributional Semantic Models. (arXiv:2105.09825v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1\">Danilo Dessi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1\">Rim Helaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1\">Daniele Riboni</a>",
          "description": "Today, we are seeing an ever-increasing number of clinical notes that contain\nclinical results, images, and textual descriptions of patient's health state.\nAll these data can be analyzed and employed to cater novel services that can\nhelp people and domain experts with their common healthcare tasks. However,\nmany technologies such as Deep Learning and tools like Word Embeddings have\nstarted to be investigated only recently, and many challenges remain open when\nit comes to healthcare domain applications. To address these challenges, we\npropose the use of Deep Learning and Word Embeddings for identifying sixteen\nmorbidity types within textual descriptions of clinical records. For this\npurpose, we have used a Deep Learning model based on Bidirectional Long-Short\nTerm Memory (LSTM) layers which can exploit state-of-the-art vector\nrepresentations of data such as Word Embeddings. We have employed pre-trained\nWord Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained\non the target domain. Furthermore, we have compared the performances of the\ndeep learning approaches against the traditional tf-idf using Support Vector\nMachine and Multilayer perceptron (our baselines). From the obtained results it\nseems that the latter outperforms the combination of Deep Learning approaches\nusing any word embeddings. Our preliminary results indicate that there are\nspecific features that make the dataset biased in favour of traditional machine\nlearning approaches.",
          "link": "http://arxiv.org/abs/2105.09632",
          "publishedOn": "2021-05-23T06:08:16.414Z",
          "wordCount": 680,
          "title": "TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zixiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1\">Rim Helaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1\">Daniele Riboni</a>",
          "description": "Empathetic response from the therapist is key to the success of clinical\npsychotherapy, especially motivational interviewing. Previous work on\ncomputational modelling of empathy in motivational interviewing has focused on\noffline, session-level assessment of therapist empathy, where empathy captures\nall efforts that the therapist makes to understand the client's perspective and\nconvey that understanding to the client. In this position paper, we propose a\nnovel task of turn-level detection of client need for empathy. Concretely, we\npropose to leverage pre-trained language models and empathy-related general\nconversation corpora in a unique labeller-detector framework, where the\nlabeller automatically annotates a motivational interviewing conversation\ncorpus with empathy labels to train the detector that determines the need for\ntherapist empathy. We also lay out our strategies of extending the detector\nwith additional-input and multi-task setups to improve its detection and\nexplainability.",
          "link": "http://arxiv.org/abs/2105.09649",
          "publishedOn": "2021-05-23T06:08:16.391Z",
          "wordCount": 586,
          "title": "Towards Detecting Need for Empathetic Response in Motivational Interviewing. (arXiv:2105.09649v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1\">Dongfang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zhilin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>",
          "description": "We consider the problem of collectively detecting multiple events,\nparticularly in cross-sentence settings. The key to dealing with the problem is\nto encode semantic information and model event inter-dependency at a\ndocument-level. In this paper, we reformulate it as a Seq2Seq task and propose\na Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level\nassociation of events and semantic information simultaneously. Specifically, a\nbidirectional decoder is firstly devised to model event inter-dependency within\na sentence when decoding the event tag vector sequence. Secondly, an\ninformation aggregation module is employed to aggregate sentence-level semantic\nand event tag information. Finally, we stack multiple bidirectional decoders\nand feed cross-sentence information, forming a multi-layer bidirectional\ntagging architecture to iteratively propagate information across sentences. We\nshow that our approach provides significant improvement in performance compared\nto the current state-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.09458",
          "publishedOn": "2021-05-23T06:08:16.366Z",
          "wordCount": 569,
          "title": "MLBiNet: A Cross-Sentence Collective Event Detection Network. (arXiv:2105.09458v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>",
          "description": "This paper describes the system developed by the Laboratoire d'analyse\nstatistique des textes (LAST) for the Lexical Complexity Prediction shared task\nat SemEval-2021. The proposed system is made up of a LightGBM model fed with\nfeatures obtained from many word frequency lists, published lexical norms and\npsychometric data. For tackling the specificity of the multi-word task, it uses\nbigram association measures. Despite that the only contextual feature used was\nsentence length, the system achieved an honorable performance in the multi-word\ntask, but poorer in the single word task. The bigram association measures were\nfound useful, but to a limited extent.",
          "link": "http://arxiv.org/abs/2105.09653",
          "publishedOn": "2021-05-23T06:08:16.352Z",
          "wordCount": 533,
          "title": "LAST at SemEval-2021 Task 1: Improving Multi-Word Complexity Prediction Using Bigram Association Measures. (arXiv:2105.09653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose \\method, a\ntraining method to obtain a single unified multilingual translation model.\nmCOLT is empowered by two techniques: (i) a contrastive learning scheme to\nclose the gap among representations of different languages, and (ii) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mCOLT achieves\ncompetitive or even better performance than a strong pre-trained model mBART on\ntens of WMT benchmarks. For non-English directions, mCOLT achieves an\nimprovement of average 10+ BLEU compared with the multilingual baseline.",
          "link": "http://arxiv.org/abs/2105.09501",
          "publishedOn": "2021-05-23T06:08:16.338Z",
          "wordCount": 576,
          "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahlou_C/0/1/0/all/0/1\">Chuhong Lahlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crayton_A/0/1/0/all/0/1\">Ancil Crayton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1\">Caroline Trier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willett_E/0/1/0/all/0/1\">Evan Willett</a>",
          "description": "In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an\nArtificial Intelligence (AI) Health Outcomes Challenge seeking solutions to\npredict risk in value-based care for incorporation into CMS Innovation Center\npayment and service delivery models. Recently, modern language models have\nplayed key roles in a number of health related tasks. This paper presents, to\nthe best of our knowledge, the first application of these models to patient\nreadmission prediction. To facilitate this, we create a dataset of 1.2 million\nmedical history samples derived from the Limited Dataset (LDS) issued by CMS.\nMoreover, we propose a comprehensive modeling solution centered on a deep\nlearning framework for this data. To demonstrate the framework, we train an\nattention-based Transformer to learn Medicare semantics in support of\nperforming downstream prediction tasks thereby achieving 0.91 AUC and 0.91\nrecall on readmission classification. We also introduce a novel data\npre-processing pipeline and discuss pertinent deployment considerations\nsurrounding model explainability and bias.",
          "link": "http://arxiv.org/abs/2105.09428",
          "publishedOn": "2021-05-23T06:08:16.214Z",
          "wordCount": 592,
          "title": "Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder. (arXiv:2105.09428v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>",
          "description": "Dependency parsing is a crucial step towards deep language understanding and,\ntherefore, widely demanded by numerous Natural Language Processing\napplications. In particular, left-to-right and top-down transition-based\nalgorithms that rely on Pointer Networks are among the most accurate approaches\nfor performing dependency parsing. Additionally, it has been observed for the\ntop-down algorithm that Pointer Networks' sequential decoding can be improved\nby implementing a hierarchical variant, more adequate to model dependency\nstructures. Considering all this, we develop a bottom-up-oriented Hierarchical\nPointer Network for the left-to-right parser and propose two novel\ntransition-based alternatives: an approach that parses a sentence in\nright-to-left order and a variant that does it from the outside in. We\nempirically test the proposed neural architecture with the different algorithms\non a wide variety of languages, outperforming the original approach in\npractically all of them and setting new state-of-the-art results on the English\nand Chinese Penn Treebanks for non-contextualized and BERT-based embeddings.",
          "link": "http://arxiv.org/abs/2105.09611",
          "publishedOn": "2021-05-23T06:08:16.202Z",
          "wordCount": 583,
          "title": "Dependency Parsing with Bottom-up Hierarchical Pointer Networks. (arXiv:2105.09611v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>",
          "description": "Neural network approaches have been applied to computational morphology with\ngreat success, improving the performance of most tasks by a large margin and\nproviding new perspectives for modeling. This paper starts with a brief\nintroduction to computational morphology, followed by a review of recent work\non computational morphology with neural network approaches, to provide an\noverview of the area. In the end, we will analyze the advantages and problems\nof neural network approaches to computational morphology, and point out some\ndirections to be explored by future research and study.",
          "link": "http://arxiv.org/abs/2105.09404",
          "publishedOn": "2021-05-23T06:08:16.168Z",
          "wordCount": 505,
          "title": "Computational Morphology with Neural Network Approaches. (arXiv:2105.09404v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose {\\name}, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. {\\name} utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that {\\name} achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.",
          "link": "http://arxiv.org/abs/2105.09601",
          "publishedOn": "2021-05-23T06:08:16.148Z",
          "wordCount": 667,
          "title": "See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1\">Gengchen Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janowicz_K/0/1/0/all/0/1\">Krzysztof Janowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Ling Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>",
          "description": "As an important part of Artificial Intelligence (AI), Question Answering (QA)\naims at generating answers to questions phrased in natural language. While\nthere has been substantial progress in open-domain question answering, QA\nsystems are still struggling to answer questions which involve geographic\nentities or concepts and that require spatial operations. In this paper, we\ndiscuss the problem of geographic question answering (GeoQA). We first\ninvestigate the reasons why geographic questions are difficult to answer by\nanalyzing challenges of geographic questions. We discuss the uniqueness of\ngeographic questions compared to general QA. Then we review existing work on\nGeoQA and classify them by the types of questions they can address. Based on\nthis survey, we provide a generic classification framework for geographic\nquestions. Finally, we conclude our work by pointing out unique future research\ndirections for GeoQA.",
          "link": "http://arxiv.org/abs/2105.09392",
          "publishedOn": "2021-05-23T06:08:16.136Z",
          "wordCount": 595,
          "title": "Geographic Question Answering: Challenges, Uniqueness, Classification, and Future Directions. (arXiv:2105.09392v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shirong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sheng Bi</a>",
          "description": "Event detection (ED) aims at detecting event trigger words in sentences and\nclassifying them into specific event types. In real-world applications, ED\ntypically does not have sufficient labelled data, thus can be formulated as a\nfew-shot learning problem. To tackle the issue of low sample diversity in\nfew-shot ED, we propose a novel knowledge-based few-shot event detection method\nwhich uses a definition-based encoder to introduce external event knowledge as\nthe knowledge prior of event types. Furthermore, as external knowledge\ntypically provides limited and imperfect coverage of event types, we introduce\nan adaptive knowledge-enhanced Bayesian meta-learning method to dynamically\nadjust the knowledge prior of event types. Experiments show our method\nconsistently and substantially outperforms a number of baselines by at least 15\nabsolute F1 points under the same few-shot settings.",
          "link": "http://arxiv.org/abs/2105.09509",
          "publishedOn": "2021-05-23T06:08:16.101Z",
          "wordCount": 568,
          "title": "Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection. (arXiv:2105.09509v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lianwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yuan Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yuqian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Ling Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhaoyin Qi</a>",
          "description": "Recent studies constructing direct interactions between the claim and each\nsingle user response (a comment or a relevant article) to capture evidence have\nshown remarkable success in interpretable claim verification. Owing to\ndifferent single responses convey different cognition of individual users\n(i.e., audiences), the captured evidence belongs to the perspective of\nindividual cognition. However, individuals' cognition of social things is not\nalways able to truly reflect the objective. There may be one-sided or biased\nsemantics in their opinions on a claim. The captured evidence correspondingly\ncontains some unobjective and biased evidence fragments, deteriorating task\nperformance. In this paper, we propose a Dual-view model based on the views of\nCollective and Individual Cognition (CICD) for interpretable claim\nverification. From the view of the collective cognition, we not only capture\nthe word-level semantics based on individual users, but also focus on\nsentence-level semantics (i.e., the overall responses) among all users and\nadjust the proportion between them to generate global evidence. From the view\nof individual cognition, we select the top-$k$ articles with high degree of\ndifference and interact with the claim to explore the local key evidence\nfragments. To weaken the bias of individual cognition-view evidence, we devise\ninconsistent loss to suppress the divergence between global and local evidence\nfor strengthening the consistent shared evidence between the both. Experiments\non three benchmark datasets confirm that CICD achieves state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2105.09567",
          "publishedOn": "2021-05-23T06:08:16.080Z",
          "wordCount": 672,
          "title": "Unified Dual-view Cognitive Model for Interpretable Claim Verification. (arXiv:2105.09567v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.12876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_P/0/1/0/all/0/1\">Pratik K. Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songlin Liu</a>",
          "description": "Recommender Systems are a subclass of machine learning systems that employ\nsophisticated information filtering strategies to reduce the search time and\nsuggest the most relevant items to any particular user. Hybrid recommender\nsystems combine multiple recommendation strategies in different ways to benefit\nfrom their complementary advantages. Some hybrid recommender systems have\ncombined collaborative filtering and content-based approaches to build systems\nthat are more robust. In this paper, we propose a hybrid recommender system,\nwhich combines Alternative Least Squares (ALS) based collaborative filtering\nwith deep learning to enhance recommendation performance as well as overcome\nthe limitations associated with the collaborative filtering approach,\nespecially concerning its cold start problem. In essence, we use the outputs\nfrom ALS (collaborative filtering) to influence the recommendations from a Deep\nNeural Network (DNN), which combines characteristic, contextual, structural and\nsequential information, in a big data processing framework. We have conducted\nseveral experiments in testing the efficacy of the proposed hybrid architecture\nin recommending smartphones to prospective customers and compared its\nperformance with other open-source recommenders. The results have shown that\nthe proposed system has outperformed several existing hybrid recommender\nsystems.",
          "link": "http://arxiv.org/abs/2105.12876",
          "publishedOn": "2021-05-28T01:42:13.849Z",
          "wordCount": 619,
          "title": "A Hybrid Recommender System for Recommending Smartphones to Prospective Customers. (arXiv:2105.12876v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chongming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Recommender systems exploit interaction history to estimate user preference,\nhaving been heavily used in a wide range of industry applications. However,\nstatic recommendation models are difficult to answer two important questions\nwell due to inherent shortcomings: (a) What exactly does a user like? (b) Why\ndoes a user like an item? The shortcomings are due to the way that static\nmodels learn user preference, i.e., without explicit instructions and active\nfeedback from users. The recent rise of conversational recommender systems\n(CRSs) changes this situation fundamentally. In a CRS, users and the system can\ndynamically communicate through natural language interactions, which provide\nunprecedented opportunities to explicitly obtain the exact preference of users.\n\nConsiderable efforts, spread across disparate settings and applications, have\nbeen put into developing CRSs. Existing models, technologies, and evaluation\nmethods for CRSs are far from mature. In this paper, we provide a systematic\nreview of the techniques used in current CRSs. We summarize the key challenges\nof developing CRSs in five directions: (1) Question-based user preference\nelicitation. (2) Multi-turn conversational recommendation strategies. (3)\nDialogue understanding and generation. (4) Exploitation-exploration trade-offs.\n(5) Evaluation and user simulation. These research directions involve multiple\nresearch fields like information retrieval (IR), natural language processing\n(NLP), and human-computer interaction (HCI). Based on these research\ndirections, we discuss some future challenges and opportunities. We provide a\nroad map for researchers from multiple communities to get started in this area.\nWe hope this survey can help to identify and address challenges in CRSs and\ninspire future research.",
          "link": "http://arxiv.org/abs/2101.09459",
          "publishedOn": "2021-05-28T01:42:13.746Z",
          "wordCount": 757,
          "title": "Advances and Challenges in Conversational Recommender Systems: A Survey. (arXiv:2101.09459v6 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Cicero Nogueira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>",
          "description": "The performance of state-of-the-art neural rankers can deteriorate\nsubstantially when exposed to noisy inputs or applied to a new domain. In this\npaper, we present a novel method for fine-tuning neural rankers that can\nsignificantly improve their robustness to out-of-domain data and query\nperturbations. Specifically, a contrastive loss that compares data points in\nthe representation space is combined with the standard ranking loss during\nfine-tuning. We use relevance labels to denote similar/dissimilar pairs, which\nallows the model to learn the underlying matching semantics across different\nquery-document pairs and leads to improved robustness. In experiments with four\npassage ranking datasets, the proposed contrastive fine-tuning method obtains\nimprovements on robustness to query reformulations, noise perturbations, and\nzero-shot transfer for both BERT and BART based rankers. Additionally, our\nexperiments show that contrastive fine-tuning outperforms data augmentation for\nrobustifying neural rankers.",
          "link": "http://arxiv.org/abs/2105.12932",
          "publishedOn": "2021-05-28T01:42:13.684Z",
          "wordCount": 564,
          "title": "Contrastive Fine-tuning Improves Robustness for Neural Rankers. (arXiv:2105.12932v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zijing Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qinliang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianxing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "With the need of fast retrieval speed and small memory footprint, document\nhashing has been playing a crucial role in large-scale information retrieval.\nTo generate high-quality hashing code, both semantics and neighborhood\ninformation are crucial. However, most existing methods leverage only one of\nthem or simply combine them via some intuitive criteria, lacking a theoretical\nprinciple to guide the integration process. In this paper, we encode the\nneighborhood information with a graph-induced Gaussian distribution, and\npropose to integrate the two types of information with a graph-driven\ngenerative model. To deal with the complicated correlations among documents, we\nfurther propose a tree-structured approximation method for learning. Under the\napproximation, we prove that the training objective can be decomposed into\nterms involving only singleton or pairwise documents, enabling the model to be\ntrained as efficiently as uncorrelated ones. Extensive experimental results on\nthree benchmark datasets show that our method achieves superior performance\nover state-of-the-art methods, demonstrating the effectiveness of the proposed\nmodel for simultaneously preserving semantic and neighborhood information.\\",
          "link": "http://arxiv.org/abs/2105.13066",
          "publishedOn": "2021-05-28T01:42:13.653Z",
          "wordCount": 614,
          "title": "Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval. (arXiv:2105.13066v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2009.02252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plachouras_V/0/1/0/all/0/1\">Vassilis Plachouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Challenging problems such as open-domain question answering, fact checking,\nslot filling and entity linking require access to large, external knowledge\nsources. While some models do well on individual tasks, developing general\nmodels is difficult as each task might require computationally expensive\nindexing of custom knowledge sources, in addition to dedicated infrastructure.\nTo catalyze research on models that condition on specific information in large\ntextual resources, we present a benchmark for knowledge-intensive language\ntasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia,\nreducing engineering turnaround through the re-use of components, as well as\naccelerating research into task-agnostic memory architectures. We test both\ntask-specific and general baselines, evaluating downstream performance in\naddition to the ability of the models to provide provenance. We find that a\nshared dense vector index coupled with a seq2seq model is a strong baseline,\noutperforming more tailor-made approaches for fact checking, open-domain\nquestion answering and dialogue, and yielding competitive results on entity\nlinking and slot filling, by generating disambiguated text. KILT data and code\nare available at https://github.com/facebookresearch/KILT.",
          "link": "http://arxiv.org/abs/2009.02252",
          "publishedOn": "2021-05-28T01:42:13.641Z",
          "wordCount": 689,
          "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks. (arXiv:2009.02252v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruoming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>",
          "description": "Recently, linear regression models, such as EASE and SLIM, have shown to\noften produce rather competitive results against more sophisticated deep\nlearning models. On the other side, the (weighted) matrix factorization\napproaches have been popular choices for recommendation in the past and widely\nadopted in the industry. In this work, we aim to theoretically understand the\nrelationship between these two approaches, which are the cornerstones of\nmodel-based recommendations. Through the derivation and analysis of the\nclosed-form solutions for two basic regression and matrix factorization\napproaches, we found these two approaches are indeed inherently related but\nalso diverge in how they \"scale-down\" the singular values of the original\nuser-item interaction matrix. This analysis also helps resolve the questions\nrelated to the regularization parameter range and model complexities. We\nfurther introduce a new learning algorithm in searching (hyper)parameters for\nthe closed-form solution and utilize it to discover the nearby models of the\nexisting solutions. The experimental results demonstrate that the basic models\nand their closed-form solutions are indeed quite competitive against the\nstate-of-the-art models, thus, confirming the validity of studying the basic\nmodels. The effectiveness of exploring the nearby models are also\nexperimentally validated.",
          "link": "http://arxiv.org/abs/2105.12937",
          "publishedOn": "2021-05-28T01:42:13.604Z",
          "wordCount": 630,
          "title": "Towards a Better Understanding of Linear Models for Recommendation. (arXiv:2105.12937v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Alfredo Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1\">Marcelo Mendoza</a>",
          "description": "Word embeddings are vital descriptors of words in unigram representations of\ndocuments for many tasks in natural language processing and information\nretrieval. The representation of queries has been one of the most critical\nchallenges in this area because it consists of a few terms and has little\ndescriptive capacity. Strategies such as average word embeddings can enrich the\nqueries' descriptive capacity since they favor the identification of related\nterms from the continuous vector representations that characterize these\napproaches. We propose a data-driven strategy to combine word embeddings. We\nuse Idf combinations of embeddings to represent queries, showing that these\nrepresentations outperform the average word embeddings recently proposed in the\nliterature. Experimental results on benchmark data show that our proposal\nperforms well, suggesting that data-driven combinations of word embeddings are\na promising line of research in ad-hoc information retrieval.",
          "link": "http://arxiv.org/abs/2105.12788",
          "publishedOn": "2021-05-28T01:42:13.579Z",
          "wordCount": 582,
          "title": "A data-driven strategy to combine word embeddings in information retrieval. (arXiv:2105.12788v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>",
          "description": "InfoNCE loss is a widely used loss function for contrastive model training.\nIt aims to estimate the mutual information between a pair of variables by\ndiscriminating between each positive pair and its associated $K$ negative\npairs. It is proved that when the sample labels are clean, the lower bound of\nmutual information estimation is tighter when more negative samples are\nincorporated, which usually yields better model performance. However, in many\nreal-world tasks the labels often contain noise, and incorporating too many\nnoisy negative samples for model training may be suboptimal. In this paper, we\nstudy how many negative samples are optimal for InfoNCE in different scenarios\nvia a semi-quantitative theoretical framework. More specifically, we first\npropose a probabilistic model to analyze the influence of the negative sampling\nratio $K$ on training sample informativeness. Then, we design a training\neffectiveness function to measure the overall influence of training samples on\nmodel learning based on their informativeness. We estimate the optimal negative\nsampling ratio using the $K$ value that maximizes the training effectiveness\nfunction. Based on our framework, we further propose an adaptive negative\nsampling method that can dynamically adjust the negative sampling ratio to\nimprove InfoNCE based model training. Extensive experiments on different\nreal-world datasets show our framework can accurately predict the optimal\nnegative sampling ratio in different tasks, and our proposed adaptive negative\nsampling method can achieve better performance than the commonly used fixed\nnegative sampling ratio strategy.",
          "link": "http://arxiv.org/abs/2105.13003",
          "publishedOn": "2021-05-28T01:42:13.553Z",
          "wordCount": 665,
          "title": "Rethinking InfoNCE: How Many Negative Samples Do You Need?. (arXiv:2105.13003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Janssen_R/0/1/0/all/0/1\">Rik D.T. Janssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proper_H/0/1/0/all/0/1\">Henderik A. Proper</a>",
          "description": "In this paper a functionality taxonomy for document search engines is\nproposed. It can be used to assess the features of a search engine, to position\nsearch engines relative to each other, or to select which search engine 'fits'\na certain situation. One is able to identify areas for improvement. During\ndevelopment, we were guided by the viewpoint of the user. We use the word\n`search engine' in the broadest sense possible, including library and web based\n(meta) search engines. The taxonomy distinguishes seven functionality areas: an\nindexing service, user profiling, query composition, query execution, result\npresentation, result refinement, and history keeping. Each of these relates and\nprovides services to other functionality areas. It can be extended whenever\nnecessary. To illustrate the validity of our taxonomy, it has been used for\ncomparing various document search engines existing today (ACM Digital Library,\nPiCarta, Copernic, AltaVista, Google, and GuideBeam). It appears that the\nfunctionality aspects covered by our taxonomy can be used for describing these\nsearch engines.",
          "link": "http://arxiv.org/abs/2105.12989",
          "publishedOn": "2021-05-28T01:42:13.504Z",
          "wordCount": 586,
          "title": "A functionality taxonomy for document search engines. (arXiv:2105.12989v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1\">Xiang-Rong Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liqin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guorui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinyao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Binding Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Siran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jingshan Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hongbo Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>",
          "description": "Traditional industrial recommenders are usually trained on a single business\ndomain and then serve for this domain. However, in large commercial platforms,\nit is often the case that the recommenders need to make click-through rate\n(CTR) predictions for multiple business domains. Different domains have\noverlapping user groups and items. Thus, there exist commonalities. Since the\nspecific user groups have disparity and the user behaviors may change in\nvarious business domains, there also have distinctions. The distinctions result\nin domain-specific data distributions, making it hard for a single shared model\nto work well on all domains. To learn an effective and efficient CTR model to\nhandle multiple domains simultaneously, we present Star Topology Adaptive\nRecommender (STAR). Concretely, STAR has the star topology, which consists of\nthe shared centered parameters and domain-specific parameters. The shared\nparameters are applied to learn commonalities of all domains, and the\ndomain-specific parameters capture domain distinction for more refined\nprediction. Given requests from different business domains, STAR can adapt its\nparameters conditioned on the domain characteristics. The experimental result\nfrom production data validates the superiority of the proposed STAR model.\nSince 2020, STAR has been deployed in the display advertising system of\nAlibaba, obtaining averaging 8.0% improvement on CTR and 6.0% on RPM (Revenue\nPer Mille).",
          "link": "http://arxiv.org/abs/2101.11427",
          "publishedOn": "2021-05-27T01:32:29.783Z",
          "wordCount": 689,
          "title": "One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. (arXiv:2101.11427v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>",
          "description": "Fairness is an important property in data-mining applications, including\nrecommender systems. In this work, we investigate a case where users of a\nrecommender system need (or want) to be fair to a protected group of items. For\nexample, in a job market, the user is the recruiter, an item is the job seeker,\nand the protected attribute is gender or race. Even if recruiters want to use a\nfair talent recommender system, the platform may not provide a fair recommender\nsystem, or recruiters may not be able to ascertain whether the recommender\nsystem's algorithm is fair. In this case, recruiters cannot utilize the\nrecommender system, or they may become unfair to job seekers. In this work, we\npropose methods to enable the users to build their own fair recommender\nsystems. Our methods can generate fair recommendations even when the platform\ndoes not (or cannot) provide fair recommender systems. The key challenge is\nthat a user does not have access to the log data of other users or the latent\nrepresentations of items. This restriction prohibits us from adopting existing\nmethods, which are designed for platforms. The main idea is that a user has\naccess to unfair recommendations provided by the platform. Our methods leverage\nthe outputs of an unfair recommender system to construct a new fair recommender\nsystem. We empirically validate that our proposed method improves fairness\nsubstantially without harming much performance of the original unfair system.",
          "link": "http://arxiv.org/abs/2105.12353",
          "publishedOn": "2021-05-27T01:32:27.868Z",
          "wordCount": 670,
          "title": "Private Recommender Systems: How Can Users Build Their Own Fair Recommender Systems without Log Data?. (arXiv:2105.12353v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1\">Hansi Hettiarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adedoyin_Olowe_M/0/1/0/all/0/1\">Mariam Adedoyin-Olowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhogal_J/0/1/0/all/0/1\">Jagdev Bhogal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaber_M/0/1/0/all/0/1\">Mohamed Medhat Gaber</a>",
          "description": "Social media is becoming a primary medium to discuss what is happening around\nthe world. Therefore, the data generated by social media platforms contain rich\ninformation which describes the ongoing events. Further, the timeliness\nassociated with these data is capable of facilitating immediate insights.\nHowever, considering the dynamic nature and high volume of data production in\nsocial media data streams, it is impractical to filter the events manually and\ntherefore, automated event detection mechanisms are invaluable to the\ncommunity. Apart from a few notable exceptions, most previous research on\nautomated event detection have focused only on statistical and syntactical\nfeatures in data and lacked the involvement of underlying semantics which are\nimportant for effective information retrieval from text since they represent\nthe connections between words and their meanings. In this paper, we propose a\nnovel method termed Embed2Detect for event detection in social media by\ncombining the characteristics in word embeddings and hierarchical agglomerative\nclustering. The adoption of word embeddings gives Embed2Detect the capability\nto incorporate powerful semantical features into event detection and overcome a\nmajor limitation inherent in previous approaches. We experimented our method on\ntwo recent real social media data sets which represent the sports and political\ndomain and also compared the results to several state-of-the-art methods. The\nobtained results show that Embed2Detect is capable of effective and efficient\nevent detection and it outperforms the recent event detection methods. For the\nsports data set, Embed2Detect achieved 27% higher F-measure than the\nbest-performed baseline and for the political data set, it was an increase of\n29%.",
          "link": "http://arxiv.org/abs/2006.05908",
          "publishedOn": "2021-05-27T01:32:27.638Z",
          "wordCount": 768,
          "title": "Embed2Detect: Temporally Clustered Embedded Words for Event Detection in Social Media. (arXiv:2006.05908v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhaoxia/0/1/0/all/0/1\">Zhaoxia</a> (Summer) <a href=\"http://arxiv.org/find/cs/1/au:+Deng/0/1/0/all/0/1\">Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1\">Ping Tak Peter Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haixin Liu</a>, Jie (Amy) <a href=\"http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1\">Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuen_H/0/1/0/all/0/1\">Hector Yuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khudia_D/0/1/0/all/0/1\">Daya Khudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaohan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1\">Ellie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1\">Dhruv Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1\">Raghuraman Krishnamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadathur_S/0/1/0/all/0/1\">Satish Nadathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changkyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumov_M/0/1/0/all/0/1\">Maxim Naumov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naghshineh_S/0/1/0/all/0/1\">Sam Naghshineh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smelyanskiy_M/0/1/0/all/0/1\">Mikhail Smelyanskiy</a>",
          "description": "Tremendous success of machine learning (ML) and the unabated growth in ML\nmodel complexity motivated many ML-specific designs in both CPU and accelerator\narchitectures to speed up the model inference. While these architectures are\ndiverse, highly optimized low-precision arithmetic is a component shared by\nmost. Impressive compute throughputs are indeed often exhibited by these\narchitectures on benchmark ML models. Nevertheless, production models such as\nrecommendation systems important to Facebook's personalization services are\ndemanding and complex: These systems must serve billions of users per month\nresponsively with low latency while maintaining high prediction accuracy,\nnotwithstanding computations with many tens of billions parameters per\ninference. Do these low-precision architectures work well with our production\nrecommendation systems? They do. But not without significant effort. We share\nin this paper our search strategies to adapt reference recommendation models to\nlow-precision hardware, our optimization of low-precision compute kernels, and\nthe design and development of tool chain so as to maintain our models' accuracy\nthroughout their lifespan during which topic trends and users' interests\ninevitably evolve. Practicing these low-precision technologies helped us save\ndatacenter capacities while deploying models with up to 5X complexity that\nwould otherwise not be deployed on traditional general-purpose CPUs. We believe\nthese lessons from the trenches promote better co-design between hardware\narchitecture and software engineering and advance the state of the art of ML in\nindustry.",
          "link": "http://arxiv.org/abs/2105.12676",
          "publishedOn": "2021-05-27T01:32:27.606Z",
          "wordCount": 697,
          "title": "Low-Precision Hardware Architectures Meet Recommendation Model Inference at Scale. (arXiv:2105.12676v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Le Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "As users often express their preferences with binary behavior data~(implicit\nfeedback), such as clicking items or buying products, implicit feedback based\nCollaborative Filtering~(CF) models predict the top ranked items a user might\nlike by leveraging implicit user-item interaction data. For each user, the\nimplicit feedback is divided into two sets: an observed item set with limited\nobserved behaviors, and a large unobserved item set that is mixed with negative\nitem behaviors and unknown behaviors. Given any user preference prediction\nmodel, researchers either designed ranking based optimization goals or relied\non negative item mining techniques for better optimization. Despite the\nperformance gain of these implicit feedback based models, the recommendation\nresults are still far from satisfactory due to the sparsity of the observed\nitem set for each user. To this end, in this paper, we explore the unique\ncharacteristics of the implicit feedback and propose Set2setRank framework for\nrecommendation. The optimization criteria of Set2setRank are two folds: First,\nwe design an item to an item set comparison that encourages each observed item\nfrom the sampled observed set is ranked higher than any unobserved item from\nthe sampled unobserved set. Second, we model set level comparison that\nencourages a margin between the distance summarized from the observed item set\nand the most \"hard\" unobserved item from the sampled negative set. Further, an\nadaptive sampling technique is designed to implement these two goals. We have\nto note that our proposed framework is model-agnostic and can be easily applied\nto most recommendation prediction approaches, and is time efficient in\npractice. Finally, extensive experiments on three real-world datasets\ndemonstrate the superiority of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.07377",
          "publishedOn": "2021-05-27T01:32:27.548Z",
          "wordCount": 731,
          "title": "Set2setRank: Collaborative Set to Set Ranking for Implicit Feedback based Recommendation. (arXiv:2105.07377v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng-Chieh Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jheng-Hong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "A vital step towards the widespread adoption of neural retrieval models is\ntheir resource efficiency throughout the training, indexing and query\nworkflows. The neural IR community made great advancements in training\neffective dual-encoder dense retrieval (DR) models recently. A dense text\nretrieval model uses a single vector representation per query and passage to\nscore a match, which enables low-latency first stage retrieval with a nearest\nneighbor search. Increasingly common, training approaches require enormous\ncompute power, as they either conduct negative passage sampling out of a\ncontinuously updating refreshing index or require very large batch sizes for\nin-batch negative sampling. Instead of relying on more compute capability, we\nintroduce an efficient topic-aware query and balanced margin sampling\ntechnique, called TAS-Balanced. We cluster queries once before training and\nsample queries out of a cluster per batch. We train our lightweight 6-layer DR\nmodel with a novel dual-teacher supervision that combines pairwise and in-batch\nnegative teachers. Our method is trainable on a single consumer-grade GPU in\nunder 48 hours (as opposed to a common configuration of 8x V100s). We show that\nour TAS-Balanced training method achieves state-of-the-art low-latency (64ms\nper query) results on two TREC Deep Learning Track query sets. Evaluated on\nNDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by\n11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces\nthe first dense retriever that outperforms every other method on recall at any\ncutoff on TREC-DL and allows more resource intensive re-ranking models to\noperate on fewer passages to improve results further.",
          "link": "http://arxiv.org/abs/2104.06967",
          "publishedOn": "2021-05-27T01:32:27.495Z",
          "wordCount": 730,
          "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling. (arXiv:2104.06967v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Automatic text summarization has enjoyed great progress over the last years.\nHowever, there is little research that investigates whether the current\nresearch focus adheres to users' needs. Importantly, these needs are dependent\non the envisioned target group of the generated summaries. One such important\ntarget group is formed by students, due to their usage of summaries in their\nstudy activities. For this reason, we investigate students' needs regarding\nautomatically generated summaries by means of a survey amongst university\nstudents and find that the current direction of the field does not fully align\nwith their needs. Motivated by our findings, we formulate three groups of\nimplications that together help us formulate a renewed perspective on future\nresearch on automatic summarization. First, the educational domain requires a\nbroader perspective on automatic summarization, beyond the approaches that are\ncurrently the standard. We illustrate how we can expand these approaches\nregarding the input material, the purpose of the summaries and their potential\nformat and we define requirements for datasets that can facilitate these\nresearch directions. Second, we propose a methodology to evaluate the\nusefulness of a summary based on the identified needs of a target group. Third,\nin more general terms, we hope that our survey will be reused to investigate\nthe needs of different user groups of automatically generated summaries to\nbroaden our perspective even further.",
          "link": "http://arxiv.org/abs/2012.07619",
          "publishedOn": "2021-05-27T01:32:27.445Z",
          "wordCount": 706,
          "title": "What Makes a Good Summary? Investigating the Focus of Automatic Summarization in an Educational Context. (arXiv:2012.07619v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenhui Li</a>",
          "description": "The heavy traffic and related issues have always been concerns for modern\ncities. With the help of deep learning and reinforcement learning, people have\nproposed various policies to solve these traffic-related problems, such as\nsmart traffic signal control systems and taxi dispatching systems. People\nusually validate these policies in a city simulator, since directly applying\nthem in the real city introduces real cost. However, these policies validated\nin the city simulator may fail in the real city if the simulator is\nsignificantly different from the real world. To tackle this problem, we need to\nbuild a real-like traffic simulation system. Therefore, in this paper, we\npropose to learn the human routing model, which is one of the most essential\npart in the traffic simulator. This problem has two major challenges. First,\nhuman routing decisions are determined by multiple factors, besides the common\ntime and distance factor. Second, current historical routes data usually covers\njust a small portion of vehicles, due to privacy and device availability\nissues. To address these problems, we propose a theory-guided residual network\nmodel, where the theoretical part can emphasize the general principles for\nhuman routing decisions (e.g., fastest route), and the residual part can\ncapture drivable condition preferences (e.g., local road or highway). Since the\ntheoretical part is composed of traditional shortest path algorithms that do\nnot need data to train, our residual network can learn human routing models\nfrom limited data. We have conducted extensive experiments on multiple\nreal-world datasets to show the superior performance of our model, especially\nwith small data. Besides, we have also illustrated why our model is better at\nrecovering real routes through case studies.",
          "link": "http://arxiv.org/abs/2105.08279",
          "publishedOn": "2021-05-27T01:32:27.437Z",
          "wordCount": 719,
          "title": "Learning to Route via Theory-Guided Residual Network. (arXiv:2105.08279v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suster_S/0/1/0/all/0/1\">Simon &#x160;uster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1\">Antonio Jimeno Yepes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_D/0/1/0/all/0/1\">David Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otmakhova_Y/0/1/0/all/0/1\">Yulia Otmakhova</a>",
          "description": "The COVID-19 pandemic has driven ever-greater demand for tools which enable\nefficient exploration of biomedical literature. Although semi-structured\ninformation resulting from concept recognition and detection of the defining\nelements of clinical trials (e.g. PICO criteria) has been commonly used to\nsupport literature search, the contributions of this abstraction remain poorly\nunderstood, especially in relation to text-based retrieval. In this study, we\ncompare the results retrieved by a standard search engine with those filtered\nusing clinically-relevant concepts and their relations. With analysis based on\nthe annotations from the TREC-COVID shared task, we obtain quantitative as well\nas qualitative insights into characteristics of relational and concept-based\nliterature exploration. Most importantly, we find that the relational concept\nselection filters the original retrieved collection in a way that decreases the\nproportion of unjudged documents and increases the precision, which means that\nthe user is likely to be exposed to a larger number of relevant documents.",
          "link": "http://arxiv.org/abs/2105.12261",
          "publishedOn": "2021-05-27T01:32:26.995Z",
          "wordCount": 643,
          "title": "Impact of detecting clinical trial elements in exploration of COVID-19 literature. (arXiv:2105.12261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1\">Yijiang Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chaobing Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">YanFeng Zhu</a>",
          "description": "Synonymous keyword retrieval has become an important problem for sponsored\nsearch ever since major search engines relax the exact match product's matching\nrequirement to a synonymous level. Since the synonymous relations between\nqueries and keywords are quite scarce, the traditional information retrieval\nframework is inefficient in this scenario. In this paper, we propose a novel\nquotient space-based retrieval framework to address this problem. Considering\nthe synonymy among keywords as a mathematical equivalence relation, we can\ncompress the synonymous keywords into one representative, and the corresponding\nquotient space would greatly reduce the size of the keyword repository. Then an\nembedding-based retrieval is directly conducted between queries and the keyword\nrepresentatives. To mitigate the semantic gap of the quotient space-based\nretrieval, a single semantic siamese model is utilized to detect both the\nkeyword--keyword and query-keyword synonymous relations. The experiments show\nthat with our quotient space-based retrieval method, the synonymous keyword\nretrieving performance can be greatly improved in terms of memory cost and\nrecall efficiency. This method has been successfully implemented in Baidu's\nonline sponsored search system and has yielded a significant improvement in\nrevenue.",
          "link": "http://arxiv.org/abs/2105.12371",
          "publishedOn": "2021-05-27T01:32:26.961Z",
          "wordCount": 607,
          "title": "Quotient Space-Based Keyword Retrieval in Sponsored Search. (arXiv:2105.12371v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parsa_M/0/1/0/all/0/1\">Mohammad S. Parsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1\">Lukasz Golab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshav_S/0/1/0/all/0/1\">Srinivasan Keshav</a>",
          "description": "The Coronavirus pandemic created a global crisis that prompted immediate\nlarge-scale action, including economic shutdowns and mobility restrictions.\nThese actions have had devastating effects on the economy, but some positive\neffects on the environment. As the world recovers from the pandemic, we ask the\nfollowing question: What is the public attitude towards climate action during\nCOVID-19 recovery and beyond? We answer this question by analyzing discussions\non the Twitter social media platform. We find that most discussions support\nclimate action and point out lessons learned during pandemic response that can\nshape future climate policy, although skeptics continue to have a presence.\nAdditionally, concerns arise in the context of climate action during the\npandemic, such as mitigating the risk of COVID-19 transmission on public\ntransit.",
          "link": "http://arxiv.org/abs/2105.12190",
          "publishedOn": "2021-05-27T01:32:26.941Z",
          "wordCount": 607,
          "title": "Climate Action During COVID-19 Recovery and Beyond: A Twitter Text Mining Study. (arXiv:2105.12190v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_N/0/1/0/all/0/1\">Nidhi Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sharmishtha Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christian_R/0/1/0/all/0/1\">Ryan Christian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gridley_J/0/1/0/all/0/1\">Jared Gridley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammad Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1\">Alex Gittens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>",
          "description": "Large amounts of threat intelligence information about malware attacks are\navailable in disparate, typically unstructured, formats. Knowledge graphs can\ncapture this information and its context using RDF triples represented by\nentities and relations. Sparse or inaccurate threat information, however, leads\nto challenges such as incomplete or erroneous triples. Generic information\nextraction (IE) models used to populate the knowledge graph cannot fully\nguarantee domain-specific context. This paper proposes a system to generate a\nMalware Knowledge Graph called MalKG, the first open-source automated knowledge\ngraph for malware threat intelligence. MalKG dataset (MT40K\\footnote{ Anonymous\nGitHub link: https://github.com/malkg-researcher/MalKG}) contains approximately\n40,000 triples generated from 27,354 unique entities and 34 relations. For\nground truth, we manually curate a knowledge graph called MT3K, with 3,027\ntriples generated from 5,741 unique entities and 22 relations. We demonstrate\nthe intelligence prediction of MalKG using two use cases. Predicting malware\nthreat information using the benchmark model achieves 80.4 for the hits@10\nmetric (predicts the top 10 options for an information class), and 0.75 for the\nMRR (mean reciprocal rank). We also propose an automated, contextual framework\nfor information extraction, both manually and automatically, at the sentence\nlevel from 1,100 malware threat reports and from the common vulnerabilities and\nexposures (CVE) database.",
          "link": "http://arxiv.org/abs/2102.05571",
          "publishedOn": "2021-05-26T01:22:09.842Z",
          "wordCount": 686,
          "title": "Predicting malware threat intelligence using KGs. (arXiv:2102.05571v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Daqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jinwen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "With the increasing scale and diversification of interaction behaviors in\nE-commerce, more and more researchers pay attention to multi-behavior\nrecommender systems that utilize interaction data of other auxiliary behaviors\nsuch as view and cart. To address these challenges in heterogeneous scenarios,\nnon-sampling methods have shown superiority over negative sampling methods.\nHowever, two observations are usually ignored in existing state-of-the-art\nnon-sampling methods based on binary regression: (1) users have different\npreference strengths for different items, so they cannot be measured simply by\nbinary implicit data; (2) the dependency across multiple behaviors varies for\ndifferent users and items. To tackle the above issue, we propose a novel\nnon-sampling learning framework named \\underline{C}riterion-guided\n\\underline{H}eterogeneous \\underline{C}ollaborative \\underline{F}iltering\n(CHCF). CHCF introduces both upper and lower bounds to indicate selection\ncriteria, which will guide user preference learning. Besides, CHCF integrates\ncriterion learning and user preference learning into a unified framework, which\ncan be trained jointly for the interaction prediction on target behavior. We\nfurther theoretically demonstrate that the optimization of Collaborative Metric\nLearning can be approximately achieved by CHCF learning framework in a\nnon-sampling form effectively. Extensive experiments on two real-world datasets\nshow that CHCF outperforms the state-of-the-art methods in heterogeneous\nscenarios.",
          "link": "http://arxiv.org/abs/2105.11876",
          "publishedOn": "2021-05-26T01:22:09.783Z",
          "wordCount": 632,
          "title": "Criterion-based Heterogeneous Collaborative Filtering for Multi-behavior Implicit Recommendation. (arXiv:2105.11876v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>",
          "description": "Personalization of natural language generation plays a vital role in a large\nspectrum of tasks, such as explainable recommendation, review summarization and\ndialog systems. In these tasks, user and item IDs are important identifiers for\npersonalization. Transformer, which is demonstrated with strong language\nmodeling capability, however, is not personalized and fails to make use of the\nuser and item IDs since the ID tokens are not even in the same semantic space\nas the words. To address this problem, we present a PErsonalized Transformer\nfor Explainable Recommendation (PETER), on which we design a simple and\neffective learning objective that utilizes the IDs to predict the words in the\ntarget explanation, so as to endow the IDs with linguistic meanings and to\nachieve personalized Transformer. Besides generating explanations, PETER can\nalso make recommendations, which makes it a unified model for the whole\nrecommendation-explanation pipeline. Extensive experiments show that our small\nunpretrained model outperforms fine-tuned BERT on the generation task, in terms\nof both effectiveness and efficiency, which highlights the importance and the\nnice utility of our design.",
          "link": "http://arxiv.org/abs/2105.11601",
          "publishedOn": "2021-05-26T01:22:09.736Z",
          "wordCount": 595,
          "title": "Personalized Transformer for Explainable Recommendation. (arXiv:2105.11601v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.10252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wrzalik_M/0/1/0/all/0/1\">Marco Wrzalik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krechel_D/0/1/0/all/0/1\">Dirk Krechel</a>",
          "description": "Many recent approaches towards neural information retrieval mitigate their\ncomputational costs by using a multi-stage ranking pipeline. In the first\nstage, a number of potentially relevant candidates are retrieved using an\nefficient retrieval model such as BM25. Although BM25 has proven decent\nperformance as a first-stage ranker, it tends to miss relevant passages. In\nthis context we propose CoRT, a simple neural first-stage ranking model that\nleverages contextual representations from pretrained language models such as\nBERT to complement term-based ranking functions while causing no significant\ndelay at query time. Using the MS MARCO dataset, we show that CoRT\nsignificantly increases the candidate recall by complementing BM25 with missing\ncandidates. Consequently, we find subsequent re-rankers achieve superior\nresults with less candidates. We further demonstrate that passage retrieval\nusing CoRT can be realized with surprisingly low latencies.",
          "link": "http://arxiv.org/abs/2010.10252",
          "publishedOn": "2021-05-26T01:22:09.612Z",
          "wordCount": 619,
          "title": "CoRT: Complementary Rankings from Transformers. (arXiv:2010.10252v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_A/0/1/0/all/0/1\">Adam Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Yuwei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher G. Brinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanhua Li</a>",
          "description": "Existing topic modeling and text segmentation methodologies generally require\nlarge datasets for training, limiting their capabilities when only small\ncollections of text are available. In this work, we reexamine the inter-related\nproblems of \"topic identification\" and \"text segmentation\" for sparse document\nlearning, when there is a single new text of interest. In developing a\nmethodology to handle single documents, we face two major challenges. First is\nsparse information: with access to only one document, we cannot train\ntraditional topic models or deep learning algorithms. Second is significant\nnoise: a considerable portion of words in any single document will produce only\nnoise and not help discern topics or segments. To tackle these issues, we\ndesign an unsupervised, computationally efficient methodology called BATS:\nBiclustering Approach to Topic modeling and Segmentation. BATS leverages three\nkey ideas to simultaneously identify topics and segment text: (i) a new\nmechanism that uses word order information to reduce sample complexity, (ii) a\nstatistically sound graph-based biclustering technique that identifies latent\nstructures of words and sentences, and (iii) a collection of effective\nheuristics that remove noise words and award important words to further improve\nperformance. Experiments on four datasets show that our approach outperforms\nseveral state-of-the-art baselines when considering topic coherence, topic\ndiversity, segmentation, and runtime comparison metrics.",
          "link": "http://arxiv.org/abs/2008.02218",
          "publishedOn": "2021-05-26T01:22:09.598Z",
          "wordCount": 706,
          "title": "BATS: A Spectral Biclustering Approach to Single Document Topic Modeling and Segmentation. (arXiv:2008.02218v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunshan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yujuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Wai Keung Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jinyoung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>",
          "description": "This companion paper supports the replication of the fashion trend\nforecasting experiments with the KERN (Knowledge Enhanced Recurrent Network)\nmethod that we presented in the ICMR 2020. We provide an artifact that allows\nthe replication of the experiments using a Python implementation. The artifact\nis easy to deploy with simple installation, training and evaluation. We\nreproduce the experiments conducted in the original paper and obtain similar\nperformance as previously reported. The replication results of the experiments\nsupport the main claims in the original paper.",
          "link": "http://arxiv.org/abs/2105.11826",
          "publishedOn": "2021-05-26T01:22:09.076Z",
          "wordCount": 532,
          "title": "Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend Forecasting. (arXiv:2105.11826v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brochier_R/0/1/0/all/0/1\">Robin Brochier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bechet_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric B&#xe9;chet</a>",
          "description": "Wikipedia, the largest open-collaborative online encyclopedia, is a corpus of\ndocuments bound together by internal hyperlinks. These links form the building\nblocks of a large network whose structure contains important information on the\nconcepts covered in this encyclopedia. The presence of a link between two\narticles, materialised by an anchor text in the source page pointing to the\ntarget page, can increase readers' understanding of a topic. However, the\nprocess of linking follows specific editorial rules to avoid both under-linking\nand over-linking. In this paper, we study the transductive and the inductive\ntasks of link prediction on several subsets of the English Wikipedia and\nidentify some key challenges behind automatic linking based on anchor text\ninformation. We propose an appropriate evaluation sampling methodology and\ncompare several algorithms. Moreover, we propose baseline models that provide a\ngood estimation of the overall difficulty of the tasks.",
          "link": "http://arxiv.org/abs/2105.11734",
          "publishedOn": "2021-05-26T01:22:09.060Z",
          "wordCount": 590,
          "title": "Predicting Links on Wikipedia with Anchor Text Information. (arXiv:2105.11734v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>",
          "description": "Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.11866",
          "publishedOn": "2021-05-26T01:22:09.029Z",
          "wordCount": 580,
          "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khalaji_M/0/1/0/all/0/1\">Mostafa Khalaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadkhah_C/0/1/0/all/0/1\">Chitra Dadkhah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharibshah_J/0/1/0/all/0/1\">Joobin Gharibshah</a>",
          "description": "Recommender Systems are inevitable to personalize user's experiences on the\nInternet. They are using different approaches to recommend the Top-K items to\nusers according to their preferences. Nowadays recommender systems have become\none of the most important parts of largescale data mining techniques. In this\npaper, we propose a Hybrid Movie Recommender System (HMRS) based on Resource\nAllocation to improve the accuracy of recommendation and solve the cold start\nproblem for a new movie. HMRS-RA uses a self-organizing mapping neural network\nto clustering the users into N clusters. The users' preferences are different\naccording to their age and gender, therefore HMRS-RA is a combination of a\nContent-Based Method for solving the cold start problem for a new movie and a\nCollaborative Filtering model besides the demographic information of users. The\nexperimental results based on the MovieLens dataset show that the HMRS-RA\nincreases the accuracy of recommendation compared to the state-of-art and\nsimilar works.",
          "link": "http://arxiv.org/abs/2105.11678",
          "publishedOn": "2021-05-26T01:22:08.918Z",
          "wordCount": 593,
          "title": "Hybrid Movie Recommender System based on Resource Allocation. (arXiv:2105.11678v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santra_A/0/1/0/all/0/1\">Abhishek Santra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komar_K/0/1/0/all/0/1\">Kanthi Komar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmick_S/0/1/0/all/0/1\">Sanjukta Bhowmick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthy_S/0/1/0/all/0/1\">Sharma Chakravarthy</a>",
          "description": "Any large complex data analysis to infer or discover meaningful\ninformation/knowledge involves the following steps (in addition to data\ncollection, cleaning, preparing the data for analysis such as attribute\nelimination): i) Modeling the data -- an approach for modeling and deriving a\ndata representation for analysis using that approach, ii) translating analysis\nobjectives into computations on the model generated; this can be as simple as a\nsingle computation (e.g., community detection) or may involve a sequence of\noperations (e.g., pair-wise community detection over multiple networks) using\nexpressions based on the model, iii) computation of the expressions generated\n-- efficiency and scalability come into picture here, and iv) drill-down of\nresults to interpret or understand them clearly. Beyond this, it is also\nmeaningful to visualize results for easier understanding. Covid-19\nvisualization dashboard presented in this paper is an example of this.\n\nThis paper covers all of the above steps of data analysis life cycle using a\ndata representation that is gaining importance for multi-entity, multi-feature\ndata sets - Multilayer Networks. We use several data sets to establish the\neffectiveness of modeling using MLNs and analyze them using the proposed\ndecoupling approach. For coverage, we use different types of MLNs for modeling,\nand community and centrality computations for analysis. The data sets used - US\ncommercial airlines, IMDb, DBLP, and Covid-19 data set. Our experimental\nanalyses using the identified steps validate modeling, breadth of objectives\nthat can be computed, and overall versatility of the life cycle approach.\nCorrectness of results is verified, where possible, using independently\navailable ground truth. We demonstrate drill-down that is afforded by this\napproach (due to structure and semantics preservation) for a better\nunderstanding and visualization of results.",
          "link": "http://arxiv.org/abs/2105.11410",
          "publishedOn": "2021-05-25T01:56:07.724Z",
          "wordCount": 773,
          "title": "From Base Data To Knowledge Discovery -- A Life Cycle Approach -- Using Multilayer Networks. (arXiv:2105.11410v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2011.03327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pykl_S/0/1/0/all/0/1\">Srinivas Pykl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guptha_V/0/1/0/all/0/1\">Vineeth Guptha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_G/0/1/0/all/0/1\">Gitanjali Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news\nand rumors are rampant on social media. Believing in rumors can cause\nsignificant harm. This is further exacerbated at the time of a pandemic. To\ntackle this, we curate and release a manually annotated dataset of 10,700\nsocial media posts and articles of real and fake news on COVID-19. We benchmark\nthe annotated dataset with four machine learning baselines - Decision Tree,\nLogistic Regression, Gradient Boost, and Support Vector Machine (SVM). We\nobtain the best performance of 93.46% F1-score with SVM. The data and code is\navailable at: https://github.com/parthpatwa/covid19-fake-news-dectection",
          "link": "http://arxiv.org/abs/2011.03327",
          "publishedOn": "2021-05-25T01:56:07.682Z",
          "wordCount": 641,
          "title": "Fighting an Infodemic: COVID-19 Fake News Dataset. (arXiv:2011.03327v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tashman_M/0/1/0/all/0/1\">Michael Tashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">John Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiayi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fengdan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morsali_A/0/1/0/all/0/1\">Atefeh Morsali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winikor_L/0/1/0/all/0/1\">Lee Winikor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerami_R/0/1/0/all/0/1\">Rouzbeh Gerami</a>",
          "description": "Reinforcement learning (RL) is an effective technique for training\ndecision-making agents through interactions with their environment. The advent\nof deep learning has been associated with highly notable successes with\nsequential decision making problems - such as defeating some of the\nhighest-ranked human players at Go. In digital advertising, real-time bidding\n(RTB) is a common method of allocating advertising inventory through real-time\nauctions. Bidding strategies need to incorporate logic for dynamically\nadjusting parameters in order to deliver pre-assigned campaign goals. Here we\ndiscuss techniques toward using RL to train bidding agents. As a campaign\nmetric we particularly focused on viewability: the percentage of inventory\nwhich goes on to be viewed by an end user.\n\nThis paper is presented as a survey of techniques and experiments which we\ndeveloped through the course of this research. We discuss expanding our\ntraining data to include edge cases by training on simulated interactions. We\ndiscuss the experimental results comparing the performance of several promising\nRL algorithms, and an approach to hyperparameter optimization of an\nactor/critic training pipeline through Bayesian optimization. Finally, we\npresent live-traffic tests of some of our RL agents against a rule-based\nfeedback-control approach, demonstrating the potential for this method as well\nas areas for further improvement. This paper therefore presents an arrangement\nof our findings in this quickly developing field, and ways that it can be\napplied to an RTB use case.",
          "link": "http://arxiv.org/abs/2105.10587",
          "publishedOn": "2021-05-25T01:56:07.674Z",
          "wordCount": 673,
          "title": "Techniques Toward Optimizing Viewability in RTB Ad Campaigns Using Reinforcement Learning. (arXiv:2105.10587v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Recommender systems usually amplify the biases in the data. The model learned\nfrom historical interactions with imbalanced item distribution will amplify the\nimbalance by over-recommending items from the major groups. Addressing this\nissue is essential for a healthy ecosystem of recommendation in the long run.\nExisting works apply bias control to the ranking targets (e.g., calibration,\nfairness, and diversity), but ignore the true reason for bias amplification and\ntrade-off the recommendation accuracy.\n\nIn this work, we scrutinize the cause-effect factors for bias amplification,\nidentifying the main reason lies in the confounder effect of imbalanced item\ndistribution on user representation and prediction score. The existence of such\nconfounder pushes us to go beyond merely modeling the conditional probability\nand embrace the causal modeling for recommendation. Towards this end, we\npropose a Deconfounded Recommender System (DecRS), which models the causal\neffect of user representation on the prediction score. The key to eliminating\nthe impact of the confounder lies in backdoor adjustment, which is however\ndifficult to do due to the infinite sample space of the confounder. For this\nchallenge, we contribute an approximation operator for backdoor adjustment\nwhich can be easily plugged into most recommender models. Lastly, we devise an\ninference strategy to dynamically regulate backdoor adjustment according to\nuser status. We instantiate DecRS on two representative models FM and NFM, and\nconduct extensive experiments over two benchmarks to validate the superiority\nof our proposed DecRS.",
          "link": "http://arxiv.org/abs/2105.10648",
          "publishedOn": "2021-05-25T01:56:07.633Z",
          "wordCount": 674,
          "title": "Deconfounded Recommendation for Alleviating Bias Amplification. (arXiv:2105.10648v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corney_D/0/1/0/all/0/1\">David Corney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1\">Maram Hasanain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1\">Tamer Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_Cedeno_A/0/1/0/all/0/1\">Alberto Barr&#xf3;n-Cede&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papotti_P/0/1/0/all/0/1\">Paolo Papotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>",
          "description": "The reporting and the analysis of current events around the globe has\nexpanded from professional, editor-lead journalism all the way to citizen\njournalism. Nowadays, politicians and other key players enjoy direct access to\ntheir audiences through social media, bypassing the filters of official cables\nor traditional media. However, the multiple advantages of free speech and\ndirect communication are dimmed by the misuse of media to spread inaccurate or\nmisleading claims. These phenomena have led to the modern incarnation of the\nfact-checker -- a professional whose main aim is to examine claims using\navailable evidence and to assess their veracity. As in other text forensics\ntasks, the amount of information available makes the work of the fact-checker\nmore difficult. With this in mind, starting from the perspective of the\nprofessional fact-checker, we survey the available intelligent technologies\nthat can support the human expert in the different steps of her fact-checking\nendeavor. These include identifying claims worth fact-checking, detecting\nrelevant previously fact-checked claims, retrieving relevant evidence to\nfact-check a claim, and actually verifying a claim. In each case, we pay\nattention to the challenges in future work and the potential impact on\nreal-world fact-checking.",
          "link": "http://arxiv.org/abs/2103.07769",
          "publishedOn": "2021-05-25T01:56:07.584Z",
          "wordCount": 692,
          "title": "Automated Fact-Checking for Assisting Human Fact-Checkers. (arXiv:2103.07769v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1\">Da-Cheng Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Che Zheng</a>",
          "description": "The dot product self-attention is known to be central and indispensable to\nstate-of-the-art Transformer models. But is it really required? This paper\ninvestigates the true importance and contribution of the dot product-based\nself-attention mechanism on the performance of Transformer models. Via\nextensive experiments, we find that (1) random alignment matrices surprisingly\nperform quite competitively and (2) learning attention weights from token-token\n(query-key) interactions is useful but not that important after all. To this\nend, we propose \\textsc{Synthesizer}, a model that learns synthetic attention\nweights without token-token interactions. In our experiments, we first show\nthat simple Synthesizers achieve highly competitive performance when compared\nagainst vanilla Transformer models across a range of tasks, including machine\ntranslation, language modeling, text generation and GLUE/SuperGLUE benchmarks.\nWhen composed with dot product attention, we find that Synthesizers\nconsistently outperform Transformers. Moreover, we conduct additional\ncomparisons of Synthesizers against Dynamic Convolutions, showing that simple\nRandom Synthesizer is not only $60\\%$ faster but also improves perplexity by a\nrelative $3.5\\%$. Finally, we show that simple factorized Synthesizers can\noutperform Linformers on encoding only tasks.",
          "link": "http://arxiv.org/abs/2005.00743",
          "publishedOn": "2021-05-25T01:56:07.532Z",
          "wordCount": 657,
          "title": "Synthesizer: Rethinking Self-Attention in Transformer Models. (arXiv:2005.00743v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_X/0/1/0/all/0/1\">Xuan Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yiqian Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kaize Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1\">Angen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuxiang Luo</a>",
          "description": "Deep matching models aim to facilitate search engines retrieving more\nrelevant documents by mapping queries and documents into semantic vectors in\nthe first-stage retrieval. When leveraging BERT as the deep matching model, the\nattention score across two words are solely built upon local contextualized\nword embeddings. It lacks prior global knowledge to distinguish the importance\nof different words, which has been proved to play a critical role in\ninformation retrieval tasks. In addition to this, BERT only performs attention\nacross sub-words tokens which weakens whole word attention representation. We\npropose a novel Global Weighted Self-Attention (GLOW) network for web document\nsearch. GLOW fuses global corpus statistics into the deep matching model. By\nadding prior weights into attention generation from global information, like\nBM25, GLOW successfully learns weighted attention scores jointly with query\nmatrix $Q$ and key matrix $K$. We also present an efficient whole word weight\nsharing solution to bring prior whole word knowledge into sub-words level\nattention. It aids Transformer to learn whole word level attention. To make our\nmodels applicable to complicated web search scenarios, we introduce combined\nfields representation to accommodate documents with multiple fields even with\nvariable number of instances. We demonstrate GLOW is more efficient to capture\nthe topical and semantic representation both in queries and documents.\nIntrinsic evaluation and experiments conducted on public data sets reveal GLOW\nto be a general framework for document retrieve task. It significantly\noutperforms BERT and other competitive baselines by a large margin while\nretaining the same model complexity with BERT.",
          "link": "http://arxiv.org/abs/2007.05186",
          "publishedOn": "2021-05-25T01:56:07.491Z",
          "wordCount": 725,
          "title": "GLOW : Global Weighted Self-Attention Network for Web Search. (arXiv:2007.05186v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Recommendation is a prevalent and critical service in information systems. To\nprovide personalized suggestions to users, industry players embrace machine\nlearning, more specifically, building predictive models based on the click\nbehavior data. This is known as the Click-Through Rate (CTR) prediction, which\nhas become the gold standard for building personalized recommendation service.\nHowever, we argue that there is a significant gap between clicks and user\nsatisfaction -- it is common that a user is \"cheated\" to click an item by the\nattractive title/cover of the item. This will severely hurt user's trust on the\nsystem if the user finds the actual content of the clicked item disappointing.\nWhat's even worse, optimizing CTR models on such flawed data will result in the\nMatthew Effect, making the seemingly attractive but actually low-quality items\nbe more frequently recommended.\n\nIn this paper, we formulate the recommendation models as a causal graph that\nreflects the cause-effect factors in recommendation, and address the clickbait\nissue by performing counterfactual inference on the causal graph. We imagine a\ncounterfactual world where each item has only exposure features (i.e., the\nfeatures that the user can see before making a click decision). By estimating\nthe click likelihood of a user in the counterfactual world, we are able to\nreduce the direct effect of exposure features and eliminate the clickbait\nissue. Experiments on real-world datasets demonstrate that our method\nsignificantly improves the post-click satisfaction of CTR models.",
          "link": "http://arxiv.org/abs/2009.09945",
          "publishedOn": "2021-05-25T01:56:07.473Z",
          "wordCount": 740,
          "title": "Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue. (arXiv:2009.09945v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1\">Peng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinger Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>",
          "description": "In most real-world large-scale online applications (e.g., e-commerce or\nfinance), customer acquisition is usually a multi-step conversion process of\naudiences. For example, an impression->click->purchase process is usually\nperformed of audiences for e-commerce platforms. However, it is more difficult\nto acquire customers in financial advertising (e.g., credit card advertising)\nthan in traditional advertising. On the one hand, the audience multi-step\nconversion path is longer. On the other hand, the positive feedback is sparser\n(class imbalance) step by step, and it is difficult to obtain the final\npositive feedback due to the delayed feedback of activation. Multi-task\nlearning is a typical solution in this direction. While considerable multi-task\nefforts have been made in this direction, a long-standing challenge is how to\nexplicitly model the long-path sequential dependence among audience multi-step\nconversions for improving the end-to-end conversion. In this paper, we propose\nan Adaptive Information Transfer Multi-task (AITM) framework, which models the\nsequential dependence among audience multi-step conversions via the Adaptive\nInformation Transfer (AIT) module. The AIT module can adaptively learn what and\nhow much information to transfer for different conversion stages. Besides, by\ncombining the Behavioral Expectation Calibrator in the loss function, the AITM\nframework can yield more accurate end-to-end conversion identification. The\nproposed framework is deployed in Meituan app, which utilizes it to real-timely\nshow a banner to the audience with a high end-to-end conversion rate for\nMeituan Co-Branded Credit Cards. Offline experimental results on both\nindustrial and public real-world datasets clearly demonstrate that the proposed\nframework achieves significantly better performance compared with\nstate-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2105.08489",
          "publishedOn": "2021-05-25T01:56:07.438Z",
          "wordCount": 722,
          "title": "Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising. (arXiv:2105.08489v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.10084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pobrotyn_P/0/1/0/all/0/1\">Przemys&#x142;aw Pobrotyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartczak_T/0/1/0/all/0/1\">Tomasz Bartczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synowiec_M/0/1/0/all/0/1\">Miko&#x142;aj Synowiec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bialobrzeski_R/0/1/0/all/0/1\">Rados&#x142;aw Bia&#x142;obrzeski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_J/0/1/0/all/0/1\">Jaros&#x142;aw Bojar</a>",
          "description": "Learning to rank is a key component of many e-commerce search engines. In\nlearning to rank, one is interested in optimising the global ordering of a list\nof items according to their utility for users.Popular approaches learn a\nscoring function that scores items individually (i.e. without the context of\nother items in the list) by optimising a pointwise, pairwise or listwise loss.\nThe list is then sorted in the descending order of the scores. Possible\ninteractions between items present in the same list are taken into account in\nthe training phase at the loss level. However, during inference, items are\nscored individually, and possible interactions between them are not considered.\nIn this paper, we propose a context-aware neural network model that learns item\nscores by applying a self-attention mechanism. The relevance of a given item is\nthus determined in the context of all other items present in the list, both in\ntraining and in inference. We empirically demonstrate significant performance\ngains of self-attention based neural architecture over Multi-LayerPerceptron\nbaselines, in particular on a dataset coming from search logs of a large scale\ne-commerce marketplace, Allegro.pl. This effect is consistent across popular\npointwise, pairwise and listwise losses.Finally, we report new state-of-the-art\nresults on MSLR-WEB30K, the learning to rank benchmark.",
          "link": "http://arxiv.org/abs/2005.10084",
          "publishedOn": "2021-05-25T01:56:07.424Z",
          "wordCount": 683,
          "title": "Context-Aware Learning to Rank with Self-Attention. (arXiv:2005.10084v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Arkadipta De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.",
          "link": "http://arxiv.org/abs/2105.11347",
          "publishedOn": "2021-05-25T01:56:07.414Z",
          "wordCount": 634,
          "title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Huifeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiuqiang He</a>",
          "description": "Click-Through Rate (CTR) prediction is critical for industrial recommender\nsystems, where most deep CTR models follow an Embedding \\& Feature Interaction\nparadigm. However, the majority of methods focus on designing network\narchitectures to better capture feature interactions while the feature\nembedding, especially for numerical features, has been overlooked. Existing\napproaches for numerical features are difficult to capture informative\nknowledge because of the low capacity or hard discretization based on the\noffline expertise feature engineering. In this paper, we propose a novel\nembedding learning framework for numerical features in CTR prediction (AutoDis)\nwith high model capacity, end-to-end training and unique representation\nproperties preserved. AutoDis consists of three core components:\nmeta-embeddings, automatic discretization and aggregation. Specifically, we\npropose meta-embeddings for each numerical field to learn global knowledge from\nthe perspective of field with a manageable number of parameters. Then the\ndifferentiable automatic discretization performs soft discretization and\ncaptures the correlations between the numerical features and meta-embeddings.\nFinally, distinctive and informative embeddings are learned via an aggregation\nfunction. Comprehensive experiments on two public and one industrial datasets\nare conducted to validate the effectiveness of AutoDis. Moreover, AutoDis has\nbeen deployed onto a mainstream advertising platform, where online A/B test\ndemonstrates the improvement over the base model by 2.1% and 2.7% in terms of\nCTR and eCPM, respectively. In addition, the code of our framework is publicly\navailable in\nMindSpore(https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/recommend/autodis).",
          "link": "http://arxiv.org/abs/2012.08986",
          "publishedOn": "2021-05-25T01:56:07.396Z",
          "wordCount": 702,
          "title": "An Embedding Learning Framework for Numerical Features in CTR Prediction. (arXiv:2012.08986v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Seongwon Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hoyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsouk Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Sehee Chung</a>",
          "description": "Sequential recommendation techniques provide users with product\nrecommendations fitting their current preferences by handling dynamic user\npreferences over time. Previous studies have focused on modeling sequential\ndynamics without much regard to which of the best-selling products (i.e., head\nitems) or niche products (i.e., tail items) should be recommended. We\nscrutinize the structural reason for why tail items are barely served in the\ncurrent sequential recommendation model, which consists of an item-embedding\nlayer, a sequence-modeling layer, and a recommendation layer. Well-designed\nsequence-modeling and recommendation layers are expected to naturally learn\nsuitable item embeddings. However, tail items are likely to fall short of this\nexpectation because the current model structure is not suitable for learning\nhigh-quality embeddings with insufficient data. Thus, tail items are rarely\nrecommended. To eliminate this issue, we propose a framework called CITIES,\nwhich aims to enhance the quality of the tail-item embeddings by training an\nembedding-inference function using multiple contextual head items so that the\nrecommendation performance improves for not only the tail items but also for\nthe head items. Moreover, our framework can infer new-item embeddings without\nan additional learning process. Extensive experiments on two real-world\ndatasets show that applying CITIES to the state-of-the-art methods improves\nrecommendation performance for both tail and head items. We conduct an\nadditional experiment to verify that CITIES can infer suitable new-item\nembeddings as well.",
          "link": "http://arxiv.org/abs/2105.10868",
          "publishedOn": "2021-05-25T01:56:07.386Z",
          "wordCount": 658,
          "title": "CITIES: Contextual Inference of Tail-Item Embeddings for Sequential Recommendation. (arXiv:2105.10868v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pobrotyn_P/0/1/0/all/0/1\">Przemys&#x142;aw Pobrotyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bialobrzeski_R/0/1/0/all/0/1\">Rados&#x142;aw Bia&#x142;obrzeski</a>",
          "description": "Learning to Rank (LTR) algorithms are usually evaluated using Information\nRetrieval metrics like Normalised Discounted Cumulative Gain (NDCG) or Mean\nAverage Precision. As these metrics rely on sorting predicted items' scores\n(and thus, on items' ranks), their derivatives are either undefined or zero\neverywhere. This makes them unsuitable for gradient-based optimisation, which\nis the usual method of learning appropriate scoring functions. Commonly used\nLTR loss functions are only loosely related to the evaluation metrics, causing\na mismatch between the optimisation objective and the evaluation criterion. In\nthis paper, we address this mismatch by proposing NeuralNDCG, a novel\ndifferentiable approximation to NDCG. Since NDCG relies on the\nnon-differentiable sorting operator, we obtain NeuralNDCG by relaxing that\noperator using NeuralSort, a differentiable approximation of sorting. As a\nresult, we obtain a new ranking loss function which is an arbitrarily accurate\napproximation to the evaluation metric, thus closing the gap between the\ntraining and the evaluation of LTR models. We introduce two variants of the\nproposed loss function. Finally, the empirical evaluation shows that our\nproposed method outperforms previous work aimed at direct optimisation of NDCG\nand is competitive with the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.07831",
          "publishedOn": "2021-05-25T01:56:07.362Z",
          "wordCount": 648,
          "title": "NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable Relaxation of Sorting. (arXiv:2102.07831v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1902.08882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1\">Ryuichi Takanobu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1\">Tao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haihong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>",
          "description": "In this paper, we investigate the task of aggregating search results from\nheterogeneous sources in an E-commerce environment. First, unlike traditional\naggregated web search that merely presents multi-sourced results in the first\npage, this new task may present aggregated results in all pages and has to\ndynamically decide which source should be presented in the current page.\nSecond, as pointed out by many existing studies, it is not trivial to rank\nitems from heterogeneous sources because the relevance scores from different\nsource systems are not directly comparable. To address these two issues, we\ndecompose the task into two subtasks in a hierarchical structure: a high-level\ntask for source selection where we model the sequential patterns of user\nbehaviors onto aggregated results in different pages so as to understand user\nintents and select the relevant sources properly; and a low-level task for item\npresentation where we formulate a slot filling process to sequentially present\nthe items instead of giving each item a relevance score when deciding the\npresentation order of heterogeneous items. Since both subtasks can be naturally\nformulated as sequential decision problems and learn from the future user\nfeedback on search results, we build our model with hierarchical reinforcement\nlearning. Extensive experiments demonstrate that our model obtains remarkable\nimprovements in search performance metrics, and achieves a higher user\nsatisfaction.",
          "link": "http://arxiv.org/abs/1902.08882",
          "publishedOn": "2021-05-25T01:56:07.346Z",
          "wordCount": 683,
          "title": "Aggregating E-commerce Search Results from Heterogeneous Sources via Hierarchical Reinforcement Learning. (arXiv:1902.08882v1 [cs.IR] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>",
          "description": "Event Detection (ED) aims to identify event trigger words from a given text\nand classify it into an event type. Most of current methods to ED rely heavily\non training instances, and almost ignore the correlation of event types. Hence,\nthey tend to suffer from data scarcity and fail to handle new unseen event\ntypes. To address these problems, we formulate ED as a process of event\nontology population: linking event instances to pre-defined event types in\nevent ontology, and propose a novel ED framework entitled OntoED with ontology\nembedding. We enrich event ontology with linkages among event types, and\nfurther induce more event-event correlations. Based on the event ontology,\nOntoED can leverage and propagate correlation knowledge, particularly from\ndata-rich to data-poor event types. Furthermore, OntoED can be applied to new\nunseen event types, by establishing linkages to existing ones. Experiments\nindicate that OntoED is more predominant and robust than previous approaches to\nED, especially in data-scarce scenarios.",
          "link": "http://arxiv.org/abs/2105.10922",
          "publishedOn": "2021-05-25T01:56:07.333Z",
          "wordCount": 598,
          "title": "OntoED: Low-resource Event Detection with Ontology Embedding. (arXiv:2105.10922v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouhollahi_Z/0/1/0/all/0/1\">Zeinab Rouhollahi</a>",
          "description": "Recently, financial institutes have been dealing with an increase in\nfinancial crimes. In this context, financial services firms started to improve\ntheir vigilance and use new technologies and approaches to identify and predict\nfinancial fraud and crime possibilities. This task is challenging as\ninstitutions need to upgrade their data and analytics capabilities to enable\nnew technologies such as Artificial Intelligence (AI) to predict and detect\nfinancial crimes. In this paper, we put a step towards AI-enabled financial\ncrime detection in general and money laundering detection in particular to\naddress this challenge. We study and analyse the recent works done in financial\ncrime detection and present a novel model to detect money laundering cases with\nminimum human intervention needs.",
          "link": "http://arxiv.org/abs/2105.10866",
          "publishedOn": "2021-05-25T01:56:07.304Z",
          "wordCount": 543,
          "title": "Towards Artificial Intelligence Enabled Financial Crime Detection. (arXiv:2105.10866v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1\">Hermawan Mulyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">Desmond Chan</a>",
          "description": "Climate change has largely impacted our daily lives. As one of its\nconsequences, we are experiencing more wildfires. In the year 2020, wildfires\nburned a record number of 8,888,297 acres in the US. To awaken people's\nattention to climate change, and to visualize the current risk of wildfires, We\ndeveloped RtFPS, \"Real-Time Fire Prediction System\". It provides a real-time\nprediction visualization of wildfire risk at specific locations base on a\nMachine Learning model. It also provides interactive map features that show the\nhistorical wildfire events with environmental info.",
          "link": "http://arxiv.org/abs/2105.10880",
          "publishedOn": "2021-05-25T01:56:07.229Z",
          "wordCount": 545,
          "title": "RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lixin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hengyi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Dehong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Suqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daiting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhicong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>",
          "description": "As the heart of a search engine, the ranking system plays a crucial role in\nsatisfying users' information demands. More recently, neural rankers fine-tuned\nfrom pre-trained language models (PLMs) establish state-of-the-art ranking\neffectiveness. However, it is nontrivial to directly apply these PLM-based\nrankers to the large-scale web search system due to the following challenging\nissues:(1) the prohibitively expensive computations of massive neural PLMs,\nespecially for long texts in the web-document, prohibit their deployments in an\nonline ranking system that demands extremely low latency;(2) the discrepancy\nbetween existing ranking-agnostic pre-training objectives and the ad-hoc\nretrieval scenarios that demand comprehensive relevance modeling is another\nmain barrier for improving the online ranking system;(3) a real-world search\nengine typically involves a committee of ranking components, and thus the\ncompatibility of the individually fine-tuned ranking model is critical for a\ncooperative ranking system. In this work, we contribute a series of\nsuccessfully applied techniques in tackling these exposed issues when deploying\nthe state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the\nonline search engine system. We first articulate a novel practice to\ncost-efficiently summarize the web document and contextualize the resultant\nsummary content with the query using a cheap yet powerful Pyramid-ERNIE\narchitecture. Then we endow an innovative paradigm to finely exploit the\nlarge-scale noisy and biased post-click behavioral data for relevance-oriented\npre-training. We also propose a human-anchored fine-tuning strategy tailored\nfor the online ranking system, aiming to stabilize the ranking signals across\nvarious online components. Extensive offline and online experimental results\nshow that the proposed techniques significantly boost the search engine's\nperformance.",
          "link": "http://arxiv.org/abs/2105.11108",
          "publishedOn": "2021-05-25T01:56:07.201Z",
          "wordCount": 701,
          "title": "Pre-trained Language Model based Ranking in Baidu Search. (arXiv:2105.11108v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.01910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyusong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>",
          "description": "Although open-domain question answering (QA) draws great attention in recent\nyears, it requires large amounts of resources for building the full system and\nis often difficult to reproduce previous results due to complex configurations.\nIn this paper, we introduce SF-QA: simple and fair evaluation framework for\nopen-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,\nwhich makes the task itself easily accessible and reproducible to research\ngroups without enough computing resources. The proposed evaluation framework is\npublicly available and anyone can contribute to the code and evaluations.",
          "link": "http://arxiv.org/abs/2101.01910",
          "publishedOn": "2021-05-24T05:08:38.800Z",
          "wordCount": 555,
          "title": "SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering. (arXiv:2101.01910v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Ze Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinnian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yumeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiancheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tanchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lifeng Sun</a>",
          "description": "Modeling powerful interactions is a critical challenge in Click-through rate\n(CTR) prediction, which is one of the most typical machine learning tasks in\npersonalized advertising and recommender systems. Although developing\nhand-crafted interactions is effective for a small number of datasets, it\ngenerally requires laborious and tedious architecture engineering for extensive\nscenarios. In recent years, several neural architecture search (NAS) methods\nhave been proposed for designing interactions automatically. However, existing\nmethods only explore limited types and connections of operators for interaction\ngeneration, leading to low generalization ability. To address these problems,\nwe propose a more general automated method for building powerful interactions\nnamed AutoPI. The main contributions of this paper are as follows: AutoPI\nadopts a more general search space in which the computational graph is\ngeneralized from existing network connections, and the interactive operators in\nthe edges of the graph are extracted from representative hand-crafted works. It\nallows searching for various powerful feature interactions to produce higher\nAUC and lower Logloss in a wide variety of applications. Besides, AutoPI\nutilizes a gradient-based search strategy for exploration with a significantly\nlow computational cost. Experimentally, we evaluate AutoPI on a diverse suite\nof benchmark datasets, demonstrating the generalizability and efficiency of\nAutoPI over hand-crafted architectures and state-of-the-art NAS algorithms.",
          "link": "http://arxiv.org/abs/2105.10484",
          "publishedOn": "2021-05-24T05:08:38.775Z",
          "wordCount": 653,
          "title": "A General Method For Automatic Discovery of Powerful Interactions In Click-Through Rate Prediction. (arXiv:2105.10484v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Keke Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1\">Linjian Mo</a>",
          "description": "Click-Through Rate (CTR) prediction plays an important role in many\nindustrial applications, and recently a lot of attention is paid to the deep\ninterest models which use attention mechanism to capture user interests from\nhistorical behaviors. However, most current models are based on sequential\nmodels which truncate the behavior sequences by a fixed length, thus have\ndifficulties in handling very long behavior sequences. Another big problem is\nthat sequences with the same length can be quite different in terms of time,\ncarrying completely different meanings. In this paper, we propose a\nnon-sequential approach to tackle the above problems. Specifically, we first\nrepresent the behavior data in a sparse key-vector format, where the vector\ncontains rich behavior info such as time, count and category. Next, we enhance\nthe Deep Interest Network to take such rich information into account by a novel\nattention network. The sparse representation makes it practical to handle large\nscale long behavior sequences. Finally, we introduce a multidimensional\npartition framework to mine behavior interactions. The framework can partition\ndata into custom designed time buckets to capture the interactions among\ninformation aggregated in different time buckets. Similarly, it can also\npartition the data into different categories and capture the interactions among\nthem. Experiments are conducted on two public datasets: one is an advertising\ndataset and the other is a production recommender dataset. Our models\noutperform other state-of-the-art models on both datasets.",
          "link": "http://arxiv.org/abs/2104.06312",
          "publishedOn": "2021-05-24T05:08:38.657Z",
          "wordCount": 689,
          "title": "A Non-sequential Approach to Deep User Interest Model for CTR Prediction. (arXiv:2104.06312v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolbitsch_M/0/1/0/all/0/1\">Matthias W&#xf6;lbitsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasler_T/0/1/0/all/0/1\">Thomas Hasler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasper_P/0/1/0/all/0/1\">Patrick Kasper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helic_D/0/1/0/all/0/1\">Denis Helic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walk_S/0/1/0/all/0/1\">Simon Walk</a>",
          "description": "In recent years, Radio Frequency Identification (RFID) technology has been\napplied to improve numerous processes, such as inventory management in retail\nstores. However, automatic localization of RFID-tagged goods in stores is still\na challenging problem. To address this issue, we equip fixtures (e.g., shelves)\nwith reference tags and use data we collect during RFID-based stocktakes to map\narticles to fixtures. Knowing the location of goods enables the implementation\nof several practical applications, such as automated Money Mapping (i.e., a\nheat map of sales across fixtures). Specifically, we conduct controlled lab\nexperiments and a case-study in two fashion retail stores to evaluate our\narticle-to-fixture prediction approaches. The approaches are based on\ncalculating distances between read event time series using DTW, and clustering\nof read events using DBSCAN. We find that, read events collected during\nRFID-based stocktakes can be used to assign articles to fixtures with an\naccuracy of more than 90%. Additionally, we conduct a pilot to investigate the\nchallenges related to the integration of such a localization system in the\nday-to-day business of retail stores. Hence, in this paper we present an\nexploratory venture into novel and practical RFID-based applications in fashion\nretails stores, beyond the scope of stock management.",
          "link": "http://arxiv.org/abs/2105.10216",
          "publishedOn": "2021-05-24T05:08:38.622Z",
          "wordCount": 632,
          "title": "RFID-based Article-to-Fixture Predictions in Real-World Fashion Stores. (arXiv:2105.10216v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianghong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1\">Eugene Agichtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallumadi_S/0/1/0/all/0/1\">Surya Kallumadi</a>",
          "description": "In search and recommendation, diversifying the multi-aspect search results\ncould help with reducing redundancy, and promoting results that might not be\nshown otherwise. Many previous methods have been proposed for this task.\nHowever, previous methods do not explicitly consider the uniformity of the\nnumber of the items' classes, or evenness, which could degrade the search and\nrecommendation quality. To address this problem, we introduce a novel method by\nadapting the Simpson's Diversity Index from biology, which enables a more\neffective and efficient quadratic search result diversification algorithm. We\nalso extend the method to balance the diversity between multiple aspects\nthrough weighted factors and further improve computational complexity by\ndeveloping a fast approximation algorithm. We demonstrate the feasibility of\nthe proposed method using the openly available Kaggle shoes competition\ndataset. Our experimental results show that our approach outperforms previous\nstate of the art diversification methods, while reducing computational\ncomplexity.",
          "link": "http://arxiv.org/abs/2105.10075",
          "publishedOn": "2021-05-24T05:08:38.573Z",
          "wordCount": 612,
          "title": "Diversifying Multi-aspect Search Results Using Simpson's Diversity Index. (arXiv:2105.10075v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10019",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Poh_D/0/1/0/all/0/1\">Daniel Poh</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Lim_B/0/1/0/all/0/1\">Bryan Lim</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "The performance of a cross-sectional currency strategy depends crucially on\naccurately ranking instruments prior to portfolio construction. While this\nranking step is traditionally performed using heuristics, or by sorting outputs\nproduced by pointwise regression or classification models, Learning to Rank\nalgorithms have recently presented themselves as competitive and viable\nalternatives. Despite improving ranking accuracy on average however, these\ntechniques do not account for the possibility that assets positioned at the\nextreme ends of the ranked list -- which are ultimately used to construct the\nlong/short portfolios -- can assume different distributions in the input space,\nand thus lead to sub-optimal strategy performance. Drawing from research in\nInformation Retrieval that demonstrates the utility of contextual information\nembedded within top-ranked documents to learn the query's characteristics to\nimprove ranking, we propose an analogous approach: exploiting the features of\nboth out- and under-performing instruments to learn a model for refining the\noriginal ranked list. Under a re-ranking framework, we adapt the Transformer\narchitecture to encode the features of extreme assets for refining our\nselection of long/short instruments obtained with an initial retrieval.\nBacktesting on a set of 31 currencies, our proposed methodology significantly\nboosts Sharpe ratios -- by approximately 20% over the original LTR algorithms\nand double that of traditional baselines.",
          "link": "http://arxiv.org/abs/2105.10019",
          "publishedOn": "2021-05-24T05:08:38.317Z",
          "wordCount": 655,
          "title": "Enhancing Cross-Sectional Currency Strategies by Ranking Refinement with Transformer-based Architectures. (arXiv:2105.10019v1 [q-fin.PM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianghong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1\">Eugene Agichtein</a>",
          "description": "To support complex search tasks, where the initial information requirements\nare complex or may change during the search, a search engine must adapt the\ninformation delivery as the user's information requirements evolve. To support\nthis dynamic ranking paradigm effectively, search result ranking must\nincorporate both the user feedback received, and the information displayed so\nfar. To address this problem, we introduce a novel reinforcement learning-based\napproach, RLIrank. We first build an adapted reinforcement learning framework\nto integrate the key components of the dynamic search. Then, we implement a new\nLearning to Rank (LTR) model for each iteration of the dynamic search, using a\nrecurrent Long Short Term Memory neural network (LSTM), which estimates the\ngain for each next result, learning from each previously ranked document. To\nincorporate the user's feedback, we develop a word-embedding variation of the\nclassic Rocchio Algorithm, to help guide the ranking towards the high-value\ndocuments. Those innovations enable RLIrank to outperform the previously\nreported methods from the TREC Dynamic Domain Tracks 2017 and exceed all the\nmethods in 2016 TREC Dynamic Domain after multiple search iterations, advancing\nthe state of the art for dynamic search.",
          "link": "http://arxiv.org/abs/2105.10124",
          "publishedOn": "2021-05-24T05:08:38.302Z",
          "wordCount": 642,
          "title": "RLIRank: Learning to Rank with Reinforcement Learning for Dynamic Search. (arXiv:2105.10124v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. A. Gloor</a>",
          "description": "This paper investigates the research question if senders of large amounts of\nirrelevant or unsolicited information - commonly called \"spammers\" - distort\nthe network structure of social networks. Two large social networks are\nanalyzed, the first extracted from the Twitter discourse about a big\ntelecommunication company, and the second obtained from three years of email\ncommunication of 200 managers working for a large multinational company. This\nwork compares network robustness and the stability of centrality and\ninteraction metrics, as well as the use of language, after removing spammers\nand the most and least connected nodes. The results show that spammers do not\nsignificantly alter the structure of the information-carrying network, for most\nof the social indicators. The authors additionally investigate the correlation\nbetween e-mail subject line and content by tracking language sentiment,\nemotionality, and complexity, addressing the cases where collecting email\nbodies is not permitted for privacy reasons. The findings extend the research\nabout robustness and stability of social networks metrics, after the\napplication of graph simplification strategies. The results have practical\nimplication for network analysts and for those company managers who rely on\nnetwork analytics (applied to company emails and social media data) to support\ntheir decision-making processes.",
          "link": "http://arxiv.org/abs/2105.10256",
          "publishedOn": "2021-05-24T05:08:38.286Z",
          "wordCount": 656,
          "title": "Measuring the impact of spammers on e-mail and Twitter networks. (arXiv:2105.10256v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kawintiranon_K/0/1/0/all/0/1\">Kornraphop Kawintiranon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaguang Liu</a>",
          "description": "General Data Protection Regulation (GDPR) becomes a standard law for data\nprotection in many countries. Currently, twelve countries adopt the regulation\nand establish their GDPR-like regulation. However, to evaluate the differences\nand similarities of these GDPR-like regulations is time-consuming and needs a\nlot of manual effort from legal experts. Moreover, GDPR-like regulations from\ndifferent countries are written in their languages leading to a more difficult\ntask since legal experts who know both languages are essential. In this paper,\nwe investigate a simple natural language processing (NLP) approach to tackle\nthe problem. We first extract chunks of information from GDPR-like documents\nand form structured data from natural language. Next, we use NLP methods to\ncompare documents to measure their similarity. Finally, we manually label a\nsmall set of data to evaluate our approach. The empirical result shows that the\nBERT model with cosine similarity outperforms other baselines. Our data and\ncode are publicly available.",
          "link": "http://arxiv.org/abs/2105.10117",
          "publishedOn": "2021-05-24T05:08:38.223Z",
          "wordCount": 600,
          "title": "Towards Automatic Comparison of Data Privacy Documents: A Preliminary Experiment on GDPR-like Laws. (arXiv:2105.10117v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianghong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahiri_S/0/1/0/all/0/1\">Sayyed M. Zahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_S/0/1/0/all/0/1\">Simon Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jadda_K/0/1/0/all/0/1\">Khalifeh Al Jadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallumadi_S/0/1/0/all/0/1\">Surya Kallumadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1\">Eugene Agichtein</a>",
          "description": "Users' clicks on Web search results are one of the key signals for evaluating\nand improving web search quality and have been widely used as part of current\nstate-of-the-art Learning-To-Rank(LTR) models. With a large volume of search\nlogs available for major search engines, effective models of searcher click\nbehavior have emerged to evaluate and train LTR models. However, when modeling\nthe users' click behavior, considering the bias of the behavior is imperative.\nIn particular, when a search result is not clicked, it is not necessarily\nchosen as not relevant by the user, but instead could have been simply missed,\nespecially for lower-ranked results. These kinds of biases in the click log\ndata can be incorporated into the click models, propagating the errors to the\nresulting LTR ranking models or evaluation metrics. In this paper, we propose\nthe De-biased Reinforcement Learning Click model (DRLC). The DRLC model relaxes\npreviously made assumptions about the users' examination behavior and resulting\nlatent states. To implement the DRLC model, convolutional neural networks are\nused as the value networks for reinforcement learning, trained to learn a\npolicy to reduce bias in the click logs. To demonstrate the effectiveness of\nthe DRLC model, we first compare performance with the previous state-of-art\napproaches using established click prediction metrics, including log-likelihood\nand perplexity. We further show that DRLC also leads to improvements in ranking\nperformance. Our experiments demonstrate the effectiveness of the DRLC model in\nlearning to reduce bias in click logs, leading to improved modeling performance\nand showing the potential for using DRLC for improving Web search quality.",
          "link": "http://arxiv.org/abs/2105.10072",
          "publishedOn": "2021-05-24T05:08:38.210Z",
          "wordCount": 698,
          "title": "De-Biased Modelling of Search Click Behavior with Reinforcement Learning. (arXiv:2105.10072v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afsar_M/0/1/0/all/0/1\">Mehdi Afsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crump_T/0/1/0/all/0/1\">Trafford Crump</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Far_B/0/1/0/all/0/1\">Behrouz Far</a>",
          "description": "Recommender systems (RSs) are software tools and algorithms developed to\nalleviate the problem of information overload, which makes it difficult for a\nuser to make right decisions. Two main paradigms toward the recommendation\nproblem are collaborative filtering and content-based filtering, which try to\nrecommend the best items using ratings and content available. These methods\ntypically face infamous problems including cold-start, diversity, scalability,\nand great computational expense. We argue that the uptake of deep learning and\nreinforcement learning methods is also questionable due to their computational\ncomplexities and uninterpretability. In this paper, we approach the\nrecommendation problem from a new prospective. We borrow ideas from cluster\nhead selection algorithms in wireless sensor networks and adapt them to the\nrecommendation problem. In particular, we propose Load Balanced Recommender\nSystem (LBRS), which uses a probabilistic scheme for item recommendation.\nFurthermore, we factor in the importance of items in the recommendation\nprocess, which significantly improves the recommendation accuracy. We also\nintroduce a method that considers a heterogeneity among items, in order to\nbalance the similarity and diversity trade-off. Finally, we propose a new\nmetric for diversity, which emphasizes the importance of diversity not only\nfrom an intra-list perspective, but also from a between-list point of view.\nWith experiments in a simulation study performed on RecSim, we show that LBRS\nis effective and can outperform baseline methods.",
          "link": "http://arxiv.org/abs/2105.09981",
          "publishedOn": "2021-05-24T05:08:38.183Z",
          "wordCount": 639,
          "title": "A Load Balanced Recommendation Approach. (arXiv:2105.09981v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kohli_H/0/1/0/all/0/1\">Harsh Kohli</a>",
          "description": "Related or ideal follow-up suggestions to a web query in search engines are\noften optimized based on several different parameters -- relevance to the\noriginal query, diversity, click probability etc. One or many rankers may be\ntrained to score each suggestion from a candidate pool based on these factors.\nThese scorers are usually pairwise classification tasks where each training\nexample consists of a user query and a single suggestion from the list of\ncandidates. We propose an architecture that takes all candidate suggestions\nassociated with a given query and outputs a suggestion block. We discuss the\nbenefits of such an architecture over traditional approaches and experiment\nwith further enforcing each individual metric through mixed-objective training.",
          "link": "http://arxiv.org/abs/2105.10152",
          "publishedOn": "2021-05-24T05:08:38.166Z",
          "wordCount": 558,
          "title": "Training Mixed-Objective Pointing Decoders for Block-Level Optimization in Search Recommendation. (arXiv:2105.10152v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1\">Andrew Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1\">Nga Than</a>",
          "description": "Topic models are widely used in studying social phenomena. We conduct a\ncomparative study examining state-of-the-art neural versus non-neural topic\nmodels, performing a rigorous quantitative and qualitative assessment on a\ndataset of tweets about the COVID-19 pandemic. Our results show that not only\ndo neural topic models outperform their classical counterparts on standard\nevaluation metrics, but they also produce more coherent topics, which are of\ngreat benefit when studying complex social problems. We also propose a novel\nregularization term for neural topic models, which is designed to address the\nwell-documented problem of mode collapse, and demonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2105.10165",
          "publishedOn": "2021-05-24T05:08:38.132Z",
          "wordCount": 605,
          "title": "Have you tried Neural Topic Models? Comparative Analysis of Neural and Non-Neural Topic Models with Application to COVID-19 Twitter Data. (arXiv:2105.10165v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamada_M/0/1/0/all/0/1\">Mohamed A. Hamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1\">Abdelrahman Abdallah</a>",
          "description": "Many computer systems for calculating the proper organization of memory are\namong the most critical issues. Using a tier cache memory (along with branching\nprediction) is an effective means of increasing modern multi-core processors'\nperformance. Designing high-performance processors is a complex task and\nrequires preliminary verification and analysis of the model level, usually used\nin analytical and simulation modeling. The refinement of extreme programming is\nan unfortunate challenge. Few experts disagree with the synthesis of access\npoints. This article demonstrates that Internet QoS and 16-bit architectures\nare always incompatible, but it's the same situation for write-back caches. The\nsolution to this problem can be implemented by analyzing simulation models of\ndifferent complexity in combination with the analytical evaluation of\nindividual algorithms. This work is devoted to designing a multi-parameter\nsimulation model of a multi-process for evaluating the performance of cache\nmemory algorithms and the optimality of the structure. Optimization of the\nstructures and algorithms of the cache memory allows you to accelerate the\ninteraction of the memory process and improve the performance of the entire\nsystem.",
          "link": "http://arxiv.org/abs/2102.03848",
          "publishedOn": "2021-05-23T06:08:17.168Z",
          "wordCount": 650,
          "title": "Estimate The Efficiency Of Multiprocessor's Cash Memory Work Algorithms. (arXiv:2102.03848v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1812.00002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1\">Sen Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Congzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>",
          "description": "Interactive news recommendation has been launched and attracted much\nattention recently. In this scenario, user's behavior evolves from single click\nbehavior to multiple behaviors including like, comment, share etc. However,\nmost of the existing methods still use single click behavior as the unique\ncriterion of judging user's preferences. Further, although heterogeneous graphs\nhave been applied in different areas, a proper way to construct a heterogeneous\ngraph for interactive news data with an appropriate learning mechanism on it is\nstill desired. To address the above concerns, we propose a graph-based\nbehavior-aware network, which simultaneously considers six different types of\nbehaviors as well as user's demand on the news diversity. We have three main\nsteps. First, we build an interaction behavior graph for multi-level and\nmulti-category data. Second, we apply DeepWalk on the behavior graph to obtain\nentity semantics, then build a graph-based convolutional neural network called\nG-CNN to learn news representations, and an attention-based LSTM to learn\nbehavior sequence representations. Third, we introduce core and coritivity\nfeatures for the behavior graph, which measure the concentration degree of\nuser's interests. These features affect the trade-off between accuracy and\ndiversity of our personalized recommendation system. Taking these features into\naccount, our system finally achieves recommending news to different users at\ntheir different levels of concentration degrees.",
          "link": "http://arxiv.org/abs/1812.00002",
          "publishedOn": "2021-05-23T06:08:17.152Z",
          "wordCount": 678,
          "title": "The Graph-Based Behavior-Aware Recommendation for Interactive News. (arXiv:1812.00002v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>",
          "description": "Conversational recommender systems (CRS) enable the traditional recommender\nsystems to explicitly acquire user preferences towards items and attributes\nthrough interactive conversations. Reinforcement learning (RL) is widely\nadopted to learn conversational recommendation policies to decide what\nattributes to ask, which items to recommend, and when to ask or recommend, at\neach conversation turn. However, existing methods mainly target at solving one\nor two of these three decision-making problems in CRS with separated\nconversation and recommendation components, which restrict the scalability and\ngenerality of CRS and fall short of preserving a stable training procedure. In\nthe light of these challenges, we propose to formulate these three\ndecision-making problems in CRS as a unified policy learning task. In order to\nsystematically integrate conversation and recommendation components, we develop\na dynamic weighted graph based RL method to learn a policy to select the action\nat each conversation turn, either asking an attribute or recommending items.\nFurther, to deal with the sample efficiency issue, we propose two action\nselection strategies for reducing the candidate action space according to the\npreference and entropy information. Experimental results on two benchmark CRS\ndatasets and a real-world E-Commerce application show that the proposed method\nnot only significantly outperforms state-of-the-art methods but also enhances\nthe scalability and stability of CRS.",
          "link": "http://arxiv.org/abs/2105.09710",
          "publishedOn": "2021-05-23T06:08:17.142Z",
          "wordCount": 642,
          "title": "Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning. (arXiv:2105.09710v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Proper_H/0/1/0/all/0/1\">H. A. Proper</a>",
          "description": "Effective information disclosure in the context of databases with a large\nconceptual schema is known to be a non-trivial problem. In particular the\nformulation of ad-hoc queries is a major problem in such contexts. Existing\napproaches for tackling this problem include graphical query interfaces, query\nby navigation, query by construction, and point to point queries. In this\nreport we propose an adoption of the query by navigation mechanism that is\nespecially geared towards the InfoAssistant product. Query by navigation is\nbased on ideas from the information retrieval world, in particular on the\nstratified hypermedia architecture. When using our approach to the formulations\nof queries, a user will first formulate a number of simple queries\ncorresponding to linear paths through the information structure. The\nformulation of the linear paths is the result of the {\\em explorative phase} of\nthe query formulation. Once users have specified a number of these linear\npaths, they may combine them to form more complex queries. Examples of such\ncombinations are: concatenation, union, intersection and selection. This last\nprocess is referred to as {\\em query by construction}, and is the {\\em\nconstructive phase} of the query formulation process.",
          "link": "http://arxiv.org/abs/2105.09562",
          "publishedOn": "2021-05-23T06:08:17.118Z",
          "wordCount": 609,
          "title": "Interactive Query Formulation using Query By Navigation. (arXiv:2105.09562v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bountrogiannis_K/0/1/0/all/0/1\">Konstantinos Bountrogiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzagkarakis_G/0/1/0/all/0/1\">George Tzagkarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalides_P/0/1/0/all/0/1\">Panagiotis Tsakalides</a>",
          "description": "Due to the importance of the lower bounding distances and the attractiveness\nof symbolic representations, the family of symbolic aggregate approximations\n(SAX) has been used extensively for encoding time series data. However, typical\nSAX-based methods rely on two restrictive assumptions; the Gaussian\ndistribution and equiprobable symbols. This paper proposes two novel\ndata-driven SAX-based symbolic representations, distinguished by their\ndiscretization steps. The first representation, oriented for general data\ncompaction and indexing scenarios, is based on the combination of kernel\ndensity estimation and Lloyd-Max quantization to minimize the information loss\nand mean squared error in the discretization step. The second method, oriented\nfor high-level mining tasks, employs the Mean-Shift clustering method and is\nshown to enhance anomaly detection in the lower-dimensional space. Besides, we\nverify on a theoretical basis a previously observed phenomenon of the intrinsic\nprocess that results in a lower than the expected variance of the intermediate\npiecewise aggregate approximation. This phenomenon causes an additional\ninformation loss but can be avoided with a simple modification. The proposed\nrepresentations possess all the attractive properties of the conventional SAX\nmethod. Furthermore, experimental evaluation on real-world datasets\ndemonstrates their superiority compared to the traditional SAX and an\nalternative data-driven SAX variant.",
          "link": "http://arxiv.org/abs/2105.09592",
          "publishedOn": "2021-05-23T06:08:17.108Z",
          "wordCount": 634,
          "title": "Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>",
          "description": "Recommender systems are gaining increasing and critical impacts on human and\nsociety since a growing number of users use them for information seeking and\ndecision making. Therefore, it is crucial to address the potential unfairness\nproblems in recommendations. Just like users have personalized preferences on\nitems, users' demands for fairness are also personalized in many scenarios.\nTherefore, it is important to provide personalized fair recommendations for\nusers to satisfy their personalized fairness demands. Besides, previous works\non fair recommendation mainly focus on association-based fairness. However, it\nis important to advance from associative fairness notions to causal fairness\nnotions for assessing fairness more properly in recommender systems. Based on\nthe above considerations, this paper focuses on achieving personalized\ncounterfactual fairness for users in recommender systems. To this end, we\nintroduce a framework for achieving counterfactually fair recommendations\nthrough adversary learning by generating feature-independent user embeddings\nfor recommendation. The framework allows recommender systems to achieve\npersonalized fairness for users while also covering non-personalized\nsituations. Experiments on two real-world datasets with shallow and deep\nrecommendation algorithms show that our method can generate fairer\nrecommendations for users with a desirable recommendation performance.",
          "link": "http://arxiv.org/abs/2105.09829",
          "publishedOn": "2021-05-23T06:08:17.079Z",
          "wordCount": 630,
          "title": "Towards Personalized Fairness based on Causal Notion. (arXiv:2105.09829v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanya_S/0/1/0/all/0/1\">Suhas Jayaram Subramanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_R/0/1/0/all/0/1\">Ravishankar Krishnaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simhadri_H/0/1/0/all/0/1\">Harsha Vardhan Simhadri</a>",
          "description": "Approximate nearest neighbor search (ANNS) is a fundamental building block in\ninformation retrieval with graph-based indices being the current\nstate-of-the-art and widely used in the industry. Recent advances in\ngraph-based indices have made it possible to index and search billion-point\ndatasets with high recall and millisecond-level latency on a single commodity\nmachine with an SSD.\n\nHowever, existing graph algorithms for ANNS support only static indices that\ncannot reflect real-time changes to the corpus required by many key real-world\nscenarios (e.g. index of sentences in documents, email, or a news index). To\novercome this drawback, the current industry practice for manifesting updates\ninto such indices is to periodically re-build these indices, which can be\nprohibitively expensive.\n\nIn this paper, we present the first graph-based ANNS index that reflects\ncorpus updates into the index in real-time without compromising on search\nperformance. Using update rules for this index, we design FreshDiskANN, a\nsystem that can index over a billion points on a workstation with an SSD and\nlimited memory, and support thousands of concurrent real-time inserts, deletes\nand searches per second each, while retaining $>95\\%$ 5-recall@5. This\nrepresents a 5-10x reduction in the cost of maintaining freshness in indices\nwhen compared to existing methods.",
          "link": "http://arxiv.org/abs/2105.09613",
          "publishedOn": "2021-05-23T06:08:17.044Z",
          "wordCount": 643,
          "title": "FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for Streaming Similarity Search. (arXiv:2105.09613v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1\">Bhaskar Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1\">Nick Craswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "An emerging recipe for achieving state-of-the-art effectiveness in neural\ndocument re-ranking involves utilizing large pre-trained language models -\ne.g., BERT - to evaluate all individual passages in the document and then\naggregating the outputs by pooling or additional Transformer layers. A major\ndrawback of this approach is high query latency due to the cost of evaluating\nevery passage in the document with BERT. To make matters worse, this high\ninference cost and latency varies based on the length of the document, with\nlonger documents requiring more time and computation. To address this\nchallenge, we adopt an intra-document cascading strategy, which prunes passages\nof a candidate document using a less expensive model, called ESM, before\nrunning a scoring model that is more expensive and effective, called ETM. We\nfound it best to train ESM (short for Efficient Student Model) via knowledge\ndistillation from the ETM (short for Effective Teacher Model) e.g., BERT. This\npruning allows us to only run the ETM model on a smaller set of passages whose\nsize does not vary by document length. Our experiments on the MS MARCO and TREC\nDeep Learning Track benchmarks suggest that the proposed Intra-Document\nCascaded Ranking Model (IDCM) leads to over 400% lower query latency by\nproviding essentially the same effectiveness as the state-of-the-art BERT-based\ndocument ranking models.",
          "link": "http://arxiv.org/abs/2105.09816",
          "publishedOn": "2021-05-23T06:08:17.014Z",
          "wordCount": 659,
          "title": "Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Joemon Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>",
          "description": "Learning from implicit feedback is one of the most common cases in the\napplication of recommender systems. Generally speaking, interacted examples are\nconsidered as positive while negative examples are sampled from uninteracted\nones. However, noisy examples are prevalent in real-world implicit feedback. A\nnoisy positive example could be interacted but it actually leads to negative\nuser preference. A noisy negative example which is uninteracted because of\nunawareness of the user could also denote potential positive user preference.\nConventional training methods overlook these noisy examples, leading to\nsub-optimal recommendation. In this work, we propose probabilistic and\nvariational recommendation denoising for implicit feedback. Through an\nempirical study, we find that different models make relatively similar\npredictions on clean examples which denote the real user preference, while the\npredictions on noisy examples vary much more across different models. Motivated\nby this observation, we propose denoising with probabilistic inference (DPI)\nwhich aims to minimize the KL-divergence between the real user preference\ndistributions parameterized by two recommendation models while maximize the\nlikelihood of data observation. We then show that DPI recovers the evidence\nlower bound of an variational auto-encoder when the real user preference is\nconsidered as the latent variables. This leads to our second learning framework\ndenoising with variational autoencoder (DVAE). We employ the proposed DPI and\nDVAE on four state-of-the-art recommendation models and conduct experiments on\nthree datasets. Experimental results demonstrate that DPI and DVAE\nsignificantly improve recommendation performance compared with normal training\nand other denoising methods. Codes will be open-sourced.",
          "link": "http://arxiv.org/abs/2105.09605",
          "publishedOn": "2021-05-23T06:08:16.990Z",
          "wordCount": 678,
          "title": "Probabilistic and Variational Recommendation Denoising. (arXiv:2105.09605v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.13096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jieni Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Junren Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shanxiang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bingwen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabo Wang</a>",
          "description": "Lattices have been conceived as a powerful tool for data hiding. While\nconventional studies and applications focus on achieving the optimal robustness\nversus distortion tradeoff, in some applications such as data hiding in\nmedical/physiological signals, the primary concern is to achieve a minimum\namount of distortion to the cover signal. In this paper, we revisit the\ncelebrated quantization index modulation (QIM) scheme and propose a\nminimum-distortion version of it, referred to as MD-QIM. The crux of MD-QIM is\nto move the data point to only the boundary of the Voronoi region of the\nlattice point indexed by a message, which suffices for subsequent correct\ndecoding. At any fixed code rate, the scheme achieves the minimum amount of\ndistortion by sacrificing the robustness to the additive white Gaussian noise\n(AWGN) attacks. Simulation results confirm that our scheme significantly\noutperforms QIM in terms of mean square error (MSE), peak signal to noise ratio\n(PSNR) and percentage residual difference (PRD).",
          "link": "http://arxiv.org/abs/2105.13096",
          "publishedOn": "2021-05-28T01:42:13.801Z",
          "wordCount": 583,
          "title": "Lattice-Based Minimum-Distortion Data Hiding. (arXiv:2105.13096v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Proulx_M/0/1/0/all/0/1\">Michael J. Proulx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eracleous_T/0/1/0/all/0/1\">Theodoros Eracleous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spencer_B/0/1/0/all/0/1\">Ben Spencer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passfield_A/0/1/0/all/0/1\">Anna Passfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sousa_A/0/1/0/all/0/1\">Alexandra de Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Ali Mohammadi</a>",
          "description": "Sensory substitution has influenced the design of many tactile visual\nsubstitution systems with the aim of offering visual aids for the blind. This\npaper focuses on whether a novel electromagnetic vibrotactile display, a four\nby four vibrotactile matrix of taxels, can serve as an aid for dynamic\ncommunication for visually impaired people. A mixed methods approach was used\nto firstly assess whether pattern complexity affected undergraduate\nparticipants' perceptive success, and secondly, if participants total score\npositively correlated with their perceived success ratings. A thematic analysis\nwas also conducted on participants' experiences with the vibrotactile display\nand what methods of interaction they used. The results indicated that complex\npatterns were less accurately perceived than simple and linear patterns\nrespectively, and no significant correlation was found between participants'\nscore and perceived success ratings. Additionally, most participants interacted\nwith the vibrotactile display in similar ways using one finger to feel one\ntaxel at a time; arguably, the most effective strategy from previous research.\nThis technology could have applications to navigational and communication aids\nfor the visually impaired and road users.",
          "link": "http://arxiv.org/abs/2105.13295",
          "publishedOn": "2021-05-28T01:42:13.789Z",
          "wordCount": 615,
          "title": "Electromagnetic actuation for a vibrotactile display: Assessing stimuli complexity and usability. (arXiv:2105.13295v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12700",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1\">Luka Murn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1\">Marc Gorriz Blanch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santamaria_M/0/1/0/all/0/1\">Maria Santamaria</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivera_F/0/1/0/all/0/1\">Fiona Rivera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>",
          "description": "Machine learning techniques for more efficient video compression and video\nenhancement have been developed thanks to breakthroughs in deep learning. The\nnew techniques, considered as an advanced form of Artificial Intelligence (AI),\nbring previously unforeseen capabilities. However, they typically come in the\nform of resource-hungry black-boxes (overly complex with little transparency\nregarding the inner workings). Their application can therefore be unpredictable\nand generally unreliable for large-scale use (e.g. in live broadcast). The aim\nof this work is to understand and optimise learned models in video processing\napplications so systems that incorporate them can be used in a more trustworthy\nmanner. In this context, the presented work introduces principles for\nsimplification of learned models targeting improved transparency in\nimplementing machine learning for video production and distribution\napplications. These principles are demonstrated on video compression examples,\nshowing how bitrate savings and reduced complexity can be achieved by\nsimplifying relevant deep learning models.",
          "link": "http://arxiv.org/abs/2105.12700",
          "publishedOn": "2021-05-27T01:32:26.907Z",
          "wordCount": 613,
          "title": "Towards Transparent Application of Machine Learning in Video Processing. (arXiv:2105.12700v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sam Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliard_G/0/1/0/all/0/1\">Grayson Hilliard</a>",
          "description": "The ubiquity of mobile phones makes mobile GUI understanding an important\ntask. Most previous works in this domain require human-created metadata of\nscreens (e.g. View Hierarchy) during inference, which unfortunately is often\nnot available or reliable enough for GUI understanding. Inspired by the\nimpressive success of Transformers in NLP tasks, targeting for purely\nvision-based GUI understanding, we extend the concepts of Words/Sentence to\nPixel-Words/Screen-Sentence, and propose a mobile GUI understanding\narchitecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the\nindividual Words, we define the Pixel-Words as atomic visual components (text\nand graphic components), which are visually consistent and semantically clear\nacross screenshots of a large variety of design styles. The Pixel-Words\nextracted from a screenshot are aggregated into Screen-Sentence with a Screen\nTransformer proposed to model their relations. Since the Pixel-Words are\ndefined as atomic visual components, the ambiguity between their visual\nappearance and semantics is dramatically reduced. We are able to make use of\nmetadata available in training data to auto-generate high-quality annotations\nfor Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words\nannotations is built based on the public RICO dataset, which will be released\nto help to address the lack of high-quality training data in this area. We\ntrain a detector to extract Pixel-Words from screenshots on this dataset and\nachieve metadata-free GUI understanding during inference. We conduct\nexperiments and show that Pixel-Words can be well extracted on RICO-PW and well\ngeneralized to a new dataset, P2S-UI, collected by ourselves. The effectiveness\nof PW2SS is further verified in the GUI understanding tasks including relation\nprediction, clickability prediction, screen retrieval, and app type\nclassification.",
          "link": "http://arxiv.org/abs/2105.11941",
          "publishedOn": "2021-05-26T01:22:09.041Z",
          "wordCount": 704,
          "title": "Understanding Mobile GUI: from Pixel-Words to Screen-Sentences. (arXiv:2105.11941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02360",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaoxia Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hua Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "In order to prevent illegal or unauthorized access of image data such as\nhuman faces and ensure legitimate users can use authorization-protected data,\nreversible adversarial attack technique is rise. Reversible adversarial\nexamples (RAE) get both attack capability and reversibility at the same time.\nHowever, the existing technique can not meet application requirements because\nof serious distortion and failure of image recovery when adversarial\nperturbations get strong. In this paper, we take advantage of Reversible Image\nTransformation technique to generate RAE and achieve reversible adversarial\nattack. Experimental results show that proposed RAE generation scheme can\nensure imperceptible image distortion and the original image can be\nreconstructed error-free. What's more, both the attack ability and the image\nquality are not limited by the perturbation amplitude.",
          "link": "http://arxiv.org/abs/1911.02360",
          "publishedOn": "2021-05-26T01:22:09.016Z",
          "wordCount": 640,
          "title": "Reversible Adversarial Attack based on Reversible Image Transformation. (arXiv:1911.02360v7 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haosen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hongxun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hujie Huang</a>",
          "description": "Transformer networks are effective at modeling long-range contextual\ninformation and have recently demonstrated exemplary performance in the natural\nlanguage processing domain. Conventionally, the temporal action proposal\ngeneration (TAPG) task is divided into two main sub-tasks: boundary prediction\nand proposal confidence prediction, which rely on the frame-level dependencies\nand proposal-level relationships separately. To capture the dependencies at\ndifferent levels of granularity, this paper intuitively presents a unified\ntemporal action proposal generation framework with original Transformers,\ncalled TAPG Transformer, which consists of a Boundary Transformer and a\nProposal Transformer. Specifically, the Boundary Transformer captures long-term\ntemporal dependencies to predict precise boundary information and the Proposal\nTransformer learns the rich inter-proposal relationships for reliable\nconfidence evaluation. Extensive experiments are conducted on two popular\nbenchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG\nTransformer outperforms state-of-the-art methods. Equipped with the existing\naction classifier, our method achieves remarkable performance on the temporal\naction localization task. Codes and models will be available.",
          "link": "http://arxiv.org/abs/2105.12043",
          "publishedOn": "2021-05-26T01:22:09.005Z",
          "wordCount": 594,
          "title": "Temporal Action Proposal Generation with Transformers. (arXiv:2105.12043v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.03955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_D/0/1/0/all/0/1\">Doyen Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Ke Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achananuparp_P/0/1/0/all/0/1\">Palakorn Achananuparp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-peng Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>",
          "description": "Food retrieval is an important task to perform analysis of food-related\ninformation, where we are interested in retrieving relevant information about\nthe queried food item such as ingredients, cooking instructions, etc. In this\npaper, we investigate cross-modal retrieval between food images and cooking\nrecipes. The goal is to learn an embedding of images and recipes in a common\nfeature space, such that the corresponding image-recipe embeddings lie close to\none another. Two major challenges in addressing this problem are 1) large\nintra-variance and small inter-variance across cross-modal food data; and 2)\ndifficulties in obtaining discriminative recipe representations. To address\nthese two problems, we propose Semantic-Consistent and Attention-based Networks\n(SCAN), which regularize the embeddings of the two modalities through aligning\noutput semantic probabilities. Besides, we exploit a self-attention mechanism\nto improve the embedding of recipes. We evaluate the performance of the\nproposed method on the large-scale Recipe1M dataset, and show that we can\noutperform several state-of-the-art cross-modal retrieval strategies for food\nimages and cooking recipes by a significant margin.",
          "link": "http://arxiv.org/abs/2003.03955",
          "publishedOn": "2021-05-26T01:22:08.996Z",
          "wordCount": 665,
          "title": "Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism. (arXiv:2003.03955v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zichao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>",
          "description": "Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes will be available.",
          "link": "http://arxiv.org/abs/2105.12085",
          "publishedOn": "2021-05-26T01:22:08.984Z",
          "wordCount": 636,
          "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kattadige_C/0/1/0/all/0/1\">Chamara Kattadige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thilakarathna_K/0/1/0/all/0/1\">Kanchana Thilakarathna</a>",
          "description": "360{\\deg} videos a.k.a. spherical videos are getting popular among users\nnevertheless, omnidirectional view of these videos demands high bandwidth and\nprocessing power at the end devices. Recently proposed viewport aware streaming\nmechanisms can reduce the amount of data transmitted by streaming a limited\nportion of the frame covering the current user viewport (VP). However, they\nstill suffer from sending a high amount of redundant data, as the fixed tile\nmechanisms can not provide finer granularity to the user VP. Though making the\ntiles smaller can provide a finer granularity for user viewport, high encoding\noverhead incurred. To overcome this trade-off, in this paper, we present a\ncomputational geometric approach based adaptive tiling mechanism named VAD360,\nwhich takes visual attention information on the 360{\\deg} video frame as the\ninput and provide a suitable non-overlapping variable size tile cover on the\nframe. Experimental results shows that VAD360 can save up to 31.1% of pixel\nredundancy before compression and 35.4% of bandwidth saving compared to\nrecently proposed fixed tile configurations, providing tile schemes within\n0.98($\\pm$0.11)s time frame.",
          "link": "http://arxiv.org/abs/2105.11563",
          "publishedOn": "2021-05-26T01:22:08.885Z",
          "wordCount": 599,
          "title": "VAD360: Viewport Aware Dynamic 360-Degree Video Frame Tiling. (arXiv:2105.11563v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunshan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yujuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Wai Keung Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jinyoung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>",
          "description": "This companion paper supports the replication of the fashion trend\nforecasting experiments with the KERN (Knowledge Enhanced Recurrent Network)\nmethod that we presented in the ICMR 2020. We provide an artifact that allows\nthe replication of the experiments using a Python implementation. The artifact\nis easy to deploy with simple installation, training and evaluation. We\nreproduce the experiments conducted in the original paper and obtain similar\nperformance as previously reported. The replication results of the experiments\nsupport the main claims in the original paper.",
          "link": "http://arxiv.org/abs/2105.11826",
          "publishedOn": "2021-05-26T01:22:08.847Z",
          "wordCount": 532,
          "title": "Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend Forecasting. (arXiv:2105.11826v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Guoqiang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yanbing Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>",
          "description": "With the explosive growth of video data, video summarization, which attempts\nto seek the minimum subset of frames while still conveying the main story, has\nbecome one of the hottest topics. Nowadays, substantial achievements have been\nmade by supervised learning techniques, especially after the emergence of deep\nlearning. However, it is extremely expensive and difficult to collect human\nannotation for large-scale video datasets. To address this problem, we propose\na convolutional attentive adversarial network (CAAN), whose key idea is to\nbuild a deep summarizer in an unsupervised way. Upon the generative adversarial\nnetwork, our overall framework consists of a generator and a discriminator. The\nformer predicts importance scores for all frames of a video while the latter\ntries to distinguish the score-weighted frame features from original frame\nfeatures. Specifically, the generator employs a fully convolutional sequence\nnetwork to extract global representation of a video, and an attention-based\nnetwork to output normalized importance scores. To learn the parameters, our\nobjective function is composed of three loss functions, which can guide the\nframe-level importance score prediction collaboratively. To validate this\nproposed method, we have conducted extensive experiments on two public\nbenchmarks SumMe and TVSum. The results show the superiority of our proposed\nmethod against other state-of-the-art unsupervised approaches. Our method even\noutperforms some published supervised approaches.",
          "link": "http://arxiv.org/abs/2105.11131",
          "publishedOn": "2021-05-25T01:56:07.554Z",
          "wordCount": 651,
          "title": "Unsupervised Video Summarization with a Convolutional Attentive Adversarial Network. (arXiv:2105.11131v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carroll_M/0/1/0/all/0/1\">Michael Carroll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_E/0/1/0/all/0/1\">Ethan Osborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_C/0/1/0/all/0/1\">Caglar Yildirim</a>",
          "description": "With the increasing availability of modern virtual reality (VR) headsets, the\nuse and applications of VR technology for gaming purposes have become more\npervasive than ever. Despite the growing popularity of VR gaming, user studies\ninto how it might affect the player experience (PX) during the gameplay are\nscarce. Accordingly, the current study investigated the effects of VR gaming\nand game genre on PX. We compared PX metrics for two game genres, strategy\n(more interactive) and racing (less interactive), across two gaming platforms,\nVR and traditional desktop gaming. Participants were randomly assigned to one\nof the gaming platforms, played both a strategy and racing game on their\ncorresponding platform, and provided PX ratings. Results revealed that,\nregardless of the game genre, participants in the VR gaming condition\nexperienced a greater level of sense of presence than did those in the desktop\ngaming condition. That said, results showed that the two gaming platforms did\nnot significantly differ from one another in PX ratings. As for the effect of\ngame genre, participants provided greater PX ratings for the strategy game than\nfor the racing game, regardless of whether the game was played on a VR headset\nor desktop computer. Collectively, these results indicate that although VR\ngaming affords a greater sense of presence in the game environment, this\nincrease in presence does not seem to translate into a more satisfactory PX\nwhen playing either a strategy or racing game.",
          "link": "http://arxiv.org/abs/2105.10754",
          "publishedOn": "2021-05-25T01:56:07.464Z",
          "wordCount": 674,
          "title": "Effects of VR Gaming and Game Genre on Player Experience. (arXiv:2105.10754v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jamali_M/0/1/0/all/0/1\">Maedeh Jamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nader Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadivi_P/0/1/0/all/0/1\">Pejman Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirani_S/0/1/0/all/0/1\">Shahram Shirani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>",
          "description": "Digital contents have grown dramatically in recent years, leading to\nincreased attention to copyright. Image watermarking has been considered one of\nthe most popular methods for copyright protection. With the recent advancements\nin applying deep neural networks in image processing, these networks have also\nbeen used in image watermarking. Robustness and imperceptibility are two\nchallenging features of watermarking methods that the trade-off between them\nshould be satisfied. In this paper, we propose to use an end-to-end network for\nwatermarking. We use a convolutional neural network (CNN) to control the\nembedding strength based on the image content. Dynamic embedding helps the\nnetwork to have the lowest effect on the visual quality of the watermarked\nimage. Different image processing attacks are simulated as a network layer to\nimprove the robustness of the model. Our method is a blind watermarking\napproach that replicates the watermark string to create a matrix of the same\nsize as the input image. Instead of diffusing the watermark data into the input\nimage, we inject the data into the feature space and force the network to do\nthis in regions that increase the robustness against various attacks.\nExperimental results show the superiority of the proposed method in terms of\nimperceptibility and robustness compared to the state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2105.11095",
          "publishedOn": "2021-05-25T01:56:06.430Z",
          "wordCount": 645,
          "title": "Robust Watermarking using Diffusion of Logo into Autoencoder Feature Maps. (arXiv:2105.11095v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "The popularity of multimodal sensors and the accessibility of the Internet\nhave brought us a massive amount of unlabeled multimodal data. Since existing\ndatasets and well-trained models are primarily unimodal, the modality gap\nbetween a unimodal network and unlabeled multimodal data poses an interesting\nproblem: how to transfer a pre-trained unimodal network to perform the same\ntask on unlabeled multimodal data? In this work, we propose multimodal\nknowledge expansion (MKE), a knowledge distillation-based framework to\neffectively utilize multimodal data without requiring labels. Opposite to\ntraditional knowledge distillation, where the student is designed to be\nlightweight and inferior to the teacher, we observe that a multimodal student\nmodel consistently denoises pseudo labels and generalizes better than its\nteacher. Extensive experiments on four tasks and different modalities verify\nthis finding. Furthermore, we connect the mechanism of MKE to semi-supervised\nlearning and offer both empirical and theoretical explanations to understand\nthe denoising capability of a multimodal student.",
          "link": "http://arxiv.org/abs/2103.14431",
          "publishedOn": "2021-05-24T05:08:40.756Z",
          "wordCount": 607,
          "title": "Multimodal Knowledge Expansion. (arXiv:2103.14431v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akazawa_T/0/1/0/all/0/1\">Teruaki Akazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1\">Yuma Kinoshita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>",
          "description": "In this paper, we propose a novel multi-color balance method for reducing\ncolor distortions caused by lighting effects. The proposed method allows us to\nadjust three target-colors chosen by a user in an input image so that each\ntarget color is the same as the corresponding destination (benchmark) one. In\ncontrast, white balancing is a typical technique for reducing the color\ndistortions, however, they cannot remove lighting effects on colors other than\nwhite. In an experiment, the proposed method is demonstrated to be able to\nremove lighting effects on selected three colors, and is compared with existing\nwhite balance adjustments.",
          "link": "http://arxiv.org/abs/2102.01893",
          "publishedOn": "2021-05-24T05:08:39.515Z",
          "wordCount": 632,
          "title": "Multi-color balancing for correctly adjusting the intensity of target colors. (arXiv:2102.01893v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">C.-H. Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1\">Mohit Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Y.-C. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Quan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1\">Tomoaki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakam_T/0/1/0/all/0/1\">Tomokazu Murakam</a>",
          "description": "Camera movement and unpredictable environmental conditions like dust and wind\ninduce noise into video feeds. We observe that popular unsupervised MOT methods\nare dependent on noise-free conditions. We show that the addition of a small\namount of artificial random noise causes a sharp degradation in model\nperformance on benchmark metrics. We resolve this problem by introducing a\nrobust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed\nsingle-head attention model helps limit the negative impact of noise by\nlearning visual representations at different segment scales. AttU-Net shows\nbetter unsupervised MOT tracking performance over variational inference-based\nstate-of-the-art baselines. We evaluate our method in the MNIST and the Atari\ngame video benchmark. We also provide two extended video datasets consisting of\ncomplex visual patterns that include Kuzushiji characters and fashion images to\nvalidate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2105.10005",
          "publishedOn": "2021-05-24T05:08:39.414Z",
          "wordCount": 592,
          "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09999",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Heng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bampis_C/0/1/0/all/0/1\">Christos G. Bampis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>",
          "description": "The layers of convolutional neural networks (CNNs) can be used to alter the\nresolution of their inputs, but the scaling factors are limited to integer\nvalues. However, in many image and video processing applications, the ability\nto resize by a fractional factor would be advantageous. One example is\nconversion between resolutions standardized for video compression, such as from\n1080p to 720p. To solve this problem, we propose an alternative building block,\nformulated as a conventional convolutional layer followed by a differentiable\nresizer. More concretely, the convolutional layer preserves the resolution of\nthe input, while the resizing operation is fully handled by the resizer. In\nthis way, any CNN architecture can be adapted for non-integer resizing. As an\napplication, we replace the resizing convolutional layer of a modern deep\ndownsampling model by the proposed building block, and apply it to an adaptive\nbitrate video streaming scenario. Our experimental results show that an\nimprovement in coding efficiency over the conventional Lanczos algorithm is\nattained, in terms of PSNR, SSIM, and VMAF on test videos.",
          "link": "http://arxiv.org/abs/2105.09999",
          "publishedOn": "2021-05-24T05:08:39.334Z",
          "wordCount": 613,
          "title": "Convolutional Block Design for Learned Fractional Downsampling. (arXiv:2105.09999v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>",
          "description": "The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.",
          "link": "http://arxiv.org/abs/2012.00641",
          "publishedOn": "2021-05-23T06:08:15.920Z",
          "wordCount": 678,
          "title": "A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1\">Dheeraj Pailla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Himanshu Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1\">Aadilmehdi Sanchawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>",
          "description": "The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.",
          "link": "http://arxiv.org/abs/2104.05947",
          "publishedOn": "2021-05-23T06:08:15.873Z",
          "wordCount": 668,
          "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2011.14477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hubert Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuijlen_M/0/1/0/all/0/1\">Mitchell van Zuijlen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_S/0/1/0/all/0/1\">Sylvia C. Pont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijntjes_M/0/1/0/all/0/1\">Maarten W.A. Wijntjes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1\">Kavita Bala</a>",
          "description": "A common strategy for improving model robustness is through data\naugmentations. Data augmentations encourage models to learn desired\ninvariances, such as invariance to horizontal flipping or small changes in\ncolor. Recent work has shown that arbitrary style transfer can be used as a\nform of data augmentation to encourage invariance to textures by creating\npainting-like images from photographs. However, a stylized photograph is not\nquite the same as an artist-created painting. Artists depict perceptually\nmeaningful cues in paintings so that humans can recognize salient components in\nscenes, an emphasis which is not enforced in style transfer. Therefore, we\nstudy how style transfer and paintings differ in their impact on model\nrobustness. First, we investigate the role of paintings as style images for\nstylization-based data augmentation. We find that style transfer functions well\neven without paintings as style images. Second, we show that learning from\npaintings as a form of perceptual data augmentation can improve model\nrobustness. Finally, we investigate the invariances learned from stylization\nand from paintings, and show that models learn different invariances from these\ndiffering forms of data. Our results provide insights into how stylization\nimproves model robustness, and provide evidence that artist-created paintings\ncan be a valuable source of data for model robustness.",
          "link": "http://arxiv.org/abs/2011.14477",
          "publishedOn": "2021-05-28T01:42:15.510Z",
          "wordCount": 678,
          "title": "What Can Style Transfer and Paintings Do For Model Robustness?. (arXiv:2011.14477v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leo Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xufei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengshan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jinyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiantao Zhou</a>",
          "description": "Deep learning models are known to be vulnerable to adversarial examples that\nare elaborately designed for malicious purposes and are imperceptible to the\nhuman perceptual system. Autoencoder, when trained solely over benign examples,\nhas been widely used for (self-supervised) adversarial detection based on the\nassumption that adversarial examples yield larger reconstruction error.\nHowever, because lacking adversarial examples in its training and the too\nstrong generalization ability of autoencoder, this assumption does not always\nhold true in practice. To alleviate this problem, we explore to detect\nadversarial examples by disentangled representations of images under the\nautoencoder structure. By disentangling input images as class features and\nsemantic features, we train an autoencoder, assisted by a discriminator\nnetwork, over both correctly paired class/semantic features and incorrectly\npaired class/semantic features to reconstruct benign and counterexamples. This\nmimics the behavior of adversarial examples and can reduce the unnecessary\ngeneralization ability of autoencoder. Compared with the state-of-the-art\nself-supervised detection methods, our method exhibits better performance in\nvarious measurements (i.e., AUC, FPR, TPR) over different datasets (MNIST,\nFashion-MNIST and CIFAR-10), different adversarial attack methods (FGSM, BIM,\nPGD, DeepFool, and CW) and different victim models (8-layer CNN and 16-layer\nVGG). We compare our method with the state-of-the-art self-supervised detection\nmethods under different adversarial attacks and different victim models (30\nattack settings), and it exhibits better performance in various measurements\n(AUC, FPR, TPR) for most attacks settings. Ideally, AUC is $1$ and our method\nachieves $0.99+$ on CIFAR-10 for all attacks. Notably, different from other\nAutoencoder-based detectors, our method can provide resistance to the adaptive\nadversary.",
          "link": "http://arxiv.org/abs/2105.03689",
          "publishedOn": "2021-05-28T01:42:15.451Z",
          "wordCount": 726,
          "title": "Self-Supervised Adversarial Example Detection by Disentangled Representation. (arXiv:2105.03689v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "This paper proposes a novel method of learning by predicting view assignments\nwith support samples (PAWS). The method trains a model to minimize a\nconsistency loss, which ensures that different views of the same unlabeled\ninstance are assigned similar pseudo-labels. The pseudo-labels are generated\nnon-parametrically, by comparing the representations of the image views to\nthose of a set of randomly sampled labeled images. The distance between the\nview representations and labeled representations is used to provide a weighting\nover class labels, which we interpret as a soft pseudo-label. By\nnon-parametrically incorporating labeled samples in this way, PAWS extends the\ndistance-metric loss used in self-supervised methods such as BYOL and SwAV to\nthe semi-supervised setting. Despite the simplicity of the approach, PAWS\noutperforms other semi-supervised methods across architectures, setting a new\nstate-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of\nthe labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to\n12x less training than the previous best methods.",
          "link": "http://arxiv.org/abs/2104.13963",
          "publishedOn": "2021-05-28T01:42:15.442Z",
          "wordCount": 641,
          "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hualie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Zhe Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zilong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>",
          "description": "Learning depth from spherical panoramas is becoming a popular research topic\nbecause a panorama has a full field-of-view of the environment and provides a\nrelatively complete description of a scene. However, applying well-studied CNNs\nfor perspective images to the standard representation of spherical panoramas,\ni.e., the equirectangular projection, is suboptimal, as it becomes distorted\ntowards the poles. Another representation is the cubemap projection, which is\ndistortion-free but discontinued on edges and limited in the field-of-view.\nThis paper introduces a new framework to fuse features from the two\nprojections, unidirectionally feeding the cubemap features to the\nequirectangular features only at the decoding stage. Unlike the recent\nbidirectional fusion approach operating at both the encoding and decoding\nstages, our fusion scheme is much more efficient. Besides, we also designed a\nmore effective fusion module for our fusion scheme. Experiments verify the\neffectiveness of our proposed fusion strategy and module, and our model\nachieves state-of-the-art performance on four popular datasets. Additional\nexperiments show that our model also has the advantages of model complexity and\ngeneralization capability.The code is available at\nhttps://github.com/alibaba/UniFuse-Unidirectional-Fusion.",
          "link": "http://arxiv.org/abs/2102.03550",
          "publishedOn": "2021-05-28T01:42:15.435Z",
          "wordCount": 662,
          "title": "UniFuse: Unidirectional Fusion for 360$^{\\circ}$ Panorama Depth Estimation. (arXiv:2102.03550v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "This paper presents a new Vision Transformer (ViT) architecture Multi-Scale\nVision Longformer, which significantly enhances the ViT of\n\\cite{dosovitskiy2020image} for encoding high-resolution images using two\ntechniques. The first is the multi-scale model structure, which provides image\nencodings at multiple scales with manageable computational cost. The second is\nthe attention mechanism of vision Longformer, which is a variant of Longformer\n\\cite{beltagy2020longformer}, originally developed for natural language\nprocessing, and achieves a linear complexity w.r.t. the number of input tokens.\nA comprehensive empirical study shows that the new ViT significantly\noutperforms several strong baselines, including the existing ViT models and\ntheir ResNet counterparts, and the Pyramid Vision Transformer from a concurrent\nwork \\cite{wang2021pyramid}, on a range of vision tasks, including image\nclassification, object detection, and segmentation. The models and source code\nare released at \\url{https://github.com/microsoft/vision-longformer}.",
          "link": "http://arxiv.org/abs/2103.15358",
          "publishedOn": "2021-05-28T01:42:15.423Z",
          "wordCount": 615,
          "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding. (arXiv:2103.15358v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1\">Ashok C. Popat</a>",
          "description": "Paragraphs are an important class of document entities. We propose a new\napproach for paragraph identification by spatial graph convolutional neural\nnetworks (GCN) applied on OCR text boxes. Two steps, namely line splitting and\nline clustering, are performed to extract paragraphs from the lines in OCR\nresults. Each step uses a beta-skeleton graph constructed from bounding boxes,\nwhere the graph edges provide efficient support for graph convolution\noperations. With only pure layout input features, the GCN model size is 3~4\norders of magnitude smaller compared to R-CNN based models, while achieving\ncomparable or better accuracies on PubLayNet and other datasets. Furthermore,\nthe GCN models show good generalization from synthetic training data to\nreal-world images, and good adaptivity for variable document styles.",
          "link": "http://arxiv.org/abs/2101.12741",
          "publishedOn": "2021-05-28T01:42:15.401Z",
          "wordCount": 592,
          "title": "General-Purpose OCR Paragraph Identification by Graph Convolutional Neural Networks. (arXiv:2101.12741v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05028",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_M/0/1/0/all/0/1\">M. Akin Yilmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tekalp_A/0/1/0/all/0/1\">A. Murat Tekalp</a>",
          "description": "Conventional video compression methods employ a linear transform and block\nmotion model, and the steps of motion estimation, mode and quantization\nparameter selection, and entropy coding are optimized individually due to\ncombinatorial nature of the end-to-end optimization problem. Learned video\ncompression allows end-to-end rate-distortion optimized training of all\nnonlinear modules, quantization parameter and entropy model simultaneously.\nWhile previous work on learned video compression considered training a\nsequential video codec based on end-to-end optimization of cost averaged over\npairs of successive frames, it is well-known in conventional video compression\nthat hierarchical, bi-directional coding outperforms sequential compression. In\nthis paper, we propose for the first time end-to-end optimization of a\nhierarchical, bi-directional motion compensated learned codec by accumulating\ncost function over fixed-size groups of pictures (GOP). Experimental results\nshow that the rate-distortion performance of our proposed learned\nbi-directional {\\it GOP coder} outperforms the state-of-the-art end-to-end\noptimized learned sequential compression as expected.",
          "link": "http://arxiv.org/abs/2008.05028",
          "publishedOn": "2021-05-28T01:42:15.262Z",
          "wordCount": 616,
          "title": "End-to-End Rate-Distortion Optimization for Bi-Directional Learned Video Compression. (arXiv:2008.05028v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sixiang_T/0/1/0/all/0/1\">Tan Sixiang</a>",
          "description": "For real-time semantic segmentation, how to increase the speed while\nmaintaining high resolution is a problem that has been discussed and solved.\nBackbone design and fusion design have always been two essential parts of\nreal-time semantic segmentation. We hope to design a light-weight network based\non previous design experience and reach the level of state-of-the-art real-time\nsemantic segmentation without any pre-training. To achieve this goal, a\nencoder-decoder architectures are proposed to solve this problem by applying a\ndecoder network onto a backbone model designed for real-time segmentation tasks\nand designed three different ways to fuse semantics and detailed information in\nthe aggregation phase. We have conducted extensive experiments on two semantic\nsegmentation benchmarks. Experiments on the Cityscapes and CamVid datasets show\nthat the proposed FRFNet strikes a balance between speed calculation and\naccuracy. It achieves 76.4\\% Mean Intersection over Union (mIoU\\%) on the\nCityscapes test dataset with the speed of 161 FPS on a single RTX 2080Ti card.\nThe Code is available at https://github.com/favoMJ/FRFNet.",
          "link": "http://arxiv.org/abs/2105.12964",
          "publishedOn": "2021-05-28T01:42:15.256Z",
          "wordCount": 593,
          "title": "Feature Reuse and Fusion for Real-time Semantic segmentation. (arXiv:2105.12964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ploeger_S/0/1/0/all/0/1\">Spencer Ploeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasovic_L/0/1/0/all/0/1\">Lucas Dasovic</a>",
          "description": "A growing branch of computer vision is object detection. Object detection is\nused in many applications such as industrial process, medical imaging analysis,\nand autonomous vehicles. The ability to detect objects in videos is crucial.\nObject detection systems are trained on large image datasets. For applications\nsuch as autonomous vehicles, it is crucial that the object detection system can\nidentify objects through multiple frames in video. There are many problems with\napplying these systems to video. Shadows or changes in brightness that can\ncause the system to incorrectly identify objects frame to frame and cause an\nunintended system response. There are many neural networks that have been used\nfor object detection and if there was a way of connecting objects between\nframes then these problems could be eliminated. For these neural networks to\nget better at identifying objects in video, they need to be re-trained. A\ndataset must be created with images that represent consecutive video frames and\nhave matching ground-truth layers. A method is proposed that can generate these\ndatasets. The ground-truth layer contains only moving objects. To generate this\nlayer, FlowNet2-Pytorch was used to create the flow mask using the novel\nMagnitude Method. As well, a segmentation mask will be generated using networks\nsuch as Mask R-CNN or Refinenet. These segmentation masks will contain all\nobjects detected in a frame. By comparing this segmentation mask to the flow\nmask ground-truth layer, a loss function is generated. This loss function can\nbe used to train a neural network to be better at making consistent predictions\non video. The system was tested on multiple video samples and a loss was\ngenerated for each frame, proving the Magnitude Method's ability to be used to\ntrain object detection neural networks in future work.",
          "link": "http://arxiv.org/abs/2105.12822",
          "publishedOn": "2021-05-28T01:42:15.250Z",
          "wordCount": 738,
          "title": "Issues in Object Detection in Videos using Common Single-Image CNNs. (arXiv:2105.12822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.02763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuanzhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Ning Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Ke Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qi Qi</a>",
          "description": "Video anomaly detection is commonly used in many applications such as\nsecurity surveillance and is very challenging.A majority of recent video\nanomaly detection approaches utilize deep reconstruction models, but their\nperformance is often suboptimal because of insufficient reconstruction error\ndifferences between normal and abnormal video frames in practice. Meanwhile,\nframe prediction-based anomaly detection methods have shown promising\nperformance. In this paper, we propose a novel and robust unsupervised video\nanomaly detection method by frame prediction with proper design which is more\nin line with the characteristics of surveillance videos. The proposed method is\nequipped with a multi-path ConvGRU-based frame prediction network that can\nbetter handle semantically informative objects and areas of different scales\nand capture spatial-temporal dependencies in normal videos. A noise tolerance\nloss is introduced during training to mitigate the interference caused by\nbackground noise. Extensive experiments have been conducted on the CUHK Avenue,\nShanghaiTech Campus, and UCSD Pedestrian datasets, and the results show that\nour proposed method outperforms existing state-of-the-art approaches.\nRemarkably, our proposed method obtains the frame-level AUROC score of 88.3% on\nthe CUHK Avenue dataset.",
          "link": "http://arxiv.org/abs/2011.02763",
          "publishedOn": "2021-05-28T01:42:15.228Z",
          "wordCount": 672,
          "title": "Robust Unsupervised Video Anomaly Detection by Multi-Path Frame Prediction. (arXiv:2011.02763v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharir_G/0/1/0/all/0/1\">Gilad Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "Leading methods in the domain of action recognition try to distill\ninformation from both the spatial and temporal dimensions of an input video.\nMethods that reach State of the Art (SotA) accuracy, usually make use of 3D\nconvolution layers as a way to abstract the temporal information from video\nframes. The use of such convolutions requires sampling short clips from the\ninput video, where each clip is a collection of closely sampled frames. Since\neach short clip covers a small fraction of an input video, multiple clips are\nsampled at inference in order to cover the whole temporal length of the video.\nThis leads to increased computational load and is impractical for real-world\napplications. We address the computational bottleneck by significantly reducing\nthe number of frames required for inference. Our approach relies on a temporal\ntransformer that applies global attention over video frames, and thus better\nexploits the salient information in each frame. Therefore our approach is very\ninput efficient, and can achieve SotA results (on Kinetics dataset) with a\nfraction of the data (frames per video), computation and latency. Specifically\non Kinetics-400, we reach $80.5$ top-1 accuracy with $\\times 30$ less frames\nper video, and $\\times 40$ faster inference than the current leading method.\nCode is available at: https://github.com/Alibaba-MIIL/STAM",
          "link": "http://arxiv.org/abs/2103.13915",
          "publishedOn": "2021-05-28T01:42:15.214Z",
          "wordCount": 674,
          "title": "An Image is Worth 16x16 Words, What is a Video Worth?. (arXiv:2103.13915v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1\">Byeongjoon Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ka_D/0/1/0/all/0/1\">Dongho Ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">David Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>",
          "description": "Despite recent advances in vehicle safety technologies, road traffic\naccidents still pose a severe threat to human lives and have become a leading\ncause of premature deaths. In particular, crosswalks present a major threat to\npedestrians, but we lack dense behavioral data to investigate the risks they\nface. Therefore, we propose a comprehensive analytical model for pedestrian\npotential risk using video footage gathered by road security cameras deployed\nat such crossings. The proposed system automatically detects vehicles and\npedestrians, calculates trajectories by frames, and extracts behavioral\nfeatures affecting the likelihood of potentially dangerous scenes between these\nobjects. Finally, we design a data cube model by using the large amount of the\nextracted features accumulated in a data warehouse to perform multidimensional\nanalysis for potential risk scenes with levels of abstraction, but this is\nbeyond the scope of this paper, and will be detailed in a future study. In our\nexperiment, we focused on extracting the various behavioral features from\nmultiple crosswalks, and visualizing and interpreting their behaviors and\nrelationships among them by camera location to show how they may or may not\ncontribute to potential risk. We validated feasibility and applicability by\napplying it in multiple crosswalks in Osan city, Korea.",
          "link": "http://arxiv.org/abs/2105.02582",
          "publishedOn": "2021-05-28T01:42:15.205Z",
          "wordCount": 672,
          "title": "Vision based Pedestrian Potential Risk Analysis based on Automated Behavior Feature Extraction for Smart and Safe City. (arXiv:2105.02582v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenyi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wendi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xu Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Text-to-Image generation in the general domain has long been an open problem,\nwhich requires both generative model and cross-modal understanding. We propose\nCogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance\nthis problem. We also demonstrate the finetuning strategies for various\ndownstream tasks, e.g. style learning, super-resolution, text-image ranking and\nfashion design, and methods to stabilize pretraining, e.g. eliminating NaN\nlosses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS\nCOCO, outperforms previous GAN-based models and a recent similar work DALL-E.",
          "link": "http://arxiv.org/abs/2105.13290",
          "publishedOn": "2021-05-28T01:42:15.197Z",
          "wordCount": 526,
          "title": "CogView: Mastering Text-to-Image Generation via Transformers. (arXiv:2105.13290v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.13782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Williams_F/0/1/0/all/0/1\">Francis Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1\">Matthew Trager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>",
          "description": "We present Neural Splines, a technique for 3D surface reconstruction that is\nbased on random feature kernels arising from infinitely-wide shallow ReLU\nnetworks. Our method achieves state-of-the-art results, outperforming recent\nneural network-based techniques and widely used Poisson Surface Reconstruction\n(which, as we demonstrate, can also be viewed as a type of kernel method).\nBecause our approach is based on a simple kernel formulation, it is easy to\nanalyze and can be accelerated by general techniques designed for kernel-based\nlearning. We provide explicit analytical expressions for our kernel and argue\nthat our formulation can be seen as a generalization of cubic spline\ninterpolation to higher dimensions. In particular, the RKHS norm associated\nwith Neural Splines biases toward smooth interpolants.",
          "link": "http://arxiv.org/abs/2006.13782",
          "publishedOn": "2021-05-28T01:42:15.191Z",
          "wordCount": 591,
          "title": "Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks. (arXiv:2006.13782v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13084",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>",
          "description": "Most consumer-grade digital cameras can only capture a limited range of\nluminance in real-world scenes due to sensor constraints. Besides, noise and\nquantization errors are often introduced in the imaging process. In order to\nobtain high dynamic range (HDR) images with excellent visual quality, the most\ncommon solution is to combine multiple images with different exposures.\nHowever, it is not always feasible to obtain multiple images of the same scene\nand most HDR reconstruction methods ignore the noise and quantization loss. In\nthis work, we propose a novel learning-based approach using a spatially dynamic\nencoder-decoder network, HDRUNet, to learn an end-to-end mapping for single\nimage HDR reconstruction with denoising and dequantization. The network\nconsists of a UNet-style base network to make full use of the hierarchical\nmulti-scale information, a condition network to perform pattern-specific\nmodulation and a weighting network for selectively retaining information.\nMoreover, we propose a Tanh_L1 loss function to balance the impact of\nover-exposed values and well-exposed values on the network learning. Our method\nachieves the state-of-the-art performance in quantitative comparisons and\nvisual quality. The proposed HDRUNet model won the second place in the single\nframe track of NITRE2021 High Dynamic Range Challenge.",
          "link": "http://arxiv.org/abs/2105.13084",
          "publishedOn": "2021-05-28T01:42:15.184Z",
          "wordCount": 638,
          "title": "HDRUNet: Single Image HDR Reconstruction with Denoising and Dequantization. (arXiv:2105.13084v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_M/0/1/0/all/0/1\">M. Ak&#x131;n Y&#x131;lmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekalp_A/0/1/0/all/0/1\">A. Murat Tekalp</a>",
          "description": "Learned frame prediction is a current problem of interest in computer vision\nand video compression. Although several deep network architectures have been\nproposed for learned frame prediction, to the best of our knowledge, there is\nno work based on using deformable convolutions for frame prediction. To this\neffect, we propose a deformable frame prediction network (DFPN) for task\noriented implicit motion modeling and next frame prediction. Experimental\nresults demonstrate that the proposed DFPN model achieves state of the art\nresults in next frame prediction. Our models and results are available at\nhttps://github.com/makinyilmaz/DFPN.",
          "link": "http://arxiv.org/abs/2105.12794",
          "publishedOn": "2021-05-28T01:42:15.162Z",
          "wordCount": 536,
          "title": "DFPN: Deformable Frame Prediction Network. (arXiv:2105.12794v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_S/0/1/0/all/0/1\">Shanchen Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xixi Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yukun Dong</a>",
          "description": "With the increasing development of garment manufacturing industry, the method\nof combining neural network with industry to reduce product redundancy has been\npaid more and more attention.In order to reduce garment redundancy and achieve\npersonalized customization, more researchers have appeared in the field of\nvirtual trying on.They try to transfer the target clothing to the reference\nfigure, and then stylize the clothes to meet user's requirements for\nfashion.But the biggest problem of virtual try on is that the shape and motion\nblocking distort the clothes, causing the patterns and texture on the clothes\nto be impossible to restore. This paper proposed a new stylized virtual try on\nnetwork, which can not only retain the authenticity of clothing texture and\npattern, but also obtain the undifferentiated stylized try on. The network is\ndivided into three sub-networks, the first is the user image, the front of the\ntarget clothing image, the semantic segmentation image and the posture heat map\nto generate a more detailed human parsing map. Second, UV position map and\ndense correspondence are used to map patterns and textures to the deformed\nsilhouettes in real time, so that they can be retained in real time, and the\nrationality of spatial structure can be guaranteed on the basis of improving\nthe authenticity of images. Third,Stylize and adjust the generated virtual try\non image. Through the most subtle changes, users can choose the texture, color\nand style of clothing to improve the user's experience.",
          "link": "http://arxiv.org/abs/2105.13183",
          "publishedOn": "2021-05-28T01:42:15.155Z",
          "wordCount": 673,
          "title": "An Efficient Style Virtual Try on Network. (arXiv:2105.13183v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Junxiao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudley_J/0/1/0/all/0/1\">John Dudley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristensson_P/0/1/0/all/0/1\">Per Ola Kristensson</a>",
          "description": "Deep learning approaches deliver state-of-the-art performance in recognition\nof spatiotemporal human motion data. However, one of the main challenges in\nthese recognition tasks is limited available training data. Insufficient\ntraining data results in over-fitting and data augmentation is one approach to\naddress this challenge. Existing data augmentation strategies, such as\ntransformations including scaling, shifting and interpolating, require\nhyperparameter optimization that can easily cost hundreds of GPU hours. In this\npaper, we present a novel automatic data augmentation model, the Imaginative\nGenerative Adversarial Network (GAN) that approximates the distribution of the\ninput data and samples new data from this distribution. It is automatic in that\nit requires no data inspection and little hyperparameter tuning and therefore\nit is a low-cost and low-effort approach to generate synthetic data. The\nproposed data augmentation strategy is fast to train and the synthetic data\nleads to higher recognition accuracy than using data augmented with a classical\napproach.",
          "link": "http://arxiv.org/abs/2105.13061",
          "publishedOn": "2021-05-28T01:42:15.149Z",
          "wordCount": 606,
          "title": "The Imaginative Generative Adversarial Network: Automatic Data Augmentation for Dynamic Skeleton-Based Hand Gesture and Human Action Recognition. (arXiv:2105.13061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Linsley_D/0/1/0/all/0/1\">Drew Linsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_G/0/1/0/all/0/1\">Girik Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junkyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govindarajan_L/0/1/0/all/0/1\">Lakshmi N Govindarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingolla_E/0/1/0/all/0/1\">Ennio Mingolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>",
          "description": "Imagine trying to track one particular fruitfly in a swarm of hundreds.\nHigher biological visual systems have evolved to track moving objects by\nrelying on both appearance and motion features. We investigate if\nstate-of-the-art deep neural networks for visual tracking are capable of the\nsame. For this, we introduce PathTracker, a synthetic visual challenge that\nasks human observers and machines to track a target object in the midst of\nidentical-looking \"distractor\" objects. While humans effortlessly learn\nPathTracker and generalize to systematic variations in task design,\nstate-of-the-art deep networks struggle. To address this limitation, we\nidentify and model circuit mechanisms in biological brains that are implicated\nin tracking objects based on motion cues. When instantiated as a recurrent\nnetwork, our circuit model learns to solve PathTracker with a robust visual\nstrategy that rivals human performance and explains a significant proportion of\ntheir decision-making on the challenge. We also show that the success of this\ncircuit model extends to object tracking in natural videos. Adding it to a\ntransformer-based architecture for object tracking builds tolerance to visual\nnuisances that affect object appearance, resulting in a new state-of-the-art\nperformance on the large-scale TrackingNet object tracking challenge. Our work\nhighlights the importance of building artificial vision models that can help us\nbetter understand human vision and improve computer vision.",
          "link": "http://arxiv.org/abs/2105.13351",
          "publishedOn": "2021-05-28T01:42:15.143Z",
          "wordCount": 651,
          "title": "Tracking Without Re-recognition in Humans and Machines. (arXiv:2105.13351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mosella_Montoro_A/0/1/0/all/0/1\">Albert Mosella-Montoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Hidalgo_J/0/1/0/all/0/1\">Javier Ruiz-Hidalgo</a>",
          "description": "Multi-modal fusion has been proved to help enhance the performance of scene\nclassification tasks. This paper presents a 2D-3D Fusion stage that combines 3D\nGeometric Features with 2D Texture Features obtained by 2D Convolutional Neural\nNetworks. To get a robust 3D Geometric embedding, a network that uses two novel\nlayers is proposed. The first layer, Multi-Neighbourhood Graph Convolution,\naims to learn a more robust geometric descriptor of the scene combining two\ndifferent neighbourhoods: one in the Euclidean space and the other in the\nFeature space. The second proposed layer, Nearest Voxel Pooling, improves the\nperformance of the well-known Voxel Pooling. Experimental results, using\nNYU-Depth-V2 and SUN RGB-D datasets, show that the proposed method outperforms\nthe current state-of-the-art in RGB-D indoor scene classification task.",
          "link": "http://arxiv.org/abs/2009.11154",
          "publishedOn": "2021-05-28T01:42:15.125Z",
          "wordCount": 605,
          "title": "2D-3D Geometric Fusion Network using Multi-Neighbourhood Graph Convolution for RGB-D Indoor Scene Classification. (arXiv:2009.11154v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saralajew_S/0/1/0/all/0/1\">Sascha Saralajew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohnemus_L/0/1/0/all/0/1\">Lars Ohnemus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewecker_L/0/1/0/all/0/1\">Lukas Ewecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asan_E/0/1/0/all/0/1\">Ebubekir Asan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isele_S/0/1/0/all/0/1\">Simon Isele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roos_S/0/1/0/all/0/1\">Stefan Roos</a>",
          "description": "In current object detection, algorithms require the object to be directly\nvisible in order to be detected. As humans, however, we intuitively use visual\ncues caused by the respective object to already make assumptions about its\nappearance. In the context of driving, such cues can be shadows during the day\nand often light reflections at night. In this paper, we study the problem of\nhow to map this intuitive human behavior to computer vision algorithms to\ndetect oncoming vehicles at night just from the light reflections they cause by\ntheir headlights. For that, we present an extensive open-source dataset\ncontaining 59746 annotated grayscale images out of 346 different scenes in a\nrural environment at night. In these images, all oncoming vehicles, their\ncorresponding light objects (e.g., headlamps), and their respective light\nreflections (e.g., light reflections on guardrails) are labeled. In this\ncontext, we discuss the characteristics of the dataset and the challenges in\nobjectively describing visual cues such as light reflections. We provide\ndifferent metrics for different ways to approach the task and report the\nresults we achieved using state-of-the-art and custom object detection models\nas a first benchmark. With that, we want to bring attention to a new and so far\nneglected field in computer vision research, encourage more researchers to\ntackle the problem, and thereby further close the gap between human performance\nand computer vision systems.",
          "link": "http://arxiv.org/abs/2105.13236",
          "publishedOn": "2021-05-28T01:42:15.118Z",
          "wordCount": 665,
          "title": "A Dataset for Provident Vehicle Detection at Night. (arXiv:2105.13236v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tsang Ing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Adriano L. I. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "In this paper, we argue that the unsatisfactory out-of-distribution (OOD)\ndetection performance of neural networks is mainly due to the SoftMax loss\nanisotropy and propensity to produce low entropy probability distributions in\ndisagreement with the principle of maximum entropy. Current out-of-distribution\n(OOD) detection approaches usually do not directly fix the SoftMax loss\ndrawbacks but rather build techniques to circumvent it. Unfortunately, those\nmethods usually produce undesired side effects (e.g., classification accuracy\ndrop, additional hyperparameters, slower inferences, and collecting extra\ndata). In the opposite direction, we propose replacing SoftMax loss with a\nnovel loss function that does not suffer from the mentioned weaknesses. The\nproposed IsoMax loss is isotropic (exclusively distance-based) and provides\nhigh entropy posterior probability distributions. Replacing the SoftMax loss by\nIsoMax loss requires no model or training changes. Additionally, the models\ntrained with IsoMax loss produce as fast and energy-efficient inferences as\nthose trained using SoftMax loss. Further, no classification accuracy drop is\nobserved. The proposed method does not rely on outlier/background data,\nhyperparameter tuning, temperature calibration, feature extraction, metric\nlearning, adversarial training, ensemble procedures, or generative models. Our\nexperiments showed that IsoMax loss works as a seamless SoftMax loss drop-in\nreplacement that significantly improves neural networks' OOD detection\nperformance. Therefore, it may be used as a baseline OOD detection approach to\nbe combined with current or future OOD detection techniques to achieve even\nhigher results.",
          "link": "http://arxiv.org/abs/2006.04005",
          "publishedOn": "2021-05-28T01:42:15.111Z",
          "wordCount": 697,
          "title": "Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vitali_E/0/1/0/all/0/1\">Emanuele Vitali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lokhmotov_A/0/1/0/all/0/1\">Anton Lokhmotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palermo_G/0/1/0/all/0/1\">Gianluca Palermo</a>",
          "description": "In this paper, we want to show the potential benefit of a dynamic auto-tuning\napproach for the inference process in the Deep Neural Network (DNN) context,\ntackling the object detection challenge. We benchmarked different neural\nnetworks to find the optimal detector for the well-known COCO 17 database, and\nwe demonstrate that even if we only consider the quality of the prediction\nthere is not a single optimal network. This is even more evident if we also\nconsider the time to solution as a metric to evaluate, and then select, the\nmost suitable network. This opens to the possibility for an adaptive\nmethodology to switch among different object detection networks according to\nrun-time requirements (e.g. maximum quality subject to a time-to-solution\nconstraint).\n\nMoreover, we demonstrated by developing an ad hoc oracle, that an additional\nproactive methodology could provide even greater benefits, allowing us to\nselect the best network among the available ones given some characteristics of\nthe processed image. To exploit this method, we need to identify some image\nfeatures that can be used to steer the decision on the most promising network.\nDespite the optimization opportunity that has been identified, we were not able\nto identify a predictor function that validates this attempt neither adopting\nclassical image features nor by using a DNN classifier.",
          "link": "http://arxiv.org/abs/2105.13279",
          "publishedOn": "2021-05-28T01:42:15.100Z",
          "wordCount": 681,
          "title": "Dynamic Network selection for the Object Detection task: why it matters and what we (didn't) achieve. (arXiv:2105.13279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chazalon_J/0/1/0/all/0/1\">Joseph Chazalon</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Carlinet_E/0/1/0/all/0/1\">Edwin Carlinet</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizi Chen</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Perret_J/0/1/0/all/0/1\">Julien Perret</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Dumenieu_B/0/1/0/all/0/1\">Bertrand Dum&#xe9;nieu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Mallet_C/0/1/0/all/0/1\">Cl&#xe9;ment Mallet</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Geraud_T/0/1/0/all/0/1\">Thierry G&#xe9;raud</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vincent Nguyen</a> (4 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nam Nguyen</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Baloun_J/0/1/0/all/0/1\">Josef Baloun</a> (6 and 7), <a href=\"http://arxiv.org/find/cs/1/au:+Lenc_L/0/1/0/all/0/1\">Ladislav Lenc</a> (6 and 7), <a href=\"http://arxiv.org/find/cs/1/au:+Kral_P/0/1/0/all/0/1\">Pavel Kr&#xe1;l</a> (6 and 7) ((1) EPITA Research and Development Lab. (LRDE), EPITA, France, (2) Univ. Gustave Eiffel, IGN-ENSG, LaSTIG, France, (3) LaD&#xe9;HiS, CRH, EHESS, France, (4) L3i, University of La Rochelle, France, (5) Liris, INSA-Lyon, France, (6) Department of Computer Science and Engineering, University of West Bohemia, Univerzitn&#xed;, Pilsen, Czech Republic, (7) NTIS - New Technologies for the Information Society, University of West Bohemia, Univerzitn&#xed;, Pilsen, Czech Republic)",
          "description": "This paper presents the final results of the ICDAR 2021 Competition on\nHistorical Map Segmentation (MapSeg), encouraging research on a series of\nhistorical atlases of Paris, France, drawn at 1/5000 scale between 1894 and\n1937. The competition featured three tasks, awarded separately. Task~1 consists\nin detecting building blocks and was won by the L3IRIS team using a\nDenseNet-121 network trained in a weakly supervised fashion. This task is\nevaluated on 3 large images containing hundreds of shapes to detect. Task~2\nconsists in segmenting map content from the larger map sheet, and was won by\nthe UWB team using a U-Net-like FCN combined with a binarization method to\nincrease detection edge accuracy. Task~3 consists in locating intersection\npoints of geo-referencing lines, and was also won by the UWB team who used a\ndedicated pipeline combining binarization, line detection with Hough transform,\ncandidate filtering, and template matching for intersection refinement. Tasks~2\nand~3 are evaluated on 95 map sheets with complex content. Dataset, evaluation\ntools and results are available under permissive licensing at\n\\url{https://icdar21-mapseg.github.io/}.",
          "link": "http://arxiv.org/abs/2105.13265",
          "publishedOn": "2021-05-28T01:42:15.093Z",
          "wordCount": 739,
          "title": "ICDAR 2021 Competition on Historical Map Segmentation. (arXiv:2105.13265v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13153",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sanguk Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_M/0/1/0/all/0/1\">Minyoung Chung</a>",
          "description": "Cardiac segmentation of atriums, ventricles, and myocardium in computed\ntomography (CT) images is an important first-line task for presymptomatic\ncardiovascular disease diagnosis. In several recent studies, deep learning\nmodels have shown significant breakthroughs in medical image segmentation\ntasks. Unlike other organs such as the lungs and liver, the cardiac organ\nconsists of multiple substructures, i.e., ventricles, atriums, aortas,\narteries, veins, and myocardium. These cardiac substructures are proximate to\neach other and have indiscernible boundaries (i.e., homogeneous intensity\nvalues), making it difficult for the segmentation network focus on the\nboundaries between the substructures. In this paper, to improve the\nsegmentation accuracy between proximate organs, we introduce a novel model to\nexploit shape and boundary-aware features. We primarily propose a shape-aware\nattention module, that exploits distance regression, which can guide the model\nto focus on the edges between substructures so that it can outperform the\nconventional contour-based attention method. In the experiments, we used the\nMulti-Modality Whole Heart Segmentation dataset that has 20 CT cardiac images\nfor training and validation, and 40 CT cardiac images for testing. The\nexperimental results show that the proposed network produces more accurate\nresults than state-of-the-art networks by improving the Dice similarity\ncoefficient score by 4.97%. Our proposed shape-aware contour attention\nmechanism demonstrates that distance transformation and boundary features\nimprove the actual attention map to strengthen the responses in the boundary\narea. Moreover, our proposed method significantly reduces the false-positive\nresponses of the final output, resulting in accurate segmentation.",
          "link": "http://arxiv.org/abs/2105.13153",
          "publishedOn": "2021-05-28T01:42:15.086Z",
          "wordCount": 678,
          "title": "Cardiac Segmentation on CT Images through Shape-Aware Contour Attentions. (arXiv:2105.13153v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jinxi Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenji Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>",
          "description": "Deep learning has demonstrated significant improvements in medical image\nsegmentation using a sufficiently large amount of training data with manual\nlabels. Acquiring well-representative labels requires expert knowledge and\nexhaustive labors. In this paper, we aim to boost the performance of\nsemi-supervised learning for medical image segmentation with limited labels\nusing a self-ensembling contrastive learning technique. To this end, we propose\nto train an encoder-decoder network at image-level with small amounts of\nlabeled images, and more importantly, we learn latent representations directly\nat feature-level by imposing contrastive loss on unlabeled images. This method\nstrengthens intra-class compactness and inter-class separability, so as to get\na better pixel classifier. Moreover, we devise a student encoder for online\nlearning and an exponential moving average version of it, called teacher\nencoder, to improve the performance iteratively in a self-ensembling manner. To\nconstruct contrastive samples with unlabeled images, two sampling strategies\nthat exploit structure similarity across medical images and utilize\npseudo-labels for construction, termed region-aware and anatomical-aware\ncontrastive sampling, are investigated. We conduct extensive experiments on an\nMRI and a CT segmentation dataset and demonstrate that in a limited label\nsetting, the proposed method achieves state-of-the-art performance. Moreover,\nthe anatomical-aware strategy that prepares contrastive samples on-the-fly\nusing pseudo-labels realizes better contrastive regularization on feature\nrepresentations.",
          "link": "http://arxiv.org/abs/2105.12924",
          "publishedOn": "2021-05-28T01:42:15.079Z",
          "wordCount": 643,
          "title": "Self-Ensembling Contrastive Learning for Semi-Supervised Medical Image Segmentation. (arXiv:2105.12924v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+S_G/0/1/0/all/0/1\">Girisha S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_U/0/1/0/all/0/1\">Ujjwal Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_M/0/1/0/all/0/1\">Manohara Pai M M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_R/0/1/0/all/0/1\">Radhika Pai</a>",
          "description": "Semantic segmentation of aerial videos has been extensively used for decision\nmaking in monitoring environmental changes, urban planning, and disaster\nmanagement. The reliability of these decision support systems is dependent on\nthe accuracy of the video semantic segmentation algorithms. The existing CNN\nbased video semantic segmentation methods have enhanced the image semantic\nsegmentation methods by incorporating an additional module such as LSTM or\noptical flow for computing temporal dynamics of the video which is a\ncomputational overhead. The proposed research work modifies the CNN\narchitecture by incorporating temporal information to improve the efficiency of\nvideo semantic segmentation.\n\nIn this work, an enhanced encoder-decoder based CNN architecture (UVid-Net)\nis proposed for UAV video semantic segmentation. The encoder of the proposed\narchitecture embeds temporal information for temporally consistent labelling.\nThe decoder is enhanced by introducing the feature-refiner module, which aids\nin accurate localization of the class labels. The proposed UVid-Net\narchitecture for UAV video semantic segmentation is quantitatively evaluated on\nextended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been\nobserved which is significantly greater than the other state-of-the-art\nalgorithms. Further, the proposed work produced promising results even for the\npre-trained model of UVid-Net on urban street scene with fine tuning the final\nlayer on UAV aerial videos.",
          "link": "http://arxiv.org/abs/2011.14284",
          "publishedOn": "2021-05-28T01:42:15.057Z",
          "wordCount": 710,
          "title": "UVid-Net: Enhanced Semantic Segmentation of UAV Aerial Videos by Embedding Temporal Information. (arXiv:2011.14284v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>",
          "description": "With the advances of data-driven machine learning research, a wide variety of\nprediction problems have been tackled. It has become critical to explore how\nmachine learning and specifically deep learning methods can be exploited to\nanalyse healthcare data. A major limitation of existing methods has been the\nfocus on grid-like data; however, the structure of physiological recordings are\noften irregular and unordered which makes it difficult to conceptualise them as\na matrix. As such, graph neural networks have attracted significant attention\nby exploiting implicit information that resides in a biological system, with\ninteractive nodes connected by edges whose weights can be either temporal\nassociations or anatomical junctions. In this survey, we thoroughly review the\ndifferent types of graph architectures and their applications in healthcare. We\nprovide an overview of these methods in a systematic manner, organized by their\ndomain of application including functional connectivity, anatomical structure\nand electrical-based analysis. We also outline the limitations of existing\ntechniques and discuss potential directions for future research.",
          "link": "http://arxiv.org/abs/2105.13137",
          "publishedOn": "2021-05-28T01:42:15.042Z",
          "wordCount": 611,
          "title": "Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past, Present and Future. (arXiv:2105.13137v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marinov_Z/0/1/0/all/0/1\">Zdravko Marinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasileva_S/0/1/0/all/0/1\">Stanka Vasileva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "Drones have become a common tool, which is utilized in many tasks such as\naerial photography, surveillance, and delivery. However, operating a drone\nrequires more and more interaction with the user. A natural and safe method for\nHuman-Drone Interaction (HDI) is using gestures. In this paper, we introduce an\nHDI framework building upon skeleton-based pose estimation. Our framework\nprovides the functionality to control the movement of the drone with simple arm\ngestures and to follow the user while keeping a safe distance. We also propose\na monocular distance estimation method, which is entirely based on image\nfeatures and does not require any additional depth sensors. To perform\ncomprehensive experiments and quantitative analysis, we create a customized\ntesting dataset. The experiments indicate that our HDI framework can achieve an\naverage of93.5% accuracy in the recognition of 11 common gestures. The code\nwill be made publicly available to foster future research. Code is available\nat: https://github.com/Zrrr1997/Pose2Drone",
          "link": "http://arxiv.org/abs/2105.13204",
          "publishedOn": "2021-05-28T01:42:15.028Z",
          "wordCount": 588,
          "title": "Pose2Drone: A Skeleton-Pose-based Framework forHuman-Drone Interaction. (arXiv:2105.13204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13067",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Laxman_K/0/1/0/all/0/1\">Kumarapu Laxman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalyan_B/0/1/0/all/0/1\">Baddam Kalyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kojjarapu_S/0/1/0/all/0/1\">Satya Raj Vineel Kojjarapu</a>",
          "description": "Recently, Conditional Generative Adversarial Network (Conditional GAN) have\nshown very promising performance in several image-to-image translation\napplications. However, the uses of these conditional GANs are quite limited to\nlow-resolution images, such as 256X256.The Pix2Pix-HD is a recent attempt to\nutilize the conditional GAN for high-resolution image synthesis. In this paper,\nwe propose a Multi-Scale Gradient based U-Net (MSG U-Net) model for\nhigh-resolution image-to-image translation up to 2048X1024 resolution. The\nproposed model is trained by allowing the flow of gradients from\nmultiple-discriminators to a single generator at multiple scales. The proposed\nMSG U-Net architecture leads to photo-realistic high-resolution image-to-image\ntranslation. Moreover, the proposed model is computationally efficient as\ncom-pared to the Pix2Pix-HD with an improvement in the inference time nearly by\n2.5 times. We provide the code of MSG U-Net model at\nhttps://github.com/laxmaniron/MSG-U-Net.",
          "link": "http://arxiv.org/abs/2105.13067",
          "publishedOn": "2021-05-28T01:42:15.010Z",
          "wordCount": 581,
          "title": "Efficient High-Resolution Image-to-Image Translation using Multi-Scale Gradient U-Net. (arXiv:2105.13067v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sateesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1\">Sanjay Haresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Awais Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1\">Andrey Konin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1\">M. Zeeshan Zia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc-Huy Tran</a>",
          "description": "We present a novel approach for unsupervised activity segmentation, which\nuses video frame clustering as a pretext task and simultaneously performs\nrepresentation learning and online clustering. This is in contrast with prior\nworks where representation learning and online clustering are often performed\nsequentially. We leverage temporal information in videos by employing temporal\noptimal transport and temporal coherence loss. In particular, we incorporate a\ntemporal regularization term into the standard optimal transport module, which\npreserves the temporal order of the activity, yielding the temporal optimal\ntransport module for computing pseudo-label cluster assignments. Next, the\ntemporal coherence loss encourages neighboring video frames to be mapped to\nnearby points while distant video frames are mapped to farther away points in\nthe embedding space. The combination of these two components results in\neffective representations for unsupervised activity segmentation. Furthermore,\nprevious methods require storing learned features for the entire dataset before\nclustering them in an offline manner, whereas our approach processes one\nmini-batch at a time in an online manner. Extensive evaluations on three public\ndatasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset,\ni.e., Desktop Assembly, show that our approach performs on par or better than\nprevious methods for unsupervised activity segmentation, despite having\nsignificantly less memory constraints.",
          "link": "http://arxiv.org/abs/2105.13353",
          "publishedOn": "2021-05-28T01:42:15.003Z",
          "wordCount": 648,
          "title": "Unsupervised Activity Segmentation by Joint Representation Learning and Online Clustering. (arXiv:2105.13353v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galatolo_A/0/1/0/all/0/1\">Alessio Galatolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilsson_A/0/1/0/all/0/1\">Alfred Nilsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlemstrand_R/0/1/0/all/0/1\">Roderick Karlemstrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yineng Wang</a>",
          "description": "The memorization problem is well-known in the field of computer vision. Liu\net al. propose a technique called Early-Learning Regularization, which improves\naccuracy on the CIFAR datasets when label noise is present. This project\nreplicates their experiments and investigates the performance on a real-world\ndataset with intrinsic noise. Results show that their experimental results are\nconsistent. We also explore Sharpness-Aware Minimization in addition to SGD and\nobserved a further 14.6 percentage points improvement. Future work includes\nusing all 6 million images and manually clean a fraction of the images to\nfine-tune a transfer learning model. Last but not the least, having access to\nclean data for testing would also improve the measurement of accuracy.",
          "link": "http://arxiv.org/abs/2105.13244",
          "publishedOn": "2021-05-28T01:42:14.989Z",
          "wordCount": 544,
          "title": "Using Early-Learning Regularization to Classify Real-World Noisy Data. (arXiv:2105.13244v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.00210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Existing vision-based action recognition is susceptible to occlusion and\nappearance variations, while wearable sensors can alleviate these challenges by\ncapturing human motion with one-dimensional time-series signal. For the same\naction, the knowledge learned from vision sensors and wearable sensors, may be\nrelated and complementary. However, there exists significantly large modality\ndifference between action data captured by wearable-sensor and vision-sensor in\ndata dimension, data distribution and inherent information content. In this\npaper, we propose a novel framework, named Semantics-aware Adaptive Knowledge\nDistillation Networks (SAKDN), to enhance action recognition in vision-sensor\nmodality (videos) by adaptively transferring and distilling the knowledge from\nmultiple wearable sensors. The SAKDN uses multiple wearable-sensors as teacher\nmodalities and uses RGB videos as student modality. To preserve local temporal\nrelationship and facilitate employing visual deep learning model, we transform\none-dimensional time-series signals of wearable sensors to two-dimensional\nimages by designing a gramian angular field based virtual image generation\nmodel. Then, we build a novel Similarity-Preserving Adaptive Multi-modal Fusion\nModule to adaptively fuse intermediate representation knowledge from different\nteacher networks. Finally, to fully exploit and transfer the knowledge of\nmultiple well-trained teacher networks to the student network, we propose a\nnovel Graph-guided Semantically Discriminative Mapping loss, which utilizes\ngraph-guided ablation analysis to produce a good visual explanation\nhighlighting the important regions across modalities and concurrently\npreserving the interrelations of original data. Experimental results on\nBerkeley-MHAD, UTD-MHAD and MMAct datasets well demonstrate the effectiveness\nof our proposed SAKDN.",
          "link": "http://arxiv.org/abs/2009.00210",
          "publishedOn": "2021-05-28T01:42:14.983Z",
          "wordCount": 741,
          "title": "Semantics-aware Adaptive Knowledge Distillation for Sensor-to-Vision Action Recognition. (arXiv:2009.00210v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dong Eui Chang</a>",
          "description": "This paper presents a vision-based modularized drone racing navigation system\nthat uses a customized convolutional neural network (CNN) for the perception\nmodule to produce high-level navigation commands and then leverages a\nstate-of-the-art planner and controller to generate low-level control commands,\nthus exploiting the advantages of both data-based and model-based approaches.\nUnlike the state-of-the-art method which only takes the current camera image as\nthe CNN input, we further add the latest three drone states as part of the\ninputs. Our method outperforms the state-of-the-art method in various track\nlayouts and offers two switchable navigation behaviors with a single trained\nnetwork. The CNN-based perception module is trained to imitate an expert policy\nthat automatically generates ground truth navigation commands based on the\npre-computed global trajectories. Owing to the extensive randomization and our\nmodified dataset aggregation (DAgger) policy during data collection, our\nnavigation system, which is purely trained in simulation with synthetic\ntextures, successfully operates in environments with randomly-chosen\nphotorealistic textures without further fine-tuning.",
          "link": "http://arxiv.org/abs/2105.12923",
          "publishedOn": "2021-05-28T01:42:14.976Z",
          "wordCount": 613,
          "title": "Robust Navigation for Racing Drones based on Imitation Learning and Modularization. (arXiv:2105.12923v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haibo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "In recent years, significant progress has been made in the research of facial\nlandmark detection. However, few prior works have thoroughly discussed about\nmodels for practical applications. Instead, they often focus on improving a\ncouple of issues at a time while ignoring the others. To bridge this gap, we\naim to explore a practical model that is accurate, robust, efficient,\ngeneralizable, and end-to-end trainable at the same time. To this end, we first\npropose a baseline model equipped with one transformer decoder as detection\nhead. In order to achieve a better accuracy, we further propose two lightweight\nmodules, namely dynamic query initialization (DQInit) and query-aware memory\n(QAMem). Specifically, DQInit dynamically initializes the queries of decoder\nfrom the inputs, enabling the model to achieve as good accuracy as the ones\nwith multiple decoder layers. QAMem is designed to enhance the discriminative\nability of queries on low-resolution feature maps by assigning separate memory\nvalues to each query rather than a shared one. With the help of QAMem, our\nmodel removes the dependence on high-resolution feature maps and is still able\nto obtain superior accuracy. Extensive experiments and analysis on three\npopular benchmarks show the effectiveness and practical advantages of the\nproposed model. Notably, our model achieves new state of the art on WFLW as\nwell as competitive results on 300W and COFW, while still running at 50+ FPS.",
          "link": "http://arxiv.org/abs/2105.13150",
          "publishedOn": "2021-05-28T01:42:14.970Z",
          "wordCount": 660,
          "title": "When Liebig's Barrel Meets Facial Landmark Detection: A Practical Model. (arXiv:2105.13150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Changye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baireddy_S/0/1/0/all/0/1\">Sriram Baireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_E/0/1/0/all/0/1\">Enyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meline_V/0/1/0/all/0/1\">Valerian Meline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caldwell_D/0/1/0/all/0/1\">Denise Caldwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_Pascuzzi_A/0/1/0/all/0/1\">Anjali S. Iyer-Pascuzzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>",
          "description": "Many plants become limp or droop through heat, loss of water, or disease.\nThis is also known as wilting. In this paper, we examine plant wilting caused\nby bacterial infection. In particular, we want to design a metric for wilting\nbased on images acquired of the plant. A quantifiable wilting metric will be\nuseful in studying bacterial wilt and identifying resistance genes. Since there\nis no standard way to estimate wilting, it is common to use ad hoc visual\nscores. This is very subjective and requires expert knowledge of the plants and\nthe disease mechanism. Our solution consists of using various wilting metrics\nacquired from RGB images of the plants. We also designed several experiments to\ndemonstrate that our metrics are effective at estimating wilting in plants.",
          "link": "http://arxiv.org/abs/2105.12926",
          "publishedOn": "2021-05-28T01:42:14.962Z",
          "wordCount": 561,
          "title": "Image-Based Plant Wilting Estimation. (arXiv:2105.12926v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1\">Pei-Ze Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_M/0/1/0/all/0/1\">Meng-Shiun Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">Hung-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-sheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>",
          "description": "In this work, we aim to address the 3D scene stylization problem - generating\nstylized images of the scene at arbitrary novel view angles. A straightforward\nsolution is to combine existing novel view synthesis and image/video style\ntransfer approaches, which often leads to blurry results or inconsistent\nappearance. Inspired by the high quality results of the neural radiance fields\n(NeRF) method, we propose a joint framework to directly render novel views with\nthe desired style. Our framework consists of two components: an implicit\nrepresentation of the 3D scene with the neural radiance field model, and a\nhypernetwork to transfer the style information into the scene representation.\nIn particular, our implicit representation model disentangles the scene into\nthe geometry and appearance branches, and the hypernetwork learns to predict\nthe parameters of the appearance branch from the reference style image. To\nalleviate the training difficulties and memory burden, we propose a two-stage\ntraining procedure and a patch sub-sampling approach to optimize the style and\ncontent losses with the neural radiance field model. After optimization, our\nmodel is able to render consistent novel views at arbitrary view angles with\narbitrary style. Both quantitative evaluation and human subject study have\ndemonstrated that the proposed method generates faithful stylization results\nwith consistent appearance across different views.",
          "link": "http://arxiv.org/abs/2105.13016",
          "publishedOn": "2021-05-28T01:42:14.955Z",
          "wordCount": 649,
          "title": "Stylizing 3D Scene via Implicit Representation and HyperNetwork. (arXiv:2105.13016v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pellegrini_L/0/1/0/all/0/1\">Lorenzo Pellegrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graffieti_G/0/1/0/all/0/1\">Gabriele Graffieti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maltoni_D/0/1/0/all/0/1\">Davide Maltoni</a>",
          "description": "On-device training for personalized learning is a challenging research\nproblem. Being able to quickly adapt deep prediction models at the edge is\nnecessary to better suit personal user needs. However, adaptation on the edge\nposes some questions on both the efficiency and sustainability of the learning\nprocess and on the ability to work under shifting data distributions. Indeed,\nnaively fine-tuning a prediction model only on the newly available data results\nin catastrophic forgetting, a sudden erasure of previously acquired knowledge.\nIn this paper, we detail the implementation and deployment of a hybrid\ncontinual learning strategy (AR1*) on a native Android application for\nreal-time on-device personalization without forgetting. Our benchmark, based on\nan extension of the CORe50 dataset, shows the efficiency and effectiveness of\nour solution.",
          "link": "http://arxiv.org/abs/2105.13127",
          "publishedOn": "2021-05-28T01:42:14.930Z",
          "wordCount": 571,
          "title": "Continual Learning at the Edge: Real-Time Training on Smartphone Devices. (arXiv:2105.13127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1\">Soham De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Samuel L. Smith</a>",
          "description": "In computer vision, it is standard practice to draw a single sample from the\ndata augmentation procedure for each unique image in the mini-batch, however it\nis not clear whether this choice is optimal for generalization. In this work,\nwe provide a detailed empirical evaluation of how the number of augmentation\nsamples per unique image influences performance on held out data. Remarkably,\nwe find that drawing multiple samples per image consistently enhances the test\naccuracy achieved for both small and large batch training, despite reducing the\nnumber of unique training examples in each mini-batch. This benefit arises even\nwhen different augmentation multiplicities perform the same number of parameter\nupdates and gradient evaluations. Our results suggest that, although the\nvariance in the gradient estimate arising from subsampling the dataset has an\nimplicit regularization benefit, the variance which arises from the data\naugmentation process harms test accuracy. By applying augmentation multiplicity\nto the recently proposed NFNet model family, we achieve a new ImageNet state of\nthe art of 86.8$\\%$ top-1 w/o extra data.",
          "link": "http://arxiv.org/abs/2105.13343",
          "publishedOn": "2021-05-28T01:42:14.918Z",
          "wordCount": 613,
          "title": "Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error. (arXiv:2105.13343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Peng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lingyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianmin Ji</a>",
          "description": "One of the main obstacles to 3D semantic segmentation is the significant\namount of endeavor required to generate expensive point-wise annotations for\nfully supervised training. To alleviate manual efforts, we propose GIDSeg, a\nnovel approach that can simultaneously learn segmentation from sparse\nannotations via reasoning global-regional structures and individual-vicinal\nproperties. GIDSeg depicts global- and individual- relation via a dynamic edge\nconvolution network coupled with a kernelized identity descriptor. The ensemble\neffects are obtained by endowing a fine-grained receptive field to a\nlow-resolution voxelized map. In our GIDSeg, an adversarial learning module is\nalso designed to further enhance the conditional constraint of identity\ndescriptors within the joint feature distribution. Despite the apparent\nsimplicity, our proposed approach achieves superior performance over\nstate-of-the-art for inferencing 3D dense segmentation with only sparse\nannotations. Particularly, with $5\\%$ annotations of raw data, GIDSeg\noutperforms other 3D segmentation methods.",
          "link": "http://arxiv.org/abs/2105.12885",
          "publishedOn": "2021-05-28T01:42:14.909Z",
          "wordCount": 587,
          "title": "3D Segmentation Learning from Sparse Annotations and Hierarchical Descriptors. (arXiv:2105.12885v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sereda_I/0/1/0/all/0/1\">Iana Sereda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osipov_G/0/1/0/all/0/1\">Grigory Osipov</a>",
          "description": "We describe how some problems (interpretability,lack of object-orientedness)\nof modern deep networks potentiallycould be solved by adapting a biologically\nplausible saccadicmechanism of perception. A sketch of such a saccadic\nvisionmodel is proposed. Proof of concept experimental results areprovided to\nsupport the proposed approach.",
          "link": "http://arxiv.org/abs/2105.13264",
          "publishedOn": "2021-05-28T01:42:14.897Z",
          "wordCount": 476,
          "title": "How saccadic vision might help with theinterpretability of deep networks. (arXiv:2105.13264v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chambon_T/0/1/0/all/0/1\">Thomas Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heitz_E/0/1/0/all/0/1\">Eric Heitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belcour_L/0/1/0/all/0/1\">Laurent Belcour</a>",
          "description": "Our objective is to compute a textural loss that can be used to train texture\ngenerators with multiple material channels typically used for physically based\nrendering such as albedo, normal, roughness, metalness, ambient occlusion, etc.\nNeural textural losses often build on top of the feature spaces of pretrained\nconvolutional neural networks. Unfortunately, these pretrained models are only\navailable for 3-channel RGB data and hence limit neural textural losses to this\nformat. To overcome this limitation, we show that passing random triplets to a\n3-channel loss provides a multi-channel loss that can be used to generate\nhigh-quality material textures.",
          "link": "http://arxiv.org/abs/2105.13012",
          "publishedOn": "2021-05-28T01:42:14.875Z",
          "wordCount": 535,
          "title": "Passing Multi-Channel Material Textures to a 3-Channel Loss. (arXiv:2105.13012v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wenjia Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>",
          "description": "Single-image super-resolution (SR) and multi-frame SR are two ways to super\nresolve low-resolution images. Single-Image SR generally handles each image\nindependently, but ignores the temporal information implied in continuing\nframes. Multi-frame SR is able to model the temporal dependency via capturing\nmotion information. However, it relies on neighbouring frames which are not\nalways available in the real world. Meanwhile, slight camera shake easily\ncauses heavy motion blur on long-distance-shot low-resolution images. To\naddress these problems, a Blind Motion Deblurring Super-Reslution Networks,\nBMDSRNet, is proposed to learn dynamic spatio-temporal information from single\nstatic motion-blurred images. Motion-blurred images are the accumulation over\ntime during the exposure of cameras, while the proposed BMDSRNet learns the\nreverse process and uses three-streams to learn Bidirectional spatio-temporal\ninformation based on well designed reconstruction loss functions to recover\nclean high-resolution images. Extensive experiments demonstrate that the\nproposed BMDSRNet outperforms recent state-of-the-art methods, and has the\nability to simultaneously deal with image deblurring and SR.",
          "link": "http://arxiv.org/abs/2105.13077",
          "publishedOn": "2021-05-28T01:42:14.843Z",
          "wordCount": 599,
          "title": "Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal Learning Meets Static Image Understanding. (arXiv:2105.13077v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_M/0/1/0/all/0/1\">Mohamed M. Sabry Aly</a>",
          "description": "Non-maximum Suppression (NMS) is an essential postprocessing step in modern\nconvolutional neural networks for object detection. Unlike convolutions which\nare inherently parallel, the de-facto standard for NMS, namely GreedyNMS,\ncannot be easily parallelized and thus could be the performance bottleneck in\nconvolutional object detection pipelines. MaxpoolNMS is introduced as a\nparallelizable alternative to GreedyNMS, which in turn enables faster speed\nthan GreedyNMS at comparable accuracy. However, MaxpoolNMS is only capable of\nreplacing the GreedyNMS at the first stage of two-stage detectors like\nFaster-RCNN. There is a significant drop in accuracy when applying MaxpoolNMS\nat the final detection stage, due to the fact that MaxpoolNMS fails to\napproximate GreedyNMS precisely in terms of bounding box selection. In this\npaper, we propose a general, parallelizable and configurable approach\nPSRR-MaxpoolNMS, to completely replace GreedyNMS at all stages in all\ndetectors. By introducing a simple Relationship Recovery module and a Pyramid\nShifted MaxpoolNMS module, our PSRR-MaxpoolNMS is able to approximate GreedyNMS\nmore precisely than MaxpoolNMS. Comprehensive experiments show that our\napproach outperforms MaxpoolNMS by a large margin, and it is proven faster than\nGreedyNMS with comparable accuracy. For the first time, PSRR-MaxpoolNMS\nprovides a fully parallelizable solution for customized hardware design, which\ncan be reused for accelerating NMS everywhere.",
          "link": "http://arxiv.org/abs/2105.12990",
          "publishedOn": "2021-05-28T01:42:14.836Z",
          "wordCount": 641,
          "title": "PSRR-MaxpoolNMS: Pyramid Shifted MaxpoolNMS with Relationship Recovery. (arXiv:2105.12990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Delong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>",
          "description": "Tremendous progress has been made on face detection in recent years using\nconvolutional neural networks. While many face detectors use designs designated\nfor the detection of face, we treat face detection as a general object\ndetection task. We implement a face detector based on YOLOv5 object detector\nand call it YOLO5Face. We add a five-point landmark regression head into it and\nuse the Wing loss function. We design detectors with different model sizes,\nfrom a large model to achieve the best performance, to a super small model for\nreal-time detection on an embedded or mobile device. Experiment results on the\nWiderFace dataset show that our face detectors can achieve state-of-the-art\nperformance in almost all the Easy, Medium, and Hard subsets, exceeding the\nmore complex designated face detectors. The code is available at\n\\url{https://www.github.com/deepcam-cn/yolov5-face}.",
          "link": "http://arxiv.org/abs/2105.12931",
          "publishedOn": "2021-05-28T01:42:14.827Z",
          "wordCount": 560,
          "title": "YOLO5Face: Why Reinventing a Face Detector. (arXiv:2105.12931v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Peng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lingyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>",
          "description": "We present a method for localizing a single camera with respect to a point\ncloud map in indoor and outdoor scenes. The problem is challenging because\ncorrespondences of local invariant features are inconsistent across the domains\nbetween image and 3D. The problem is even more challenging as the method must\nhandle various environmental conditions such as illumination, weather, and\nseasonal changes. Our method can match equirectangular images to the 3D range\nprojections by extracting cross-domain symmetric place descriptors. Our key\ninsight is to retain condition-invariant 3D geometry features from limited data\nsamples while eliminating the condition-related features by a designed\nGenerative Adversarial Network. Based on such features, we further design a\nspherical convolution network to learn viewpoint-invariant symmetric place\ndescriptors. We evaluate our method on extensive self-collected datasets, which\ninvolve \\textit{Long-term} (variant appearance conditions),\n\\textit{Large-scale} (up to $2km$ structure/unstructured environment), and\n\\textit{Multistory} (four-floor confined space). Our method surpasses other\ncurrent state-of-the-arts by achieving around $3$ times higher place retrievals\nto inconsistent environments, and above $3$ times accuracy on online\nlocalization. To highlight our method's generalization capabilities, we also\nevaluate the recognition across different datasets. With a single trained\nmodel, i3dLoc can demonstrate reliable visual localization in random\nconditions.",
          "link": "http://arxiv.org/abs/2105.12883",
          "publishedOn": "2021-05-28T01:42:14.820Z",
          "wordCount": 644,
          "title": "i3dLoc: Image-to-range Cross-domain Localization Robust to Inconsistent Environmental Conditions. (arXiv:2105.12883v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cardenuto_J/0/1/0/all/0/1\">Jo&#xe3;o P. Cardenuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>",
          "description": "The scientific image integrity area presents a challenging research\nbottleneck, the lack of available datasets to design and evaluate forensic\ntechniques. Its data sensitivity creates a legal hurdle that prevents one to\nrely on real tampered cases to build any sort of accessible forensic benchmark.\nTo mitigate this bottleneck, we present an extendable open-source library that\nreproduces the most common image forgery operations reported by the research\nintegrity community: duplication, retouching, and cleaning. Using this library\nand realistic scientific images, we create a large scientific forgery image\nbenchmark (39,423 images) with an enriched ground-truth. In addition, concerned\nabout the high number of retracted papers due to image duplication, this work\nevaluates the state-of-the-art copy-move detection methods in the proposed\ndataset, using a new metric that asserts consistent match detection between the\nsource and the copied region. The dataset and source-code will be freely\navailable upon acceptance of the paper.",
          "link": "http://arxiv.org/abs/2105.12872",
          "publishedOn": "2021-05-28T01:42:14.808Z",
          "wordCount": 571,
          "title": "Benchmarking Scientific Image Forgery Detectors. (arXiv:2105.12872v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xudong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>",
          "description": "Self-attention has been successfully applied to video representation learning\ndue to the effectiveness of modeling long range dependencies. Existing\napproaches build the dependencies merely by computing the pairwise correlations\nalong spatial and temporal dimensions simultaneously. However, spatial\ncorrelations and temporal correlations represent different contextual\ninformation of scenes and temporal reasoning. Intuitively, learning spatial\ncontextual information first will benefit temporal modeling. In this paper, we\npropose a separable self-attention (SSA) module, which models spatial and\ntemporal correlations sequentially, so that spatial contexts can be efficiently\nused in temporal modeling. By adding SSA module into 2D CNN, we build a SSA\nnetwork (SSAN) for video representation learning. On the task of video action\nrecognition, our approach outperforms state-of-the-art methods on\nSomething-Something and Kinetics-400 datasets. Our models often outperform\ncounterparts with shallower network and fewer modalities. We further verify the\nsemantic learning ability of our method in visual-language task of video\nretrieval, which showcases the homogeneity of video representations and text\nembeddings. On MSR-VTT and Youcook2 datasets, video representations learnt by\nSSA significantly improve the state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2105.13033",
          "publishedOn": "2021-05-28T01:42:14.749Z",
          "wordCount": 605,
          "title": "SSAN: Separable Self-Attention Network for Video Representation Learning. (arXiv:2105.13033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hanley_M/0/1/0/all/0/1\">Margot Hanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barocas_S/0/1/0/all/0/1\">Solon Barocas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1\">Karen Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azenkot_S/0/1/0/all/0/1\">Shiri Azenkot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissenbaum_H/0/1/0/all/0/1\">Helen Nissenbaum</a>",
          "description": "Scholars have recently drawn attention to a range of controversial issues\nposed by the use of computer vision for automatically generating descriptions\nof people in images. Despite these concerns, automated image description has\nbecome an important tool to ensure equitable access to information for blind\nand low vision people. In this paper, we investigate the ethical dilemmas faced\nby companies that have adopted the use of computer vision for producing alt\ntext: textual descriptions of images for blind and low vision people, We use\nFacebook's automatic alt text tool as our primary case study. First, we analyze\nthe policies that Facebook has adopted with respect to identity categories,\nsuch as race, gender, age, etc., and the company's decisions about whether to\npresent these terms in alt text. We then describe an alternative -- and manual\n-- approach practiced in the museum community, focusing on how museums\ndetermine what to include in alt text descriptions of cultural artifacts. We\ncompare these policies, using notable points of contrast to develop an analytic\nframework that characterizes the particular apprehensions behind these policy\nchoices. We conclude by considering two strategies that seem to sidestep some\nof these concerns, finding that there are no easy ways to avoid the normative\ndilemmas posed by the use of computer vision to automate alt text.",
          "link": "http://arxiv.org/abs/2105.12754",
          "publishedOn": "2021-05-28T01:42:14.697Z",
          "wordCount": 678,
          "title": "Computer Vision and Conflicting Values: Describing People with Automated Alt Text. (arXiv:2105.12754v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Prashant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Sabyasachi Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1\">Vanshil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondameedi_V/0/1/0/all/0/1\">Vineetha Kondameedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Abhinav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Akshaj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_C/0/1/0/all/0/1\">Chiranjib Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vinay Viswanathan</a>",
          "description": "Accurate reconstruction of static environments from LiDAR scans of scenes\ncontaining dynamic objects, which we refer to as Dynamic to Static Translation\n(DST), is an important area of research in Autonomous Navigation. This problem\nhas been recently explored for visual SLAM, but to the best of our knowledge no\nwork has been attempted to address DST for LiDAR scans. The problem is of\ncritical importance due to wide-spread adoption of LiDAR in Autonomous\nVehicles. We show that state-of the art methods developed for the visual domain\nwhen adapted for LiDAR scans perform poorly.\n\nWe develop DSLR, a deep generative model which learns a mapping between\ndynamic scan to its static counterpart through an adversarially trained\nautoencoder. Our model yields the first solution for DST on LiDAR that\ngenerates static scans without using explicit segmentation labels. DSLR cannot\nalways be applied to real world data due to lack of paired dynamic-static\nscans. Using Unsupervised Domain Adaptation, we propose DSLR-UDA for transfer\nto real world data and experimentally show that this performs well in real\nworld settings. Additionally, if segmentation information is available, we\nextend DSLR to DSLR-Seg to further improve the reconstruction quality.\n\nDSLR gives the state of the art performance on simulated and real-world\ndatasets and also shows at least 4x improvement. We show that DSLR, unlike the\nexisting baselines, is a practically viable model with its reconstruction\nquality within the tolerable limits for tasks pertaining to autonomous\nnavigation like SLAM in dynamic environments.",
          "link": "http://arxiv.org/abs/2105.12774",
          "publishedOn": "2021-05-28T01:42:14.689Z",
          "wordCount": 708,
          "title": "DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially Trained Autoencoder. (arXiv:2105.12774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1\">Renjie Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>",
          "description": "We propose Joint-DetNAS, a unified NAS framework for object detection, which\nintegrates 3 key components: Neural Architecture Search, pruning, and Knowledge\nDistillation. Instead of naively pipelining these techniques, our Joint-DetNAS\noptimizes them jointly. The algorithm consists of two core processes: student\nmorphism optimizes the student's architecture and removes the redundant\nparameters, while dynamic distillation aims to find the optimal matching\nteacher. For student morphism, weight inheritance strategy is adopted, allowing\nthe student to flexibly update its architecture while fully utilize the\npredecessor's weights, which considerably accelerates the search; To facilitate\ndynamic distillation, an elastic teacher pool is trained via integrated\nprogressive shrinking strategy, from which teacher detectors can be sampled\nwithout additional cost in subsequent searches. Given a base detector as the\ninput, our algorithm directly outputs the derived student detector with high\nperformance without additional training. Experiments demonstrate that our\nJoint-DetNAS outperforms the naive pipelining approach by a great margin. Given\na classic R101-FPN as the base detector, Joint-DetNAS is able to boost its mAP\nfrom 41.4 to 43.9 on MS COCO and reduce the latency by 47%, which is on par\nwith the SOTA EfficientDet while requiring less search cost. We hope our\nproposed method can provide the community with a new way of jointly optimizing\nNAS, KD and pruning.",
          "link": "http://arxiv.org/abs/2105.12971",
          "publishedOn": "2021-05-28T01:42:14.682Z",
          "wordCount": 660,
          "title": "Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic Distillation. (arXiv:2105.12971v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">Aimon Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Nabeel Mohammed</a>",
          "description": "Pretraining has sparked groundswell of interest in deep learning workflows to\nlearn from limited data and improve generalization. While this is common for 2D\nimage classification tasks, its application to 3D medical imaging tasks like\nchest CT interpretation is limited. We explore the idea of whether pretraining\na model on realistic videos could improve performance rather than training the\nmodel from scratch, intended for tuberculosis type classification from chest CT\nscans. To incorporate both spatial and temporal features, we develop a hybrid\nconvolutional neural network (CNN) and recurrent neural network (RNN) model,\nwhere the features are extracted from each axial slice of the CT scan by a CNN,\nthese sequence of image features are input to a RNN for classification of the\nCT scan. Our model termed as ViPTT-Net, was trained on over 1300 video clips\nwith labels of human activities, and then fine-tuned on chest CT scans with\nlabels of tuberculosis type. We find that pretraining the model on videos lead\nto better representations and significantly improved model validation\nperformance from a kappa score of 0.17 to 0.35, especially for\nunder-represented class samples. Our best method achieved 2nd place in the\nImageCLEF 2021 Tuberculosis - TBT classification task with a kappa score of\n0.20 on the final test set with only image information (without using clinical\nmeta-data). All codes and models are made available.",
          "link": "http://arxiv.org/abs/2105.12810",
          "publishedOn": "2021-05-28T01:42:14.629Z",
          "wordCount": 677,
          "title": "ViPTT-Net: Video pretraining of spatio-temporal model for tuberculosis type classification from chest CT scans. (arXiv:2105.12810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>",
          "description": "Recent advances in unsupervised domain adaptation have seen considerable\nprogress in semantic segmentation. Existing methods either align different\ndomains with adversarial training or involve the self-learning that utilizes\npseudo labels to conduct supervised training. The former always suffers from\nthe unstable training caused by adversarial training and only focuses on the\ninter-domain gap that ignores intra-domain knowledge. The latter tends to put\noverconfident label prediction on wrong categories, which propagates errors to\nmore samples. To solve these problems, we propose a two-stage adaptive semantic\nsegmentation method based on the local Lipschitz constraint that satisfies both\ndomain alignment and domain-specific exploration under a unified principle. In\nthe first stage, we propose the local Lipschitzness regularization as the\nobjective function to align different domains by exploiting intra-domain\nknowledge, which explores a promising direction for non-adversarial adaptive\nsemantic segmentation. In the second stage, we use the local Lipschitzness\nregularization to estimate the probability of satisfying Lipschitzness for each\npixel, and then dynamically sets the threshold of pseudo labels to conduct\nself-learning. Such dynamical self-learning effectively avoids the error\npropagation caused by noisy labels. Optimization in both stages is based on the\nsame principle, i.e., the local Lipschitz constraint, so that the knowledge\nlearned in the first stage can be maintained in the second stage. Further, due\nto the model-agnostic property, our method can easily adapt to any CNN-based\nsemantic segmentation networks. Experimental results demonstrate the excellent\nperformance of our method on standard benchmarks.",
          "link": "http://arxiv.org/abs/2105.12939",
          "publishedOn": "2021-05-28T01:42:14.618Z",
          "wordCount": 666,
          "title": "Unsupervised Adaptive Semantic Segmentation with Local Lipschitz Constraint. (arXiv:2105.12939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McCrae_S/0/1/0/all/0/1\">Scott McCrae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kehan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakhor_A/0/1/0/all/0/1\">Avideh Zakhor</a>",
          "description": "As computer-generated content and deepfakes make steady improvements,\nsemantic approaches to multimedia forensics will become more important. In this\npaper, we introduce a novel classification architecture for identifying\nsemantic inconsistencies between video appearance and text caption in social\nmedia news posts. We develop a multi-modal fusion framework to identify\nmismatches between videos and captions in social media posts by leveraging an\nensemble method based on textual analysis of the caption, automatic audio\ntranscription, semantic video analysis, object detection, named entity\nconsistency, and facial verification. To train and test our approach, we curate\na new video-based dataset of 4,000 real-world Facebook news posts for analysis.\nOur multi-modal approach achieves 60.5% classification accuracy on random\nmismatches between caption and appearance, compared to accuracy below 50% for\nuni-modal models. Further ablation studies confirm the necessity of fusion\nacross modalities for correctly identifying semantic inconsistencies.",
          "link": "http://arxiv.org/abs/2105.12855",
          "publishedOn": "2021-05-28T01:42:14.610Z",
          "wordCount": 570,
          "title": "Multi-Modal Semantic Inconsistency Detection in Social Media News Posts. (arXiv:2105.12855v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dahal_A/0/1/0/all/0/1\">Ashok Dahal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1\">Ciaran Eising</a>",
          "description": "Electric Vehicles are increasingly common, with inductive chargepads being\nconsidered a convenient and efficient means of charging electric vehicles.\nHowever, drivers are typically poor at aligning the vehicle to the necessary\naccuracy for efficient inductive charging, making the automated alignment of\nthe two charging plates desirable. In parallel to the electrification of the\nvehicular fleet, automated parking systems that make use of surround-view\ncamera systems are becoming increasingly popular. In this work, we propose a\nsystem based on the surround-view camera architecture to detect, localize and\nautomatically align the vehicle with the inductive chargepad. The visual design\nof the chargepads is not standardized and not necessarily known beforehand.\nTherefore a system that relies on offline training will fail in some\nsituations. Thus we propose an online learning method that leverages the\ndriver's actions when manually aligning the vehicle with the chargepad and\ncombine it with weak supervision from semantic segmentation and depth to learn\na classifier to auto-annotate the chargepad in the video for further training.\nIn this way, when faced with a previously unseen chargepad, the driver needs\nonly manually align the vehicle a single time. As the chargepad is flat on the\nground, it is not easy to detect it from a distance. Thus, we propose using a\nVisual SLAM pipeline to learn landmarks relative to the chargepad to enable\nalignment from a greater range. We demonstrate the working system on an\nautomated vehicle as illustrated in the video https://youtu.be/_cLCmkW4UYo. To\nencourage further research, we will share a chargepad dataset used in this\nwork.",
          "link": "http://arxiv.org/abs/2105.12763",
          "publishedOn": "2021-05-28T01:42:14.562Z",
          "wordCount": 697,
          "title": "An Online Learning System for Wireless Charging Alignment using Surround-view Fisheye Cameras. (arXiv:2105.12763v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dahan_E/0/1/0/all/0/1\">Eran Dahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diskin_T/0/1/0/all/0/1\">Tzvi Diskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amram_A/0/1/0/all/0/1\">Amit Amram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_O/0/1/0/all/0/1\">Omer Koren</a>",
          "description": "Detection and classification of objects in overhead images are two important\nand challenging problems in computer vision. Among various research areas in\nthis domain, the task of fine-grained classification of objects in overhead\nimages has become ubiquitous in diverse real-world applications, due to recent\nadvances in high-resolution satellite and airborne imaging systems. The small\ninter-class variations and the large intra class variations caused by the fine\ngrained nature make it a challenging task, especially in low-resource cases. In\nthis paper, we introduce COFGA a new open dataset for the advancement of\nfine-grained classification research. The 2,104 images in the dataset are\ncollected from an airborne imaging system at 5 15 cm ground sampling distance,\nproviding higher spatial resolution than most public overhead imagery datasets.\nThe 14,256 annotated objects in the dataset were classified into 2 classes, 15\nsubclasses, 14 unique features, and 8 perceived colors a total of 37 distinct\nlabels making it suitable to the task of fine-grained classification more than\nany other publicly available overhead imagery dataset. We compare COFGA to\nother overhead imagery datasets and then describe some distinguished fine-grain\nclassification approaches that were explored during an open data-science\ncompetition we have conducted for this task.",
          "link": "http://arxiv.org/abs/2105.12786",
          "publishedOn": "2021-05-28T01:42:14.514Z",
          "wordCount": 639,
          "title": "cofga: A Dataset for Fine Grained Classification of Objects from Aerial Imagery. (arXiv:2105.12786v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rongrong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1\">Chiu Man Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "Segmentation-based scene text detection methods have been widely adopted for\narbitrary-shaped text detection recently, since they make accurate pixel-level\npredictions on curved text instances and can facilitate real-time inference\nwithout time-consuming processing on anchors. However, current\nsegmentation-based models are unable to learn the shapes of curved texts and\noften require complex label assignments or repeated feature aggregations for\nmore accurate detection. In this paper, we propose RSCA: a Real-time\nSegmentation-based Context-Aware model for arbitrary-shaped scene text\ndetection, which sets a strong baseline for scene text detection with two\nsimple yet effective strategies: Local Context-Aware Upsampling and Dynamic\nText-Spine Labeling, which model local spatial transformation and simplify\nlabel assignments separately. Based on these strategies, RSCA achieves\nstate-of-the-art performance in both speed and accuracy, without complex label\nassignments or repeated feature aggregations. We conduct extensive experiments\non multiple benchmarks to validate the effectiveness of our method. RSCA-640\nreaches 83.9% F-measure at 48.3 FPS on CTW1500 dataset.",
          "link": "http://arxiv.org/abs/2105.12789",
          "publishedOn": "2021-05-28T01:42:14.506Z",
          "wordCount": 589,
          "title": "RSCA: Real-time Segmentation-based Context-Aware Scene Text Detection. (arXiv:2105.12789v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Baozhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofstee_P/0/1/0/all/0/1\">Peter Hofstee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peltenburg_J/0/1/0/all/0/1\">Johan Peltenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alars_Z/0/1/0/all/0/1\">Zaid Alars</a>",
          "description": "Data-free compression raises a new challenge because the original training\ndataset for a pre-trained model to be compressed is not available due to\nprivacy or transmission issues. Thus, a common approach is to compute a\nreconstructed training dataset before compression. The current reconstruction\nmethods compute the reconstructed training dataset with a generator by\nexploiting information from the pre-trained model. However, current\nreconstruction methods focus on extracting more information from the\npre-trained model but do not leverage network engineering. This work is the\nfirst to consider network engineering as an approach to design the\nreconstruction method. Specifically, we propose the AutoReCon method, which is\na neural architecture search-based reconstruction method. In the proposed\nAutoReCon method, the generator architecture is designed automatically given\nthe pre-trained model for reconstruction. Experimental results show that using\ngenerators discovered by the AutoRecon method always improve the performance of\ndata-free compression.",
          "link": "http://arxiv.org/abs/2105.12151",
          "publishedOn": "2021-05-27T01:32:29.396Z",
          "wordCount": 582,
          "title": "AutoReCon: Neural Architecture Search-based Reconstruction for Data-free Compression. (arXiv:2105.12151v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12386",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jinyang Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>",
          "description": "In this paper, we propose a new deep image compression framework called\nComplexity and Bitrate Adaptive Network (CBANet), which aims to learn one\nsingle network to support variable bitrate coding under different computational\ncomplexity constraints. In contrast to the existing state-of-the-art learning\nbased image compression frameworks that only consider the rate-distortion\ntrade-off without introducing any constraint related to the computational\ncomplexity, our CBANet considers the trade-off between the rate and distortion\nunder dynamic computational complexity constraints. Specifically, to decode the\nimages with one single decoder under various computational complexity\nconstraints, we propose a new multi-branch complexity adaptive module, in which\neach branch only takes a small portion of the computational budget of the\ndecoder. The reconstructed images with different visual qualities can be\nreadily generated by using different numbers of branches. Furthermore, to\nachieve variable bitrate decoding with one single decoder, we propose a bitrate\nadaptive module to project the representation from a base bitrate to the\nexpected representation at a target bitrate for transmission. Then it will\nproject the transmitted representation at the target bitrate back to that at\nthe base bitrate for the decoding process. The proposed bit adaptive module can\nsignificantly reduce the storage requirement for deployment platforms. As a\nresult, our CBANet enables one single codec to support multiple bitrate\ndecoding under various computational complexity constraints. Comprehensive\nexperiments on two benchmark datasets demonstrate the effectiveness of our\nCBANet for deep image compression.",
          "link": "http://arxiv.org/abs/2105.12386",
          "publishedOn": "2021-05-27T01:32:29.296Z",
          "wordCount": 687,
          "title": "CBANet: Towards Complexity and Bitrate Adaptive Deep Image Compression using a Single Network. (arXiv:2105.12386v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_K/0/1/0/all/0/1\">Kinjal Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Arindam Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sudip Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Ujjwal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Pedestrian Detection is the most critical module of an Autonomous Driving\nsystem. Although a camera is commonly used for this purpose, its quality\ndegrades severely in low-light night time driving scenarios. On the other hand,\nthe quality of a thermal camera image remains unaffected in similar conditions.\nThis paper proposes an end-to-end multimodal fusion model for pedestrian\ndetection using RGB and thermal images. Its novel spatio-contextual deep\nnetwork architecture is capable of exploiting the multimodal input efficiently.\nIt consists of two distinct deformable ResNeXt-50 encoders for feature\nextraction from the two modalities. Fusion of these two encoded features takes\nplace inside a multimodal feature embedding module (MuFEm) consisting of\nseveral groups of a pair of Graph Attention Network and a feature fusion unit.\nThe output of the last feature fusion unit of MuFEm is subsequently passed to\ntwo CRFs for their spatial refinement. Further enhancement of the features is\nachieved by applying channel-wise attention and extraction of contextual\ninformation with the help of four RNNs traversing in four different directions.\nFinally, these feature maps are used by a single-stage decoder to generate the\nbounding box of each pedestrian and the score map. We have performed extensive\nexperiments of the proposed framework on three publicly available multimodal\npedestrian detection benchmark datasets, namely KAIST, CVC-14, and UTokyo. The\nresults on each of them improved the respective state-of-the-art performance. A\nshort video giving an overview of this work along with its qualitative results\ncan be seen at https://youtu.be/FDJdSifuuCs.",
          "link": "http://arxiv.org/abs/2105.12713",
          "publishedOn": "2021-05-27T01:32:29.286Z",
          "wordCount": 683,
          "title": "Spatio-Contextual Deep Network Based Multimodal Pedestrian Detection For Autonomous Driving. (arXiv:2105.12713v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Carabarin_L/0/1/0/all/0/1\">Lizeth Gonzalez-Carabarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huijben_I/0/1/0/all/0/1\">Iris A.M. Huijben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1\">Bastiaan S. Veeling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_A/0/1/0/all/0/1\">Alexandre Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1\">Ruud J.G. van Sloun</a>",
          "description": "Unstructured neural network pruning algorithms have achieved impressive\ncompression rates. However, the resulting - typically irregular - sparse\nmatrices hamper efficient hardware implementations, leading to additional\nmemory usage and complex control logic that diminishes the benefits of\nunstructured pruning. This has spurred structured coarse-grained pruning\nsolutions that prune entire filters or even layers, enabling efficient\nimplementation at the expense of reduced flexibility. Here we propose a\nflexible new pruning mechanism that facilitates pruning at different\ngranularities (weights, kernels, filters/feature maps), while retaining\nefficient memory organization (e.g. pruning exactly k-out-of-n weights for\nevery output neuron, or pruning exactly k-out-of-n kernels for every feature\nmap). We refer to this algorithm as Dynamic Probabilistic Pruning (DPP). DPP\nleverages the Gumbel-softmax relaxation for differentiable k-out-of-n sampling,\nfacilitating end-to-end optimization. We show that DPP achieves competitive\ncompression rates and classification accuracy when pruning common deep learning\nmodels trained on different benchmark datasets for image classification.\nRelevantly, the non-magnitude-based nature of DPP allows for joint optimization\nof pruning and weight quantization in order to even further compress the\nnetwork, which we show as well. Finally, we propose novel information theoretic\nmetrics that show the confidence and pruning diversity of pruning masks within\na layer.",
          "link": "http://arxiv.org/abs/2105.12686",
          "publishedOn": "2021-05-27T01:32:29.267Z",
          "wordCount": 644,
          "title": "Dynamic Probabilistic Pruning: A general framework for hardware-constrained pruning at different granularities. (arXiv:2105.12686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.02264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boguszewski_A/0/1/0/all/0/1\">Adrian Boguszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batorski_D/0/1/0/all/0/1\">Dominik Batorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziemba_Jankowska_N/0/1/0/all/0/1\">Natalia Ziemba-Jankowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziedzic_T/0/1/0/all/0/1\">Tomasz Dziedzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zambrzycka_A/0/1/0/all/0/1\">Anna Zambrzycka</a>",
          "description": "Monitoring of land cover and land use is crucial in natural resources\nmanagement. Automatic visual mapping can carry enormous economic value for\nagriculture, forestry, or public administration. Satellite or aerial images\ncombined with computer vision and deep learning enable precise assessment and\ncan significantly speed up change detection. Aerial imagery usually provides\nimages with much higher pixel resolution than satellite data allowing more\ndetailed mapping. However, there is still a lack of aerial datasets made for\nthe segmentation, covering rural areas with a resolution of tens centimeters\nper pixel, manual fine labels, and highly publicly important environmental\ninstances like buildings, woods, water, or roads.\n\nHere we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for\nsemantic segmentation. We collected images of 216.27 sq. km rural areas across\nPoland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per\npixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine\nannotated four following classes of objects: buildings, woodlands, water, and\nroads. Additionally, we report simple benchmark results, achieving 85.56% of\nmean intersection over union on the test set. It proves that the automatic\nmapping of land cover is possible with a relatively small, cost-efficient,\nRGB-only dataset. The dataset is publicly available at https://landcover.ai",
          "link": "http://arxiv.org/abs/2005.02264",
          "publishedOn": "2021-05-27T01:32:29.260Z",
          "wordCount": 689,
          "title": "LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery. (arXiv:2005.02264v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlett_T/0/1/0/all/0/1\">Torsten Schlett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henniger_O/0/1/0/all/0/1\">Olaf Henniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1\">Javier Galbally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "The performance of face analysis and recognition systems depends on the\nquality of the acquired face data, which is influenced by numerous factors.\nAutomatically assessing the quality of face data in terms of biometric utility\ncan thus be useful to detect low-quality data and make decisions accordingly.\nThis survey provides an overview of the face image quality assessment\nliterature, which predominantly focuses on single visible wavelength face image\ninput. A trend towards deep learning based methods is observed, including\nnotable conceptual differences among the recent approaches, such as the\nintegration of quality assessment into face recognition models. Besides image\nselection, face image quality assessment can also be used in a variety of other\napplication scenarios, which are discussed herein. Open issues and challenges\nare pointed out, i.a. highlighting the importance of comparability for\nalgorithm evaluations, and the challenge for future work to create deep\nlearning approaches that are interpretable in addition to providing accurate\nutility predictions.",
          "link": "http://arxiv.org/abs/2009.01103",
          "publishedOn": "2021-05-27T01:32:29.221Z",
          "wordCount": 617,
          "title": "Face Image Quality Assessment: A Literature Survey. (arXiv:2009.01103v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1\">Mohamad Wehbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamann_T/0/1/0/all/0/1\">Tim Hamann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_J/0/1/0/all/0/1\">Jens Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaempf_P/0/1/0/all/0/1\">Peter Kaempf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1\">Dario Zanca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1\">Bjoern Eskofier</a>",
          "description": "Most online handwriting recognition systems require the use of specific\nwriting surfaces to extract positional data. In this paper we present a online\nhandwriting recognition system for word recognition which is based on inertial\nmeasurement units (IMUs) for digitizing text written on paper. This is obtained\nby means of a sensor-equipped pen that provides acceleration, angular velocity,\nand magnetic forces streamed via Bluetooth. Our model combines convolutional\nand bidirectional LSTM networks, and is trained with the Connectionist Temporal\nClassification loss that allows the interpretation of raw sensor data into\nwords without the need of sequence segmentation. We use a dataset of words\ncollected using multiple sensor-enhanced pens and evaluate our model on\ndistinct test sets of seen and unseen words achieving a character error rate of\n17.97% and 17.08%, respectively, without the use of a dictionary or language\nmodel",
          "link": "http://arxiv.org/abs/2105.12434",
          "publishedOn": "2021-05-27T01:32:29.208Z",
          "wordCount": 576,
          "title": "Towards an IMU-based Pen Online Handwriting Recognizer. (arXiv:2105.12434v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1\">Yash Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>",
          "description": "This paper proposes a procedure to train a scene text recognition model using\na robust learned surrogate of edit distance. The proposed method borrows from\nself-paced learning and filters out the training examples that are hard for the\nsurrogate. The filtering is performed by judging the quality of the\napproximation, using a ramp function, enabling end-to-end training. Following\nthe literature, the experiments are conducted in a post-tuning setup, where a\ntrained scene text recognition model is tuned using the learned surrogate of\nedit distance. The efficacy is demonstrated by improvements on various\nchallenging scene text datasets such as IIIT-5K, SVT, ICDAR, SVTP, and CUTE.\nThe proposed method provides an average improvement of $11.2 \\%$ on total edit\ndistance and an error reduction of $9.5\\%$ on accuracy.",
          "link": "http://arxiv.org/abs/2103.04635",
          "publishedOn": "2021-05-27T01:32:29.188Z",
          "wordCount": 580,
          "title": "FEDS -- Filtered Edit Distance Surrogate. (arXiv:2103.04635v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laugros_A/0/1/0/all/0/1\">Alfred Laugros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caplier_A/0/1/0/all/0/1\">Alice Caplier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ospici_M/0/1/0/all/0/1\">Matthieu Ospici</a>",
          "description": "Neural Networks are sensitive to various corruptions that usually occur in\nreal-world applications such as blurs, noises, low-lighting conditions, etc. To\nestimate the robustness of neural networks to these common corruptions, we\ngenerally use a group of modeled corruptions gathered into a benchmark.\nUnfortunately, no objective criterion exists to determine whether a benchmark\nis representative of a large diversity of independent corruptions. In this\npaper, we propose a metric called corruption overlapping score, which can be\nused to reveal flaws in corruption benchmarks. Two corruptions overlap when the\nrobustnesses of neural networks to these corruptions are correlated. We argue\nthat taking into account overlappings between corruptions can help to improve\nexisting benchmarks or build better ones.",
          "link": "http://arxiv.org/abs/2105.12357",
          "publishedOn": "2021-05-27T01:32:29.167Z",
          "wordCount": 545,
          "title": "Using the Overlapping Score to Improve Corruption Benchmarks. (arXiv:2105.12357v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "We study the problem of unsupervised discovery and segmentation of object\nparts, which, as an intermediate local representation, are capable of finding\nintrinsic object structure and providing more explainable recognition results.\nRecent unsupervised methods have greatly relaxed the dependency on annotated\ndata which are costly to obtain, but still rely on additional information such\nas object segmentation mask or saliency map. To remove such a dependency and\nfurther improve the part segmentation performance, we develop a novel approach\nby disentangling the appearance and shape representations of object parts\nfollowed with reconstruction losses without using additional object mask\ninformation. To avoid degenerated solutions, a bottleneck block is designed to\nsqueeze and expand the appearance representation, leading to a more effective\ndisentanglement between geometry and appearance. Combined with a\nself-supervised part classification loss and an improved geometry concentration\nconstraint, we can segment more consistent parts with semantic meanings.\nComprehensive experiments on a wide variety of objects such as face, bird, and\nPASCAL VOC objects demonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2105.12405",
          "publishedOn": "2021-05-27T01:32:29.073Z",
          "wordCount": 605,
          "title": "Unsupervised Part Segmentation through Disentangling Appearance and Shape. (arXiv:2105.12405v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaozhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafie_M/0/1/0/all/0/1\">Manouchehr Rafie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curcio_I/0/1/0/all/0/1\">Igor Curcio</a>",
          "description": "In recent years, video data has dominated internet traffic and becomes one of\nthe major data formats. With the emerging 5G and internet of things (IoT)\ntechnologies, more and more videos are generated by edge devices, sent across\nnetworks, and consumed by machines. The volume of video consumed by machine is\nexceeding the volume of video consumed by humans. Machine vision tasks include\nobject detection, segmentation, tracking, and other machine-based applications,\nwhich are quite different from those for human consumption. On the other hand,\ndue to large volumes of video data, it is essential to compress video before\ntransmission. Thus, efficient video coding for machines (VCM) has become an\nimportant topic in academia and industry. In July 2019, the international\nstandardization organization, i.e., MPEG, created an Ad-Hoc group named VCM to\nstudy the requirements for potential standardization work. In this paper, we\nwill address the recent development activities in the MPEG VCM group.\nSpecifically, we will first provide an overview of the MPEG VCM group including\nuse cases, requirements, processing pipelines, plan for potential VCM\nstandards, followed by the evaluation framework including machine-vision tasks,\ndataset, evaluation metrics, and anchor generation. We then introduce\ntechnology solutions proposed so far and discuss the recent responses to the\nCall for Evidence issued by MPEG VCM group.",
          "link": "http://arxiv.org/abs/2105.12653",
          "publishedOn": "2021-05-27T01:32:29.055Z",
          "wordCount": 650,
          "title": "Recent Standard Development Activities on Video Coding for Machines. (arXiv:2105.12653v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaofei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhong Dong</a>",
          "description": "Inspired by the conclusion that humans choose the visual cortex regions\ncorresponding to the real size of an object to analyze its features when\nidentifying objects in the real world, this paper presents a framework,\nSizeNet, which is based on both the real sizes and features of objects to solve\nobject recognition problems. SizeNet was used for object recognition\nexperiments on the homemade Rsize dataset, and was compared with the\nstate-of-the-art methods AlexNet, VGG-16, Inception V3, Resnet-18, and\nDenseNet-121. The results showed that SizeNet provides much higher accuracy\nrates for object recognition than the other algorithms. SizeNet can solve the\ntwo problems of correctly recognizing objects with highly similar features but\nreal sizes that are obviously different from each other, and correctly\ndistinguishing a target object from interference objects whose real sizes are\nobviously different from the target object. This is because SizeNet recognizes\nobjects based not only on their features, but also on their real size. The real\nsize of an object can help exclude the interference object's categories whose\nreal size ranges do not match the real size of the object, which greatly\nreduces the object's categories' number in the label set used for the\ndownstream object recognition based on object features. SizeNet is of great\nsignificance for studying the interpretable computer vision. Our code and\ndataset will thus be made public.",
          "link": "http://arxiv.org/abs/2105.06188",
          "publishedOn": "2021-05-27T01:32:28.808Z",
          "wordCount": 674,
          "title": "SizeNet: Object Recognition via Object Real Size-based Convolutional Networks. (arXiv:2105.06188v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jou_S/0/1/0/all/0/1\">Shyh Yaw Jou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chung Yen Su</a>",
          "description": "In the paper of ExquisiteNetV1, the ability of classification of\nExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and\nbetter model ExquisiteNetV2. We conduct many experiments to evaluate its\nperformance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known\nmodels on 15 credible datasets under the same condition. According to the\nexperimental results, ExquisiteNetV2 gets the highest classification accuracy\nover half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts\nof parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing\nspeed.",
          "link": "http://arxiv.org/abs/2105.09008",
          "publishedOn": "2021-05-27T01:32:28.780Z",
          "wordCount": 539,
          "title": "A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_N/0/1/0/all/0/1\">Nguyen Anh Minh Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duthon_P/0/1/0/all/0/1\">Pierre Duthon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoudour_L/0/1/0/all/0/1\">Louahdi Khoudour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crouzil_A/0/1/0/all/0/1\">Alain Crouzil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velastin_S/0/1/0/all/0/1\">Sergio A. Velastin</a>",
          "description": "The ability to accurately detect and localize objects is recognized as being\nthe most important for the perception of self-driving cars. From 2D to 3D\nobject detection, the most difficult is to determine the distance from the\nego-vehicle to objects. Expensive technology like LiDAR can provide a precise\nand accurate depth information, so most studies have tended to focus on this\nsensor showing a performance gap between LiDAR-based methods and camera-based\nmethods. Although many authors have investigated how to fuse LiDAR with RGB\ncameras, as far as we know there are no studies to fuse LiDAR and stereo in a\ndeep neural network for the 3D object detection task. This paper presents\nSLS-Fusion, a new approach to fuse data from 4-beam LiDAR and a stereo camera\nvia a neural network for depth estimation to achieve better dense depth maps\nand thereby improves 3D object detection performance. Since 4-beam LiDAR is\ncheaper than the well-known 64-beam LiDAR, this approach is also classified as\na low-cost sensors-based method. Through evaluation on the KITTI benchmark, it\nis shown that the proposed method significantly improves depth estimation\nperformance compared to a baseline method. Also, when applying it to 3D object\ndetection, a new state of the art on low-cost sensor based method is achieved.",
          "link": "http://arxiv.org/abs/2103.03977",
          "publishedOn": "2021-05-27T01:32:28.740Z",
          "wordCount": 689,
          "title": "Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimationand 3D Object Detection. (arXiv:2103.03977v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Keyulu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mozhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1\">Stefanie Jegelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>",
          "description": "Graph Neural Networks (GNNs) have been studied through the lens of expressive\npower and generalization. However, their optimization properties are less well\nunderstood. We take the first step towards analyzing GNN training by studying\nthe gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that\ndespite the non-convexity of training, convergence to a global minimum at a\nlinear rate is guaranteed under mild assumptions that we validate on real-world\ngraphs. Second, we study what may affect the GNNs' training speed. Our results\nshow that the training of GNNs is implicitly accelerated by skip connections,\nmore depth, and/or a good label distribution. Empirical results confirm that\nour theoretical results for linearized GNNs align with the training behavior of\nnonlinear GNNs. Our results provide the first theoretical support for the\nsuccess of GNNs with skip connections in terms of optimization, and suggest\nthat deep GNNs with skip connections would be promising in practice.",
          "link": "http://arxiv.org/abs/2105.04550",
          "publishedOn": "2021-05-27T01:32:28.728Z",
          "wordCount": 622,
          "title": "Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth. (arXiv:2105.04550v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walters_C/0/1/0/all/0/1\">Celyn Walters</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1\">Oscar Mendez</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a> (1) ((1) CVSSP, University of Surrey)",
          "description": "Across a wide range of applications, from autonomous vehicles to medical\nimaging, multi-spectral images provide an opportunity to extract additional\ninformation not present in color images. One of the most important steps in\nmaking this information readily available is the accurate estimation of dense\ncorrespondences between different spectra.\n\nDue to the nature of cross-spectral images, most correspondence solving\ntechniques for the visual domain are simply not applicable. Furthermore, most\ncross-spectral techniques utilize spectra-specific characteristics to perform\nthe alignment. In this work, we aim to address the dense correspondence\nestimation problem in a way that generalizes to more than one spectrum. We do\nthis by introducing a novel cycle-consistency metric that allows us to\nself-supervise. This, combined with our spectra-agnostic loss functions, allows\nus to train the same network across multiple spectra.\n\nWe demonstrate our approach on the challenging task of dense RGB-FIR\ncorrespondence estimation. We also show the performance of our unmodified\nnetwork on the cases of RGB-NIR and RGB-RGB, where we achieve higher accuracy\nthan similar self-supervised approaches. Our work shows that cross-spectral\ncorrespondence estimation can be solved in a common framework that learns to\ngeneralize alignment across spectra.",
          "link": "http://arxiv.org/abs/2103.10768",
          "publishedOn": "2021-05-27T01:32:28.598Z",
          "wordCount": 671,
          "title": "There and Back Again: Self-supervised Multispectral Correspondence Estimation. (arXiv:2103.10768v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daliparthi_V/0/1/0/all/0/1\">Venkata Satya Sai Ajay Daliparthi</a>",
          "description": "In recent years, deep neural networks (DNNs) achieved state-of-the-art\nperformance on many computer vision tasks. However, the one typical drawback of\nthese DNNs is the requirement of massive labeled data. Even though few-shot\nlearning methods addressed this problem through metric-learning and\nmeta-learning techniques, in this work, we address this problem from a\nneuroscience perspective. We propose a theory named Ikshana, to explain the\nfunctioning of the human brain, while humans understand an image. By following\nthe Ikshana theory, we propose a novel neural-inspired CNN architecture named\nIkshanaNet for semantic segmentation. The empirical results demonstrate the\neffectiveness of our method on few data samples, outperforming several\nbaselines, on the Cityscapes and the CamVid benchmarks.",
          "link": "http://arxiv.org/abs/2101.10837",
          "publishedOn": "2021-05-27T01:32:28.592Z",
          "wordCount": 573,
          "title": "Ikshana: A Theory of Human Scene Understanding Mechanism. (arXiv:2101.10837v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shi-Wei Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jyh-Horng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jo-Yu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1\">Chien-Hao Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Meng-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fang-Pang Lin</a>",
          "description": "In the monsoon season, sudden flood events occur frequently in urban areas,\nwhich hamper the social and economic activities and may threaten the\ninfrastructure and lives. The use of an efficient large-scale waterlogging\nsensing and information system can provide valuable real-time disaster\ninformation to facilitate disaster management and enhance awareness of the\ngeneral public to alleviate losses during and after flood disasters. Therefore,\nin this study, a visual sensing approach driven by deep neural networks and\ninformation and communication technology was developed to provide an end-to-end\nmechanism to realize waterlogging sensing and event-location mapping. The use\nof a deep sensing system in the monsoon season in Taiwan was demonstrated, and\nwaterlogging events were predicted on the island-wide scale. The system could\nsense approximately 2379 vision sources through an internet of video things\nframework and transmit the event-location information in 5 min. The proposed\napproach can sense waterlogging events at a national scale and provide an\nefficient and highly scalable alternative to conventional waterlogging sensing\nmethods.",
          "link": "http://arxiv.org/abs/2103.05927",
          "publishedOn": "2021-05-27T01:32:28.542Z",
          "wordCount": 636,
          "title": "Deep Sensing of Urban Waterlogging. (arXiv:2103.05927v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keser_M/0/1/0/all/0/1\">Mert Keser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savkin_A/0/1/0/all/0/1\">Artem Savkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Synthetic data generation is an appealing approach to generate novel traffic\nscenarios in autonomous driving. However, deep learning techniques trained\nsolely on synthetic data encounter dramatic performance drops when they are\ntested on real data. Such performance drop is commonly attributed to the domain\ngap between real and synthetic data. Domain adaptation methods have been\napplied to mitigate the aforementioned domain gap. These methods achieve\nvisually appealing results, but the translated samples usually introduce\nsemantic inconsistencies. In this work, we propose a new, unsupervised,\nend-to-end domain adaptation network architecture that enables semantically\nconsistent domain adaptation between synthetic and real data. We evaluate our\narchitecture on the downstream task of semantic segmentation and show that our\nmethod achieves superior performance compared to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.08704",
          "publishedOn": "2021-05-27T01:32:28.526Z",
          "wordCount": 569,
          "title": "Content Disentanglement for Semantically Consistent Synthetic-to-Real Domain Adaptation. (arXiv:2105.08704v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongfei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>",
          "description": "In this paper, we quest the capability of transferring the quality of natural\nscene images to the images that are not acquired by optical cameras (e.g.,\nscreen content images, SCIs), rooted in the widely accepted view that the human\nvisual system has adapted and evolved through the perception of natural\nenvironment. Here, we develop the first unsupervised domain adaptation based no\nreference quality assessment method for SCIs, leveraging rich subjective\nratings of the natural images (NIs). In general, it is a non-trivial task to\ndirectly transfer the quality prediction model from NIs to a new type of\ncontent (i.e., SCIs) that holds dramatically different statistical\ncharacteristics. Inspired by the transferability of pair-wise relationship, the\nproposed quality measure operates based on the philosophy of improving the\ntransferability and discriminability simultaneously. In particular, we\nintroduce three types of losses which complementarily and explicitly regularize\nthe feature space of ranking in a progressive manner. Regarding feature\ndiscriminatory capability enhancement, we propose a center based loss to\nrectify the classifier and improve its prediction capability not only for\nsource domain (NI) but also the target domain (SCI). For feature discrepancy\nminimization, the maximum mean discrepancy (MMD) is imposed on the extracted\nranking features of NIs and SCIs. Furthermore, to further enhance the feature\ndiversity, we introduce the correlation penalization between different feature\ndimensions, leading to the features with lower rank and higher diversity.\nExperiments show that our method can achieve higher performance on different\nsource-target settings based on a light-weight convolution neural network. The\nproposed method also sheds light on learning quality assessment measures for\nunseen application-specific content without the cumbersome and costing\nsubjective evaluations.",
          "link": "http://arxiv.org/abs/2008.08561",
          "publishedOn": "2021-05-27T01:32:28.521Z",
          "wordCount": 751,
          "title": "No-reference Screen Content Image Quality Assessment with Unsupervised Domain Adaptation. (arXiv:2008.08561v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gerg_I/0/1/0/all/0/1\">Isaac D. Gerg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>",
          "description": "Synthetic aperture sonar (SAS) requires precise time-of-flight measurements\nof the transmitted/received waveform to produce well-focused imagery. It is not\nuncommon for errors in these measurements to be present resulting in image\ndefocusing. To overcome this, an \\emph{autofocus} algorithm is employed as a\npost-processing step after image reconstruction to improve image focus. A\nparticular class of these algorithms can be framed as a sharpness/contrast\nmetric-based optimization. To improve convergence, a hand-crafted weighting\nfunction to remove \"bad\" areas of the image is sometimes applied to the\nimage-under-test before the optimization procedure. Additionally, dozens of\niterations are necessary for convergence which is a large compute burden for\nlow size, weight, and power (SWaP) systems. We propose a deep learning\ntechnique to overcome these limitations and implicitly learn the weighting\nfunction in a data-driven manner. Our proposed method, which we call Deep\nAutofocus, uses features from the single-look-complex (SLC) to estimate the\nphase correction which is applied in $k$-space. Furthermore, we train our\nalgorithm on batches of training imagery so that during deployment, only a\nsingle iteration of our method is sufficient to autofocus. We show results\ndemonstrating the robustness of our technique by comparing our results to four\ncommonly used image sharpness metrics. Our results demonstrate Deep Autofocus\ncan produce imagery perceptually better than common iterative techniques but at\na lower computational cost. We conclude that Deep Autofocus can provide a more\nfavorable cost-quality trade-off than alternatives with significant potential\nof future research.",
          "link": "http://arxiv.org/abs/2103.10312",
          "publishedOn": "2021-05-27T01:32:28.515Z",
          "wordCount": 702,
          "title": "Real-Time, Deep Synthetic Aperture Sonar (SAS) Autofocus. (arXiv:2103.10312v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>",
          "description": "Popular network pruning algorithms reduce redundant information by optimizing\nhand-crafted models, and may cause suboptimal performance and long time in\nselecting filters. We innovatively introduce adaptive exemplar filters to\nsimplify the algorithm design, resulting in an automatic and efficient pruning\napproach called EPruner. Inspired by the face recognition community, we use a\nmessage passing algorithm Affinity Propagation on the weight matrices to obtain\nan adaptive number of exemplars, which then act as the preserved filters.\nEPruner breaks the dependency on the training data in determining the\n\"important\" filters and allows the CPU implementation in seconds, an order of\nmagnitude faster than GPU based SOTAs. Moreover, we show that the weights of\nexemplars provide a better initialization for the fine-tuning. On VGGNet-16,\nEPruner achieves a 76.34%-FLOPs reduction by removing 88.80% parameters, with\n0.06% accuracy improvement on CIFAR-10. In ResNet-152, EPruner achieves a\n65.12%-FLOPs reduction by removing 64.18% parameters, with only 0.71% top-5\naccuracy loss on ILSVRC-2012. Our code can be available at\nhttps://github.com/lmbxmu/EPruner.",
          "link": "http://arxiv.org/abs/2101.07985",
          "publishedOn": "2021-05-27T01:32:28.448Z",
          "wordCount": 658,
          "title": "Network Pruning using Adaptive Exemplar Filters. (arXiv:2101.07985v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewaisha_M/0/1/0/all/0/1\">Mahmoud Ewaisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siam_M/0/1/0/all/0/1\">Mennatullah Siam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdy_W/0/1/0/all/0/1\">Waleed Hamdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helmi_M/0/1/0/all/0/1\">Muhammad Helmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>",
          "description": "Moving object segmentation is a crucial task for autonomous vehicles as it\ncan be used to segment objects in a class agnostic manner based on their motion\ncues. It enables the detection of unseen objects during training (e.g., moose\nor a construction truck) based on their motion and independent of their\nappearance. Although pixel-wise motion segmentation has been studied in\nautonomous driving literature, it has been rarely addressed at the instance\nlevel, which would help separate connected segments of moving objects leading\nto better trajectory planning. As the main issue is the lack of large public\ndatasets, we create a new InstanceMotSeg dataset comprising of 12.9K samples\nimproving upon our KITTIMoSeg dataset. In addition to providing instance level\nannotations, we have added 4 additional classes which is crucial for studying\nclass agnostic motion segmentation. We adapt YOLACT and implement a\nmotion-based class agnostic instance segmentation model which would act as a\nbaseline for the dataset. We also extend it to an efficient multi-task model\nwhich additionally provides semantic instance segmentation sharing the encoder.\nThe model then learns separate prototype coefficients within the class agnostic\nand semantic heads providing two independent paths of object detection for\nredundant safety. To obtain real-time performance, we study different efficient\nencoders and obtain 39 fps on a Titan Xp GPU using MobileNetV2 with an\nimprovement of 10% mAP relative to the baseline. Our model improves the\nprevious state of the art motion segmentation method by 3.3%. The dataset and\nqualitative results video are shared in our website at\nhttps://sites.google.com/view/instancemotseg/.",
          "link": "http://arxiv.org/abs/2008.07008",
          "publishedOn": "2021-05-27T01:32:28.256Z",
          "wordCount": 766,
          "title": "Monocular Instance Motion Segmentation for Autonomous Driving: KITTI InstanceMotSeg Dataset and Multi-task Baseline. (arXiv:2008.07008v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingchen Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_H/0/1/0/all/0/1\">Hongbo Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>",
          "description": "Camouflaged object detection (COD), which aims to identify the objects that\nconceal themselves into the surroundings, has recently drawn increasing\nresearch efforts in the field of computer vision. In practice, the success of\ndeep learning based COD is mainly determined by two key factors, including (i)\nA significantly large receptive field, which provides rich context information,\nand (ii) An effective fusion strategy, which aggregates the rich multi-level\nfeatures for accurate COD. Motivated by these observations, in this paper, we\npropose a novel deep learning based COD approach, which integrates the large\nreceptive field and effective feature fusion into a unified framework.\nSpecifically, we first extract multi-level features from a backbone network.\nThe resulting features are then fed to the proposed dual-branch mixture\nconvolution modules, each of which utilizes multiple asymmetric convolutional\nlayers and two dilated convolutional layers to extract rich context features\nfrom a large receptive field. Finally, we fuse the features using\nspecially-designed multi-level interactive fusion modules, each of which\nemploys an attention mechanism along with feature interaction for effective\nfeature fusion. Our method detects camouflaged objects with an effective fusion\nstrategy, which aggregates the rich context information from a large receptive\nfield. All of these designs meet the requirements of COD well, allowing the\naccurate detection of camouflaged objects. Extensive experiments on widely-used\nbenchmark datasets demonstrate that our method is capable of accurately\ndetecting camouflaged objects and outperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2101.05687",
          "publishedOn": "2021-05-27T01:32:28.250Z",
          "wordCount": 696,
          "title": "Accurate Camouflaged Object Detection via Mixture Convolution and Interactive Fusion. (arXiv:2101.05687v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Llorca_D/0/1/0/all/0/1\">David Fern&#xe1;ndez Llorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Antonio Hern&#xe1;ndez Mart&#xed;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daza_I/0/1/0/all/0/1\">Iv&#xe1;n Garc&#xed;a Daza</a>",
          "description": "The need to accurately estimate the speed of road vehicles is becoming\nincreasingly important for at least two main reasons. First, the number of\nspeed cameras installed worldwide has been growing in recent years, as the\nintroduction and enforcement of appropriate speed limits is considered one of\nthe most effective means to increase the road safety. Second, traffic\nmonitoring and forecasting in road networks plays a fundamental role to enhance\ntraffic, emissions and energy consumption in smart cities, being the speed of\nthe vehicles one of the most relevant parameters of the traffic state. Among\nthe technologies available for the accurate detection of vehicle speed, the use\nof vision-based systems brings great challenges to be solved, but also great\npotential advantages, such as the drastic reduction of costs due to the absence\nof expensive range sensors, and the possibility of identifying vehicles\naccurately. This paper provides a review of vision-based vehicle speed\nestimation. We describe the terminology, the application domains, and propose a\ncomplete taxonomy of a large selection of works that categorizes all stages\ninvolved. An overview of performance evaluation metrics and available datasets\nis provided. Finally, we discuss current limitations and future directions.",
          "link": "http://arxiv.org/abs/2101.06159",
          "publishedOn": "2021-05-27T01:32:28.244Z",
          "wordCount": 673,
          "title": "Vision-based Vehicle Speed Estimation: A Survey. (arXiv:2101.06159v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bron_E/0/1/0/all/0/1\">Esther E. Bron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papma_J/0/1/0/all/0/1\">Janne M. Papma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiskoot_L/0/1/0/all/0/1\">Lize C. Jiskoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Venkatraghavan_V/0/1/0/all/0/1\">Vikram Venkatraghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linders_J/0/1/0/all/0/1\">Jara Linders</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aalten_P/0/1/0/all/0/1\">Pauline Aalten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deyn_P/0/1/0/all/0/1\">Peter Paul De Deyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biessels_G/0/1/0/all/0/1\">Geert Jan Biessels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Claassen_J/0/1/0/all/0/1\">Jurgen A.H.R. Claassen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Middelkoop_H/0/1/0/all/0/1\">Huub A.M. Middelkoop</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smits_M/0/1/0/all/0/1\">Marion Smits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niessen_W/0/1/0/all/0/1\">Wiro J. Niessen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Swieten_J/0/1/0/all/0/1\">John C. van Swieten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Flier_W/0/1/0/all/0/1\">Wiesje M. van der Flier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramakers_I/0/1/0/all/0/1\">Inez H.G.B. Ramakers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lugt_A/0/1/0/all/0/1\">Aad van der Lugt</a>",
          "description": "This work validates the generalizability of MRI-based classification of\nAlzheimer's disease (AD) patients and controls (CN) to an external data set and\nto the task of prediction of conversion to AD in individuals with mild\ncognitive impairment (MCI). We used a conventional support vector machine (SVM)\nand a deep convolutional neural network (CNN) approach based on structural MRI\nscans that underwent either minimal pre-processing or more extensive\npre-processing into modulated gray matter (GM) maps. Classifiers were optimized\nand evaluated using cross-validation in the ADNI (334 AD, 520 CN). Trained\nclassifiers were subsequently applied to predict conversion to AD in ADNI MCI\npatients (231 converters, 628 non-converters) and in the independent Health-RI\nParelsnoer data set. From this multi-center study representing a tertiary\nmemory clinic population, we included 199 AD patients, 139 participants with\nsubjective cognitive decline, 48 MCI patients converting to dementia, and 91\nMCI patients who did not convert to dementia. AD-CN classification based on\nmodulated GM maps resulted in a similar AUC for SVM (0.940) and CNN (0.933).\nApplication to conversion prediction in MCI yielded significantly higher\nperformance for SVM (0.756) than for CNN (0.742). In external validation,\nperformance was slightly decreased. For AD-CN, it again gave similar AUCs for\nSVM (0.896) and CNN (0.876). For prediction in MCI, performances decreased for\nboth SVM (0.665) and CNN (0.702). Both with SVM and CNN, classification based\non modulated GM maps significantly outperformed classification based on\nminimally processed images. Deep and conventional classifiers performed equally\nwell for AD classification and their performance decreased only slightly when\napplied to the external cohort. We expect that this work on external validation\ncontributes towards translation of machine learning to clinical practice.",
          "link": "http://arxiv.org/abs/2012.08769",
          "publishedOn": "2021-05-27T01:32:28.228Z",
          "wordCount": 805,
          "title": "Cross-Cohort Generalizability of Deep and Conventional Machine Learning for MRI-based Diagnosis and Prediction of Alzheimer's Disease. (arXiv:2012.08769v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>",
          "description": "Active stereo cameras that recover depth from structured light captures have\nbecome a cornerstone sensor modality for 3D scene reconstruction and\nunderstanding tasks across application domains. Existing active stereo cameras\nproject a pseudo-random dot pattern on object surfaces to extract disparity\nindependently of object texture. Such hand-crafted patterns are designed in\nisolation from the scene statistics, ambient illumination conditions, and the\nreconstruction method. In this work, we propose the first method to jointly\nlearn structured illumination and reconstruction, parameterized by a\ndiffractive optical element and a neural network, in an end-to-end fashion. To\nthis end, we introduce a novel differentiable image formation model for active\nstereo, relying on both wave and geometric optics, and a novel trinocular\nreconstruction network. The jointly optimized pattern, which we dub \"Polka\nLines,\" together with the reconstruction network, achieve state-of-the-art\nactive-stereo depth estimates across imaging conditions. We validate the\nproposed method in simulation and on a hardware prototype, and show that our\nmethod outperforms existing active stereo systems.",
          "link": "http://arxiv.org/abs/2011.13117",
          "publishedOn": "2021-05-27T01:32:28.221Z",
          "wordCount": 619,
          "title": "Polka Lines: Learning Structured Illumination and Reconstruction for Active Stereo. (arXiv:2011.13117v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02101",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bozorgtabar_B/0/1/0/all/0/1\">Behzad Bozorgtabar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thiran_J/0/1/0/all/0/1\">Jean-Philippe Thiran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Although generative adversarial network (GAN) based style transfer is state\nof the art in histopathology color-stain normalization, they do not explicitly\nintegrate structural information of tissues. We propose a self-supervised\napproach to incorporate semantic guidance into a GAN based stain normalization\nframework and preserve detailed structural information. Our method does not\nrequire manual segmentation maps which is a significant advantage over existing\nmethods. We integrate semantic information at different layers between a\npre-trained semantic network and the stain color normalization network. The\nproposed scheme outperforms other color normalization methods leading to better\nclassification and segmentation performance.",
          "link": "http://arxiv.org/abs/2008.02101",
          "publishedOn": "2021-05-27T01:32:28.188Z",
          "wordCount": 558,
          "title": "Structure Preserving Stain Normalization of Histopathology Images Using Self-Supervised Semantic Guidance. (arXiv:2008.02101v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.14439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zequn Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "Dynamic routing networks, aimed at finding the best routing paths in the\nnetworks, have achieved significant improvements to neural networks in terms of\naccuracy and efficiency. In this paper, we see dynamic routing networks in a\nfresh light, formulating a routing method as a mapping from a sample space to a\nrouting space. From the perspective of space mapping, prevalent methods of\ndynamic routing didn't consider how inference paths would be distributed in the\nrouting space. Thus, we propose a novel method, termed CoDiNet, to model the\nrelationship between a sample space and a routing space by regularizing the\ndistribution of routing paths with the properties of consistency and diversity.\nSpecifically, samples with similar semantics should be mapped into the same\narea in routing space, while those with dissimilar semantics should be mapped\ninto different areas. Moreover, we design a customizable dynamic routing\nmodule, which can strike a balance between accuracy and efficiency. When\ndeployed upon ResNet models, our method achieves higher performance and\neffectively reduces average computational cost on four widely used datasets.",
          "link": "http://arxiv.org/abs/2005.14439",
          "publishedOn": "2021-05-27T01:32:28.183Z",
          "wordCount": 646,
          "title": "CoDiNet: Path Distribution Modeling with Consistency and Diversity for Dynamic Routing. (arXiv:2005.14439v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.08717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deruyttere_T/0/1/0/all/0/1\">Thierry Deruyttere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collell_G/0/1/0/all/0/1\">Guillem Collell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>",
          "description": "We propose a new spatial memory module and a spatial reasoner for the Visual\nGrounding (VG) task. The goal of this task is to find a certain object in an\nimage based on a given textual query. Our work focuses on integrating the\nregions of a Region Proposal Network (RPN) into a new multi-step reasoning\nmodel which we have named a Multimodal Spatial Region Reasoner (MSRR). The\nintroduced model uses the object regions from an RPN as initialization of a 2D\nspatial memory and then implements a multi-step reasoning process scoring each\nregion according to the query, hence why we call it a multimodal reasoner. We\nevaluate this new model on challenging datasets and our experiments show that\nour model that jointly reasons over the object regions of the image and words\nof the query largely improves accuracy compared to current state-of-the-art\nmodels.",
          "link": "http://arxiv.org/abs/2003.08717",
          "publishedOn": "2021-05-27T01:32:28.177Z",
          "wordCount": 626,
          "title": "Giving Commands to a Self-driving Car: A Multimodal Reasoner for Visual Grounding. (arXiv:2003.08717v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>",
          "description": "Although hierarchical structures are popular in recent vision transformers,\nthey require sophisticated designs and massive datasets to work well. In this\nwork, we explore the idea of nesting basic local transformers on\nnon-overlapping image blocks and aggregating them in a hierarchical manner. We\nfind that the block aggregation function plays a critical role in enabling\ncross-block non-local information communication. This observation leads us to\ndesign a simplified architecture with minor code changes upon the original\nvision transformer and obtains improved performance compared to existing\nmethods. Our empirical results show that the proposed method NesT converges\nfaster and requires much less training data to achieve good generalization. For\nexample, a NesT with 68M parameters trained on ImageNet for 100/300 epochs\nachieves $82.3\\%/83.8\\%$ accuracy evaluated on $224\\times 224$ image size,\noutperforming previous methods with up to $57\\%$ parameter reduction. Training\na NesT with 6M parameters from scratch on CIFAR10 achieves $96\\%$ accuracy\nusing a single GPU, setting a new state of the art for vision transformers.\nBeyond image classification, we extend the key idea to image generation and\nshow NesT leads to a strong decoder that is 8$\\times$ faster than previous\ntransformer based generators. Furthermore, we also propose a novel method for\nvisually interpreting the learned model.",
          "link": "http://arxiv.org/abs/2105.12723",
          "publishedOn": "2021-05-27T01:32:28.171Z",
          "wordCount": 629,
          "title": "Aggregating Nested Transformers. (arXiv:2105.12723v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12430",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yuguang Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Junling Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>",
          "description": "Chest X-rays are the most commonly available and affordable radiological\nexamination for screening thoracic diseases. According to the domain knowledge\nof screening chest X-rays, the pathological information usually lay on the lung\nand heart regions. However, it is costly to acquire region-level annotation in\npractice, and model training mainly relies on image-level class labels in a\nweakly supervised manner, which is highly challenging for computer-aided chest\nX-ray screening. To address this issue, some methods have been proposed\nrecently to identify local regions containing pathological information, which\nis vital for thoracic disease classification. Inspired by this, we propose a\nnovel deep learning framework to explore discriminative information from lung\nand heart regions. We design a feature extractor equipped with a multi-scale\nattention module to learn global attention maps from global images. To exploit\ndisease-specific cues effectively, we locate lung and heart regions containing\npathological information by a well-trained pixel-wise segmentation model to\ngenerate binarization masks. By introducing element-wise logical AND operator\non the learned global attention maps and the binarization masks, we obtain\nlocal attention maps in which pixels are $1$ for lung and heart region and $0$\nfor other regions. By zeroing features of non-lung and heart regions in\nattention maps, we can effectively exploit their disease-specific cues in lung\nand heart regions. Compared to existing methods fusing global and local\nfeatures, we adopt feature weighting to avoid weakening visual cues unique to\nlung and heart regions. Evaluated by the benchmark split on the publicly\navailable chest X-ray14 dataset, the comprehensive experiments show that our\nmethod achieves superior performance compared to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.12430",
          "publishedOn": "2021-05-27T01:32:28.155Z",
          "wordCount": 721,
          "title": "Weighing Features of Lung and Heart Regions for Thoracic Disease Classification. (arXiv:2105.12430v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Philipp_G/0/1/0/all/0/1\">George Philipp</a>",
          "description": "In essence, a neural network is an arbitrary differentiable, parametrized\nfunction. Choosing a neural network architecture for any task is as complex as\nsearching the space of those functions. For the last few years, 'neural\narchitecture design' has been largely synonymous with 'neural architecture\nsearch' (NAS), i.e. brute-force, large-scale search. NAS has yielded\nsignificant gains on practical tasks. However, NAS methods end up searching for\na local optimum in architecture space in a small neighborhood around\narchitectures that often go back decades, based on CNN or LSTM.\n\nIn this work, we present a different and complementary approach to\narchitecture design, which we term 'zero-shot architecture design' (ZSAD). We\ndevelop methods that can predict, without any training, whether an archi",
          "link": "http://arxiv.org/abs/2105.12210",
          "publishedOn": "2021-05-27T01:32:28.149Z",
          "wordCount": 957,
          "title": "The Nonlinearity Coefficient -- A Practical Guide to Neural Architecture Design. (arXiv:2105.12210v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yujia Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>",
          "description": "Camouflaged object detection (COD) is a challenging task due to the low\nboundary contrast between the object and its surroundings. In addition, the\nappearance of camouflaged objects varies significantly, e.g., object size and\nshape, aggravating the difficulties of accurate COD. In this paper, we propose\na novel Context-aware Cross-level Fusion Network (C2F-Net) to address the\nchallenging COD task. Specifically, we propose an Attention-induced Cross-level\nFusion Module (ACFM) to integrate the multi-level features with informative\nattention coefficients. The fused features are then fed to the proposed\nDual-branch Global Context Module (DGCM), which yields multi-scale feature\nrepresentations for exploiting rich global context information. In C2F-Net, the\ntwo modules are conducted on high-level features using a cascaded manner.\nExtensive experiments on three widely used benchmark datasets demonstrate that\nour C2F-Net is an effective COD model and outperforms state-of-the-art models\nremarkably. Our code is publicly available at:\nhttps://github.com/thograce/C2FNet.",
          "link": "http://arxiv.org/abs/2105.12555",
          "publishedOn": "2021-05-27T01:32:28.143Z",
          "wordCount": 586,
          "title": "Context-aware Cross-level Fusion Network for Camouflaged Object Detection. (arXiv:2105.12555v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abraham_J/0/1/0/all/0/1\">Joshua Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wloka_C/0/1/0/all/0/1\">Calden Wloka</a>",
          "description": "Satellite imagery is widely used in many application sectors, including\nagriculture, navigation, and urban planning. Frequently, satellite imagery\ninvolves both large numbers of images as well as high pixel counts, making\nsatellite datasets computationally expensive to analyze. Recent approaches to\nsatellite image analysis have largely emphasized deep learning methods. Though\nextremely powerful, deep learning has some drawbacks, including the requirement\nof specialized computing hardware and a high reliance on training data. When\ndealing with large satellite datasets, the cost of both computational resources\nand training data annotation may be prohibitive.",
          "link": "http://arxiv.org/abs/2105.12633",
          "publishedOn": "2021-05-27T01:32:28.137Z",
          "wordCount": 515,
          "title": "Edge Detection for Satellite Images without Deep Networks. (arXiv:2105.12633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.13958",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_J/0/1/0/all/0/1\">Jiahong Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V Sullivan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tapert_S/0/1/0/all/0/1\">Susan F. Tapert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M Pohl</a>",
          "description": "Many neurological diseases are characterized by gradual deterioration of\nbrain structure and function. Large longitudinal MRI datasets have revealed\nsuch deterioration, in part, by applying machine and deep learning to predict\ndiagnosis. A popular approach is to apply Convolutional Neural Networks (CNN)\nto extract informative features from each visit of the longitudinal MRI and\nthen use those features to classify each visit via Recurrent Neural Networks\n(RNNs). Such modeling neglects the progressive nature of the disease, which may\nresult in clinically implausible classifications across visits. To avoid this\nissue, we propose to combine features across visits by coupling feature\nextraction with a novel longitudinal pooling layer and enforce consistency of\nthe classification across visits in line with disease progression. We evaluate\nthe proposed method on the longitudinal structural MRIs from three neuroimaging\ndatasets: Alzheimer's Disease Neuroimaging Initiative (ADNI, N=404), a dataset\ncomposed of 274 normal controls and 329 patients with Alcohol Use Disorder\n(AUD), and 255 youths from the National Consortium on Alcohol and\nNeuroDevelopment in Adolescence (NCANDA). In all three experiments our method\nis superior to other widely used approaches for longitudinal classification\nthus making a unique contribution towards more accurate tracking of the impact\nof conditions on the brain. The code is available at\nhttps://github.com/ouyangjiahong/longitudinal-pooling.",
          "link": "http://arxiv.org/abs/2003.13958",
          "publishedOn": "2021-05-27T01:32:28.132Z",
          "wordCount": 700,
          "title": "Longitudinal Pooling & Consistency Regularization to Model Disease Progression from MRIs. (arXiv:2003.13958v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Soo Min Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>",
          "description": "Animals locomote for various reasons: to search for food, find suitable\nhabitat, pursue prey, escape from predators, or seek a mate. The grand scale of\nbiodiversity contributes to the great locomotory design and mode diversity.\nVarious creatures make use of legs, wings, fins and other means to move through\nthe world. In this report, we refer to the locomotion of general biological\nspecies as biolocomotion. We present a computational approach to detect\nbiolocomotion in unprocessed video.\n\nSignificantly, the motion exhibited by the body parts of a biological entity\nto navigate through an environment can be modeled by a combination of an\noverall positional advance with an overlaid asymmetric oscillatory pattern, a\ndistinctive signature that tends to be absent in non-biological objects in\nlocomotion. We exploit this key trait of positional advance with asymmetric\noscillation along with differences in an object's common motion (extrinsic\nmotion) and localized motion of its parts (intrinsic motion) to detect\nbiolocomotion. An algorithm is developed to measure the presence of these\ntraits in tracked objects to determine if they correspond to a biological\nentity in locomotion. An alternative algorithm, based on generic features\ncombined with learning is assembled out of components from allied areas of\ninvestigation, also is presented as a basis of comparison.\n\nA novel biolocomotion dataset encompassing a wide range of moving biological\nand non-biological objects in natural settings is provided. Also, biolocomotion\nannotations to an extant camouflage animals dataset are provided. Quantitative\nresults indicate that the proposed algorithm considerably outperforms the\nalternative approach, supporting the hypothesis that biolocomotion can be\ndetected reliably based on its distinct signature of positional advance with\nasymmetric oscillation and extrinsic/intrinsic motion dissimilarity.",
          "link": "http://arxiv.org/abs/2105.12661",
          "publishedOn": "2021-05-27T01:32:28.113Z",
          "wordCount": 701,
          "title": "Detecting Biological Locomotion in Video: A Computational Approach. (arXiv:2105.12661v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandran_A/0/1/0/all/0/1\">Arun Chandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_X/0/1/0/all/0/1\">Xuan Jing</a>",
          "description": "As a prevailing task in video surveillance and forensics field, person\nre-identification (re-ID) aims to match person images captured from\nnon-overlapped cameras. In unconstrained scenarios, person images often suffer\nfrom the resolution mismatch problem, i.e., \\emph{Cross-Resolution Person\nRe-ID}. To overcome this problem, most existing methods restore low resolution\n(LR) images to high resolution (HR) by super-resolution (SR). However, they\nonly focus on the HR feature extraction and ignore the valid information from\noriginal LR images. In this work, we explore the influence of resolutions on\nfeature extraction and develop a novel method for cross-resolution person re-ID\ncalled \\emph{\\textbf{M}ulti-Resolution \\textbf{R}epresentations \\textbf{J}oint\n\\textbf{L}earning} (\\textbf{MRJL}). Our method consists of a Resolution\nReconstruction Network (RRN) and a Dual Feature Fusion Network (DFFN). The RRN\nuses an input image to construct a HR version and a LR version with an encoder\nand two decoders, while the DFFN adopts a dual-branch structure to generate\nperson representations from multi-resolution images. Comprehensive experiments\non five benchmarks verify the superiority of the proposed MRJL over the\nrelevent state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.12684",
          "publishedOn": "2021-05-27T01:32:28.107Z",
          "wordCount": 613,
          "title": "Low Resolution Information Also Matters: Learning Multi-Resolution Representations for Person Re-Identification. (arXiv:2105.12684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1\">Feifei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jian Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shaoning Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Lu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>",
          "description": "Weakly-Supervised Object Detection (WSOD) and Localization (WSOL), i.e.,\ndetecting multiple and single instances with bounding boxes in an image using\nimage-level labels, are long-standing and challenging tasks in the CV\ncommunity. With the success of deep neural networks in object detection, both\nWSOD and WSOL have received unprecedented attention. Hundreds of WSOD and WSOL\nmethods and numerous techniques have been proposed in the deep learning era. To\nthis end, in this paper, we consider WSOL is a sub-task of WSOD and provide a\ncomprehensive survey of the recent achievements of WSOD. Specifically, we\nfirstly describe the formulation and setting of the WSOD, including the\nbackground, challenges, basic framework. Meanwhile, we summarize and analyze\nall advanced techniques and training tricks for improving detection\nperformance. Then, we introduce the widely-used datasets and evaluation metrics\nof WSOD. Lastly, we discuss the future directions of WSOD. We believe that\nthese summaries can help pave a way for future research on WSOD and WSOL.",
          "link": "http://arxiv.org/abs/2105.12694",
          "publishedOn": "2021-05-27T01:32:28.101Z",
          "wordCount": 608,
          "title": "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey. (arXiv:2105.12694v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herath_S/0/1/0/all/0/1\">Samitha Herath</a>",
          "description": "We propose a framework for early action recognition and anticipation by\ncorrelating past features with the future using three novel similarity measures\ncalled Jaccard vector similarity, Jaccard cross-correlation and Jaccard\nFrobenius inner product over covariances. Using these combinations of novel\nlosses and using our framework, we obtain state-of-the-art results for early\naction recognition in UCF101 and JHMDB datasets by obtaining 91.7 % and 83.5 %\naccuracy respectively for an observation percentage of 20. Similarly, we obtain\nstate-of-the-art results for Epic-Kitchen55 and Breakfast datasets for action\nanticipation by obtaining 20.35 and 41.8 top-1 accuracy respectively.",
          "link": "http://arxiv.org/abs/2105.12414",
          "publishedOn": "2021-05-27T01:32:28.095Z",
          "wordCount": 534,
          "title": "Anticipating human actions by correlating past with the future with Jaccard similarity measures. (arXiv:2105.12414v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yuxuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>",
          "description": "Recent works have shown that a rich set of semantic directions exist in the\nlatent space of Generative Adversarial Networks (GANs), which enables various\nfacial attribute editing applications. However, existing methods may suffer\npoor attribute variation disentanglement, leading to unwanted change of other\nattributes when altering the desired one. The semantic directions used by\nexisting methods are at attribute level, which are difficult to model complex\nattribute correlations, especially in the presence of attribute distribution\nbias in GAN's training set. In this paper, we propose a novel framework (IALS)\nthat performs Instance-Aware Latent-Space Search to find semantic directions\nfor disentangled attribute editing. The instance information is injected by\nleveraging the supervision from a set of attribute classifiers evaluated on the\ninput images. We further propose a Disentanglement-Transformation (DT) metric\nto quantify the attribute transformation and disentanglement efficacy and find\nthe optimal control factor between attribute-level and instance-specific\ndirections based on it. Experimental results on both GAN-generated and\nreal-world images collectively show that our method outperforms\nstate-of-the-art methods proposed recently by a wide margin. Code is available\nat https://github.com/yxuhan/IALS.",
          "link": "http://arxiv.org/abs/2105.12660",
          "publishedOn": "2021-05-27T01:32:28.089Z",
          "wordCount": 613,
          "title": "Disentangled Face Attribute Editing via Instance-Aware Latent Space Search. (arXiv:2105.12660v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We propose Predict then Interpolate (PI), a simple algorithm for learning\ncorrelations that are stable across environments. The algorithm follows from\nthe intuition that when using a classifier trained on one environment to make\npredictions on examples from another environment, its mistakes are informative\nas to which correlations are unstable. In this work, we prove that by\ninterpolating the distributions of the correct predictions and the wrong\npredictions, we can uncover an oracle distribution where the unstable\ncorrelation vanishes. Since the oracle interpolation coefficients are not\naccessible, we use group distributionally robust optimization to minimize the\nworst-case risk across all such interpolations. We evaluate our method on both\ntext classification and image classification. Empirical results demonstrate\nthat our algorithm is able to learn robust classifiers (outperforms IRM by\n23.85% on synthetic environments and 12.41% on natural environments). Our code\nand data are available at https://github.com/YujiaBao/Predict-then-Interpolate.",
          "link": "http://arxiv.org/abs/2105.12628",
          "publishedOn": "2021-05-27T01:32:28.072Z",
          "wordCount": 591,
          "title": "Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers. (arXiv:2105.12628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayan Nag</a>",
          "description": "Self-supervised learning and pre-training strategies have developed over the\nlast few years especially for Convolutional Neural Networks (CNNs). Recently\napplication of such methods can also be noticed for Graph Neural Networks\n(GNNs). In this paper, we have used a graph based self-supervised learning\nstrategy with different loss functions (Barlow Twins[ 7], HSIC[ 4], VICReg[ 1])\nwhich have shown promising results when applied with CNNs previously. We have\nalso proposed a hybrid loss function combining the advantages of VICReg and\nHSIC and called it as VICRegHSIC. The performance of these aforementioned\nmethods have been compared when applied to two different datasets namely MUTAG\nand PROTEINS. Moreover, the impact of different batch sizes, projector\ndimensions and data augmentation strategies have also been explored. The\nresults are preliminary and we will be continuing to explore with other\ndatasets.",
          "link": "http://arxiv.org/abs/2105.12247",
          "publishedOn": "2021-05-27T01:32:28.067Z",
          "wordCount": 578,
          "title": "Graph Self Supervised Learning: the BT, the HSIC, and the VICReg. (arXiv:2105.12247v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Namuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songkuk Kim</a>",
          "description": "Bayesian neural networks (BNNs) have shown success in the areas of\nuncertainty estimation and robustness. However, a crucial challenge prohibits\ntheir use in practice: Bayesian NNs require a large number of predictions to\nproduce reliable results, leading to a significant increase in computational\ncost. To alleviate this issue, we propose spatial smoothing, a method that\nensembles neighboring feature map points of CNNs. By simply adding a few blur\nlayers to the models, we empirically show that the spatial smoothing improves\naccuracy, uncertainty estimation, and robustness of BNNs across a whole range\nof ensemble sizes. In particular, BNNs incorporating the spatial smoothing\nachieve high predictive performance merely with a handful of ensembles.\nMoreover, this method also can be applied to canonical deterministic neural\nnetworks to improve the performances. A number of evidences suggest that the\nimprovements can be attributed to the smoothing and flattening of the loss\nlandscape. In addition, we provide a fundamental explanation for prior works -\nnamely, global average pooling, pre-activation, and ReLU6 - by addressing to\nthem as special cases of the spatial smoothing. These not only enhance\naccuracy, but also improve uncertainty estimation and robustness by making the\nloss landscape smoother in the same manner as the spatial smoothing. The code\nis available at https://github.com/xxxnell/spatial-smoothing.",
          "link": "http://arxiv.org/abs/2105.12639",
          "publishedOn": "2021-05-27T01:32:28.061Z",
          "wordCount": 654,
          "title": "Blurs Make Results Clearer: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muglikar_M/0/1/0/all/0/1\">Manasi Muglikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1\">Mathias Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>",
          "description": "We propose a generic event camera calibration framework using image\nreconstruction. Instead of relying on blinking LED patterns or external\nscreens, we show that neural-network-based image reconstruction is well suited\nfor the task of intrinsic and extrinsic calibration of event cameras. The\nadvantage of our proposed approach is that we can use standard calibration\npatterns that do not rely on active illumination. Furthermore, our approach\nenables the possibility to perform extrinsic calibration between frame-based\nand event-based sensors without additional complexity. Both simulation and\nreal-world experiments indicate that calibration through image reconstruction\nis accurate under common distortion models and a wide variety of distortion\nparameters",
          "link": "http://arxiv.org/abs/2105.12362",
          "publishedOn": "2021-05-27T01:32:28.056Z",
          "wordCount": 539,
          "title": "How to Calibrate Your Event Camera. (arXiv:2105.12362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_M/0/1/0/all/0/1\">Mathew Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_T/0/1/0/all/0/1\">Tomer Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataer_Cansizoglu_E/0/1/0/all/0/1\">Esra Ataer-Cansizoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jae-Woo Choi</a>",
          "description": "Matching and recommending products is beneficial for both customers and\ncompanies. With the rapid increase in home goods e-commerce, there is an\nincreasing demand for quantitative methods for providing such recommendations\nfor millions of products. This approach is facilitated largely by online stores\nsuch as Amazon and Wayfair, in which the goal is to maximize overall sales.\nInstead of focusing on overall sales, we take a product design perspective, by\nemploying big-data analysis for determining the design qualities of a highly\nrecommended product. Specifically, we focus on the visual style compatibility\nof such products. We build off previous work which implemented a style-based\nsimilarity metric for thousands of furniture products. Using analysis and\nvisualization, we extract attributes of furniture products that are highly\ncompatible style-wise. We propose a designer in-the-loop workflow that mirrors\nmethods of displaying similar products to consumers browsing e-commerce\nwebsites. Our findings are useful when designing new products, since they\nprovide insight regarding what furniture will be strongly compatible across\nmultiple styles, and hence, more likely to be recommended.",
          "link": "http://arxiv.org/abs/2105.12256",
          "publishedOn": "2021-05-27T01:32:28.049Z",
          "wordCount": 644,
          "title": "Style Similarity as Feedback for Product Design. (arXiv:2105.12256v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platinsky_L/0/1/0/all/0/1\">Lukas Platinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speichert_S/0/1/0/all/0/1\">Stefanie Speichert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Blazej Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheel_O/0/1/0/all/0/1\">Oliver Scheel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yawei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "We investigate what grade of sensor data is required for training an\nimitation-learning-based AV planner on human expert demonstration.\nMachine-learned planners are very hungry for training data, which is usually\ncollected using vehicles equipped with the same sensors used for autonomous\noperation. This is costly and non-scalable. If cheaper sensors could be used\nfor collection instead, data availability would go up, which is crucial in a\nfield where data volume requirements are large and availability is small. We\npresent experiments using up to 1000 hours worth of expert demonstration and\nfind that training with 10x lower-quality data outperforms 1x AV-grade data in\nterms of planner performance. The important implication of this is that cheaper\nsensors can indeed be used. This serves to improve data access and democratize\nthe field of imitation-based motion planning. Alongside this, we perform a\nsensitivity analysis of planner performance as a function of perception range,\nfield-of-view, accuracy, and data volume, and the reason why lower-quality data\nstill provide good planning results.",
          "link": "http://arxiv.org/abs/2105.12337",
          "publishedOn": "2021-05-27T01:32:28.044Z",
          "wordCount": 625,
          "title": "What data do we need for training an AV motion planner?. (arXiv:2105.12337v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jemni_S/0/1/0/all/0/1\">Sana Khamekhem Jemni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>",
          "description": "Handwritten document images can be highly affected by degradation for\ndifferent reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.),\nbad scanning process and so on. These artifacts raise many readability issues\nfor current Handwritten Text Recognition (HTR) algorithms and severely devalue\ntheir efficiency. In this paper, we propose an end to end architecture based on\nGenerative Adversarial Networks (GANs) to recover the degraded documents into a\nclean and readable form. Unlike the most well-known document binarization\nmethods, which try to improve the visual quality of the degraded document, the\nproposed architecture integrates a handwritten text recognizer that promotes\nthe generated document image to be more readable. To the best of our knowledge,\nthis is the first work to use the text information while binarizing handwritten\ndocuments. Extensive experiments conducted on degraded Arabic and Latin\nhandwritten documents demonstrate the usefulness of integrating the recognizer\nwithin the GAN architecture, which improves both the visual quality and the\nreadability of the degraded document images. Moreover, we outperform the state\nof the art in H-DIBCO 2018 challenge, after fine tuning our pre-trained model\nwith synthetically degraded Latin handwritten images, on this task.",
          "link": "http://arxiv.org/abs/2105.12710",
          "publishedOn": "2021-05-27T01:32:28.027Z",
          "wordCount": 637,
          "title": "Enhance to Read Better: An Improved Generative Adversarial Network for Handwritten Document Image Enhancement. (arXiv:2105.12710v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanafani_H/0/1/0/all/0/1\">Hussain Kanafani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghauri_J/0/1/0/all/0/1\">Junaid Ahmed Ghauri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>",
          "description": "Video summarization aims at generating a compact yet representative visual\nsummary that conveys the essence of the original video. The advantage of\nunsupervised approaches is that they do not require human annotations to learn\nthe summarization capability and generalize to a wider range of domains.\nPrevious work relies on the same type of deep features, typically based on a\nmodel pre-trained on ImageNet data. Therefore, we propose the incorporation of\nmultiple feature sources with chunk and stride fusion to provide more\ninformation about the visual content. For a comprehensive evaluation on the two\nbenchmarks TVSum and SumMe, we compare our method with four state-of-the-art\napproaches. Two of these approaches were implemented by ourselves to reproduce\nthe reported results. Our evaluation shows that we obtain state-of-the-art\nresults on both datasets, while also highlighting the shortcomings of previous\nwork with regard to the evaluation methodology. Finally, we perform error\nanalysis on videos for the two benchmark datasets to summarize and spot the\nfactors that lead to misclassifications.",
          "link": "http://arxiv.org/abs/2105.12532",
          "publishedOn": "2021-05-27T01:32:28.021Z",
          "wordCount": 610,
          "title": "Unsupervised Video Summarization via Multi-source Features. (arXiv:2105.12532v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12700",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1\">Luka Murn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1\">Marc Gorriz Blanch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santamaria_M/0/1/0/all/0/1\">Maria Santamaria</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivera_F/0/1/0/all/0/1\">Fiona Rivera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>",
          "description": "Machine learning techniques for more efficient video compression and video\nenhancement have been developed thanks to breakthroughs in deep learning. The\nnew techniques, considered as an advanced form of Artificial Intelligence (AI),\nbring previously unforeseen capabilities. However, they typically come in the\nform of resource-hungry black-boxes (overly complex with little transparency\nregarding the inner workings). Their application can therefore be unpredictable\nand generally unreliable for large-scale use (e.g. in live broadcast). The aim\nof this work is to understand and optimise learned models in video processing\napplications so systems that incorporate them can be used in a more trustworthy\nmanner. In this context, the presented work introduces principles for\nsimplification of learned models targeting improved transparency in\nimplementing machine learning for video production and distribution\napplications. These principles are demonstrated on video compression examples,\nshowing how bitrate savings and reduced complexity can be achieved by\nsimplifying relevant deep learning models.",
          "link": "http://arxiv.org/abs/2105.12700",
          "publishedOn": "2021-05-27T01:32:28.016Z",
          "wordCount": 613,
          "title": "Towards Transparent Application of Machine Learning in Video Processing. (arXiv:2105.12700v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1\">Celia Cintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1\">Skyler Speakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1\">Girmaw Abebe Tadesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinwande_V/0/1/0/all/0/1\">Victor Akinwande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McFowland_E/0/1/0/all/0/1\">Edward McFowland III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weldemariam_K/0/1/0/all/0/1\">Komminist Weldemariam</a>",
          "description": "Generative Adversarial Networks (GANs) have recently achieved unprecedented\nsuccess in photo-realistic image synthesis from low-dimensional random noise.\nThe ability to synthesize high-quality content at a large scale brings\npotential risks as the generated samples may lead to misinformation that can\ncreate severe social, political, health, and business hazards. We propose\nSubsetGAN to identify generated content by detecting a subset of anomalous\nnode-activations in the inner layers of pre-trained neural networks. These\nnodes, as a group, maximize a non-parametric measure of divergence away from\nthe expected distribution of activations created from real data. This enable us\nto identify synthesised images without prior knowledge of their distribution.\nSubsetGAN efficiently scores subsets of nodes and returns the group of nodes\nwithin the pre-trained classifier that contributed to the maximum score. The\nclassifier can be a general fake classifier trained over samples from multiple\nsources or the discriminator network from different GANs. Our approach shows\nconsistently higher detection power than existing detection methods across\nseveral state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different\nproportions of generated content.",
          "link": "http://arxiv.org/abs/2105.12479",
          "publishedOn": "2021-05-27T01:32:28.010Z",
          "wordCount": 632,
          "title": "Pattern Detection in the Activation Space for Identifying Synthesized Content. (arXiv:2105.12479v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Savkin_A/0/1/0/all/0/1\">Artem Savkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Synthetic data has been applied in many deep learning based computer vision\ntasks. Limited performance of algorithms trained solely on synthetic data has\nbeen approached with domain adaptation techniques such as the ones based on\ngenerative adversarial framework. We demonstrate how adversarial training alone\ncan introduce semantic inconsistencies in translated images. To tackle this\nissue we propose density prematching strategy using KLIEP-based density ratio\nestimation procedure. Finally, we show that aforementioned strategy improves\nquality of translated images of underlying method and their usability for the\nsemantic segmentation task in the context of autonomous driving.",
          "link": "http://arxiv.org/abs/2105.12549",
          "publishedOn": "2021-05-27T01:32:28.005Z",
          "wordCount": 552,
          "title": "KLIEP-based Density Ratio Estimation for Semantically Consistent Synthetic to Real Images Adaptation in Urban Traffic Scenes. (arXiv:2105.12549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_R/0/1/0/all/0/1\">Rui Pimentel de Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_J/0/1/0/all/0/1\">Jakob Grimm Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevre_J/0/1/0/all/0/1\">Jonas Le Fevre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandao_M/0/1/0/all/0/1\">Martim Brand&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayacan_E/0/1/0/all/0/1\">Erdal Kayacan</a>",
          "description": "In this work we showcase the design and assessment of the performance of a\nmulti-camera UAV, when coupled with state-of-the-art planning and mapping\nalgorithms for autonomous navigation. The system leverages state-of-the-art\nreceding horizon exploration techniques for Next-Best-View (NBV) planning with\n3D and semantic information, provided by a reconfigurable multi stereo camera\nsystem. We employ our approaches in an autonomous drone-based inspection task\nand evaluate them in an autonomous exploration and mapping scenario. We discuss\nthe advantages and limitations of using multi stereo camera flying systems, and\nthe trade-off between number of cameras and mapping performance.",
          "link": "http://arxiv.org/abs/2105.12691",
          "publishedOn": "2021-05-27T01:32:27.987Z",
          "wordCount": 542,
          "title": "On the Advantages of Multiple Stereo Vision Camera Designs for Autonomous Drone Navigation. (arXiv:2105.12691v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuhang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lianfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sara Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1\">Hod Lipson</a>",
          "description": "Ability to generate intelligent and generalizable facial expressions is\nessential for building human-like social robots. At present, progress in this\nfield is hindered by the fact that each facial expression needs to be\nprogrammed by humans. In order to adapt robot behavior in real time to\ndifferent situations that arise when interacting with human subjects, robots\nneed to be able to train themselves without requiring human labels, as well as\nmake fast action decisions and generalize the acquired knowledge to diverse and\nnew contexts. We addressed this challenge by designing a physical animatronic\nrobotic face with soft skin and by developing a vision-based self-supervised\nlearning framework for facial mimicry. Our algorithm does not require any\nknowledge of the robot's kinematic model, camera calibration or predefined\nexpression set. By decomposing the learning process into a generative model and\nan inverse model, our framework can be trained using a single motor babbling\ndataset. Comprehensive evaluations show that our method enables accurate and\ndiverse face mimicry across diverse human subjects. The project website is at\nthis http URL",
          "link": "http://arxiv.org/abs/2105.12724",
          "publishedOn": "2021-05-27T01:32:27.982Z",
          "wordCount": 632,
          "title": "Smile Like You Mean It: Driving Animatronic Robotic Face with Learned Models. (arXiv:2105.12724v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1\">Pak-Hei Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1\">Ana I.L. Namburete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>",
          "description": "The objective of this work is to segment any arbitrary structures of interest\n(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D\nsegmentation). We show that high accuracy can be achieved by simply propagating\nthe 2D slice segmentation with an affinity matrix between consecutive slices,\nwhich can be learnt in a self-supervised manner, namely slice reconstruction.\nSpecifically, we compare the proposed framework, termed as Sli2Vol, with\nsupervised approaches and two other unsupervised/ self-supervised slice\nregistration approaches, on 8 public datasets (both CT and MRI scans), spanning\n9 different SOIs. Without any parameter-tuning, the same model achieves\nsuperior performance with Dice scores (0-100 scale) of over 80 for most of the\nbenchmarks, including the ones that are unseen during training. Our results\nshow generalizability of the proposed approach across data from different\nmachines and with different SOIs: a major use case of semi-automatic\nsegmentation methods where fully supervised approaches would normally struggle.\nThe source code will be made publicly available at\nhttps://github.com/pakheiyeung/Sli2Vol.",
          "link": "http://arxiv.org/abs/2105.12722",
          "publishedOn": "2021-05-27T01:32:27.975Z",
          "wordCount": 613,
          "title": "Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerer_M/0/1/0/all/0/1\">Matthias K&#xfc;mmerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ori Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>",
          "description": "Since 2014 transfer learning has become the key driver for the improvement of\nspatial saliency prediction; however, with stagnant progress in the last 3-5\nyears. We conduct a large-scale transfer learning study which tests different\nImageNet backbones, always using the same read out architecture and learning\nprotocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze\nII with ResNet50 features we improve the performance on saliency prediction\nfrom 78% to 85%. However, as we continue to test better ImageNet models as\nbackbones (such as EfficientNetB5) we observe no additional improvement on\nsaliency prediction. By analyzing the backbones further, we find that\ngeneralization to other datasets differs substantially, with models being\nconsistently overconfident in their fixation predictions. We show that by\ncombining multiple backbones in a principled manner a good confidence\ncalibration on unseen datasets can be achieved. This yields a significant leap\nin benchmark performance in and out-of-domain with a 15 percent point\nimprovement over DeepGaze II to 93% on MIT1003, marking a new state of the art\non the MIT/Tuebingen Saliency Benchmark in all available metrics (AUC: 88.3%,\nsAUC: 79.4%, CC: 82.4%).",
          "link": "http://arxiv.org/abs/2105.12441",
          "publishedOn": "2021-05-27T01:32:27.969Z",
          "wordCount": 621,
          "title": "Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling. (arXiv:2105.12441v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christian Berger</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dozza_M/0/1/0/all/0/1\">Marco Dozza</a> (2) ((1) Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden, (2) Department of Maritime Sciences and Mechanics, Chalmers University of Technology, Gothenburg, Sweden)",
          "description": "Pedestrian trajectory prediction in urban scenarios is essential for\nautomated driving. This task is challenging because the behavior of pedestrians\nis influenced by both their own history paths and the interactions with others.\nPrevious research modeled these interactions with pooling mechanisms or\naggregating with hand-crafted attention weights. In this paper, we present the\nSocial Interaction-Weighted Spatio-Temporal Convolutional Neural Network\n(Social-IWSTCNN), which includes both the spatial and the temporal features. We\npropose a novel design, namely the Social Interaction Extractor, to learn the\nspatial and social interaction features of pedestrians. Most previous works\nused ETH and UCY datasets which include five scenes but do not cover urban\ntraffic scenarios extensively for training and evaluation. In this paper, we\nuse the recently released large-scale Waymo Open Dataset in urban traffic\nscenarios, which includes 374 urban training scenes and 76 urban testing scenes\nto analyze the performance of our proposed algorithm in comparison to the\nstate-of-the-art (SOTA) models. The results show that our algorithm outperforms\nSOTA algorithms such as Social-LSTM, Social-GAN, and Social-STGCNN on both\nAverage Displacement Error (ADE) and Final Displacement Error (FDE).\nFurthermore, our Social-IWSTCNN is 54.8 times faster in data pre-processing\nspeed, and 4.7 times faster in total test speed than the current best SOTA\nalgorithm Social-STGCNN.",
          "link": "http://arxiv.org/abs/2105.12436",
          "publishedOn": "2021-05-27T01:32:27.963Z",
          "wordCount": 699,
          "title": "Social-IWSTCNN: A Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network for Pedestrian Trajectory Prediction in Urban Traffic Scenarios. (arXiv:2105.12436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Audibert_R/0/1/0/all/0/1\">Rafael Baldasso Audibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maschio_V/0/1/0/all/0/1\">Vinicius Marinho Maschio</a>",
          "description": "Kids have an amazing capacity to use modern electronic devices such as\ntablets, smartphones, etc. This has been incredibly boosted by the ease of\naccess of these devices given the expansion of such devices through the world,\nreaching even third world countries. Also, it is well known that children tend\nto have difficulty learning some subjects at pre-school. We as a society focus\nextensively on alphabetization, but in the end, children end up having\ndifferences in another essential area: Mathematics. With this work, we create\nthe basis for an intuitive application that could join the fact that children\nhave a lot of ease when using such technological applications, trying to shrink\nthe gap between a fun and enjoyable activity with something that will improve\nthe children knowledge and ability to understand concepts when in a low age, by\nusing a novel convolutional neural network to achieve so, named FINNger.",
          "link": "http://arxiv.org/abs/2105.12281",
          "publishedOn": "2021-05-27T01:32:27.948Z",
          "wordCount": 583,
          "title": "FINNger -- Applying artificial intelligence to ease math learning for children. (arXiv:2105.12281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Rushabh Patel</a>",
          "description": "Invasive ductal carcinoma is a prevalent, potentially deadly disease\nassociated with a high rate of morbidity and mortality. Its malignancy is the\nsecond leading cause of death from cancer in women. The mammogram is an\nextremely useful resource for mass detection and invasive ductal carcinoma\ndiagnosis. We are proposing a method for Invasive ductal carcinoma that will\nuse convolutional neural networks (CNN) on mammograms to assist radiologists in\ndiagnosing the disease. Due to the varying image clarity and structure of\ncertain mammograms, it is difficult to observe major cancer characteristics\nsuch as microcalcification and mass, and it is often difficult to interpret and\ndiagnose these attributes. The aim of this study is to establish a novel method\nfor fully automated feature extraction and classification in invasive ductal\ncarcinoma computer-aided diagnosis (CAD) systems. This article presents a tumor\nclassification algorithm that makes novel use of convolutional neural networks\non breast mammogram images to increase feature extraction and training speed.\nThe algorithm makes two contributions.",
          "link": "http://arxiv.org/abs/2105.12564",
          "publishedOn": "2021-05-27T01:32:27.943Z",
          "wordCount": 611,
          "title": "Predicting invasive ductal carcinoma using a Reinforcement Sample Learning Strategy using Deep Learning. (arXiv:2105.12564v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1\">Francesco Croce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>",
          "description": "Adversarial training (AT) in order to achieve adversarial robustness wrt\nsingle $l_p$-threat models has been discussed extensively. However, for\nsafety-critical systems adversarial robustness should be achieved wrt all\n$l_p$-threat models simultaneously. In this paper we develop a simple and\nefficient training scheme to achieve adversarial robustness against the union\nof $l_p$-threat models. Our novel $l_1+l_\\infty$-AT scheme is based on\ngeometric considerations of the different $l_p$-balls and costs as much as\nnormal adversarial training against a single $l_p$-threat model. Moreover, we\nshow that using our $l_1+l_\\infty$-AT scheme one can fine-tune with just 3\nepochs any $l_p$-robust model (for $p \\in \\{1,2,\\infty\\}$) and achieve multiple\nnorm adversarial robustness. In this way we boost the previous state-of-the-art\nreported for multiple-norm robustness by more than $6\\%$ on CIFAR-10 and report\nup to our knowledge the first ImageNet models with multiple norm robustness.\nMoreover, we study the general transfer of adversarial robustness between\ndifferent threat models and in this way boost the previous SOTA\n$l_1$-robustness on CIFAR-10 by almost $10\\%$.",
          "link": "http://arxiv.org/abs/2105.12508",
          "publishedOn": "2021-05-27T01:32:27.929Z",
          "wordCount": 625,
          "title": "Adversarial robustness against multiple $l_p$-threat models at the price of one and how to quickly fine-tune robust models to another threat model. (arXiv:2105.12508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shijie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dapeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haobin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinguo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "Recent years have witnessed significant progress in person re-identification\n(ReID). However, current ReID approaches suffer from considerable performance\ndegradation when the test target domains exhibit different characteristics from\nthe training ones, known as the domain shift problem. To make ReID more\npractical and generalizable, we formulate person re-identification as a Domain\nGeneralization (DG) problem and propose a novel training framework, named\nMultiple Domain Experts Collaborative Learning (MD-ExCo). Specifically, the\nMD-ExCo consists of a universal expert and several domain experts. Each domain\nexpert focuses on learning from a specific domain, and periodically\ncommunicates with other domain experts to regulate its learning strategy in the\nmeta-learning manner to avoid overfitting. Besides, the universal expert\ngathers knowledge from the domain experts, and also provides supervision to\nthem as feedback. Extensive experiments on DG-ReID benchmarks show that our\nMD-ExCo outperforms the state-of-the-art methods by a large margin, showing its\nability to improve the generalization capability of the ReID models.",
          "link": "http://arxiv.org/abs/2105.12355",
          "publishedOn": "2021-05-27T01:32:27.917Z",
          "wordCount": 599,
          "title": "Multiple Domain Experts Collaborative Learning: Multi-Source Domain Generalization For Person Re-Identification. (arXiv:2105.12355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1\">Allan Gr&#xf8;nlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tranberg_J/0/1/0/all/0/1\">Jonas Tranberg</a>",
          "description": "High resolution data models like grid terrain models made from LiDAR data are\na prerequisite for modern day Geographic Information Systems applications.\nBesides providing the foundation for the very accurate digital terrain models,\nLiDAR data is also extensively used to classify which parts of the considered\nsurface comprise relevant elements like water, buildings and vegetation. In\nthis paper we consider the problem of classifying which areas of a given\nsurface are fortified by for instance, roads, sidewalks, parking spaces, paved\ndriveways and terraces. We consider using LiDAR data and orthophotos, combined\nand alone, to show how well the modern machine learning algorithms Gradient\nBoosted Trees and Convolutional Neural Networks are able to detect fortified\nareas on large real world data. The LiDAR data features, in particular the\nintensity feature that measures the signal strength of the return, that we\nconsider in this project are heavily dependent on the actual LiDAR sensor that\nmade the measurement. This is highly problematic, in particular for the\ngeneralisation capability of pattern matching algorithms, as this means that\ndata features for test data may be very different from the data the model is\ntrained on. We propose an algorithmic solution to this problem by designing a\nneural net embedding architecture that transforms data from all the different\nsensor systems into a new common representation that works as well as if the\ntraining data and test data originated from the same sensor. The final\nalgorithm result has an accuracy above 96 percent, and an AUC score above 0.99.",
          "link": "http://arxiv.org/abs/2105.12385",
          "publishedOn": "2021-05-27T01:32:27.901Z",
          "wordCount": 676,
          "title": "Learning to Detect Fortified Areas. (arXiv:2105.12385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bergamini_L/0/1/0/all/0/1\">Luca Bergamini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yawei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheel_O/0/1/0/all/0/1\">Oliver Scheel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chih Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Blazej Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "In this work, we present a simple end-to-end trainable machine learning\nsystem capable of realistically simulating driving experiences. This can be\nused for the verification of self-driving system performance without relying on\nexpensive and time-consuming road testing. In particular, we frame the\nsimulation problem as a Markov Process, leveraging deep neural networks to\nmodel both state distribution and transition function. These are trainable\ndirectly from the existing raw observations without the need for any\nhandcrafting in the form of plant or kinematic models. All that is needed is a\ndataset of historical traffic episodes. Our formulation allows the system to\nconstruct never seen scenes that unfold realistically reacting to the\nself-driving car's behaviour. We train our system directly from 1,000 hours of\ndriving logs and measure both realism, reactivity of the simulation as the two\nkey properties of the simulation. At the same time, we apply the method to\nevaluate the performance of a recently proposed state-of-the-art ML planning\nsystem trained from human driving logs. We discover this planning system is\nprone to previously unreported causal confusion issues that are difficult to\ntest by non-reactive simulation. To the best of our knowledge, this is the\nfirst work that directly merges highly realistic data-driven simulations with a\nclosed-loop evaluation for self-driving vehicles. We make the data, code, and\npre-trained models publicly available to further stimulate simulation\ndevelopment.",
          "link": "http://arxiv.org/abs/2105.12332",
          "publishedOn": "2021-05-27T01:32:27.895Z",
          "wordCount": 680,
          "title": "SimNet: Learning Reactive Self-driving Simulations from Real-world Observations. (arXiv:2105.12332v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12409",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Valsesia_D/0/1/0/all/0/1\">Diego Valsesia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Magli_E/0/1/0/all/0/1\">Enrico Magli</a>",
          "description": "Recent advances have shown how deep neural networks can be extremely\neffective at super-resolving remote sensing imagery, starting from a\nmultitemporal collection of low-resolution images. However, existing models\nhave neglected the issue of temporal permutation, whereby the temporal ordering\nof the input images does not carry any relevant information for the\nsuper-resolution task and causes such models to be inefficient with the, often\nscarce, ground truth data that available for training. Thus, models ought not\nto learn feature extractors that rely on temporal ordering. In this paper, we\nshow how building a model that is fully invariant to temporal permutation\nsignificantly improves performance and data efficiency. Moreover, we study how\nto quantify the uncertainty of the super-resolved image so that the final user\nis informed on the local quality of the product. We show how uncertainty\ncorrelates with temporal variation in the series, and how quantifying it\nfurther improves model performance. Experiments on the Proba-V challenge\ndataset show significant improvements over the state of the art without the\nneed for self-ensembling, as well as improved data efficiency, reaching the\nperformance of the challenge winner with just 25% of the training data.",
          "link": "http://arxiv.org/abs/2105.12409",
          "publishedOn": "2021-05-27T01:32:27.889Z",
          "wordCount": 625,
          "title": "Permutation invariance and uncertainty in multitemporal image super-resolution. (arXiv:2105.12409v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Junfu Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Despite existing pioneering works on sign language translation (SLT), there\nis a non-trivial obstacle, i.e., the limited quantity of parallel sign-text\ndata. To tackle this parallel data bottleneck, we propose a sign\nback-translation (SignBT) approach, which incorporates massive spoken language\ntexts into SLT training. With a text-to-gloss translation model, we first\nback-translate the monolingual text to its gloss sequence. Then, the paired\nsign sequence is generated by splicing pieces from an estimated gloss-to-sign\nbank at the feature level. Finally, the synthetic parallel data serves as a\nstrong supplement for the end-to-end training of the encoder-decoder SLT\nframework.\n\nTo promote the SLT research, we further contribute CSL-Daily, a large-scale\ncontinuous SLT dataset. It provides both spoken language translations and\ngloss-level annotations. The topic revolves around people's daily lives (e.g.,\ntravel, shopping, medical care), the most likely SLT application scenario.\nExtensive experimental results and analysis of SLT methods are reported on\nCSL-Daily. With the proposed sign back-translation method, we obtain a\nsubstantial improvement over previous state-of-the-art SLT methods.",
          "link": "http://arxiv.org/abs/2105.12397",
          "publishedOn": "2021-05-27T01:32:27.882Z",
          "wordCount": 622,
          "title": "Improving Sign Language Translation with Monolingual Data by Sign Back-Translation. (arXiv:2105.12397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_B/0/1/0/all/0/1\">Benjamin Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildreth_D/0/1/0/all/0/1\">Dalton Hildreth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Duowen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baran_I/0/1/0/all/0/1\">Ilya Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vova Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Adriana Schulz</a>",
          "description": "Assembly modeling is a core task of computer aided design (CAD), comprising\naround one third of the work in a CAD workflow. Optimizing this process\ntherefore represents a huge opportunity in the design of a CAD system, but\ncurrent research of assembly based modeling is not directly applicable to\nmodern CAD systems because it eschews the dominant data structure of modern\nCAD: parametric boundary representations (BREPs). CAD assembly modeling defines\nassemblies as a system of pairwise constraints, called mates, between parts,\nwhich are defined relative to BREP topology rather than in world coordinates\ncommon to existing work. We propose SB-GCN, a representation learning scheme on\nBREPs that retains the topological structure of parts, and use these learned\nrepresentations to predict CAD type mates. To train our system, we compiled the\nfirst large scale dataset of BREP CAD assemblies, which we are releasing along\nwith benchmark mate prediction tasks. Finally, we demonstrate the compatibility\nof our model with an existing commercial CAD system by building a tool that\nassists users in mate creation by suggesting mate completions, with 72.2%\naccuracy.",
          "link": "http://arxiv.org/abs/2105.12238",
          "publishedOn": "2021-05-27T01:32:27.861Z",
          "wordCount": 638,
          "title": "SB-GCN: Structured BREP Graph Convolutional Network for Automatic Mating of CAD Assemblies. (arXiv:2105.12238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morais_J/0/1/0/all/0/1\">Joel Tom&#xe1;s Morais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_A/0/1/0/all/0/1\">Ant&#xf3;nio Ramires Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_A/0/1/0/all/0/1\">Andr&#xe9; Leite Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faria_B/0/1/0/all/0/1\">Bruno Faria</a>",
          "description": "In recent years the interest in segmentation has been growing, being used in\na wide range of applications such as fraud detection, anomaly detection in\npublic health and intrusion detection. We present an ablation study of\nFgSegNet_v2, analysing its three stages: (i) Encoder, (ii) Feature Pooling\nModule and (iii) Decoder. The result of this study is a proposal of a variation\nof the aforementioned method that surpasses state of the art results. Three\ndatasets are used for testing: CDNet2014, SBI2015 and CityScapes. In CDNet2014\nwe got an overall improvement compared to the state of the art, mainly in the\nLowFrameRate subset. The presented approach is promising as it produces\ncomparable results with the state of the art (SBI2015 and Cityscapes datasets)\nin very different conditions, such as different lighting conditions.",
          "link": "http://arxiv.org/abs/2105.12311",
          "publishedOn": "2021-05-27T01:32:27.855Z",
          "wordCount": 573,
          "title": "Performance Analysis of a Foreground Segmentation Neural Network Model. (arXiv:2105.12311v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhongzhen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guoyi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiajie Xu</a>",
          "description": "Depth completion aims at inferring a dense depth image from sparse depth\nmeasurement since glossy, transparent or distant surface cannot be scanned\nproperly by the sensor. Most of existing methods directly interpolate the\nmissing depth measurements based on pixel-wise image content and the\ncorresponding neighboring depth values. Consequently, this leads to blurred\nboundaries or inaccurate structure of object. To address these problems, we\npropose a novel self-guided instance-aware network (SG-IANet) that: (1) utilize\nself-guided mechanism to extract instance-level features that is needed for\ndepth restoration, (2) exploit the geometric and context information into\nnetwork learning to conform to the underlying constraints for edge clarity and\nstructure consistency, (3) regularize the depth estimation and mitigate the\nimpact of noise by instance-aware learning, and (4) train with synthetic data\nonly by domain randomization to bridge the reality gap. Extensive experiments\non synthetic and real world dataset demonstrate that our proposed method\noutperforms previous works. Further ablation studies give more insights into\nthe proposed method and demonstrate the generalization capability of our model.",
          "link": "http://arxiv.org/abs/2105.12186",
          "publishedOn": "2021-05-27T01:32:27.849Z",
          "wordCount": 607,
          "title": "Self-Guided Instance-Aware Network for Depth Completion and Enhancement. (arXiv:2105.12186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorley_A/0/1/0/all/0/1\">Alexander Thorley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huaqi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Styles_I/0/1/0/all/0/1\">Iain B Styles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hyung Jin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marvao_A/0/1/0/all/0/1\">Antonio de Marvao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ORegan_D/0/1/0/all/0/1\">Declan P. O&#x27;Regan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jinming Duan</a>",
          "description": "Data-driven deep learning approaches to image registration can be less\naccurate than conventional iterative approaches, especially when training data\nis limited. To address this whilst retaining the fast inference speed of deep\nlearning, we propose VR-Net, a novel cascaded variational network for\nunsupervised deformable image registration. Using the variable splitting\noptimization scheme, we first convert the image registration problem,\nestablished in a generic variational framework, into two sub-problems, one with\na point-wise, closed-form solution while the other one is a denoising problem.\nWe then propose two neural layers (i.e. warping layer and intensity consistency\nlayer) to model the analytical solution and a residual U-Net to formulate the\ndenoising problem (i.e. generalized denoising layer). Finally, we cascade the\nwarping layer, intensity consistency layer, and generalized denoising layer to\nform the VR-Net. Extensive experiments on three (two 2D and one 3D) cardiac\nmagnetic resonance imaging datasets show that VR-Net outperforms\nstate-of-the-art deep learning methods on registration accuracy, while\nmaintains the fast inference speed of deep learning and the data-efficiency of\nvariational model.",
          "link": "http://arxiv.org/abs/2105.12227",
          "publishedOn": "2021-05-27T01:32:27.843Z",
          "wordCount": 655,
          "title": "Learning a Model-Driven Variational Network for Deformable Image Registration. (arXiv:2105.12227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Srishti Yadav</a>",
          "description": "Unlike deep learning which requires large training datasets, correlation\nfilter-based trackers like Kernelized Correlation Filter (KCF) uses implicit\nproperties of tracked images (circulant matrices) for training in real-time.\nDespite their practical application in tracking, a need for a better\nunderstanding of the fundamentals associated with KCF in terms of\ntheoretically, mathematically, and experimentally exists. This thesis first\ndetails the workings prototype of the tracker and investigates its\neffectiveness in real-time applications and supporting visualizations. We\nfurther address some of the drawbacks of the tracker in cases of occlusions,\nscale changes, object rotation, out-of-view and model drift with our novel\nRGB-D Kernel Correlation tracker. We also study the use of particle filters to\nimprove trackers' accuracy. Our results are experimentally evaluated using a)\nstandard dataset and b) real-time using the Microsoft Kinect V2 sensor. We\nbelieve this work will set the basis for a better understanding of the\neffectiveness of kernel-based correlation filter trackers and to further define\nsome of its possible advantages in tracking.",
          "link": "http://arxiv.org/abs/2105.12161",
          "publishedOn": "2021-05-27T01:32:27.836Z",
          "wordCount": 598,
          "title": "Occlusion Aware Kernel Correlation Filter Tracker using RGB-D. (arXiv:2105.12161v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wentao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>",
          "description": "In this paper, we address the makeup transfer and removal tasks\nsimultaneously, which aim to transfer the makeup from a reference image to a\nsource image and remove the makeup from the with-makeup image respectively.\nExisting methods have achieved much advancement in constrained scenarios, but\nit is still very challenging for them to transfer makeup between images with\nlarge pose and expression differences, or handle makeup details like blush on\ncheeks or highlight on the nose. In addition, they are hardly able to control\nthe degree of makeup during transferring or to transfer a specified part in the\ninput face. In this work, we propose the PSGAN++, which is capable of\nperforming both detail-preserving makeup transfer and effective makeup removal.\nFor makeup transfer, PSGAN++ uses a Makeup Distill Network to extract makeup\ninformation, which is embedded into spatial-aware makeup matrices. We also\ndevise an Attentive Makeup Morphing module that specifies how the makeup in the\nsource image is morphed from the reference image, and a makeup detail loss to\nsupervise the model within the selected makeup detail area. On the other hand,\nfor makeup removal, PSGAN++ applies an Identity Distill Network to embed the\nidentity information from with-makeup images into identity matrices. Finally,\nthe obtained makeup/identity matrices are fed to a Style Transfer Network that\nis able to edit the feature maps to achieve makeup transfer or removal. To\nevaluate the effectiveness of our PSGAN++, we collect a Makeup Transfer In the\nWild dataset that contains images with diverse poses and expressions and a\nMakeup Transfer High-Resolution dataset that contains high-resolution images.\nExperiments demonstrate that PSGAN++ not only achieves state-of-the-art results\nwith fine makeup details even in cases of large pose/expression differences but\nalso can perform partial or degree-controllable makeup transfer.",
          "link": "http://arxiv.org/abs/2105.12324",
          "publishedOn": "2021-05-27T01:32:27.826Z",
          "wordCount": 730,
          "title": "PSGAN++: Robust Detail-Preserving Makeup Transfer and Removal. (arXiv:2105.12324v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mingu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung Quang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seungju Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "Adversarial attack is aimed at fooling the target classifier with\nimperceptible perturbation. Adversarial examples, which are carefully crafted\nwith a malicious purpose, can lead to erroneous predictions, resulting in\ncatastrophic accidents. To mitigate the effects of adversarial attacks, we\npropose a novel purification model called CAP-GAN. CAP-GAN takes account of the\nidea of pixel-level and feature-level consistency to achieve reasonable\npurification under cycle-consistent learning. Specifically, we utilize the\nguided attention module and knowledge distillation to convey meaningful\ninformation to the purification model. Once a model is fully trained, inputs\nwould be projected into the purification model and transformed into clean-like\nimages. We vary the capacity of the adversary to argue the robustness against\nvarious types of attack strategies. On the CIFAR-10 dataset, CAP-GAN\noutperforms other pre-processing based defenses under both black-box and\nwhite-box settings.",
          "link": "http://arxiv.org/abs/2102.07304",
          "publishedOn": "2021-05-26T01:22:11.250Z",
          "wordCount": 607,
          "title": "CAP-GAN: Towards Adversarial Robustness with Cycle-consistent Attentional Purification. (arXiv:2102.07304v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haq_F/0/1/0/all/0/1\">Fitash Ul Haq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Donghwan Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1\">Lionel C. Briand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stifter_T/0/1/0/all/0/1\">Thomas Stifter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "Automatically detecting the positions of key-points (e.g., facial key-points\nor finger key-points) in an image is an essential problem in many applications,\nsuch as driver's gaze detection and drowsiness detection in automated driving\nsystems. With the recent advances of Deep Neural Networks (DNNs), Key-Points\ndetection DNNs (KP-DNNs) have been increasingly employed for that purpose.\nNevertheless, KP-DNN testing and validation have remained a challenging problem\nbecause KP-DNNs predict many independent key-points at the same time -- where\neach individual key-point may be critical in the targeted application -- and\nimages can vary a great deal according to many factors.\n\nIn this paper, we present an approach to automatically generate test data for\nKP-DNNs using many-objective search. In our experiments, focused on facial\nkey-points detection DNNs developed for an industrial automotive application,\nwe show that our approach can generate test suites to severely mispredict, on\naverage, more than 93% of all key-points. In comparison, random search-based\ntest data generation can only severely mispredict 41% of them. Many of these\nmispredictions, however, are not avoidable and should not therefore be\nconsidered failures. We also empirically compare state-of-the-art,\nmany-objective search algorithms and their variants, tailored for test suite\ngeneration. Furthermore, we investigate and demonstrate how to learn specific\nconditions, based on image characteristics (e.g., head posture and skin color),\nthat lead to severe mispredictions. Such conditions serve as a basis for risk\nanalysis or DNN retraining.",
          "link": "http://arxiv.org/abs/2012.06511",
          "publishedOn": "2021-05-26T01:22:11.222Z",
          "wordCount": 717,
          "title": "Automatic Test Suite Generation for Key-Points Detection DNNs using Many-Objective Search (Experience Paper). (arXiv:2012.06511v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">Hengrong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Changchun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>",
          "description": "Photoacoustic (PA) computed tomography (PACT) reconstructs the initial\npressure distribution from raw PA signals. The standard reconstruction of\nmedical image could cause the artifacts due to interferences or ill-posed\nsetup. Recently, deep learning has been used to reconstruct the PA image with\nill-posed conditions. Most works remove the artifacts from image domain, and\ncompensate the limited-view from dataset. In this paper, we propose a jointed\nfeature fusion framework (JEFF-Net) based on deep learning to reconstruct the\nPA image using limited-view data. The cross-domain features from limited-view\nposition-wise data and the reconstructed image are fused by a backtracked\nsupervision. Specifically, our results could generate superior performance,\nwhose artifacts are drastically reduced in the output compared to ground-truth\n(full-view reconstructed result). In this paper, a quarter position-wise data\n(32 channels) is fed into model, which outputs another 3-quarters-view data (96\nchannels). Moreover, two novel losses are designed to restrain the artifacts by\nsufficiently manipulating superposed data. The numerical and in-vivo results\nhave demonstrated the superior performance of our method to reconstruct the\nfull-view image without artifacts. Finally, quantitative evaluations show that\nour proposed method outperformed the ground-truth in some metrics.",
          "link": "http://arxiv.org/abs/2012.02472",
          "publishedOn": "2021-05-26T01:22:11.211Z",
          "wordCount": 660,
          "title": "A Jointed Feature Fusion Framework for Photoacoustic Reconstruction. (arXiv:2012.02472v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liping Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>",
          "description": "Rotation detection serves as a fundamental building block in many visual\napplications involving aerial image, scene text, and face etc. Differing from\nthe dominant regression-based approaches for orientation estimation, this paper\nexplores a relatively less-studied methodology based on classification. The\nhope is to inherently dismiss the boundary discontinuity issue as encountered\nby the regression-based detectors. We propose new techniques to push its\nfrontier in two aspects: i) new encoding mechanism: the design of two Densely\nCoded Labels (DCL) for angle classification, to replace the Sparsely Coded\nLabel (SCL) in existing classification-based detectors, leading to three times\ntraining speed increase as empirically observed across benchmarks, further with\nnotable improvement in detection accuracy; ii) loss re-weighting: we propose\nAngle Distance and Aspect Ratio Sensitive Weighting (ADARSW), which improves\nthe detection accuracy especially for square-like objects, by making DCL-based\ndetectors sensitive to angular distance and object's aspect ratio. Extensive\nexperiments and visual analysis on large-scale public datasets for aerial\nimages i.e. DOTA, UCAS-AOD, HRSC2016, as well as scene text dataset ICDAR2015\nand MLT, show the effectiveness of our approach. The source code is available\nat https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow and is also\nintegrated in our open source rotation detection benchmark:\nhttps://github.com/yangxue0827/RotationDetection.",
          "link": "http://arxiv.org/abs/2011.09670",
          "publishedOn": "2021-05-26T01:22:11.188Z",
          "wordCount": 694,
          "title": "Dense Label Encoding for Boundary Discontinuity Free Rotation Detection. (arXiv:2011.09670v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.06550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1\">Mostafa Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_S/0/1/0/all/0/1\">Sylvain Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raison_A/0/1/0/all/0/1\">Adrien Raison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1\">Radu Horaud</a>",
          "description": "We address the problem of analyzing the performance of 3D face alignment\n(3DFA) algorithms. Traditionally, performance analysis relies on carefully\nannotated datasets. Here, these annotations correspond to the 3D coordinates of\na set of pre-defined facial landmarks. However, this annotation process, be it\nmanual or automatic, is rarely error-free, which strongly biases the analysis.\nIn contrast, we propose a fully unsupervised methodology based on robust\nstatistics and a parametric confidence test. We revisit the problem of robust\nestimation of the rigid transformation between two point sets and we describe\ntwo algorithms, one based on a mixture between a Gaussian and a uniform\ndistribution, and another one based on the generalized Student's\nt-distribution. We show that these methods are robust to up to 50% outliers,\nwhich makes them suitable for mapping a face, from an unknown pose to a frontal\npose, in the presence of facial expressions and occlusions. Using these methods\nin conjunction with large datasets of face images, we build a statistical\nfrontal facial model and an associated parametric confidence metric, eventually\nused for performance analysis. We empirically show that the proposed pipeline\nis neither method-biased nor data-biased, and that it can be used to assess\nboth the performance of 3DFA algorithms and the accuracy of annotations of face\ndatasets.",
          "link": "http://arxiv.org/abs/2004.06550",
          "publishedOn": "2021-05-26T01:22:11.172Z",
          "wordCount": 707,
          "title": "Unsupervised Performance Analysis of 3D Face Alignment. (arXiv:2004.06550v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Steffen Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "Contrastive learning has recently seen tremendous success in self-supervised\nlearning. So far, however, it is largely unclear why the learned\nrepresentations generalize so effectively to a large variety of downstream\ntasks. We here prove that feedforward models trained with objectives belonging\nto the commonly used InfoNCE family learn to implicitly invert the underlying\ngenerative model of the observed data. While the proofs make certain\nstatistical assumptions about the generative model, we observe empirically that\nour findings hold even if these assumptions are severely violated. Our theory\nhighlights a fundamental connection between contrastive learning, generative\nmodeling, and nonlinear independent component analysis, thereby furthering our\nunderstanding of the learned representations as well as providing a theoretical\nfoundation to derive more effective contrastive losses.",
          "link": "http://arxiv.org/abs/2102.08850",
          "publishedOn": "2021-05-26T01:22:11.140Z",
          "wordCount": 605,
          "title": "Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seuss_D/0/1/0/all/0/1\">Dominik Seu&#xdf;</a>",
          "description": "After the tremendous advances of deep learning and other AI methods, more\nattention is flowing into other properties of modern approaches, such as\ninterpretability, fairness, etc. combined in frameworks like Responsible AI.\nTwo research directions, namely Explainable AI and Uncertainty Quantification\nare becoming more and more important, but have been so far never combined and\njointly explored. In this paper, I show how both research areas provide\npotential for combination, why more research should be done in this direction\nand how this would lead to an increase in trustability in AI systems.",
          "link": "http://arxiv.org/abs/2105.11828",
          "publishedOn": "2021-05-26T01:22:11.128Z",
          "wordCount": 525,
          "title": "Bridging the Gap Between Explainable AI and Uncertainty Quantification to Enhance Trustability. (arXiv:2105.11828v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weijian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "To calculate the model accuracy on a computer vision task, e.g., object\nrecognition, we usually require a test set composing of test samples and their\nground truth labels. Whilst standard usage cases satisfy this requirement, many\nreal-world scenarios involve unlabeled test data, rendering common model\nevaluation methods infeasible. We investigate this important and under-explored\nproblem, Automatic model Evaluation (AutoEval). Specifically, given a labeled\ntraining set and a classifier, we aim to estimate the classification accuracy\non unlabeled test datasets. We construct a meta-dataset: a dataset comprised of\ndatasets generated from the original images via various transformations such as\nrotation, background substitution, foreground scaling, etc. As the\nclassification accuracy of the model on each sample (dataset) is known from the\noriginal dataset labels, our task can be solved via regression. Using the\nfeature statistics to represent the distribution of a sample dataset, we can\ntrain regression models (e.g., a regression neural network) to predict model\nperformance. Using synthetic meta-dataset and real-world datasets in training\nand testing, respectively, we report a reasonable and promising prediction of\nthe model accuracy. We also provide insights into the application scope,\nlimitation, and potential future direction of AutoEval.",
          "link": "http://arxiv.org/abs/2007.02915",
          "publishedOn": "2021-05-26T01:22:11.110Z",
          "wordCount": 658,
          "title": "Are Labels Always Necessary for Classifier Accuracy Evaluation?. (arXiv:2007.02915v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.05569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tsang Ing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Adriano L. I. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Out-of-distribution (OOD) detection approaches usually present special\nrequirements (e.g., hyperparameter validation, collection of outlier data) and\nproduce side effects (e.g., classification accuracy drop, slower\nenergy-inefficient inferences). We argue that these issues are a consequence of\nthe SoftMax loss anisotropy and disagreement with the maximum entropy\nprinciple. Thus, we propose the IsoMax loss and the entropic score. The\nseamless drop-in replacement of the SoftMax loss by IsoMax loss requires\nneither additional data collection nor hyperparameter validation. The trained\nmodels do not exhibit classification accuracy drop and produce fast\nenergy-efficient inferences. Moreover, our experiments show that training\nneural networks with IsoMax loss significantly improves their OOD detection\nperformance. The IsoMax loss exhibits state-of-the-art performance under the\nmentioned conditions (fast energy-efficient inference, no classification\naccuracy drop, no collection of outlier data, and no hyperparameter\nvalidation), which we call the seamless OOD detection task. In future work,\ncurrent OOD detection methods may replace the SoftMax loss with the IsoMax loss\nto improve their performance on the commonly studied non-seamless OOD detection\nproblem.",
          "link": "http://arxiv.org/abs/1908.05569",
          "publishedOn": "2021-05-26T01:22:11.104Z",
          "wordCount": 746,
          "title": "Entropic Out-of-Distribution Detection. (arXiv:1908.05569v13 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naphade_M/0/1/0/all/0/1\">Milind Naphade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1\">David C. Anastasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaodong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yue Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pranamesh Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">Christian E. Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1\">Vitaly Ablavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>",
          "description": "The AI City Challenge was created with two goals in mind: (1) pushing the\nboundaries of research and development in intelligent video analysis for\nsmarter cities use cases, and (2) assessing tasks where the level of\nperformance is enough to cause real-world adoption. Transportation is a segment\nripe for such adoption. The fifth AI City Challenge attracted 305 participating\nteams across 38 countries, who leveraged city-scale real traffic data and\nhigh-quality synthetic data to compete in five challenge tracks. Track 1\naddressed video-based automatic vehicle counting, where the evaluation being\nconducted on both algorithmic effectiveness and computational efficiency. Track\n2 addressed city-scale vehicle re-identification with augmented synthetic data\nto substantially increase the training set for the task. Track 3 addressed\ncity-scale multi-target multi-camera vehicle tracking. Track 4 addressed\ntraffic anomaly detection. Track 5 was a new track addressing vehicle retrieval\nusing natural language descriptions. The evaluation system shows a general\nleader board of all submitted results, and a public leader board of results\nlimited to the contest participation rules, where teams are not allowed to use\nexternal data in their work. The public leader board shows results more close\nto real-world situations where annotated data is limited. Results show the\npromise of AI in Smarter Transportation. State-of-the-art performance for some\ntasks shows that these technologies are ready for adoption in real-world\nsystems.",
          "link": "http://arxiv.org/abs/2104.12233",
          "publishedOn": "2021-05-26T01:22:11.098Z",
          "wordCount": 702,
          "title": "The 5th AI City Challenge. (arXiv:2104.12233v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dodel_D/0/1/0/all/0/1\">David Dodel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schotz_M/0/1/0/all/0/1\">Michael Sch&#xf6;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vodisch_N/0/1/0/all/0/1\">Niclas V&#xf6;disch</a>",
          "description": "This paper presents the FSOCO dataset, a collaborative dataset for\nvision-based cone detection systems in Formula Student Driverless competitions.\nIt contains human annotated ground truth labels for both bounding boxes and\ninstance-wise segmentation masks. The data buy-in philosophy of FSOCO asks\nstudent teams to contribute to the database first before being granted access\nensuring continuous growth. By providing clear labeling guidelines and tools\nfor a sophisticated raw image selection, new annotations are guaranteed to meet\nthe desired quality. The effectiveness of the approach is shown by comparing\nprediction results of a network trained on FSOCO and its unregulated\npredecessor. The FSOCO dataset can be found at fsoco-dataset.com.",
          "link": "http://arxiv.org/abs/2012.07139",
          "publishedOn": "2021-05-26T01:22:11.044Z",
          "wordCount": 577,
          "title": "FSOCO: The Formula Student Objects in Context Dataset. (arXiv:2012.07139v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.08721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Simon-Martin Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>",
          "description": "While deep learning strategies achieve outstanding results in computer vision\ntasks, one issue remains: The current strategies rely heavily on a huge amount\nof labeled data. In many real-world problems, it is not feasible to create such\nan amount of labeled training data. Therefore, it is common to incorporate\nunlabeled data into the training process to reach equal results with fewer\nlabels. Due to a lot of concurrent research, it is difficult to keep track of\nrecent developments. In this survey, we provide an overview of often used ideas\nand methods in image classification with fewer labels. We compare 34 methods in\ndetail based on their performance and their commonly used ideas rather than a\nfine-grained taxonomy. In our analysis, we identify three major trends that\nlead to future research opportunities. 1. State-of-the-art methods are\nscaleable to real-world applications in theory but issues like class imbalance,\nrobustness, or fuzzy labels are not considered. 2. The degree of supervision\nwhich is needed to achieve comparable results to the usage of all labels is\ndecreasing and therefore methods need to be extended to settings with a\nvariable number of classes. 3. All methods share some common ideas but we\nidentify clusters of methods that do not share many ideas. We show that\ncombining ideas from different clusters can lead to better performance.",
          "link": "http://arxiv.org/abs/2002.08721",
          "publishedOn": "2021-05-26T01:22:11.032Z",
          "wordCount": 720,
          "title": "A survey on Semi-, Self- and Unsupervised Learning for Image Classification. (arXiv:2002.08721v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baier_G/0/1/0/all/0/1\">Gerald Baier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschemps_A/0/1/0/all/0/1\">Antonin Deschemps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Michael Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1\">Naoto Yokoya</a>",
          "description": "We synthesize both optical RGB and synthetic aperture radar (SAR) remote\nsensing images from land cover maps and auxiliary raster data using generative\nadversarial networks (GANs). In remote sensing, many types of data, such as\ndigital elevation models (DEMs) or precipitation maps, are often not reflected\nin land cover maps but still influence image content or structure. Including\nsuch data in the synthesis process increases the quality of the generated\nimages and exerts more control on their characteristics. Spatially adaptive\nnormalization layers fuse both inputs and are applied to a full-blown generator\narchitecture consisting of encoder and decoder to take full advantage of the\ninformation content in the auxiliary raster data. Our method successfully\nsynthesizes medium (10 m) and high (1 m) resolution images when trained with\nthe corresponding data set. We show the advantage of data fusion of land cover\nmaps and auxiliary information using mean intersection over unions (mIoUs),\npixel accuracy, and Fr\\'echet inception distances (FIDs) using pretrained U-Net\nsegmentation models. Handpicked images exemplify how fusing information avoids\nambiguities in the synthesized images. By slightly editing the input, our\nmethod can be used to synthesize realistic changes, i.e., raising the water\nlevels. The source code is available at https://github.com/gbaier/rs_img_synth\nand we published the newly created high-resolution dataset at\nhttps://ieee-dataport.org/open-access/geonrw.",
          "link": "http://arxiv.org/abs/2011.11314",
          "publishedOn": "2021-05-26T01:22:11.004Z",
          "wordCount": 684,
          "title": "Synthesizing Optical and SAR Imagery From Land Cover Maps and Auxiliary Raster Data. (arXiv:2011.11314v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.00943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_T/0/1/0/all/0/1\">Tahmida Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Billah_M/0/1/0/all/0/1\">Mohammad Billah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mahmudul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "Most of the existing works on human activity analysis focus on recognition or\nearly recognition of the activity labels from complete or partial observations.\nSimilarly, almost all of the existing video captioning approaches focus on the\nobserved events in videos. Predicting the labels and the captions of future\nactivities where no frames of the predicted activities have been observed is a\nchallenging problem, with important applications that require anticipatory\nresponse. In this work, we propose a system that can infer the labels and the\ncaptions of a sequence of future activities. Our proposed network for label\nprediction of a future activity sequence has three branches where the first\nbranch takes visual features from the objects present in the scene, the second\nbranch takes observed sequential activity features, and the third branch\ncaptures the last observed activity features. The predicted labels and the\nobserved scene context are then mapped to meaningful captions using a\nsequence-to-sequence learning-based method. Experiments on four challenging\nactivity analysis datasets and a video description dataset demonstrate that our\nlabel prediction approach achieves comparable performance with the\nstate-of-the-arts and our captioning framework outperform the\nstate-of-the-arts.",
          "link": "http://arxiv.org/abs/1908.00943",
          "publishedOn": "2021-05-26T01:22:10.979Z",
          "wordCount": 673,
          "title": "Prediction and Description of Near-Future Activities in Video. (arXiv:1908.00943v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safin_A/0/1/0/all/0/1\">Aleksandr Safin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Maxim Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drobyshev_N/0/1/0/all/0/1\">Nikita Drobyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voynov_O/0/1/0/all/0/1\">Oleg Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1\">Alexander Filippov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "Depth maps captured with commodity sensors are often of low quality and\nresolution; these maps need to be enhanced to be used in many applications.\nState-of-the-art data-driven methods of depth map super-resolution rely on\nregistered pairs of low- and high-resolution depth maps of the same scenes.\nAcquisition of real-world paired data requires specialized setups. Another\nalternative, generating low-resolution maps from high-resolution maps by\nsubsampling, adding noise and other artificial degradation methods, does not\nfully capture the characteristics of real-world low-resolution images. As a\nconsequence, supervised learning methods trained on such artificial paired data\nmay not perform well on real-world low-resolution inputs. We consider an\napproach to depth map enhancement based on learning from unpaired data. While\nmany techniques for unpaired image-to-image translation have been proposed,\nmost are not directly applicable to depth maps. We propose an unpaired learning\nmethod for simultaneous depth enhancement and super-resolution, which is based\non a learnable degradation model and surface normal estimates as features to\nproduce more accurate depth maps. We demonstrate that our method outperforms\nexisting unpaired methods and performs on par with paired methods on a new\nbenchmark for unpaired learning that we developed.",
          "link": "http://arxiv.org/abs/2105.12038",
          "publishedOn": "2021-05-26T01:22:10.966Z",
          "wordCount": null,
          "title": "Towards Unpaired Depth Enhancement and Super-Resolution in the Wild. (arXiv:2105.12038v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yueyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "Existing deep learning methods for diagnosis of gastric cancer commonly use\nconvolutional neural networks (CNN). Recently, the Visual Transformer (VT) has\nattracted a major attention because of its performance and efficiency, but its\napplications are mostly in the field of computer vision. In this paper, a\nmulti-scale visual transformer model, referred to as GasHis-Transformer, is\nproposed for gastric histopathology image classification (GHIC), which enables\nthe automatic classification of microscopic gastric images into abnormal and\nnormal cases. The GasHis-Transformer model consists of two key modules: a\nglobal information module (GIM) and a local information module (LIM) to extract\npathological features effectively. In our experiments, a public hematoxylin and\neosin (H&E) stained gastric histopathology dataset with 280 abnormal or normal\nimages using the GasHis-Transformer model is applied to estimate precision,\nrecall, F1-score, and accuracy on the testing set as 98.0%, 100.0%, 96.0% and\n98.0% respectively. Furthermore, a critical study is conducted to evaluate the\nrobustness of GasHis-Transformer according to add ten different noises\nincluding adversarial attack and traditional image noise. In addition, a\nclinically meaningful study is executed to test the gastric cancer\nidentification of GasHis-Transformerwith 420 abnormal images and achieves 96.2%\naccuracy. Finally, a comparative study is performed to test the\ngeneralizability with both H&E and Immunohistochemical (IHC) stained images on\na lymphoma image dataset, a breast cancer dataset and a cervical cancer\ndataset, producing comparable F1-scores (85.6%, 82.8% and 65.7%, respectively)\nand accuracy (83.9%, 89.4% and 65.7%, respectively) respectively. In\nconclusion, GasHis-Transformerdemonstrates a high classification performance\nand shows its significant potential in histopathology image analysis.",
          "link": "http://arxiv.org/abs/2104.14528",
          "publishedOn": "2021-05-26T01:22:10.966Z",
          "wordCount": 730,
          "title": "GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification. (arXiv:2104.14528v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Namysl_M/0/1/0/all/0/1\">Marcin Namysl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esser_A/0/1/0/all/0/1\">Alexander M. Esser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Joachim K&#xf6;hler</a>",
          "description": "Table extraction is an important but still unsolved problem. In this paper,\nwe introduce a flexible end-to-end table extraction system. We develop two\nrule-based algorithms that perform the complete table recognition process and\nsupport the most frequent table formats found in the scientific literature.\nMoreover, to incorporate the extraction of semantic information into the table\nrecognition process, we develop a graph-based table interpretation method. We\nconduct extensive experiments on the challenging table recognition benchmarks\nICDAR 2013 and ICDAR 2019. Our table recognition approach achieves results\ncompetitive with state-of-the-art approaches. Moreover, our complete\ninformation extraction system exhibited a high F1 score of 0.7380 proving the\nutility of our approach.",
          "link": "http://arxiv.org/abs/2105.11879",
          "publishedOn": "2021-05-26T01:22:10.960Z",
          "wordCount": null,
          "title": "Tab.IAIS: Flexible Table Recognition and Semantic Interpretation System. (arXiv:2105.11879v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wentao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenyang Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Few-shot learning is a challenging task since only few instances are given\nfor recognizing an unseen class. One way to alleviate this problem is to\nacquire a strong inductive bias via meta-learning on similar tasks. In this\npaper, we show that such inductive bias can be learned from a flat collection\nof unlabeled images, and instantiated as transferable representations among\nseen and unseen classes. Specifically, we propose a novel part-based\nself-supervised representation learning scheme to learn transferable\nrepresentations by maximizing the similarity of an image to its discriminative\npart. To mitigate the overfitting in few-shot classification caused by data\nscarcity, we further propose a part augmentation strategy by retrieving extra\nimages from a base dataset. We conduct systematic studies on miniImageNet and\ntieredImageNet benchmarks. Remarkably, our method yields impressive results,\noutperforming the previous best unsupervised methods by 7.74% and 9.24% under\n5-way 1-shot and 5-way 5-shot settings, which are comparable with\nstate-of-the-art supervised methods.",
          "link": "http://arxiv.org/abs/2105.11874",
          "publishedOn": "2021-05-26T01:22:10.959Z",
          "wordCount": null,
          "title": "Few-Shot Learning with Part Discovery and Augmentation from Unlabeled Images. (arXiv:2105.11874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1\">Cheng Yu Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1\">Mu-Chun Su</a>",
          "description": "In recent years, developing AI for robotics has raised much attention. The\ninteraction of vision and language of robots is particularly difficult. We\nconsider that giving robots an understanding of visual semantics and language\nsemantics will improve inference ability. In this paper, we propose a novel\nmethod-VSGM (Visual Semantic Graph Memory), which uses the semantic graph to\nobtain better visual image features, improve the robot's visual understanding\nability. By providing prior knowledge of the robot and detecting the objects in\nthe image, it predicts the correlation between the attributes of the object and\nthe objects and converts them into a graph-based representation; and mapping\nthe object in the image to be a top-down egocentric map. Finally, the important\nobject features of the current task are extracted by Graph Neural Networks. The\nmethod proposed in this paper is verified in the ALFRED (Action Learning From\nRealistic Environments and Directives) dataset. In this dataset, the robot\nneeds to perform daily indoor household tasks following the required language\ninstructions. After the model is added to the VSGM, the task success rate can\nbe improved by 6~10%.",
          "link": "http://arxiv.org/abs/2105.08959",
          "publishedOn": "2021-05-26T01:22:10.958Z",
          "wordCount": 640,
          "title": "VSGM -- Enhance robot task understanding ability through visual semantic graph. (arXiv:2105.08959v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenhui Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Ran Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>",
          "description": "Deep learning networks have shown promising performance for accurate object\nlocalization in medial images, but require large amount of annotated data for\nsupervised training, which is expensive and expertise burdensome. To address\nthis problem, we present a one-shot framework for organ and landmark\nlocalization in volumetric medical images, which does not need any annotation\nduring the training stage and could be employed to locate any landmarks or\norgans in test images given a support (reference) image during the inference\nstage. Our main idea comes from that tissues and organs from different human\nbodies have a similar relative position and context. Therefore, we could\npredict the relative positions of their non-local patches, thus locate the\ntarget organ. Our framework is composed of three parts: (1) A projection\nnetwork trained to predict the 3D offset between any two patches from the same\nvolume, where human annotations are not required. In the inference stage, it\ntakes one given landmark in a reference image as a support patch and predicts\nthe offset from a random patch to the corresponding landmark in the test\n(query) volume. (2) A coarse-to-fine framework contains two projection\nnetworks, providing more accurate localization of the target. (3) Based on the\ncoarse-to-fine model, we transfer the organ boundingbox (B-box) detection to\nlocating six extreme points along x, y and z directions in the query volume.\nExperiments on multi-organ localization from head-and-neck (HaN) CT volumes\nshowed that our method acquired competitive performance in real time, which is\nmore accurate and 10^5 times faster than template matching methods with the\nsame setting. Code is available: https://github.com/LWHYC/RPR-Loc.",
          "link": "http://arxiv.org/abs/2012.07043",
          "publishedOn": "2021-05-26T01:22:10.934Z",
          "wordCount": 745,
          "title": "Contrastive Learning of Relative Position Regression for One-Shot Object Localization in 3D Medical Images. (arXiv:2012.07043v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wai Teng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Matthew Kay Fei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Chuping Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Weng-Fai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Goh</a>",
          "description": "Deep neural networks (DNN) have achieved remarkable success in computer\nvision (CV). However, training and inference of DNN models are both memory and\ncomputation intensive, incurring significant overhead in terms of energy\nconsumption and silicon area. In particular, inference is much more\ncost-sensitive than training because training can be done offline with powerful\nplatforms, while inference may have to be done on battery powered devices with\nconstrained form factors, especially for mobile or edge vision applications. In\norder to accelerate DNN inference, model quantization was proposed. However\nprevious works only focus on the quantization rate without considering the\nefficiency of operations. In this paper, we propose Dendrite-Tree based Neural\nNetwork (DTNN) for energy-efficient inference with table lookup operations\nenabled by activation quantization. In DTNN both costly weight access and\narithmetic computations are eliminated for inference. We conducted experiments\non various kinds of DNN models such as LeNet-5, MobileNet, VGG, and ResNet with\ndifferent datasets, including MNIST, Cifar10/Cifar100, SVHN, and ImageNet. DTNN\nachieved significant energy saving (19.4X and 64.9X improvement on ResNet-18\nand VGG-11 with ImageNet, respectively) with negligible loss of accuracy. To\nfurther validate the effectiveness of DTNN and compare with state-of-the-art\nlow energy implementation for edge vision, we design and implement DTNN based\nMLP image classifiers using off-the-shelf FPGAs. The results show that DTNN on\nthe FPGA, with higher accuracy, could achieve orders of magnitude better energy\nconsumption and latency compared with the state-of-the-art low energy\napproaches reported that use ASIC chips.",
          "link": "http://arxiv.org/abs/2105.11848",
          "publishedOn": "2021-05-26T01:22:10.924Z",
          "wordCount": null,
          "title": "DTNN: Energy-efficient Inference with Dendrite Tree Inspired Neural Networks for Edge Vision Applications. (arXiv:2105.11848v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11863",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Avetisian_M/0/1/0/all/0/1\">Manvel Avetisian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burenko_I/0/1/0/all/0/1\">Ilya Burenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egorov_K/0/1/0/all/0/1\">Konstantin Egorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nesterov_A/0/1/0/all/0/1\">Aleksandr Nesterov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nikolaev_A/0/1/0/all/0/1\">Aleksandr Nikolaev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponomarchuk_A/0/1/0/all/0/1\">Alexander Ponomarchuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sokolova_E/0/1/0/all/0/1\">Elena Sokolova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tuzhilin_A/0/1/0/all/0/1\">Alex Tuzhilin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umerenkov_D/0/1/0/all/0/1\">Dmitry Umerenkov</a>",
          "description": "Analysis of chest CT scans can be used in detecting parts of lungs that are\naffected by infectious diseases such as COVID-19.Determining the volume of\nlungs affected by lesions is essential for formulating treatment\nrecommendations and prioritizingpatients by severity of the disease. In this\npaper we adopted an approach based on using an ensemble of deep\nconvolutionalneural networks for segmentation of slices of lung CT scans. Using\nour models we are able to segment the lesions, evaluatepatients dynamics,\nestimate relative volume of lungs affected by lesions and evaluate the lung\ndamage stage. Our modelswere trained on data from different medical centers. We\ncompared predictions of our models with those of six experiencedradiologists\nand our segmentation model outperformed most of them. On the task of\nclassification of disease severity, ourmodel outperformed all the radiologists.",
          "link": "http://arxiv.org/abs/2105.11863",
          "publishedOn": "2021-05-26T01:22:10.924Z",
          "wordCount": null,
          "title": "CoRSAI: A System for Robust Interpretation of CT Scans of COVID-19 Patients Using Deep Learning. (arXiv:2105.11863v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02360",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaoxia Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hua Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "In order to prevent illegal or unauthorized access of image data such as\nhuman faces and ensure legitimate users can use authorization-protected data,\nreversible adversarial attack technique is rise. Reversible adversarial\nexamples (RAE) get both attack capability and reversibility at the same time.\nHowever, the existing technique can not meet application requirements because\nof serious distortion and failure of image recovery when adversarial\nperturbations get strong. In this paper, we take advantage of Reversible Image\nTransformation technique to generate RAE and achieve reversible adversarial\nattack. Experimental results show that proposed RAE generation scheme can\nensure imperceptible image distortion and the original image can be\nreconstructed error-free. What's more, both the attack ability and the image\nquality are not limited by the perturbation amplitude.",
          "link": "http://arxiv.org/abs/1911.02360",
          "publishedOn": "2021-05-26T01:22:10.854Z",
          "wordCount": 640,
          "title": "Reversible Adversarial Attack based on Reversible Image Transformation. (arXiv:1911.02360v7 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qiang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chong Peng</a>",
          "description": "It is a challenging task to remove heavy and mixed types of noise from\nHyperspectral images (HSIs). In this paper, we propose a novel nonconvex\napproach to RPCA for HSI denoising, which adopts the log-determinant rank\napproximation and a novel $\\ell_{2,\\log}$ norm, to restrict the low-rank or\ncolumn-wise sparse properties for the component matrices, respectively.For the\n$\\ell_{2,\\log}$-regularized shrinkage problem, we develop an efficient,\nclosed-form solution, which is named $\\ell_{2,\\log}$-shrinkage operator, which\ncan be generally used in other problems. Extensive experiments on both\nsimulated and real HSIs demonstrate the effectiveness of the proposed method in\ndenoising HSIs.",
          "link": "http://arxiv.org/abs/2105.11927",
          "publishedOn": "2021-05-26T01:22:10.840Z",
          "wordCount": 532,
          "title": "Hyperspectral Image Denoising with Log-Based Robust PCA. (arXiv:2105.11927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12107",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_M/0/1/0/all/0/1\">M. Ak&#x131;n Y&#x131;lmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_O/0/1/0/all/0/1\">Onur Kele&#x15f;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guven_H/0/1/0/all/0/1\">Hilal G&#xfc;ven</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tekalp_A/0/1/0/all/0/1\">A. Murat Tekalp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_J/0/1/0/all/0/1\">Junaid Malik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan K&#x131;ranyaz</a>",
          "description": "In end-to-end optimized learned image compression, it is standard practice to\nuse a convolutional variational autoencoder with generalized divisive\nnormalization (GDN) to transform images into a latent space. Recently,\nOperational Neural Networks (ONNs) that learn the best non-linearity from a set\nof alternatives, and their self-organized variants, Self-ONNs, that approximate\nany non-linearity via Taylor series have been proposed to address the\nlimitations of convolutional layers and a fixed nonlinear activation. In this\npaper, we propose to replace the convolutional and GDN layers in the\nvariational autoencoder with self-organized operational layers, and propose a\nnovel self-organized variational autoencoder (Self-VAE) architecture that\nbenefits from stronger non-linearity. The experimental results demonstrate that\nthe proposed Self-VAE yields improvements in both rate-distortion performance\nand perceptual image quality.",
          "link": "http://arxiv.org/abs/2105.12107",
          "publishedOn": "2021-05-26T01:22:10.659Z",
          "wordCount": null,
          "title": "Self-Organized Variational Autoencoders (Self-VAE) for Learned Image Compression. (arXiv:2105.12107v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karakaya_D/0/1/0/all/0/1\">Diclehan Karakaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulucan_O/0/1/0/all/0/1\">Oguzhan Ulucan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turkan_M/0/1/0/all/0/1\">Mehmet Turkan</a>",
          "description": "High dynamic range (HDR) imaging enables to immortalize natural scenes\nsimilar to the way that they are perceived by human observers. With regular low\ndynamic range (LDR) capture/display devices, significant details may not be\npreserved in images due to the huge dynamic range of natural scenes. To\nminimize the information loss and produce high quality HDR-like images for LDR\nscreens, this study proposes an efficient multi-exposure fusion (MEF) approach\nwith a simple yet effective weight extraction method relying on principal\ncomponent analysis, adaptive well-exposedness and saliency maps. These weight\nmaps are later refined through a guided filter and the fusion is carried out by\nemploying a pyramidal decomposition. Experimental comparisons with existing\ntechniques demonstrate that the proposed method produces very strong\nstatistical and visual results.",
          "link": "http://arxiv.org/abs/2105.11809",
          "publishedOn": "2021-05-26T01:22:10.657Z",
          "wordCount": 564,
          "title": "PAS-MEF: Multi-exposure image fusion based on principal component analysis, adaptive well-exposedness and saliency map. (arXiv:2105.11809v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francisco_P/0/1/0/all/0/1\">P&#xe9;rez-Hern&#xe1;ndez Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_R/0/1/0/all/0/1\">Rodr&#xed;guez-Ortega Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yassir_B/0/1/0/all/0/1\">Benhammou Yassir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francisco_H/0/1/0/all/0/1\">Herrera Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siham_T/0/1/0/all/0/1\">Tabik Siham</a>",
          "description": "The detection of critical infrastructures is of high importance in several\nfields such as security, anomaly detection, land use planning and land use\nchange detection. However, critical infrastructures detection in aerial and\nsatellite images is still a challenge as each one has completely different size\nand requires different spacial resolution to be identified correctly.\nHeretofore, there are no special datasets for training critical infrastructures\ndetectors. This paper presents a smart dataset as well as a\nresolution-independent critical infrastructure detection system. In particular,\nguided by the performance of the detection model, we built a dataset organized\ninto two scales, small and large scale, and designed a two-stage deep learning\ndetection of different scale critical infrastructures (DetDSCI) methodology in\northo-images. DetDSCI methodology first determines the input image zoom level\nusing a classification model, then analyses the input image with the\nappropriate scale detection model. Our experiments show that DetDSCI\nmethodology achieves up to 37,53% F1 improvement with respect to the baseline\ndetector.",
          "link": "http://arxiv.org/abs/2105.11844",
          "publishedOn": "2021-05-26T01:22:10.650Z",
          "wordCount": null,
          "title": "Small and large scale critical infrastructures detection based on deep learning using high resolution orthogonal images. (arXiv:2105.11844v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koo_I/0/1/0/all/0/1\">Inyong Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minki Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>",
          "description": "Few-shot learning often involves metric learning-based classifiers, which\npredict the image label by comparing the distance between the extracted feature\nvector and class representations. However, applying global pooling in the\nbackend of the feature extractor may not produce an embedding that correctly\nfocuses on the class object. In this work, we propose a novel framework that\ngenerates class representations by extracting features from class-relevant\nregions of the images. Given only a few exemplary images with image-level\nlabels, our framework first localizes the class objects by spatially\ndecomposing the similarity between the images and their class prototypes. Then,\nenhanced class representations are achieved from the localization results. We\nalso propose a loss function to enhance distinctions of the refined features.\nOur method outperforms the baseline few-shot model in miniImageNet and\ntieredImageNet benchmarks.",
          "link": "http://arxiv.org/abs/2105.11715",
          "publishedOn": "2021-05-26T01:22:10.617Z",
          "wordCount": null,
          "title": "Improving Few-shot Learning with Weakly-supervised Object Localization. (arXiv:2105.11715v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Killedar_V/0/1/0/all/0/1\">Vinayak Killedar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokala_P/0/1/0/all/0/1\">Praveen Kumar Pokala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seelamantula_C/0/1/0/all/0/1\">Chandra Sekhar Seelamantula</a>",
          "description": "We address the problem of compressed sensing using a deep generative prior\nmodel and consider both linear and learned nonlinear sensing mechanisms, where\nthe nonlinear one involves either a fully connected neural network or a\nconvolutional neural network. Recently, it has been argued that the\ndistribution of natural images do not lie in a single manifold but rather lie\nin a union of several submanifolds. We propose a sparsity-driven latent space\nsampling (SDLSS) framework and develop a proximal meta-learning (PML) algorithm\nto enforce sparsity in the latent space. SDLSS allows the range-space of the\ngenerator to be considered as a union-of-submanifolds. We also derive the\nsample complexity bounds within the SDLSS framework for the linear measurement\nmodel. The results demonstrate that for a higher degree of compression, the\nSDLSS method is more efficient than the state-of-the-art method. We first\nconsider a comparison between linear and nonlinear sensing mechanisms on\nFashion-MNIST dataset and show that the learned nonlinear version is superior\nto the linear one. Subsequent comparisons with the deep compressive sensing\n(DCS) framework proposed in the literature are reported. We also consider the\neffect of the dimension of the latent space and the sparsity factor in\nvalidating the SDLSS framework. Performance quantification is carried out by\nemploying three objective metrics: peak signal-to-noise ratio (PSNR),\nstructural similarity index metric (SSIM), and reconstruction error (RE).",
          "link": "http://arxiv.org/abs/2105.11956",
          "publishedOn": "2021-05-26T01:22:10.617Z",
          "wordCount": 666,
          "title": "Learning Generative Prior with Latent Space Sparsity Constraints. (arXiv:2105.11956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barchid_S/0/1/0/all/0/1\">Sami Barchid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mennesson_J/0/1/0/all/0/1\">Jos&#xe9; Mennesson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1\">Chaabane Dj&#xe9;raba</a>",
          "description": "Many research works focus on leveraging the complementary geometric\ninformation of indoor depth sensors in vision tasks performed by deep\nconvolutional neural networks, notably semantic segmentation. These works deal\nwith a specific vision task known as \"RGB-D Indoor Semantic Segmentation\". The\nchallenges and resulting solutions of this task differ from its standard RGB\ncounterpart. This results in a new active research topic. The objective of this\npaper is to introduce the field of Deep Convolutional Neural Networks for RGB-D\nIndoor Semantic Segmentation. This review presents the most popular public\ndatasets, proposes a categorization of the strategies employed by recent\ncontributions, evaluates the performance of the current state-of-the-art, and\ndiscusses the remaining challenges and promising directions for future works.",
          "link": "http://arxiv.org/abs/2105.11925",
          "publishedOn": "2021-05-26T01:22:10.610Z",
          "wordCount": 552,
          "title": "Review on Indoor RGB-D Semantic Segmentation with Deep Convolutional Neural Networks. (arXiv:2105.11925v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12106",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pervin_M/0/1/0/all/0/1\">Mst. Tasnim Pervin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_L/0/1/0/all/0/1\">Linmi Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huq_A/0/1/0/all/0/1\">Aminul Huq</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Z/0/1/0/all/0/1\">Zuoxiang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_L/0/1/0/all/0/1\">Li Huo</a>",
          "description": "Segmentation is considered to be a very crucial task in medical image\nanalysis. This task has been easier since deep learning models have taken over\nwith its high performing behavior. However, deep learning models dependency on\nlarge data proves it to be an obstacle in medical image analysis because of\ninsufficient data samples. Several data augmentation techniques have been used\nto mitigate this problem. We propose a new augmentation method by introducing\nadversarial learning attack techniques, specifically Fast Gradient Sign Method\n(FGSM). Furthermore, We have also introduced the concept of Inverse FGSM\n(InvFGSM), which works in the opposite manner of FGSM for the data\naugmentation. This two approaches worked together to improve the segmentation\naccuracy, as well as helped the model to gain robustness against adversarial\nattacks. The overall analysis of experiments indicates a novel use of\nadversarial machine learning along with robustness enhancement.",
          "link": "http://arxiv.org/abs/2105.12106",
          "publishedOn": "2021-05-26T01:22:10.602Z",
          "wordCount": 594,
          "title": "Adversarial Attack Driven Data Augmentation for Accurate And Robust Medical Image Segmentation. (arXiv:2105.12106v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Hongzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanbiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jigui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "The cost aggregation strategy shows a crucial role in learning-based stereo\nmatching tasks, where 3D convolutional filters obtain state of the art but\nrequire intensive computation resources, while 2D operations need less GPU\nmemory but are sensitive to domain shift. In this paper, we decouple the 4D\ncubic cost volume used by 3D convolutional filters into sequential cost maps\nalong the direction of disparity instead of dealing with it at once by\nexploiting a recurrent cost aggregation strategy. Furthermore, a novel\nrecurrent module, Stacked Recurrent Hourglass (SRH), is proposed to process\neach cost map. Our hourglass network is constructed based on Gated Recurrent\nUnits (GRUs) and down/upsampling layers, which provides GRUs larger receptive\nfields. Then two hourglass networks are stacked together, while multi-scale\ninformation is processed by skip connections to enhance the performance of the\npipeline in textureless areas. The proposed architecture is implemented in an\nend-to-end pipeline and evaluated on public datasets, which reduces GPU memory\nconsumption by up to 56.1\\% compared with PSMNet using stacked hourglass 3D\nCNNs without the degradation of accuracy. Then, we further demonstrate the\nscalability of the proposed method on several high-resolution pairs, while\npreviously learned approaches often fail due to the memory constraint. The code\nis released at \\url{https://github.com/hongzhidu/SRHNet}.",
          "link": "http://arxiv.org/abs/2105.11587",
          "publishedOn": "2021-05-26T01:22:10.588Z",
          "wordCount": 642,
          "title": "SRH-Net: Stacked Recurrent Hourglass Network for Stereo Matching. (arXiv:2105.11587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11748",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Weiyi Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1\">Colin Jacobs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>",
          "description": "Automatic lesion segmentation on thoracic CT enables rapid quantitative\nanalysis of lung involvement in COVID- 19 infections. Obtaining voxel-level\nannotations for training segmentation networks is prohibitively expensive.\nTherefore we propose a weakly-supervised segmentation method based on dense\nregression activation maps (dRAM). Most advanced weakly supervised segmentation\napproaches exploit class activation maps (CAMs) to localize objects generated\nfrom high-level semantic features at a coarse resolution. As a result, CAMs\nprovide coarse outlines that do not align precisely with the object\nsegmentations. Instead, we exploit dense features from a segmentation network\nto compute dense regression activation maps (dRAMs) for preserving local\ndetails. During training, dRAMs are pooled lobe-wise to regress the per-lobe\nlesion percentage. In such a way, the network achieves additional information\nregarding the lesion quantification in comparison with the classification\napproach. Furthermore, we refine dRAMs based on an attention module and dense\nconditional random field trained together with the main regression task. The\nrefined dRAMs are served as the pseudo labels for training a final segmentation\nnetwork. When evaluated on 69 CT scans, our method substantially improves the\nintersection over union from 0.335 in the CAM-based weakly supervised\nsegmentation method to 0.495.",
          "link": "http://arxiv.org/abs/2105.11748",
          "publishedOn": "2021-05-26T01:22:10.561Z",
          "wordCount": 688,
          "title": "Dense Regression Activation Maps For Lesion Segmentation in CT scans of COVID-19 patients. (arXiv:2105.11748v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuta Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Imari Sato</a>",
          "description": "Recovering the 3D geometry of a purely texture-less object with generally\nunknown surface reflectance (e.g. non-Lambertian) is regarded as a challenging\ntask in multi-view reconstruction. The major obstacle revolves around\nestablishing cross-view correspondences where photometric constancy is\nviolated. This paper proposes a simple and practical solution to overcome this\nchallenge based on a co-located camera-light scanner device. Unlike existing\nsolutions, we do not explicitly solve for correspondence. Instead, we argue the\nproblem is generally well-posed by multi-view geometrical and photometric\nconstraints, and can be solved from a small number of input views. We formulate\nthe reconstruction task as a joint energy minimization over the surface\ngeometry and reflectance. Despite this energy is highly non-convex, we develop\nan optimization algorithm that robustly recovers globally optimal shape and\nreflectance even from a random initialization. Extensive experiments on both\nsimulated and real data have validated our method, and possible future\nextensions are discussed.",
          "link": "http://arxiv.org/abs/2105.11599",
          "publishedOn": "2021-05-26T01:22:10.556Z",
          "wordCount": 596,
          "title": "Multi-view 3D Reconstruction of a Texture-less Smooth Surface of Unknown Generic Reflectance. (arXiv:2105.11599v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yanyun Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>",
          "description": "Convolutional neural networks (CNNs) are highly successful for\nsuper-resolution (SR) but often require sophisticated architectures with heavy\nmemory cost and computational overhead, significantly restricts their practical\ndeployments on resource-limited devices. In this paper, we proposed a novel\ncontrastive self-distillation (CSD) framework to simultaneously compress and\naccelerate various off-the-shelf SR models. In particular, a channel-splitting\nsuper-resolution network can first be constructed from a target teacher network\nas a compact student network. Then, we propose a novel contrastive loss to\nimprove the quality of SR images and PSNR/SSIM via explicit knowledge transfer.\nExtensive experiments demonstrate that the proposed CSD scheme effectively\ncompresses and accelerates several standard SR models such as EDSR, RCAN and\nCARN. Code is available at https://github.com/Booooooooooo/CSD.",
          "link": "http://arxiv.org/abs/2105.11683",
          "publishedOn": "2021-05-26T01:22:10.529Z",
          "wordCount": 558,
          "title": "Towards Compact Single Image Super-Resolution via Contrastive Self-distillation. (arXiv:2105.11683v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weihong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qifang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhuoyao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1\">Qin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1\">Qiang Huo</a>",
          "description": "Recent grid-based document representations like BERTgrid allow the\nsimultaneous encoding of the textual and layout information of a document in a\n2D feature map so that state-of-the-art image segmentation and/or object\ndetection models can be straightforwardly leveraged to extract key information\nfrom documents. However, such methods have not achieved comparable performance\nto state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK\nyet. In this paper, we propose a new multi-modal backbone network by\nconcatenating a BERTgrid to an intermediate layer of a CNN model, where the\ninput of CNN is a document image and the BERTgrid is a grid of word embeddings,\nto generate a more powerful grid-based document representation, named\nViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal\nbackbone network are trained jointly. Our experimental results demonstrate that\nthis joint training strategy improves significantly the representation ability\nof ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction\napproach has achieved state-of-the-art performance on real-world datasets.",
          "link": "http://arxiv.org/abs/2105.11672",
          "publishedOn": "2021-05-26T01:22:10.523Z",
          "wordCount": 620,
          "title": "ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for Key Information Extraction from Documents. (arXiv:2105.11672v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1806.06530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deitsch_S/0/1/0/all/0/1\">Sergiu Deitsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buerhop_Lutz_C/0/1/0/all/0/1\">Claudia Buerhop-Lutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sovetkin_E/0/1/0/all/0/1\">Evgenii Sovetkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steland_A/0/1/0/all/0/1\">Ansgar Steland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallwitz_F/0/1/0/all/0/1\">Florian Gallwitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1\">Christian Riess</a>",
          "description": "High resolution electroluminescence (EL) images captured in the infrared\nspectrum allow to visually and non-destructively inspect the quality of\nphotovoltaic (PV) modules. Currently, however, such a visual inspection\nrequires trained experts to discern different kinds of defects, which is\ntime-consuming and expensive. Automated segmentation of cells is therefore a\nkey step in automating the visual inspection workflow. In this work, we propose\na robust automated segmentation method for extraction of individual solar cells\nfrom EL images of PV modules. This enables controlled studies on large amounts\nof data to understanding the effects of module degradation over time-a process\nnot yet fully understood. The proposed method infers in several steps a\nhigh-level solar module representation from low-level edge features. An\nimportant step in the algorithm is to formulate the segmentation problem in\nterms of lens calibration by exploiting the plumbline constraint. We evaluate\nour method on a dataset of various solar modules types containing a total of\n408 solar cells with various defects. Our method robustly solves this task with\na median weighted Jaccard index of 94.47% and an $F_1$ score of 97.62%, both\nindicating a very high similarity between automatically segmented and ground\ntruth solar cell masks.",
          "link": "http://arxiv.org/abs/1806.06530",
          "publishedOn": "2021-05-26T01:22:10.488Z",
          "wordCount": 685,
          "title": "Segmentation of Photovoltaic Module Cells in Uncalibrated Electroluminescence Images. (arXiv:1806.06530v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shuai_B/0/1/0/all/0/1\">Bing Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berneshawi_A/0/1/0/all/0/1\">Andrew Berneshawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>",
          "description": "In this paper, we focus on improving online multi-object tracking (MOT). In\nparticular, we introduce a region-based Siamese Multi-Object Tracking network,\nwhich we name SiamMOT. SiamMOT includes a motion model that estimates the\ninstance's movement between two frames such that detected instances are\nassociated. To explore how the motion modelling affects its tracking\ncapability, we present two variants of Siamese tracker, one that implicitly\nmodels motion and one that models it explicitly. We carry out extensive\nquantitative experiments on three different MOT datasets: MOT17, TAO-person and\nCaltech Roadside Pedestrians, showing the importance of motion modelling for\nMOT and the ability of SiamMOT to substantially outperform the\nstate-of-the-art. Finally, SiamMOT also outperforms the winners of ACM MM'20\nHiEve Grand Challenge on HiEve dataset. Moreover, SiamMOT is efficient, and it\nruns at 17 FPS for 720P videos on a single modern GPU. Codes are available in\n\\url{https://github.com/amazon-research/siam-mot}.",
          "link": "http://arxiv.org/abs/2105.11595",
          "publishedOn": "2021-05-26T01:22:10.468Z",
          "wordCount": 574,
          "title": "SiamMOT: Siamese Multi-Object Tracking. (arXiv:2105.11595v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_N/0/1/0/all/0/1\">Noah Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chugunov_I/0/1/0/all/0/1\">Ilya Chugunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>",
          "description": "Depth cameras are emerging as a cornerstone modality with diverse\napplications that directly or indirectly rely on measured depth, including\npersonal devices, robotics, and self-driving vehicles. Although time-of-flight\n(ToF) methods have fueled these applications, the precision and robustness of\nToF methods is limited by relying on photon time-tagging or modulation after\nphoto-conversion. Successful optical modulation approaches have been restricted\nfiber-coupled modulation with large coupling losses or interferometric\nmodulation with sub-cm range, and the precision gap between interferometric\nmethods and ToF methods is more than three orders of magnitudes. In this work,\nwe close this gap and propose a computational imaging method for all-optical\nfree-space correlation before photo-conversion that achieves micron-scale depth\nresolution with robustness to surface reflectance and ambient light with\nconventional silicon intensity sensors. To this end, we solve two technical\nchallenges: modulating at GHz rates and computational phase unwrapping. We\npropose an imaging approach with resonant polarization modulators and devise a\nnovel optical dual-pass frequency-doubling which achieves high modulation\ncontrast at more than 10GHz. At the same time, centimeter-wave modulation\ntogether with a small modulation bandwidth render existing phase unwrapping\nmethods ineffective. We tackle this problem with a neural phase unwrapping\nmethod that exploits that adjacent wraps are often highly correlated. We\nvalidate the proposed method in simulation and experimentally, where it\nachieves micron-scale depth precision. We demonstrate precise depth sensing\nindependently of surface texture and ambient light and compare against existing\nanalog demodulation methods, which we outperform across all tested scenarios.",
          "link": "http://arxiv.org/abs/2105.11606",
          "publishedOn": "2021-05-26T01:22:10.454Z",
          "wordCount": 672,
          "title": "Centimeter-Wave Free-Space Time-of-Flight Imaging. (arXiv:2105.11606v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuhui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruili Wang</a>",
          "description": "Text-based person search is a sub-task in the field of image retrieval, which\naims to retrieve target person images according to a given textual description.\nThe significant feature gap between two modalities makes this task very\nchallenging. Many existing methods attempt to utilize local alignment to\naddress this problem in the fine-grained level. However, most relevant methods\nintroduce additional models or complicated training and evaluation strategies,\nwhich are hard to use in realistic scenarios. In order to facilitate the\npractical application, we propose a simple but effective end-to-end learning\nframework for text-based person search named TIPCB (i.e., Text-Image Part-based\nConvolutional Baseline). Firstly, a novel dual-path local alignment network\nstructure is proposed to extract visual and textual local representations, in\nwhich images are segmented horizontally and texts are aligned adaptively. Then,\nwe propose a multi-stage cross-modal matching strategy, which eliminates the\nmodality gap from three feature levels, including low level, local level and\nglobal level. Extensive experiments are conducted on the widely-used benchmark\ndataset (CUHK-PEDES) and verify that our method outperforms the\nstate-of-the-art methods by 3.69%, 2.95% and 2.31% in terms of Top-1, Top-5 and\nTop-10. Our code has been released in https://github.com/OrangeYHChen/TIPCB.",
          "link": "http://arxiv.org/abs/2105.11628",
          "publishedOn": "2021-05-26T01:22:10.448Z",
          "wordCount": 639,
          "title": "TIPCB: A Simple but Effective Part-based Convolutional Baseline for Text-based Person Search. (arXiv:2105.11628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yaya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>",
          "description": "By adding human-imperceptible perturbations to images, DNNs can be easily\nfooled. As one of the mainstream methods, feature space targeted attacks\nperturb images by modulating their intermediate feature maps, for the\ndiscrepancy between the intermediate source and target features is minimized.\nHowever, the current choice of pixel-wise Euclidean Distance to measure the\ndiscrepancy is questionable because it unreasonably imposes a\nspatial-consistency constraint on the source and target features. Intuitively,\nan image can be categorized as \"cat\" no matter the cat is on the left or right\nof the image. To address this issue, we propose to measure this discrepancy\nusing statistic alignment. Specifically, we design two novel approaches called\nPair-wise Alignment Attack and Global-wise Alignment Attack, which attempt to\nmeasure similarities between feature maps by high-order statistics with\ntranslation invariance. Furthermore, we systematically analyze the layer-wise\ntransferability with varied difficulties to obtain highly reliable attacks.\nExtensive experiments verify the effectiveness of our proposed method, and it\noutperforms the state-of-the-art algorithms by a large margin. Our code is\npublicly available at https://github.com/yaya-cheng/PAA-GAA.",
          "link": "http://arxiv.org/abs/2105.11645",
          "publishedOn": "2021-05-26T01:22:10.440Z",
          "wordCount": 603,
          "title": "Feature Space Targeted Attacks by Statistic Alignment. (arXiv:2105.11645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Ground surface detection in point cloud is widely used as a key module in\nautonomous driving systems. Different from previous approaches which are mostly\ndeveloped for lidars with high beam resolution, e.g. Velodyne HDL-64, this\npaper proposes ground detection techniques applicable to much sparser point\ncloud captured by lidars with low beam resolution, e.g. Velodyne VLP-16. The\napproach is based on the RANSAC scheme of plane fitting. Inlier verification\nfor plane hypotheses is enhanced by exploiting the point-wise tangent, which is\na local feature available to compute regardless of the density of lidar beams.\nGround surface which is not perfectly planar is fitted by multiple\n(specifically 4 in our implementation) disjoint plane regions. By assuming\nthese plane regions to be rectanglar and exploiting the integral image\ntechnique, our approach approximately finds the optimal region partition and\nplane hypotheses under the RANSAC scheme with real-time computational\ncomplexity.",
          "link": "http://arxiv.org/abs/2105.11649",
          "publishedOn": "2021-05-26T01:22:10.434Z",
          "wordCount": 575,
          "title": "On Enhancing Ground Surface Detection from Sparse Lidar Point Cloud. (arXiv:2105.11649v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>",
          "description": "Representation of semantic context and local details is the essential issue\nfor building modern semantic segmentation models. However, the\ninterrelationship between semantic context and local details is not well\nexplored in previous works. In this paper, we propose a Dynamic Dual Sampling\nModule (DDSM) to conduct dynamic affinity modeling and propagate semantic\ncontext to local details, which yields a more discriminative representation.\nSpecifically, a dynamic sampling strategy is used to sparsely sample\nrepresentative pixels and channels in the higher layer, forming adaptive\ncompact support for each pixel and channel in the lower layer. The sampled\nfeatures with high semantics are aggregated according to the affinities and\nthen propagated to detailed lower-layer features, leading to a fine-grained\nsegmentation result with well-preserved boundaries. Experiment results on both\nCityscapes and Camvid datasets validate the effectiveness and efficiency of the\nproposed approach. Code and models will be available at\n\\url{x3https://github.com/Fantasticarl/DDSM}.",
          "link": "http://arxiv.org/abs/2105.11657",
          "publishedOn": "2021-05-26T01:22:10.427Z",
          "wordCount": 581,
          "title": "Dynamic Dual Sampling Module for Fine-Grained Semantic Segmentation. (arXiv:2105.11657v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mosser_L/0/1/0/all/0/1\">Lukas Mosser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeini_E/0/1/0/all/0/1\">Ehsan Zabihi Naeini</a>",
          "description": "Deep neural networks offer numerous potential applications across geoscience,\nfor example, one could argue that they are the state-of-the-art method for\npredicting faults in seismic datasets. In quantitative reservoir\ncharacterization workflows, it is common to incorporate the uncertainty of\npredictions thus such subsurface models should provide calibrated probabilities\nand the associated uncertainties in their predictions. It has been shown that\npopular Deep Learning-based models are often miscalibrated, and due to their\ndeterministic nature, provide no means to interpret the uncertainty of their\npredictions. We compare three different approaches to obtaining probabilistic\nmodels based on convolutional neural networks in a Bayesian formalism, namely\nDeep Ensembles, Concrete Dropout, and Stochastic Weight Averaging-Gaussian\n(SWAG). These methods are consistently applied to fault detection case studies\nwhere Deep Ensembles use independently trained models to provide fault\nprobabilities, Concrete Dropout represents an extension to the popular Dropout\ntechnique to approximate Bayesian neural networks, and finally, we apply SWAG,\na recent method that is based on the Bayesian inference equivalence of\nmini-batch Stochastic Gradient Descent. We provide quantitative results in\nterms of model calibration and uncertainty representation, as well as\nqualitative results on synthetic and real seismic datasets. Our results show\nthat the approximate Bayesian methods, Concrete Dropout and SWAG, both provide\nwell-calibrated predictions and uncertainty attributes at a lower computational\ncost when compared to the baseline Deep Ensemble approach. The resulting\nuncertainties also offer a possibility to further improve the model performance\nas well as enhancing the interpretability of the models.",
          "link": "http://arxiv.org/abs/2105.12115",
          "publishedOn": "2021-05-26T01:22:10.388Z",
          "wordCount": 700,
          "title": "Calibration and Uncertainty Quantification of Bayesian Convolutional Neural Networks for Geophysical Applications. (arXiv:2105.12115v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yawen Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Recent breakthroughs of Neural Architecture Search (NAS) extend the field's\nresearch scope towards a broader range of vision tasks and more diversified\nsearch spaces. While existing NAS methods mostly design architectures on a\nsingle task, algorithms that look beyond single-task search are surging to\npursue a more efficient and universal solution across various tasks. Many of\nthem leverage transfer learning and seek to preserve, reuse, and refine network\ndesign knowledge to achieve higher efficiency in future tasks. However, the\nenormous computational cost and experiment complexity of cross-task NAS are\nimposing barriers for valuable research in this direction. Existing NAS\nbenchmarks all focus on one type of vision task, i.e., classification. In this\nwork, we propose TransNAS-Bench-101, a benchmark dataset containing network\nperformance across seven tasks, covering classification, regression,\npixel-level prediction, and self-supervised tasks. This diversity provides\nopportunities to transfer NAS methods among tasks and allows for more complex\ntransfer schemes to evolve. We explore two fundamentally different types of\nsearch space: cell-level search space and macro-level search space. With 7,352\nbackbones evaluated on seven tasks, 51,464 trained models with detailed\ntraining information are provided. With TransNAS-Bench-101, we hope to\nencourage the advent of exceptional NAS algorithms that raise cross-task search\nefficiency and generalizability to the next level. Our dataset file will be\navailable at Mindspore, VEGA.",
          "link": "http://arxiv.org/abs/2105.11871",
          "publishedOn": "2021-05-26T01:22:10.365Z",
          "wordCount": 672,
          "title": "TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search. (arXiv:2105.11871v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhicheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuhui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shengyong Chen</a>",
          "description": "Person re-identification (re-ID) tackles the problem of matching person\nimages with the same identity from different cameras. In practical\napplications, due to the differences in camera performance and distance between\ncameras and persons of interest, captured person images usually have various\nresolutions. We name this problem as Cross-Resolution Person Re-identification\nwhich brings a great challenge for matching correctly. In this paper, we\npropose a Deep High-Resolution Pseudo-Siamese Framework (PS-HRNet) to solve the\nabove problem. Specifically, in order to restore the resolution of\nlow-resolution images and make reasonable use of different channel information\nof feature maps, we introduce and innovate VDSR module with channel attention\n(CA) mechanism, named as VDSR-CA. Then we reform the HRNet by designing a novel\nrepresentation head to extract discriminating features, named as HRNet-ReID. In\naddition, a pseudo-siamese framework is constructed to reduce the difference of\nfeature distributions between low-resolution images and high-resolution images.\nThe experimental results on five cross-resolution person datasets verify the\neffectiveness of our proposed approach. Compared with the state-of-the-art\nmethods, our proposed PS-HRNet improves 3.4\\%, 6.2\\%, 2.5\\%,1.1\\% and 4.2\\% at\nRank-1 on MLR-Market-1501, MLR-CUHK03, MLR-VIPeR, MLR-DukeMTMC-reID, and CAVIAR\ndatasets, respectively. Our code is available at\n\\url{https://github.com/zhguoqing}.",
          "link": "http://arxiv.org/abs/2105.11722",
          "publishedOn": "2021-05-26T01:22:10.359Z",
          "wordCount": 632,
          "title": "Deep High-Resolution Representation Learning for Cross-Resolution Person Re-identification. (arXiv:2105.11722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "Steerable CNN imposes the prior knowledge of transformation invariance or\nequivariance in the network architecture to enhance the the network robustness\non geometry transformation of data and reduce overfitting. It has been an\nintuitive and widely used technique to construct a steerable filter by\naugmenting a filter with its transformed copies in the past decades, which is\nnamed as filter transform in this paper. Recently, the problem of steerable CNN\nhas been studied from aspect of group representation theory, which reveals the\nfunction space structure of a steerable kernel function. However, it is not yet\nclear on how this theory is related to the filter transform technique. In this\npaper, we show that kernel constructed by filter transform can also be\ninterpreted in the group representation theory. This interpretation help\ncomplete the puzzle of steerable CNN theory and provides a novel and simple\napproach to implement steerable convolution operators. Experiments are executed\non multiple datasets to verify the feasibility of the proposed approach.",
          "link": "http://arxiv.org/abs/2105.11636",
          "publishedOn": "2021-05-26T01:22:10.353Z",
          "wordCount": 596,
          "title": "FILTRA: Rethinking Steerable CNN by Filter Transform. (arXiv:2105.11636v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jia-Wang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Huangying Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>",
          "description": "We propose a monocular depth estimator SC-Depth, which requires only\nunlabelled videos for training and enables the scale-consistent prediction at\ninference time. Our contributions include: (i) we propose a geometry\nconsistency loss, which penalizes the inconsistency of predicted depths between\nadjacent views; (ii) we propose a self-discovered mask to automatically\nlocalize moving objects that violate the underlying static scene assumption and\ncause noisy signals during training; (iii) we demonstrate the efficacy of each\ncomponent with a detailed ablation study and show high-quality depth estimation\nresults in both KITTI and NYUv2 datasets. Moreover, thanks to the capability of\nscale-consistent prediction, we show that our monocular-trained deep networks\nare readily integrated into the ORB-SLAM2 system for more robust and accurate\ntracking. The proposed hybrid Pseudo-RGBD SLAM shows compelling results in\nKITTI, and it generalizes well to the KAIST dataset without additional\ntraining. Finally, we provide several demos for qualitative evaluation.",
          "link": "http://arxiv.org/abs/2105.11610",
          "publishedOn": "2021-05-26T01:22:10.347Z",
          "wordCount": 595,
          "title": "Unsupervised Scale-consistent Depth Learning from Video. (arXiv:2105.11610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sam Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliard_G/0/1/0/all/0/1\">Grayson Hilliard</a>",
          "description": "The ubiquity of mobile phones makes mobile GUI understanding an important\ntask. Most previous works in this domain require human-created metadata of\nscreens (e.g. View Hierarchy) during inference, which unfortunately is often\nnot available or reliable enough for GUI understanding. Inspired by the\nimpressive success of Transformers in NLP tasks, targeting for purely\nvision-based GUI understanding, we extend the concepts of Words/Sentence to\nPixel-Words/Screen-Sentence, and propose a mobile GUI understanding\narchitecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the\nindividual Words, we define the Pixel-Words as atomic visual components (text\nand graphic components), which are visually consistent and semantically clear\nacross screenshots of a large variety of design styles. The Pixel-Words\nextracted from a screenshot are aggregated into Screen-Sentence with a Screen\nTransformer proposed to model their relations. Since the Pixel-Words are\ndefined as atomic visual components, the ambiguity between their visual\nappearance and semantics is dramatically reduced. We are able to make use of\nmetadata available in training data to auto-generate high-quality annotations\nfor Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words\nannotations is built based on the public RICO dataset, which will be released\nto help to address the lack of high-quality training data in this area. We\ntrain a detector to extract Pixel-Words from screenshots on this dataset and\nachieve metadata-free GUI understanding during inference. We conduct\nexperiments and show that Pixel-Words can be well extracted on RICO-PW and well\ngeneralized to a new dataset, P2S-UI, collected by ourselves. The effectiveness\nof PW2SS is further verified in the GUI understanding tasks including relation\nprediction, clickability prediction, screen retrieval, and app type\nclassification.",
          "link": "http://arxiv.org/abs/2105.11941",
          "publishedOn": "2021-05-26T01:22:10.340Z",
          "wordCount": 704,
          "title": "Understanding Mobile GUI: from Pixel-Words to Screen-Sentences. (arXiv:2105.11941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burgess_C/0/1/0/all/0/1\">Cassandra Burgess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neisinger_C/0/1/0/all/0/1\">Cordelia Neisinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinner_R/0/1/0/all/0/1\">Rafael Dinner</a>",
          "description": "We present a novel convolutional neural network that learns to match images\nof an object taken from different viewpoints or by different optical sensors.\nOur Re-Identification Across Domain Network (RADON) scores pairs of input\nimages from different domains on similarity. Our approach extends previous work\non Siamese networks and modifies them to more challenging use cases, including\nlow- and no-shot learning, in which few images of a specific target are\navailable for training. RADON shows strong performance on cross-view vehicle\nmatching and cross-domain person identification in a no-shot learning\nenvironment.",
          "link": "http://arxiv.org/abs/2105.12056",
          "publishedOn": "2021-05-26T01:22:10.323Z",
          "wordCount": 524,
          "title": "Matching Targets Across Domains with RADON, the Re-Identification Across Domain Network. (arXiv:2105.12056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.08514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "We propose a novel network pruning approach by information preserving of\npre-trained network weights (filters). Network pruning with the information\npreserving is formulated as a matrix sketch problem, which is efficiently\nsolved by the off-the-shelf Frequent Direction method. Our approach, referred\nto as FilterSketch, encodes the second-order information of pre-trained\nweights, which enables the representation capacity of pruned networks to be\nrecovered with a simple fine-tuning procedure. FilterSketch requires neither\ntraining from scratch nor data-driven iterative optimization, leading to a\nseveral-orders-of-magnitude reduction of time cost in the optimization of\npruning. Experiments on CIFAR-10 show that FilterSketch reduces 63.3% of FLOPs\nand prunes 59.9% of network parameters with negligible accuracy cost for\nResNet-110. On ILSVRC-2012, it reduces 45.5% of FLOPs and removes 43.0% of\nparameters with only 0.69% accuracy drop for ResNet-50. Our code and pruned\nmodels can be found at https://github.com/lmbxmu/FilterSketch.",
          "link": "http://arxiv.org/abs/2001.08514",
          "publishedOn": "2021-05-26T01:22:10.316Z",
          "wordCount": 637,
          "title": "Filter Sketch for Network Pruning. (arXiv:2001.08514v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Ji Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1\">Benjamin Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>",
          "description": "The rapid progress in 3D scene understanding has come with growing demand for\ndata; however, collecting and annotating 3D scenes (e.g. point clouds) are\nnotoriously hard. For example, the number of scenes (e.g. indoor rooms) that\ncan be accessed and scanned might be limited; even given sufficient data,\nacquiring 3D labels (e.g. instance masks) requires intensive human labor. In\nthis paper, we explore data-efficient learning for 3D point cloud. As a first\nstep towards this direction, we propose Contrastive Scene Contexts, a 3D\npre-training method that makes use of both point-level correspondences and\nspatial contexts in a scene. Our method achieves state-of-the-art results on a\nsuite of benchmarks where training data or labels are scarce. Our study reveals\nthat exhaustive labelling of 3D point clouds might be unnecessary; and\nremarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89%\n(instance segmentation) and 96% (semantic segmentation) of the baseline\nperformance that uses full annotations.",
          "link": "http://arxiv.org/abs/2012.09165",
          "publishedOn": "2021-05-26T01:22:10.306Z",
          "wordCount": 620,
          "title": "Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts. (arXiv:2012.09165v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1\">Waqas Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_A/0/1/0/all/0/1\">Amir Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nebhen_J/0/1/0/all/0/1\">Jamel Nebhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Neeraj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_F/0/1/0/all/0/1\">Faisal Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RehmanJaved_A/0/1/0/all/0/1\">Abdul RehmanJaved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1\">Thippa Reddy Gadekallu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalil_Z/0/1/0/all/0/1\">Zunera Jalil</a>",
          "description": "Cash payment is still king in several markets, accounting for more than 90\\\nof the payments in almost all the developing countries. The usage of mobile\nphones is pretty ordinary in this present era. Mobile phones have become an\ninseparable friend for many users, serving much more than just communication\ntools. Every subsequent person is heavily relying on them due to multifaceted\nusage and affordability. Every person wants to manage his/her daily\ntransactions and related issues by using his/her mobile phone. With the rise\nand advancements of mobile-specific security, threats are evolving as well. In\nthis paper, we provide a survey of various security models for mobile phones.\nWe explore multiple proposed models of the mobile payment system (MPS), their\ntechnologies and comparisons, payment methods, different security mechanisms\ninvolved in MPS, and provide analysis of the encryption technologies,\nauthentication methods, and firewall in MPS. We also present current challenges\nand future directions of mobile phone security.",
          "link": "http://arxiv.org/abs/2105.12097",
          "publishedOn": "2021-05-26T01:22:10.300Z",
          "wordCount": 602,
          "title": "Security in Next Generation Mobile Payment Systems: A Comprehensive Survey. (arXiv:2105.12097v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1\">Etienne Bennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1\">Victor Bouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1\">Antoine Toubhans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>",
          "description": "Few-Shot Learning (FSL) algorithms have made substantial progress in learning\nnovel concepts with just a handful of labelled data. To classify query\ninstances from novel classes encountered at test-time, they only require a\nsupport set composed of a few labelled samples. FSL benchmarks commonly assume\nthat those queries come from the same distribution as instances in the support\nset. However, in a realistic set-ting, data distribution is plausibly subject\nto change, a situation referred to as Distribution Shift (DS). The present work\naddresses the new and challenging problem of Few-Shot Learning under\nSupport/Query Shift (FSQS) i.e., when support and query instances are sampled\nfrom related but different distributions. Our contributions are the following.\nFirst, we release a testbed for FSQS, including datasets, relevant baselines\nand a protocol for a rigorous and reproducible evaluation. Second, we observe\nthat well-established FSL algorithms unsurprisingly suffer from a considerable\ndrop in accuracy when facing FSQS, stressing the significance of our study.\nFinally, we show that transductive algorithms can limit the inopportune effect\nof DS. In particular, we study both the role of Batch-Normalization and Optimal\nTransport (OT) in aligning distributions, bridging Unsupervised Domain\nAdaptation with FSL. This results in a new method that efficiently combines OT\nwith the celebrated Prototypical Networks. We bring compelling experiments\ndemonstrating the advantage of our method. Our work opens an exciting line of\nresearch by providing a testbed and strong baselines. Our code is available at\nhttps://github.com/ebennequin/meta-domain-shift.",
          "link": "http://arxiv.org/abs/2105.11804",
          "publishedOn": "2021-05-26T01:22:10.294Z",
          "wordCount": 683,
          "title": "Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1904.10165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jinwen Ma</a>",
          "description": "Tensor completion and robust principal component analysis have been widely\nused in machine learning while the key problem relies on the minimization of a\ntensor rank that is very challenging. A common way to tackle this difficulty is\nto approximate the tensor rank with the $\\ell_1-$norm of singular values based\non its Tensor Singular Value Decomposition (T-SVD). Besides, the sparsity of a\ntensor is also measured by its $\\ell_1-$norm. However, the $\\ell_1$ penalty is\nessentially biased and thus the result will deviate. In order to sidestep the\nbias, we propose a novel non-convex tensor rank surrogate function and a novel\nnon-convex sparsity measure. In this new setting by using the concavity instead\nof the convexity, a majorization minimization algorithm is further designed for\ntensor completion and robust principal component analysis. Furthermore, we\nanalyze its theoretical properties. Finally, the experiments on natural and\nhyperspectral images demonstrate the efficacy and efficiency of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/1904.10165",
          "publishedOn": "2021-05-26T01:22:10.288Z",
          "wordCount": 616,
          "title": "T-SVD Based Non-convex Tensor Completion and Robust Principal Component Analysis. (arXiv:1904.10165v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haosen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hongxun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hujie Huang</a>",
          "description": "Transformer networks are effective at modeling long-range contextual\ninformation and have recently demonstrated exemplary performance in the natural\nlanguage processing domain. Conventionally, the temporal action proposal\ngeneration (TAPG) task is divided into two main sub-tasks: boundary prediction\nand proposal confidence prediction, which rely on the frame-level dependencies\nand proposal-level relationships separately. To capture the dependencies at\ndifferent levels of granularity, this paper intuitively presents a unified\ntemporal action proposal generation framework with original Transformers,\ncalled TAPG Transformer, which consists of a Boundary Transformer and a\nProposal Transformer. Specifically, the Boundary Transformer captures long-term\ntemporal dependencies to predict precise boundary information and the Proposal\nTransformer learns the rich inter-proposal relationships for reliable\nconfidence evaluation. Extensive experiments are conducted on two popular\nbenchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG\nTransformer outperforms state-of-the-art methods. Equipped with the existing\naction classifier, our method achieves remarkable performance on the temporal\naction localization task. Codes and models will be available.",
          "link": "http://arxiv.org/abs/2105.12043",
          "publishedOn": "2021-05-26T01:22:10.268Z",
          "wordCount": 594,
          "title": "Temporal Action Proposal Generation with Transformers. (arXiv:2105.12043v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.12197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>",
          "description": "We present a method to search for a probe (or query) image representation\nagainst a large gallery in the encrypted domain. We require that the probe and\ngallery images be represented in terms of a fixed-length representation, which\nis typical for representations obtained from learned networks. Our encryption\nscheme is agnostic to how the fixed-length representation is obtained and can\ntherefore be applied to any fixed-length representation in any application\ndomain. Our method, dubbed HERS (Homomorphically Encrypted Representation\nSearch), operates by (i) compressing the representation towards its estimated\nintrinsic dimensionality with minimal loss of accuracy (ii) encrypting the\ncompressed representation using the proposed fully homomorphic encryption\nscheme, and (iii) efficiently searching against a gallery of encrypted\nrepresentations directly in the encrypted domain, without decrypting them.\nNumerical results on large galleries of face, fingerprint, and object datasets\nsuch as ImageNet show that, for the first time, accurate and fast image search\nwithin the encrypted domain is feasible at scale (500 seconds; $275\\times$\nspeed up over state-of-the-art for encrypted search against a gallery of 100\nmillion).",
          "link": "http://arxiv.org/abs/2003.12197",
          "publishedOn": "2021-05-26T01:22:10.262Z",
          "wordCount": 640,
          "title": "HERS: Homomorphically Encrypted Representation Search. (arXiv:2003.12197v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Corujo_L/0/1/0/all/0/1\">Luis A. Corujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">Peter A. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kieson_E/0/1/0/all/0/1\">Emily Kieson</a>",
          "description": "Creating intelligent systems capable of recognizing emotions is a difficult\ntask, especially when looking at emotions in animals. This paper describes the\nprocess of designing a \"proof of concept\" system to recognize emotions in\nhorses. This system is formed by two elements, a detector and a model. The\ndetector is a faster region-based convolutional neural network that detects\nhorses in an image. The second one, the model, is a convolutional neural\nnetwork that predicts the emotion of those horses. These two models were\ntrained with multiple images of horses until they achieved high accuracy in\ntheir tasks, creating therefore the desired system. 400 images of horses were\nused to train both the detector and the model while 80 were used to validate\nthe system. Once the two components were validated they were combined into a\ntestable system that would detect equine emotions based on established\nbehavioral ethograms indicating emotional affect through head, neck, ear,\nmuzzle, and eye position. The system showed an accuracy of between 69% and 74%\non the validation set, demonstrating that it is possible to predict emotions in\nanimals using autonomous intelligent systems. It is a first \"proof of concept\"\napproach that can be enhanced in many ways. Such a system has multiple\napplications including further studies in the growing field of animal emotions\nas well as in the veterinary field to determine the physical welfare of horses\nor other livestock.",
          "link": "http://arxiv.org/abs/2105.11953",
          "publishedOn": "2021-05-26T01:22:10.256Z",
          "wordCount": 675,
          "title": "Emotion Recognition in Horses with Convolutional Neural Networks. (arXiv:2105.11953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zichao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>",
          "description": "Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes will be available.",
          "link": "http://arxiv.org/abs/2105.12085",
          "publishedOn": "2021-05-26T01:22:10.250Z",
          "wordCount": 636,
          "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.03955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_D/0/1/0/all/0/1\">Doyen Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Ke Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achananuparp_P/0/1/0/all/0/1\">Palakorn Achananuparp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-peng Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>",
          "description": "Food retrieval is an important task to perform analysis of food-related\ninformation, where we are interested in retrieving relevant information about\nthe queried food item such as ingredients, cooking instructions, etc. In this\npaper, we investigate cross-modal retrieval between food images and cooking\nrecipes. The goal is to learn an embedding of images and recipes in a common\nfeature space, such that the corresponding image-recipe embeddings lie close to\none another. Two major challenges in addressing this problem are 1) large\nintra-variance and small inter-variance across cross-modal food data; and 2)\ndifficulties in obtaining discriminative recipe representations. To address\nthese two problems, we propose Semantic-Consistent and Attention-based Networks\n(SCAN), which regularize the embeddings of the two modalities through aligning\noutput semantic probabilities. Besides, we exploit a self-attention mechanism\nto improve the embedding of recipes. We evaluate the performance of the\nproposed method on the large-scale Recipe1M dataset, and show that we can\noutperform several state-of-the-art cross-modal retrieval strategies for food\nimages and cooking recipes by a significant margin.",
          "link": "http://arxiv.org/abs/2003.03955",
          "publishedOn": "2021-05-26T01:22:10.244Z",
          "wordCount": 665,
          "title": "Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism. (arXiv:2003.03955v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Liyue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capaldi_D/0/1/0/all/0/1\">Dante Capaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>",
          "description": "Deep learning affords enormous opportunities to augment the armamentarium of\nbiomedical imaging, albeit its design and implementation have potential flaws.\nFundamentally, most deep learning models are driven entirely by data without\nconsideration of any prior knowledge, which dramatically increases the\ncomplexity of neural networks and limits the application scope and model\ngeneralizability. Here we establish a geometry-informed deep learning framework\nfor ultra-sparse 3D tomographic image reconstruction. We introduce a novel\nmechanism for integrating geometric priors of the imaging system. We\ndemonstrate that the seamless inclusion of known priors is essential to enhance\nthe performance of 3D volumetric computed tomography imaging with ultra-sparse\nsampling. The study opens new avenues for data-driven biomedical imaging and\npromises to provide substantially improved imaging tools for various clinical\nimaging and image-guided interventions.",
          "link": "http://arxiv.org/abs/2105.11692",
          "publishedOn": "2021-05-26T01:22:10.219Z",
          "wordCount": 574,
          "title": "A Geometry-Informed Deep Learning Framework for Ultra-Sparse 3D Tomographic Image Reconstruction. (arXiv:2105.11692v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yangting Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "Reinforcement learning (RL)-based neural architecture search (NAS) generally\nguarantees better convergence yet suffers from the requirement of huge\ncomputational resources compared with gradient-based approaches, due to the\nrollout bottleneck -- exhaustive training for each sampled generation on proxy\ntasks. In this paper, we propose a general pipeline to accelerate the\nconvergence of the rollout process as well as the RL process in NAS. It is\nmotivated by the interesting observation that both the architecture and the\nparameter knowledge can be transferred between different experiments and even\ndifferent tasks. We first introduce an uncertainty-aware critic (value\nfunction) in Proximal Policy Optimization (PPO) to utilize the architecture\nknowledge in previous experiments, which stabilizes the training process and\nreduces the searching time by 4 times. Further, an architecture knowledge pool\ntogether with a block similarity function is proposed to utilize parameter\nknowledge and reduces the searching time by 2 times. It is the first to\nintroduce block-level weight sharing in RLbased NAS. The block similarity\nfunction guarantees a 100% hitting ratio with strict fairness. Besides, we show\nthat a simply designed off-policy correction factor used in \"replay buffer\" in\nRL optimization can further reduce half of the searching time. Experiments on\nthe Mobile Neural Architecture Search (MNAS) search space show the proposed\nFast Neural Architecture Search (FNAS) accelerates standard RL-based NAS\nprocess by ~10x (e.g. ~256 2x2 TPUv2 x days / 20,000 GPU x hour -> 2,000 GPU x\nhour for MNAS), and guarantees better performance on various vision tasks.",
          "link": "http://arxiv.org/abs/2105.11694",
          "publishedOn": "2021-05-26T01:22:10.213Z",
          "wordCount": 684,
          "title": "FNAS: Uncertainty-Aware Fast Neural Architecture Search. (arXiv:2105.11694v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaigh_C/0/1/0/all/0/1\">Cheikh Brahim El Vaigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renoust_B/0/1/0/all/0/1\">Benjamin Renoust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1\">Hajime Nagahara</a>",
          "description": "The rise of digitization of cultural documents offers large-scale contents,\nopening the road for development of AI systems in order to preserve, search,\nand deliver cultural heritage. To organize such cultural content also means to\nclassify them, a task that is very familiar to modern computer science.\nContextual information is often the key to structure such real world data, and\nwe propose to use it in form of a knowledge graph. Such a knowledge graph,\ncombined with content analysis, enhances the notion of proximity between\nartworks so it improves the performances in classification tasks. In this\npaper, we propose a novel use of a knowledge graph, that is constructed on\nannotated data and pseudo-labeled data. With label propagation, we boost\nartwork classification by training a model using a graph convolutional network,\nrelying on the relationships between entities of the knowledge graph. Following\na transductive learning framework, our experiments show that relying on a\nknowledge graph modeling the relations between labeled data and unlabeled data\nallows to achieve state-of-the-art results on multiple classification tasks on\na dataset of paintings, and on a dataset of Buddha statues. Additionally, we\nshow state-of-the-art results for the difficult case of dealing with unbalanced\ndata, with the limitation of disregarding classes with extremely low degrees in\nthe knowledge graph.",
          "link": "http://arxiv.org/abs/2105.11852",
          "publishedOn": "2021-05-26T01:22:10.200Z",
          "wordCount": 654,
          "title": "GCNBoost: Artwork Classification by Label Propagation through a Knowledge Graph. (arXiv:2105.11852v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roh_H/0/1/0/all/0/1\">Hyungmin Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>",
          "description": "In this paper, we introduce a novel deep neural network suitable for\nmulti-scale analysis and propose efficient model-agnostic methods that help the\nnetwork extract information from high-frequency domains to reconstruct clearer\nimages. Our model can be applied to multi-scale image enhancement problems\nincluding denoising, deblurring and single image super-resolution. Experiments\non SIDD, Flickr2K, DIV2K, and REDS datasets show that our method achieves\nstate-of-the-art performance on each task. Furthermore, we show that our model\ncan overcome the over-smoothing problem commonly observed in existing\nPSNR-oriented methods and generate more natural high-resolution images by\napplying adversarial training.",
          "link": "http://arxiv.org/abs/2105.11711",
          "publishedOn": "2021-05-26T01:22:10.194Z",
          "wordCount": 520,
          "title": "High-Frequency aware Perceptual Image Enhancement. (arXiv:2105.11711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11765",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thebille_A/0/1/0/all/0/1\">Ann-Katrin Thebille</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dietrich_E/0/1/0/all/0/1\">Esther Dietrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klaus_M/0/1/0/all/0/1\">Martin Klaus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gernhold_L/0/1/0/all/0/1\">Lukas Gernhold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lennartz_M/0/1/0/all/0/1\">Maximilian Lennartz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuppe_C/0/1/0/all/0/1\">Christoph Kuppe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kramann_R/0/1/0/all/0/1\">Rafael Kramann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huber_T/0/1/0/all/0/1\">Tobias B. Huber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sauter_G/0/1/0/all/0/1\">Guido Sauter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puelles_V/0/1/0/all/0/1\">Victor G. Puelles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmermann_M/0/1/0/all/0/1\">Marina Zimmermann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonn_S/0/1/0/all/0/1\">Stefan Bonn</a>",
          "description": "The automated analysis of medical images is currently limited by technical\nand biological noise and bias. The same source tissue can be represented by\nvastly different images if the image acquisition or processing protocols vary.\nFor an image analysis pipeline, it is crucial to compensate such biases to\navoid misinterpretations. Here, we evaluate, compare, and improve existing\ngenerative model architectures to overcome domain shifts for immunofluorescence\n(IF) and Hematoxylin and Eosin (H&E) stained microscopy images. To determine\nthe performance of the generative models, the original and transformed images\nwere segmented or classified by deep neural networks that were trained only on\nimages of the target bias. In the scope of our analysis, U-Net cycleGANs\ntrained with an additional identity and an MS-SSIM-based loss and Fixed-Point\nGANs trained with an additional structure loss led to the best results for the\nIF and H&E stained samples, respectively. Adapting the bias of the samples\nsignificantly improved the pixel-level segmentation for human kidney glomeruli\nand podocytes and improved the classification accuracy for human prostate\nbiopsies by up to 14%.",
          "link": "http://arxiv.org/abs/2105.11765",
          "publishedOn": "2021-05-26T01:22:10.173Z",
          "wordCount": 651,
          "title": "Deep learning-based bias transfer for overcoming laboratory differences of microscopic images. (arXiv:2105.11765v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>",
          "description": "Light can undergo complex interactions with multiple scene surfaces of\ndifferent material types before being reflected towards a detector. During this\ntransport, every surface reflection and propagation is encoded in the\nproperties of the photons that ultimately reach the detector, including travel\ntime, direction, intensity, wavelength and polarization. Conventional imaging\nsystems capture intensity by integrating over all other dimensions of the light\ninto a single quantity, hiding this rich scene information in the accumulated\nmeasurements. Existing methods can untangle these into their spatial and\ntemporal dimensions, fueling geometric scene understanding. However, examining\npolarimetric material properties jointly with geometric properties is an open\nchallenge that could enable unprecedented capabilities beyond geometric\nunderstanding, allowing to incorporate material-dependent semantics. In this\nwork, we propose a computational light-transport imaging method that captures\nthe spatially- and temporally-resolved complete polarimetric response of a\nscene. Our method hinges on a novel 7D tensor theory of light transport. We\ndiscover low-rank structures in the polarimetric tensor dimension and propose a\ndata-driven rotating ellipsometry method that learns to exploit redundancy of\nthe polarimetric structures. We instantiate our theory in two imaging\nprototypes: spatio-polarimetric imaging and coaxial temporal-polarimetric\nimaging. This allows us to decompose scene light transport into temporal,\nspatial, and complete polarimetric dimensions that unveil scene properties\nhidden to conventional methods. We validate the applicability of our method on\ndiverse tasks, including shape reconstruction with subsurface scattering,\nseeing through scattering medium, untangling multi-bounce light transport,\nbreaking metamerism with polarization, and spatio-polarimetric decomposition of\ncrystals.",
          "link": "http://arxiv.org/abs/2105.11609",
          "publishedOn": "2021-05-26T01:22:10.165Z",
          "wordCount": 665,
          "title": "Polarimetric Spatio-Temporal Light Transport Probing. (arXiv:2105.11609v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velumani_K/0/1/0/all/0/1\">Kaaviya Velumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Lozano_R/0/1/0/all/0/1\">Raul Lopez-Lozano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madec_S/0/1/0/all/0/1\">Simon Madec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillet_J/0/1/0/all/0/1\">Joss Gillet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comar_A/0/1/0/all/0/1\">Alexis Comar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baret_F/0/1/0/all/0/1\">Frederic Baret</a>",
          "description": "Early-stage plant density is an essential trait that determines the fate of a\ngenotype under given environmental conditions and management practices. The use\nof RGB images taken from UAVs may replace traditional visual counting in fields\nwith improved throughput, accuracy and access to plant localization. However,\nhigh-resolution (HR) images are required to detect small plants present at\nearly stages. This study explores the impact of image ground sampling distance\n(GSD) on the performances of maize plant detection at 3-5 leaves stage using\nFaster-RCNN. Data collected at HR (GSD=0.3cm) over 6 contrasted sites were used\nfor model training. Two additional sites with images acquired both at high and\nlow (GSD=0.6cm) resolution were used for model evaluation. Results show that\nFaster-RCNN achieved very good plant detection and counting (rRMSE=0.08)\nperformances when native HR images are used both for training and validation.\nSimilarly, good performances were observed (rRMSE=0.11) when the model is\ntrained over synthetic low-resolution (LR) images obtained by down-sampling the\nnative training HR images, and applied to the synthetic LR validation images.\nConversely, poor performances are obtained when the model is trained on a given\nspatial resolution and applied to another spatial resolution. Training on a mix\nof HR and LR images allows to get very good performances on the native HR\n(rRMSE=0.06) and synthetic LR (rRMSE=0.10) images. However, very low\nperformances are still observed over the native LR images (rRMSE=0.48), mainly\ndue to the poor quality of the native LR images. Finally, an advanced\nsuper-resolution method based on GAN (generative adversarial network) that\nintroduces additional textural information derived from the native HR images\nwas applied to the native LR validation images. Results show some significant\nimprovement (rRMSE=0.22) compared to bicubic up-sampling approach.",
          "link": "http://arxiv.org/abs/2105.11857",
          "publishedOn": "2021-05-26T01:22:10.151Z",
          "wordCount": 741,
          "title": "Estimates of maize plant density from UAV RGB images using Faster-RCNN detection model: impact of the spatial resolution. (arXiv:2105.11857v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yucel_M/0/1/0/all/0/1\">Mehmet Kerim Yucel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimaridou_V/0/1/0/all/0/1\">Valia Dimaridou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drosou_A/0/1/0/all/0/1\">Anastasios Drosou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saa_Garriga_A/0/1/0/all/0/1\">Albert Sa&#xe0;-Garriga</a>",
          "description": "Monocular (relative or metric) depth estimation is a critical task for\nvarious applications, such as autonomous vehicles, augmented reality and image\nediting. In recent years, with the increasing availability of mobile devices,\naccurate and mobile-friendly depth models have gained importance. Increasingly\naccurate models typically require more computational resources, which inhibits\nthe use of such models on mobile devices. The mobile use case is arguably the\nmost unrestricted one, which requires highly accurate yet mobile-friendly\narchitectures. Therefore, we try to answer the following question: How can we\nimprove a model without adding further complexity (i.e. parameters)? Towards\nthis end, we systematically explore the design space of a relative depth\nestimation model from various dimensions and we show, with key design choices\nand ablation studies, even an existing architecture can reach highly\ncompetitive performance to the state of the art, with a fraction of the\ncomplexity. Our study spans an in-depth backbone model selection process,\nknowledge distillation, intermediate predictions, model pruning and loss\nrebalancing. We show that our model, using only DIW as the supervisory dataset,\nachieves 0.1156 WHDR on DIW with 2.6M parameters and reaches 37 FPS on a mobile\nGPU, without pruning or hardware-specific optimization. A pruned version of our\nmodel achieves 0.1208 WHDR on DIW with 1M parameters and reaches 44 FPS on a\nmobile GPU.",
          "link": "http://arxiv.org/abs/2105.12053",
          "publishedOn": "2021-05-26T01:22:10.142Z",
          "wordCount": 661,
          "title": "Real-time Monocular Depth Estimation with Sparse Supervision on Mobile. (arXiv:2105.12053v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Divam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_W/0/1/0/all/0/1\">Wei Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_T/0/1/0/all/0/1\">Trenton Tabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Jeff Schneider</a>",
          "description": "Accurate layout estimation is crucial for planning and navigation in robotics\napplications, such as self-driving. In this paper, we introduce the Stereo\nBird's Eye ViewNetwork (SBEVNet), a novel supervised end-to-end framework for\nestimation of bird's eye view layout from a pair of stereo images. Although our\nnetwork reuses some of the building blocks from the state-of-the-art deep\nlearning networks for disparity estimation, we show that explicit depth\nestimation is neither sufficient nor necessary. Instead, the learning of a good\ninternal bird's eye view feature representation is effective for layout\nestimation. Specifically, we first generate a disparity feature volume using\nthe features of the stereo images and then project it to the bird's eye view\ncoordinates. This gives us coarse-grained information about the scene\nstructure. We also apply inverse perspective mapping (IPM) to map the input\nimages and their features to the bird's eye view. This gives us fine-grained\ntexture information. Concatenating IPM features with the projected feature\nvolume creates a rich bird's eye view representation which is useful for\nspatial reasoning. We use this representation to estimate the BEV semantic map.\nAdditionally, we show that using the IPM features as a supervisory signal for\nstereo features can give an improvement in performance. We demonstrate our\napproach on two datasets:the KITTI dataset and a synthetically generated\ndataset from the CARLA simulator. For both of these datasets, we establish\nstate-of-the-art performance compared to baseline techniques.",
          "link": "http://arxiv.org/abs/2105.11705",
          "publishedOn": "2021-05-26T01:22:10.129Z",
          "wordCount": 658,
          "title": "SBEVNet: End-to-End Deep Stereo Layout Estimation. (arXiv:2105.11705v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiou_M/0/1/0/all/0/1\">Meng-Jiun Chiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chun-Yu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roger Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Detecting human-object interactions (HOI) is an important step toward a\ncomprehensive visual understanding of machines. While detecting non-temporal\nHOIs (e.g., sitting on a chair) from static images is feasible, it is unlikely\neven for humans to guess temporal-related HOIs (e.g., opening/closing a door)\nfrom a single video frame, where the neighboring frames play an essential role.\nHowever, conventional HOI methods operating on only static images have been\nused to predict temporal-related interactions, which is essentially guessing\nwithout temporal contexts and may lead to sub-optimal performance. In this\npaper, we bridge this gap by detecting video-based HOIs with explicit temporal\ninformation. We first show that a naive temporal-aware variant of a common\naction detection baseline does not work on video-based HOIs due to a\nfeature-inconsistency issue. We then propose a simple yet effective\narchitecture named Spatial-Temporal HOI Detection (ST-HOI) utilizing temporal\ninformation such as human and object trajectories, correctly-localized visual\nfeatures, and spatial-temporal masking pose features. We construct a new video\nHOI benchmark dubbed VidHOI where our proposed approach serves as a solid\nbaseline.",
          "link": "http://arxiv.org/abs/2105.11731",
          "publishedOn": "2021-05-26T01:22:10.081Z",
          "wordCount": 632,
          "title": "ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos. (arXiv:2105.11731v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1\">Pengfei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>",
          "description": "In this paper, we investigate a new variant of neural architecture search\n(NAS) paradigm -- searching with random labels (RLNAS). The task sounds\ncounter-intuitive for most existing NAS algorithms since random label provides\nfew information on the performance of each candidate architecture. Instead, we\npropose a novel NAS framework based on ease-of-convergence hypothesis, which\nrequires only random labels during searching. The algorithm involves two steps:\nfirst, we train a SuperNet using random labels; second, from the SuperNet we\nextract the sub-network whose weights change most significantly during the\ntraining. Extensive experiments are evaluated on multiple datasets (e.g.\nNAS-Bench-201 and ImageNet) and multiple search spaces (e.g. DARTS-like and\nMobileNet-like). Very surprisingly, RLNAS achieves comparable or even better\nresults compared with state-of-the-art NAS methods such as PC-DARTS, Single\nPath One-Shot, even though the counterparts utilize full ground truth labels\nfor searching. We hope our finding could inspire new understandings on the\nessential of NAS.",
          "link": "http://arxiv.org/abs/2101.11834",
          "publishedOn": "2021-05-26T01:22:10.014Z",
          "wordCount": null,
          "title": "Neural Architecture Search with Random Labels. (arXiv:2101.11834v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Juhua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>",
          "description": "Cluster discrimination is an effective pretext task for unsupervised\nrepresentation learning, which often consists of two phases: clustering and\ndiscrimination. Clustering is to assign each instance a pseudo label that will\nbe used to learn representations in discrimination. The main challenge resides\nin clustering since many prevalent clustering methods (e.g., k-means) have to\nrun in a batch mode that goes multiple iterations over the whole data.\nRecently, a balanced online clustering method, i.e., SwAV, is proposed for\nrepresentation learning. However, the assignment is optimized within only a\nsmall subset of data, which can be suboptimal. To address these challenges, we\nfirst investigate the objective of clustering-based representation learning\nfrom the perspective of distance metric learning. Based on this, we propose a\nnovel clustering-based pretext task with online \\textbf{Co}nstrained\n\\textbf{K}-m\\textbf{e}ans (\\textbf{CoKe}) to learn representations and\nrelations between instances simultaneously. Compared with the balanced\nclustering that each cluster has exactly the same size, we only constrain the\nminimum size of clusters to flexibly capture the inherent data structure. More\nimportantly, our online assignment method has a theoretical guarantee to\napproach the global optimum. Finally, two variance reduction strategies are\nproposed to make the clustering robust for different augmentations. Without\nkeeping representations of instances, the data is accessed in an online mode in\nCoKe while a single view of instances at each iteration is sufficient to\ndemonstrate a better performance than contrastive learning methods relying on\ntwo views. Extensive experiments on ImageNet verify the efficacy of our\nproposal. Code will be released.",
          "link": "http://arxiv.org/abs/2105.11527",
          "publishedOn": "2021-05-26T01:22:09.934Z",
          "wordCount": null,
          "title": "Unsupervised Visual Representation Learning by Online Constrained K-Means. (arXiv:2105.11527v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dehui Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaonan Luo</a>",
          "description": "Zero-shot action recognition can recognize samples of unseen classes that are\nunavailable in training by exploring common latent semantic representation in\nsamples. However, most methods neglected the connotative relation and\nextensional relation between the action classes, which leads to the poor\ngeneralization ability of the zero-shot learning. Furthermore, the learned\nclassifier incline to predict the samples of seen class, which leads to poor\nclassification performance. To solve the above problems, we propose a two-stage\ndeep neural network for zero-shot action recognition, which consists of a\nfeature generation sub-network serving as the sampling stage and a graph\nattention sub-network serving as the classification stage. In the sampling\nstage, we utilize a generative adversarial networks (GAN) trained by action\nfeatures and word vectors of seen classes to synthesize the action features of\nunseen classes, which can balance the training sample data of seen classes and\nunseen classes. In the classification stage, we construct a knowledge graph\n(KG) based on the relationship between word vectors of action classes and\nrelated objects, and propose a graph convolution network (GCN) based on\nattention mechanism, which dynamically updates the relationship between action\nclasses and objects, and enhances the generalization ability of zero-shot\nlearning. In both stages, we all use word vectors as bridges for feature\ngeneration and classifier generalization from seen classes to unseen classes.\nWe compare our method with state-of-the-art methods on UCF101 and HMDB51\ndatasets. Experimental results show that our proposed method improves the\nclassification performance of the trained classifier and achieves higher\naccuracy.",
          "link": "http://arxiv.org/abs/2105.11789",
          "publishedOn": "2021-05-26T01:22:09.822Z",
          "wordCount": 701,
          "title": "GAN for Vision, KG for Relation: a Two-stage Deep Network for Zero-shot Action Recognition. (arXiv:2105.11789v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tian-Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuan-Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>",
          "description": "Place recognition plays an essential role in the field of autonomous driving\nand robot navigation. Although a number of point cloud based methods have been\nproposed and achieved promising results, few of them take the size difference\nof objects into consideration. For small objects like pedestrians and vehicles,\nlarge receptive fields will capture unrelated information, while small\nreceptive fields would fail to encode complete geometric information for large\nobjects such as buildings. We argue that fixed receptive fields are not well\nsuited for place recognition, and propose a novel Adaptive Receptive Field\nModule (ARFM), which can adaptively adjust the size of the receptive field\nbased on the input point cloud. We also present a novel network architecture,\nnamed TransLoc3D, to obtain discriminative global descriptors of point clouds\nfor the place recognition task. TransLoc3D consists of a 3D sparse\nconvolutional module, an ARFM module, an external transformer network which\naims to capture long range dependency and a NetVLAD layer. Experiments show\nthat our method outperforms prior state-of-the-art results, with an improvement\nof 1.1\\% on average recall@1 on the Oxford RobotCar dataset, and 0.8\\% on the\nB.D. dataset.",
          "link": "http://arxiv.org/abs/2105.11605",
          "publishedOn": "2021-05-26T01:22:09.795Z",
          "wordCount": 624,
          "title": "TransLoc3D : Point Cloud based Large-scale Place Recognition using Adaptive Receptive Fields. (arXiv:2105.11605v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jianhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>",
          "description": "Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural\nnetworks, have attracted great attentions from researchers and industry. The\nmost efficient way to train deep SNNs is through ANN-SNN conversion. However,\nthe conversion usually suffers from accuracy loss and long inference time,\nwhich impede the practical application of SNN. In this paper, we theoretically\nanalyze ANN-SNN conversion and derive sufficient conditions of the optimal\nconversion. To better correlate ANN-SNN and get greater accuracy, we propose\nRate Norm Layer to replace the ReLU activation function in source ANN training,\nenabling direct conversion from a trained ANN to an SNN. Moreover, we propose\nan optimal fit curve to quantify the fit between the activation value of source\nANN and the actual firing rate of target SNN. We show that the inference time\ncan be reduced by optimizing the upper bound of the fit curve in the revised\nANN to achieve fast inference. Our theory can explain the existing work on fast\nreasoning and get better results. The experimental results show that the\nproposed method achieves near loss less conversion with VGG-16,\nPreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster\nreasoning performance under 0.265x energy consumption of the typical method.\nThe code is available at\nhttps://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.",
          "link": "http://arxiv.org/abs/2105.11654",
          "publishedOn": "2021-05-26T01:22:09.789Z",
          "wordCount": 679,
          "title": "Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep Spiking Neural Networks. (arXiv:2105.11654v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1\">Tao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruhui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1\">Haibing Guan</a>",
          "description": "In this paper, we propose an effective method for fast and accurate scene\nparsing called Bidirectional Alignment Network (BiAlignNet). Previously, one\nrepresentative work BiSeNet~\\cite{bisenet} uses two different paths (Context\nPath and Spatial Path) to achieve balanced learning of semantics and details,\nrespectively. However, the relationship between the two paths is not well\nexplored. We argue that both paths can benefit each other in a complementary\nway. Motivated by this, we propose a novel network by aligning two-path\ninformation into each other through a learned flow field. To avoid the noise\nand semantic gaps, we introduce a Gated Flow Alignment Module to align both\nfeatures in a bidirectional way. Moreover, to make the Spatial Path learn more\ndetailed information, we present an edge-guided hard pixel mining loss to\nsupervise the aligned learning process. Our method achieves 80.1\\% and 78.5\\%\nmIoU in validation and test set of Cityscapes while running at 30 FPS with full\nresolution inputs. Code and models will be available at\n\\url{https://github.com/jojacola/BiAlignNet}.",
          "link": "http://arxiv.org/abs/2105.11651",
          "publishedOn": "2021-05-26T01:22:09.771Z",
          "wordCount": 608,
          "title": "Fast and Accurate Scene Parsing via Bi-direction Alignment Networks. (arXiv:2105.11651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhengjun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_L/0/1/0/all/0/1\">Lubin Weng</a>",
          "description": "We propose a novel method for fine-grained high-quality image segmentation of\nboth objects and scenes. Inspired by dilation and erosion from morphological\nimage processing techniques, we treat the pixel level segmentation problems as\nsqueezing object boundary. From this perspective, we propose \\textbf{Boundary\nSqueeze} module: a novel and efficient module that squeezes the object boundary\nfrom both inner and outer directions which leads to precise mask\nrepresentation. To generate such squeezed representation, we propose a new\nbidirectionally flow-based warping process and design specific loss signals to\nsupervise the learning process. Boundary Squeeze Module can be easily applied\nto both instance and semantic segmentation tasks as a plug-and-play module by\nbuilding on top of existing models. We show that our simple yet effective\ndesign can lead to high qualitative results on several different datasets and\nwe also provide several different metrics on boundary to prove the\neffectiveness over previous work. Moreover, the proposed module is\nlight-weighted and thus has potential for practical usage. Our method yields\nlarge gains on COCO, Cityscapes, for both instance and semantic segmentation\nand outperforms previous state-of-the-art PointRend in both accuracy and speed\nunder the same setting. Code and model will be available.",
          "link": "http://arxiv.org/abs/2105.11668",
          "publishedOn": "2021-05-26T01:22:09.755Z",
          "wordCount": 628,
          "title": "BoundarySqueeze: Image Segmentation as Boundary Squeezing. (arXiv:2105.11668v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Archibald_T/0/1/0/all/0/1\">Taylor Archibald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggemann_M/0/1/0/all/0/1\">Mason Poggemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_T/0/1/0/all/0/1\">Tony Martinez</a>",
          "description": "Stroke order and velocity are helpful features in the fields of signature\nverification, handwriting recognition, and handwriting synthesis. Recovering\nthese features from offline handwritten text is a challenging and well-studied\nproblem. We propose a new model called TRACE (Trajectory Recovery by an\nAdaptively-trained Convolutional Encoder). TRACE is a differentiable approach\nthat uses a convolutional recurrent neural network (CRNN) to infer temporal\nstroke information from long lines of offline handwritten text with many\ncharacters and dynamic time warping (DTW) to align predictions and ground truth\npoints. TRACE is perhaps the first system to be trained end-to-end on entire\nlines of text of arbitrary width and does not require the use of dynamic\nexemplars. Moreover, the system does not require images to undergo any\npre-processing, nor do the predictions require any post-processing.\nConsequently, the recovered trajectory is differentiable and can be used as a\nloss function for other tasks, including synthesizing offline handwritten text.\n\nWe demonstrate that temporal stroke information recovered by TRACE from\noffline data can be used for handwriting synthesis and establish the first\nbenchmarks for a stroke trajectory recovery system trained on the IAM online\nhandwriting dataset.",
          "link": "http://arxiv.org/abs/2105.11559",
          "publishedOn": "2021-05-26T01:22:09.743Z",
          "wordCount": 650,
          "title": "TRACE: A Differentiable Approach to Line-level Stroke Recovery for Offline Handwritten Text. (arXiv:2105.11559v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>",
          "description": "Most existing deep learning-based pan-sharpening methods have several widely\nrecognized issues, such as spectral distortion and insufficient spatial texture\nenhancement, we propose a novel pan-sharpening convolutional neural network\nbased on a high-pass modification block. Different from existing methods, the\nproposed block is designed to learn the high-pass information, leading to\nenhance spatial information in each band of the multi-spectral-resolution\nimages. To facilitate the generation of visually appealing pan-sharpened\nimages, we propose a perceptual loss function and further optimize the model\nbased on high-level features in the near-infrared space. Experiments\ndemonstrate the superior performance of the proposed method compared to the\nstate-of-the-art pan-sharpening methods, both quantitatively and qualitatively.\nThe proposed model is open-sourced at https://github.com/jiaming-wang/HMB.",
          "link": "http://arxiv.org/abs/2105.11576",
          "publishedOn": "2021-05-26T01:22:09.730Z",
          "wordCount": 575,
          "title": "Pan-sharpening via High-pass Modification Convolutional Neural Network. (arXiv:2105.11576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">S. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">L. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">J. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>",
          "description": "The Graph Neural Network (GNN) has achieved remarkable success in graph data\nrepresentation. However, the previous work only considered the ideal balanced\ndataset, and the practical imbalanced dataset was rarely considered, which, on\nthe contrary, is of more significance for the application of GNN. Traditional\nmethods such as resampling, reweighting and synthetic samples that deal with\nimbalanced datasets are no longer applicable in GNN. Ensemble models can handle\nimbalanced datasets better compared with single estimator. Besides, ensemble\nlearning can achieve higher estimation accuracy and has better reliability\ncompared with the single estimator. In this paper, we propose an ensemble model\ncalled AdaGCN, which uses a Graph Convolutional Network (GCN) as the base\nestimator during adaptive boosting. In AdaGCN, a higher weight will be set for\nthe training samples that are not properly classified by the previous\nclassifier, and transfer learning is used to reduce computational cost and\nincrease fitting capability. Experiments show that the AdaGCN model we proposed\nachieves better performance than GCN, GraphSAGE, GAT, N-GCN and the most of\nadvanced reweighting and resampling methods on synthetic imbalanced datasets,\nwith an average improvement of 4.3%. Our model also improves state-of-the-art\nbaselines on all of the challenging node classification tasks we consider:\nCora, Citeseer, Pubmed, and NELL.",
          "link": "http://arxiv.org/abs/2105.11625",
          "publishedOn": "2021-05-26T01:22:09.701Z",
          "wordCount": 656,
          "title": "AdaGCN:Adaptive Boosting Algorithm for Graph Convolutional Networks on Imbalanced Node Classification. (arXiv:2105.11625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zins_M/0/1/0/all/0/1\">Matthieu Zins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_G/0/1/0/all/0/1\">Gilles Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1\">Marie-Odile Berger</a>",
          "description": "In this paper, we propose a method for coarse camera pose computation which\nis robust to viewing conditions and does not require a detailed model of the\nscene. This method meets the growing need of easy deployment of robotics or\naugmented reality applications in any environments, especially those for which\nno accurate 3D model nor huge amount of ground truth data are available. It\nexploits the ability of deep learning techniques to reliably detect objects\nregardless of viewing conditions. Previous works have also shown that\nabstracting the geometry of a scene of objects by an ellipsoid cloud allows to\ncompute the camera pose accurately enough for various application needs. Though\npromising, these approaches use the ellipses fitted to the detection bounding\nboxes as an approximation of the imaged objects. In this paper, we go one step\nfurther and propose a learning-based method which detects improved elliptic\napproximations of objects which are coherent with the 3D ellipsoid in terms of\nperspective projection. Experiments prove that the accuracy of the computed\npose significantly increases thanks to our method and is more robust to the\nvariability of the boundaries of the detection boxes. This is achieved with\nvery little effort in terms of training data acquisition -- a few hundred\ncalibrated images of which only three need manual object annotation. Code and\nmodels are released at\nhttps://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization.",
          "link": "http://arxiv.org/abs/2105.11494",
          "publishedOn": "2021-05-26T01:22:09.665Z",
          "wordCount": 678,
          "title": "3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation. (arXiv:2105.11494v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11486",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nalwade_A/0/1/0/all/0/1\">Ashwin Nalwade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kisa_J/0/1/0/all/0/1\">Jackie Kisa</a>",
          "description": "Multi-modal magnetic resonance imaging (MRI) is a crucial method for\nanalyzing the human brain. It is usually used for diagnosing diseases and for\nmaking valuable decisions regarding the treatments - for instance, checking for\ngliomas in the human brain. With varying degrees of severity and detection,\nproperly diagnosing gliomas is one of the most daunting and significant\nanalysis tasks in modern-day medicine. Our primary focus is on working with\ndifferent approaches to perform the segmentation of brain tumors in multimodal\nMRI scans. Now, the quantity, variability of the data used for training has\nalways been considered to be crucial for developing excellent models. Hence, we\nalso want to experiment with Knowledge Distillation techniques.",
          "link": "http://arxiv.org/abs/2105.11486",
          "publishedOn": "2021-05-26T01:22:09.591Z",
          "wordCount": 556,
          "title": "Experimenting with Knowledge Distillation techniques for performing Brain Tumor Segmentation. (arXiv:2105.11486v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1\">Tao Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1\">Qing Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>",
          "description": "GuessWhat?! is a two-player visual dialog guessing game where player A asks a\nsequence of yes/no questions (Questioner) and makes a final guess (Guesser)\nabout a target object in an image, based on answers from player B (Oracle).\nBased on this dialog history between the Questioner and the Oracle, a Guesser\nmakes a final guess of the target object. Previous baseline Oracle model\nencodes no visual information in the model, and it cannot fully understand\ncomplex questions about color, shape, relationships and so on. Most existing\nwork for Guesser encode the dialog history as a whole and train the Guesser\nmodels from scratch on the GuessWhat?! dataset. This is problematic since\nlanguage encoder tend to forget long-term history and the GuessWhat?! data is\nsparse in terms of learning visual grounding of objects. Previous work for\nQuestioner introduces state tracking mechanism into the model, but it is\nlearned as a soft intermediates without any prior vision-linguistic insights.\nTo bridge these gaps, in this paper we propose Vilbert-based Oracle, Guesser\nand Questioner, which are all built on top of pretrained vision-linguistic\nmodel, Vilbert. We introduce two-way background/target fusion mechanism into\nVilbert-Oracle to account for both intra and inter-object questions. We propose\na unified framework for Vilbert-Guesser and Vilbert-Questioner, where\nstate-estimator is introduced to best utilize Vilbert's power on single-turn\nreferring expression comprehension. Experimental results show that our proposed\nmodels outperform state-of-the-art models significantly by 7%, 10%, 12% for\nOracle, Guesser and End-to-End Questioner respectively.",
          "link": "http://arxiv.org/abs/2105.11541",
          "publishedOn": "2021-05-26T01:22:09.546Z",
          "wordCount": 675,
          "title": "Learning Better Visual Dialog Agents with Pretrained Visual-Linguistic Representation. (arXiv:2105.11541v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>",
          "description": "Salient human detection (SHD) in dynamic 360{\\deg} immersive videos is of\ngreat importance for various applications such as robotics, inter-human and\nhuman-object interaction in augmented reality. However, 360{\\deg} video SHD has\nbeen seldom discussed in the computer vision community due to a lack of\ndatasets with large-scale omnidirectional videos and rich annotations. To this\nend, we propose SHD360, the first 360{\\deg} video SHD dataset collecting\nvarious real-life daily scenes, providing six-level hierarchical annotations\nfor 6,268 key frames uniformly sampled from 37,403 omnidirectional video frames\nat 4K resolution. Specifically, each collected key frame is labeled with a\nsuper-class, a sub-class, associated attributes (e.g., geometrical distortion),\nbounding boxes and per-pixel object-/instance-level masks. As a result, our\nSHD360 contains totally 16,238 salient human instances with manually annotated\npixel-wise ground truth. Since so far there is no method proposed for 360{\\deg}\nSHD, we systematically benchmark 11 representative state-of-the-art salient\nobject detection (SOD) approaches on our SHD360, and explore key issues derived\nfrom extensive experimenting results. We hope our proposed dataset and\nbenchmark could serve as a good starting point for advancing human-centric\nresearches towards 360{\\deg} panoramic data. Our dataset and benchmark will be\npublicly available at https://github.com/PanoAsh/SHD360.",
          "link": "http://arxiv.org/abs/2105.11578",
          "publishedOn": "2021-05-26T01:22:09.507Z",
          "wordCount": 649,
          "title": "SHD360: A Benchmark Dataset for Salient Human Detection in 360{\\deg} Videos. (arXiv:2105.11578v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan T&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-T&#xfc;r</a>",
          "description": "Interactive robots navigating photo-realistic environments face challenges\nunderlying vision-and-language navigation (VLN), but in addition, they need to\nbe trained to handle the dynamic nature of dialogue. However, research in\nCooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts\nwith a guide in natural language in order to reach a goal, treats the dialogue\nhistory as a VLN-style static instruction. In this paper, we present VISITRON,\na navigator better suited to the interactive regime inherent to CVDN by being\ntrained to: i) identify and associate object-level concepts and semantics\nbetween the environment and dialogue history, ii) identify when to interact vs.\nnavigate via imitation learning of a binary classification head. We perform\nextensive ablations with VISITRON to gain empirical insights and improve\nperformance on CVDN. VISITRON is competitive with models on the static CVDN\nleaderboard. We also propose a generalized interactive regime to fine-tune and\nevaluate VISITRON and future such models with pre-trained guides for\nadaptability.",
          "link": "http://arxiv.org/abs/2105.11589",
          "publishedOn": "2021-05-26T01:22:09.481Z",
          "wordCount": 617,
          "title": "VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator. (arXiv:2105.11589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Suprateek Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_J/0/1/0/all/0/1\">Jennifer S. Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fani_N/0/1/0/all/0/1\">Negar Fani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Anuj Srivastava</a>",
          "description": "There is increasing evidence on the importance of brain morphology in\npredicting and classifying mental disorders. However, the vast majority of\ncurrent shape approaches rely heavily on vertex-wise analysis that may not\nsuccessfully capture complexities of subcortical structures. Additionally, the\npast works do not include interactions between these structures and exposure\nfactors. Predictive modeling with such interactions is of paramount interest in\nheterogeneous mental disorders such as PTSD, where trauma exposure interacts\nwith brain shape changes to influence behavior. We propose a comprehensive\nframework that overcomes these limitations by representing brain substructures\nas continuous parameterized surfaces and quantifying their shape differences\nusing elastic shape metrics. Using the elastic shape metric, we compute shape\nsummaries of subcortical data and represent individual shapes by their\nprincipal scores. These representations allow visualization tools that help\nlocalize changes when these PCs are varied. Subsequently, these PCs, the\nauxiliary exposure variables, and their interactions are used for regression\nmodeling. We apply our method to data from the Grady Trauma Project, where the\ngoal is to predict clinical measures of PTSD using shapes of brain\nsubstructures. Our analysis revealed considerably greater predictive power\nunder the elastic shape analysis than widely used approaches such as\nvertex-wise shape analysis and even volumetric analysis. It helped identify\nlocal deformations in brain shapes related to change in PTSD severity. To our\nknowledge, this is one of the first brain shape analysis approaches that can\nseamlessly integrate the pre-processing steps under one umbrella for improved\naccuracy and are naturally able to account for interactions between brain shape\nand additional covariates to yield superior predictive performance when\nmodeling clinical outcomes.",
          "link": "http://arxiv.org/abs/2105.11547",
          "publishedOn": "2021-05-26T01:22:09.444Z",
          "wordCount": 724,
          "title": "Elastic Shape Analysis of Brain Structures for Predictive Modeling of PTSD. (arXiv:2105.11547v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sheng Chen</a>",
          "description": "Interactive single-image segmentation is ubiquitous in the scientific and\ncommercial imaging software. In this work, we focus on the single-image\nsegmentation problem only with some seeds such as scribbles. Inspired by the\ndynamic receptive field in the human being's visual system, we propose the\nGaussian dynamic convolution (GDC) to fast and efficiently aggregate the\ncontextual information for neural networks. The core idea is randomly selecting\nthe spatial sampling area according to the Gaussian distribution offsets. Our\nGDC can be easily used as a module to build lightweight or complex segmentation\nnetworks. We adopt the proposed GDC to address the typical single-image\nsegmentation tasks. Furthermore, we also build a Gaussian dynamic pyramid\nPooling to show its potential and generality in common semantic segmentation.\nExperiments demonstrate that the GDC outperforms other existing convolutions on\nthree benchmark segmentation datasets including Pascal-Context, Pascal-VOC\n2012, and Cityscapes. Additional experiments are also conducted to illustrate\nthat the GDC can produce richer and more vivid features compared with other\nconvolutions. In general, our GDC is conducive to the convolutional neural\nnetworks to form an overall impression of the image.",
          "link": "http://arxiv.org/abs/2104.08783",
          "publishedOn": "2021-05-25T01:56:12.706Z",
          "wordCount": 645,
          "title": "Gaussian Dynamic Convolution for Efficient Single-Image Segmentation. (arXiv:2104.08783v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10949",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenfeng Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiaming Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Sihang Zhang</a>",
          "description": "Hyperspectral images (HSIs) have been widely used in a variety of\napplications thanks to the rich spectral information they are able to provide.\nAmong all HSI processing tasks, HSI denoising is a crucial step. Recently, deep\nlearning-based image denoising methods have made great progress and achieved\ngreat performance. However, existing methods tend to ignore the correlations\nbetween adjacent spectral bands, leading to problems such as spectral\ndistortion and blurred edges in denoised results. In this study, we propose a\nnovel HSI denoising network, termed SSCAN, that combines group convolutions and\nattention modules. Specifically, we use a group convolution with a spatial\nattention module to facilitate feature extraction by directing models'\nattention to band-wise important features. We propose a spectral-spatial\nattention block (SSAB) to exploit the spatial and spectral information in\nhyperspectral images in an effective manner. In addition, we adopt residual\nlearning operations with skip connections to ensure training stability. The\nexperimental results indicate that the proposed SSCAN outperforms several\nstate-of-the-art HSI denoising algorithms.",
          "link": "http://arxiv.org/abs/2105.10949",
          "publishedOn": "2021-05-25T01:56:12.688Z",
          "wordCount": 621,
          "title": "SSCAN: A Spatial-spectral Cross Attention Network for Hyperspectral Image Denoising. (arXiv:2105.10949v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11432",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Samuel_D/0/1/0/all/0/1\">Dinesh Jackson Samuel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baskaran_R/0/1/0/all/0/1\">Rajesh Kanna Baskaran</a>",
          "description": "Tuberculosis is a contagious disease which is one of the leading causes of\ndeath, globally. The general diagnosis methods for tuberculosis include\nmicroscopic examination, tuberculin skin test, culture method, enzyme linked\nimmunosorbent assay (ELISA) and electronic nose system. World Health\nOrganization (WHO) recommends standard microscopic examination for early\ndiagnosis of tuberculosis. In microscopy, the technician examines field of\nviews (FOVs) in sputum smear for presence of any TB bacilli and counts the\nnumber of TB bacilli per FOV to report the level of severity. This process is\ntime consuming with an increased concentration for an experienced staff to\nexamine a single sputum smear. The examination demands for skilled technicians\nin high-prevalence countries which may lead to overload, fatigue and diminishes\nthe quality of microscopy. Thus, a computer assisted system is proposed and\ndesigned for the detection of tuberculosis bacilli to assist pathologists with\nincreased sensitivity and specificity. The manual efforts in detecting and\ncounting the number of TB bacilli is greatly minimized. The system obtains\nZiehl-Neelsen stained microscopic images from conventional microscope at 100x\nmagnification and passes the data to the detection system. Initially the\nsegmentation of TB bacilli was done using RGB thresholding and Sauvola's\nadaptive thresholding algorithm. To eliminate the non-TB bacilli from coarse\nlevel segmentation, shape descriptors like area, perimeter, convex hull, major\naxis length and eccentricity are used to extract only the TB bacilli features.\nFinally, the TB bacilli are counted using the generated bounding boxes to\nreport the level of severity.",
          "link": "http://arxiv.org/abs/2105.11432",
          "publishedOn": "2021-05-25T01:56:12.674Z",
          "wordCount": 687,
          "title": "Design to automate the detection and counting of Tuberculosis(TB) bacilli. (arXiv:2105.11432v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06544",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chuanlong Li</a>",
          "description": "Cerebrovascular accident, or commonly known as stroke, is an acute disease\nwith extreme impact on patients and healthcare systems and is the second\nlargest cause of death worldwide. Fast and precise stroke lesion detection and\nlocation is an extreme important process with regards to stroke diagnosis,\ntreatment, and prognosis. Except from the manual segmentation approach, machine\nlearning based segmentation methods are the most promising ones when\nconsidering efficiency and accuracy, and convolutional neural network based\nmodels are the first of its kind. However, most of these neural network models\ndo not really align with the brain anatomical structures. Intuitively, this\nwork presents a more brain alike model which mimics the anatomical structure of\nthe human visual cortex. Through the preliminary experiments on the stroke\nlesion segmentation task, the proposed model is found to be able to perform\nequally well or better to the de-facto standard U-Net. Part of the\nimplementation will be made available at https://github.com/DarkoBomer/VCA-Net.",
          "link": "http://arxiv.org/abs/2105.06544",
          "publishedOn": "2021-05-25T01:56:12.668Z",
          "wordCount": 630,
          "title": "Stroke Lesion Segmentation with Visual Cortex Anatomy Alike Neural Nets. (arXiv:2105.06544v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1\">Quentin Paletta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbod_G/0/1/0/all/0/1\">Guillaume Arbod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>",
          "description": "A number of industrial applications, such as smart grids, power plant\noperation, hybrid system management or energy trading, could benefit from\nimproved short-term solar forecasting, addressing the intermittent energy\nproduction from solar panels. However, current approaches to modelling the\ncloud cover dynamics from sky images still lack precision regarding the spatial\nconfiguration of clouds, their temporal dynamics and physical interactions with\nsolar radiation. Benefiting from a growing number of large datasets, data\ndriven methods are being developed to address these limitations with promising\nresults. In this study, we compare four commonly used Deep Learning\narchitectures trained to forecast solar irradiance from sequences of\nhemispherical sky images and exogenous variables. To assess the relative\nperformance of each model, we used the Forecast Skill metric based on the smart\npersistence model, as well as ramp and time distortion metrics. The results\nshow that encoding spatiotemporal aspects of the sequence of sky images greatly\nimproved the predictions with 10 min ahead Forecast Skill reaching 20.4% on the\ntest year. However, based on the experimental data, we conclude that, with a\ncommon setup, Deep Learning models tend to behave just as a 'very smart\npersistence model', temporally aligned with the persistence model while\nmitigating its most penalising errors. Thus, despite being captured by the sky\ncameras, models often miss fundamental events causing large irradiance changes\nsuch as clouds obscuring the sun. We hope that our work will contribute to a\nshift of this approach to irradiance forecasting, from reactive to\nanticipatory.",
          "link": "http://arxiv.org/abs/2102.00721",
          "publishedOn": "2021-05-25T01:56:12.661Z",
          "wordCount": 743,
          "title": "Benchmarking of Deep Learning Irradiance Forecasting Models from Sky Images -- an in-depth Analysis. (arXiv:2102.00721v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.11853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cassidy_B/0/1/0/all/0/1\">Bill Cassidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reeves_N/0/1/0/all/0/1\">Neil D. Reeves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_P/0/1/0/all/0/1\">Pappachan Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillespie_D/0/1/0/all/0/1\">David Gillespie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OShea_C/0/1/0/all/0/1\">Claire O&#x27;Shea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Satyan Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun G. Maiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulton_A/0/1/0/all/0/1\">Andrew Boulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armstrong_D/0/1/0/all/0/1\">David Armstrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafi_B/0/1/0/all/0/1\">Bijan Najafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Justina Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>",
          "description": "Every 20 seconds, a limb is amputated somewhere in the world due to diabetes.\nThis is a global health problem that requires a global solution. The MICCAI\nchallenge discussed in this paper, which concerns the automated detection of\ndiabetic foot ulcers using machine learning techniques, will accelerate the\ndevelopment of innovative healthcare technology to address this unmet medical\nneed. In an effort to improve patient care and reduce the strain on healthcare\nsystems, recent research has focused on the creation of cloud-based detection\nalgorithms. These can be consumed as a service by a mobile app that patients\n(or a carer, partner or family member) could use themselves at home to monitor\ntheir condition and to detect the appearance of a diabetic foot ulcer (DFU).\nCollaborative work between Manchester Metropolitan University, Lancashire\nTeaching Hospital and the Manchester University NHS Foundation Trust has\ncreated a repository of 4,000 DFU images for the purpose of supporting research\ntoward more advanced methods of DFU detection. Based on a joint effort\ninvolving the lead scientists of the UK, US, India and New Zealand, this\nchallenge will solicit original work, and promote interactions between\nresearchers and interdisciplinary collaborations. This paper presents a dataset\ndescription and analysis, assessment methods, benchmark algorithms and initial\nevaluation results. It facilitates the challenge by providing useful insights\ninto state-of-the-art and ongoing research. This grand challenge takes on even\ngreater urgency in a peri and post-pandemic period, where stresses on resource\nutilization will increase the need for technology that allows people to remain\nactive, healthy and intact in their home.",
          "link": "http://arxiv.org/abs/2004.11853",
          "publishedOn": "2021-05-25T01:56:12.655Z",
          "wordCount": 762,
          "title": "DFUC2020: Analysis Towards Diabetic Foot Ulcer Detection. (arXiv:2004.11853v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Needell_C/0/1/0/all/0/1\">Coen D. Needell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bainbridge_W/0/1/0/all/0/1\">Wilma A. Bainbridge</a>",
          "description": "Various work has suggested that the memorability of an image is consistent\nacross people, and thus can be treated as an intrinsic property of an image.\nUsing computer vision models, we can make specific predictions about what\npeople will remember or forget. While older work has used now-outdated deep\nlearning architectures to predict image memorability, innovations in the field\nhave given us new techniques to apply to this problem. Here, we propose and\nevaluate five alternative deep learning models which exploit developments in\nthe field from the last five years, largely the introduction of residual neural\nnetworks, which are intended to allow the model to use semantic information in\nthe memorability estimation process. These new models were tested against the\nprior state of the art with a combined dataset built to optimize both\nwithin-category and across-category predictions. Our findings suggest that the\nkey prior memorability network had overstated its generalizability and was\noverfit on its training set. Our new models outperform this prior model,\nleading us to conclude that Residual Networks outperform simpler convolutional\nneural networks in memorability regression. We make our new state-of-the-art\nmodel readily available to the research community, allowing memory researchers\nto make predictions about memorability on a wider range of images.",
          "link": "http://arxiv.org/abs/2105.10598",
          "publishedOn": "2021-05-25T01:56:12.640Z",
          "wordCount": 663,
          "title": "Embracing New Techniques in Deep Learning for Estimating Image Memorability. (arXiv:2105.10598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shomron_G/0/1/0/all/0/1\">Gil Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_F/0/1/0/all/0/1\">Freddy Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurzum_S/0/1/0/all/0/1\">Samer Kurzum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiser_U/0/1/0/all/0/1\">Uri Weiser</a>",
          "description": "Quantization is a technique used in deep neural networks (DNNs) to increase\nexecution performance and hardware efficiency. Uniform post-training\nquantization (PTQ) methods are common, since they can be implemented\nefficiently in hardware and do not require extensive hardware resources or a\ntraining set. Mapping FP32 models to INT8 using uniform PTQ yields models with\nnegligible accuracy degradation; however, reducing precision below 8 bits with\nPTQ is challenging, as accuracy degradation becomes noticeable, due to the\nincrease in quantization noise. In this paper, we propose a sparsity-aware\nquantization (SPARQ) method, in which the unstructured and dynamic activation\nsparsity is leveraged in different representation granularities. 4-bit\nquantization, for example, is employed by dynamically examining the bits of\n8-bit values and choosing a window of 4 bits, while first skipping zero-value\nbits. Moreover, instead of quantizing activation-by-activation to 4 bits, we\nfocus on pairs of 8-bit activations and examine whether one of the two is equal\nto zero. If one is equal to zero, the second can opportunistically use the\nother's 4-bit budget; if both do not equal zero, then each is dynamically\nquantized to 4 bits, as described. SPARQ achieves minor accuracy degradation,\n2x speedup over widely used hardware architectures, and a practical hardware\nimplementation. The code is available at https://github.com/gilshm/sparq.",
          "link": "http://arxiv.org/abs/2105.11010",
          "publishedOn": "2021-05-25T01:56:12.620Z",
          "wordCount": 636,
          "title": "Post-Training Sparsity-Aware Quantization. (arXiv:2105.11010v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1912.04070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "Although synthetic training data has been shown to be beneficial for tasks\nsuch as human pose estimation, its use for RGB human action recognition is\nrelatively unexplored. Our goal in this work is to answer the question whether\nsynthetic humans can improve the performance of human action recognition, with\na particular focus on generalization to unseen viewpoints. We make use of the\nrecent advances in monocular 3D human body reconstruction from real action\nsequences to automatically render synthetic training videos for the action\nlabels. We make the following contributions: (i) we investigate the extent of\nvariations and augmentations that are beneficial to improving performance at\nnew viewpoints. We consider changes in body shape and clothing for individuals,\nas well as more action relevant augmentations such as non-uniform frame\nsampling, and interpolating between the motion of individuals performing the\nsame action; (ii) We introduce a new data generation methodology, SURREACT,\nthat allows training of spatio-temporal CNNs for action classification; (iii)\nWe substantially improve the state-of-the-art action recognition performance on\nthe NTU RGB+D and UESTC standard human action multi-view benchmarks; Finally,\n(iv) we extend the augmentation approach to in-the-wild videos from a subset of\nthe Kinetics dataset to investigate the case when only one-shot training data\nis available, and demonstrate improvements in this case as well.",
          "link": "http://arxiv.org/abs/1912.04070",
          "publishedOn": "2021-05-25T01:56:12.604Z",
          "wordCount": 695,
          "title": "Synthetic Humans for Action Recognition from Unseen Viewpoints. (arXiv:1912.04070v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hruby_P/0/1/0/all/0/1\">Petr Hruby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajdla_T/0/1/0/all/0/1\">Tomas Pajdla</a>",
          "description": "We present a technique for a complete 3D reconstruction of small objects\nmoving in front of a textured background. It is a particular variation of\nmultibody structure from motion, which specializes to two objects only. The\nscene is captured in several static configurations between which the relative\npose of the two objects may change. We reconstruct every static configuration\nindividually and segment the points locally by finding multiple poses of\ncameras that capture the scene's other configurations. Then, the local\nsegmentation results are combined, and the reconstructions are merged into the\nresulting model of the scene. In experiments with real artifacts, we show that\nour approach has practical advantages when reconstructing 3D objects from all\nsides. In this setting, our method outperforms the state-of-the-art. We\nintegrate our method into the state of the art 3D reconstruction pipeline\nCOLMAP.",
          "link": "http://arxiv.org/abs/2105.11352",
          "publishedOn": "2021-05-25T01:56:12.586Z",
          "wordCount": 582,
          "title": "Reconstructing Small 3D Objects in front of a Textured Background. (arXiv:2105.11352v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11241",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mann_P/0/1/0/all/0/1\">Prerak Mann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jain_S/0/1/0/all/0/1\">Sahaj Jain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mittal_S/0/1/0/all/0/1\">Saurabh Mittal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhat_A/0/1/0/all/0/1\">Aruna Bhat</a>",
          "description": "SARS-CoV-2, also known as COVID-19 or Coronavirus, is a viral contagious\ndisease that is infected by a novel coronavirus, and has been rapidly spreading\nacross the globe. It is very important to test and isolate people to reduce\nspread, and from here comes the need to do this quickly and efficiently.\nAccording to some studies, Chest-CT outperforms RT-PCR lab testing, which is\nthe current standard, when diagnosing COVID-19 patients. Due to this, computer\nvision researchers have developed various deep learning systems that can\npredict COVID-19 using a Chest-CT scan correctly to a certain degree. The\naccuracy of these systems is limited since deep learning neural networks such\nas CNNs (Convolutional Neural Networks) need a significantly large quantity of\ndata for training in order to produce good quality results. Since the disease\nis relatively recent and more focus has been on CXR (Chest XRay) images, the\navailable chest CT Scan image dataset is much less. We propose a method, by\nutilizing GANs, to generate synthetic chest CT images of both positive and\nnegative COVID-19 patients. Using a pre-built predictive model, we concluded\nthat around 40% of the generated images are correctly predicted as COVID-19\npositive. The dataset thus generated can be used to train a CNN-based\nclassifier which can help determine COVID-19 in a patient with greater\naccuracy.",
          "link": "http://arxiv.org/abs/2105.11241",
          "publishedOn": "2021-05-25T01:56:12.578Z",
          "wordCount": 709,
          "title": "Generation of COVID-19 Chest CT Scan Images using Generative Adversarial Networks. (arXiv:2105.11241v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.04971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daniel_T/0/1/0/all/0/1\">Tal Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurutach_T/0/1/0/all/0/1\">Thanard Kurutach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1\">Aviv Tamar</a>",
          "description": "In anomaly detection (AD), one seeks to identify whether a test sample is\nabnormal, given a data set of normal samples. A recent and promising approach\nto AD relies on deep generative models, such as variational autoencoders\n(VAEs), for unsupervised learning of the normal data distribution. In\nsemi-supervised AD (SSAD), the data also includes a small sample of labeled\nanomalies. In this work, we propose two variational methods for training VAEs\nfor SSAD. The intuitive idea in both methods is to train the encoder to\n`separate' between latent vectors for normal and outlier data. We show that\nthis idea can be derived from principled probabilistic formulations of the\nproblem, and propose simple and effective algorithms. Our methods can be\napplied to various data types, as we demonstrate on SSAD datasets ranging from\nnatural images to astronomy and medicine, can be combined with any VAE model\narchitecture, and are naturally compatible with ensembling. When comparing to\nstate-of-the-art SSAD methods that are not specific to particular data types,\nwe obtain marked improvement in outlier detection.",
          "link": "http://arxiv.org/abs/1911.04971",
          "publishedOn": "2021-05-25T01:56:12.572Z",
          "wordCount": 631,
          "title": "Deep Variational Semi-Supervised Novelty Detection. (arXiv:1911.04971v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08590",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abdar_M/0/1/0/all/0/1\">Moloud Abdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salari_S/0/1/0/all/0/1\">Soorena Salari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qahremani_S/0/1/0/all/0/1\">Sina Qahremani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_H/0/1/0/all/0/1\">Hak-Keung Lam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1\">Sadiq Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1\">U. Rajendra Acharya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>",
          "description": "The COVID-19 (Coronavirus disease 2019) has infected more than 151 million\npeople and caused approximately 3.17 million deaths around the world up to the\npresent. The rapid spread of COVID-19 is continuing to threaten human's life\nand health. Therefore, the development of computer-aided detection (CAD)\nsystems based on machine and deep learning methods which are able to accurately\ndifferentiate COVID-19 from other diseases using chest computed tomography (CT)\nand X-Ray datasets is essential and of immediate priority. Different from most\nof the previous studies which used either one of CT or X-ray images, we\nemployed both data types with sufficient samples in implementation. On the\nother hand, due to the extreme sensitivity of this pervasive virus, model\nuncertainty should be considered, while most previous studies have overlooked\nit. Therefore, we propose a novel powerful fusion model named\n$UncertaintyFuseNet$ that consists of an uncertainty module: Ensemble Monte\nCarlo (EMC) dropout. The obtained results prove the effectiveness of our\nproposed fusion for COVID-19 detection using CT scan and X-Ray datasets. Also,\nour proposed $UncertaintyFuseNet$ model is significantly robust to noise and\nperforms well with the previously unseen data. The source codes and models of\nthis study are available at:\nhttps://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.",
          "link": "http://arxiv.org/abs/2105.08590",
          "publishedOn": "2021-05-25T01:56:12.552Z",
          "wordCount": 733,
          "title": "UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion with Ensemble Monte Carlo Dropout for COVID-19 Detection. (arXiv:2105.08590v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jin-Ha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1\">Marcella Astrid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Muhammad Zaigham Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Ik Lee</a>",
          "description": "With the increase in the learning capability of deep convolution-based\narchitectures, various applications of such models have been proposed over\ntime. In the field of anomaly detection, improvements in deep learning opened\nnew prospects of exploration for the researchers whom tried to automate the\nlabor-intensive features of data collection. First, in terms of data\ncollection, it is impossible to anticipate all the anomalies that might exist\nin a given environment. Second, assuming we limit the possibilities of\nanomalies, it will still be hard to record all these scenarios for the sake of\ntraining a model. Third, even if we manage to record a significant amount of\nabnormal data, it's laborious to annotate this data on pixel or even frame\nlevel. Various approaches address the problem by proposing one-class\nclassification using generative models trained on only normal data. In such\nmethods, only the normal data is used, which is abundantly available and\ndoesn't require significant human input. However, these are trained with only\nnormal data and at the test time, given abnormal data as input, may often\ngenerate normal-looking output. This happens due to the hallucination\ncharacteristic of generative models. Next, these systems are designed to not\nuse abnormal examples during the training. In this paper, we propose anomaly\ndetection with negative learning (ADNL), which employs the negative learning\nconcept for the enhancement of anomaly detection by utilizing a very small\nnumber of labeled anomaly data as compared with the normal data during\ntraining. The idea is to limit the reconstruction capability of a generative\nmodel using the given a small amount of anomaly examples. This way, the network\nnot only learns to reconstruct normal data but also encloses the normal\ndistribution far from the possible distribution of anomalies.",
          "link": "http://arxiv.org/abs/2105.11058",
          "publishedOn": "2021-05-25T01:56:12.545Z",
          "wordCount": 717,
          "title": "Deep Visual Anomaly detection with Negative Learning. (arXiv:2105.11058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yecheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziming Zhang</a>",
          "description": "Graph convolutional networks (GCNs) are widely used in graph-based\napplications such as graph classification and segmentation. However, current\nGCNs have limitations on implementation such as network architectures due to\ntheir irregular inputs. In contrast, convolutional neural networks (CNNs) are\ncapable of extracting rich features from large-scale input data, but they do\nnot support general graph inputs. To bridge the gap between GCNs and CNNs, in\nthis paper we study the problem of how to effectively and efficiently map\ngeneral graphs to 2D grids that CNNs can be directly applied to, while\npreserving graph topology as much as possible. We therefore propose two novel\ngraph-to-grid mapping schemes, namely, {\\em graph-preserving grid layout\n(GPGL)} and its extension {\\em Hierarchical GPGL (H-GPGL)} for computational\nefficiency. We formulate the GPGL problem as integer programming and further\npropose an approximate yet efficient solver based on a penalized Kamada-Kawai\nmethod, a well-known optimization algorithm in 2D graph drawing. We propose a\nnovel vertex separation penalty that encourages graph vertices to lay on the\ngrid without any overlap. Along with this image representation, even extra 2D\nmaxpooling layers contribute to the PointNet, a widely applied point-based\nneural network. We demonstrate the empirical success of GPGL on general graph\nclassification with small graphs and H-GPGL on 3D point cloud segmentation with\nlarge graphs, based on 2D CNNs including VGG16, ResNet50 and multi-scale maxout\n(MSM) CNN.",
          "link": "http://arxiv.org/abs/2105.11016",
          "publishedOn": "2021-05-25T01:56:12.537Z",
          "wordCount": 668,
          "title": "Revisiting 2D Convolutional Neural Networks for Graph-based Applications. (arXiv:2105.11016v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wenjie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Q/0/1/0/all/0/1\">Qing Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mingyan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>",
          "description": "In this paper, we present an attention-guided deformable convolutional\nnetwork for hand-held multi-frame high dynamic range (HDR) imaging, namely\nADNet. This problem comprises two intractable challenges of how to handle\nsaturation and noise properly and how to tackle misalignments caused by object\nmotion or camera jittering. To address the former, we adopt a spatial attention\nmodule to adaptively select the most appropriate regions of various exposure\nlow dynamic range (LDR) images for fusion. For the latter one, we propose to\nalign the gamma-corrected images in the feature-level with a Pyramid, Cascading\nand Deformable (PCD) alignment module. The proposed ADNet shows\nstate-of-the-art performance compared with previous methods, achieving a\nPSNR-$l$ of 39.4471 and a PSNR-$\\mu$ of 37.6359 in NTIRE 2021 Multi-Frame HDR\nChallenge.",
          "link": "http://arxiv.org/abs/2105.10697",
          "publishedOn": "2021-05-25T01:56:12.530Z",
          "wordCount": 571,
          "title": "ADNet: Attention-guided Deformable Convolutional Network for High Dynamic Range Imaging. (arXiv:2105.10697v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yulin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1\">Soung Chang Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>",
          "description": "Noisy neural networks (NoisyNNs) refer to the inference and training of NNs\nin the presence of noise. Noise is inherent in most communication and storage\nsystems; hence, NoisyNNs emerge in many new applications, including federated\nedge learning, where wireless devices collaboratively train a NN over a noisy\nwireless channel, or when NNs are implemented/stored in an analog storage\nmedium. This paper studies a fundamental problem of NoisyNNs: how to estimate\nthe uncontaminated NN weights from their noisy observations or manifestations.\nWhereas all prior works relied on the maximum likelihood (ML) estimation to\nmaximize the likelihood function of the estimated NN weights, this paper\ndemonstrates that the ML estimator is in general suboptimal. To overcome the\nsuboptimality of the conventional ML estimator, we put forth an\n$\\text{MMSE}_{pb}$ estimator to minimize a compensated mean squared error (MSE)\nwith a population compensator and a bias compensator. Our approach works well\nfor NoisyNNs arising in both 1) noisy inference, where noise is introduced only\nin the inference phase on the already-trained NN weights; and 2) noisy\ntraining, where noise is introduced over the course of training. Extensive\nexperiments on the CIFAR-10 and SST-2 datasets with different NN architectures\nverify the significant performance gains of the $\\text{MMSE}_{pb}$ estimator\nover the ML estimator when used to denoise the NoisyNN. For noisy inference,\nthe average gains are up to $156\\%$ for a noisy ResNet34 model and $14.7\\%$ for\na noisy BERT model; for noisy training, the average gains are up to $18.1$ dB\nfor a noisy ResNet18 model.",
          "link": "http://arxiv.org/abs/2105.10699",
          "publishedOn": "2021-05-25T01:56:12.524Z",
          "wordCount": 717,
          "title": "Denoising Noisy Neural Networks: A Bayesian Approach with Compensation. (arXiv:2105.10699v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mina Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maes_P/0/1/0/all/0/1\">Pattie Maes</a>",
          "description": "Egocentric visual context detection can support intelligence augmentation\napplications. We created a wearable system, called PAL, for wearable,\npersonalized, and privacy-preserving egocentric visual context detection. PAL\nhas a wearable device with a camera, heart-rate sensor, on-device deep\nlearning, and audio input/output. PAL also has a mobile/web application for\npersonalized context labeling. We used on-device deep learning models for\ngeneric object and face detection, low-shot custom face and context recognition\n(e.g., activities like brushing teeth), and custom context clustering (e.g.,\nindoor locations). The models had over 80\\% accuracy in in-the-wild contexts\n(~1000 images) and we tested PAL for intelligence augmentation applications\nlike behavior change. We have made PAL is open-source to further support\nintelligence augmentation using personalized and privacy-preserving egocentric\nvisual contexts.",
          "link": "http://arxiv.org/abs/2105.10735",
          "publishedOn": "2021-05-25T01:56:12.517Z",
          "wordCount": 560,
          "title": "PAL: Intelligence Augmentation using Egocentric Visual Context Detection. (arXiv:2105.10735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1\">David B. Lindell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1\">Julien N. P. Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>",
          "description": "Numerical integration is a foundational technique in scientific computing and\nis at the core of many computer vision applications. Among these applications,\nneural volume rendering has recently been proposed as a new paradigm for view\nsynthesis, achieving photorealistic image quality. However, a fundamental\nobstacle to making these methods practical is the extreme computational and\nmemory requirements caused by the required volume integrations along the\nrendered rays during training and inference. Millions of rays, each requiring\nhundreds of forward passes through a neural network are needed to approximate\nthose integrations with Monte Carlo sampling. Here, we propose automatic\nintegration, a new framework for learning efficient, closed-form solutions to\nintegrals using coordinate-based neural networks. For training, we instantiate\nthe computational graph corresponding to the derivative of the network. The\ngraph is fitted to the signal to integrate. After optimization, we reassemble\nthe graph to obtain a network that represents the antiderivative. By the\nfundamental theorem of calculus, this enables the calculation of any definite\nintegral in two evaluations of the network. Applying this approach to neural\nrendering, we improve a tradeoff between rendering speed and image quality:\nimproving render times by greater than 10 times with a tradeoff of slightly\nreduced image quality.",
          "link": "http://arxiv.org/abs/2012.01714",
          "publishedOn": "2021-05-25T01:56:12.496Z",
          "wordCount": 670,
          "title": "AutoInt: Automatic Integration for Fast Neural Volume Rendering. (arXiv:2012.01714v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Summaira_J/0/1/0/all/0/1\">Jabeen Summaira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoib_A/0/1/0/all/0/1\">Amin Muhammad Shoib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_J/0/1/0/all/0/1\">Jabbar Abdul</a>",
          "description": "Deep Learning has implemented a wide range of applications and has become\nincreasingly popular in recent years. The goal of multimodal deep learning is\nto create models that can process and link information using various\nmodalities. Despite the extensive development made for unimodal learning, it\nstill cannot cover all the aspects of human learning. Multimodal learning helps\nto understand and analyze better when various senses are engaged in the\nprocessing of information. This paper focuses on multiple types of modalities,\ni.e., image, video, text, audio, body gestures, facial expressions, and\nphysiological signals. Detailed analysis of past and current baseline\napproaches and an in-depth study of recent advancements in multimodal deep\nlearning applications has been provided. A fine-grained taxonomy of various\nmultimodal deep learning applications is proposed, elaborating on different\napplications in more depth. Architectures and datasets used in these\napplications are also discussed, along with their evaluation metrics. Last,\nmain issues are highlighted separately for each domain along with their\npossible future research directions.",
          "link": "http://arxiv.org/abs/2105.11087",
          "publishedOn": "2021-05-25T01:56:12.490Z",
          "wordCount": 601,
          "title": "Recent Advances and Trends in Multimodal Deep Learning: A Review. (arXiv:2105.11087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singhania_D/0/1/0/all/0/1\">Dipika Singhania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_R/0/1/0/all/0/1\">Rahul Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>",
          "description": "Temporal convolutional networks (TCNs) are a commonly used architecture for\ntemporal video segmentation. TCNs however, tend to suffer from\nover-segmentation errors and require additional refinement modules to ensure\nsmoothness and temporal coherency. In this work, we propose a novel temporal\nencoder-decoder to tackle the problem of sequence fragmentation. In particular,\nthe decoder follows a coarse-to-fine structure with an implicit ensemble of\nmultiple temporal resolutions. The ensembling produces smoother segmentations\nthat are more accurate and better-calibrated, bypassing the need for additional\nrefinement modules. In addition, we enhance our training with a\nmulti-resolution feature-augmentation strategy to promote robustness to varying\ntemporal resolutions. Finally, to support our architecture and encourage\nfurther sequence coherency, we propose an action loss that penalizes\nmisclassifications at the video level. Experiments show that our stand-alone\narchitecture, together with our novel feature-augmentation strategy and new\nloss, outperforms the state-of-the-art on three temporal video segmentation\nbenchmarks.",
          "link": "http://arxiv.org/abs/2105.10859",
          "publishedOn": "2021-05-25T01:56:12.484Z",
          "wordCount": 571,
          "title": "Coarse to Fine Multi-Resolution Temporal Convolutional Network. (arXiv:2105.10859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengchao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baipeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bing Zhang</a>",
          "description": "Change detection is the process of identifying pixel-wise differences of\nbi-temporal co-registered images. It is of great significance to Earth\nobservation. Recently, with the emerging of deep learning (DL), deep\nconvolutional neural networks (CNNs) based methods have shown their power and\nfeasibility in the field of change detection. However, there is still a lack of\neffective supervision for change feature learning. In this work, a feature\nconstraint change detection network (FCCDN) is proposed. We constrain features\nboth on bi-temporal feature extraction and feature fusion. More specifically,\nwe propose a dual encoder-decoder network backbone for the change detection\ntask. At the center of the backbone, we design a non-local feature pyramid\nnetwork to extract and fuse multi-scale features. To fuse bi-temporal features\nin a robust way, we build a dense connection-based feature fusion module.\nMoreover, a self-supervised learning-based strategy is proposed to constrain\nfeature learning. Based on FCCDN, we achieve state-of-the-art performance on\ntwo building change detection datasets (LEVIR-CD and WHU). On the LEVIR-CD\ndataset, we achieve IoU of 0.8569 and F1 score of 0.9229. On the WHU dataset,\nwe achieve IoU of 0.8820 and F1 score of 0.9373. Moreover, we, for the first\ntime, achieve the acquire of accurate bi-temporal semantic segmentation results\nwithout using semantic segmentation labels. It is vital for the application of\nchange detection because it saves the cost of labeling.",
          "link": "http://arxiv.org/abs/2105.10860",
          "publishedOn": "2021-05-25T01:56:12.478Z",
          "wordCount": 676,
          "title": "FCCDN: Feature Constraint Network for VHR Image Change Detection. (arXiv:2105.10860v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>",
          "description": "In this paper, we propose HOME, a framework tackling the motion forecasting\nproblem with an image output representing the probability distribution of the\nagent's future location. This method allows for a simple architecture with\nclassic convolution networks coupled with attention mechanism for agent\ninteractions, and outputs an unconstrained 2D top-view representation of the\nagent's possible future. Based on this output, we design two methods to sample\na finite set of agent's future locations. These methods allow us to control the\noptimization trade-off between miss rate and final displacement error for\nmultiple modalities without having to retrain any part of the model. We apply\nour method to the Argoverse Motion Forecasting Benchmark and achieve 1st place\non the online leaderboard.",
          "link": "http://arxiv.org/abs/2105.10968",
          "publishedOn": "2021-05-25T01:56:12.470Z",
          "wordCount": 551,
          "title": "HOME: Heatmap Output for future Motion Estimation. (arXiv:2105.10968v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miceli_M/0/1/0/all/0/1\">Milagros Miceli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posada_J/0/1/0/all/0/1\">Julian Posada</a>",
          "description": "Developers of computer vision algorithms outsource some of the labor involved\nin annotating training data through business process outsourcing companies and\ncrowdsourcing platforms. Many data annotators are situated in the Global South\nand are considered independent contractors. This paper focuses on the\nexperiences of Argentinian and Venezuelan annotation workers. Through\nqualitative methods, we explore the discourses encoded in the task instructions\nthat these workers follow to annotate computer vision datasets. Our preliminary\nfindings indicate that annotation instructions reflect worldviews imposed on\nworkers and, through their labor, on datasets. Moreover, we observe that\nfor-profit goals drive task instructions and that managers and algorithms make\nsure annotations are done according to requesters' commands. This configuration\npresents a form of commodified labor that perpetuates power asymmetries while\nreinforcing social inequalities and is compelled to reproduce them into\ndatasets and, subsequently, in computer vision systems.",
          "link": "http://arxiv.org/abs/2105.10990",
          "publishedOn": "2021-05-25T01:56:12.445Z",
          "wordCount": 595,
          "title": "Wisdom for the Crowd: Discoursive Power in Annotation Instructions for Computer Vision. (arXiv:2105.10990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomas_H/0/1/0/all/0/1\">Henri Tomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1\">Marcus Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dionido_R/0/1/0/all/0/1\">Raimarc Dionido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ty_M/0/1/0/all/0/1\">Mark Ty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirando_J/0/1/0/all/0/1\">Jonric Mirando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casimiro_J/0/1/0/all/0/1\">Joel Casimiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1\">Rowel Atienza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guinto_R/0/1/0/all/0/1\">Richard Guinto</a>",
          "description": "One of the most fundamental and information-laden actions humans do is to\nlook at objects. However, a survey of current works reveals that existing\ngaze-related datasets annotate only the pixel being looked at, and not the\nboundaries of a specific object of interest. This lack of object annotation\npresents an opportunity for further advancing gaze estimation research. To this\nend, we present a challenging new task called gaze object prediction, where the\ngoal is to predict a bounding box for a person's gazed-at object. To train and\nevaluate gaze networks on this task, we present the Gaze On Objects (GOO)\ndataset. GOO is composed of a large set of synthetic images (GOO Synth)\nsupplemented by a smaller subset of real images (GOO-Real) of people looking at\nobjects in a retail environment. Our work establishes extensive baselines on\nGOO by re-implementing and evaluating selected state-of-the art models on the\ntask of gaze following and domain adaptation. Code is available on github.",
          "link": "http://arxiv.org/abs/2105.10793",
          "publishedOn": "2021-05-25T01:56:12.437Z",
          "wordCount": 619,
          "title": "GOO: A Dataset for Gaze Object Prediction in Retail Environments. (arXiv:2105.10793v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10827",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Larrazabal_A/0/1/0/all/0/1\">Agostina J. Larrazabal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martinez_C/0/1/0/all/0/1\">C&#xe9;sar Mart&#xed;nez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>",
          "description": "Despite the astonishing performance of deep-learning based approaches for\nvisual tasks such as semantic segmentation, they are known to produce\nmiscalibrated predictions, which could be harmful for critical decision-making\nprocesses. Ensemble learning has shown to not only boost the performance of\nindividual models but also reduce their miscalibration by averaging independent\npredictions. In this scenario, model diversity has become a key factor, which\nfacilitates individual models converging to different functional solutions. In\nthis work, we introduce Orthogonal Ensemble Networks (OEN), a novel framework\nto explicitly enforce model diversity by means of orthogonal constraints. The\nproposed method is based on the hypothesis that inducing orthogonality among\nthe constituents of the ensemble will increase the overall model diversity. We\nresort to a new pairwise orthogonality constraint which can be used to\nregularize a sequential ensemble training process, resulting on improved\npredictive performance and better calibrated model outputs. We benchmark the\nproposed framework in two challenging brain lesion segmentation tasks --brain\ntumor and white matter hyper-intensity segmentation in MR images. The\nexperimental results show that our approach produces more robust and\nwell-calibrated ensemble models and can deal with challenging tasks in the\ncontext of biomedical image segmentation.",
          "link": "http://arxiv.org/abs/2105.10827",
          "publishedOn": "2021-05-25T01:56:12.410Z",
          "wordCount": 639,
          "title": "Orthogonal Ensemble Networks for Biomedical Image Segmentation. (arXiv:2105.10827v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duarte_K/0/1/0/all/0/1\">Kevin Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh S. Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Neural networks trained on real-world datasets with long-tailed label\ndistributions are biased towards frequent classes and perform poorly on\ninfrequent classes. The imbalance in the ratio of positive and negative samples\nfor each class skews network output probabilities further from ground-truth\ndistributions. We propose a method, Partial Label Masking (PLM), which utilizes\nthis ratio during training. By stochastically masking labels during loss\ncomputation, the method balances this ratio for each class, leading to improved\nrecall on minority classes and improved precision on frequent classes. The\nratio is estimated adaptively based on the network's performance by minimizing\nthe KL divergence between predicted and ground-truth distributions. Whereas\nmost existing approaches addressing data imbalance are mainly focused on\nsingle-label classification and do not generalize well to the multi-label case,\nthis work proposes a general approach to solve the long-tail data imbalance\nissue for multi-label classification. PLM is versatile: it can be applied to\nmost objective functions and it can be used alongside other strategies for\nclass imbalance. Our method achieves strong performance when compared to\nexisting methods on both multi-label (MultiMNIST and MSCOCO) and single-label\n(imbalanced CIFAR-10 and CIFAR-100) image classification datasets.",
          "link": "http://arxiv.org/abs/2105.10782",
          "publishedOn": "2021-05-25T01:56:12.390Z",
          "wordCount": 632,
          "title": "PLM: Partial Label Masking for Imbalanced Multi-label Classification. (arXiv:2105.10782v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ohtsubo_Y/0/1/0/all/0/1\">Yusuke Ohtsubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsukawa_T/0/1/0/all/0/1\">Tetsu Matsukawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_E/0/1/0/all/0/1\">Einoshin Suzuki</a>",
          "description": "In this paper, we propose a deep invertible hybrid model which integrates\ndiscriminative and generative learning at a latent space level for\nsemi-supervised few-shot classification. Various tasks for classifying new\nspecies from image data can be modeled as a semi-supervised few-shot\nclassification, which assumes a labeled and unlabeled training examples and a\nsmall support set of the target classes. Predicting target classes with a few\nsupport examples per class makes the learning task difficult for existing\nsemi-supervised classification methods, including selftraining, which\niteratively estimates class labels of unlabeled training examples to learn a\nclassifier for the training classes. To exploit unlabeled training examples\neffectively, we adopt as the objective function the composite likelihood, which\nintegrates discriminative and generative learning and suits better with deep\nneural networks than the parameter coupling prior, the other popular integrated\nlearning approach. In our proposed model, the discriminative and generative\nmodels are respectively Prototypical Networks, which have shown excellent\nperformance in various kinds of few-shot learning, and Normalizing Flow a deep\ninvertible model which returns the exact marginal likelihood unlike the other\nthree major methods, i.e., VAE, GAN, and autoregressive model. Our main\noriginality lies in our integration of these components at a latent space\nlevel, which is effective in preventing overfitting. Experiments using\nmini-ImageNet and VGG-Face datasets show that our method outperforms\nselftraining based Prototypical Networks.",
          "link": "http://arxiv.org/abs/2105.10644",
          "publishedOn": "2021-05-25T01:56:12.384Z",
          "wordCount": 656,
          "title": "Semi-Supervised Few-Shot Classification with Deep Invertible Hybrid Models. (arXiv:2105.10644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jieqian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirn_M/0/1/0/all/0/1\">Matthew Hirn</a>",
          "description": "We provide a new model for texture synthesis based on a multiscale,\nmultilayer feature extractor. Within the model, textures are represented by a\nset of statistics computed from ReLU wavelet coefficients at different layers,\nscales and orientations. A new image is synthesized by matching the target\nstatistics via an iterative projection algorithm. We explain the necessity of\nthe different types of pre-defined wavelet filters used in our model and the\nadvantages of multilayer structures for image synthesis. We demonstrate the\npower of our model by generating samples of high quality textures and providing\ninsights into deep representations for texture images.",
          "link": "http://arxiv.org/abs/2105.10825",
          "publishedOn": "2021-05-25T01:56:12.377Z",
          "wordCount": 531,
          "title": "Texture synthesis via projection onto multiscale, multilayer statistics. (arXiv:2105.10825v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Siming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenpei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vouga_E/0/1/0/all/0/1\">Etienne Vouga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>",
          "description": "This paper introduces HPNet, a novel deep-learning approach for segmenting a\n3D shape represented as a point cloud into primitive patches. The key to deep\nprimitive segmentation is learning a feature representation that can separate\npoints of different primitives. Unlike utilizing a single feature\nrepresentation, HPNet leverages hybrid representations that combine one learned\nsemantic descriptor, two spectral descriptors derived from predicted geometric\nparameters, as well as an adjacency matrix that encodes sharp edges. Moreover,\ninstead of merely concatenating the descriptors, HPNet optimally combines\nhybrid representations by learning combination weights. This weighting module\nbuilds on the entropy of input features. The output primitive segmentation is\nobtained from a mean-shift clustering module. Experimental results on benchmark\ndatasets ANSI and ABCParts show that HPNet leads to significant performance\ngains from baseline approaches.",
          "link": "http://arxiv.org/abs/2105.10620",
          "publishedOn": "2021-05-25T01:56:12.371Z",
          "wordCount": 566,
          "title": "HPNet: Deep Primitive Segmentation Using Hybrid Representations. (arXiv:2105.10620v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10650",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ji_H/0/1/0/all/0/1\">Hangjie Ji</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lafata_K/0/1/0/all/0/1\">Kyle Lafata</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mowery_Y/0/1/0/all/0/1\">Yvonne Mowery</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Brizel_D/0/1/0/all/0/1\">David Brizel</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bertozzi_A/0/1/0/all/0/1\">Andrea L. Bertozzi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yin_F/0/1/0/all/0/1\">Fang-Fang Yin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_C/0/1/0/all/0/1\">Chunhao Wang</a>",
          "description": "This paper develops a method of biologically guided deep learning for\npost-radiation FDG-PET image outcome prediction based on pre-radiation images\nand radiotherapy dose information. Based on the classic reaction-diffusion\nmechanism, a novel biological model was proposed using a partial differential\nequation that incorporates spatial radiation dose distribution as a\npatient-specific treatment information variable. A 7-layer\nencoder-decoder-based convolutional neural network (CNN) was designed and\ntrained to learn the proposed biological model. As such, the model could\ngenerate post-radiation FDG-PET image outcome predictions with possible\ntime-series transition from pre-radiotherapy image states to post-radiotherapy\nstates. The proposed method was developed using 64 oropharyngeal patients with\npaired FDG-PET studies before and after 20Gy delivery (2Gy/daily fraction) by\nIMRT. In a two-branch deep learning execution, the proposed CNN learns specific\nterms in the biological model from paired FDG-PET images and spatial dose\ndistribution as in one branch, and the biological model generates post-20Gy\nFDG-PET image prediction in the other branch. The proposed method successfully\ngenerated post-20Gy FDG-PET image outcome prediction with breakdown\nillustrations of biological model components. Time-series FDG-PET image\npredictions were generated to demonstrate the feasibility of disease response\nrendering. The developed biologically guided deep learning method achieved\npost-20Gy FDG-PET image outcome predictions in good agreement with ground-truth\nresults. With break-down biological modeling components, the outcome image\npredictions could be used in adaptive radiotherapy decision-making to optimize\npersonalized plans for the best outcome in the future.",
          "link": "http://arxiv.org/abs/2105.10650",
          "publishedOn": "2021-05-25T01:56:12.365Z",
          "wordCount": 702,
          "title": "Post-Radiotherapy PET Image Outcome Prediction by Deep Learning under Biological Model Guidance: A Feasibility Study of Oropharyngeal Cancer Application. (arXiv:2105.10650v1 [physics.med-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuangjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sehgal_N/0/1/0/all/0/1\">Naveen Sehgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>",
          "description": "The ultimate goal for an inference model is to be robust and functional in\nreal life applications. However, training vs. test data domain gaps often\nnegatively affect model performance. This issue is especially critical for the\nmonocular 3D human pose estimation problem, in which 3D human data is often\ncollected in a controlled lab setting. In this paper, we focus on alleviating\nthe negative effect of domain shift by presenting our adapted human pose (AHuP)\napproach that addresses adaptation problems in both appearance and pose spaces.\nAHuP is built around a practical assumption that in real applications, data\nfrom target domain could be inaccessible or only limited information can be\nacquired. We illustrate the 3D pose estimation performance of AHuP in two\nscenarios. First, when source and target data differ significantly in both\nappearance and pose spaces, in which we learn from synthetic 3D human data\n(with zero real 3D human data) and show comparable performance with the\nstate-of-the-art 3D pose estimation models that have full access to the real 3D\nhuman pose benchmarks for training. Second, when source and target datasets\ndiffer mainly in the pose space, in which AHuP approach can be applied to\nfurther improve the performance of the state-of-the-art models when tested on\nthe datasets different from their training dataset.",
          "link": "http://arxiv.org/abs/2105.10837",
          "publishedOn": "2021-05-25T01:56:12.318Z",
          "wordCount": 653,
          "title": "Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data. (arXiv:2105.10837v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chih-Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun-Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>",
          "description": "Video-based person re-identification (Re-ID) aims at matching the video\ntracklets with cropped video frames for identifying the pedestrians under\ndifferent cameras. However, there exists severe spatial and temporal\nmisalignment for those cropped tracklets due to the imperfect detection and\ntracking results generated with obsolete methods. To address this issue, we\npresent a simple re-Detect and Link (DL) module which can effectively reduce\nthose unexpected noise through applying the deep learning-based detection and\ntracking on the cropped tracklets. Furthermore, we introduce an improved model\ncalled Coarse-to-Fine Axial-Attention Network (CF-AAN). Based on the typical\nNon-local Network, we replace the non-local module with three 1-D\nposition-sensitive axial attentions, in addition to our proposed coarse-to-fine\nstructure. With the developed CF-AAN, compared to the original non-local\noperation, we can not only significantly reduce the computation cost but also\nobtain the state-of-the-art performance (91.3% in rank-1 and 86.5% in mAP) on\nthe large-scale MARS dataset. Meanwhile, by simply adopting our DL module for\ndata alignment, to our surprise, several baseline models can achieve better or\ncomparable results with the current state-of-the-arts. Besides, we discover the\nerrors not only for the identity labels of tracklets but also for the\nevaluation protocol for the test data of MARS. We hope that our work can help\nthe community for the further development of invariant representation without\nthe hassle of the spatial and temporal alignment and dataset noise. The code,\ncorrected labels, evaluation protocol, and the aligned data will be available\nat https://github.com/jackie840129/CF-AAN.",
          "link": "http://arxiv.org/abs/2105.10678",
          "publishedOn": "2021-05-25T01:56:12.281Z",
          "wordCount": 683,
          "title": "Video-based Person Re-identification without Bells and Whistles. (arXiv:2105.10678v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guolei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nikola Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Significant progress on the crowd counting problem has been achieved by\nintegrating larger context into convolutional neural networks (CNNs). This\nindicates that global scene context is essential, despite the seemingly\nbottom-up nature of the problem. This may be explained by the fact that context\nknowledge can adapt and improve local feature extraction to a given scene. In\nthis paper, we therefore investigate the role of global context for crowd\ncounting. Specifically, a pure transformer is used to extract features with\nglobal information from overlapping image patches. Inspired by classification,\nwe add a context token to the input sequence, to facilitate information\nexchange with tokens corresponding to image patches throughout transformer\nlayers. Due to the fact that transformers do not explicitly model the\ntried-and-true channel-wise interactions, we propose a token-attention module\n(TAM) to recalibrate encoded features through channel-wise attention informed\nby the context token. Beyond that, it is adopted to predict the total person\ncount of the image through regression-token module (RTM). Extensive experiments\ndemonstrate that our method achieves state-of-the-art performance on various\ndatasets, including ShanghaiTech, UCF-QNRF, JHU-CROWD++ and NWPU. On the\nlarge-scale JHU-CROWD++ dataset, our method improves over the previous best\nresults by 26.9% and 29.9% in terms of MAE and MSE, respectively.",
          "link": "http://arxiv.org/abs/2105.10926",
          "publishedOn": "2021-05-25T01:56:12.259Z",
          "wordCount": 633,
          "title": "Boosting Crowd Counting with Transformers. (arXiv:2105.10926v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Guoliang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Runwei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>",
          "description": "Although monocular 3D human pose estimation methods have made significant\nprogress, it's far from being solved due to the inherent depth ambiguity.\nInstead, exploiting multi-view information is a practical way to achieve\nabsolute 3D human pose estimation. In this paper, we propose a simple yet\neffective pipeline for weakly-supervised cross-view 3D human pose estimation.\nBy only using two camera views, our method can achieve state-of-the-art\nperformance in a weakly-supervised manner, requiring no 3D ground truth but\nonly 2D annotations. Specifically, our method contains two steps: triangulation\nand refinement. First, given the 2D keypoints that can be obtained through any\nclassic 2D detection methods, triangulation is performed across two views to\nlift the 2D keypoints into coarse 3D poses.Then, a novel cross-view U-shaped\ngraph convolutional network (CV-UGCN), which can explore the spatial\nconfigurations and cross-view correlations, is designed to refine the coarse 3D\nposes. In particular, the refinement progress is achieved through\nweakly-supervised learning, in which geometric and structure-aware consistency\nchecks are performed. We evaluate our method on the standard benchmark dataset,\nHuman3.6M. The Mean Per Joint Position Error on the benchmark dataset is 27.4\nmm, which outperforms the state-of-the-arts remarkably (27.4 mm vs 30.2 mm).",
          "link": "http://arxiv.org/abs/2105.10882",
          "publishedOn": "2021-05-25T01:56:12.239Z",
          "wordCount": 622,
          "title": "Weakly-supervised Cross-view 3D Human Pose Estimation. (arXiv:2105.10882v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1\">Marco Visca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1\">Sampo Kuutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1\">Roger Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>",
          "description": "Terrain traversability analysis plays a major role in ensuring safe robotic\nnavigation in unstructured environments. However, real-time constraints\nfrequently limit the accuracy of online tests, especially in scenarios where\nrealistic robot-terrain interactions are complex to model. In this context, we\npropose a deep learning framework, trained in an end-to-end fashion from\nelevation maps and trajectories, to estimate the occurrence of failure events.\nThe network is first trained and tested in simulation over synthetic maps\ngenerated by the OpenSimplex algorithm. The prediction performance of the Deep\nLearning framework is illustrated by being able to retain over 94% recall of\nthe original simulator at 30% of the computational time. Finally, the network\nis transferred and tested on real elevation maps collected by the SEEKER\nconsortium during the Martian rover test trial in the Atacama desert in Chile.\nWe show that transferring and fine-tuning of an application-independent\npre-trained model retains better performance than training uniquely on scarcely\navailable real data.",
          "link": "http://arxiv.org/abs/2105.10937",
          "publishedOn": "2021-05-25T01:56:12.225Z",
          "wordCount": 606,
          "title": "Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>",
          "description": "Recently, DETR and Deformable DETR have been proposed to eliminate the need\nfor many hand-designed components in object detection while demonstrating good\nperformance as previous complex hand-crafted detectors. However, their\nperformance on Video Object Detection (VOD) has not been well explored. In this\npaper, we present TransVOD, an end-to-end video object detection model based on\na spatial-temporal Transformer architecture. The goal of this paper is to\nstreamline the pipeline of VOD, effectively removing the need for many\nhand-crafted components for feature aggregation, e.g., optical flow, recurrent\nneural networks, relation networks. Besides, benefited from the object query\ndesign in DETR, our method does not need complicated post-processing methods\nsuch as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and\nclean. In particular, we present temporal Transformer to aggregate both the\nspatial object queries and the feature memories of each frame. Our temporal\nTransformer consists of three components: Temporal Deformable Transformer\nEncoder (TDTE) to encode the multiple frame spatial details, Temporal Query\nEncoder (TQE) to fuse object queries, and Temporal Deformable Transformer\nDecoder to obtain current frame detection results. These designs boost the\nstrong baseline deformable DETR by a significant margin (3%-4% mAP) on the\nImageNet VID dataset. TransVOD yields comparable results performance on the\nbenchmark of ImageNet VID. We hope our TransVOD can provide a new perspective\nfor video object detection. Code will be made publicly available at\nhttps://github.com/SJTU-LuHe/TransVOD.",
          "link": "http://arxiv.org/abs/2105.10920",
          "publishedOn": "2021-05-25T01:56:12.218Z",
          "wordCount": 670,
          "title": "End-to-End Video Object Detection with Spatial-Temporal Transformers. (arXiv:2105.10920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kourbane_I/0/1/0/all/0/1\">Ikram Kourbane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genc_Y/0/1/0/all/0/1\">Yakup Genc</a>",
          "description": "Existing RGB-based 2D hand pose estimation methods learn the joint locations\nfrom a single resolution, which is not suitable for different hand sizes. To\ntackle this problem, we propose a new deep learning-based framework that\nconsists of two main modules. The former presents a segmentation-based approach\nto detect the hand skeleton and localize the hand bounding box. The second\nmodule regresses the 2D joint locations through a multi-scale heatmap\nregression approach that exploits the predicted hand skeleton as a constraint\nto guide the model. Furthermore, we construct a new dataset that is suitable\nfor both hand detection and pose estimation. We qualitatively and\nquantitatively validate our method on two datasets. Results demonstrate that\nthe proposed method outperforms state-of-the-art and can recover the pose even\nin cluttered images and complex poses.",
          "link": "http://arxiv.org/abs/2105.10904",
          "publishedOn": "2021-05-25T01:56:12.190Z",
          "wordCount": 568,
          "title": "Skeleton-aware multi-scale heatmap regression for 2D hand pose estimation. (arXiv:2105.10904v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Animesh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordo_A/0/1/0/all/0/1\">Albert Gordo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>",
          "description": "We study the problem of learning how to predict attribute-object compositions\nfrom images, and its generalization to unseen compositions missing from the\ntraining data. To the best of our knowledge, this is a first large-scale study\nof this problem, involving hundreds of thousands of compositions. We train our\nframework with images from Instagram using hashtags as noisy weak supervision.\nWe make careful design choices for data collection and modeling, in order to\nhandle noisy annotations and unseen compositions. Finally, extensive\nevaluations show that learning to compose classifiers outperforms late fusion\nof individual attribute and object predictions, especially in the case of\nunseen attribute-object pairs.",
          "link": "http://arxiv.org/abs/2105.11373",
          "publishedOn": "2021-05-25T01:56:12.152Z",
          "wordCount": null,
          "title": "Large-Scale Attribute-Object Compositions. (arXiv:2105.11373v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>",
          "description": "Recently, satellites with high temporal resolution have fostered wide\nattention in various practical applications. Due to limitations of bandwidth\nand hardware cost, however, the spatial resolution of such satellites is\nconsiderably low, largely limiting their potentials in scenarios that require\nspatially explicit information. To improve image resolution, numerous\napproaches based on training low-high resolution pairs have been proposed to\naddress the super-resolution (SR) task. Despite their success, however,\nlow/high spatial resolution pairs are usually difficult to obtain in satellites\nwith a high temporal resolution, making such approaches in SR impractical to\nuse. In this paper, we proposed a new unsupervised learning framework, called\n\"MIP\", which achieves SR tasks without low/high resolution image pairs. First,\nrandom noise maps are fed into a designed generative adversarial network (GAN)\nfor reconstruction. Then, the proposed method converts the reference image to\nlatent space as the migration image prior. Finally, we update the input noise\nvia an implicit method, and further transfer the texture and structured\ninformation from the reference image. Extensive experimental results on the\nDraper dataset show that MIP achieves significant improvements over\nstate-of-the-art methods both quantitatively and qualitatively. The proposed\nMIP is open-sourced at this http URL",
          "link": "http://arxiv.org/abs/2105.03579",
          "publishedOn": "2021-05-25T01:56:12.150Z",
          "wordCount": null,
          "title": "Unsupervised Remote Sensing Super-Resolution via Migration Image Prior. (arXiv:2105.03579v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "In this paper, we tackle the problem of convolutional neural network design.\nInstead of focusing on the design of the overall architecture, we investigate a\ndesign space that is usually overlooked, i.e. adjusting the channel\nconfigurations of predefined networks. We find that this adjustment can be\nachieved by shrinking widened baseline networks and leads to superior\nperformance. Based on that, we articulate the heterogeneity hypothesis: with\nthe same training protocol, there exists a layer-wise differentiated network\narchitecture (LW-DNA) that can outperform the original network with regular\nchannel configurations but with a lower level of model complexity.\n\nThe LW-DNA models are identified without extra computational cost or training\ntime compared with the original network. This constraint leads to controlled\nexperiments which direct the focus to the importance of layer-wise specific\nchannel configurations. LW-DNA models come with advantages related to\noverfitting, i.e. the relative relationship between model complexity and\ndataset size. Experiments are conducted on various networks and datasets for\nimage classification, visual tracking and image restoration. The resultant\nLW-DNA models consistently outperform the baseline models. Code is available at\nhttps://github.com/ofsoundof/Heterogeneity_Hypothesis.",
          "link": "http://arxiv.org/abs/2006.16242",
          "publishedOn": "2021-05-25T01:56:12.149Z",
          "wordCount": null,
          "title": "The Heterogeneity Hypothesis: Finding Layer-Wise Differentiated Network Architectures. (arXiv:2006.16242v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>",
          "description": "D shape generation is a fundamental operation in computer graphics. While\nsignificant progress has been made, especially with recent deep generative\nmodels, it remains a challenge to synthesize high-quality shapes with rich\ngeometric details and complex structure, in a controllable manner. To tackle\nthis, we introduce DSG-Net, a deep neural network that learns a disentangled\nstructured and geometric mesh representation for 3D shapes, where two key\naspects of shapes, geometry, and structure, are encoded in a synergistic manner\nto ensure plausibility of the generated shapes, while also being disentangled\nas much as possible. This supports a range of novel shape generation\napplications with disentangled control, such as interpolation of structure\n(geometry) while keeping geometry (structure) unchanged. To achieve this, we\nsimultaneously learn structure and geometry through variational autoencoders\n(VAEs) in a hierarchical manner for both, with bijective mappings at each\nlevel. In this manner, we effectively encode geometry and structure in separate\nlatent spaces, while ensuring their compatibility: the structure is used to\nguide the geometry and vice versa. At the leaf level, the part geometry is\nrepresented using a conditional part VAE, to encode high-quality geometric\ndetails, guided by the structure context as the condition. Our method not only\nsupports controllable generation applications but also produces high-quality\nsynthesized shapes, outperforming state-of-the-art methods. The code has been\nreleased at https://github.com/IGLICT/DSG-Net.",
          "link": "http://arxiv.org/abs/2008.05440",
          "publishedOn": "2021-05-25T01:56:12.149Z",
          "wordCount": null,
          "title": "DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape Generation. (arXiv:2008.05440v3 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertiche_H/0/1/0/all/0/1\">Hugo Bertiche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1\">Meysam Madadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>",
          "description": "We present a methodology to automatically obtain Pose Space Deformation (PSD)\nbasis for rigged garments through deep learning. Classical approaches rely on\nPhysically Based Simulations (PBS) to animate clothes. These are general\nsolutions that, given a sufficiently fine-grained discretization of space and\ntime, can achieve highly realistic results. However, they are computationally\nexpensive and any scene modification prompts the need of re-simulation. Linear\nBlend Skinning (LBS) with PSD offers a lightweight alternative to PBS, though,\nit needs huge volumes of data to learn proper PSD. We propose using deep\nlearning, formulated as an implicit PBS, to unsupervisedly learn realistic\ncloth Pose Space Deformations in a constrained scenario: dressed humans.\nFurthermore, we show it is possible to train these models in an amount of time\ncomparable to a PBS of a few sequences. To the best of our knowledge, we are\nthe first to propose a neural simulator for cloth. While deep-based approaches\nin the domain are becoming a trend, these are data-hungry models. Moreover,\nauthors often propose complex formulations to better learn wrinkles from PBS\ndata. Supervised learning leads to physically inconsistent predictions that\nrequire collision solving to be used. Also, dependency on PBS data limits the\nscalability of these solutions, while their formulation hinders its\napplicability and compatibility. By proposing an unsupervised methodology to\nlearn PSD for LBS models (3D animation standard), we overcome both of these\ndrawbacks. Results obtained show cloth-consistency in the animated garments and\nmeaningful pose-dependant folds and wrinkles. Our solution is extremely\nefficient, handles multiple layers of cloth, allows unsupervised outfit\nresizing and can be easily applied to any custom 3D avatar.",
          "link": "http://arxiv.org/abs/2012.11310",
          "publishedOn": "2021-05-25T01:56:12.149Z",
          "wordCount": null,
          "title": "PBNS: Physically Based Neural Simulator for Unsupervised Garment Pose Space Deformation. (arXiv:2012.11310v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bohara_B/0/1/0/all/0/1\">Bharat Bohara</a>",
          "description": "Choosing a decision threshold is one of the challenging job in any\nclassification tasks. How much the model is accurate, if the deciding boundary\nis not picked up carefully, its entire performance would go in vain. On the\nother hand, for imbalance classification where one of the classes is dominant\nover another, relying on the conventional method of choosing threshold would\nresult in poor performance. Even if the threshold or decision boundary is\nproperly chosen based on machine learning strategies like SVM and decision\ntree, it will fail at some point for dynamically varying databases and in case\nof identity-features that are more or less similar, like in face recognition\nand person re-identification models. Hence, with the need for adaptability of\nthe decision threshold selection for imbalanced classification and incremental\ndatabase size, an online optimization-based statistical feature learning\nadaptive technique is developed and tested on the LFW datasets and\nself-prepared athletes datasets. This method of adopting adaptive threshold\nresulted in 12-45% improvement in the model accuracy compared to the fixed\nthreshold {0.3,0.5,0.7} that are usually taken via the hit-and-trial method in\nany classification and identification tasks. Source code for the complete\nalgorithm is available at: https://github.com/Varat7v2/adaptive-threshold",
          "link": "http://arxiv.org/abs/2012.14305",
          "publishedOn": "2021-05-25T01:56:12.148Z",
          "wordCount": null,
          "title": "Adaptive Threshold for Online Object Recognition and Re-identification Tasks. (arXiv:2012.14305v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Behmanesh_M/0/1/0/all/0/1\">Maysam Behmanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adibi_P/0/1/0/all/0/1\">Peyman Adibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karshenas_H/0/1/0/all/0/1\">Hossein Karshenas</a>",
          "description": "Support vector machines (SVMs) are powerful supervised learning tools\ndeveloped to solve classification problems. However, SVMs are likely to perform\npoorly in the classification of imbalanced data. The rough set theory presents\na mathematical tool for inference in nondeterministic cases that provides\nmethods for removing irrelevant information from data. In this work, we propose\nan approach that efficiently used fuzzy rough set theory in weighted least\nsquares twin support vector machine called FRLSTSVM for classification of\nimbalanced data. The first innovation is introducing a new fuzzy rough\nset-based under-sampling strategy to make the classifier robust in terms of the\nimbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM,\ndata points from the minority class remain unchanged while a subset of data\npoints in the majority class are selected using a new method. In this model, we\nembed the weight biases in the LSTSVM formulations to overcome the bias\nphenomenon in the original twin SVM for the classification of imbalanced data.\nIn order to determine these weights in this formulation, we introduce a new\nstrategy that uses fuzzy rough set theory as the second innovation.\nExperimental results on the famous imbalanced datasets, compared to the related\ntraditional SVM-based methods, demonstrate the superiority of the proposed\nFRLSTSVM model in the imbalanced data classification.",
          "link": "http://arxiv.org/abs/2105.01198",
          "publishedOn": "2021-05-25T01:56:12.148Z",
          "wordCount": null,
          "title": "Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set Theory for Imbalanced Data Classification. (arXiv:2105.01198v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Co_K/0/1/0/all/0/1\">Kenneth T. Co</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Gonzalez_L/0/1/0/all/0/1\">Luis Mu&#xf1;oz-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanthan_L/0/1/0/all/0/1\">Leslie Kanthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1\">Emil C. Lupu</a>",
          "description": "Universal Adversarial Perturbations (UAPs) are a prominent class of\nadversarial examples that exploit the systemic vulnerabilities and enable\nphysically realizable and robust attacks against Deep Neural Networks (DNNs).\nUAPs generalize across many different inputs; this leads to realistic and\neffective attacks that can be applied at scale. In this paper we propose\nHyperNeuron, an efficient and scalable algorithm that allows for the real-time\ndetection of UAPs by identifying suspicious neuron hyper-activations. Our\nresults show the effectiveness of HyperNeuron on multiple tasks (image\nclassification, object detection), against a wide variety of universal attacks,\nand in realistic scenarios, like perceptual ad-blocking and adversarial\npatches. HyperNeuron is able to simultaneously detect both adversarial mask and\npatch UAPs with comparable or better performance than existing UAP defenses\nwhilst introducing a significantly reduced latency of only 0.86 milliseconds\nper image. This suggests that many realistic and practical universal attacks\ncan be reliably mitigated in real-time, which shows promise for the robust\ndeployment of machine learning systems.",
          "link": "http://arxiv.org/abs/2105.07334",
          "publishedOn": "2021-05-25T01:56:12.138Z",
          "wordCount": null,
          "title": "Real-time Detection of Practical Universal Adversarial Perturbations. (arXiv:2105.07334v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jincheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Channel pruning and tensor decomposition have received extensive attention in\nconvolutional neural network compression. However, these two techniques are\ntraditionally deployed in an isolated manner, leading to significant accuracy\ndrop when pursuing high compression rates. In this paper, we propose a\nCollaborative Compression (CC) scheme, which joints channel pruning and tensor\ndecomposition to compress CNN models by simultaneously learning the model\nsparsity and low-rankness. Specifically, we first investigate the compression\nsensitivity of each layer in the network, and then propose a Global Compression\nRate Optimization that transforms the decision problem of compression rate into\nan optimization problem. After that, we propose multi-step heuristic\ncompression to remove redundant compression units step-by-step, which fully\nconsiders the effect of the remaining compression space (i.e., unremoved\ncompression units). Our method demonstrates superior performance gains over\nprevious ones on various datasets and backbone architectures. For example, we\nachieve 52.9% FLOPs reduction by removing 48.4% parameters on ResNet-50 with\nonly a Top-1 accuracy drop of 0.56% on ImageNet 2012.",
          "link": "http://arxiv.org/abs/2105.11228",
          "publishedOn": "2021-05-25T01:56:12.137Z",
          "wordCount": null,
          "title": "Towards Compact CNNs via Collaborative Compression. (arXiv:2105.11228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yusheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Juan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stilla_U/0/1/0/all/0/1\">Uwe Stilla</a>",
          "description": "We tackle the problem of place recognition from point cloud data and\nintroduce a self-attention and orientation encoding network (SOE-Net) that\nfully explores the relationship between points and incorporates long-range\ncontext into point-wise local descriptors. Local information of each point from\neight orientations is captured in a PointOE module, whereas long-range feature\ndependencies among local descriptors are captured with a self-attention unit.\nMoreover, we propose a novel loss function called Hard Positive Hard Negative\nquadruplet loss (HPHN quadruplet), that achieves better performance than the\ncommonly used metric learning loss. Experiments on various benchmark datasets\ndemonstrate superior performance of the proposed network over the current\nstate-of-the-art approaches. Our code is released publicly at\nhttps://github.com/Yan-Xia/SOE-Net.",
          "link": "http://arxiv.org/abs/2011.12430",
          "publishedOn": "2021-05-25T01:56:12.137Z",
          "wordCount": null,
          "title": "SOE-Net: A Self-Attention and Orientation Encoding Network for Point Cloud based Place Recognition. (arXiv:2011.12430v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaofeng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Gege Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shaokai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>",
          "description": "Recent advances on Vision Transformers (ViT) have shown that\nself-attention-based networks, which take advantage of long-range dependencies\nmodeling ability, surpassed traditional convolution neural networks (CNNs) in\nmost vision tasks. To further expand the applicability for computer vision,\nmany improved variants are proposed to re-design the Transformer architecture\nby considering the superiority of CNNs, i.e., locality, translation invariance,\nfor better performance. However, these methods only consider the standard\naccuracy or computation cost of the model. In this paper, we rethink the design\nprinciples of ViTs based on the robustness. We found some design components\ngreatly harm the robustness and generalization ability of ViTs while some\nothers are beneficial. By combining the robust design components, we propose\nRobust Vision Transformer (RVT). RVT is a new vision transformer, which has\nsuperior performance and strong robustness. We further propose two new\nplug-and-play techniques called position-aware attention rescaling and\npatch-wise augmentation to train our RVT. The experimental results on ImageNet\nand six robustness benchmarks show the advanced robustness and generalization\nability of RVT compared with previous Transformers and state-of-the-art CNNs.\nOur RVT-S* also achieves Top-1 rank on multiple robustness leaderboards\nincluding ImageNet-C and ImageNet-Sketch. The code will be available at\nhttps://github.com/vtddggg/Robust-Vision-Transformer.",
          "link": "http://arxiv.org/abs/2105.07926",
          "publishedOn": "2021-05-25T01:56:12.137Z",
          "wordCount": null,
          "title": "Rethinking the Design Principles of Robust Vision Transformer. (arXiv:2105.07926v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuangjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaofei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_N/0/1/0/all/0/1\">Nihang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>",
          "description": "Monocular 3D human pose estimation from a single RGB image has received a lot\nattentions in the past few year. Pose inference models with competitive\nperformance however require supervision with 3D pose ground truth data or at\nleast known pose priors in their target domain. Yet, these data requirements in\nmany real-world applications with data collection constraints may not be\nachievable. In this paper, we present a heuristic weakly supervised solution,\ncalled HW-HuP to estimate 3D human pose in contexts that no ground truth 3D\ndata is accessible, even for fine-tuning. HW-HuP learns partial pose priors\nfrom public 3D human pose datasets and uses easy-to-access observations from\nthe target domain to iteratively estimate 3D human pose and shape in an\noptimization and regression hybrid cycle. In our design, depth data as an\nauxiliary information is employed as weak supervision during training, yet it\nis not needed for the inference. We evaluate HW-HuP performance qualitatively\non datasets of both in-bed human and infant poses, where no ground truth 3D\npose is provided neither any target prior. We also test HW-HuP performance\nquantitatively on a publicly available motion capture dataset against the 3D\nground truth. HW-HuP is also able to be extended to other input modalities for\npose estimation tasks especially under adverse vision conditions, such as\nocclusion or full darkness. On the Human3.6M benchmark, HW-HuP shows 104.1mm in\nMPJPE and 50.4mm in PA MPJPE, comparable to the existing state-of-the-art\napproaches that benefit from full 3D pose supervision.",
          "link": "http://arxiv.org/abs/2105.10996",
          "publishedOn": "2021-05-25T01:56:12.136Z",
          "wordCount": 691,
          "title": "Heuristic Weakly Supervised 3D Human Pose Estimation in Novel Contexts without Any 3D Pose Ground Truth. (arXiv:2105.10996v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ron_R/0/1/0/all/0/1\">Roey Ron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbaz_G/0/1/0/all/0/1\">Gil Elbaz</a>",
          "description": "We show a straightforward and useful methodology for performing instance\nsegmentation using synthetic data. We apply this methodology on a basic case\nand derived insights through quantitative analysis. We created a new public\ndataset: The Expo Markers Dataset intended for detection and segmentation\ntasks. This dataset contains 5,000 synthetic photorealistic images with their\ncorresponding pixel-perfect segmentation ground truth. The goal is to achieve\nhigh performance on manually-gathered and annotated real-world data of custom\nobjects. We do that by creating 3D models of the target objects and other\npossible distraction objects and place them within a simulated environment.\nExpo Markers were chosen for this task, fitting our requirements of a custom\nobject due to the exact texture, size and 3D shape. An additional advantage is\nthe availability of this object in offices around the world for easy testing\nand validation of our results. We generate the data using a domain\nrandomization technique that also simulates other photorealistic objects in the\nscene, known as distraction objects. These objects provide visual complexity,\nocclusions, and lighting challenges to help our model gain robustness in\ntraining. We are also releasing our manually-gathered datasets used for\ncomparison and evaluation of our synthetic dataset. This white-paper provides\nstrong evidence that photorealistic simulated data can be used in practical\nreal world applications as a more scalable and flexible solution than\nmanually-captured data. Code is available at the following address:\nhttps://github.com/DataGenResearchTeam/expo_markers",
          "link": "http://arxiv.org/abs/2007.14354",
          "publishedOn": "2021-05-25T01:56:12.136Z",
          "wordCount": null,
          "title": "Detection and Segmentation of Custom Objects using High Distraction Photorealistic Synthetic Data. (arXiv:2007.14354v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.03824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yongqiang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1\">Ming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yepang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1\">Shing-Chi Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>",
          "description": "Deep learning models are widely used for image analysis. While they offer\nhigh performance in terms of accuracy, people are concerned about if these\nmodels inappropriately make inferences using irrelevant features that are not\nencoded from the target object in a given image. To address the concern, we\npropose a metamorphic testing approach that assesses if a given inference is\nmade based on irrelevant features. Specifically, we propose two novel\nmetamorphic relations to detect such inappropriate inferences. We applied our\napproach to 10 image classification models and 10 object detection models, with\nthree large datasets, i.e., ImageNet, COCO, and Pascal VOC. Over 5.3% of the\ntop-5 correct predictions made by the image classification models are subject\nto inappropriate inferences using irrelevant features. The corresponding rate\nfor the object detection models is over 8.5%. Based on the findings, we further\ndesigned a new image generation strategy that can effectively attack existing\nmodels. Comparing with a baseline approach, our strategy can double the success\nrate of attacks.",
          "link": "http://arxiv.org/abs/1909.03824",
          "publishedOn": "2021-05-25T01:56:12.116Z",
          "wordCount": null,
          "title": "Testing Deep Learning Models for Image Analysis Using Object-Relevant Metamorphic Relations. (arXiv:1909.03824v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03847",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zeng_H/0/1/0/all/0/1\">Hong-Ye Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Song-Han Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yu-Chong Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_D/0/1/0/all/0/1\">De-Sen Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1\">Kang Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xu-Ming He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lou_E/0/1/0/all/0/1\">Edmond Lou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>",
          "description": "Objective: The spinous process angle (SPA) is one of the essential parameters\nto denote three-dimensional (3-D) deformity of spine. We propose an automatic\nsegmentation method based on Stacked Hourglass Network (SHN) to detect the\nspinous processes (SP) on ultrasound (US) spine images and to measure the SPAs\nof clinical scoliotic subjects. Methods: The network was trained to detect\nvertebral SP and laminae as five landmarks on 1200 ultrasound transverse images\nand validated on 100 images. All the processed transverse images with\nhighlighted SP and laminae were reconstructed into a 3D image volume, and the\nSPAs were measured on the projected coronal images. The trained network was\ntested on 400 images by calculating the percentage of correct keypoints (PCK);\nand the SPA measurements were evaluated on 50 scoliotic subjects by comparing\nthe results from US images and radiographs. Results: The trained network\nachieved a high average PCK (86.8%) on the test datasets, particularly the PCK\nof SP detection was 90.3%. The SPAs measured from US and radiographic methods\nshowed good correlation (r>0.85), and the mean absolute differences (MAD)\nbetween two modalities were 3.3{\\deg}, which was less than the clinical\nacceptance error (5{\\deg}). Conclusion: The vertebral features can be\naccurately segmented on US spine images using SHN, and the measurement results\nof SPA from US data was comparable to the gold standard from radiography.",
          "link": "http://arxiv.org/abs/2105.03847",
          "publishedOn": "2021-05-25T01:56:12.116Z",
          "wordCount": null,
          "title": "Automatic segmentation of vertebral features on ultrasound spine images using Stacked Hourglass Network. (arXiv:2105.03847v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matias_A/0/1/0/all/0/1\">Andr&#xe9; Vict&#xf3;ria Matias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amorim_J/0/1/0/all/0/1\">Jo&#xe3;o Gustavo Atkinson Amorim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macarini_L/0/1/0/all/0/1\">Luiz Antonio Buschetto Macarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerentini_A/0/1/0/all/0/1\">Allan Cerentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onofre_A/0/1/0/all/0/1\">Alexandre Sherlley Casimiro Onofre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onofre_F/0/1/0/all/0/1\">Fabiana Botelho de Miranda Onofre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daltoe_F/0/1/0/all/0/1\">Felipe Perozzo Dalto&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_M/0/1/0/all/0/1\">Marcelo Ricardo Stemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wangenheim_A/0/1/0/all/0/1\">Aldo von Wangenheim</a>",
          "description": "Cytology is a low-cost and non-invasive diagnostic procedure employed to\nsupport the diagnosis of a broad range of pathologies. Computer Vision\ntechnologies, by automatically generating quantitative and objective\ndescriptions of examinations' contents, can help minimize the chances of\nmisdiagnoses and shorten the time required for analysis. To identify the\nstate-of-art of computer vision techniques currently applied to cytology, we\nconducted a Systematic Literature Review. We analyzed papers published in the\nlast 5 years. The initial search was executed in September 2020 and resulted in\n431 articles. After applying the inclusion/exclusion criteria, 157 papers\nremained, which we analyzed to build a picture of the tendencies and problems\npresent in this research area, highlighting the computer vision methods,\nstaining techniques, evaluation metrics, and the availability of the used\ndatasets and computer code. As a result, we identified that the most used\nmethods in the analyzed works are deep learning-based (70 papers), while fewer\nworks employ classic computer vision only (101 papers). The most recurrent\nmetric used for classification and object detection was the accuracy (33 papers\nand 5 papers), while for segmentation it was the Dice Similarity Coefficient\n(38 papers). Regarding staining techniques, Papanicolaou was the most employed\none (130 papers), followed by H&E (20 papers) and Feulgen (5 papers). Twelve of\nthe datasets used in the papers are publicly available, with the DTU/Herlev\ndataset being the most used one. We conclude that there still is a lack of\nhigh-quality datasets for many types of stains and most of the works are not\nmature enough to be applied in a daily clinical diagnostic routine. We also\nidentified a growing tendency towards adopting deep learning-based approaches\nas the methods of choice.",
          "link": "http://arxiv.org/abs/2105.11277",
          "publishedOn": "2021-05-25T01:56:12.115Z",
          "wordCount": null,
          "title": "What is the State of the Art of Computer Vision-Assisted Cytology? A Systematic Literature Review. (arXiv:2105.11277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sumeet S. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karayev_S/0/1/0/all/0/1\">Sergey Karayev</a>",
          "description": "We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.",
          "link": "http://arxiv.org/abs/2103.06450",
          "publishedOn": "2021-05-25T01:56:12.115Z",
          "wordCount": null,
          "title": "Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Existing methods for vision-and-language learning typically require designing\ntask-specific architectures and objectives for each task. For example, a\nmulti-label answer classifier for visual question answering, a region scorer\nfor referring expression comprehension, and a language decoder for image\ncaptioning, etc. To alleviate these hassles, in this work, we propose a unified\nframework that learns different tasks in a single architecture with the same\nlanguage modeling objective, i.e., multimodal conditional text generation,\nwhere our models learn to generate labels in text based on the visual and\ntextual inputs. On 7 popular vision-and-language benchmarks, including visual\nquestion answering, referring expression comprehension, visual commonsense\nreasoning, most of which have been previously modeled as discriminative tasks,\nour generative approach (with a single unified architecture) reaches comparable\nperformance to recent task-specific state-of-the-art vision-and-language\nmodels. Moreover, our generative approach shows better generalization ability\non questions that have rare answers. Also, we show that our framework allows\nmulti-task learning in a single architecture with a single set of parameters,\nachieving similar performance to separately optimized single-task models. Our\ncode is publicly available at: https://github.com/j-min/VL-T5",
          "link": "http://arxiv.org/abs/2102.02779",
          "publishedOn": "2021-05-25T01:56:12.114Z",
          "wordCount": null,
          "title": "Unifying Vision-and-Language Tasks via Text Generation. (arXiv:2102.02779v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kitayama_M/0/1/0/all/0/1\">Masaki Kitayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>",
          "description": "In this paper, we propose a method for generating visually protected images,\nreferred to as gradient-preserving images. The protected images allow us to\ndirectly extract Histogram-of-Oriented-Gradients (HOG) features for\nprivacy-preserving machine learning. In an experiment, HOG features extracted\nfrom gradient-preserving images are applied to a face recognition algorithm to\ndemonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2104.01350",
          "publishedOn": "2021-05-25T01:56:12.113Z",
          "wordCount": null,
          "title": "Generation of Gradient-Preserving Images allowing HOG Feature Extraction. (arXiv:2104.01350v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Ziang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>",
          "description": "For visual object recognition tasks, the illumination variations can cause\ndistinct changes in object appearance and thus confuse the deep neural network\nbased recognition models. Especially for some rare illumination conditions,\ncollecting sufficient training samples could be time-consuming and expensive.\nTo solve this problem, in this paper we propose a novel neural network\narchitecture called Separating-Illumination Network (Sill-Net). Sill-Net learns\nto separate illumination features from images, and then during training we\naugment training samples with these separated illumination features in the\nfeature space. Experimental results demonstrate that our approach outperforms\ncurrent state-of-the-art methods in several object classification benchmarks.",
          "link": "http://arxiv.org/abs/2102.03539",
          "publishedOn": "2021-05-25T01:56:12.111Z",
          "wordCount": null,
          "title": "Sill-Net: Feature Augmentation with Separated Illumination Representation. (arXiv:2102.03539v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Junbin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xindi Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "We introduce NExT-QA, a rigorously designed video question answering\n(VideoQA) benchmark to advance video understanding from describing to\nexplaining the temporal actions. Based on the dataset, we set up multi-choice\nand open-ended QA tasks targeting causal action reasoning, temporal action\nreasoning, and common scene comprehension. Through extensive analysis of\nbaselines and established VideoQA techniques, we find that top-performing\nmethods excel at shallow scene descriptions but are weak in causal and temporal\naction reasoning. Furthermore, the models that are effective on multi-choice\nQA, when adapted to open-ended QA, still struggle in generalizing the answers.\nThis raises doubt on the ability of these models to reason and highlights\npossibilities for improvement. With detailed results for different question\ntypes and heuristic observations for future works, we hope NExT-QA will guide\nthe next generation of VQA research to go beyond superficial scene description\ntowards a deeper understanding of videos. (The dataset and related resources\nare available at https://github.com/doc-doc/NExT-QA.git)",
          "link": "http://arxiv.org/abs/2105.08276",
          "publishedOn": "2021-05-25T01:56:12.111Z",
          "wordCount": null,
          "title": "NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions. (arXiv:2105.08276v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Glover_A/0/1/0/all/0/1\">Arren Glover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinale_A/0/1/0/all/0/1\">Aiko Dinale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_L/0/1/0/all/0/1\">Leandro De Souza Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamford_S/0/1/0/all/0/1\">Simeon Bamford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolozzi_C/0/1/0/all/0/1\">Chiara Bartolozzi</a>",
          "description": "There have been a number of corner detection methods proposed for event\ncameras in the last years, since event-driven computer vision has become more\naccessible. Current state-of-the-art have either unsatisfactory accuracy or\nreal-time performance when considered for practical use; random motion using a\nlive camera in an unconstrained environment. In this paper, we present yet\nanother method to perform corner detection, dubbed look-up event-Harris\n(luvHarris), that employs the Harris algorithm for high accuracy but manages an\nimproved event throughput. Our method has two major contributions, 1. a novel\n\"threshold ordinal event-surface\" that removes certain tuning parameters and is\nwell suited for Harris operations, and 2. an implementation of the Harris\nalgorithm such that the computational load per-event is minimised and\ncomputational heavy convolutions are performed only 'as-fast-as-possible', i.e.\nonly as computational resources are available. The result is a practical,\nreal-time, and robust corner detector that runs more than $2.6\\times$ the speed\nof current state-of-the-art; a necessity when using high-resolution\nevent-camera in real-time. We explain the considerations taken for the\napproach, compare the algorithm to current state-of-the-art in terms of\ncomputational performance and detection accuracy, and discuss the validity of\nthe proposed approach for event cameras.",
          "link": "http://arxiv.org/abs/2105.11443",
          "publishedOn": "2021-05-25T01:56:12.110Z",
          "wordCount": null,
          "title": "luvHarris: A Practical Corner Detector for Event-cameras. (arXiv:2105.11443v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zitian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yulu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Lejian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yue Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Guanghui Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>",
          "description": "Vision and language understanding techniques have achieved remarkable\nprogress, but currently it is still difficult to well handle problems involving\nvery fine-grained details. For example, when the robot is told to \"bring me the\nbook in the girl's left hand\", most existing methods would fail if the girl\nholds one book respectively in her left and right hand. In this work, we\nintroduce a new task named human-centric relation segmentation (HRS), as a\nfine-grained case of HOI-det. HRS aims to predict the relations between the\nhuman and surrounding entities and identify the relation-correlated human\nparts, which are represented as pixel-level masks. For the above exemplar case,\nour HRS task produces results in the form of relation triplets <girl [left\nhand], hold, book> and exacts segmentation masks of the book, with which the\nrobot can easily accomplish the grabbing task. Correspondingly, we collect a\nnew Person In Context (PIC) dataset for this new task, which contains 17,122\nhigh-resolution images and densely annotated entity segmentation and relations,\nincluding 141 object categories, 23 relation categories and 25 semantic human\nparts. We also propose a Simultaneous Matching and Segmentation (SMS) framework\nas a solution to the HRS task. I Outputs of the three branches are fused to\nproduce the final HRS results. Extensive experiments on PIC and V-COCO datasets\nshow that the proposed SMS method outperforms baselines with the 36 FPS\ninference speed.",
          "link": "http://arxiv.org/abs/2105.11168",
          "publishedOn": "2021-05-25T01:56:12.103Z",
          "wordCount": null,
          "title": "Human-centric Relation Segmentation: Dataset and Solution. (arXiv:2105.11168v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yu-Cheng Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Existing video polyp segmentation (VPS) models typically employ convolutional\nneural networks (CNNs) to extract features. However, due to their limited\nreceptive fields, CNNs can not fully exploit the global temporal and spatial\ninformation in successive video frames, resulting in false-positive\nsegmentation results. In this paper, we propose the novel PNS-Net\n(Progressively Normalized Self-attention Network), which can efficiently learn\nrepresentations from polyp videos with real-time speed (~140fps) on a single\nRTX 2080 GPU and no post-processing. Our PNS-Net is based solely on a basic\nnormalized self-attention block, equipping with recurrence and CNNs entirely.\nExperiments on challenging VPS datasets demonstrate that the proposed PNS-Net\nachieves state-of-the-art performance. We also conduct extensive experiments to\nstudy the effectiveness of the channel split, soft-attention, and progressive\nlearning strategy. We find that our PNS-Net works well under different\nsettings, making it a promising solution to the VPS task.",
          "link": "http://arxiv.org/abs/2105.08468",
          "publishedOn": "2021-05-25T01:56:12.102Z",
          "wordCount": null,
          "title": "Progressively Normalized Self-Attention Network for Video Polyp Segmentation. (arXiv:2105.08468v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1\">Aditya Challa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1\">Sravan Danda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagar_B/0/1/0/all/0/1\">B.S.Daya Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1\">Laurent Najman</a>",
          "description": "Hyperspectral images (HSI) consist of rich spatial and spectral information,\nwhich can potentially be used for several applications. However, noise, band\ncorrelations and high dimensionality restrict the applicability of such data.\nThis is recently addressed using creative deep learning network architectures\nsuch as ResNet, SSRN, and A2S2K. However, the last layer, i.e the\nclassification layer, remains unchanged and is taken to be the softmax\nclassifier. In this article, we propose to use a watershed classifier.\nWatershed classifier extends the watershed operator from Mathematical\nMorphology for classification. In its vanilla form, the watershed classifier\ndoes not have any trainable parameters. In this article, we propose a novel\napproach to train deep learning networks to obtain representations suitable for\nthe watershed classifier. The watershed classifier exploits the connectivity\npatterns, a characteristic of HSI datasets, for better inference. We show that\nexploiting such characteristics allows the Triplet-Watershed to achieve\nstate-of-art results in supervised and semi-supervised contexts. These results\nare validated on Indianpines (IP), University of Pavia (UP), Kennedy Space\nCenter (KSC) and University of Houston (UH) datasets, relying on simple convnet\narchitecture using a quarter of parameters compared to previous\nstate-of-the-art networks.",
          "link": "http://arxiv.org/abs/2103.09384",
          "publishedOn": "2021-05-25T01:56:12.100Z",
          "wordCount": null,
          "title": "Triplet-Watershed for Hyperspectral Image Classification. (arXiv:2103.09384v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Baorui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>",
          "description": "Reconstructing continuous surfaces from 3D point clouds is a fundamental\noperation in 3D geometry processing. Several recent state-of-the-art methods\naddress this problem using neural networks to learn signed distance functions\n(SDFs). In this paper, we introduce \\textit{Neural-Pull}, a new approach that\nis simple and leads to high quality SDFs. Specifically, we train a neural\nnetwork to pull query 3D locations to their closest points on the surface using\nthe predicted signed distance values and the gradient at the query locations,\nboth of which are computed by the network itself. The pulling operation moves\neach query location with a stride given by the distance predicted by the\nnetwork. Based on the sign of the distance, this may move the query location\nalong or against the direction of the gradient of the SDF. This is a\ndifferentiable operation that allows us to update the signed distance value and\nthe gradient simultaneously during training. Our outperforming results under\nwidely used benchmarks demonstrate that we can learn SDFs more accurately and\nflexibly for surface reconstruction and single image reconstruction than the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2011.13495",
          "publishedOn": "2021-05-25T01:56:12.095Z",
          "wordCount": null,
          "title": "Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces. (arXiv:2011.13495v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhihao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingfu Zhang</a>",
          "description": "Deep subspace clustering networks have attracted much attention in subspace\nclustering, in which an auto-encoder non-linearly maps the input data into a\nlatent space, and a fully connected layer named self-expressiveness module is\nintroduced to learn the affinity matrix via a typical regularization term\n(e.g., sparse or low-rank). However, the adopted regularization terms ignore\nthe connectivity within each subspace, limiting their clustering performance.\nIn addition, the adopted framework suffers from the coupling issue between the\nauto-encoder module and the self-expressiveness module, making the network\ntraining non-trivial. To tackle these two issues, we propose a novel deep\nsubspace clustering method named Maximum Entropy Subspace Clustering Network\n(MESC-Net). Specifically, MESC-Net maximizes the entropy of the affinity matrix\nto promote the connectivity within each subspace, in which its elements\ncorresponding to the same subspace are uniformly and densely distributed.\nFurthermore, we design a novel framework to explicitly decouple the\nauto-encoder module and the self-expressiveness module. We also theoretically\nprove that the learned affinity matrix satisfies the block-diagonal property\nunder the independent subspaces. Extensive quantitative and qualitative results\non commonly used benchmark datasets validate MESC-Net significantly outperforms\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2012.03176",
          "publishedOn": "2021-05-25T01:56:12.095Z",
          "wordCount": null,
          "title": "Maximum Entropy Subspace Clustering Network. (arXiv:2012.03176v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1\">Teng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Haocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Learning discriminative representation using large-scale face datasets in the\nwild is crucial for real-world applications, yet it remains challenging. The\ndifficulties lie in many aspects and this work focus on computing resource\nconstraint and long-tailed class distribution. Recently, classification-based\nrepresentation learning with deep neural networks and well-designed losses have\ndemonstrated good recognition performance. However, the computing and memory\ncost linearly scales up to the number of identities (classes) in the training\nset, and the learning process suffers from unbalanced classes. In this work, we\npropose a dynamic class queue (DCQ) to tackle these two problems. Specifically,\nfor each iteration during training, a subset of classes for recognition are\ndynamically selected and their class weights are dynamically generated\non-the-fly which are stored in a queue. Since only a subset of classes is\nselected for each iteration, the computing requirement is reduced. By using a\nsingle server without model parallel, we empirically verify in large-scale\ndatasets that 10% of classes are sufficient to achieve similar performance as\nusing all classes. Moreover, the class weights are dynamically generated in a\nfew-shot manner and therefore suitable for tail classes with only a few\ninstances. We show clear improvement over a strong baseline in the largest\npublic dataset Megaface Challenge2 (MF2) which has 672K identities and over 88%\nof them have less than 10 instances. Code is available at\nhttps://github.com/bilylee/DCQ",
          "link": "http://arxiv.org/abs/2105.11113",
          "publishedOn": "2021-05-25T01:56:12.085Z",
          "wordCount": null,
          "title": "Dynamic Class Queue for Large Scale Face Recognition In the Wild. (arXiv:2105.11113v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04525",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1\">Michael Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1\">Evis Sala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1\">Leonardo Rundo</a>",
          "description": "Automatic segmentation methods are an important advancement in medical image\nanalysis. Machine learning techniques, and deep neural networks in particular,\nare the state-of-the-art for most medical image segmentation tasks. Issues with\nclass imbalance pose a significant challenge in medical datasets, with lesions\noften occupying a considerably smaller volume relative to the background. Loss\nfunctions used in the training of deep learning algorithms differ in their\nrobustness to class imbalance, with direct consequences for model convergence.\nThe most commonly used loss functions for segmentation are based on either the\ncross entropy loss, Dice loss or a combination of the two. We propose a Unified\nFocal loss, a new framework that generalises Dice and cross entropy-based\nlosses for handling class imbalance. We evaluate our proposed loss function on\nthree highly class imbalanced, publicly available medical imaging datasets:\nBreast Ultrasound 2017 (BUS2017), Brain Tumour Segmentation 2020 (BraTS20) and\nKidney Tumour Segmentation 2019 (KiTS19). We compare our loss function\nperformance against six Dice or cross entropy-based loss functions, and\ndemonstrate that our proposed loss function is robust to class imbalance,\noutperforming the other loss functions across datasets. Finally, we use the\nUnified Focal loss together with deep supervision to achieve state-of-the-art\nresults without modification of the original U-Net architecture, with a mean\nDice similarity coefficient (DSC)=0.948 on BUS2017, enhancing tumour region\nDSC=0.800 on BraTS20 and kidney tumour DSC=0.758 on KiTS19. This highlights the\nimportance of carefully selecting a suitable loss function prior to the use of\nmore complex architectures.",
          "link": "http://arxiv.org/abs/2102.04525",
          "publishedOn": "2021-05-25T01:56:12.084Z",
          "wordCount": null,
          "title": "Unified Focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation. (arXiv:2102.04525v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aygunes_B/0/1/0/all/0/1\">Bulut Aygunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_S/0/1/0/all/0/1\">Selim Aksoy</a>",
          "description": "Multisource image analysis that leverages complementary spectral, spatial,\nand structural information benefits fine-grained object recognition that aims\nto classify an object into one of many similar subcategories. However, for\nmultisource tasks that involve relatively small objects, even the smallest\nregistration errors can introduce high uncertainty in the classification\nprocess. We approach this problem from a weakly supervised learning perspective\nin which the input images correspond to larger neighborhoods around the\nexpected object locations where an object with a given class label is present\nin the neighborhood without any knowledge of its exact location. The proposed\nmethod uses a single-source deep instance attention model with parallel\nbranches for joint localization and classification of objects, and extends this\nmodel into a multisource setting where a reference source that is assumed to\nhave no location uncertainty is used to aid the fusion of multiple sources in\nfour different levels: probability level, logit level, feature level, and pixel\nlevel. We show that all levels of fusion provide higher accuracies compared to\nthe state-of-the-art, with the best performing method of feature-level fusion\nresulting in 53% accuracy for the recognition of 40 different types of trees,\ncorresponding to an improvement of 5.7% over the best performing baseline when\nRGB, multispectral, and LiDAR data are used. We also provide an in-depth\ncomparison by evaluating each model at various parameter complexity settings,\nwhere the increased model capacity results in a further improvement of 6.3%\nover the default capacity setting.",
          "link": "http://arxiv.org/abs/2105.10983",
          "publishedOn": "2021-05-25T01:56:12.083Z",
          "wordCount": null,
          "title": "Weakly Supervised Instance Attention for Multisource Fine-Grained Object Recognition. (arXiv:2105.10983v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingmei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1\">Le Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Point cloud semantic segmentation is a crucial task in 3D scene\nunderstanding. Existing methods mainly focus on employing a large number of\nannotated labels for supervised semantic segmentation. Nonetheless, manually\nlabeling such large point clouds for the supervised segmentation task is\ntime-consuming. In order to reduce the number of annotated labels, we propose a\nsemi-supervised semantic point cloud segmentation network, named SSPC-Net,\nwhere we train the semantic segmentation network by inferring the labels of\nunlabeled points from the few annotated 3D points. In our method, we first\npartition the whole point cloud into superpoints and build superpoint graphs to\nmine the long-range dependencies in point clouds. Based on the constructed\nsuperpoint graph, we then develop a dynamic label propagation method to\ngenerate the pseudo labels for the unsupervised superpoints. Particularly, we\nadopt a superpoint dropout strategy to dynamically select the generated pseudo\nlabels. In order to fully exploit the generated pseudo labels of the\nunsupervised superpoints, we furthermore propose a coupled attention mechanism\nfor superpoint feature embedding. Finally, we employ the cross-entropy loss to\ntrain the semantic segmentation network with the labels of the supervised\nsuperpoints and the pseudo labels of the unsupervised superpoints. Experiments\non various datasets demonstrate that our semi-supervised segmentation method\ncan achieve better performance than the current semi-supervised segmentation\nmethod with fewer annotated 3D points. Our code is available at\nhttps://github.com/MMCheng/SSPC-Net.",
          "link": "http://arxiv.org/abs/2104.07861",
          "publishedOn": "2021-05-25T01:56:12.083Z",
          "wordCount": null,
          "title": "SSPC-Net: Semi-supervised Semantic 3D Point Cloud Segmentation Network. (arXiv:2104.07861v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chaonan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shaohua Zheng</a>",
          "description": "Purpose: Colorectal cancer (CRC) is the second most common cause of cancer\nmortality worldwide. Colonoscopy is a widely used technique for colon screening\nand polyp lesions diagnosis. Nevertheless, manual screening using colonoscopy\nsuffers from a substantial miss rate of polyps and is an overwhelming burden\nfor endoscopists. Computer-aided diagnosis (CAD) for polyp detection has the\npotential to reduce human error and human burden. However, current polyp\ndetection methods based on object detection framework need many handcrafted\npre-processing and post-processing operations or user guidance that require\ndomain-specific knowledge.\n\nMethods: In this paper, we propose a convolution in transformer (COTR)\nnetwork for end-to-end polyp detection. Motivated by the detection transformer\n(DETR), COTR is constituted by a CNN for feature extraction, transformer\nencoder layers interleaved with convolutional layers for feature encoding and\nrecalibration, transformer decoder layers for object querying, and a\nfeed-forward network for detection prediction. Considering the slow convergence\nof DETR, COTR embeds convolution layers into transformer encoder for feature\nreconstruction and convergence acceleration.\n\nResults: Experimental results on two public polyp datasets show that COTR\nachieved 91.49\\% precision, 82.69% sensitivity, and 86.87% F1-score on the\nETIS-LARIB, and 91.67% precision, 93.54% sensitivity, and 92.60% F1-score on\nthe CVC-ColonDB.\n\nConclusion: This study proposed an end to end detection method based on\ndetection transformer for colorectal polyp detection. Experimental results on\nETIS-LARIB and CVC-ColonDB dataset demonstrated that the proposed model\nachieved comparable performance against state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.10925",
          "publishedOn": "2021-05-25T01:56:12.081Z",
          "wordCount": null,
          "title": "COTR: Convolution in Transformer Network for End to End Polyp Detection. (arXiv:2105.10925v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Recently, query based object detection frameworks achieve comparable\nperformance with previous state-of-the-art object detectors. However, how to\nfully leverage such frameworks to perform instance segmentation remains an open\nproblem. In this paper, we present QueryInst (Instances as Queries), a query\nbased instance segmentation method driven by parallel supervision on dynamic\nmask heads. The key insight of QueryInst is to leverage the intrinsic\none-to-one correspondence in object queries across different stages, as well as\none-to-one correspondence between mask RoI features and object queries in the\nsame stage. This approach eliminates the explicit multi-stage mask head\nconnection and the proposal distribution inconsistency issues inherent in\nnon-query based multi-stage instance segmentation methods. We conduct extensive\nexperiments on three challenging benchmarks, i.e., COCO, CityScapes, and\nYouTube-VIS to evaluate the effectiveness of QueryInst in instance segmentation\nand video instance segmentation (VIS) task. Specifically, using ResNet-101-FPN\nbackbone, QueryInst obtains 48.1 box AP and 42.8 mask AP on COCO test-dev,\nwhich is 2 points higher than HTC in terms of both box AP and mask AP, while\nruns 2.4 times faster. For video instance segmentation, QueryInst achieves the\nbest performance among all online VIS approaches and strikes a decent\nspeed-accuracy trade-off. Code is available at\n\\url{https://github.com/hustvl/QueryInst}.",
          "link": "http://arxiv.org/abs/2105.01928",
          "publishedOn": "2021-05-25T01:56:12.072Z",
          "wordCount": null,
          "title": "Instances as Queries. (arXiv:2105.01928v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattick_A/0/1/0/all/0/1\">Alexander Mattick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_M/0/1/0/all/0/1\">Martin Mayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seuret_M/0/1/0/all/0/1\">Mathias Seuret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christlein_V/0/1/0/all/0/1\">Vincent Christlein</a>",
          "description": "As of recent generative adversarial networks have allowed for big leaps in\nthe realism of generated images in diverse domains, not the least of which\nbeing handwritten text generation. The generation of realistic-looking\nhand-written text is important because it can be used for data augmentation in\nhandwritten text recognition (HTR) systems or human-computer interaction. We\npropose SmartPatch, a new technique increasing the performance of current\nstate-of-the-art methods by augmenting the training feedback with a tailored\nsolution to mitigate pen-level artifacts. We combine the well-known patch loss\nwith information gathered from the parallel trained handwritten text\nrecognition system and the separate characters of the word. This leads to a\nmore enhanced local discriminator and results in more realistic and\nhigher-quality generated handwritten words.",
          "link": "http://arxiv.org/abs/2105.10528",
          "publishedOn": "2021-05-25T01:56:12.070Z",
          "wordCount": null,
          "title": "SmartPatch: Improving Handwritten Word Imitation with Patch Discriminators. (arXiv:2105.10528v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10738",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Jin Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_C/0/1/0/all/0/1\">Chuan Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Junwei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Lio&#x27;</a>",
          "description": "Single image super-resolution (SISR) aims to obtain a high-resolution output\nfrom one low-resolution image. Currently, deep learning-based SISR approaches\nhave been widely discussed in medical image processing, because of their\npotential to achieve high-quality, high spatial resolution images without the\ncost of additional scans. However, most existing methods are designed for\nscale-specific SR tasks and are unable to generalise over magnification scales.\nIn this paper, we propose an approach for medical image arbitrary-scale\nsuper-resolution (MIASSR), in which we couple meta-learning with generative\nadversarial networks (GANs) to super-resolve medical images at any scale of\nmagnification in (1, 4]. Compared to state-of-the-art SISR algorithms on\nsingle-modal magnetic resonance (MR) brain images (OASIS-brains) and\nmulti-modal MR brain images (BraTS), MIASSR achieves comparable fidelity\nperformance and the best perceptual quality with the smallest model size. We\nalso employ transfer learning to enable MIASSR to tackle SR tasks of new\nmedical modalities, such as cardiac MR images (ACDC) and chest computed\ntomography images (COVID-CT). The source code of our work is also public. Thus,\nMIASSR has the potential to become a new foundational pre-/post-processing step\nin clinical image analysis tasks such as reconstruction, image quality\nenhancement, and segmentation.",
          "link": "http://arxiv.org/abs/2105.10738",
          "publishedOn": "2021-05-25T01:56:12.070Z",
          "wordCount": null,
          "title": "MIASSR: An Approach for Medical Image Arbitrary Scale Super-Resolution. (arXiv:2105.10738v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yijin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bangbang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "Visual localization is of great importance in robotics and computer vision.\nRecently, scene coordinate regression based methods have shown good performance\nin visual localization in small static scenes. However, it still estimates\ncamera poses from many inferior scene coordinates. To address this problem, we\npropose a novel visual localization framework that establishes 2D-to-3D\ncorrespondences between the query image and the 3D map with a series of\nlearnable scene-specific landmarks. In the landmark generation stage, the 3D\nsurfaces of the target scene are over-segmented into mosaic patches whose\ncenters are regarded as the scene-specific landmarks. To robustly and\naccurately recover the scene-specific landmarks, we propose the Voting with\nSegmentation Network (VS-Net) to segment the pixels into different landmark\npatches with a segmentation branch and estimate the landmark locations within\neach patch with a landmark location voting branch. Since the number of\nlandmarks in a scene may reach up to 5000, training a segmentation network with\nsuch a large number of classes is both computation and memory costly for the\ncommonly used cross-entropy loss. We propose a novel prototype-based triplet\nloss with hard negative mining, which is able to train semantic segmentation\nnetworks with a large number of labels efficiently. Our proposed VS-Net is\nextensively tested on multiple public benchmarks and can outperform\nstate-of-the-art visual localization methods. Code and models are available at\n\\href{https://github.com/zju3dv/VS-Net}{https://github.com/zju3dv/VS-Net}.",
          "link": "http://arxiv.org/abs/2105.10886",
          "publishedOn": "2021-05-25T01:56:12.068Z",
          "wordCount": null,
          "title": "VS-Net: Voting with Segmentation for Visual Localization. (arXiv:2105.10886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1\">Sandareka Wickramanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>",
          "description": "Learning concepts that are consistent with human perception is important for\nDeep Neural Networks to win end-user trust. Post-hoc interpretation methods\nlack transparency in the feature representations learned by the models. This\nwork proposes a guided learning approach with an additional concept layer in a\nCNN- based architecture to learn the associations between visual features and\nword phrases. We design an objective function that optimizes both prediction\naccuracy and semantics of the learned feature representations. Experiment\nresults demonstrate that the proposed model can learn concepts that are\nconsistent with human perception and their corresponding contributions to the\nmodel decision without compromising accuracy. Further, these learned concepts\nare transferable to new classes of objects that have similar concepts.",
          "link": "http://arxiv.org/abs/2101.03919",
          "publishedOn": "2021-05-25T01:56:12.067Z",
          "wordCount": null,
          "title": "Comprehensible Convolutional Neural Networks via Guided Concept Learning. (arXiv:2101.03919v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1\">Divya Kothandaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present an unsupervised adaptation approach for visual scene understanding\nin unstructured traffic environments. Our method is designed for unstructured\nreal-world scenarios with dense and heterogeneous traffic consisting of cars,\ntrucks, two-and three-wheelers, and pedestrians. We describe a new semantic\nsegmentation technique based on unsupervised domain adaptation (DA), that can\nidentify the class or category of each region in RGB images or videos. We also\npresent a novel self-training algorithm (Alt-Inc) for multi-source DA that\nimproves the accuracy. Our overall approach is a deep learning-based technique\nand consists of an unsupervised neural network that achieves 87.18% accuracy on\nthe challenging India Driving Dataset. Our method works well on roads that may\nnot be well-marked or may include dirt, unidentifiable debris, potholes, etc. A\nkey aspect of our approach is that it can also identify objects that are\nencountered by the model for the fist time during the testing phase. We compare\nour method against the state-of-the-art methods and show an improvement of\n5.17% - 42.9%. Furthermore, we also conduct user studies that qualitatively\nvalidate the improvements in visual scene understanding of unstructured driving\nenvironments.",
          "link": "http://arxiv.org/abs/2010.03523",
          "publishedOn": "2021-05-25T01:56:12.066Z",
          "wordCount": null,
          "title": "BoMuDANet: Unsupervised Adaptation for Visual Scene Understanding in Unstructured Driving Environments. (arXiv:2010.03523v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai-Kuang Ma</a>",
          "description": "Malicious application of deepfakes (i.e., technologies can generate target\nfaces or face attributes) has posed a huge threat to our society. The fake\nmultimedia content generated by deepfake models can harm the reputation and\neven threaten the property of the person who has been impersonated.\nFortunately, the adversarial watermark could be used for combating deepfake\nmodels, leading them to generate distorted images. The existing methods require\nan individual training process for every facial image, to generate the\nadversarial watermark against a specific deepfake model, which are extremely\ninefficient. To address this problem, we propose a universal adversarial attack\nmethod on deepfake models, to generate a Cross-Model Universal Adversarial\nWatermark (CMUA-Watermark) that can protect thousands of facial images from\nmultiple deepfake models. Specifically, we first propose a cross-model\nuniversal attack pipeline by attacking multiple deepfake models and combining\ngradients from these models iteratively. Then we introduce a batch-based method\nto alleviate the conflict of adversarial watermarks generated by different\nfacial images. Finally, we design a more reasonable and comprehensive\nevaluation method for evaluating the effectiveness of the adversarial\nwatermark. Experimental results demonstrate that the proposed CMUA-Watermark\ncan effectively distort the fake facial images generated by deepfake models and\nsuccessfully protect facial images from deepfakes in real scenes.",
          "link": "http://arxiv.org/abs/2105.10872",
          "publishedOn": "2021-05-25T01:56:12.061Z",
          "wordCount": 656,
          "title": "CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes. (arXiv:2105.10872v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Weizhi An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hehuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>",
          "description": "Recent studies imply that deep neural networks are vulnerable to adversarial\nexamples -- inputs with a slight but intentional perturbation are incorrectly\nclassified by the network. Such vulnerability makes it risky for some\nsecurity-related applications (e.g., semantic segmentation in autonomous cars)\nand triggers tremendous concerns on the model reliability. For the first time,\nwe comprehensively evaluate the robustness of existing UDA methods and propose\na robust UDA approach. It is rooted in two observations: (i) the robustness of\nUDA methods in semantic segmentation remains unexplored, which pose a security\nconcern in this field; and (ii) although commonly used self-supervision (e.g.,\nrotation and jigsaw) benefits image tasks such as classification and\nrecognition, they fail to provide the critical supervision signals that could\nlearn discriminative representation for segmentation tasks. These observations\nmotivate us to propose adversarial self-supervision UDA (or ASSUDA) that\nmaximizes the agreement between clean images and their adversarial examples by\na contrastive loss in the output space. Extensive empirical studies on commonly\nused benchmarks demonstrate that ASSUDA is resistant to adversarial attacks.",
          "link": "http://arxiv.org/abs/2105.10843",
          "publishedOn": "2021-05-25T01:56:12.054Z",
          "wordCount": null,
          "title": "Exploring Robustness of Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2105.10843v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_T/0/1/0/all/0/1\">Tianyu Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>",
          "description": "The convolution operation is a central building block of neural network\narchitectures widely used in computer vision. The size of the convolution\nkernels determines both the expressiveness of convolutional neural networks\n(CNN), as well as the number of learnable parameters. Increasing the network\ncapacity to capture rich pixel relationships requires increasing the number of\nlearnable parameters, often leading to overfitting and/or lack of robustness.\nIn this paper, we propose a powerful novel building block, the\nhyper-convolution, which implicitly represents the convolution kernel as a\nfunction of kernel coordinates. Hyper-convolutions enable decoupling the kernel\nsize, and hence its receptive field, from the number of learnable parameters.\nIn our experiments, focused on challenging biomedical image segmentation tasks,\nwe demonstrate that replacing regular convolutions with hyper-convolutions\nleads to more efficient architectures that achieve improved accuracy. Our\nanalysis also shows that learned hyper-convolutions are naturally regularized,\nwhich can offer better generalization performance. We believe that\nhyper-convolutions can be a powerful building block in future neural network\narchitectures solving computer vision tasks.",
          "link": "http://arxiv.org/abs/2105.10559",
          "publishedOn": "2021-05-25T01:56:12.049Z",
          "wordCount": 601,
          "title": "Hyper-Convolution Networks for Biomedical Image Segmentation. (arXiv:2105.10559v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10967",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Byun_J/0/1/0/all/0/1\">Jaeseok Byun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We consider the challenging blind denoising problem for Poisson-Gaussian\nnoise, in which no additional information about clean images or noise level\nparameters is available. Particularly, when only \"single\" noisy images are\navailable for training a denoiser, the denoising performance of existing\nmethods was not satisfactory. Recently, the blind pixelwise affine image\ndenoiser (BP-AIDE) was proposed and significantly improved the performance in\nthe above setting, to the extent that it is competitive with denoisers which\nutilized additional information. However, BP-AIDE seriously suffered from slow\ninference time due to the inefficiency of noise level estimation procedure and\nthat of the blind-spot network (BSN) architecture it used. To that end, we\npropose Fast Blind Image Denoiser (FBI-Denoiser) for Poisson-Gaussian noise,\nwhich consists of two neural network models; 1) PGE-Net that estimates\nPoisson-Gaussian noise parameters 2000 times faster than the conventional\nmethods and 2) FBI-Net that realizes a much more efficient BSN for pixelwise\naffine denoiser in terms of the number of parameters and inference speed.\nConsequently, we show that our FBI-Denoiser blindly trained solely based on\nsingle noisy images can achieve the state-of-the-art performance on several\nreal-world noisy image benchmark datasets with much faster inference time (x\n10), compared to BP-AIDE. The official code of our method is available at\nhttps://github.com/csm9493/FBI-Denoiser.",
          "link": "http://arxiv.org/abs/2105.10967",
          "publishedOn": "2021-05-25T01:56:12.041Z",
          "wordCount": null,
          "title": "FBI-Denoiser: Fast Blind Image Denoiser for Poisson-Gaussian Noise. (arXiv:2105.10967v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04951",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1\">Zhinan Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiyu Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>",
          "description": "Change detection for remote sensing images is widely applied for urban change\ndetection, disaster assessment and other fields. However, most of the existing\nCNN-based change detection methods still suffer from the problem of inadequate\npseudo-changes suppression and insufficient feature representation. In this\nwork, an unsupervised change detection method based on Task-related\nSelf-supervised Learning Change Detection network with smooth mechanism(TSLCD)\nis proposed to eliminate it. The main contributions include: (1) the\ntask-related self-supervised learning module is introduced to extract spatial\nfeatures more effectively. (2) a hard-sample-mining loss function is applied to\npay more attention to the hard-to-classify samples. (3) a smooth mechanism is\nutilized to remove some of pseudo-changes and noise. Experiments on four remote\nsensing change detection datasets reveal that the proposed TSLCD method\nachieves the state-of-the-art for change detection task.",
          "link": "http://arxiv.org/abs/2105.04951",
          "publishedOn": "2021-05-25T01:56:12.000Z",
          "wordCount": null,
          "title": "Task-Related Self-Supervised Learning for Remote Sensing Image Change Detection. (arXiv:2105.04951v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kourbane_I/0/1/0/all/0/1\">Ikram Kourbane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genc_Y/0/1/0/all/0/1\">Yakup Genc</a>",
          "description": "Hand pose estimation is a crucial part of a wide range of augmented reality\nand human-computer interaction applications. Predicting the 3D hand pose from a\nsingle RGB image is challenging due to occlusion and depth ambiguities.\nGCN-based (Graph Convolutional Networks) methods exploit the structural\nrelationship similarity between graphs and hand joints to model kinematic\ndependencies between joints. These techniques use predefined or globally\nlearned joint relationships, which may fail to capture pose-dependent\nconstraints. To address this problem, we propose a two-stage GCN-based\nframework that learns per-pose relationship constraints. Specifically, the\nfirst phase quantizes the 2D/3D space to classify the joints into 2D/3D blocks\nbased on their locality. This spatial dependency information guides this phase\nto estimate reliable 2D and 3D poses. The second stage further improves the 3D\nestimation through a GCN-based module that uses an adaptative nearest neighbor\nalgorithm to determine joint relationships. Extensive experiments show that our\nmulti-stage GCN approach yields an efficient model that produces accurate 2D/3D\nhand poses and outperforms the state-of-the-art on two public datasets.",
          "link": "http://arxiv.org/abs/2105.10902",
          "publishedOn": "2021-05-25T01:56:11.965Z",
          "wordCount": 618,
          "title": "A hybrid classification-regression approach for 3D hand pose estimation using graph convolutional networks. (arXiv:2105.10902v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martins_E/0/1/0/all/0/1\">Eloi Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brito_J/0/1/0/all/0/1\">Jos&#xe9; Henrique Brito</a>",
          "description": "In this paper we propose a system capable of tracking multiple soccer players\nin different types of video quality. The main goal, in contrast to most\nstate-of-art soccer player tracking systems, is the ability of execute\neffectively tracking in videos of low-quality. We adapted a state-of-art\nMultiple Object Tracking to the task. In order to do that adaptation, we\ncreated a Detection and a Tracking Dataset for 3 different qualities of video.\nThe results of our system are conclusive of its high performance.",
          "link": "http://arxiv.org/abs/2105.10700",
          "publishedOn": "2021-05-25T01:56:11.958Z",
          "wordCount": 507,
          "title": "Soccer Player Tracking in Low Quality Video. (arXiv:2105.10700v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Searching for a more compact network width recently serves as an effective\nway of channel pruning for the deployment of convolutional neural networks\n(CNNs) under hardware constraints. To fulfill the searching, a one-shot\nsupernet is usually leveraged to efficiently evaluate the performance\n\\wrt~different network widths. However, current methods mainly follow a\n\\textit{unilaterally augmented} (UA) principle for the evaluation of each\nwidth, which induces the training unfairness of channels in supernet. In this\npaper, we introduce a new supernet called Bilaterally Coupled Network (BCNet)\nto address this issue. In BCNet, each channel is fairly trained and responsible\nfor the same amount of network widths, thus each network width can be evaluated\nmore accurately. Besides, we leverage a stochastic complementary strategy for\ntraining the BCNet, and propose a prior initial population sampling method to\nboost the performance of the evolutionary search. Extensive experiments on\nbenchmark CIFAR-10 and ImageNet datasets indicate that our method can achieve\nstate-of-the-art or competing performance over other baseline methods.\nMoreover, our method turns out to further boost the performance of NAS models\nby refining their network widths. For example, with the same FLOPs budget, our\nobtained EfficientNet-B0 achieves 77.36\\% Top-1 accuracy on ImageNet dataset,\nsurpassing the performance of original setting by 0.48\\%.",
          "link": "http://arxiv.org/abs/2105.10533",
          "publishedOn": "2021-05-25T01:56:11.773Z",
          "wordCount": 645,
          "title": "BCNet: Searching for Network Width with Bilaterally Coupled Network. (arXiv:2105.10533v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_P/0/1/0/all/0/1\">Pascal Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smajic_A/0/1/0/all/0/1\">Alen Smajic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehler_A/0/1/0/all/0/1\">Alexander Mehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrami_G/0/1/0/all/0/1\">Giuseppe Abrami</a>",
          "description": "As global trends are shifting towards data-driven industries, the demand for\nautomated algorithms that can convert digital images of scanned documents into\nmachine readable information is rapidly growing. Besides the opportunity of\ndata digitization for the application of data analytic tools, there is also a\nmassive improvement towards automation of processes, which previously would\nrequire manual inspection of the documents. Although the introduction of\noptical character recognition technologies mostly solved the task of converting\nhuman-readable characters from images into machine-readable characters, the\ntask of extracting table semantics has been less focused on over the years. The\nrecognition of tables consists of two main tasks, namely table detection and\ntable structure recognition. Most prior work on this problem focuses on either\ntask without offering an end-to-end solution or paying attention to real\napplication conditions like rotated images or noise artefacts inside the\ndocument image. Recent work shows a clear trend towards deep learning\napproaches coupled with the use of transfer learning for the task of table\nstructure recognition due to the lack of sufficiently large datasets. In this\npaper we present a multistage pipeline named Multi-Type-TD-TSR, which offers an\nend-to-end solution for the problem of table recognition. It utilizes\nstate-of-the-art deep learning models for table detection and differentiates\nbetween 3 different types of tables based on the tables' borders. For the table\nstructure recognition we use a deterministic non-data driven algorithm, which\nworks on all table types. We additionally present two algorithms. One for\nunbordered tables and one for bordered tables, which are the base of the used\ntable structure recognition algorithm. We evaluate Multi-Type-TD-TSR on the\nICDAR 2019 table structure recognition dataset and achieve a new\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2105.11021",
          "publishedOn": "2021-05-25T01:56:11.695Z",
          "wordCount": 760,
          "title": "Multi-Type-TD-TSR -- Extracting Tables from Document Images using a Multi-stage Pipeline for Table Detection and Table Structure Recognition: from OCR to Structured Table Representations. (arXiv:2105.11021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyazono_T/0/1/0/all/0/1\">Taiga Miyazono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1\">Brian Kenji Iwana</a>",
          "description": "Book covers are intentionally designed and provide an introduction to a book.\nHowever, they typically require professional skills to design and produce the\ncover images. Thus, we propose a generative neural network that can produce\nbook covers based on an easy-to-use layout graph. The layout graph contains\nobjects such as text, natural scene objects, and solid color spaces. This\nlayout graph is embedded using a graph convolutional neural network and then\nused with a mask proposal generator and a bounding-box generator and filled\nusing an object proposal generator. Next, the objects are compiled into a\nsingle image and the entire network is trained using a combination of\nadversarial training, perceptual training, and reconstruction. Finally, a Style\nRetention Network (SRNet) is used to transfer the learned font style onto the\ndesired text. Using the proposed method allows for easily controlled and unique\nbook covers.",
          "link": "http://arxiv.org/abs/2105.11088",
          "publishedOn": "2021-05-25T01:56:11.532Z",
          "wordCount": 574,
          "title": "Towards Book Cover Design via Layout Graphs. (arXiv:2105.11088v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1908.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1\">M.G. Sarwar Murshed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_C/0/1/0/all/0/1\">Christopher Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1\">Daqing Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Nazar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananthanarayanan_G/0/1/0/all/0/1\">Ganesh Ananthanarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1\">Faraz Hussain</a>",
          "description": "Resource-constrained IoT devices, such as sensors and actuators, have become\nubiquitous in recent years. This has led to the generation of large quantities\nof data in real-time, which is an appealing target for AI systems. However,\ndeploying machine learning models on such end-devices is nearly impossible. A\ntypical solution involves offloading data to external computing systems (such\nas cloud servers) for further processing but this worsens latency, leads to\nincreased communication costs, and adds to privacy concerns. To address this\nissue, efforts have been made to place additional computing devices at the edge\nof the network, i.e close to the IoT devices where the data is generated.\nDeploying machine learning systems on such edge computing devices alleviates\nthe above issues by allowing computations to be performed close to the data\nsources. This survey describes major research efforts where machine learning\nsystems have been deployed at the edge of computer networks, focusing on the\noperational aspects including compression techniques, tools, frameworks, and\nhardware used in successful applications of intelligent edge systems.",
          "link": "http://arxiv.org/abs/1908.00080",
          "publishedOn": "2021-05-25T01:56:11.519Z",
          "wordCount": 688,
          "title": "Machine Learning at the Network Edge: A Survey. (arXiv:1908.00080v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chaoqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "This paper explores semi-supervised anomaly detection, a more practical\nsetting for anomaly detection where a small additional set of labeled samples\nare provided. Based on the analysis of Deep SAD, the state-of-the-art for\nsemi-supervised anomaly detection, we propose a new KL-divergence based\nobjective function and show that two factors: the mutual information between\nthe data and latent representations, and the entropy of latent representations,\nconstitute an integral objective function for anomaly detection. To resolve the\ncontradiction in simultaneously optimizing the two factors, we propose a novel\nencoder-decoder-encoder structure, with the first encoder focusing on\noptimizing the mutual information and the second encoder focusing on optimizing\nthe entropy. The two encoders are enforced to share similar encoding with a\nconsistent constraint on their latent representations. Extensive experiments\nhave revealed that the proposed method significantly outperforms several\nstate-of-the-arts on multiple benchmark datasets, including medical diagnosis\nand several classic anomaly detection benchmarks.",
          "link": "http://arxiv.org/abs/2012.04905",
          "publishedOn": "2021-05-25T01:56:11.504Z",
          "wordCount": 608,
          "title": "ESAD: End-to-end Deep Semi-supervised Anomaly Detection. (arXiv:2012.04905v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Estrade_V/0/1/0/all/0/1\">Vincent Estrade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daudon_M/0/1/0/all/0/1\">Michel Daudon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_E/0/1/0/all/0/1\">Emmanuel Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernhard_J/0/1/0/all/0/1\">Jean-Christophe Bernhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bladou_F/0/1/0/all/0/1\">Franck Bladou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robert_G/0/1/0/all/0/1\">Gregoire Robert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senneville_B/0/1/0/all/0/1\">Baudouin Denis de Senneville</a>",
          "description": "Objective: To assess automatic computer-aided in-situ recognition of\nmorphological features of pure and mixed urinary stones using intraoperative\ndigital endoscopic images acquired in a clinical setting. Materials and\nmethods: In this single-centre study, an experienced urologist intraoperatively\nand prospectively examined the surface and section of all kidney stones\nencountered. Calcium oxalate monohydrate (COM/Ia), dihydrate (COD/IIb) and uric\nacid (UA/IIIb) morphological criteria were collected and classified to generate\nannotated datasets. A deep convolutional neural network (CNN) was trained to\npredict the composition of both pure and mixed stones. To explain the\npredictions of the deep neural network model, coarse localisation heat-maps\nwere plotted to pinpoint key areas identified by the network. Results: This\nstudy included 347 and 236 observations of stone surface and stone section,\nrespectively. A highest sensitivity of 98 % was obtained for the type \"pure\nIIIb/UA\" using surface images. The most frequently encountered morphology was\nthat of the type \"pure Ia/COM\"; it was correctly predicted in 91 % and 94 % of\ncases using surface and section images, respectively. Of the mixed type\n\"Ia/COM+IIb/COD\", Ia/COM was predicted in 84 % of cases using surface images,\nIIb/COD in 70 % of cases, and both in 65 % of cases. Concerning mixed\nIa/COM+IIIb/UA stones, Ia/COM was predicted in 91 % of cases using section\nimages, IIIb/UA in 69 % of cases, and both in 74 % of cases. Conclusions: This\npreliminary study demonstrates that deep convolutional neural networks are\npromising to identify kidney stone composition from endoscopic images acquired\nintraoperatively. Both pure and mixed stone composition could be discriminated.\nCollected in a clinical setting, surface and section images analysed by deep\nCNN provide valuable information about stone morphology for computer-aided\ndiagnosis.",
          "link": "http://arxiv.org/abs/2105.10686",
          "publishedOn": "2021-05-25T01:56:11.442Z",
          "wordCount": 742,
          "title": "Towards Automatic Recognition of Pure & Mixed Stones using Intraoperative Endoscopic Digital Images. (arXiv:2105.10686v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jong Hak Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Woncheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>",
          "description": "Recently a number of studies demonstrated impressive performance on diverse\nvision-language multi-modal tasks such as image captioning and visual question\nanswering by extending the BERT architecture with multi-modal pre-training\nobjectives. In this work we explore a broad set of multi-modal representation\nlearning tasks in the medical domain, specifically using radiology images and\nthe unstructured report. We propose Medical Vision Language Learner (MedViLL)\nwhich adopts a Transformer-based architecture combined with a novel multimodal\nattention masking scheme to maximize generalization performance for both\nvision-language understanding tasks (image-report retrieval, disease\nclassification, medical visual question answering) and vision-language\ngeneration task (report generation). By rigorously evaluating the proposed\nmodel on four downstream tasks with two chest X-ray image datasets (MIMIC-CXR\nand Open-I), we empirically demonstrate the superior downstream task\nperformance of MedViLL against various baselines including task-specific\narchitectures.",
          "link": "http://arxiv.org/abs/2105.11333",
          "publishedOn": "2021-05-25T01:56:11.311Z",
          "wordCount": 586,
          "title": "Multi-modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training. (arXiv:2105.11333v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "The development of practical applications, such as autonomous driving and\nrobotics, has brought increasing attention to 3D point cloud understanding.\nWhile deep learning has achieved remarkable success on image-based tasks, there\nare many unique challenges faced by deep neural networks in processing massive,\nunstructured and noisy 3D points. To demonstrate the latest progress of deep\nlearning for 3D point cloud understanding, this paper summarizes recent\nremarkable research contributions in this area from several different\ndirections (classification, segmentation, detection, tracking, flow estimation,\nregistration, augmentation and completion), together with commonly used\ndatasets, metrics and state-of-the-art performances. More information regarding\nthis survey can be found at:\nhttps://github.com/SHI-Labs/3D-Point-Cloud-Learning.",
          "link": "http://arxiv.org/abs/2009.08920",
          "publishedOn": "2021-05-25T01:56:11.291Z",
          "wordCount": 562,
          "title": "Deep Learning for 3D Point Cloud Understanding: A Survey. (arXiv:2009.08920v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Djuric_N/0/1/0/all/0/1\">Nemanja Djuric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Henggang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huahua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_F/0/1/0/all/0/1\">Fang-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Luisa San Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Song Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Rui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayan_A/0/1/0/all/0/1\">Alyssa Dayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sidney Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_B/0/1/0/all/0/1\">Brian C. Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_G/0/1/0/all/0/1\">Gregory P. Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallespi_Gonzalez_C/0/1/0/all/0/1\">Carlos Vallespi-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wellington_C/0/1/0/all/0/1\">Carl K. Wellington</a>",
          "description": "One of the critical pieces of the self-driving puzzle is understanding the\nsurroundings of a self-driving vehicle (SDV) and predicting how these\nsurroundings will change in the near future. To address this task we propose\nMultiXNet, an end-to-end approach for detection and motion prediction based\ndirectly on lidar sensor data. This approach builds on prior work by handling\nmultiple classes of traffic actors, adding a jointly trained second-stage\ntrajectory refinement step, and producing a multimodal probability distribution\nover future actor motion that includes both multiple discrete traffic behaviors\nand calibrated continuous position uncertainties. The method was evaluated on\nlarge-scale, real-world data collected by a fleet of SDVs in several cities,\nwith the results indicating that it outperforms existing state-of-the-art\napproaches.",
          "link": "http://arxiv.org/abs/2006.02000",
          "publishedOn": "2021-05-25T01:56:11.263Z",
          "wordCount": 645,
          "title": "MultiXNet: Multiclass Multistage Multimodal Motion Prediction. (arXiv:2006.02000v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00567",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaeta_I/0/1/0/all/0/1\">Isabella M. Gaeta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jha_A/0/1/0/all/0/1\">Aadarsh Jha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Millis_B/0/1/0/all/0/1\">Bryan A. Millis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahadevan_Jansen_A/0/1/0/all/0/1\">Anita Mahadevan-Jansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tyska_M/0/1/0/all/0/1\">Matthew J. Tyska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>",
          "description": "Background: The quantitative analysis of microscope videos often requires\ninstance segmentation and tracking of cellular and subcellular objects. The\ntraditional method consists of two stages: (1) performing instance object\nsegmentation of each frame, and (2) associating objects frame-by-frame.\nRecently, pixel-embedding-based deep learning approaches these two steps\nsimultaneously as a single stage holistic solution. In computer vision,\nannotated training data with consistent segmentation and tracking is resource\nintensive, the severity of which is multiplied in microscopy imaging due to (1)\ndense objects (e.g., overlapping or touching), and (2) high dynamics (e.g.,\nirregular motion and mitosis). Adversarial simulations have provided successful\nsolutions to alleviate the lack of such annotations in dynamics scenes in\ncomputer vision, such as using simulated environments (e.g., computer games) to\ntrain real-world self-driving systems. Methods: In this paper, we propose an\nannotation-free synthetic instance segmentation and tracking (ASIST) method\nwith adversarial simulation and single-stage pixel-embedding based learning.\nContribution: The contribution of this paper is three-fold: (1) the proposed\nmethod aggregates adversarial simulations and single-stage pixel-embedding\nbased deep learning; (2) the method is assessed with both the cellular (i.e.,\nHeLa cells) and subcellular (i.e., microvilli) objects; and (3) to the best of\nour knowledge, this is the first study to explore annotation-free instance\nsegmentation and tracking study for microscope videos. Results: The ASIST\nmethod achieved an important step forward, when compared with fully supervised\napproaches: ASIST shows 7% to 11% higher segmentation, detection and tracking\nperformance on microvilli relative to fully supervised methods, and comparable\nperformance on Hela cell videos.",
          "link": "http://arxiv.org/abs/2101.00567",
          "publishedOn": "2021-05-25T01:56:11.234Z",
          "wordCount": 733,
          "title": "ASIST: Annotation-free Synthetic Instance Segmentation and Tracking by Adversarial Simulations. (arXiv:2101.00567v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lanqing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhiyuan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1\">Saiprasad Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>",
          "description": "Constructing effective image priors is critical to solving ill-posed inverse\nproblems in image processing and imaging. Recent works proposed to exploit\nimage non-local similarity for inverse problems by grouping similar patches and\ndemonstrated state-of-the-art results in many applications. However, compared\nto classic methods based on filtering or sparsity, most of the non-local\nalgorithms are time-consuming, mainly due to the highly inefficient and\nredundant block matching step, where the distance between each pair of\noverlapping patches needs to be computed. In this work, we propose a novel\nSelf-Convolution operator to exploit image non-local similarity in a\nself-supervised way. The proposed Self-Convolution can generalize the\ncommonly-used block matching step and produce equivalent results with much\ncheaper computation. Furthermore, by applying Self-Convolution, we propose an\neffective multi-modality image restoration scheme, which is much more efficient\nthan conventional block matching for non-local modeling. Experimental results\ndemonstrate that (1) Self-Convolution can significantly speed up most of the\npopular non-local image restoration algorithms, with two-fold to nine-fold\nfaster block matching, and (2) the proposed multi-modality image restoration\nscheme achieves superior denoising results in both efficiency and effectiveness\non RGB-NIR images. The code is publicly available at\n\\href{https://github.com/GuoLanqing/Self-Convolution}.",
          "link": "http://arxiv.org/abs/2006.13714",
          "publishedOn": "2021-05-25T01:56:11.219Z",
          "wordCount": 661,
          "title": "Exploiting Non-Local Priors via Self-Convolution For Highly-Efficient Image Restoration. (arXiv:2006.13714v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1\">Jeff Ichnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "Searching for objects in indoor organized environments such as homes or\noffices is part of our everyday activities. When looking for a target object,\nwe jointly reason about the rooms and containers the object is likely to be in;\nthe same type of container will have a different probability of having the\ntarget depending on the room it is in. We also combine geometric and semantic\ninformation to infer what container is best to search, or what other objects\nare best to move, if the target object is hidden from view. We propose to use a\n3D scene graph representation to capture the hierarchical, semantic, and\ngeometric aspects of this problem. To exploit this representation in a search\nprocess, we introduce Hierarchical Mechanical Search (HMS), a method that\nguides an agent's actions towards finding a target object specified with a\nnatural language description. HMS is based on a novel neural network\narchitecture that uses neural message passing of vectors with visual,\ngeometric, and linguistic information to allow HMS to reason across layers of\nthe graph while combining semantic and geometric cues. HMS is evaluated on a\nnovel dataset of 500 3D scene graphs with dense placements of semantically\nrelated objects in storage locations, and is shown to be significantly better\nthan several baselines at finding objects and close to the oracle policy in\nterms of the median number of actions required. Additional qualitative results\ncan be found at https://ai.stanford.edu/mech-search/hms.",
          "link": "http://arxiv.org/abs/2012.04060",
          "publishedOn": "2021-05-25T01:56:11.189Z",
          "wordCount": null,
          "title": "Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search. (arXiv:2012.04060v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zichang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>",
          "description": "Magnetic resonance imaging (MRI) is a powerful imaging modality that\nrevolutionizes medicine and biology. The imaging speed of high-dimensional MRI\nis often limited, which constrains its practical utility. Recently, low-rank\ntensor models have been exploited to enable fast MR imaging with sparse\nsampling. Most existing methods use some pre-defined sampling design, and\nactive sensing has not been explored for low-rank tensor imaging. In this\npaper, we introduce an active low-rank tensor model for fast MR imaging. We\npropose an active sampling method based on a Query-by-Committee model, making\nuse of the benefits of low-rank tensor structure. Numerical experiments on a\n3-D MRI data set demonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2012.12496",
          "publishedOn": "2021-05-25T01:56:11.189Z",
          "wordCount": null,
          "title": "Active Sampling for Accelerated MRI with Low-Rank Tensors. (arXiv:2012.12496v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1\">Letitia Parcalabescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>",
          "description": "We investigate the reasoning ability of pretrained vision and language (V&L)\nmodels in two tasks that require multimodal integration: (1) discriminating a\ncorrect image-sentence pair from an incorrect one, and (2) counting entities in\nan image. We evaluate three pretrained V&L models on these tasks: ViLBERT,\nViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results\nshow that models solve task (1) very well, as expected, since all models are\npretrained on task (1). However, none of the pretrained V&L models is able to\nadequately solve task (2), our counting probe, and they cannot generalise to\nout-of-distribution quantities. We propose a number of explanations for these\nfindings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of\ncatastrophic forgetting on task (1). Concerning our results on the counting\nprobe, we find evidence that all models are impacted by dataset bias, and also\nfail to individuate entities in the visual input. While a selling point of\npretrained V&L models is their ability to solve complex tasks, our findings\nsuggest that understanding their reasoning and grounding capabilities requires\nmore targeted investigations on specific phenomena.",
          "link": "http://arxiv.org/abs/2012.12352",
          "publishedOn": "2021-05-25T01:56:11.188Z",
          "wordCount": 688,
          "title": "Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks. (arXiv:2012.12352v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1\">Hugo Touvron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; J&#xe9;gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1\">Julien Mairal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>",
          "description": "In this paper, we question if self-supervised learning provides new\nproperties to Vision Transformer (ViT) that stand out compared to convolutional\nnetworks (convnets). Beyond the fact that adapting self-supervised methods to\nthis architecture works particularly well, we make the following observations:\nfirst, self-supervised ViT features contain explicit information about the\nsemantic segmentation of an image, which does not emerge as clearly with\nsupervised ViTs, nor with convnets. Second, these features are also excellent\nk-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study\nalso underlines the importance of momentum encoder, multi-crop training, and\nthe use of small patches with ViTs. We implement our findings into a simple\nself-supervised method, called DINO, which we interpret as a form of\nself-distillation with no labels. We show the synergy between DINO and ViTs by\nachieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",
          "link": "http://arxiv.org/abs/2104.14294",
          "publishedOn": "2021-05-25T01:56:11.188Z",
          "wordCount": null,
          "title": "Emerging Properties in Self-Supervised Vision Transformers. (arXiv:2104.14294v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tianxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>",
          "description": "Privacy protection on human biological information has drawn increasing\nattention in recent years, among which face anonymization plays an importance\nrole. We propose a novel approach which protects identity information of facial\nimages from leakage with slightest modification. Specifically, we disentangle\nidentity representation from other facial attributes leveraging the power of\ngenerative adversarial networks trained on a conditional multi-scale\nreconstruction (CMR) loss and an identity loss. We evaulate the disentangle\nability of our model, and propose an effective method for identity\nanonymization, namely Anonymous Identity Generation (AIG), to reach the goal of\nface anonymization meanwhile maintaining similarity to the original image as\nmuch as possible. Quantitative and qualitative results demonstrate our method's\nsuperiority compared with the SOTAs on both visual quality and anonymization\nsuccess rate.",
          "link": "http://arxiv.org/abs/2105.11137",
          "publishedOn": "2021-05-25T01:56:11.150Z",
          "wordCount": null,
          "title": "Face Anonymization by Manipulating Decoupled Identity Representation. (arXiv:2105.11137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shijie Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>",
          "description": "The performance of object detection, to a great extent, depends on the\navailability of large annotated datasets. To alleviate the annotation cost, the\nresearch community has explored a number of ways to exploit unlabeled or weakly\nlabeled data. However, such efforts have met with limited success so far. In\nthis work, we revisit the problem with a pragmatic standpoint, trying to\nexplore a new balance between detection performance and annotation cost by\njointly exploiting fully and weakly annotated data. Specifically, we propose a\nweakly- and semi-supervised object detection framework (WSSOD), which involves\na two-stage learning procedure. An agent detector is first trained on a joint\ndataset and then used to predict pseudo bounding boxes on weakly-annotated\nimages. The underlying assumptions in the current as well as common\nsemi-supervised pipelines are also carefully examined under a unified EM\nformulation. On top of this framework, weakly-supervised loss (WSL), label\nattention and random pseudo-label sampling (RPS) strategies are introduced to\nrelax these assumptions, bringing additional improvement on the efficacy of the\ndetection pipeline. The proposed framework demonstrates remarkable performance\non PASCAL-VOC and MSCOCO benchmark, achieving a high performance comparable to\nthose obtained in fully-supervised settings, with only one third of the\nannotations.",
          "link": "http://arxiv.org/abs/2105.11293",
          "publishedOn": "2021-05-25T01:56:11.149Z",
          "wordCount": null,
          "title": "WSSOD: A New Pipeline for Weakly- and Semi-Supervised Object Detection. (arXiv:2105.11293v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shivam Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasukurthi_N/0/1/0/all/0/1\">Nikhil Kasukurthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1\">Harshit Pande</a>",
          "description": "Screening for the diagnosis of glaucoma through a fundus image can be\ndetermined by the optic cup to disc diameter ratio (CDR), which requires the\nsegmentation of the cup and disc regions. In this paper, we propose two novel\napproaches, namely Parameter-Shared Branched Network (PSBN) andWeak Region of\nInterest Model-based segmentation (WRoIM) to identify disc and cup boundaries.\nUnlike the previous approaches, the proposed methods are trained end-to-end\nthrough a single neural network architecture and use dynamic cropping instead\nof manual or traditional computer vision-based cropping. We are able to achieve\nsimilar performance as that of state-of-the-art approaches with less number of\nnetwork parameters. Our experiments include comparison with different best\nknown methods on publicly available Drishti-GS1 and RIM-ONE v3 datasets. With\n$7.8 \\times 10^6$ parameters our approach achieves a Dice score of 0.96/0.89\nfor disc/cup segmentation on Drishti-GS1 data whereas the existing\nstate-of-the-art approach uses $19.8\\times 10^6$ parameters to achieve a dice\nscore of 0.97/0.89.",
          "link": "http://arxiv.org/abs/2105.11364",
          "publishedOn": "2021-05-25T01:56:11.148Z",
          "wordCount": null,
          "title": "Dynamic region proposal networks for semantic segmentation in automated glaucoma screening. (arXiv:2105.11364v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">YeongHyeon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">JongHee Jung</a>",
          "description": "Road accident can be triggered by wet road because it decreases skid\nresistance. To prevent the road accident, detecting road surface abnomality is\nhighly useful. In this paper, we propose the deep learning based cost-effective\nreal-time anomaly detection architecture, naming with non-compression\nauto-encoder (NCAE). The proposed architecture can reflect forward and backward\ncausality of time series information via convolutional operation. Moreover, the\nabove architecture shows higher anomaly detection performance of published\nanomaly detection model via experiments. We conclude that NCAE as a\ncutting-edge model for road surface anomaly detection with 4.20\\% higher AUROC\nand 2.99 times faster decision than before.",
          "link": "http://arxiv.org/abs/2103.12992",
          "publishedOn": "2021-05-25T01:56:11.147Z",
          "wordCount": null,
          "title": "Non-Compression Auto-Encoder for Detecting Road Surface Abnormality via Vehicle Driving Noise. (arXiv:2103.12992v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zongyan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhenyong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Zero-shot object recognition or zero-shot learning aims to transfer the\nobject recognition ability among the semantically related categories, such as\nfine-grained animal or bird species. However, the images of different\nfine-grained objects tend to merely exhibit subtle differences in appearance,\nwhich will severely deteriorate zero-shot object recognition. To reduce the\nsuperfluous information in the fine-grained objects, in this paper, we propose\nto learn the redundancy-free features for generalized zero-shot learning. We\nachieve our motivation by projecting the original visual features into a new\n(redundancy-free) feature space and then restricting the statistical dependence\nbetween these two feature spaces. Furthermore, we require the projected\nfeatures to keep and even strengthen the category relationship in the\nredundancy-free feature space. In this way, we can remove the redundant\ninformation from the visual features without losing the discriminative\ninformation. We extensively evaluate the performance on four benchmark\ndatasets. The results show that our redundancy-free feature based generalized\nzero-shot learning (RFF-GZSL) approach can achieve competitive results compared\nwith the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2006.08939",
          "publishedOn": "2021-05-25T01:56:11.137Z",
          "wordCount": 706,
          "title": "Learning the Redundancy-free Features for Generalized Zero-Shot Object Recognition. (arXiv:2006.08939v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1\">Bilal Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Baz_A/0/1/0/all/0/1\">Ayman El-Baz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>",
          "description": "Prostate cancer (PCa) is the second deadliest form of cancer in males, and it\ncan be clinically graded by examining the structural representations of Gleason\ntissues. This paper proposes the first attempt, to the best of our knowledge,\nfor segmenting the Gleason tissues to grade PCa via the whole slide images\n(WSI). Also, the proposed approach encompasses two main contributions: 1) A\nsynergy of hybrid dilation factors and hierarchical decomposition of latent\nspace representation for effective Gleason tissues extraction, and 2) A\nthree-tiered loss function which can penalize different semantic segmentation\nmodels for accurately extracting the highly correlated patterns. In addition to\nthis, the proposed framework has been extensively evaluated on a large-scale\nPCa dataset containing 10,516 whole slide scans (with around 71.7M patches),\nwhere it outperforms state-of-the-art schemes by 3.22% (in terms of mean\nintersection-over-union) for extracting the Gleason tissues and 6.91% (in terms\nof F1 score) for grading the progression of PCa.",
          "link": "http://arxiv.org/abs/2011.00527",
          "publishedOn": "2021-05-25T01:56:11.087Z",
          "wordCount": 663,
          "title": "A Dilated Residual Hierarchically Fashioned Segmentation Framework for Extracting Gleason Tissues and Grading Prostate Cancer from Whole Slide Images. (arXiv:2011.00527v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11179",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kornilova_A/0/1/0/all/0/1\">A. Kornilova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirilenko_I/0/1/0/all/0/1\">I. Kirilenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iarosh_D/0/1/0/all/0/1\">D. Iarosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kutuev_V/0/1/0/all/0/1\">V. Kutuev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strutovsky_M/0/1/0/all/0/1\">M. Strutovsky</a>",
          "description": "Mobile microscopy is a newly formed field that emerged from a combination of\noptical microscopy capabilities and spread, functionality, and ever-increasing\ncomputing resources of mobile devices. Despite the idea of creating a system\nthat would successfully merge a microscope, numerous computer vision methods,\nand a mobile device is regularly examined, the resulting implementations still\nrequire the presence of a qualified operator to control specimen digitization.\nIn this paper, we address the task of surpassing this constraint and present a\n``smart'' mobile microscope concept aimed at automatic digitization of the most\nvaluable visual information about the specimen. We perform this through\ncombining automated microscope setup control and classic techniques such as\nauto-focusing, in-focus filtering, and focus-stacking -- adapted and optimized\nas parts of a mobile cross-platform library.",
          "link": "http://arxiv.org/abs/2105.11179",
          "publishedOn": "2021-05-25T01:56:11.077Z",
          "wordCount": null,
          "title": "Smart mobile microscopy: towards fully-automated digitization. (arXiv:2105.11179v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Guoqiang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yanbing Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>",
          "description": "With the explosive growth of video data, video summarization, which attempts\nto seek the minimum subset of frames while still conveying the main story, has\nbecome one of the hottest topics. Nowadays, substantial achievements have been\nmade by supervised learning techniques, especially after the emergence of deep\nlearning. However, it is extremely expensive and difficult to collect human\nannotation for large-scale video datasets. To address this problem, we propose\na convolutional attentive adversarial network (CAAN), whose key idea is to\nbuild a deep summarizer in an unsupervised way. Upon the generative adversarial\nnetwork, our overall framework consists of a generator and a discriminator. The\nformer predicts importance scores for all frames of a video while the latter\ntries to distinguish the score-weighted frame features from original frame\nfeatures. Specifically, the generator employs a fully convolutional sequence\nnetwork to extract global representation of a video, and an attention-based\nnetwork to output normalized importance scores. To learn the parameters, our\nobjective function is composed of three loss functions, which can guide the\nframe-level importance score prediction collaboratively. To validate this\nproposed method, we have conducted extensive experiments on two public\nbenchmarks SumMe and TVSum. The results show the superiority of our proposed\nmethod against other state-of-the-art unsupervised approaches. Our method even\noutperforms some published supervised approaches.",
          "link": "http://arxiv.org/abs/2105.11131",
          "publishedOn": "2021-05-25T01:56:11.076Z",
          "wordCount": null,
          "title": "Unsupervised Video Summarization with a Convolutional Attentive Adversarial Network. (arXiv:2105.11131v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dehui Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lichun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>",
          "description": "3D action recognition is referred to as the classification of action\nsequences which consist of 3D skeleton joints. While many research work are\ndevoted to 3D action recognition, it mainly suffers from three problems: highly\ncomplicated articulation, a great amount of noise, and a low implementation\nefficiency. To tackle all these problems, we propose a real-time 3D action\nrecognition framework by integrating the locally aggregated kinematic-guided\nskeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first\ndefine the skeletonlet as a few combinations of joint offsets grouped in terms\nof kinematic principle, and then represent an action sequence using LAKS, which\nconsists of a denoising phase and a locally aggregating phase. The denoising\nphase detects the noisy action data and adjust it by replacing all the features\nwithin it with the features of the corresponding previous frame, while the\nlocally aggregating phase sums the difference between an offset feature of the\nskeletonlet and its cluster center together over all the offset features of the\nsequence. Finally, the SHA model which combines sparse representation with a\nhashing model, aiming at promoting the recognition accuracy while maintaining a\nhigh efficiency. Experimental results on MSRAction3D, UTKinectAction3D and\nFlorence3DAction datasets demonstrate that the proposed method outperforms\nstate-of-the-art methods in both recognition accuracy and implementation\nefficiency.",
          "link": "http://arxiv.org/abs/2105.11312",
          "publishedOn": "2021-05-25T01:56:11.075Z",
          "wordCount": null,
          "title": "Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model. (arXiv:2105.11312v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>",
          "description": "When training control policies for robot manipulation via deep learning,\nsim-to-real transfer can help satisfy the large data requirements. In this\npaper, we study the problem of zero-shot sim-to-real when the task requires\nboth highly precise control, with sub-millimetre error tolerance, and full\nworkspace generalisation. Our framework involves a coarse-to-fine controller,\nwhere trajectories initially begin with classical motion planning based on pose\nestimation, and transition to an end-to-end controller which maps images to\nactions and is trained in simulation with domain randomisation. In this way, we\nachieve precise control whilst also generalising the controller across the\nworkspace and keeping the generality and robustness of vision-based, end-to-end\ncontrol. Real-world experiments on a range of different tasks show that, by\nexploiting the best of both worlds, our framework significantly outperforms\npurely motion planning methods, and purely learning-based methods. Furthermore,\nwe answer a range of questions on best practices for precise sim-to-real\ntransfer, such as how different image sensor modalities and image feature\nrepresentations perform.",
          "link": "http://arxiv.org/abs/2105.11283",
          "publishedOn": "2021-05-25T01:56:11.073Z",
          "wordCount": null,
          "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across the Workspace. (arXiv:2105.11283v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>",
          "description": "Image-to-image translation aims to preserve source contents while translating\nto discriminative target styles between two visual domains. Most works apply\nadversarial learning in the ambient image space, which could be computationally\nexpensive and challenging to train. In this paper, we propose to deploy an\nenergy-based model (EBM) in the latent space of a pretrained autoencoder for\nthis task. The pretrained autoencoder serves as both a latent code extractor\nand an image reconstruction worker. Our model, LETIT, is based on the\nassumption that two domains share the same latent space, where latent\nrepresentation is implicitly decomposed as a content code and a domain-specific\nstyle code. Instead of explicitly extracting the two codes and applying\nadaptive instance normalization to combine them, our latent EBM can implicitly\nlearn to transport the source style code to the target style code while\npreserving the content code, an advantage over existing image translation\nmethods. This simplified solution is also more efficient in the one-sided\nunpaired image translation setting. Qualitative and quantitative comparisons\ndemonstrate superior translation quality and faithfulness for content\npreservation. Our model is the first to be applicable to\n1024$\\times$1024-resolution unpaired image translation to the best of our\nknowledge.",
          "link": "http://arxiv.org/abs/2012.00649",
          "publishedOn": "2021-05-25T01:56:11.073Z",
          "wordCount": null,
          "title": "Unpaired Image-to-Image Translation via Latent Energy Transport. (arXiv:2012.00649v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alavi_A/0/1/0/all/0/1\">Azadeh Alavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brungel_R/0/1/0/all/0/1\">Raphael Brungel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassidy_B/0/1/0/all/0/1\">Bill Cassidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_M/0/1/0/all/0/1\">Manu Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckert_J/0/1/0/all/0/1\">Johannes Ruckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olshansky_M/0/1/0/all/0/1\">Moshe Olshansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hideo Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1\">Saeed Hassanpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_C/0/1/0/all/0/1\">Christoph M. Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ascher_D/0/1/0/all/0/1\">David Ascher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_A/0/1/0/all/0/1\">Anping Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajita_H/0/1/0/all/0/1\">Hiroki Kajita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillespie_D/0/1/0/all/0/1\">David Gillespie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reeves_N/0/1/0/all/0/1\">Neil D. Reeves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappachan_J/0/1/0/all/0/1\">Joseph Pappachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OShea_C/0/1/0/all/0/1\">Claire O&#x27;Shea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>",
          "description": "There has been a substantial amount of research involving computer methods\nand technology for the detection and recognition of diabetic foot ulcers\n(DFUs), but there is a lack of systematic comparisons of state-of-the-art deep\nlearning object detection frameworks applied to this problem. DFUC2020 provided\nparticipants with a comprehensive dataset consisting of 2,000 images for\ntraining and 2,000 images for testing. This paper summarises the results of\nDFUC2020 by comparing the deep learning-based algorithms proposed by the\nwinning teams: Faster R-CNN, three variants of Faster R-CNN and an ensemble\nmethod; YOLOv3; YOLOv5; EfficientDet; and a new Cascade Attention Network. For\neach deep learning method, we provide a detailed description of model\narchitecture, parameter settings for training and additional stages including\npre-processing, data augmentation and post-processing. We provide a\ncomprehensive evaluation for each method. All the methods required a data\naugmentation stage to increase the number of images available for training and\na post-processing stage to remove false positives. The best performance was\nobtained from Deformable Convolution, a variant of Faster R-CNN, with a mean\naverage precision (mAP) of 0.6940 and an F1-Score of 0.7434. Finally, we\ndemonstrate that the ensemble method based on different deep learning methods\ncan enhanced the F1-Score but not the mAP.",
          "link": "http://arxiv.org/abs/2010.03341",
          "publishedOn": "2021-05-25T01:56:11.065Z",
          "wordCount": null,
          "title": "Deep Learning in Diabetic Foot Ulcers Detection: A Comprehensive Evaluation. (arXiv:2010.03341v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yicong Zhou</a>",
          "description": "Handwritten Text Line Segmentation (HTLS) is a low-level but important task\nfor many higher-level document processing tasks like handwritten text\nrecognition. It is often formulated in terms of semantic segmentation or object\ndetection in deep learning. However, both formulations have serious\nshortcomings. The former requires heavy post-processing of splitting/merging\nadjacent segments, while the latter may fail on dense or curved texts. In this\npaper, we propose a novel Line Counting formulation for HTLS -- that involves\ncounting the number of text lines from the top at every pixel location. This\nformulation helps learn an end-to-end HTLS solution that directly predicts\nper-pixel line number for a given document image. Furthermore, we propose a\ndeep neural network (DNN) model LineCounter to perform HTLS through the Line\nCounting formulation. Our extensive experiments on the three public datasets\n(ICDAR2013-HSC, HIT-MW, and VML-AHTE) demonstrate that LineCounter outperforms\nstate-of-the-art HTLS approaches. Source code is available at\nhttps://github.com/Leedeng/Line-Counter.",
          "link": "http://arxiv.org/abs/2105.11307",
          "publishedOn": "2021-05-25T01:56:11.058Z",
          "wordCount": null,
          "title": "LineCounter: Learning Handwritten Text Line Segmentation by Counting. (arXiv:2105.11307v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.08121",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Poitevin_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Poitevin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1\">Yee-Ting Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Holmes_S/0/1/0/all/0/1\">Susan Holmes</a>",
          "description": "Cryo-electron microscopy (cryo-EM) is capable of producing reconstructed 3D\nimages of biomolecules at near-atomic resolution. As such, it represents one of\nthe most promising imaging techniques in structural biology. However, raw\ncryo-EM images are only highly corrupted - noisy and band-pass filtered - 2D\nprojections of the target 3D biomolecules. Reconstructing the 3D molecular\nshape starts with the removal of image outliers, the estimation of the\norientation of the biomolecule that has produced the given 2D image, and the\nestimation of camera parameters to correct for intensity defects. Current\ntechniques performing these tasks are often computationally expensive, while\nthe dataset sizes keep growing. There is a need for next-generation algorithms\nthat preserve accuracy while improving speed and scalability. In this paper, we\ncombine variational autoencoders (VAEs) and generative adversarial networks\n(GANs) to learn a low-dimensional latent representation of cryo-EM images. We\nperform an exploratory analysis of the obtained latent space, that is shown to\nhave a structure of \"orbits\", in the sense of Lie group theory, consistent with\nthe acquisition procedure of cryo-EM images. This analysis leads us to design\nan estimation method for orientation and camera parameters of single-particle\ncryo-EM images, together with an outliers detection procedure. As such, it\nopens the door to geometric approaches for unsupervised estimations of\norientations and camera parameters, making possible fast cryo-EM biomolecule\nreconstruction.",
          "link": "http://arxiv.org/abs/1911.08121",
          "publishedOn": "2021-05-25T01:56:11.055Z",
          "wordCount": 700,
          "title": "Estimation of Orientation and Camera Parameters from Cryo-Electron Microscopy Images with Variational Autoencoders and Generative Adversarial Networks. (arXiv:1911.08121v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11356",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sundaresan_V/0/1/0/all/0/1\">Vaanathi Sundaresan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Griffanti_L/0/1/0/all/0/1\">Ludovica Griffanti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a>",
          "description": "Gliomas appear with wide variation in their characteristics both in terms of\ntheir appearance and location on brain MR images, which makes robust tumour\nsegmentation highly challenging, and leads to high inter-rater variability even\nin manual segmentations. In this work, we propose a triplanar ensemble network,\nwith an independent tumour core prediction module, for accurate segmentation of\nthese tumours and their sub-regions. On evaluating our method on the MICCAI\nBrain Tumor Segmentation (BraTS) challenge validation dataset, for tumour\nsub-regions, we achieved a Dice similarity coefficient of 0.77 for both\nenhancing tumour (ET) and tumour core (TC). In the case of the whole tumour\n(WT) region, we achieved a Dice value of 0.89, which is on par with the\ntop-ranking methods from BraTS'17-19. Our method achieved an evaluation score\nthat was the equal 5th highest value (with our method ranking in 10th place) in\nthe BraTS'20 challenge, with mean Dice values of 0.81, 0.89 and 0.84 on ET, WT\nand TC regions respectively on the BraTS'20 unseen test dataset.",
          "link": "http://arxiv.org/abs/2105.11356",
          "publishedOn": "2021-05-25T01:56:11.033Z",
          "wordCount": null,
          "title": "Brain tumour segmentation using a triplanar ensemble of U-Nets. (arXiv:2105.11356v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Mengxiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>",
          "description": "Recently the crowd counting has received more and more attention. Especially\nthe technology of high-density environment has become an important research\ncontent, and the relevant methods for the existence of extremely dense crowd\nare not optimal. In this paper, we propose a multi-level attentive\nConvolutional Neural Network (MLAttnCNN) for crowd counting. We extract\nhigh-level contextual information with multiple different scales applied in\npooling, and use multi-level attention modules to enrich the characteristics at\ndifferent layers to achieve more efficient multi-scale feature fusion, which is\nable to be used to generate a more accurate density map with dilated\nconvolutions and a $1\\times 1$ convolution. The extensive experiments on three\navailable public datasets show that our proposed network achieves\noutperformance to the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2105.11422",
          "publishedOn": "2021-05-25T01:56:11.032Z",
          "wordCount": 553,
          "title": "Multi-Level Attentive Convoluntional Neural Network for Crowd Counting. (arXiv:2105.11422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eustratiadis_P/0/1/0/all/0/1\">Panagiotis Eustratiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouk_H/0/1/0/all/0/1\">Henry Gouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>",
          "description": "Stochastic Neural Networks (SNNs) that inject noise into their hidden layers\nhave recently been shown to achieve strong robustness against adversarial\nattacks. However, existing SNNs are usually heuristically motivated, and often\nrely on adversarial training, which is computationally costly. We propose a new\nSNN that achieves state-of-the-art performance without relying on adversarial\ntraining, and enjoys solid theoretical justification. Specifically, while\nexisting SNNs inject learned or hand-tuned isotropic noise, our SNN learns an\nanisotropic noise distribution to optimize a learning-theoretic bound on\nadversarial robustness. We evaluate our method on a number of popular\nbenchmarks, show that it can be applied to different architectures, and that it\nprovides robustness to a variety of white-box and black-box attacks, while\nbeing simple and fast to train compared to existing alternatives.",
          "link": "http://arxiv.org/abs/2010.08852",
          "publishedOn": "2021-05-25T01:56:11.009Z",
          "wordCount": 589,
          "title": "Weight-Covariance Alignment for Adversarially Robust Neural Networks. (arXiv:2010.08852v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Human keypoint detection from a single image is very challenging due to\nocclusion, blur, illumination and scale variance. In this paper, we address\nthis problem from three aspects by devising an efficient network structure,\nproposing three effective training strategies, and exploiting four useful\npostprocessing techniques. First, we find that context information plays an\nimportant role in reasoning human body configuration and invisible keypoints.\nInspired by this, we propose a cascaded context mixer (CCM), which efficiently\nintegrates spatial and channel context information and progressively refines\nthem. Then, to maximize CCM's representation capability, we develop a\nhard-negative person detection mining strategy and a joint-training strategy by\nexploiting abundant unlabeled data. It enables CCM to learn discriminative\nfeatures from massive diverse poses. Third, we present several sub-pixel\nrefinement techniques for postprocessing keypoint predictions to improve\ndetection accuracy. Extensive experiments on the MS COCO keypoint detection\nbenchmark demonstrate the superiority of the proposed method over\nrepresentative state-of-the-art (SOTA) methods. Our single model achieves\ncomparable performance with the winner of the 2018 COCO Keypoint Detection\nChallenge. The final ensemble model sets a new SOTA on this benchmark.",
          "link": "http://arxiv.org/abs/2002.00537",
          "publishedOn": "2021-05-25T01:56:10.981Z",
          "wordCount": 649,
          "title": "Towards High Performance Human Keypoint Detection. (arXiv:2002.00537v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1\">Mikolaj Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>",
          "description": "State-of-the-art performance for many emerging edge applications is achieved\nby deep neural networks (DNNs). Often, these DNNs are location and time\nsensitive, and the parameters of a specific DNN must be delivered from an edge\nserver to the edge device rapidly and efficiently to carry out time-sensitive\ninference tasks. We introduce AirNet, a novel training and analog transmission\nmethod that allows efficient wireless delivery of DNNs. We first train the DNN\nwith noise injection to counter the wireless channel noise. We also employ\npruning to reduce the channel bandwidth necessary for transmission, and perform\nknowledge distillation from a larger model to achieve satisfactory performance,\ndespite the channel perturbations. We show that AirNet achieves significantly\nhigher test accuracy compared to digital alternatives under the same bandwidth\nand power constraints. It also exhibits graceful degradation with channel\nquality, which reduces the requirement for accurate channel estimation.",
          "link": "http://arxiv.org/abs/2105.11166",
          "publishedOn": "2021-05-25T01:56:10.960Z",
          "wordCount": null,
          "title": "AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sowmen Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Arup Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Md. Ruhul Amin</a>",
          "description": "The creation of altered and manipulated faces has become more common due to\nthe improvement of DeepFake generation methods. Simultaneously, we have seen\ndetection models' development for differentiating between a manipulated and\noriginal face from image or video content. We have observed that most publicly\navailable DeepFake detection datasets have limited variations, where a single\nface is used in many videos, resulting in an oversampled training dataset. Due\nto this, deep neural networks tend to overfit to the facial features instead of\nlearning to detect manipulation features of DeepFake content. As a result, most\ndetection architectures perform poorly when tested on unseen data. In this\npaper, we provide a quantitative analysis to investigate this problem and\npresent a solution to prevent model overfitting due to the high volume of\nsamples generated from a small number of actors. We introduce Face-Cutout, a\ndata augmentation method for training Convolutional Neural Networks (CNN), to\nimprove DeepFake detection. In this method, training images with various\nocclusions are dynamically generated using face landmark information\nirrespective of orientation. Unlike other general-purpose augmentation methods,\nit focuses on the facial information that is crucial for DeepFake detection.\nOur method achieves a reduction in LogLoss of 15.2% to 35.3% on different\ndatasets, compared to other occlusion-based augmentation techniques. We show\nthat Face-Cutout can be easily integrated with any CNN-based recognition model\nand improve detection performance.",
          "link": "http://arxiv.org/abs/2102.09603",
          "publishedOn": "2021-05-25T01:56:10.959Z",
          "wordCount": 690,
          "title": "Improving DeepFake Detection Using Dynamic Face Augmentation. (arXiv:2102.09603v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12764",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ghorbanzade_G/0/1/0/all/0/1\">Ghazale Ghorbanzade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nabizadeh_ShahreBabak_Z/0/1/0/all/0/1\">Zahra Nabizadeh-ShahreBabak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karimi_N/0/1/0/all/0/1\">Nader Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emami_A/0/1/0/all/0/1\">Ali Emami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khadivi_P/0/1/0/all/0/1\">Pejman Khadivi</a>",
          "description": "Brain signals could be used to control devices to assist individuals with\ndisabilities. Signals such as electroencephalograms are complicated and hard to\ninterpret. A set of signals are collected and should be classified to identify\nthe intention of the subject. Different approaches have tried to reduce the\nnumber of channels before sending them to a classifier. We are proposing a deep\nlearning-based method for selecting an informative subset of channels that\nproduce high classification accuracy. The proposed network could be trained for\nan individual subject for the selection of an appropriate set of channels.\nReduction of the number of channels could reduce the complexity of\nbrain-computer-interface devices. Our method could find a subset of channels.\nThe accuracy of our approach is comparable with a model trained on all\nchannels. Hence, our model's temporal and power costs are low, while its\naccuracy is kept high.",
          "link": "http://arxiv.org/abs/2007.12764",
          "publishedOn": "2021-05-25T01:56:10.916Z",
          "wordCount": 618,
          "title": "Selection of Proper EEG Channels for Subject Intention Classification Using Deep Learning. (arXiv:2007.12764v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingbo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "Existing face restoration researches typically relies on either the\ndegradation prior or explicit guidance labels for training, which often results\nin limited generalization ability over real-world images with heterogeneous\ndegradations and rich background contents. In this paper, we investigate the\nmore challenging and practical \"dual-blind\" version of the problem by lifting\nthe requirements on both types of prior, termed as \"Face Renovation\"(FR).\nSpecifically, we formulated FR as a semantic-guided generation problem and\ntackle it with a collaborative suppression and replenishment (CSR) approach.\nThis leads to HiFaceGAN, a multi-stage framework containing several nested CSR\nunits that progressively replenish facial details based on the hierarchical\nsemantic guidance extracted from the front-end content-adaptive suppression\nmodules. Extensive experiments on both synthetic and real face images have\nverified the superior performance of HiFaceGAN over a wide range of challenging\nrestoration subtasks, demonstrating its versatility, robustness and\ngeneralization ability towards real-world face processing applications.",
          "link": "http://arxiv.org/abs/2005.05005",
          "publishedOn": "2021-05-25T01:56:10.900Z",
          "wordCount": 626,
          "title": "HiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment. (arXiv:2005.05005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.03798",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Najgebauer_P/0/1/0/all/0/1\">Patryk Najgebauer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scherer_R/0/1/0/all/0/1\">Rafal Scherer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rutkowski_L/0/1/0/all/0/1\">Leszek Rutkowski</a>",
          "description": "Image compression is one of the essential methods of image processing. Its\nmost prominent advantage is the significant reduction of image size allowing\nfor more efficient storage and transfer. However, lossy compression is\nassociated with the loss of some image details in favor of reducing its size.\nIn compressed images, the deficiencies are manifested by noticeable defects in\nthe form of artifacts; the most common are block artifacts, ringing effect, or\nblur. In this article, we propose three models of fully convolutional networks\nwith different configurations and examine their abilities in reducing\ncompression artifacts. In the experiments, we research the extent to which the\nresults are improved for models that will process the image in a similar way to\nthe compression algorithm, and whether the initialization with predefined\nfilters would allow for better image reconstruction than developed solely\nduring learning.",
          "link": "http://arxiv.org/abs/1907.03798",
          "publishedOn": "2021-05-25T01:56:10.832Z",
          "wordCount": 611,
          "title": "Fully Convolutional Network for Removing DCT Artefacts From Images. (arXiv:1907.03798v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11361",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1\">Ankita Joshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1\">Yi Hong</a>",
          "description": "Deep diffeomorphic registration faces significant challenges for\nhigh-dimensional images, especially in terms of memory limits. Existing\napproaches either downsample original images, or approximate underlying\ntransformations, or reduce model size. The information loss during the\napproximation or insufficient model capacity is a hindrance to the registration\naccuracy for high-dimensional images, e.g., 3D medical volumes. In this paper,\nwe propose a Dividing and Downsampling mixed Registration network (DDR-Net), a\ngeneral architecture that preserves most of the image information at multiple\nscales. DDR-Net leverages the global context via downsampling the input and\nutilizes the local details from divided chunks of the input images. This design\nreduces the network input size and its memory cost; meanwhile, by fusing global\nand local information, DDR-Net obtains both coarse-level and fine-level\nalignments in the final deformation fields. We evaluate DDR-Net on three public\ndatasets, i.e., OASIS, IBSR18, and 3DIRCADB-01, and the experimental results\ndemonstrate our approach outperforms existing approaches.",
          "link": "http://arxiv.org/abs/2105.11361",
          "publishedOn": "2021-05-25T01:56:10.812Z",
          "wordCount": 590,
          "title": "DDR-Net: Dividing and Downsampling Mixed Network for Diffeomorphic Image Registration. (arXiv:2105.11361v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07962",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Basak_H/0/1/0/all/0/1\">Hritam Basak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_R/0/1/0/all/0/1\">Rukhshanda Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rana_A/0/1/0/all/0/1\">Ajay Rana</a>",
          "description": "The rapid increment of morbidity of brain stroke in the last few years have\nbeen a driving force towards fast and accurate segmentation of stroke lesions\nfrom brain MRI images. With the recent development of deep-learning,\ncomputer-aided and segmentation methods of ischemic stroke lesions have been\nuseful for clinicians in early diagnosis and treatment planning. However, most\nof these methods suffer from inaccurate and unreliable segmentation results\nbecause of their inability to capture sufficient contextual features from the\nMRI volumes. To meet these requirements, 3D convolutional neural networks have\nbeen proposed, which, however, suffer from huge computational requirements. To\nmitigate these problems, we propose a novel Dimension Fusion Edge-guided\nnetwork (DFENet) that can meet both of these requirements by fusing the\nfeatures of 2D and 3D CNNs. Unlike other methods, our proposed network uses a\nparallel partial decoder (PPD) module for aggregating and upsampling selected\nfeatures, rich in important contextual information. Additionally, we use an\nedge-guidance and enhanced mixing loss for constantly supervising and\nimprovising the learning process of the network. The proposed method is\nevaluated on publicly available Anatomical Tracings of Lesions After Stroke\n(ATLAS) dataset, resulting in mean DSC, IoU, Precision and Recall values of\n0.5457, 0.4015, 0.6371, and 0.4969 respectively. The results, when compared to\nother state-of-the-art methods, outperforms them by a significant margin.\nTherefore, the proposed model is robust, accurate, superior to the existing\nmethods, and can be relied upon for biomedical applications.",
          "link": "http://arxiv.org/abs/2105.07962",
          "publishedOn": "2021-05-25T01:56:10.631Z",
          "wordCount": 704,
          "title": "DFENet: A Novel Dimension Fusion Edge Guided Network for Brain MRI Segmentation. (arXiv:2105.07962v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fuyan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>",
          "description": "Facial Expression Recognition (FER) in the wild is extremely challenging due\nto occlusions, variant head poses, face deformation and motion blur under\nunconstrained conditions. Although substantial progresses have been made in\nautomatic FER in the past few decades, previous studies are mainly designed for\nlab-controlled FER. Real-world occlusions, variant head poses and other issues\ndefinitely increase the difficulty of FER on account of these\ninformation-deficient regions and complex backgrounds. Different from previous\npure CNNs based methods, we argue that it is feasible and practical to\ntranslate facial images into sequences of visual words and perform expression\nrecognition from a global perspective. Therefore, we propose Convolutional\nVisual Transformers to tackle FER in the wild by two main steps. First, we\npropose an attentional selective fusion (ASF) for leveraging the feature maps\ngenerated by two-branch CNNs. The ASF captures discriminative information by\nfusing multiple features with global-local attention. The fused feature maps\nare then flattened and projected into sequences of visual words. Second,\ninspired by the success of Transformers in natural language processing, we\npropose to model relationships between these visual words with global\nself-attention. The proposed method are evaluated on three public in-the-wild\nfacial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same\nsettings, extensive experiments demonstrate that our method shows superior\nperformance over other methods, setting new state of the art on RAF-DB with\n88.14%, FERPlus with 88.81% and AffectNet with 61.85%. We also conduct\ncross-dataset evaluation on CK+ show the generalization capability of the\nproposed method.",
          "link": "http://arxiv.org/abs/2103.16854",
          "publishedOn": "2021-05-25T01:56:10.623Z",
          "wordCount": 701,
          "title": "Robust Facial Expression Recognition with Convolutional Visual Transformers. (arXiv:2103.16854v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Andr&#xe9;s C. Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan D.Wegner</a>",
          "description": "Accurate mapping of oil palm is important for understanding its past and\nfuture impact on the environment. We propose to map and count oil palms by\nestimating tree densities per pixel for large-scale analysis. This allows for\nfine-grained analysis, for example regarding different planting patterns. To\nthat end, we propose a new, active deep learning method to estimate oil palm\ndensity at large scale from Sentinel-2 satellite images, and apply it to\ngenerate complete maps for Malaysia and Indonesia. What makes the regression of\noil palm density challenging is the need for representative reference data that\ncovers all relevant geographical conditions across a large territory.\nSpecifically for density estimation, generating reference data involves\ncounting individual trees. To keep the associated labelling effort low we\npropose an active learning (AL) approach that automatically chooses the most\nrelevant samples to be labelled. Our method relies on estimates of the\nepistemic model uncertainty and of the diversity among samples, making it\npossible to retrieve an entire batch of relevant samples in a single iteration.\nMoreover, our algorithm has linear computational complexity and is easily\nparallelisable to cover large areas. We use our method to compute the first oil\npalm density map with $10\\,$m Ground Sampling Distance (GSD) , for all of\nIndonesia and Malaysia and for two different years, 2017 and 2019. The maps\nhave a mean absolute error of $\\pm$7.3 trees/$ha$, estimated from an\nindependent validation set. We also analyse density variations between\ndifferent states within a country and compare them to official estimates.\nAccording to our estimates there are, in total, $>1.2$ billion oil palms in\nIndonesia covering $>$15 million $ha$, and $>0.5$ billion oil palms in Malaysia\ncovering $>6$ million $ha$.",
          "link": "http://arxiv.org/abs/2105.11207",
          "publishedOn": "2021-05-25T01:56:10.571Z",
          "wordCount": 735,
          "title": "Mapping oil palm density at country scale: An active learning approach. (arXiv:2105.11207v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>",
          "description": "This paper proposes a novel deep learning-based video object matting method\nthat can achieve temporally coherent matting results. Its key component is an\nattention-based temporal aggregation module that maximizes image matting\nnetworks' strength for video matting networks. This module computes temporal\ncorrelations for pixels adjacent to each other along the time axis in feature\nspace to be robust against motion noises. We also design a novel loss term to\ntrain the attention weights, which drastically boosts the video matting\nperformance. Besides, we show how to effectively solve the trimap generation\nproblem by fine-tuning a state-of-the-art video object segmentation network\nwith a sparse set of user-annotated keyframes. To facilitate video matting and\ntrimap generation networks' training, we construct a large-scale video matting\ndataset with 80 training and 28 validation foreground video clips with\nground-truth alpha mattes. Experimental results show that our method can\ngenerate high-quality alpha mattes for various videos featuring appearance\nchange, occlusion, and fast motion. Our code and dataset can be found at\nhttps://github.com/yunkezhang/TCVOM",
          "link": "http://arxiv.org/abs/2105.11427",
          "publishedOn": "2021-05-25T01:56:10.521Z",
          "wordCount": 608,
          "title": "Attention-guided Temporal Coherent Video Object Matting. (arXiv:2105.11427v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10556",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1\">Julio Silva-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paya_Bosch_E/0/1/0/all/0/1\">Elena Pay&#xe1;-Bosch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1\">Gabriel Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Prostate cancer is one of the most prevalent cancers worldwide. One of the\nkey factors in reducing its mortality is based on early detection. The\ncomputer-aided diagnosis systems for this task are based on the glandular\nstructural analysis in histology images. Hence, accurate gland detection and\nsegmentation is crucial for a successful prediction. The methodological basis\nof this work is a prostate gland segmentation based on U-Net convolutional\nneural network architectures modified with residual and multi-resolution\nblocks, trained using data augmentation techniques. The residual configuration\noutperforms in the test subset the previous state-of-the-art approaches in an\nimage-level comparison, reaching an average Dice Index of 0.77.",
          "link": "http://arxiv.org/abs/2105.10556",
          "publishedOn": "2021-05-25T01:56:10.378Z",
          "wordCount": 558,
          "title": "Prostate Gland Segmentation in Histology Images via Residual and Multi-Resolution U-Net. (arXiv:2105.10556v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hannah Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1\">Girmaw Abebe Tadesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1\">Celia Cintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1\">Skyler Speakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1\">Kush Varshney</a>",
          "description": "Recent advances in deep learning have led to breakthroughs in the development\nof automated skin disease classification. As we observe an increasing interest\nin these models in the dermatology space, it is crucial to address aspects such\nas the robustness towards input data distribution shifts. Current skin disease\nmodels could make incorrect inferences for test samples from different hardware\ndevices and clinical settings or unknown disease samples, which are\nout-of-distribution (OOD) from the training samples.To this end, we propose a\nsimple yet effective approach that detect these OOD samples prior to making any\ndecision. The detection is performed via scanning in the latent space\nrepresentation (e.g., activations of the inner layers of any pre-trained skin\ndisease classifier). The input samples could also perturbed to maximise\ndivergence of OOD samples. We validate our ODD detection approach in two use\ncases: 1) identify samples collected from different protocols, and 2) detect\nsamples from unknown disease classes. Additionally, we evaluate the performance\nof the proposed approach and compare it with other state-of-the-art methods.\nFurthermore, data-driven dermatology applications may deepen the disparity in\nclinical care across racial and ethnic groups since most datasets are reported\nto suffer from bias in skin tone distribution. Therefore, we also evaluate the\nfairness of these OOD detection methods across different skin tones. Our\nexperiments resulted in competitive performance across multiple datasets in\ndetecting OOD samples, which could be used (in the future) to design more\neffective transfer learning techniques prior to inferring on these samples.",
          "link": "http://arxiv.org/abs/2105.11160",
          "publishedOn": "2021-05-25T01:56:10.353Z",
          "wordCount": 699,
          "title": "Out-of-Distribution Detection in Dermatology using Input Perturbation and Subset Scanning. (arXiv:2105.11160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "On the existing benchmark datasets, THUMOS14 and ActivityNet, temporal action\nlocalization techniques have achieved great success. However, there are still\nexisting some problems, such as the source of the action is too single, there\nare only sports categories in THUMOS14, coarse instances with uncertain\nboundaries in ActivityNet and HACS Segments interfering with proposal\ngeneration and behavior prediction. To take temporal action localization to a\nnew level, we develop FineAction, a new large-scale fined video dataset\ncollected from existing video datasets and web videos. Overall, this dataset\ncontains 139K fined action instances densely annotated in almost 17K untrimmed\nvideos spanning 106 action categories. FineAction has a more fined definition\nof action categories and high-quality annotations to reduce the boundary\nuncertainty compared to the existing action localization datasets. We\nsystematically investigate representative methods of temporal action\nlocalization on our dataset and obtain some interesting findings with further\nanalysis. Experimental results reveal that our FineAction brings new challenges\nfor action localization on fined and multi-label instances with shorter\nduration. This dataset will be public in the future and we hope our FineAction\ncould advance research towards temporal action localization. Our dataset\nwebsite is at https://deeperaction.github.io/fineaction/.",
          "link": "http://arxiv.org/abs/2105.11107",
          "publishedOn": "2021-05-25T01:56:10.346Z",
          "wordCount": 638,
          "title": "FineAction: A Fined Video Dataset for Temporal Action Localization. (arXiv:2105.11107v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salazar_Gomez_G/0/1/0/all/0/1\">Gustavo A. Salazar-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saavedra_Ruiz_M/0/1/0/all/0/1\">Miguel A. Saavedra-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Cano_V/0/1/0/all/0/1\">Victor A. Romero-Cano</a>",
          "description": "This paper tackles the 3D object detection problem, which is of vital\nimportance for applications such as autonomous driving. Our framework uses a\nMachine Learning (ML) pipeline on a combination of monocular camera and LiDAR\ndata to detect vehicles in the surrounding 3D space of a moving platform. It\nuses frustum region proposals generated by State-Of-The-Art (SOTA) 2D object\ndetectors to segment LiDAR point clouds into point clusters which represent\npotentially individual objects. We evaluate the performance of classical ML\nalgorithms as part of an holistic pipeline for estimating the parameters of 3D\nbounding boxes which surround the vehicles around the moving platform. Our\nresults demonstrate an efficient and accurate inference on a validation set,\nachieving an overall accuracy of 87.1%.",
          "link": "http://arxiv.org/abs/2105.11060",
          "publishedOn": "2021-05-25T01:56:10.325Z",
          "wordCount": 571,
          "title": "High-level camera-LiDAR fusion for 3D object detection with machine learning. (arXiv:2105.11060v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hewei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathan_M/0/1/0/all/0/1\">Muhammad Salman Pathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Soumyabrata Dev</a>",
          "description": "The area of computer vision is one of the most discussed topics amongst many\nscholars, and stereo matching is its most important sub fields. After the\nparallax map is transformed into a depth map, it can be applied to many\nintelligent fields. In this paper, a stereo matching algorithm based on visual\nsensitive information is proposed by using standard images from Middlebury\ndataset. Aiming at the limitation of traditional stereo matching algorithms\nregarding the cost window, a cost aggregation algorithm based on the dynamic\nwindow is proposed, and the disparity image is optimized by using left and\nright consistency detection to further reduce the error matching rate. The\nexperimental results show that the proposed algorithm can effectively enhance\nthe stereo matching effect of the image providing significant improvement in\naccuracy as compared with the classical census algorithm. The proposed model\ncode, dataset, and experimental results are available at\nhttps://github.com/WangHewei16/Stereo-Matching.",
          "link": "http://arxiv.org/abs/2105.10831",
          "publishedOn": "2021-05-25T01:56:10.227Z",
          "wordCount": 592,
          "title": "Stereo Matching Based on Visual Sensitive Information. (arXiv:2105.10831v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11239",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorent_R/0/1/0/all/0/1\">Reuben Dorent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rizzi_M/0/1/0/all/0/1\">Michele Rizzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardinale_F/0/1/0/all/0/1\">Francesco Cardinale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frazzini_V/0/1/0/all/0/1\">Valerio Frazzini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navarro_V/0/1/0/all/0/1\">Vincent Navarro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Essert_C/0/1/0/all/0/1\">Caroline Essert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ollivier_I/0/1/0/all/0/1\">Ir&#xe8;ne Ollivier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1\">Rachel Sparks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1\">John S. Duncan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>",
          "description": "Accurate segmentation of brain resection cavities (RCs) aids in postoperative\nanalysis and determining follow-up treatment. Convolutional neural networks\n(CNNs) are the state-of-the-art image segmentation technique, but require large\nannotated datasets for training. Annotation of 3D medical images is\ntime-consuming, requires highly-trained raters, and may suffer from high\ninter-rater variability. Self-supervised learning strategies can leverage\nunlabeled data for training.\n\nWe developed an algorithm to simulate resections from preoperative magnetic\nresonance images (MRIs). We performed self-supervised training of a 3D CNN for\nRC segmentation using our simulation method. We curated EPISURG, a dataset\ncomprising 430 postoperative and 268 preoperative MRIs from 430 refractory\nepilepsy patients who underwent resective neurosurgery. We fine-tuned our model\non three small annotated datasets from different institutions and on the\nannotated images in EPISURG, comprising 20, 33, 19 and 133 subjects.\n\nThe model trained on data with simulated resections obtained median\n(interquartile range) Dice score coefficients (DSCs) of 81.7 (16.4), 82.4\n(36.4), 74.9 (24.2) and 80.5 (18.7) for each of the four datasets. After\nfine-tuning, DSCs were 89.2 (13.3), 84.1 (19.8), 80.2 (20.1) and 85.2 (10.8).\nFor comparison, inter-rater agreement between human annotators from our\nprevious study was 84.0 (9.9).\n\nWe present a self-supervised learning strategy for 3D CNNs using simulated\nRCs to accurately segment real RCs on postoperative MRI. Our method generalizes\nwell to data from different institutions, pathologies and modalities. Source\ncode, segmentation models and the EPISURG dataset are available at\nhttps://github.com/fepegar/ressegijcars .",
          "link": "http://arxiv.org/abs/2105.11239",
          "publishedOn": "2021-05-25T01:56:10.182Z",
          "wordCount": 720,
          "title": "A self-supervised learning strategy for postoperative brain cavity segmentation simulating resections. (arXiv:2105.11239v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11187",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kiourt_C/0/1/0/all/0/1\">Chairi Kiourt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feretzakis_G/0/1/0/all/0/1\">Georgios Feretzakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalamarinis_K/0/1/0/all/0/1\">Konstantinos Dalamarinis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalles_D/0/1/0/all/0/1\">Dimitris Kalles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pantos_G/0/1/0/all/0/1\">Georgios Pantos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papadopoulos_I/0/1/0/all/0/1\">Ioannis Papadopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kouris_S/0/1/0/all/0/1\">Spyros Kouris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ioannakis_G/0/1/0/all/0/1\">George Ioannakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loupelis_E/0/1/0/all/0/1\">Evangelos Loupelis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sakagianni_A/0/1/0/all/0/1\">Aikaterini Sakagianni</a>",
          "description": "The main objective of this work is to utilize state-of-the-art deep learning\napproaches for the identification of pulmonary embolism in CTPA-Scans for\nCOVID-19 patients, provide an initial assessment of their performance and,\nultimately, provide a fast-track prototype solution (system). We adopted and\nassessed some of the most popular convolutional neural network architectures\nthrough transfer learning approaches, to strive to combine good model accuracy\nwith fast training. Additionally, we exploited one of the most popular\none-stage object detection models for the localization (through object\ndetection) of the pulmonary embolism regions-of-interests. The models of both\napproaches are trained on an original CTPA-Scan dataset, where we annotated of\n673 CTPA-Scan images with 1,465 bounding boxes in total, highlighting pulmonary\nembolism regions-of-interests. We provide a brief assessment of some\nstate-of-the-art image classification models by achieving validation accuracies\nof 91% in pulmonary embolism classification. Additionally, we achieved a\nprecision of about 68% on average in the object detection model for the\npulmonary embolism localization under 50% IoU threshold. For both approaches,\nwe provide the entire training pipelines for future studies (step by step\nprocesses through source code). In this study, we present some of the most\naccurate and fast deep learning models for pulmonary embolism identification in\nCTPA-Scans images, through classification and localization (object detection)\napproaches for patients infected by COVID-19. We provide a fast-track solution\n(system) for the research community of the area, which combines both\nclassification and object detection models for improving the precision of\nidentifying pulmonary embolisms.",
          "link": "http://arxiv.org/abs/2105.11187",
          "publishedOn": "2021-05-25T01:56:10.175Z",
          "wordCount": 774,
          "title": "Pulmonary embolism identification in computerized tomography pulmonary angiography scans with deep learning technologies in COVID-19 patients. (arXiv:2105.11187v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wentong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>",
          "description": "In contrast to the oriented bounding boxes, point set representation has\ngreat potential to capture the detailed structure of instances with the\narbitrary orientations, large aspect ratios and dense distribution in aerial\nimages. However, the conventional point set-based approaches are handcrafted\nwith the fixed locations using points-to-points supervision, which hurts their\nflexibility on the fine-grained feature extraction. To address these\nlimitations, in this paper, we propose a novel approach to aerial object\ndetection, named Oriented RepPoints. Specifically, we suggest to employ a set\nof adaptive points to capture the geometric and spatial information of the\narbitrary-oriented objects, which is able to automatically arrange themselves\nover the object in a spatial and semantic scenario. To facilitate the\nsupervised learning, the oriented conversion function is proposed to explicitly\nmap the adaptive point set into an oriented bounding box. Moreover, we\nintroduce an effective quality assessment measure to select the point set\nsamples for training, which can choose the representative items with respect to\ntheir potentials on orientated object detection. Furthermore, we suggest a\nspatial constraint to penalize the outlier points outside the ground-truth\nbounding box. In addition to the traditional evaluation metric mAP focusing on\noverlap ratio, we propose a new metric mAOE to measure the orientation accuracy\nthat is usually neglected in the previous studies on oriented object detection.\nExperiments on three widely used datasets including DOTA, HRSC2016 and UCAS-AOD\ndemonstrate that our proposed approach is effective.",
          "link": "http://arxiv.org/abs/2105.11111",
          "publishedOn": "2021-05-25T01:56:10.168Z",
          "wordCount": 660,
          "title": "Oriented RepPoints for Aerial Object Detection. (arXiv:2105.11111v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qinwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Modern deep neural networks suffer from performance degradation when\nevaluated on testing data under different distributions from training data.\nDomain generalization aims at tackling this problem by learning transferable\nknowledge from multiple source domains in order to generalize to unseen target\ndomains. This paper introduces a novel Fourier-based perspective for domain\ngeneralization. The main assumption is that the Fourier phase information\ncontains high-level semantics and is not easily affected by domain shifts. To\nforce the model to capture phase information, we develop a novel Fourier-based\ndata augmentation strategy called amplitude mix which linearly interpolates\nbetween the amplitude spectrums of two images. A dual-formed consistency loss\ncalled co-teacher regularization is further introduced between the predictions\ninduced from original and augmented images. Extensive experiments on three\nbenchmarks have demonstrated that the proposed method is able to achieve\nstate-of-the-arts performance for domain generalization.",
          "link": "http://arxiv.org/abs/2105.11120",
          "publishedOn": "2021-05-25T01:56:10.161Z",
          "wordCount": 574,
          "title": "A Fourier-based Framework for Domain Generalization. (arXiv:2105.11120v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yueyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>",
          "description": "Recently, most siamese network based trackers locate targets via object\nclassification and bounding-box regression. Generally, they select the\nbounding-box with maximum classification confidence as the final prediction.\nThis strategy may miss the right result due to the accuracy misalignment\nbetween classification and regression. In this paper, we propose a novel\nsiamese tracking algorithm called SiamRCR, addressing this problem with a\nsimple, light and effective solution. It builds reciprocal links between\nclassification and regression branches, which can dynamically re-weight their\nlosses for each positive sample. In addition, we add a localization branch to\npredict the localization accuracy, so that it can work as the replacement of\nthe regression assistance link during inference. This branch makes the training\nand inference more consistent. Extensive experimental results demonstrate the\neffectiveness of SiamRCR and its superiority over the state-of-the-art\ncompetitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.\nMoreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.",
          "link": "http://arxiv.org/abs/2105.11237",
          "publishedOn": "2021-05-25T01:56:10.134Z",
          "wordCount": 606,
          "title": "SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking. (arXiv:2105.11237v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fani_M/0/1/0/all/0/1\">Mehrnaz Fani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clausi_D/0/1/0/all/0/1\">David A. Clausi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1\">John Zelek</a>",
          "description": "Puck localization is an important problem in ice hockey video analytics\nuseful for analyzing the game, determining play location, and assessing puck\npossession. The problem is challenging due to the small size of the puck,\nexcessive motion blur due to high puck velocity and occlusions due to players\nand boards. In this paper, we introduce and implement a network for puck\nlocalization in broadcast hockey video. The network leverages expert NHL\nplay-by-play annotations and uses temporal context to locate the puck. Player\nlocations are incorporated into the network through an attention mechanism by\nencoding player positions with a Gaussian-based spatial heatmap drawn at player\npositions. Since event occurrence on the rink and puck location are related, we\nalso perform event recognition by augmenting the puck localization network with\nan event recognition head and training the network through multi-task learning.\nExperimental results demonstrate that the network is able to localize the puck\nwith an AUC of $73.1 \\%$ on the test set. The puck location can be inferred in\n720p broadcast videos at $5$ frames per second. It is also demonstrated that\nmulti-task learning with puck location improves event recognition accuracy.",
          "link": "http://arxiv.org/abs/2105.10563",
          "publishedOn": "2021-05-25T01:56:10.084Z",
          "wordCount": 629,
          "title": "Puck localization and multi-task event recognition in broadcast hockey videos. (arXiv:2105.10563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gasimova_A/0/1/0/all/0/1\">Aydan Gasimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montana_G/0/1/0/all/0/1\">Giovanni Montana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "Gathering manually annotated images for the purpose of training a predictive\nmodel is far more challenging in the medical domain than for natural images as\nit requires the expertise of qualified radiologists. We therefore propose to\ntake advantage of past radiological exams (specifically, knee X-ray\nexaminations) and formulate a framework capable of learning the correspondence\nbetween the images and reports, and hence be capable of generating diagnostic\nreports for a given X-ray examination consisting of an arbitrary number of\nimage views. We demonstrate how aggregating the image features of individual\nexams and using them as conditional inputs when training a language generation\nmodel results in auto-generated exam reports that correlate well with\nradiologist-generated reports.",
          "link": "http://arxiv.org/abs/2105.10702",
          "publishedOn": "2021-05-25T01:56:10.013Z",
          "wordCount": 548,
          "title": "Automated Knee X-ray Report Generation. (arXiv:2105.10702v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Ting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shiping Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_A/0/1/0/all/0/1\">Aidong Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiying Wang</a>",
          "description": "Video prediction is a challenging task with wide application prospects in\nmeteorology and robot systems. Existing works fail to trade off short-term and\nlong-term prediction performances and extract robust latent dynamics laws in\nvideo frames. We propose a two-branch seq-to-seq deep model to disentangle the\nTaylor feature and the residual feature in video frames by a novel recurrent\nprediction module (TaylorCell) and residual module. TaylorCell can expand the\nvideo frames' high-dimensional features into the finite Taylor series to\ndescribe the latent laws. In TaylorCell, we propose the Taylor prediction unit\n(TPU) and the memory correction unit (MCU). TPU employs the first input frame's\nderivative information to predict the future frames, avoiding error\naccumulation. MCU distills all past frames' information to correct the\npredicted Taylor feature from TPU. Correspondingly, the residual module\nextracts the residual feature complementary to the Taylor feature. On three\ngeneralist datasets (Moving MNIST, TaxiBJ, Human 3.6), our model outperforms or\nreaches state-of-the-art models, and ablation experiments demonstrate the\neffectiveness of our model in long-term prediction.",
          "link": "http://arxiv.org/abs/2105.11062",
          "publishedOn": "2021-05-25T01:56:09.991Z",
          "wordCount": 612,
          "title": "Taylor saves for later: disentanglement for video prediction using Taylor representation. (arXiv:2105.11062v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10603",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sadhu_S/0/1/0/all/0/1\">Subhash Chandra Sadhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1\">Abhishek Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maeda_T/0/1/0/all/0/1\">Tomohiro Maeda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Swedish_T/0/1/0/all/0/1\">Tristan Swedish</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_R/0/1/0/all/0/1\">Ryan Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinha_L/0/1/0/all/0/1\">Lagnojita Sinha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>",
          "description": "Time of flight based Non-line-of-sight (NLOS) imaging approaches require\nprecise calibration of illumination and detector positions on the visible scene\nto produce reasonable results. If this calibration error is sufficiently high,\nreconstruction can fail entirely without any indication to the user. In this\nwork, we highlight the necessity of building autocalibration into NLOS\nreconstruction in order to handle mis-calibration. We propose a forward model\nof NLOS measurements that is differentiable with respect to both, the hidden\nscene albedo, and virtual illumination and detector positions. With only a mean\nsquared error loss and no regularization, our model enables joint\nreconstruction and recovery of calibration parameters by minimizing the\nmeasurement residual using gradient descent. We demonstrate our method is able\nto produce robust reconstructions using simulated and real data where the\ncalibration error applied causes other state of the art algorithms to fail.",
          "link": "http://arxiv.org/abs/2105.10603",
          "publishedOn": "2021-05-25T01:56:09.985Z",
          "wordCount": 589,
          "title": "Automatic calibration of time of flight based non-line-of-sight reconstruction. (arXiv:2105.10603v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruobing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jikuan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqiong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenlong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haixia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "3D ultrasound (US) has become prevalent due to its rich spatial and\ndiagnostic information not contained in 2D US. Moreover, 3D US can contain\nmultiple standard planes (SPs) in one shot. Thus, automatically localizing SPs\nin 3D US has the potential to improve user-independence and\nscanning-efficiency. However, manual SP localization in 3D US is challenging\nbecause of the low image quality, huge search space and large anatomical\nvariability. In this work, we propose a novel multi-agent reinforcement\nlearning (MARL) framework to simultaneously localize multiple SPs in 3D US. Our\ncontribution is four-fold. First, our proposed method is general and it can\naccurately localize multiple SPs in different challenging US datasets. Second,\nwe equip the MARL system with a recurrent neural network (RNN) based\ncollaborative module, which can strengthen the communication among agents and\nlearn the spatial relationship among planes effectively. Third, we explore to\nadopt the neural architecture search (NAS) to automatically design the network\narchitecture of both the agents and the collaborative module. Last, we believe\nwe are the first to realize automatic SP localization in pelvic US volumes, and\nnote that our approach can handle both normal and abnormal uterus cases.\nExtensively validated on two challenging datasets of the uterus and fetal\nbrain, our proposed method achieves the average localization accuracy of 7.03\ndegrees/1.59mm and 9.75 degrees/1.19mm. Experimental results show that our\nlight-weight MARL model has higher accuracy than state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.10626",
          "publishedOn": "2021-05-25T01:56:09.966Z",
          "wordCount": 703,
          "title": "Searching Collaborative Agents for Multi-plane Localization in 3D Ultrasound. (arXiv:2105.10626v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McGuire_M/0/1/0/all/0/1\">Michael McGuire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soman_C/0/1/0/all/0/1\">Chinmay Soman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diers_B/0/1/0/all/0/1\">Brian Diers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>",
          "description": "We report promising results for high-throughput on-field soybean pod count\nwith small mobile robots and machine-vision algorithms. Our results show that\nthe machine-vision based soybean pod counts are strongly correlated with\nsoybean yield. While pod counts has a strong correlation with soybean yield,\npod counting is extremely labor intensive, and has been difficult to automate.\nOur results establish that an autonomous robot equipped with vision sensors can\nautonomously collect soybean data at maturity. Machine-vision algorithms can be\nused to estimate pod-counts across a large diversity panel planted across\nexperimental units (EUs, or plots) in a high-throughput, automated manner. We\nreport a correlation of 0.67 between our automated pod counts and soybean\nyield. The data was collected in an experiment consisting of 1463 single-row\nplots maintained by the University of Illinois soybean breeding program during\nthe 2020 growing season. We also report a correlation of 0.88 between automated\npod counts and manual pod counts over a smaller data set of 16 plots.",
          "link": "http://arxiv.org/abs/2105.10568",
          "publishedOn": "2021-05-25T01:56:09.941Z",
          "wordCount": 601,
          "title": "High Throughput Soybean Pod-Counting with In-Field Robotic Data Collection and Machine-Vision Based Data Analysis. (arXiv:2105.10568v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>",
          "description": "The existing solutions for object detection distillation rely on the\navailability of both a teacher model and ground-truth labels. We propose a new\nperspective to relax this constraint. In our framework, a student is first\ntrained with pseudo labels generated by the teacher, and then fine-tuned using\nlabeled data, if any available. Extensive experiments demonstrate improvements\nover existing object detection distillation algorithms. In addition, decoupling\nthe teacher and ground-truth distillation in this framework provides\ninteresting properties such: as 1) using unlabeled data to further improve the\nstudent's performance, 2) combining multiple teacher models of different\narchitectures, even with different object categories, and 3) reducing the need\nfor labeled data (with only 20% of COCO labels, this method achieves the same\nperformance as the model trained on the entire set of labels). Furthermore, a\nby-product of this approach is the potential usage for domain adaptation. We\nverify these properties through extensive experiments.",
          "link": "http://arxiv.org/abs/2105.10633",
          "publishedOn": "2021-05-25T01:56:09.881Z",
          "wordCount": 570,
          "title": "Revisiting Knowledge Distillation for Object Detection. (arXiv:2105.10633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08059",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1\">Yilmaz Korkmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1\">Salman UH Dar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1\">Muzaffer &#xd6;zbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>",
          "description": "Supervised deep learning has swiftly become a workhorse for accelerated MRI\nin recent years, offering state-of-the-art performance in image reconstruction\nfrom undersampled acquisitions. Training deep supervised models requires large\ndatasets of undersampled and fully-sampled acquisitions typically from a\nmatching set of subjects. Given scarce access to large medical datasets, this\nlimitation has sparked interest in unsupervised methods that reduce reliance on\nfully-sampled ground-truth data. A common framework is based on the deep image\nprior, where network-driven regularization is enforced directly during\ninference on undersampled acquisitions. Yet, canonical convolutional\narchitectures are suboptimal in capturing long-range relationships, and\nrandomly initialized networks may hamper convergence. To address these\nlimitations, here we introduce a novel unsupervised MRI reconstruction method\nbased on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a\ndeep adversarial network with cross-attention transformer blocks to map noise\nand latent variables onto MR images. This unconditional network learns a\nhigh-quality MRI prior in a self-supervised encoding task. A zero-shot\nreconstruction is performed on undersampled test data, where inference is\nperformed by optimizing network parameters, latent and noise variables to\nensure maximal consistency to multi-coil MRI data. Comprehensive experiments on\nbrain MRI datasets clearly demonstrate the superior performance of SLATER\nagainst several state-of-the-art unsupervised methods.",
          "link": "http://arxiv.org/abs/2105.08059",
          "publishedOn": "2021-05-24T05:08:43.206Z",
          "wordCount": 666,
          "title": "Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers. (arXiv:2105.08059v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.12041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Quan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fagui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>",
          "description": "The way features propagate in Fully Convolutional Networks is of momentous\nimportance to capture multi-scale contexts for obtaining precise segmentation\nmasks. This paper proposes a novel series-parallel hybrid paradigm called the\nChained Context Aggregation Module (CAM) to diversify feature propagation. CAM\ngains features of various spatial scales through chain-connected ladder-style\ninformation flows and fuses them in a two-stage process, namely pre-fusion and\nre-fusion. The serial flow continuously increases receptive fields of output\nneurons and those in parallel encode different region-based contexts. Each\ninformation flow is a shallow encoder-decoder with appropriate down-sampling\nscales to sufficiently capture contextual information. We further adopt an\nattention model in CAM to guide feature re-fusion. Based on these developments,\nwe construct the Chained Context Aggregation Network (CANet), which employs an\nasymmetric decoder to recover precise spatial details of prediction maps. We\nconduct extensive experiments on six challenging datasets, including Pascal VOC\n2012, Pascal Context, Cityscapes, CamVid, SUN-RGBD and GATECH. Results evidence\nthat CANet achieves state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2002.12041",
          "publishedOn": "2021-05-24T05:08:43.184Z",
          "wordCount": 647,
          "title": "Attention-guided Chained Context Aggregation for Semantic Segmentation. (arXiv:2002.12041v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.01456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouaknine_A/0/1/0/all/0/1\">A. Ouaknine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1\">A. Newson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebut_J/0/1/0/all/0/1\">J. Rebut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tupin_F/0/1/0/all/0/1\">F. Tupin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">P. P&#xe9;rez</a>",
          "description": "High quality perception is essential for autonomous driving (AD) systems. To\nreach the accuracy and robustness that are required by such systems, several\ntypes of sensors must be combined. Currently, mostly cameras and laser scanners\n(lidar) are deployed to build a representation of the world around the vehicle.\nWhile radar sensors have been used for a long time in the automotive industry,\nthey are still under-used for AD despite their appealing characteristics\n(notably, their ability to measure the relative speed of obstacles and to\noperate even in adverse weather conditions). To a large extent, this situation\nis due to the relative lack of automotive datasets with real radar signals that\nare both raw and annotated. In this work, we introduce CARRADA, a dataset of\nsynchronized camera and radar recordings with range-angle-Doppler annotations.\nWe also present a semi-automatic annotation approach, which was used to\nannotate the dataset, and a radar semantic segmentation baseline, which we\nevaluate on several metrics. Both our code and dataset are available online.",
          "link": "http://arxiv.org/abs/2005.01456",
          "publishedOn": "2021-05-24T05:08:43.164Z",
          "wordCount": 685,
          "title": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations. (arXiv:2005.01456v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.08051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zibo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Ahmed Shehab Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OReilly_J/0/1/0/all/0/1\">James O&#x27;Reilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yan Tong</a>",
          "description": "A novel Identity-Free conditional Generative Adversarial Network (IF-GAN) was\nproposed for Facial Expression Recognition (FER) to explicitly reduce high\ninter-subject variations caused by identity-related facial attributes, e.g.,\nage, race, and gender. As part of an end-to-end system, a cGAN was designed to\ntransform a given input facial expression image to an \"average\" identity face\nwith the same expression as the input. Then, identity-free FER is possible\nsince the generated images have the same synthetic \"average\" identity and\ndiffer only in their displayed expressions. Experiments on four facial\nexpression datasets, one with spontaneous expressions, show that IF-GAN\noutperforms the baseline CNN and achieves state-of-the-art performance for FER.",
          "link": "http://arxiv.org/abs/1903.08051",
          "publishedOn": "2021-05-24T05:08:43.156Z",
          "wordCount": 576,
          "title": "Identity-Free Facial Expression Recognition using conditional Generative Adversarial Network. (arXiv:1903.08051v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Etten_A/0/1/0/all/0/1\">Adam Van Etten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_D/0/1/0/all/0/1\">Daniel Hogan</a>",
          "description": "Building footprints provide a useful proxy for a great many humanitarian\napplications. For example, building footprints are useful for high fidelity\npopulation estimates, and quantifying population statistics is fundamental to\n~1/4 of the United Nations Sustainable Development Goals Indicators. In this\npaper we (the SpaceNet Partners) discuss efforts to develop techniques for\nprecise building footprint localization, tracking, and change detection via the\nSpaceNet Multi-Temporal Urban Development Challenge (also known as SpaceNet 7).\nIn this NeurIPS 2020 competition, participants were asked identify and track\nbuildings in satellite imagery time series collected over rapidly urbanizing\nareas. The competition centered around a brand new open source dataset of\nPlanet Labs satellite imagery mosaics at 4m resolution, which includes 24\nimages (one per month) covering ~100 unique geographies. Tracking individual\nbuildings at this resolution is quite challenging, yet the winning participants\ndemonstrated impressive performance with the newly developed SpaceNet Change\nand Object Tracking (SCOT) metric. This paper details the top-5 winning\napproaches, as well as analysis of results that yielded a handful of\ninteresting anecdotes such as decreasing performance with latitude.",
          "link": "http://arxiv.org/abs/2102.11958",
          "publishedOn": "2021-05-24T05:08:43.135Z",
          "wordCount": 652,
          "title": "The SpaceNet Multi-Temporal Urban Development Challenge. (arXiv:2102.11958v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyusong Lee</a>",
          "description": "Text-to-image retrieval is an essential task in cross-modal information\nretrieval, i.e., retrieving relevant images from a large and unlabelled dataset\ngiven textual queries. In this paper, we propose VisualSparta, a novel\n(Visual-text Sparse Transformer Matching) model that shows significant\nimprovement in terms of both accuracy and efficiency. VisualSparta is capable\nof outperforming previous state-of-the-art scalable methods in MSCOCO and\nFlickr30K. We also show that it achieves substantial retrieving speed\nadvantages, i.e., for a 1 million image index, VisualSparta using CPU gets\n~391X speedup compared to CPU vector search and ~5.4X speedup compared to\nvector search with GPU acceleration. Experiments show that this speed advantage\neven gets bigger for larger datasets because VisualSparta can be efficiently\nimplemented as an inverted index. To the best of our knowledge, VisualSparta is\nthe first transformer-based text-to-image retrieval model that can achieve\nreal-time searching for large-scale datasets, with significant accuracy\nimprovement compared to previous state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2101.00265",
          "publishedOn": "2021-05-24T05:08:43.123Z",
          "wordCount": 629,
          "title": "VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words. (arXiv:2101.00265v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alghonaim_R/0/1/0/all/0/1\">Raghad Alghonaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>",
          "description": "Domain randomisation is a very popular method for visual sim-to-real transfer\nin robotics, due to its simplicity and ability to achieve transfer without any\nreal-world images at all. Nonetheless, a number of design choices must be made\nto achieve optimal transfer. In this paper, we perform a comprehensive\nbenchmarking study on these different choices, with two key experiments\nevaluated on a real-world object pose estimation task. First, we study the\nrendering quality, and find that a small number of high-quality images is\nsuperior to a large number of low-quality images. Second, we study the type of\nrandomisation, and find that both distractors and textures are important for\ngeneralisation to novel environments.",
          "link": "http://arxiv.org/abs/2011.07112",
          "publishedOn": "2021-05-24T05:08:43.117Z",
          "wordCount": 581,
          "title": "Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer. (arXiv:2011.07112v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10856",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yadav_O/0/1/0/all/0/1\">Ojasvi Yadav</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosal_K/0/1/0/all/0/1\">Koustav Ghosal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lutz_S/0/1/0/all/0/1\">Sebastian Lutz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smolic_A/0/1/0/all/0/1\">Aljosa Smolic</a>",
          "description": "We address the problem of exposure correction of dark, blurry and noisy\nimages captured in low-light conditions in the wild. Classical image-denoising\nfilters work well in the frequency space but are constrained by several factors\nsuch as the correct choice of thresholds, frequency estimates etc. On the other\nhand, traditional deep networks are trained end-to-end in the RGB space by\nformulating this task as an image-translation problem. However, that is done\nwithout any explicit constraints on the inherent noise of the dark images and\nthus produce noisy and blurry outputs. To this end we propose a DCT/FFT based\nmulti-scale loss function, which when combined with traditional losses, trains\na network to translate the important features for visually pleasing output. Our\nloss function is end-to-end differentiable, scale-agnostic, and generic; i.e.,\nit can be applied to both RAW and JPEG images in most existing frameworks\nwithout additional overhead. Using this loss function, we report significant\nimprovements over the state-of-the-art using quantitative metrics and\nsubjective tests.",
          "link": "http://arxiv.org/abs/2104.10856",
          "publishedOn": "2021-05-24T05:08:42.990Z",
          "wordCount": 632,
          "title": "Frequency Domain Loss Function for Deep Exposure Correction of Dark Images. (arXiv:2104.10856v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10490",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1\">Julio Silva-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sales_M/0/1/0/all/0/1\">Mar&#xed;a A. Sales</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Molina_R/0/1/0/all/0/1\">Rafael Molina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "The Gleason scoring system is the primary diagnostic and prognostic tool for\nprostate cancer. In recent years, with the development of digitisation devices,\nthe use of computer vision techniques for the analysis of biopsies has\nincreased. However, to the best of the authors' knowledge, the development of\nalgorithms to automatically detect individual cribriform patterns belonging to\nGleason grade 4 has not yet been studied in the literature. The objective of\nthe work presented in this paper is to develop a deep-learning-based system\nable to support pathologists in the daily analysis of prostate biopsies. The\nmethodological core of this work is a patch-wise predictive model based on\nconvolutional neural networks able to determine the presence of cancerous\npatterns. In particular, we train from scratch a simple self-design\narchitecture. The cribriform pattern is detected by retraining the set of\nfilters of the last convolutional layer in the network. From the reconstructed\nprediction map, we compute the percentage of each Gleason grade in the tissue\nto feed a multi-layer perceptron which provides a biopsy-level score.mIn our\nSICAPv2 database, composed of 182 annotated whole slide images, we obtained a\nCohen's quadratic kappa of 0.77 in the test set for the patch-level Gleason\ngrading with the proposed architecture trained from scratch. Our results\noutperform previous ones reported in the literature. Furthermore, this model\nreaches the level of fine-tuned state-of-the-art architectures in a\npatient-based four groups cross validation. In the cribriform pattern detection\ntask, we obtained an area under ROC curve of 0.82. Regarding the biopsy Gleason\nscoring, we achieved a quadratic Cohen's Kappa of 0.81 in the test subset.\nShallow CNN architectures trained from scratch outperform current\nstate-of-the-art methods for Gleason grades classification.",
          "link": "http://arxiv.org/abs/2105.10490",
          "publishedOn": "2021-05-24T05:08:42.984Z",
          "wordCount": 748,
          "title": "Going Deeper through the Gleason Scoring Scale: An Automatic end-to-end System for Histology Prostate Grading and Cribriform Pattern Detection. (arXiv:2105.10490v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Real_R/0/1/0/all/0/1\">Ric Real</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopsill_J/0/1/0/all/0/1\">James Gopsill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1\">David Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snider_C/0/1/0/all/0/1\">Chris Snider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hicks_B/0/1/0/all/0/1\">Ben Hicks</a>",
          "description": "Prior work has shown Convolutional Neural Networks (CNNs) trained on\nsurrogate Computer Aided Design (CAD) models are able to detect and classify\nreal-world artefacts from photographs. The applications of which support\ntwinning of digital and physical assets in design, including rapid extraction\nof part geometry from model repositories, information search \\& retrieval and\nidentifying components in the field for maintenance, repair, and recording. The\nperformance of CNNs in classification tasks have been shown dependent on\ntraining data set size and number of classes. Where prior works have used\nrelatively small surrogate model data sets ($<100$ models), the question\nremains as to the ability of a CNN to differentiate between models in\nincreasingly large model repositories. This paper presents a method for\ngenerating synthetic image data sets from online CAD model repositories, and\nfurther investigates the capacity of an off-the-shelf CNN architecture trained\non synthetic data to classify models as class size increases. 1,000 CAD models\nwere curated and processed to generate large scale surrogate data sets,\nfeaturing model coverage at steps of 10$^{\\circ}$, 30$^{\\circ}$, 60$^{\\circ}$,\nand 120$^{\\circ}$ degrees. The findings demonstrate the capability of computer\nvision algorithms to classify artefacts in model repositories of up to 200,\nbeyond this point the CNN's performance is observed to deteriorate\nsignificantly, limiting its present ability for automated twinning of physical\nto digital artefacts. Although, a match is more often found in the top-5\nresults showing potential for information search and retrieval on large\nrepositories of surrogate models.",
          "link": "http://arxiv.org/abs/2105.10448",
          "publishedOn": "2021-05-24T05:08:42.966Z",
          "wordCount": 704,
          "title": "Distinguishing artefacts: evaluating the saturation point of convolutional neural networks. (arXiv:2105.10448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10420",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1\">Julio Silva-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Prostate cancer is one of the main diseases affecting men worldwide. The gold\nstandard for diagnosis and prognosis is the Gleason grading system. In this\nprocess, pathologists manually analyze prostate histology slides under\nmicroscope, in a high time-consuming and subjective task. In the last years,\ncomputer-aided-diagnosis (CAD) systems have emerged as a promising tool that\ncould support pathologists in the daily clinical practice. Nevertheless, these\nsystems are usually trained using tedious and prone-to-error pixel-level\nannotations of Gleason grades in the tissue. To alleviate the need of manual\npixel-wise labeling, just a handful of works have been presented in the\nliterature. Motivated by this, we propose a novel weakly-supervised\ndeep-learning model, based on self-learning CNNs, that leverages only the\nglobal Gleason score of gigapixel whole slide images during training to\naccurately perform both, grading of patch-level patterns and biopsy-level\nscoring. To evaluate the performance of the proposed method, we perform\nextensive experiments on three different external datasets for the patch-level\nGleason grading, and on two different test sets for global Grade Group\nprediction. We empirically demonstrate that our approach outperforms its\nsupervised counterpart on patch-level Gleason grading by a large margin, as\nwell as state-of-the-art methods on global biopsy-level scoring. Particularly,\nthe proposed model brings an average improvement on the Cohen's quadratic kappa\n(k) score of nearly 18% compared to full-supervision for the patch-level\nGleason grading task.",
          "link": "http://arxiv.org/abs/2105.10420",
          "publishedOn": "2021-05-24T05:08:42.960Z",
          "wordCount": 674,
          "title": "Self-learning for weakly supervised Gleason grading of local patterns. (arXiv:2105.10420v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Greer_H/0/1/0/all/0/1\">Hastings Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1\">Roland Kwitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vialard_F/0/1/0/all/0/1\">Francois-Xavier Vialard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>",
          "description": "Learning maps between data samples is fundamental. Applications range from\nrepresentation learning, image translation and generative modeling, to the\nestimation of spatial deformations. Such maps relate feature vectors, or map\nbetween feature spaces. Well-behaved maps should be regular, which can be\nimposed explicitly or may emanate from the data itself. We explore what induces\nregularity for spatial transformations, e.g., when computing image\nregistrations. Classical optimization-based models compute maps between pairs\nof samples and rely on an appropriate regularizer for well-posedness. Recent\ndeep learning approaches have attempted to avoid using such regularizers\naltogether by relying on the sample population instead. We explore if it is\npossible to obtain spatial regularity using an inverse consistency loss only\nand elucidate what explains map regularity in such a context. We find that deep\nnetworks combined with an inverse consistency loss and randomized off-grid\ninterpolation yield well behaved, approximately diffeomorphic, spatial\ntransformations. Despite the simplicity of this approach, our experiments\npresent compelling evidence, on both synthetic and real data, that regular maps\ncan be obtained without carefully tuned explicit regularizers, while achieving\ncompetitive registration performance.",
          "link": "http://arxiv.org/abs/2105.04459",
          "publishedOn": "2021-05-24T05:08:42.954Z",
          "wordCount": 626,
          "title": "ICON: Learning Regular Maps Through Inverse Consistency. (arXiv:2105.04459v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burgos_C/0/1/0/all/0/1\">Carlos Mauricio Villegas Burgos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamivakas_N/0/1/0/all/0/1\">Nick Vamivakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhao Zhu</a>",
          "description": "Deep learning using Convolutional Neural Networks (CNNs) has been shown to\nsignificantly out-performed many conventional vision algorithms. Despite\nefforts to increase the CNN efficiency both algorithmically and with\nspecialized hardware, deep learning remains difficult to deploy in\nresource-constrained environments. In this paper, we propose an end-to-end\nframework to explore optically compute the CNNs in free-space, much like a\ncomputational camera. Compared to existing free-space optics-based approaches\nwhich are limited to processing single-channel (i.e., grayscale) inputs, we\npropose the first general approach, based on nanoscale meta-surface optics,\nthat can process RGB data directly from the natural scenes. Our system achieves\nup to an order of magnitude energy saving, simplifies the sensor design, all\nthe while sacrificing little network accuracy.",
          "link": "http://arxiv.org/abs/2011.11728",
          "publishedOn": "2021-05-24T05:08:42.948Z",
          "wordCount": 581,
          "title": "End-to-End Framework for Efficient Deep Learning Using Metasurfaces Optics. (arXiv:2011.11728v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khoshsirat_A/0/1/0/all/0/1\">Aria Khoshsirat</a>",
          "description": "Quantifying uncertainty in a model's predictions is important as it enables\nthe safety of an AI system to be increased by acting on the model's output in\nan informed manner. This is crucial for applications where the cost of an error\nis high, such as in autonomous vehicle control, medical image analysis,\nfinancial estimations or legal fields. Deep Neural Networks are powerful\npredictors that have recently achieved state-of-the-art performance on a wide\nspectrum of tasks. Quantifying predictive uncertainty in DNNs is a challenging\nand yet on-going problem. In this paper we propose a complete framework to\ncapture and quantify all of these three types of uncertainties in DNNs for\nimage classification. This framework includes an ensemble of CNNs for model\nuncertainty, a supervised reconstruction auto-encoder to capture distributional\nuncertainty and using the output of activation functions in the last layer of\nthe network, to capture data uncertainty. Finally we demonstrate the efficiency\nof our method on popular image datasets for classification.",
          "link": "http://arxiv.org/abs/2011.08712",
          "publishedOn": "2021-05-24T05:08:42.941Z",
          "wordCount": 651,
          "title": "Quantifying Uncertainty from Different Sources in Deep Neural Networks for Image Classification. (arXiv:2011.08712v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>",
          "description": "The problem of knowledge-based visual question answering involves answering\nquestions that require external knowledge in addition to the content of the\nimage. Such knowledge typically comes in a variety of forms, including visual,\ntextual, and commonsense knowledge. The use of more knowledge sources, however,\nalso increases the chance of retrieving more irrelevant or noisy facts, making\nit difficult to comprehend the facts and find the answer. To address this\nchallenge, we propose Multi-modal Answer Validation using External knowledge\n(MAVEx), where the idea is to validate a set of promising answer candidates\nbased on answer-specific knowledge retrieval. This is in contrast to existing\napproaches that search for the answer in a vast collection of often irrelevant\nfacts. Our approach aims to learn which knowledge source should be trusted for\neach answer candidate and how to validate the candidate using that source. We\nconsider a multi-modal setting, relying on both textual and visual knowledge\nresources, including images searched using Google, sentences from Wikipedia\narticles, and concepts from ConceptNet. Our experiments with OK-VQA, a\nchallenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2103.12248",
          "publishedOn": "2021-05-24T05:08:42.923Z",
          "wordCount": 642,
          "title": "Multi-Modal Answer Validation for Knowledge-Based VQA. (arXiv:2103.12248v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.08614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1\">Kuldeep Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajhans Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan Turaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aswin Sankaranarayanan</a>",
          "description": "Recently, there has been substantial progress in image synthesis from\nsemantic labelmaps. However, methods used for this task assume the availability\nof complete and unambiguous labelmaps, with instance boundaries of objects, and\nclass labels for each pixel. This reliance on heavily annotated inputs\nrestricts the application of image synthesis techniques to real-world\napplications, especially under uncertainty due to weather, occlusion, or noise.\nOn the other hand, algorithms that can synthesize images from sparse labelmaps\nor sketches are highly desirable as tools that can guide content creators and\nartists to quickly generate scenes by simply specifying locations of a few\nobjects. In this paper, we address the problem of complex scene completion from\nsparse labelmaps. Under this setting, very few details about the scene (30\\% of\nobject instances) are available as input for image synthesis. We propose a\ntwo-stage deep network based method, called `Halluci-Net', that learns\nco-occurence relationships between objects in scenes, and then exploits these\nrelationships to produce a dense and complete labelmap. The generated dense\nlabelmap can then be used as input by state-of-the-art image synthesis\ntechniques like pix2pixHD to obtain the final image. The proposed method is\nevaluated on the Cityscapes dataset and it outperforms two baselines methods on\nperformance metrics like Fr\\'echet Inception Distance (FID), semantic\nsegmentation accuracy, and similarity in object co-occurrences. We also show\nqualitative results on a subset of ADE20K dataset that contains bedroom images.",
          "link": "http://arxiv.org/abs/2004.08614",
          "publishedOn": "2021-05-24T05:08:42.917Z",
          "wordCount": 709,
          "title": "Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships. (arXiv:2004.08614v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_S/0/1/0/all/0/1\">Seiya Matsuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "Various fonts give us various impressions, which are often represented by\nwords. This paper proposes Impressions2Font (Imp2Font) that generates font\nimages with specific impressions. Imp2Font is an extended version of\nconditional generative adversarial networks (GANs). More precisely, Imp2Font\naccepts an arbitrary number of impression words as the condition to generate\nthe font images. These impression words are converted into a soft-constraint\nvector by an impression embedding module built on a word embedding technique.\nQualitative and quantitative evaluations prove that Imp2Font generates font\nimages with higher quality than comparative methods by providing multiple\nimpression words or even unlearned words.",
          "link": "http://arxiv.org/abs/2103.10036",
          "publishedOn": "2021-05-24T05:08:42.910Z",
          "wordCount": 553,
          "title": "Impressions2Font: Generating Fonts by Specifying Impressions. (arXiv:2103.10036v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>",
          "description": "Visual attention is one of the most significant characteristics for selecting\nand understanding the outside redundancy world. The nature of complex scenes\nincludes enormous redundancy. The human vision system can not process all\ninformation simultaneously because of visual information bottleneck. The human\nvisual system mainly focuses on dominant parts of the scenes to reduce the\ninput visual redundancy information. It is commonly known as visual attention\nprediction or visual saliency map. This paper proposes a new psychophysical\nsaliency prediction architecture, WECSF, inspired by human low-level visual\ncortex function. The model consists of opponent color channels, wavelet\ntransform, wavelet energy map, and contrast sensitivity function for extracting\nlow-level image features and maximum approximation to the human visual system.\nThe proposed model is evaluated several datasets, including MIT1003, MIT300,\nTORONTO, SID4VAM and UCF Sports dataset to explain its efficiency. We also\nquantitatively and qualitatively compared the performance of saliency\nprediction with other state-of-the-art models. Our model achieved very stable\nand good performance. Second, we also confirmed that Fourier and\nspectral-inspired saliency prediction models achieved outperformance compared\nto other start-of-the-art non-neural networks and even deep neural network\nmodels on psychophysical synthesis images. Finally, the proposed model also can\nbe applied to spatial-temporal saliency prediction and got better performance.",
          "link": "http://arxiv.org/abs/2011.04076",
          "publishedOn": "2021-05-24T05:08:42.904Z",
          "wordCount": 706,
          "title": "An Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v8 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tayyab_M/0/1/0/all/0/1\">Muhammad Tayyab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Ahmad Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahalanobis_A/0/1/0/all/0/1\">Abhijit Mahalanobis</a>",
          "description": "We propose an efficient and straightforward method for compressing deep\nconvolutional neural networks (CNNs) that uses basis filters to represent the\nconvolutional layers, and optimizes the performance of the compressed network\ndirectly in the basis space. Specifically, any spatial convolution layer of the\nCNN can be replaced by two successive convolution layers: the first is a set of\nthree-dimensional orthonormal basis filters, followed by a layer of\none-dimensional filters that represents the original spatial filters in the\nbasis space. We jointly fine-tune both the basis and the filter representation\nto directly mitigate any performance loss due to the truncation. Generality of\nthe proposed approach is demonstrated by applying it to several well known deep\nCNN architectures and data sets for image classification and object detection.\nWe also present the execution time and power usage at different compression\nlevels on the Xavier Jetson AGX processor.",
          "link": "http://arxiv.org/abs/2105.10436",
          "publishedOn": "2021-05-24T05:08:42.897Z",
          "wordCount": 582,
          "title": "Compressing Deep CNNs using Basis Representation and Spectral Fine-tuning. (arXiv:2105.10436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daneshjou_R/0/1/0/all/0/1\">Roxana Daneshjou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovarik_C/0/1/0/all/0/1\">Carrie Kovarik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Justin M Ko</a>",
          "description": "Artificial intelligence (AI) algorithms using deep learning have advanced the\nclassification of skin disease images; however these algorithms have been\nmostly applied \"in silico\" and not validated clinically. Most dermatology AI\nalgorithms perform binary classification tasks (e.g. malignancy versus benign\nlesions), but this task is not representative of dermatologists' diagnostic\nrange. The American Academy of Dermatology Task Force on Augmented Intelligence\npublished a position statement emphasizing the importance of clinical\nvalidation to create human-computer synergy, termed augmented intelligence\n(AuI). Liu et al's recent paper, \"A deep learning system for differential\ndiagnosis of skin diseases\" represents a significant advancement of AI in\ndermatology, bringing it closer to clinical impact. However, significant issues\nmust be addressed before this algorithm can be integrated into clinical\nworkflow. These issues include accurate and equitable model development,\ndefining and assessing appropriate clinical outcomes, and real-world\nintegration.",
          "link": "http://arxiv.org/abs/2105.10477",
          "publishedOn": "2021-05-24T05:08:42.891Z",
          "wordCount": 591,
          "title": "Towards Realization of Augmented Intelligence in Dermatology: Advances and Future Directions. (arXiv:2105.10477v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.02692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyeon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hyung Jin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>",
          "description": "We propose a self-supervised visual learning method by predicting the\nvariable playback speeds of a video. Without semantic labels, we learn the\nspatio-temporal visual representation of the video by leveraging the variations\nin the visual appearance according to different playback speeds under the\nassumption of temporal coherence. To learn the spatio-temporal visual\nvariations in the entire video, we have not only predicted a single playback\nspeed but also generated clips of various playback speeds and directions with\nrandomized starting points. Hence the visual representation can be successfully\nlearned from the meta information (playback speeds and directions) of the\nvideo. We also propose a new layer dependable temporal group normalization\nmethod that can be applied to 3D convolutional networks to improve the\nrepresentation learning performance where we divide the temporal features into\nseveral groups and normalize each one using the different corresponding\nparameters. We validate the effectiveness of our method by fine-tuning it to\nthe action recognition and video retrieval tasks on UCF-101 and HMDB-51.",
          "link": "http://arxiv.org/abs/2003.02692",
          "publishedOn": "2021-05-24T05:08:42.873Z",
          "wordCount": 638,
          "title": "Self-Supervised Visual Learning by Variable Playback Speeds Prediction of a Video. (arXiv:2003.02692v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1\">Fabio Poiesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1\">Davide Boscaini</a>",
          "description": "An effective 3D descriptor should be invariant to different geometric\ntransformations, such as scale and rotation, repeatable in the case of\nocclusions and clutter, and generalisable in different contexts when data is\ncaptured with different sensors. We present a simple but yet effective method\nto learn generalisable and distinctive 3D local descriptors that can be used to\nregister point clouds captured in different contexts with different sensors.\nPoint cloud patches are extracted, canonicalised with respect to their local\nreference frame, and encoded into scale and rotation-invariant compact\ndescriptors by a point permutation-invariant deep neural network. Our\ndescriptors can effectively generalise across sensor modalities from locally\nand randomly sampled points. We evaluate and compare our descriptors with\nalternative handcrafted and deep learning-based descriptors on several indoor\nand outdoor datasets reconstructed using both RGBD sensors and laser scanners.\nOur descriptors outperform most recent descriptors by a large margin in terms\nof generalisation, and become the state of the art also in benchmarks where\ntraining and testing are performed in the same scenarios.",
          "link": "http://arxiv.org/abs/2105.10382",
          "publishedOn": "2021-05-24T05:08:42.867Z",
          "wordCount": 604,
          "title": "Generalisable and distinctive 3D local deep descriptors for point cloud registration. (arXiv:2105.10382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shumeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaixin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "Deep learning has achieved promising segmentation performance on 3D left\natrium MR images. However, annotations for segmentation tasks are expensive,\ncostly and difficult to obtain. In this paper, we introduce a novel\nhierarchical consistency regularized mean teacher framework for 3D left atrium\nsegmentation. In each iteration, the student model is optimized by multi-scale\ndeep supervision and hierarchical consistency regularization, concurrently.\nExtensive experiments have shown that our method achieves competitive\nperformance as compared with full annotation, outperforming other\nstateof-the-art semi-supervised segmentation methods.",
          "link": "http://arxiv.org/abs/2105.10369",
          "publishedOn": "2021-05-24T05:08:42.861Z",
          "wordCount": 535,
          "title": "Hierarchical Consistency Regularized Mean Teacher for Semi-supervised 3D Left Atrium Segmentation. (arXiv:2105.10369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Debasmit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1\">Yash Bhalgat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>",
          "description": "In this work, we propose a data-driven scheme to initialize the parameters of\na deep neural network. This is in contrast to traditional approaches which\nrandomly initialize parameters by sampling from transformed standard\ndistributions. Such methods do not use the training data to produce a more\ninformed initialization. Our method uses a sequential layer-wise approach where\neach layer is initialized using its input activations. The initialization is\ncast as an optimization problem where we minimize a combination of encoding and\ndecoding losses of the input activations, which is further constrained by a\nuser-defined latent code. The optimization problem is then restructured into\nthe well-known Sylvester equation, which has fast and efficient gradient-free\nsolutions. Our data-driven method achieves a boost in performance compared to\nrandom initialization methods, both before start of training and after training\nis over. We show that our proposed method is especially effective in few-shot\nand fine-tuning settings. We conclude this paper with analyses on time\ncomplexity and the effect of different latent codes on the recognition\nperformance.",
          "link": "http://arxiv.org/abs/2105.10335",
          "publishedOn": "2021-05-24T05:08:42.855Z",
          "wordCount": 616,
          "title": "Data-driven Weight Initialization with Sylvester Solvers. (arXiv:2105.10335v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2010.14925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Rui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>",
          "description": "We present MedMNIST, a collection of 10 pre-processed medical open datasets.\nMedMNIST is standardized to perform classification tasks on lightweight 28x28\nimages, which requires no background knowledge. Covering the primary data\nmodalities in medical image analysis, it is diverse on data scale (from 100 to\n100,000) and tasks (binary/multi-class, ordinal regression and multi-label).\nMedMNIST could be used for educational purpose, rapid prototyping, multi-modal\nmachine learning or AutoML in medical image analysis. Moreover, MedMNIST\nClassification Decathlon is designed to benchmark AutoML algorithms on all 10\ndatasets; We have compared several baseline methods, including open-source or\ncommercial AutoML tools. The datasets, evaluation code and baseline methods for\nMedMNIST are publicly available at https://medmnist.github.io/.",
          "link": "http://arxiv.org/abs/2010.14925",
          "publishedOn": "2021-05-24T05:08:42.839Z",
          "wordCount": 612,
          "title": "MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. (arXiv:2010.14925v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1\">Patrik Joslin Kenfack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arapov_D/0/1/0/all/0/1\">Daniil Dmitrievich Arapov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1\">Rasheed Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1\">S.M. Ahsan Kazmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Adil Mehmood Khan</a>",
          "description": "Generative adversarial networks (GANs) are one of the greatest advances in AI\nin recent years. With their ability to directly learn the probability\ndistribution of data, and then sample synthetic realistic data. Many\napplications have emerged, using GANs to solve classical problems in machine\nlearning, such as data augmentation, class unbalance problems, and fair\nrepresentation learning. In this paper, we analyze and highlight fairness\nconcerns of GANs model. In this regard, we show empirically that GANs models\nmay inherently prefer certain groups during the training process and therefore\nthey're not able to homogeneously generate data from different groups during\nthe testing phase. Furthermore, we propose solutions to solve this issue by\nconditioning the GAN model towards samples' group or using ensemble method\n(boosting) to allow the GAN model to leverage distributed structure of data\nduring the training phase and generate groups at equal rate during the testing\nphase.",
          "link": "http://arxiv.org/abs/2103.00950",
          "publishedOn": "2021-05-24T05:08:42.833Z",
          "wordCount": 624,
          "title": "On the Fairness of Generative Adversarial Networks (GANs). (arXiv:2103.00950v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Celong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>",
          "description": "In this paper, we present an efficient and robust deep learning solution for\nnovel view synthesis of complex scenes. In our approach, a 3D scene is\nrepresented as a light field, i.e., a set of rays, each of which has a\ncorresponding color when reaching the image plane. For efficient novel view\nrendering, we adopt a 4D parameterization of the light field, where each ray is\ncharacterized by a 4D parameter. We then formulate the light field as a 4D\nfunction that maps 4D coordinates to corresponding color values. We train a\ndeep fully connected network to optimize this implicit function and memorize\nthe 3D scene. Then, the scene-specific model is used to synthesize novel views.\nDifferent from previous light field approaches which require dense view\nsampling to reliably render novel views, our method can render novel views by\nsampling rays and querying the color for each ray from the network directly,\nthus enabling high-quality light field rendering with a sparser set of training\nimages. Our method achieves state-of-the-art novel view synthesis results while\nmaintaining an interactive frame rate.",
          "link": "http://arxiv.org/abs/2105.07112",
          "publishedOn": "2021-05-24T05:08:42.820Z",
          "wordCount": 635,
          "title": "NeLF: Practical Novel View Synthesis with Neural Light Field. (arXiv:2105.07112v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oleksiienko_I/0/1/0/all/0/1\">Illia Oleksiienko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Real-time detection of objects in the 3D scene is one of the tasks an\nautonomous agent needs to perform for understanding its surroundings. While\nrecent Deep Learning-based solutions achieve satisfactory performance, their\nhigh computational cost renders their application in real-life settings in\nwhich computations need to be performed on embedded platforms intractable. In\nthis paper, we analyze the efficiency of two popular voxel-based 3D object\ndetection methods providing a good compromise between high performance and\nspeed based on two aspects, their ability to detect objects located at large\ndistances from the agent and their ability to operate in real time on embedded\nplatforms equipped with high-performance GPUs. Our experiments show that these\nmethods mostly fail to detect distant small objects due to the sparsity of the\ninput point clouds at large distances. Moreover, models trained on near objects\nachieve similar or better performance compared to those trained on all objects\nin the scene. This means that the models learn object appearance\nrepresentations mostly from near objects. Our findings suggest that a\nconsiderable part of the computations of existing methods is focused on\nlocations of the scene that do not contribute with successful detection. This\nmeans that the methods can achieve a speed-up of $40$-$60\\%$ by restricting\noperation to near objects while not sacrificing much in performance.",
          "link": "http://arxiv.org/abs/2105.10316",
          "publishedOn": "2021-05-24T05:08:42.764Z",
          "wordCount": 659,
          "title": "Analysis of voxel-based 3D object detection methods efficiency for real-time embedded systems. (arXiv:2105.10316v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salas_R/0/1/0/all/0/1\">Rosemberg Rodriguez Salas</a> (LIGM), <a href=\"http://arxiv.org/find/cs/1/au:+Dokladalova_E/0/1/0/all/0/1\">Eva Dokladalova</a> (LIGM), <a href=\"http://arxiv.org/find/cs/1/au:+Dokladal_P/0/1/0/all/0/1\">Petr Dokl&#xe1;dal</a> (CMM)",
          "description": "Deep convolutional neural networks accuracy is heavily impacted by rotations\nof the input data. In this paper, we propose a convolutional predictor that is\ninvariant to rotations in the input. This architecture is capable of predicting\nthe angular orientation without angle-annotated data. Furthermore, the\npredictor maps continuously the random rotation of the input to a circular\nspace of the prediction. For this purpose, we use the roto-translation\nproperties existing in the Scattering Transform Networks with a series of 3D\nConvolutions. We validate the results by training with upright and randomly\nrotated samples. This allows further applications of this work on fields like\nautomatic re-orientation of randomly oriented datasets.",
          "link": "http://arxiv.org/abs/2105.10175",
          "publishedOn": "2021-05-24T05:08:42.723Z",
          "wordCount": 565,
          "title": "Rotation invariant CNN using scattering transform for image classification. (arXiv:2105.10175v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10214",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nakanishi_M/0/1/0/all/0/1\">Masaki Nakanishi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sato_K/0/1/0/all/0/1\">Kazuki Sato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Terada_H/0/1/0/all/0/1\">Hideo Terada</a>",
          "description": "In image anomaly detection, Autoencoders are the popular methods that\nreconstruct the input image that might contain anomalies and output a clean\nimage with no abnormalities. These Autoencoder-based methods usually calculate\nthe anomaly score from the reconstruction error, the difference between the\ninput image and the reconstructed image. On the other hand, the accuracy of the\nreconstruction is insufficient in many of these methods, so it leads to\ndegraded accuracy of anomaly detection. To improve the accuracy of the\nreconstruction, we consider defining loss function in the frequency domain. In\ngeneral, we know that natural images contain many low-frequency components and\nfew high-frequency components. Hence, to improve the accuracy of the\nreconstruction of high-frequency components, we introduce a new loss function\nnamed weighted frequency domain loss(WFDL). WFDL provides a sharper\nreconstructed image, which contributes to improving the accuracy of anomaly\ndetection. In this paper, we show our method's superiority over the\nconventional Autoencoder methods by comparing it with AUROC on the MVTec AD\ndataset.",
          "link": "http://arxiv.org/abs/2105.10214",
          "publishedOn": "2021-05-24T05:08:42.711Z",
          "wordCount": 605,
          "title": "Anomaly Detection By Autoencoder Based On Weighted Frequency Domain Loss. (arXiv:2105.10214v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lumin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yingda Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>",
          "description": "Human pose estimation has achieved significant progress in recent years.\nHowever, most of the recent methods focus on improving accuracy using\ncomplicated models and ignoring real-time efficiency. To achieve a better\ntrade-off between accuracy and efficiency, we propose a novel neural\narchitecture search (NAS) method, termed ViPNAS, to search networks in both\nspatial and temporal levels for fast online video pose estimation. In the\nspatial level, we carefully design the search space with five different\ndimensions including network depth, width, kernel size, group number, and\nattentions. In the temporal level, we search from a series of temporal feature\nfusions to optimize the total accuracy and speed across multiple video frames.\nTo the best of our knowledge, we are the first to search for the temporal\nfeature fusion and automatic computation allocation in videos. Extensive\nexperiments demonstrate the effectiveness of our approach on the challenging\nCOCO2017 and PoseTrack2018 datasets. Our discovered model family, S-ViPNAS and\nT-ViPNAS, achieve significantly higher inference speed (CPU real-time) without\nsacrificing the accuracy compared to the previous state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.10154",
          "publishedOn": "2021-05-24T05:08:42.704Z",
          "wordCount": 617,
          "title": "ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search. (arXiv:2105.10154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1\">Kanchana Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "Vision transformers (ViT) have demonstrated impressive performance across\nvarious machine vision problems. These models are based on multi-head\nself-attention mechanisms that can flexibly attend to a sequence of image\npatches to encode contextual cues. An important question is how such\nflexibility in attending image-wide context conditioned on a given patch can\nfacilitate handling nuisances in natural images e.g., severe occlusions, domain\nshifts, spatial permutations, adversarial and natural perturbations. We\nsystematically study this question via an extensive set of experiments\nencompassing three ViT families and comparisons with a high-performing\nconvolutional neural network (CNN). We show and analyze the following\nintriguing properties of ViT: (a) Transformers are highly robust to severe\nocclusions, perturbations and domain shifts, e.g., retain as high as 60% top-1\naccuracy on ImageNet even after randomly occluding 80% of the image content.\n(b) The robust performance to occlusions is not due to a bias towards local\ntextures, and ViTs are significantly less biased towards textures compared to\nCNNs. When properly trained to encode shape-based features, ViTs demonstrate\nshape recognition capability comparable to that of human visual system,\npreviously unmatched in the literature. (c) Using ViTs to encode shape\nrepresentation leads to an interesting consequence of accurate semantic\nsegmentation without pixel-level supervision. (d) Off-the-shelf features from a\nsingle ViT model can be combined to create a feature ensemble, leading to high\naccuracy rates across a range of classification datasets in both traditional\nand few-shot learning paradigms. We show effective features of ViTs are due to\nflexible and dynamic receptive fields possible via the self-attention\nmechanism.",
          "link": "http://arxiv.org/abs/2105.10497",
          "publishedOn": "2021-05-24T05:08:42.674Z",
          "wordCount": 698,
          "title": "Intriguing Properties of Vision Transformers. (arXiv:2105.10497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinshuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Gang Wei</a>",
          "description": "Domain shift has always been one of the primary issues in video object\nsegmentation (VOS), for which models suffer from degeneration when tested on\nunfamiliar datasets. Recently, many online methods have emerged to narrow the\nperformance gap between training data (source domain) and test data (target\ndomain) by fine-tuning on annotations of test data which are usually in\nshortage. In this paper, we propose a novel method to tackle domain shift by\nfirst introducing adversarial domain adaptation to the VOS task, with\nsupervised training on the source domain and unsupervised training on the\ntarget domain. By fusing appearance and motion features with a convolution\nlayer, and by adding supervision onto the motion branch, our model achieves\nstate-of-the-art performance on DAVIS2016 with 82.6% mean IoU score after\nsupervised training. Meanwhile, our adversarial domain adaptation strategy\nsignificantly raises the performance of the trained model when applied on\nFBMS59 and Youtube-Object, without exploiting extra annotations.",
          "link": "http://arxiv.org/abs/2105.10201",
          "publishedOn": "2021-05-24T05:08:42.646Z",
          "wordCount": 584,
          "title": "DAVOS: Semi-Supervised Video Object Segmentation via Adversarial Domain Adaptation. (arXiv:2105.10201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10445",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1\">Julio Silva-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Prostate cancer is one of the main diseases affecting men worldwide. The\nGleason scoring system is the primary diagnostic tool for prostate cancer. This\nis obtained via the visual analysis of cancerous patterns in prostate biopsies\nperformed by expert pathologists, and the aggregation of the main Gleason\ngrades in a combined score. Computer-aided diagnosis systems allow to reduce\nthe workload of pathologists and increase the objectivity. Recently, efforts\nhave been made in the literature to develop algorithms aiming the direct\nestimation of the global Gleason score at biopsy/core level with global labels.\nHowever, these algorithms do not cover the accurate localization of the Gleason\npatterns into the tissue. In this work, we propose a deep-learning-based system\nable to detect local cancerous patterns in the prostate tissue using only the\nglobal-level Gleason score during training. The methodological core of this\nwork is the proposed weakly-supervised-trained convolutional neural network,\nWeGleNet, based on a multi-class segmentation layer after the feature\nextraction module, a global-aggregation, and the slicing of the background\nclass for the model loss estimation during training. We obtained a Cohen's\nquadratic kappa (k) of 0.67 for the pixel-level prediction of cancerous\npatterns in the validation cohort. We compared the model performance for\nsemantic segmentation of Gleason grades with supervised state-of-the-art\narchitectures in the test cohort. We obtained a pixel-level k of 0.61 and a\nmacro-averaged f1-score of 0.58, at the same level as fully-supervised methods.\nRegarding the estimation of the core-level Gleason score, we obtained a k of\n0.76 and 0.67 between the model and two different pathologists. WeGleNet is\ncapable of performing the semantic segmentation of Gleason grades similarly to\nfully-supervised methods without requiring pixel-level annotations.",
          "link": "http://arxiv.org/abs/2105.10445",
          "publishedOn": "2021-05-24T05:08:42.636Z",
          "wordCount": 738,
          "title": "WeGleNet: A Weakly-Supervised Convolutional Neural Network for the Semantic Segmentation of Gleason Grades in Prostate Histology Images. (arXiv:2105.10445v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhehua Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shoudong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yiting Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Alex Pui-Wai Lee</a>",
          "description": "This paper presents a novel algorithm that registers a collection of\nmono-modal 3D images in a simultaneous fashion, named as Direct Simultaneous\nRegistration (DSR). The algorithm optimizes global poses of local frames\ndirectly based on the intensities of images (without extracting features from\nthe images). To obtain the optimal result, we start with formulating a Direct\nBundle Adjustment (DBA) problem which jointly optimizes pose parameters of\nlocal frames and intensities of panoramic image. By proving the independence of\nthe pose from panoramic image in the iterative process, DSR is proposed and\nproved to be able to generate the same optimal poses as DBA, but without\noptimizing the intensities of the panoramic image. The proposed DSR method is\nparticularly suitable in mono-modal registration and in the scenarios where\ndistinct features are not available, such as Transesophageal Echocardiography\n(TEE) images. The proposed method is validated via simulated and in-vivo 3D TEE\nimages. It is shown that the proposed method outperforms conventional\nsequential registration method in terms of accuracy and the obtained results\ncan produce good alignment in in-vivo images.",
          "link": "http://arxiv.org/abs/2105.10087",
          "publishedOn": "2021-05-24T05:08:42.626Z",
          "wordCount": 618,
          "title": "Direct Simultaneous Multi-Image Registration. (arXiv:2105.10087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10239",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ambati_A/0/1/0/all/0/1\">Anirudh Ambati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>",
          "description": "Covid-19 global pandemic continues to devastate health care systems across\nthe world. In many countries, the 2nd wave is very severe. Economical and rapid\ntesting, as well as diagnosis, is urgently needed to control the pandemic. At\npresent, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR)\ntesting can be the fastest, scalable, and non-invasive method. The existing\nmethods suffer due to the limited CXR samples available from Covid-19. Thus,\ninspired by the limitations of the open-source work in this field, we propose\nattention guided contrastive CNN architecture (AC-CovidNet) for Covid-19\ndetection in CXR images. The proposed method learns the robust and\ndiscriminative features with the help of contrastive loss. Moreover, the\nproposed method gives more importance to the infected regions as guided by the\nattention mechanism. We compute the sensitivity of the proposed method over the\npublicly available Covid-19 dataset. It is observed that the proposed\nAC-CovidNet exhibits very promising performance as compared to the existing\nmethods even with limited training data. It can tackle the bottleneck of CXR\nCovid-19 datasets being faced by the researchers. The code used in this paper\nis released publicly at \\url{https://github.com/shivram1987/AC-CovidNet/}.",
          "link": "http://arxiv.org/abs/2105.10239",
          "publishedOn": "2021-05-24T05:08:42.619Z",
          "wordCount": 687,
          "title": "AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images. (arXiv:2105.10239v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yu-Cheng Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shouyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Ge Gao</a>",
          "description": "Owing to the difficulties of mining spatial-temporal cues, the existing\napproaches for video salient object detection (VSOD) are limited in\nunderstanding complex and noisy scenarios, and often fail in inferring\nprominent objects. To alleviate such shortcomings, we propose a simple yet\nefficient architecture, termed Guidance and Teaching Network (GTNet), to\nindependently distil effective spatial and temporal cues with implicit guidance\nand explicit teaching at feature- and decision-level, respectively. To be\nspecific, we (a) introduce a temporal modulator to implicitly bridge features\nfrom motion into the appearance branch, which is capable of fusing cross-modal\nfeatures collaboratively, and (b) utilise motion-guided mask to propagate the\nexplicit cues during the feature aggregation. This novel learning strategy\nachieves satisfactory results via decoupling the complex spatial-temporal cues\nand mapping informative cues across different modalities. Extensive experiments\non three challenging benchmarks show that the proposed method can run at ~28\nfps on a single TITAN Xp GPU and perform competitively against 14 cutting-edge\nbaselines.",
          "link": "http://arxiv.org/abs/2105.10110",
          "publishedOn": "2021-05-24T05:08:42.612Z",
          "wordCount": 601,
          "title": "Guidance and Teaching Network for Video Salient Object Detection. (arXiv:2105.10110v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10341",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bragilevsky_L/0/1/0/all/0/1\">Lior Bragilevsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1\">Ivan V. Baji&#x107;</a>",
          "description": "In the race to bring Artificial Intelligence (AI) to the edge, collaborative\nintelligence has emerged as a promising way to lighten the computation load on\nedge devices that run applications based on Deep Neural Networks (DNNs).\nTypically, a deep model is split at a certain layer into edge and cloud\nsub-models. The deep feature tensor produced by the edge sub-model is\ntransmitted to the cloud, where the remaining computationally intensive\nworkload is performed by the cloud sub-model. The communication channel between\nthe edge and cloud is imperfect, which will result in missing data in the deep\nfeature tensor received at the cloud side. In this study, we examine the\neffectiveness of four low-rank tensor completion methods in recovering missing\ndata in the deep feature tensor. We consider both sparse tensors, such as those\nproduced by the VGG16 model, as well as non-sparse tensors, such as those\nproduced by ResNet34 model. We study tensor completion effectiveness in both\nconplexity-constrained and unconstrained scenario.",
          "link": "http://arxiv.org/abs/2105.10341",
          "publishedOn": "2021-05-24T05:08:42.595Z",
          "wordCount": 620,
          "title": "Error Resilient Collaborative Intelligence via Low-Rank Tensor Completion. (arXiv:2105.10341v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kwan Ho Ryan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Haozhi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">John Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>",
          "description": "This work attempts to provide a plausible theoretical framework that aims to\ninterpret modern deep (convolutional) networks from the principles of data\ncompression and discriminative representation. We show that for\nhigh-dimensional multi-class data, the optimal linear discriminative\nrepresentation maximizes the coding rate difference between the whole dataset\nand the average of all the subsets. We show that the basic iterative gradient\nascent scheme for optimizing the rate reduction objective naturally leads to a\nmulti-layer deep network, named ReduNet, that shares common characteristics of\nmodern deep networks. The deep layered architectures, linear and nonlinear\noperators, and even parameters of the network are all explicitly constructed\nlayer-by-layer via forward propagation, instead of learned via back\npropagation. All components of so-obtained \"white-box\" network have precise\noptimization, statistical, and geometric interpretation. Moreover, all linear\noperators of the so-derived network naturally become multi-channel convolutions\nwhen we enforce classification to be rigorously shift-invariant. The derivation\nalso indicates that such a deep convolution network is significantly more\nefficient to construct and learn in the spectral domain. Our preliminary\nsimulations and experiments clearly verify the effectiveness of both the rate\nreduction objective and the associated ReduNet. All code and data are available\nat https://github.com/Ma-Lab-Berkeley.",
          "link": "http://arxiv.org/abs/2105.10446",
          "publishedOn": "2021-05-24T05:08:42.589Z",
          "wordCount": 672,
          "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10194",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_L/0/1/0/all/0/1\">Lianru Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yokoya_N/0/1/0/all/0/1\">Naoto Yokoya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heiden_U/0/1/0/all/0/1\">Uta Heiden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_B/0/1/0/all/0/1\">Bing Zhang</a>",
          "description": "Over the past decades, enormous efforts have been made to improve the\nperformance of linear or nonlinear mixing models for hyperspectral unmixing,\nyet their ability to simultaneously generalize various spectral variabilities\nand extract physically meaningful endmembers still remains limited due to the\npoor ability in data fitting and reconstruction and the sensitivity to various\nspectral variabilities. Inspired by the powerful learning ability of deep\nlearning, we attempt to develop a general deep learning approach for\nhyperspectral unmixing, by fully considering the properties of endmembers\nextracted from the hyperspectral imagery, called endmember-guided unmixing\nnetwork (EGU-Net). Beyond the alone autoencoder-like architecture, EGU-Net is a\ntwo-stream Siamese deep network, which learns an additional network from the\npure or nearly-pure endmembers to correct the weights of another unmixing\nnetwork by sharing network parameters and adding spectrally meaningful\nconstraints (e.g., non-negativity and sum-to-one) towards a more accurate and\ninterpretable unmixing solution. Furthermore, the resulting general framework\nis not only limited to pixel-wise spectral unmixing but also applicable to\nspatial information modeling with convolutional operators for spatial-spectral\nunmixing. Experimental results conducted on three different datasets with the\nground-truth of abundance maps corresponding to each material demonstrate the\neffectiveness and superiority of the EGU-Net over state-of-the-art unmixing\nalgorithms. The codes will be available from the website:\nhttps://github.com/danfenghong/IEEE_TNNLS_EGU-Net.",
          "link": "http://arxiv.org/abs/2105.10194",
          "publishedOn": "2021-05-24T05:08:42.582Z",
          "wordCount": 679,
          "title": "Endmember-Guided Unmixing Network (EGU-Net): A General Deep Learning Framework for Self-Supervised Hyperspectral Unmixing. (arXiv:2105.10194v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouraoui_Z/0/1/0/all/0/1\">Zied Bouraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jameel_S/0/1/0/all/0/1\">Shoaib Jameel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1\">Steven Schockaert</a>",
          "description": "Few-shot learning (FSL) is the task of learning to recognize previously\nunseen categories of images from a small number of training examples. This is a\nchallenging task, as the available examples may not be enough to unambiguously\ndetermine which visual features are most characteristic of the considered\ncategories. To alleviate this issue, we propose a method that additionally\ntakes into account the names of the image classes. While the use of class names\nhas already been explored in previous work, our approach differs in two key\naspects. First, while previous work has aimed to directly predict visual\nprototypes from word embeddings, we found that better results can be obtained\nby treating visual and text-based prototypes separately. Second, we propose a\nsimple strategy for learning class name embeddings using the BERT language\nmodel, which we found to substantially outperform the GloVe vectors that were\nused in previous work. We furthermore propose a strategy for dealing with the\nhigh dimensionality of these vectors, inspired by models for aligning\ncross-lingual word embeddings. We provide experiments on miniImageNet, CUB and\ntieredImageNet, showing that our approach consistently improves the\nstate-of-the-art in metric-based FSL.",
          "link": "http://arxiv.org/abs/2105.10195",
          "publishedOn": "2021-05-24T05:08:42.562Z",
          "wordCount": 626,
          "title": "Aligning Visual Prototypes with BERT Embeddings for Few-Shot Learning. (arXiv:2105.10195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akazawa_T/0/1/0/all/0/1\">Teruaki Akazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1\">Yuma Kinoshita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>",
          "description": "In this paper, we propose a novel multi-color balance adjustment for color\nconstancy. The proposed method, called \"n-color balancing,\" allows us not only\nto perfectly correct n target colors on the basis of corresponding ground truth\ncolors but also to correct colors other than the n colors. In contrast,\nalthough white-balancing can perfectly adjust white, colors other than white\nare not considered in the framework of white-balancing in general. In an\nexperiment, the proposed multi-color balancing is demonstrated to outperform\nboth conventional white and multi-color balance adjustments including\nBradford's model.",
          "link": "http://arxiv.org/abs/2105.10228",
          "publishedOn": "2021-05-24T05:08:42.552Z",
          "wordCount": 572,
          "title": "Multi-color balance for color constancy. (arXiv:2105.10228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1\">Dat Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhamifar_E/0/1/0/all/0/1\">Ehsan Elhamifar</a>",
          "description": "We develop a novel compositional generative model for zero- and few-shot\nlearning to recognize fine-grained classes with a few or no training samples.\nOur key observation is that generating holistic features for fine-grained\nclasses fails to capture small attribute differences between classes.\nTherefore, we propose a feature composition framework that learns to extract\nattribute features from training samples and combines them to construct\nfine-grained features for rare and unseen classes. Feature composition allows\nus to not only selectively compose features of every class from only relevant\ntraining samples, but also obtain diversity among composed features via\nchanging samples used for the composition. In addition, instead of building\nholistic features for classes, we use our attribute features to form dense\nrepresentations capable of capturing fine-grained attribute details of classes.\nWe propose a training scheme that uses a discriminative model to construct\nfeatures that are subsequently used to train the model itself. Therefore, we\ndirectly train the discriminative model on the composed features without\nlearning a separate generative model. We conduct experiments on four popular\ndatasets of DeepFashion, AWA2, CUB, and SUN, showing the effectiveness of our\nmethod.",
          "link": "http://arxiv.org/abs/2105.10438",
          "publishedOn": "2021-05-24T05:08:42.522Z",
          "wordCount": 604,
          "title": "Compositional Fine-Grained Low-Shot Learning. (arXiv:2105.10438v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ask_K/0/1/0/all/0/1\">Katrina Ask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Maheen Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1\">Pia Haubro Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>",
          "description": "Orthopedic disorders are a common cause for euthanasia among horses, which\noften could have been avoided with earlier detection. These conditions often\ncreate varying degrees of subtle but long-term pain. It is challenging to train\na visual pain recognition method with video data depicting such pain, since the\nresulting pain behavior also is subtle, sparsely appearing, and varying, making\nit challenging for even an expert human labeler to provide accurate\nground-truth for the data. We show that transferring features from a dataset of\nhorses with acute nociceptive pain (where labeling is less ambiguous) can aid\nthe learning to recognize more complex orthopedic pain. Moreover, we present a\nhuman expert baseline for the problem, as well as an extensive empirical study\nof various domain transfer methods and of what is detected by the pain\nrecognition method trained on acute pain in the orthopedic dataset. Finally,\nthis is accompanied with a discussion around the challenges posed by real-world\nanimal behavior datasets and how best practices can be established for similar\nfine-grained action recognition tasks. Our code is available at\nhttps://github.com/sofiabroome/painface-recognition.",
          "link": "http://arxiv.org/abs/2105.10313",
          "publishedOn": "2021-05-24T05:08:42.516Z",
          "wordCount": 629,
          "title": "Sharing Pain: Using Domain Transfer Between Pain Types for Recognition of Sparse Pain Expressions in Horses. (arXiv:2105.10313v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10238",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zapaishchykova_A/0/1/0/all/0/1\">Anna Zapaishchykova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreizin_D/0/1/0/all/0/1\">David Dreizin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoshuo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jie Ying Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roohi_S/0/1/0/all/0/1\">Shahrooz Faghih Roohi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>",
          "description": "Pelvic ring disruptions result from blunt injury mechanisms and are often\nfound in patients with multi-system trauma. To grade pelvic fracture severity\nin trauma victims based on whole-body CT, the Tile AO/OTA classification is\nfrequently used. Due to the high volume of whole-body trauma CTs generated in\nbusy trauma centers, an automated approach to Tile classification would provide\nsubstantial value, e.,g., to prioritize the reading queue of the attending\ntrauma radiologist. In such scenario, an automated method should perform\ngrading based on a transparent process and based on interpretable features to\nenable interaction with human readers and lower their workload by offering\ninsights from a first automated read of the scan. This paper introduces an\nautomated yet interpretable pelvic trauma decision support system to assist\nradiologists in fracture detection and Tile grade classification. The method\noperates similarly to human interpretation of CT scans and first detects\ndistinct pelvic fractures on CT with high specificity using a Faster-RCNN model\nthat are then interpreted using a structural causal model based on clinical\nbest practices to infer an initial Tile grade. The Bayesian causal model and\nfinally, the object detector are then queried for likely co-occurring fractures\nthat may have been rejected initially due to the highly specific operating\npoint of the detector, resulting in an updated list of detected fractures and\ncorresponding final Tile grade. Our method is transparent in that it provides\nfinding location and type using the object detector, as well as information on\nimportant counterfactuals that would invalidate the system's recommendation and\nachieves an AUC of 83.3%/85.1% for translational/rotational instability.\nDespite being designed for human-machine teaming, our approach does not\ncompromise on performance compared to previous black-box approaches.",
          "link": "http://arxiv.org/abs/2105.10238",
          "publishedOn": "2021-05-24T05:08:42.510Z",
          "wordCount": 736,
          "title": "An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma. (arXiv:2105.10238v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1\">Tomas Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shih-En Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1\">Yaser Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1\">Jason Saragih</a>",
          "description": "We present a learning-based method for building driving-signal aware\nfull-body avatars. Our model is a conditional variational autoencoder that can\nbe animated with incomplete driving signals, such as human pose and facial\nkeypoints, and produces a high-quality representation of human geometry and\nview-dependent appearance. The core intuition behind our method is that better\ndrivability and generalization can be achieved by disentangling the driving\nsignals and remaining generative factors, which are not available during\nanimation. To this end, we explicitly account for information deficiency in the\ndriving signal by introducing a latent space that exclusively captures the\nremaining information, thus enabling the imputation of the missing factors\nrequired during full-body animation, while remaining faithful to the driving\nsignal. We also propose a learnable localized compression for the driving\nsignal which promotes better generalization, and helps minimize the influence\nof global chance-correlations often found in real datasets. For a given driving\nsignal, the resulting variational model produces a compact space of uncertainty\nfor missing factors that allows for an imputation strategy best suited to a\nparticular application. We demonstrate the efficacy of our approach on the\nchallenging problem of full-body animation for virtual telepresence with\ndriving signals acquired from minimal sensors placed in the environment and\nmounted on a VR-headset.",
          "link": "http://arxiv.org/abs/2105.10441",
          "publishedOn": "2021-05-24T05:08:42.501Z",
          "wordCount": 648,
          "title": "Driving-Signal Aware Full-Body Avatars. (arXiv:2105.10441v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_G/0/1/0/all/0/1\">Guang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wennan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lin Xu</a>",
          "description": "Many efforts have been devoted to designing sampling, mining, and weighting\nstrategies in high-level deep metric learning (DML) loss objectives. However,\nlittle attention has been paid to low-level but essential data transformation.\nIn this paper, we develop a novel mechanism, the independent domain embedding\naugmentation learning ({IDEAL}) method. It can simultaneously learn multiple\nindependent embedding spaces for multiple domains generated by predefined data\ntransformations. Our IDEAL is orthogonal to existing DML techniques and can be\nseamlessly combined with prior DML approaches for enhanced performance.\nEmpirical results on visual retrieval tasks demonstrate the superiority of the\nproposed method. For example, the IDEAL improves the performance of MS loss by\na large margin, 84.5\\% $\\rightarrow$ 87.1\\% on Cars-196, and 65.8\\%\n$\\rightarrow$ 69.5\\% on CUB-200 at Recall$@1$. Our IDEAL with MS loss also\nachieves the new state-of-the-art performance on three image retrieval\nbenchmarks, \\ie, \\emph{Cars-196}, \\emph{CUB-200}, and \\emph{SOP}. It\noutperforms the most recent DML approaches, such as Circle loss and XBM,\nsignificantly. The source code and pre-trained models of our method will be\navailable at\\emph{\\url{https://github.com/emdata-ailab/IDEAL}}.",
          "link": "http://arxiv.org/abs/2105.10112",
          "publishedOn": "2021-05-24T05:08:42.495Z",
          "wordCount": 610,
          "title": "IDEAL: Independent Domain Embedding Augmentation Learning. (arXiv:2105.10112v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jefferson_B/0/1/0/all/0/1\">Brett Jefferson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joslyn_C/0/1/0/all/0/1\">Cliff Joslyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purvine_E/0/1/0/all/0/1\">Emilie Purvine</a>",
          "description": "As data grows in size and complexity, finding frameworks which aid in\ninterpretation and analysis has become critical. This is particularly true when\ndata comes from complex systems where extensive structure is available, but\nmust be drawn from peripheral sources. In this paper we argue that in such\nsituations, sheaves can provide a natural framework to analyze how well a\nstatistical model fits at the local level (that is, on subsets of related\ndatapoints) vs the global level (on all the data). The sheaf-based approach\nthat we propose is suitably general enough to be useful in a range of\napplications, from analyzing sensor networks to understanding the feature space\nof a deep learning model.",
          "link": "http://arxiv.org/abs/2105.10414",
          "publishedOn": "2021-05-24T05:08:42.478Z",
          "wordCount": 561,
          "title": "Sheaves as a Framework for Understanding and Interpreting Model Fit. (arXiv:2105.10414v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fandong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Mammogram mass detection is crucial for diagnosing and preventing the breast\ncancers in clinical practice. The complementary effect of multi-view mammogram\nimages provides valuable information about the breast anatomical prior\nstructure and is of great significance in digital mammography interpretation.\nHowever, unlike radiologists who can utilize the natural reasoning ability to\nidentify masses based on multiple mammographic views, how to endow the existing\nobject detection models with the capability of multi-view reasoning is vital\nfor decision-making in clinical diagnosis but remains the boundary to explore.\nIn this paper, we propose an Anatomy-aware Graph convolutional Network (AGN),\nwhich is tailored for mammogram mass detection and endows existing detection\nmethods with multi-view reasoning ability. The proposed AGN consists of three\nsteps. Firstly, we introduce a Bipartite Graph convolutional Network (BGN) to\nmodel the intrinsic geometric and semantic relations of ipsilateral views.\nSecondly, considering that the visual asymmetry of bilateral views is widely\nadopted in clinical practice to assist the diagnosis of breast lesions, we\npropose an Inception Graph convolutional Network (IGN) to model the structural\nsimilarities of bilateral views. Finally, based on the constructed graphs, the\nmulti-view information is propagated through nodes methodically, which equips\nthe features learned from the examined view with multi-view reasoning ability.\nExperiments on two standard benchmarks reveal that AGN significantly exceeds\nthe state-of-the-art performance. Visualization results show that AGN provides\ninterpretable visual cues for clinical diagnosis.",
          "link": "http://arxiv.org/abs/2105.10160",
          "publishedOn": "2021-05-24T05:08:42.472Z",
          "wordCount": 685,
          "title": "Act Like a Radiologist: Towards Reliable Multi-view Correspondence Reasoning for Mammogram Mass Detection. (arXiv:2105.10160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banik_S/0/1/0/all/0/1\">Soubarna Banik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gracia_A/0/1/0/all/0/1\">Alejandro Mendoza Gracia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>",
          "description": "3D human pose estimation is a difficult task, due to challenges such as\noccluded body parts and ambiguous poses. Graph convolutional networks encode\nthe structural information of the human skeleton in the form of an adjacency\nmatrix, which is beneficial for better pose prediction. We propose one such\ngraph convolutional network named PoseGraphNet for 3D human pose regression\nfrom 2D poses. Our network uses an adaptive adjacency matrix and kernels\nspecific to neighbor groups. We evaluate our model on the Human3.6M dataset\nwhich is a standard dataset for 3D pose estimation. Our model's performance is\nclose to the state-of-the-art, but with much fewer parameters. The model learns\ninteresting adjacency relations between joints that have no physical\nconnections, but are behaviorally similar.",
          "link": "http://arxiv.org/abs/2105.10379",
          "publishedOn": "2021-05-24T05:08:42.465Z",
          "wordCount": 563,
          "title": "3D Human Pose Regression using Graph Convolutional Network. (arXiv:2105.10379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1\">Jingyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiachen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haichuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yanyun Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Hidden features in neural network usually fail to learn informative\nrepresentation for 3D segmentation as supervisions are only given on output\nprediction, while this can be solved by omni-scale supervision on intermediate\nlayers. In this paper, we bring the first omni-scale supervision method to\npoint cloud segmentation via the proposed gradual Receptive Field Component\nReasoning (RFCR), where target Receptive Field Component Codes (RFCCs) are\ndesigned to record categories within receptive fields for hidden units in the\nencoder. Then, target RFCCs will supervise the decoder to gradually infer the\nRFCCs in a coarse-to-fine categories reasoning manner, and finally obtain the\nsemantic labels. Because many hidden features are inactive with tiny magnitude\nand make minor contributions to RFCC prediction, we propose a Feature\nDensification with a centrifugal potential to obtain more unambiguous features,\nand it is in effect equivalent to entropy regularization over features. More\nactive features can further unleash the potential of our omni-supervision\nmethod. We embed our method into four prevailing backbones and test on three\nchallenging benchmarks. Our method can significantly improve the backbones in\nall three datasets. Specifically, our method brings new state-of-the-art\nperformances for S3DIS as well as Semantic3D and ranks the 1st in the ScanNet\nbenchmark among all the point-based methods. Code will be publicly available at\nhttps://github.com/azuki-miho/RFCR.",
          "link": "http://arxiv.org/abs/2105.10203",
          "publishedOn": "2021-05-24T05:08:42.459Z",
          "wordCount": 658,
          "title": "Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning. (arXiv:2105.10203v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1\">Ricard Durall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1\">Stanislav Frolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>",
          "description": "Transformer models have recently attracted much interest from computer vision\nresearchers and have since been successfully employed for several problems\ntraditionally addressed with convolutional neural networks. At the same time,\nimage synthesis using generative adversarial networks (GANs) has drastically\nimproved over the last few years. The recently proposed TransGAN is the first\nGAN using only transformer-based architectures and achieves competitive results\nwhen compared to convolutional GANs. However, since transformers are\ndata-hungry architectures, TransGAN requires data augmentation, an auxiliary\nsuper-resolution task during training, and a masking prior to guide the\nself-attention mechanism. In this paper, we study the combination of a\ntransformer-based generator and convolutional discriminator and successfully\nremove the need of the aforementioned required design choices. We evaluate our\napproach by conducting a benchmark of well-known CNN discriminators, ablate the\nsize of the transformer-based generator, and show that combining both\narchitectural elements into a hybrid model leads to better results.\nFurthermore, we investigate the frequency spectrum properties of generated\nimages and observe that our model retains the benefits of an attention based\ngenerator.",
          "link": "http://arxiv.org/abs/2105.10189",
          "publishedOn": "2021-05-24T05:08:42.452Z",
          "wordCount": 599,
          "title": "Combining Transformer Generators with Convolutional Discriminators. (arXiv:2105.10189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Collier_M/0/1/0/all/0/1\">Mark Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokiopoulou_E/0/1/0/all/0/1\">Efi Kokiopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1\">Rodolphe Jenatton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berent_J/0/1/0/all/0/1\">Jesse Berent</a>",
          "description": "Large scale image classification datasets often contain noisy labels. We take\na principled probabilistic approach to modelling input-dependent, also known as\nheteroscedastic, label noise in these datasets. We place a multivariate Normal\ndistributed latent variable on the final hidden layer of a neural network\nclassifier. The covariance matrix of this latent variable, models the aleatoric\nuncertainty due to label noise. We demonstrate that the learned covariance\nstructure captures known sources of label noise between semantically similar\nand co-occurring classes. Compared to standard neural network training and\nother baselines, we show significantly improved accuracy on Imagenet ILSVRC\n2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a\nnew state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These\ndatasets range from over 1M to over 300M training examples and from 1k classes\nto more than 21k classes. Our method is simple to use, and we provide an\nimplementation that is a drop-in replacement for the final fully-connected\nlayer in a deep classifier.",
          "link": "http://arxiv.org/abs/2105.10305",
          "publishedOn": "2021-05-24T05:08:42.430Z",
          "wordCount": 609,
          "title": "Correlated Input-Dependent Label Noise in Large-Scale Image Classification. (arXiv:2105.10305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10233",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Juvonen_M/0/1/0/all/0/1\">Markus Juvonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siltanen_S/0/1/0/all/0/1\">Samuli Siltanen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moura_F/0/1/0/all/0/1\">Fernando Silva de Moura</a>",
          "description": "The photographic dataset collected for the Helsinki Deblur Challenge 2021\n(HDC2021) contains pairs of images taken by two identical cameras of the same\ntarget but with different conditions. One camera is always in focus and\nproduces sharp and low-noise images the other camera produces blurred and noisy\nimages as it is gradually more and more out of focus and has a higher ISO\nsetting. Even though the dataset was designed and captured with the HDC2021 in\nmind it can be used for any testing and benchmarking of image deblurring\nalgorithms. The data is available here: https://doi.org/10.5281/zenodo.477228",
          "link": "http://arxiv.org/abs/2105.10233",
          "publishedOn": "2021-05-24T05:08:42.415Z",
          "wordCount": 536,
          "title": "Helsinki Deblur Challenge 2021: description of photographic data. (arXiv:2105.10233v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "Face recognition has achieved significant progress in deep-learning era due\nto the ultra-large-scale and well-labeled datasets. However, training on\nultra-large-scale datasets is time-consuming and takes up a lot of hardware\nresource. Therefore, how to design an appropriate training approach is very\ncrucial and indispensable. The computational and hardware cost of training\nultra-large-scale datasets mainly focuses on the Fully-Connected (FC) layer\nrather than convolutional layers. To this end, we propose a novel training\napproach for ultra-large-scale face datasets, termed Faster Face Classification\n(F$^2$C). In F$^2$C, we first define a Gallery Net and a Probe Net that are\nused to generate identities' centers and extract faces' features for face\nrecognition, respectively. Gallery Net has the same structure as Probe Net and\ninherits the parameters from Probe Net with a moving average paradigm. After\nthat, to reduce the training time and hardware resource occupancy of the FC\nlayer, we propose the Dynamic Class Pool that stores the features from Gallery\nNet and calculates the inner product (logits) with positive samples (its\nidentities appear in Dynamic Class Pool) in each mini-batch. Dynamic Class Pool\ncan be regarded as a substitute for the FC layer and its size is much smaller\nthan FC, which is the reason why Dynamic Class Pool can largely reduce the time\nand resource cost. For negative samples (its identities are not appear in the\nDynamic Class Pool), we minimize the cosine similarities between negative\nsamples and Dynamic Class Pool. Then, to improve the update efficiency and\nspeed of Dynamic Class Pool's parameters, we design the Dual Loaders including\nIdentity-based and Instance-based Loaders. Dual Loaders load images from given\ndataset by instances and identities to generate batches for training.",
          "link": "http://arxiv.org/abs/2105.10375",
          "publishedOn": "2021-05-24T05:08:42.409Z",
          "wordCount": 731,
          "title": "An Efficient Training Approach for Very Large Scale Face Recognition. (arXiv:2105.10375v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ayazoglu_M/0/1/0/all/0/1\">Mustafa Ayazoglu</a>",
          "description": "Single-Image Super Resolution (SISR) is a classical computer vision problem\nand it has been studied for over decades. With the recent success of deep\nlearning methods, recent work on SISR focuses solutions with deep learning\nmethodologies and achieves state-of-the-art results. However most of the\nstate-of-the-art SISR methods contain millions of parameters and layers, which\nlimits their practical applications. In this paper, we propose a hardware\n(Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization\nrobust real-time super resolution network (XLSR). The proposed model's building\nblock is inspired from root modules for Image classification. We successfully\napplied root modules to SISR problem, further more to make the model uint8\nquantization robust we used Clipped ReLU at the last layer of the network and\nachieved great balance between reconstruction quality and runtime. Furthermore,\nalthough the proposed network contains 30x fewer parameters than VDSR its\nperformance surpasses it on Div2K validation set. The network proved itself by\nwinning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.",
          "link": "http://arxiv.org/abs/2105.10288",
          "publishedOn": "2021-05-24T05:08:42.383Z",
          "wordCount": 619,
          "title": "Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices. (arXiv:2105.10288v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutillon_A/0/1/0/all/0/1\">Arnaud Boutillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conze_P/0/1/0/all/0/1\">Pierre-Henri Conze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_C/0/1/0/all/0/1\">Christelle Pons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burdin_V/0/1/0/all/0/1\">Val&#xe9;rie Burdin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borotikar_B/0/1/0/all/0/1\">Bhushan Borotikar</a>",
          "description": "Automatic segmentation of magnetic resonance (MR) images is crucial for\nmorphological evaluation of the pediatric musculoskeletal system in clinical\npractice. However, the accuracy and generalization performance of individual\nsegmentation models are limited due to the restricted amount of annotated\npediatric data. Hence, we propose to train a segmentation model on multiple\ndatasets, arising from different parts of the anatomy, in a multi-task and\nmulti-domain learning framework. This approach allows to overcome the inherent\nscarcity of pediatric data while benefiting from a more robust shared\nrepresentation. The proposed segmentation network comprises shared\nconvolutional filters, domain-specific batch normalization parameters that\ncompute the respective dataset statistics and a domain-specific segmentation\nlayer. Furthermore, a supervised contrastive regularization is integrated to\nfurther improve generalization capabilities, by promoting intra-domain\nsimilarity and impose inter-domain margins in embedded space. We evaluate our\ncontributions on two pediatric imaging datasets of the ankle and shoulder\njoints for bone segmentation. Results demonstrate that the proposed model\noutperforms state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2105.10310",
          "publishedOn": "2021-05-24T05:08:42.370Z",
          "wordCount": 624,
          "title": "Multi-Task, Multi-Domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets. (arXiv:2105.10310v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhitong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>",
          "description": "Recently, text detection has attracted sufficient attention in the field of\ncomputer vision and artificial intelligence. Among the existing approaches,\nregression-based models are limited to handle the texts with arbitrary shapes,\nwhile segmentation-based algorithms have high computational costs and suffer\nfrom the text adhesion problem. In this paper, we propose a new one-stage text\ndetector, termed as Bold Outline Text Detector (BOTD), which is able to process\nthe arbitrary-shaped text with low model complexity. Different from previous\nworks, BOTD utilizes the Polar Minimum Distance (PMD) to encode the shortest\ndistance between the center point and the contour of the text instance, and\ngenerates a Center Mask (CM) for each text instance. After learning the PMD\nheat map and CM map, the final results can be obtained with a simple Text\nReconstruction Module (TRM). Since the CM resides within the text box exactly,\nthe text adhesion problem is avoided naturally. Meanwhile, all the points on\nthe text contour share the same PMD, so the complexity of BOTD is much lower\nthan existing segmentation-based methods. Experimental results on three\nreal-world benchmarks show the state-of-the-art performance of BOTD.",
          "link": "http://arxiv.org/abs/2011.14714",
          "publishedOn": "2021-05-24T05:08:42.278Z",
          "wordCount": 675,
          "title": "BOTD: Bold Outline Text Detector. (arXiv:2011.14714v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuqian Fu</a>",
          "description": "Junctions reflect the important geometrical structure information of the\nimage, and are of primary significance to applications such as image matching\nand motion analysis. Previous event-based feature extraction methods are mainly\nfocused on corners, which mainly find their locations, however, ignoring the\ngeometrical structure information like orientations and scales of edges. This\npaper adapts the frame-based a-contrario junction detector(ACJ) to event data,\nproposing the event-based a-contrario junction detector(e-ACJ), which yields\njunctions' locations while giving the scales and orientations of their\nbranches. The proposed method relies on an a-contrario model and can operate on\nasynchronous events directly without generating synthesized event frames. We\nevaluate the performance on public event datasets. The result shows our method\nsuccessfully finds the orientations and scales of branches, while maintaining\nhigh accuracy in junction's location.",
          "link": "http://arxiv.org/abs/2101.11251",
          "publishedOn": "2021-05-24T05:08:42.266Z",
          "wordCount": 583,
          "title": "e-ACJ: Accurate Junction Extraction For Event Cameras. (arXiv:2101.11251v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_K/0/1/0/all/0/1\">Keivan Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plesh_R/0/1/0/all/0/1\">Richard Plesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_P/0/1/0/all/0/1\">Peter Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuckers_S/0/1/0/all/0/1\">Stephanie Schuckers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swyka_T/0/1/0/all/0/1\">Timothy Swyka</a>",
          "description": "In this work, we utilize progressive growth-based Generative Adversarial\nNetworks (GANs) to develop the Clarkson Fingerprint Generator (CFG). We\ndemonstrate that the CFG is capable of generating realistic, high fidelity,\n$512\\times512$ pixels, full, plain impression fingerprints. Our results suggest\nthat the fingerprints generated by the CFG are unique, diverse, and resemble\nthe training dataset in terms of minutiae configuration and quality, while not\nrevealing the underlying identities of the training data. We make the\npre-trained CFG model and the synthetically generated dataset publicly\navailable at https://github.com/keivanB/Clarkson_Finger_Gen",
          "link": "http://arxiv.org/abs/2105.10403",
          "publishedOn": "2021-05-24T05:08:42.259Z",
          "wordCount": 526,
          "title": "High Fidelity Fingerprint Generation: Quality, Uniqueness, and Privacy. (arXiv:2105.10403v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingliang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "As remote sensing (RS) data obtained from different sensors become available\nlargely and openly, multimodal data processing and analysis techniques have\nbeen garnering increasing interest in the RS and geoscience community. However,\ndue to the gap between different modalities in terms of imaging sensors,\nresolutions, and contents, embedding their complementary information into a\nconsistent, compact, accurate, and discriminative representation, to a great\nextent, remains challenging. To this end, we propose a shared and specific\nfeature learning (S2FL) model. S2FL is capable of decomposing multimodal RS\ndata into modality-shared and modality-specific components, enabling the\ninformation blending of multi-modalities more effectively, particularly for\nheterogeneous data sources. Moreover, to better assess multimodal baselines and\nthe newly-proposed S2FL model, three multimodal RS benchmark datasets, i.e.,\nHouston2013 -- hyperspectral and multispectral data, Berlin -- hyperspectral\nand synthetic aperture radar (SAR) data, Augsburg -- hyperspectral, SAR, and\ndigital surface model (DSM) data, are released and used for land cover\nclassification. Extensive experiments conducted on the three datasets\ndemonstrate the superiority and advancement of our S2FL model in the task of\nland cover classification in comparison with previously-proposed\nstate-of-the-art baselines. Furthermore, the baseline codes and datasets used\nin this paper will be made available freely at\nhttps://github.com/danfenghong/ISPRS_S2FL.",
          "link": "http://arxiv.org/abs/2105.10196",
          "publishedOn": "2021-05-24T05:08:42.253Z",
          "wordCount": 662,
          "title": "Multimodal Remote Sensing Benchmark Datasets for Land Cover Classification with A Shared and Specific Feature Learning Model. (arXiv:2105.10196v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12056",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Woinoski_T/0/1/0/all/0/1\">Timothy Woinoski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1\">Ivan V. Baji&#x107;</a>",
          "description": "In this work, we propose a swimming analytics system for automatically\ndetermining swimmer stroke rates from overhead race video (ORV). General ORV is\ndefined as any footage of swimmers in competition, taken for the purposes of\nviewing or analysis. Examples of this are footage from live streams,\nbroadcasts, or specialized camera equipment, with or without camera motion.\nThese are the most typical forms of swimming competition footage. We detail how\nto create a system that will automatically collect swimmer stroke rates in any\ncompetition, given the video of the competition of interest. With this\ninformation, better systems can be created and additions to our analytics\nsystem can be proposed to automatically extract other swimming metrics of\ninterest.",
          "link": "http://arxiv.org/abs/2104.12056",
          "publishedOn": "2021-05-24T05:08:42.130Z",
          "wordCount": 590,
          "title": "Swimmer Stroke Rate Estimation From Overhead Race Video. (arXiv:2104.12056v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1\">Qiyuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "In this paper, we propose the pyramid fusion dark channel prior (PF-DCP) for\nsingle image dehazing. Based on the well-known Dark Channel Prior (DCP), we\nintroduce an easy yet effective approach PF-DCP by employing the DCP algorithm\nat a pyramid of multi-scale images to alleviate the problem of patch size\nselection. In this case, we obtain the final transmission map by fusing\ntransmission maps at each level to recover a high-quality haze-free image.\nExperiments on RESIDE SOTS show that PF-DCP not only outperforms the\ntraditional prior-based methods with a large margin but also achieves\ncomparable or even better results of state-of-art deep learning approaches.\nFurthermore, the visual quality is also greatly improved with much fewer color\ndistortions and halo artifacts.",
          "link": "http://arxiv.org/abs/2105.10192",
          "publishedOn": "2021-05-24T05:08:42.112Z",
          "wordCount": 549,
          "title": "Pyramid Fusion Dark Channel Prior for Single Image Dehazing. (arXiv:2105.10192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Boyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hujun Yin</a>",
          "description": "Graph convolutional networks (GCNs) have achieved great success in dealing\nwith data of non-Euclidean structures. Their success directly attributes to\nfitting graph structures effectively to data such as in social media and\nknowledge databases. For image processing applications, the use of graph\nstructures and GCNs have not been fully explored. In this paper, we propose a\nnovel encoder-decoder network with added graph convolutions by converting\nfeature maps to vertexes of a pre-generated graph to synthetically construct\ngraph-structured data. By doing this, we inexplicitly apply graph Laplacian\nregularization to the feature maps, making them more structured. The\nexperiments show that it significantly boosts performance for image restoration\ntasks, including deblurring and super-resolution. We believe it opens up\nopportunities for GCN-based approaches in more applications.",
          "link": "http://arxiv.org/abs/2105.10465",
          "publishedOn": "2021-05-24T05:08:42.089Z",
          "wordCount": 571,
          "title": "Graph Convolutional Networks in Feature Space for Image Deblurring and Super-resolution. (arXiv:2105.10465v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diallo_A/0/1/0/all/0/1\">A&#xef;ssatou Diallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>",
          "description": "Ordinal embedding aims at finding a low dimensional representation of objects\nfrom a set of constraints of the form \"item $j$ is closer to item $i$ than item\n$k$\". Typically, each object is mapped onto a point vector in a low dimensional\nmetric space. We argue that mapping to a density instead of a point vector\nprovides some interesting advantages, including an inherent reflection of the\nuncertainty about the representation itself and its relative location in the\nspace. Indeed, in this paper, we propose to embed each object as a Gaussian\ndistribution. We investigate the ability of these embeddings to capture the\nunderlying structure of the data while satisfying the constraints, and explore\nproperties of the representation. Experiments on synthetic and real-world\ndatasets showcase the advantages of our approach. In addition, we illustrate\nthe merit of modelling uncertainty, which enriches the visual perception of the\nmapped objects in the space.",
          "link": "http://arxiv.org/abs/2105.10457",
          "publishedOn": "2021-05-24T05:08:42.060Z",
          "wordCount": 575,
          "title": "Elliptical Ordinal Embedding. (arXiv:2105.10457v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aniruddha Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>",
          "description": "Large-scale unlabeled data has allowed recent progress in self-supervised\nlearning methods that learn rich visual representations. State-of-the-art\nself-supervised methods for learning representations from images (MoCo and\nBYOL) use an inductive bias that different augmentations (e.g. random crops) of\nan image should produce similar embeddings. We show that such methods are\nvulnerable to backdoor attacks where an attacker poisons a part of the\nunlabeled data by adding a small trigger (known to the attacker) to the images.\nThe model performance is good on clean test images but the attacker can\nmanipulate the decision of the model by showing the trigger at test time.\nBackdoor attacks have been studied extensively in supervised learning and to\nthe best of our knowledge, we are the first to study them for self-supervised\nlearning. Backdoor attacks are more practical in self-supervised learning since\nthe unlabeled data is large and as a result, an inspection of the data to avoid\nthe presence of poisoned data is prohibitive. We show that in our targeted\nattack, the attacker can produce many false positives for the target category\nby using the trigger at test time. We also propose a knowledge distillation\nbased defense algorithm that succeeds in neutralizing the attack. Our code is\navailable here: https://github.com/UMBCvision/SSL-Backdoor .",
          "link": "http://arxiv.org/abs/2105.10123",
          "publishedOn": "2021-05-24T05:08:42.024Z",
          "wordCount": 633,
          "title": "Backdoor Attacks on Self-Supervised Learning. (arXiv:2105.10123v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1\">Sara Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenning Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_K/0/1/0/all/0/1\">Kelley Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steadman_D/0/1/0/all/0/1\">Dawnie Steadman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mockus_A/0/1/0/all/0/1\">Audris Mockus</a>",
          "description": "Annotating images for semantic segmentation requires intense manual labor and\nis a time-consuming and expensive task especially for domains with a scarcity\nof experts, such as Forensic Anthropology. We leverage the evolving nature of\nimages depicting the decay process in human decomposition data to design a\nsimple yet effective pseudo-pixel-level label generation technique to reduce\nthe amount of effort for manual annotation of such images. We first identify\nsequences of images with a minimum variation that are most suitable to share\nthe same or similar annotation using an unsupervised approach. Given one\nuser-annotated image in each sequence, we propagate the annotation to the\nremaining images in the sequence by merging it with annotations produced by a\nstate-of-the-art CAM-based pseudo label generation technique. To evaluate the\nquality of our pseudo-pixel-level labels, we train two semantic segmentation\nmodels with VGG and ResNet backbones on images labeled using our pseudo\nlabeling method and those of a state-of-the-art method. The results indicate\nthat using our pseudo-labels instead of those generated using the\nstate-of-the-art method in the training process improves the mean-IoU and the\nfrequency-weighted-IoU of the VGG and ResNet-based semantic segmentation models\nby 3.36%, 2.58%, 10.39%, and 12.91% respectively.",
          "link": "http://arxiv.org/abs/2105.09975",
          "publishedOn": "2021-05-24T05:08:42.013Z",
          "wordCount": 630,
          "title": "Pseudo Pixel-level Labeling for Images with Evolving Content. (arXiv:2105.09975v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weligampola_H/0/1/0/all/0/1\">Harshana Weligampola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayatilaka_G/0/1/0/all/0/1\">Gihan Jayatilaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sritharan_S/0/1/0/all/0/1\">Suren Sritharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_P/0/1/0/all/0/1\">Parakrama Ekanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragel_R/0/1/0/all/0/1\">Roshan Ragel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herath_V/0/1/0/all/0/1\">Vijitha Herath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godaliyadda_R/0/1/0/all/0/1\">Roshan Godaliyadda</a>",
          "description": "Intrinsic Image Decomposition is an open problem of generating the\nconstituents of an image. Generating reflectance and shading from a single\nimage is a challenging task specifically when there is no ground truth. There\nis a lack of unsupervised learning approaches for decomposing an image into\nreflectance and shading using a single image. We propose a neural network\narchitecture capable of this decomposition using physics-based parameters\nderived from the image. Through experimental results, we show that (a) the\nproposed methodology outperforms the existing deep learning-based IID\ntechniques and (b) the derived parameters improve the efficacy significantly.\nWe conclude with a closer analysis of the results (numerical and example\nimages) showing several avenues for improvement.",
          "link": "http://arxiv.org/abs/2105.10076",
          "publishedOn": "2021-05-24T05:08:42.000Z",
          "wordCount": 563,
          "title": "An Optical physics inspired CNN approach for intrinsic image decomposition. (arXiv:2105.10076v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1\">Prahal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1\">Masoumeh Aminzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training.",
          "link": "http://arxiv.org/abs/2105.09996",
          "publishedOn": "2021-05-24T05:08:41.986Z",
          "wordCount": 565,
          "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09993",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_K/0/1/0/all/0/1\">Kwan-Yee K. Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>",
          "description": "This paper addresses the problem of reconstructing the surface shape of\ntransparent objects. The difficulty of this problem originates from the\nviewpoint dependent appearance of a transparent object, which quickly makes\nreconstruction methods tailored for diffuse surfaces fail disgracefully. In\nthis paper, we introduce a fixed viewpoint approach to dense surface\nreconstruction of transparent objects based on refraction of light. We present\na simple setup that allows us to alter the incident light paths before light\nrays enter the object by immersing the object partially in a liquid, and\ndevelop a method for recovering the object surface through reconstructing and\ntriangulating such incident light paths. Our proposed approach does not need to\nmodel the complex interactions of light as it travels through the object,\nneither does it assume any parametric form for the object shape nor the exact\nnumber of refractions and reflections taken place along the light paths. It can\ntherefore handle transparent objects with a relatively complex shape and\nstructure, with unknown and inhomogeneous refractive index. We also show that\nfor thin transparent objects, our proposed acquisition setup can be further\nsimplified by adopting a single refraction approximation. Experimental results\non both synthetic and real data demonstrate the feasibility and accuracy of our\nproposed approach.",
          "link": "http://arxiv.org/abs/2105.09993",
          "publishedOn": "2021-05-24T05:08:41.887Z",
          "wordCount": 662,
          "title": "Dense Reconstruction of Transparent Objects by Altering Incident Light Paths Through Refraction. (arXiv:2105.09993v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">C.-H. Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1\">Mohit Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Y.-C. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Quan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1\">Tomoaki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakam_T/0/1/0/all/0/1\">Tomokazu Murakam</a>",
          "description": "Camera movement and unpredictable environmental conditions like dust and wind\ninduce noise into video feeds. We observe that popular unsupervised MOT methods\nare dependent on noise-free conditions. We show that the addition of a small\namount of artificial random noise causes a sharp degradation in model\nperformance on benchmark metrics. We resolve this problem by introducing a\nrobust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed\nsingle-head attention model helps limit the negative impact of noise by\nlearning visual representations at different segment scales. AttU-Net shows\nbetter unsupervised MOT tracking performance over variational inference-based\nstate-of-the-art baselines. We evaluate our method in the MNIST and the Atari\ngame video benchmark. We also provide two extended video datasets consisting of\ncomplex visual patterns that include Kuzushiji characters and fashion images to\nvalidate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2105.10005",
          "publishedOn": "2021-05-24T05:08:41.880Z",
          "wordCount": 592,
          "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identifying\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-05-24T05:08:41.872Z",
          "wordCount": 583,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nianjuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangbo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "Single image super-resolution (SISR) deals with a fundamental problem of\nupsampling a low-resolution (LR) image to its high-resolution (HR) version.\nLast few years have witnessed impressive progress propelled by deep learning\nmethods. However, one critical challenge faced by existing methods is to strike\na sweet spot of deep model complexity and resulting SISR quality. This paper\naddresses this pain point by proposing a linearly-assembled pixel-adaptive\nregression network (LAPAR), which casts the direct LR to HR mapping learning\ninto a linear coefficient regression task over a dictionary of multiple\npredefined filter bases. Such a parametric representation renders our model\nhighly lightweight and easy to optimize while achieving state-of-the-art\nresults on SISR benchmarks. Moreover, based on the same idea, LAPAR is extended\nto tackle other restoration tasks, e.g., image denoising and JPEG image\ndeblocking, and again, yields strong performance. The code is available at\nhttps://github.com/dvlab-research/Simple-SR.",
          "link": "http://arxiv.org/abs/2105.10422",
          "publishedOn": "2021-05-24T05:08:41.840Z",
          "wordCount": 587,
          "title": "LAPAR: Linearly-Assembled Pixel-Adaptive Regression Network for Single Image Super-Resolution and Beyond. (arXiv:2105.10422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carton_F/0/1/0/all/0/1\">Florence Carton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1\">David Filliat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1\">Jaonary Rabarisoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quoc Cuong Pham</a>",
          "description": "In recent years, we have witnessed increasingly high performance in the field\nof autonomous end-to-end driving. In particular, more and more research is\nbeing done on driving in urban environments, where the car has to follow high\nlevel commands to navigate. However, few evaluations are made on the ability of\nthese agents to react in an unexpected situation. Specifically, no evaluations\nare conducted on the robustness of driving agents in the event of a bad\nhigh-level command. We propose here an evaluation method, namely a benchmark\nthat allows to assess the robustness of an agent, and to appreciate its\nunderstanding of the environment through its ability to keep a safe behavior,\nregardless of the instruction.",
          "link": "http://arxiv.org/abs/2105.10014",
          "publishedOn": "2021-05-24T05:08:41.803Z",
          "wordCount": 567,
          "title": "Evaluating Robustness over High Level Driving Instruction for Autonomous Driving. (arXiv:2105.10014v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mansi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tushar_K/0/1/0/all/0/1\">Kadvekar Rohit Tushar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panneer_A/0/1/0/all/0/1\">Avinash Panneer</a>",
          "description": "The task of predicting smooth and edge-consistent depth maps is notoriously\ndifficult for single image depth estimation. This paper proposes a novel\nBilateral Grid based 3D convolutional neural network, dubbed as 3DBG-UNet, that\nparameterizes high dimensional feature space by encoding compact 3D bilateral\ngrids with UNets and infers sharp geometric layout of the scene. Further,\nanother novel 3DBGES-UNet model is introduced that integrate 3DBG-UNet for\ninferring an accurate depth map given a single color view. The 3DBGES-UNet\nconcatenates 3DBG-UNet geometry map with the inception network edge\naccentuation map and a spatial object's boundary map obtained by leveraging\nsemantic segmentation and train the UNet model with ResNet backbone. Both\nmodels are designed with a particular attention to explicitly account for edges\nor minute details. Preserving sharp discontinuities at depth edges is critical\nfor many applications such as realistic integration of virtual objects in AR\nvideo or occlusion-aware view synthesis for 3D display applications.The\nproposed depth prediction network achieves state-of-the-art performance in both\nqualitative and quantitative evaluations on the challenging NYUv2-Depth data.\nThe code and corresponding pre-trained weights will be made publicly available.",
          "link": "http://arxiv.org/abs/2105.10129",
          "publishedOn": "2021-05-24T05:08:41.797Z",
          "wordCount": 653,
          "title": "A Novel 3D-UNet Deep Learning Framework Based on High-Dimensional Bilateral Grid for Edge Consistent Single Image Depth Estimation. (arXiv:2105.10129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1\">Adyasha Maharana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannan_D/0/1/0/all/0/1\">Darryl Hannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Story visualization is an under-explored task that falls at the intersection\nof many important research directions in both computer vision and natural\nlanguage processing. In this task, given a series of natural language captions\nwhich compose a story, an agent must generate a sequence of images that\ncorrespond to the captions. Prior work has introduced recurrent generative\nmodels which outperform text-to-image synthesis models on this task. However,\nthere is room for improvement of generated images in terms of visual quality,\ncoherence and relevance. We present a number of improvements to prior modeling\napproaches, including (1) the addition of a dual learning framework that\nutilizes video captioning to reinforce the semantic alignment between the story\nand generated images, (2) a copy-transform mechanism for\nsequentially-consistent story visualization, and (3) MART-based transformers to\nmodel complex interactions between frames. We present ablation studies to\ndemonstrate the effect of each of these techniques on the generative power of\nthe model for both individual images as well as the entire narrative.\nFurthermore, due to the complexity and generative nature of the task, standard\nevaluation metrics do not accurately reflect performance. Therefore, we also\nprovide an exploration of evaluation metrics for the model, focused on aspects\nof the generated frames such as the presence/quality of generated characters,\nthe relevance to captions, and the diversity of the generated images. We also\npresent correlation experiments of our proposed automated metrics with human\nevaluations. Code and data available at:\nhttps://github.com/adymaharana/StoryViz",
          "link": "http://arxiv.org/abs/2105.10026",
          "publishedOn": "2021-05-24T05:08:41.790Z",
          "wordCount": 686,
          "title": "Improving Generation and Evaluation of Visual Stories via Semantic Consistency. (arXiv:2105.10026v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bassiouny_R/0/1/0/all/0/1\">Rodina Bassiouny</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Adel Mohamed</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Umapathy_K/0/1/0/all/0/1\">Karthi Umapathy</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naimul Khan</a> (1) ((1) Ryerson University, Toronto, Canada, (2) Mount Sinai Hospital, University of Toronto, Toronto, Canada)",
          "description": "Over the last few decades, Lung Ultrasound (LUS) has been increasingly used\nto diagnose and monitor different lung diseases in neonates. It is a non\ninvasive tool that allows a fast bedside examination while minimally handling\nthe neonate. Acquiring a LUS scan is easy, but understanding the artifacts\nconcerned with each respiratory disease is challenging. Mixed artifact patterns\nfound in different respiratory diseases may limit LUS readability by the\noperator. While machine learning (ML), especially deep learning can assist in\nautomated analysis, simply feeding the ultrasound images to an ML model for\ndiagnosis is not enough to earn the trust of medical professionals. The\nalgorithm should output LUS features that are familiar to the operator instead.\nTherefore, in this paper we present a unique approach for extracting seven\nmeaningful LUS features that can be easily associated with a specific\npathological lung condition: Normal pleura, irregular pleura, thick pleura,\nAlines, Coalescent B-lines, Separate B-lines and Consolidations. These\nartifacts can lead to early prediction of infants developing later respiratory\ndistress symptoms. A single multi-class region proposal-based object detection\nmodel faster-RCNN (fRCNN) was trained on lower posterior lung ultrasound videos\nto detect these LUS features which are further linked to four common neonatal\ndiseases. Our results show that fRCNN surpasses single stage models such as\nRetinaNet and can successfully detect the aforementioned LUS features with a\nmean average precision of 86.4%. Instead of a fully automatic diagnosis from\nimages without any interpretability, detection of such LUS features leave the\nultimate control of diagnosis to the clinician, which can result in a more\ntrustworthy intelligent system.",
          "link": "http://arxiv.org/abs/2105.10081",
          "publishedOn": "2021-05-24T05:08:41.783Z",
          "wordCount": 769,
          "title": "An interpretable object detection based model for the diagnosis of neonatal lung diseases using Ultrasound images. (arXiv:2105.10081v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "The popularity of multimodal sensors and the accessibility of the Internet\nhave brought us a massive amount of unlabeled multimodal data. Since existing\ndatasets and well-trained models are primarily unimodal, the modality gap\nbetween a unimodal network and unlabeled multimodal data poses an interesting\nproblem: how to transfer a pre-trained unimodal network to perform the same\ntask on unlabeled multimodal data? In this work, we propose multimodal\nknowledge expansion (MKE), a knowledge distillation-based framework to\neffectively utilize multimodal data without requiring labels. Opposite to\ntraditional knowledge distillation, where the student is designed to be\nlightweight and inferior to the teacher, we observe that a multimodal student\nmodel consistently denoises pseudo labels and generalizes better than its\nteacher. Extensive experiments on four tasks and different modalities verify\nthis finding. Furthermore, we connect the mechanism of MKE to semi-supervised\nlearning and offer both empirical and theoretical explanations to understand\nthe denoising capability of a multimodal student.",
          "link": "http://arxiv.org/abs/2103.14431",
          "publishedOn": "2021-05-24T05:08:41.629Z",
          "wordCount": 607,
          "title": "Multimodal Knowledge Expansion. (arXiv:2103.14431v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ercelik_E/0/1/0/all/0/1\">Eme&#xe7; Er&#xe7;elik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurtsever_E/0/1/0/all/0/1\">Ekim Yurtsever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>",
          "description": "3D object detection is a core component of automated driving systems.\nState-of-the-art methods fuse RGB imagery and LiDAR point cloud data\nframe-by-frame for 3D bounding box regression. However, frame-by-frame 3D\nobject detection suffers from noise, field-of-view obstruction, and sparsity.\nWe propose a novel Temporal Fusion Module (TFM) to use information from\nprevious time-steps to mitigate these problems. First, a state-of-the-art\nfrustum network extracts point cloud features from raw RGB and LiDAR point\ncloud data frame-by-frame. Then, our TFM module fuses these features with a\nrecurrent neural network. As a result, 3D object detection becomes robust\nagainst single frame failures and transient occlusions. Experiments on the\nKITTI object tracking dataset show the efficiency of the proposed TFM, where we\nobtain ~6%, ~4%, and ~6% improvements on Car, Pedestrian, and Cyclist classes,\nrespectively, compared to frame-by-frame baselines. Furthermore, ablation\nstudies reinforce that the subject of improvement is temporal fusion and show\nthe effects of different placements of TFM in the object detection pipeline.\nOur code is open-source and available at\nhttps://github.com/emecercelik/Temp-Frustum-Net.git.",
          "link": "http://arxiv.org/abs/2104.12106",
          "publishedOn": "2021-05-24T05:08:41.622Z",
          "wordCount": 626,
          "title": "Temp-Frustum Net: 3D Object Detection with Temporal Fusion. (arXiv:2104.12106v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satya Rajendra Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MS_S/0/1/0/all/0/1\">Shruthi MS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventrapragada_S/0/1/0/all/0/1\">Sairathan Ventrapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasharatha_S/0/1/0/all/0/1\">Saivamshi Salla Dasharatha</a>",
          "description": "Deep learning has shown a great improvement in the performance of visual\ntasks. Image retrieval is the task of extracting the visually similar images\nfrom a database for a query image. The feature matching is performed to rank\nthe images. Various hand-designed features have been derived in past to\nrepresent the images. Nowadays, the power of deep learning is being utilized\nfor automatic feature learning from data in the field of biomedical image\nanalysis. Autoencoder and Siamese networks are two deep learning models to\nlearn the latent space (i.e., features or embedding). Autoencoder works based\non the reconstruction of the image from latent space. Siamese network utilizes\nthe triplets to learn the intra-class similarity and inter-class dissimilarity.\nMoreover, Autoencoder is unsupervised, whereas Siamese network is supervised.\nWe propose a Joint Triplet Autoencoder Network (JTANet) by facilitating the\ntriplet learning in autoencoder framework. A joint supervised learning for\nSiamese network and unsupervised learning for Autoencoder is performed.\nMoreover, the Encoder network of Autoencoder is shared with Siamese network and\nreferred as the Siamcoder network. The features are extracted by using the\ntrained Siamcoder network for retrieval purpose. The experiments are performed\nover Histopathological Routine Colon Cancer dataset. We have observed the\npromising performance using the proposed JTANet model against the Autoencoder\nand Siamese models for colon cancer nuclei retrieval in histopathological\nimages.",
          "link": "http://arxiv.org/abs/2105.10262",
          "publishedOn": "2021-05-24T05:08:41.616Z",
          "wordCount": 659,
          "title": "Joint Triplet Autoencoder for Histopathological Colon Cancer Nuclei Retrieval. (arXiv:2105.10262v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santos_E/0/1/0/all/0/1\">Ezequiel Fran&#xe7;a dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontenelle_G/0/1/0/all/0/1\">Gabriel Fontenelle</a>",
          "description": "This paper presents a game, controlled by computer vision, in identification\nof hand gestures (hand-tracking). The proposed work is based on image\nsegmentation and construction of a convex hull with Jarvis Algorithm , and\ndetermination of the pattern based on the extraction of area characteristics in\nthe convex hull.",
          "link": "http://arxiv.org/abs/2105.10063",
          "publishedOn": "2021-05-24T05:08:41.575Z",
          "wordCount": 487,
          "title": "Uma implementa\\c{c}\\~ao do jogo Pedra, Papel e Tesoura utilizando Visao Computacional. (arXiv:2105.10063v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Nghia Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>",
          "description": "This paper presents a temporal classification method for all three subtasks\nof symbol segmentation, symbol recognition and relation classification in\nonline handwritten mathematical expressions (HMEs). The classification model is\ntrained by multiple paths of symbols and spatial relations derived from the\nSymbol Relation Tree (SRT) representation of HMEs. The method benefits from\nglobal context of a deep bidirectional Long Short-term Memory network, which\nlearns the temporal classification directly from online handwriting by the\nConnectionist Temporal Classification loss. To recognize an online HME, a\nsymbol-level parse tree with Context-Free Grammar is constructed, where symbols\nand spatial relations are obtained from the temporal classification results. We\nshow the effectiveness of the proposed method on the two latest CROHME\ndatasets.",
          "link": "http://arxiv.org/abs/2105.10156",
          "publishedOn": "2021-05-24T05:08:41.549Z",
          "wordCount": 557,
          "title": "Global Context for improving recognition of Online Handwritten Mathematical Expressions. (arXiv:2105.10156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1\">Huy Quang Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>",
          "description": "Toward a computer-assisted marking for descriptive math questions,this paper\npresents clustering of online handwritten mathematical expressions (OnHMEs) to\nhelp human markers to mark them efficiently and reliably. We propose a\ngenerative sequence similarity function for computing a similarity score of two\nOnHMEs based on a sequence-to-sequence OnHME recognizer. Each OnHME is\nrepresented by a similarity-based representation (SbR) vector. The SbR matrix\nis inputted to the k-means algorithm for clustering OnHMEs. Experiments are\nconducted on an answer dataset (Dset_Mix) of 200 OnHMEs mixed of real patterns\nand synthesized patterns for each of 10 questions and a real online handwritten\nmathematical answer dataset of 122 student answers at most for each of 15\nquestions (NIER_CBT). The best clustering results achieved around 0.916 and\n0.915 for purity, and around 0.556 and 0.702 for the marking cost on Dset_Mix\nand NIER_CBT, respectively. Our method currently outperforms the previous\nmethods for clustering HMEs.",
          "link": "http://arxiv.org/abs/2105.10159",
          "publishedOn": "2021-05-24T05:08:41.541Z",
          "wordCount": 603,
          "title": "GSSF: A Generative Sequence Similarity Function based on a Seq2Seq model for clustering online handwritten mathematical answers. (arXiv:2105.10159v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1\">Jana Kierdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1\">Immanuel Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kicherer_A/0/1/0/all/0/1\">Anna Kicherer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabawa_L/0/1/0/all/0/1\">Laura Zabawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drees_L/0/1/0/all/0/1\">Lukas Drees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>",
          "description": "The need for accurate yield estimates for viticulture is becoming more\nimportant due to increasing competition in the wine market worldwide. One of\nthe most promising methods to estimate the harvest is berry counting, as it can\nbe approached non-destructively, and its process can be automated. In this\narticle, we present a method that addresses the challenge of occluded berries\nwith leaves to obtain a more accurate estimate of the number of berries that\nwill enable a better estimate of the harvest. We use generative adversarial\nnetworks, a deep learning-based approach that generates a likely scenario\nbehind the leaves exploiting learned patterns from images with non-occluded\nberries. Our experiments show that the estimate of the number of berries after\napplying our method is closer to the manually counted reference. In contrast to\napplying a factor to the berry count, our approach better adapts to local\nconditions by directly involving the appearance of the visible berries.\nFurthermore, we show that our approach can identify which areas in the image\nshould be changed by adding new berries without explicitly requiring\ninformation about hidden areas.",
          "link": "http://arxiv.org/abs/2105.10325",
          "publishedOn": "2021-05-24T05:08:41.520Z",
          "wordCount": 644,
          "title": "Behind the leaves -- Estimation of occluded grapevine berries with conditional generative adversarial networks. (arXiv:2105.10325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdullahi_S/0/1/0/all/0/1\">Sani M. Abdullahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuifa_S/0/1/0/all/0/1\">Sun Shuifa</a>",
          "description": "Cancelable biometric techniques have been used to prevent the compromise of\nbiometric data by generating and using their corresponding cancelable templates\nfor user authentication. However, the non-invertible distance preserving\ntransformation methods employed in various schemes are often vulnerable to\ninformation leakage since matching is performed in the transformed domain. In\nthis paper, we propose a non-invertible distance preserving scheme based on\nvector permutation and shift-order process. First, the dimension of feature\nvectors is reduced using kernelized principle component analysis (KPCA) prior\nto randomly permuting the extracted vector features. A shift-order process is\nthen applied to the generated features in order to achieve non-invertibility\nand combat similarity-based attacks. The generated hash codes are resilient to\ndifferent security and privacy attacks whilst fulfilling the major revocability\nand unlinkability requirements. Experimental evaluation conducted on 6 datasets\nof FVC2002 and FVC2004 reveals a high-performance accuracy of the proposed\nscheme better than other existing state-of-the-art schemes.",
          "link": "http://arxiv.org/abs/2105.10227",
          "publishedOn": "2021-05-24T05:08:41.502Z",
          "wordCount": 601,
          "title": "Random Hash Code Generation for Cancelable Fingerprint Templates using Vector Permutation and Shift-order Process. (arXiv:2105.10227v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Leilei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lin Xu</a>",
          "description": "Scale variation is one of the most challenging problems in face detection.\nModern face detectors employ feature pyramids to deal with scale variation.\nHowever, it might break the feature consistency across different scales of\nfaces. In this paper, we propose a simple yet effective method named the\nreceptive field pyramids (RFP) method to enhance the representation ability of\nfeature pyramids. It can learn different receptive fields in each feature map\nadaptively based on the varying scales of detected faces. Empirical results on\ntwo face detection benchmark datasets, i.e., WIDER FACE and UFDD, demonstrate\nthat our proposed method can accelerate the inference rate significantly while\nachieving state-of-the-art performance. The source code of our method is\navailable at \\url{https://github.com/emdata-ailab/EMface}.",
          "link": "http://arxiv.org/abs/2105.10104",
          "publishedOn": "2021-05-24T05:08:41.496Z",
          "wordCount": 557,
          "title": "EMface: Detecting Hard Faces by Exploring Receptive Field Pyraminds. (arXiv:2105.10104v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yuri Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1\">Koji Mineshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_K/0/1/0/all/0/1\">Kazuhiro Ueda</a>",
          "description": "There has been a widely held view that visual representations (e.g.,\nphotographs and illustrations) do not depict negation, for example, one that\ncan be expressed by a sentence \"the train is not coming\". This view is\nempirically challenged by analyzing the real-world visual representations of\ncomic (manga) illustrations. In the experiment using image captioning tasks, we\ngave people comic illustrations and asked them to explain what they could read\nfrom them. The collected data showed that some comic illustrations could depict\nnegation without any aid of sequences (multiple panels) or conventional devices\n(special symbols). This type of comic illustrations was subjected to further\nexperiments, classifying images into those containing negation and those not\ncontaining negation. While this image classification was easy for humans, it\nwas difficult for data-driven machines, i.e., deep learning models (CNN), to\nachieve the same high performance. Given the findings, we argue that some comic\nillustrations evoke background knowledge and thus can depict negation with\npurely visual elements.",
          "link": "http://arxiv.org/abs/2105.10131",
          "publishedOn": "2021-05-24T05:08:41.489Z",
          "wordCount": 617,
          "title": "Visual representation of negation: Real world data analysis on comic image design. (arXiv:2105.10131v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chih-Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hsuan-Cheng Liao</a>",
          "description": "Within the context of autonomous driving, safety-related metrics for deep\nneural networks have been widely studied for image classification and object\ndetection. In this paper, we further consider safety-aware correctness and\nrobustness metrics specialized for semantic segmentation. The novelty of our\nproposal is to move beyond pixel-level metrics: Given two images with each\nhaving N pixels being class-flipped, the designed metrics should, depending on\nthe clustering of pixels being class-flipped or the location of occurrence,\nreflect a different level of safety criticality. The result evaluated on an\nautonomous driving dataset demonstrates the validity and practicality of our\nproposed methodology.",
          "link": "http://arxiv.org/abs/2105.10142",
          "publishedOn": "2021-05-24T05:08:41.476Z",
          "wordCount": 531,
          "title": "Safety Metrics for Semantic Segmentation in Autonomous Driving. (arXiv:2105.10142v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_B/0/1/0/all/0/1\">Boaz Shmueli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Soumya Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>",
          "description": "Datasets with induced emotion labels are scarce but of utmost importance for\nmany NLP tasks. We present a new, automated method for collecting texts along\nwith their induced reaction labels. The method exploits the online use of\nreaction GIFs, which capture complex affective states. We show how to augment\nthe data with induced emotion and induced sentiment labels. We use our method\nto create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K\ntweets. We provide baselines for three new tasks, including induced sentiment\nprediction and multilabel classification of induced emotions. Our method and\ndataset open new research opportunities in emotion detection and affective\ncomputing.",
          "link": "http://arxiv.org/abs/2105.09967",
          "publishedOn": "2021-05-24T05:08:41.422Z",
          "wordCount": 563,
          "title": "Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter. (arXiv:2105.09967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1\">Anastasiya Dykyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1\">Paulo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1\">Eric Jul</a>",
          "description": "Capillaries are the smallest vessels in the body responsible for the delivery\nof oxygen and nutrients to the surrounding cells. Various diseases have been\nshown to alter the density of nutritive capillaries and the flow velocity of\nerythrocytes. In previous studies, capillary density and flow velocity have\nbeen assessed manually by trained specialists. Manual analysis of a 20-second\nlong microvascular video takes on average 20 minutes and requires extensive\ntraining. Several studies have reported that manual analysis hinders the\napplication of microvascular microscopy in a clinical setting. In this paper,\nwe present a fully automated system, called CapillaryNet, that can automate\nmicrovascular microscopy analysis so it can be used as a clinical application.\nMoreover, CapillaryNet measures several microvascular parameters that\nresearchers were previously unable to quantify, i.e. capillary hematocrit and\nintra-capillary flow velocity heterogeneity.",
          "link": "http://arxiv.org/abs/2104.11574",
          "publishedOn": "2021-05-24T05:08:41.208Z",
          "wordCount": 607,
          "title": "CapillaryNet: An Automated System to Analyze Microcirculation Videos from Handheld Vital Microscopy. (arXiv:2104.11574v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>",
          "description": "Object detection can be regarded as a pixel clustering task, and its boundary\nis determined by four extreme points (leftmost, top, rightmost, and bottom).\nHowever, most studies focus on the center or corner points of the object, which\nare actually conditional results of the extreme points. In this paper, we\npresent an Extreme-Point-Prediction-Based object detector (EPP-Net), which\ndirectly regresses the relative displacement vector between each pixel and the\nfour extreme points. We also propose a new metric to measure the similarity\nbetween two groups of extreme points, namely, Extreme Intersection over Union\n(EIoU), and incorporate this EIoU as a new regression loss. Moreover, we\npropose a novel branch to predict the EIoU between the ground-truth and the\nprediction results, and combine it with the classification confidence as the\nranking keyword in non-maximum suppression. On the MS-COCO dataset, our method\nachieves an average precision (AP) of 44.0% with ResNet-50 and an AP of 48.3%\nwith ResNeXt-101-DCN. The proposed EPP-Net provides a new method to detect\nobjects and outperforms state-of-the-art anchor-free detectors.",
          "link": "http://arxiv.org/abs/2104.14066",
          "publishedOn": "2021-05-23T06:08:18.236Z",
          "wordCount": 613,
          "title": "Objects as Extreme Points. (arXiv:2104.14066v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leshchinskiy_B/0/1/0/all/0/1\">Brandon Leshchinskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Requena_Mesa_C/0/1/0/all/0/1\">Christian Requena-Mesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chishtie_F/0/1/0/all/0/1\">Farrukh Chishtie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1\">Natalia D&#xed;az-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1\">Oc&#xe9;ane Boulais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aruna Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pina_A/0/1/0/all/0/1\">Aaron Pi&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raissi_C/0/1/0/all/0/1\">Chedy Ra&#xef;ssi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1\">Alexander Lavin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1\">Dava Newman</a>",
          "description": "As climate change increases the intensity of natural disasters, society needs\nbetter tools for adaptation. Floods, for example, are the most frequent natural\ndisaster, and better tools for flood risk communication could increase the\nsupport for flood-resilient infrastructure development. Our work aims to enable\nmore visual communication of large-scale climate impacts via visualizing the\noutput of coastal flood models as satellite imagery. We propose the first deep\nlearning pipeline to ensure physical-consistency in synthetic visual satellite\nimagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it\nproduces imagery that is physically-consistent with the output of an\nexpert-validated storm surge model (NOAA SLOSH). By evaluating the imagery\nrelative to physics-based flood maps, we find that our proposed framework\noutperforms baseline models in both physical-consistency and photorealism. We\nenvision our work to be the first step towards a global visualization of how\nclimate change shapes our landscape. Continuing on this path, we show that the\nproposed pipeline generalizes to visualize arctic sea ice melt. We also publish\na dataset of over 25k labelled image-pairs to study image-to-image translation\nin Earth observation.",
          "link": "http://arxiv.org/abs/2104.04785",
          "publishedOn": "2021-05-23T06:08:18.215Z",
          "wordCount": 685,
          "title": "Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization. (arXiv:2104.04785v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13482",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_K/0/1/0/all/0/1\">Kang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoyun Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fakai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1\">Chihung Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Lingyun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chang-Fu Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_S/0/1/0/all/0/1\">Shun Miao</a>",
          "description": "Bone mineral density (BMD) is a clinically critical indicator of\nosteoporosis, usually measured by dual-energy X-ray absorptiometry (DEXA). Due\nto the limited accessibility of DEXA machines and examinations, osteoporosis is\noften under-diagnosed and under-treated, leading to increased fragility\nfracture risks. Thus it is highly desirable to obtain BMDs with alternative\ncost-effective and more accessible medical imaging examinations such as X-ray\nplain films. In this work, we formulate the BMD estimation from plain hip X-ray\nimages as a regression problem. Specifically, we propose a new semi-supervised\nself-training algorithm to train the BMD regression model using images coupled\nwith DEXA measured BMDs and unlabeled images with pseudo BMDs. Pseudo BMDs are\ngenerated and refined iteratively for unlabeled images during self-training. We\nalso present a novel adaptive triplet loss to improve the model's regression\naccuracy. On an in-house dataset of 1,090 images (819 unique patients), our BMD\nestimation method achieves a high Pearson correlation coefficient of 0.8805 to\nground-truth BMDs. It offers good feasibility to use the more accessible and\ncheaper X-ray imaging for opportunistic osteoporosis screening.",
          "link": "http://arxiv.org/abs/2103.13482",
          "publishedOn": "2021-05-23T06:08:18.206Z",
          "wordCount": 649,
          "title": "Semi-Supervised Learning for Bone Mineral Density Estimation in Hip X-ray Images. (arXiv:2103.13482v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruimin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiayi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baofeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunlei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jie Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongjiang Wei</a>",
          "description": "Quantitative susceptibility mapping (QSM) has demonstrated great potential in\nquantifying tissue susceptibility in various brain diseases. However, the\nintrinsic ill-posed inverse problem relating the tissue phase to the underlying\nsusceptibility distribution affects the accuracy for quantifying tissue\nsusceptibility. Recently, deep learning has shown promising results to improve\naccuracy by reducing the streaking artifacts. However, there exists a mismatch\nbetween the observed phase and the theoretical forward phase estimated by the\nsusceptibility label. In this study, we proposed a model-based deep learning\narchitecture that followed the STI (susceptibility tensor imaging) physical\nmodel, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the\nrelationship between STI-derived phase contrast induced by the susceptibility\ntensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The\nconvolution neural networks are embedded into the physical model to learn a\nregularization term containing prior information. ki33 and phase induced by\nki13 and ki23 terms were used as the labels for network training. Quantitative\nevaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed\ndeep learning QSM methods. The results showed that MoDL-QSM achieved superior\nperformance, demonstrating its potential for future applications.",
          "link": "http://arxiv.org/abs/2101.08413",
          "publishedOn": "2021-05-23T06:08:18.199Z",
          "wordCount": 660,
          "title": "MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility Mapping. (arXiv:2101.08413v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1\">Xiaoyu Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>",
          "description": "Human motion prediction aims to forecast future human poses given a sequence\nof past 3D skeletons. While this problem has recently received increasing\nattention, it has mostly been tackled for single humans in isolation. In this\npaper we explore this problem from a novel perspective, involving humans\nperforming collaborative tasks. We assume that the input of our system are two\nsequences of past skeletons for two interacting persons, and we aim to predict\nthe future motion for each of them. For this purpose, we devise a novel cross\ninteraction attention mechanism that exploits historical information of both\npersons and learns to predict cross dependencies between self poses and the\nposes of the other person in spite of their spatial or temporal distance. Since\nno dataset to train such interactive situations is available, we have captured\nExPI (Extreme Pose Interaction), a new lab-based person interaction dataset of\nprofessional dancers performing acrobatics. ExPI contains 115 sequences with\n30k frames and 60k instances with annotated 3D body poses and shapes. We\nthoroughly evaluate our cross-interaction network on this dataset and show that\nboth in short-term and long-term predictions, it consistently outperforms\nbaselines that independently reason for each person. We plan to release our\ncode jointly with the dataset and the train/test splits to spur future research\non the topic.",
          "link": "http://arxiv.org/abs/2105.08825",
          "publishedOn": "2021-05-23T06:08:18.192Z",
          "wordCount": 661,
          "title": "Multi-Person Extreme Motion Prediction with Cross-Interaction Attention. (arXiv:2105.08825v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>",
          "description": "The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.",
          "link": "http://arxiv.org/abs/2012.00641",
          "publishedOn": "2021-05-23T06:08:18.175Z",
          "wordCount": 678,
          "title": "A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08147",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rister_B/0/1/0/all/0/1\">Blaine Rister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L. Rubin</a>",
          "description": "Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently\nobtained to determine the extent of lung disease and are a valuable source of\ndata for creating artificial intelligence models. Most work to date assessing\ndisease severity on chest imaging has focused on segmenting computed tomography\n(CT) images; however, given that CTs are performed much less frequently than\nchest X-rays for COVID-19 patients, automated lung lesion segmentation on chest\nX-rays could be clinically valuable. There currently exists a universal\nshortage of chest X-rays with ground truth COVID-19 lung lesion annotations,\nand manually contouring lung opacities is a tedious, labor-intensive task. To\naccelerate severity detection and augment the amount of publicly available\nchest X-ray training data for supervised deep learning (DL) models, we leverage\nexisting annotated CT images to generate frontal projection \"chest X-ray\"\nimages for training COVID-19 chest X-ray models. In this paper, we propose an\nautomated pipeline for segmentation of COVID-19 lung lesions on chest X-rays\ncomprised of a Mask R-CNN trained on a mixed dataset of open-source chest\nX-rays and coronal X-ray projections computed from annotated volumetric CTs. On\na test set containing 40 chest X-rays of COVID-19 positive patients, our model\nachieved IoU scores of 0.81 $\\pm$ 0.03 and 0.79 $\\pm$ 0.03 when trained on a\ndataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50\nprojections from CTs, respectively. Our model far outperforms current baselines\nwith limited supervised training and may assist in automated COVID-19 severity\nquantification on chest X-rays.",
          "link": "http://arxiv.org/abs/2105.08147",
          "publishedOn": "2021-05-23T06:08:18.169Z",
          "wordCount": 779,
          "title": "COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs. (arXiv:2105.08147v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hingane_S/0/1/0/all/0/1\">Shreeshail Hingane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Artistic style transfer aims to transfer the style characteristics of one\nimage onto another image while retaining its content. Existing approaches\ncommonly leverage various normalization techniques, although these face\nlimitations in adequately transferring diverse textures to different spatial\nlocations. Self-Attention-based approaches have tackled this issue with partial\nsuccess but suffer from unwanted artifacts. Motivated by these observations,\nthis paper aims to combine the best of both worlds: self-attention and\nnormalization. That yields a new plug-and-play module that we name\nSelf-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially\na spatially adaptive normalization module whose parameters are inferred through\nattention on the content and style image. We demonstrate that plugging SAFIN\ninto the base network of another state-of-the-art method results in enhanced\nstylization. We also develop a novel base network composed of Wavelet Transform\nfor multi-scale style transfer, which when combined with SAFIN, produces\nvisually appealing results with lesser unwanted textures.",
          "link": "http://arxiv.org/abs/2105.06129",
          "publishedOn": "2021-05-23T06:08:18.162Z",
          "wordCount": 616,
          "title": "SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization. (arXiv:2105.06129v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Speth_J/0/1/0/all/0/1\">Jeremy Speth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vance_N/0/1/0/all/0/1\">Nathan Vance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1\">Patrick Flynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin Bowyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1\">Adam Czajka</a>",
          "description": "Remote photoplethysmography (rPPG), a family of techniques for monitoring\nblood volume changes, may be especially useful for widespread contactless\nhealth monitoring using face video from consumer-grade visible-light cameras.\nThe COVID-19 pandemic has caused the widespread use of protective face masks.\nWe found that occlusions from cloth face masks increased the mean absolute\nerror of heart rate estimation by more than 80\\% when deploying methods\ndesigned on unmasked faces. We show that augmenting unmasked face videos by\nadding patterned synthetic face masks forces the model to attend to the\nperiocular and forehead regions, improving performance and closing the gap\nbetween masked and unmasked pulse estimation. To our knowledge, this paper is\nthe first to analyse the impact of face masks on the accuracy of pulse\nestimation and offers several novel contributions: (a) 3D CNN-based method\ndesigned for remote photoplethysmography in a presence of face masks, (b) two\npublicly available pulse estimation datasets acquired from 86 unmasked and 61\nmasked subjects, (c) evaluations of handcrafted algorithms and a 3D CNN trained\non videos of unmasked faces and with masks synthetically added, and (d) data\naugmentation method to add a synthetic mask to a face video.",
          "link": "http://arxiv.org/abs/2101.04096",
          "publishedOn": "2021-05-23T06:08:18.154Z",
          "wordCount": 714,
          "title": "Remote Pulse Estimation in the Presence of Face Masks. (arXiv:2101.04096v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xinya Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaisiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>",
          "description": "Despite previous success in generating audio-driven talking heads, most of\nthe previous studies focus on the correlation between speech content and the\nmouth shape. Facial emotion, which is one of the most important features on\nnatural human faces, is always neglected in their methods. In this work, we\npresent Emotional Video Portraits (EVP), a system for synthesizing high-quality\nvideo portraits with vivid emotional dynamics driven by audios. Specifically,\nwe propose the Cross-Reconstructed Emotion Disentanglement technique to\ndecompose speech into two decoupled spaces, i.e., a duration-independent\nemotion space and a duration dependent content space. With the disentangled\nfeatures, dynamic 2D emotional facial landmarks can be deduced. Then we propose\nthe Target-Adaptive Face Synthesis technique to generate the final high-quality\nvideo portraits, by bridging the gap between the deduced landmarks and the\nnatural head poses of target videos. Extensive experiments demonstrate the\neffectiveness of our method both qualitatively and quantitatively.",
          "link": "http://arxiv.org/abs/2104.07452",
          "publishedOn": "2021-05-23T06:08:18.147Z",
          "wordCount": 612,
          "title": "Audio-Driven Emotional Video Portraits. (arXiv:2104.07452v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1\">Ekta U. Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingjian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis G. Banerjee</a>",
          "description": "Object recognition in unseen indoor environments remains a challenging\nproblem for visual perception of mobile robots. In this letter, we propose the\nuse of topologically persistent features, which rely on the objects' shape\ninformation, to address this challenge. In particular, we extract two kinds of\nfeatures, namely, sparse persistence image (PI) and amplitude, by applying\npersistent homology to multi-directional height function-based filtrations of\nthe cubical complexes representing the object segmentation maps. The features\nare then used to train a fully connected network for recognition. For\nperformance evaluation, in addition to a widely used shape dataset and a\nbenchmark indoor scenes dataset, we collect a new dataset, comprising scene\nimages from two different environments, namely, a living room and a mock\nwarehouse. The scenes are captured using varying camera poses under different\nillumination conditions and include up to five different objects from a given\nset of fourteen objects. On the benchmark indoor scenes dataset, sparse PI\nfeatures show better recognition performance in unseen environments than the\nfeatures learned using the widely used ResNetV2-56 and EfficientNet-B4 models.\nFurther, they provide slightly higher recall and accuracy values than Faster\nR-CNN, an end-to-end object detection method, and its state-of-the-art variant,\nDomain Adaptive Faster R-CNN. The performance of our methods also remains\nrelatively unchanged from the training environment (living room) to the unseen\nenvironment (mock warehouse) in the new dataset. In contrast, the performance\nof the object detection methods drops substantially. We also implement the\nproposed method on a real-world robot to demonstrate its usefulness.",
          "link": "http://arxiv.org/abs/2010.03196",
          "publishedOn": "2021-05-23T06:08:18.140Z",
          "wordCount": 761,
          "title": "Visual Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08506",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1\">Sara Atito Ali Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1\">Mehmet Can Yavuz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sen_M/0/1/0/all/0/1\">Mehmet Umut Sen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulsen_F/0/1/0/all/0/1\">Fatih Gulsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tutar_O/0/1/0/all/0/1\">Onur Tutar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korkmazer_B/0/1/0/all/0/1\">Bora Korkmazer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samanci_C/0/1/0/all/0/1\">Cesur Samanci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirolu_S/0/1/0/all/0/1\">Sabri Sirolu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamid_R/0/1/0/all/0/1\">Rauf Hamid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eryurekli_A/0/1/0/all/0/1\">Ali Ergun Eryurekli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mammadov_T/0/1/0/all/0/1\">Toghrul Mammadov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yanikoglu_B/0/1/0/all/0/1\">Berrin Yanikoglu</a>",
          "description": "Detecting COVID-19 in computed tomography (CT) or radiography images has been\nproposed as a supplement to the definitive RT-PCR test. We present a deep\nlearning ensemble for detecting COVID-19 infection, combining slice-based (2D)\nand volume-based (3D) approaches. The 2D system detects the infection on each\nCT slice independently, combining them to obtain the patient-level decision via\ndifferent methods (averaging and long-short term memory networks). The 3D\nsystem takes the whole CT volume to arrive to the patient-level decision in one\nstep. A new high resolution chest CT scan dataset, called the IST-C dataset, is\nalso collected in this work. The proposed ensemble, called IST-CovNet, obtains\n90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting\nCOVID-19 among normal controls and other types of lung pathologies; and 93.69%\naccuracy and 0.99 AUC score on the publicly available MosMed dataset that\nconsists of COVID-19 scans and normal controls only. The system is deployed at\nIstanbul University Cerrahpasa School of Medicine.",
          "link": "http://arxiv.org/abs/2105.08506",
          "publishedOn": "2021-05-23T06:08:18.131Z",
          "wordCount": 693,
          "title": "COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches. (arXiv:2105.08506v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "We show how to use random field theory in a supervised, energy-based model\nfor multiple pseudo image classification of 2D integer matrices. In the model,\neach row of a 2D integer matrix is a pseudo image where a local receptive field\nfocuses on multiple portions of individual rows for simultaneous learning. The\nmodel is used for a classification task consisting of presence of patient\nbiomarkers indicative of a particular disease.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-05-23T06:08:18.110Z",
          "wordCount": 540,
          "title": "Multiple Simultaneous Pseudo Image Classification with Random Fields and a Deep Belief Network for Disease Indication. (arXiv:2104.10762v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masaya Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "Various fonts give different impressions, such as legible, rough, and\ncomic-text.This paper aims to analyze the correlation between the local shapes,\nor parts, and the impression of fonts. By focusing on local shapes instead of\nthe whole letter shape, we can realize letter-shape independent and more\ngeneral analysis. The analysis is performed by newly combining SIFT and\nDeepSets, to extract an arbitrary number of essential parts from a particular\nfont and aggregate them to infer the font impressions by nonlinear regression.\nOur qualitative and quantitative analyses prove that (1)fonts with similar\nparts have similar impressions, (2)many impressions, such as legible and rough,\nlargely depend on specific parts, (3)several impressions are very irrelevant to\nparts.",
          "link": "http://arxiv.org/abs/2103.14216",
          "publishedOn": "2021-05-23T06:08:18.103Z",
          "wordCount": 569,
          "title": "Which Parts determine the Impression of the Font?. (arXiv:2103.14216v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retrain. Our key innovation is to redefine\nthe gradient to a new synaptic parameter, allowing better exploration of\nnetwork structures by taking full advantage of the competition between pruning\nand regrowth of connections. The experimental results show that the proposed\nmethod achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset\nso far. Moreover, it reaches a $\\sim$3.5% accuracy loss under unprecedented\n0.73% connectivity, which reveals remarkable structure refining capability in\nSNNs. Our work suggests that there exists extremely high redundancy in deep\nSNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring .",
          "link": "http://arxiv.org/abs/2105.04916",
          "publishedOn": "2021-05-23T06:08:18.096Z",
          "wordCount": 696,
          "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1\">Quentin Paletta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>",
          "description": "Improving irradiance forecasting is critical to further increase the share of\nsolar in the energy mix. On a short time scale, fish-eye cameras on the ground\nare used to capture cloud displacements causing the local variability of the\nelectricity production. As most of the solar radiation comes directly from the\nSun, current forecasting approaches use its position in the image as a\nreference to interpret the cloud cover dynamics. However, existing Sun tracking\nmethods rely on external data and a calibration of the camera, which requires\naccess to the device. To address these limitations, this study introduces an\nimage-based Sun tracking algorithm to localise the Sun in the image when it is\nvisible and interpolate its daily trajectory from past observations. We\nvalidate the method on a set of sky images collected over a year at SIRTA's\nlab. Experimental results show that the proposed method provides robust smooth\nSun trajectories with a mean absolute error below 1% of the image size.",
          "link": "http://arxiv.org/abs/2012.01059",
          "publishedOn": "2021-05-23T06:08:18.079Z",
          "wordCount": 634,
          "title": "A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuqing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1\">Olivia Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>",
          "description": "Policies trained in simulation often fail when transferred to the real world\ndue to the `reality gap' where the simulator is unable to accurately capture\nthe dynamics and visual properties of the real world. Current approaches to\ntackle this problem, such as domain randomization, require prior knowledge and\nengineering to determine how much to randomize system parameters in order to\nlearn a policy that is robust to sim-to-real transfer while also not being too\nconservative. We propose a method for automatically tuning simulator system\nparameters to match the real world using only raw RGB images of the real world\nwithout the need to define rewards or estimate state. Our key insight is to\nreframe the auto-tuning of parameters as a search problem where we iteratively\nshift the simulation system parameters to approach the real-world system\nparameters. We propose a Search Param Model (SPM) that, given a sequence of\nobservations and actions and a set of system parameters, predicts whether the\ngiven parameters are higher or lower than the true parameters used to generate\nthe observations. We evaluate our method on multiple robotic control tasks in\nboth sim-to-sim and sim-to-real transfer, demonstrating significant improvement\nover naive domain randomization. Project videos and code at\nhttps://yuqingd.github.io/autotuned-sim2real/",
          "link": "http://arxiv.org/abs/2104.07662",
          "publishedOn": "2021-05-23T06:08:18.072Z",
          "wordCount": 681,
          "title": "Auto-Tuned Sim-to-Real Transfer. (arXiv:2104.07662v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Aditya Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girishekar_E/0/1/0/all/0/1\">Eshwar Shamanna Girishekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Padmakar Anil Deshpande</a>",
          "description": "Automated image captioning is one of the applications of Deep Learning which\ninvolves fusion of work done in computer vision and natural language\nprocessing, and it is typically performed using Encoder-Decoder architectures.\nIn this project, we have implemented and experimented with various flavors of\nmulti-modal image captioning networks where ResNet101, DenseNet121 and VGG19\nbased CNN Encoders and Attention based LSTM Decoders were explored. We have\nstudied the effect of beam size and the use of pretrained word embeddings and\ncompared them to baseline CNN encoder and RNN decoder architecture. The goal is\nto analyze the performance of each approach using various evaluation metrics\nincluding BLEU, CIDEr, ROUGE and METEOR. We have also explored model\nexplainability using Visual Attention Maps (VAM) to highlight parts of the\nimages which has maximum contribution for predicting each word of the generated\ncaption.",
          "link": "http://arxiv.org/abs/2105.09906",
          "publishedOn": "2021-05-23T06:08:18.065Z",
          "wordCount": 582,
          "title": "Empirical Analysis of Image Caption Generation using Deep Learning. (arXiv:2105.09906v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">YiMin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yingjie Xi</a>",
          "description": "Detection faults in seismic data is a crucial step for seismic structural\ninterpretation, reservoir characterization and well placement. Some recent\nworks regard it as an image segmentation task. The task of image segmentation\nrequires huge labels, especially 3D seismic data, which has a complex structure\nand lots of noise. Therefore, its annotation requires expert experience and a\nhuge workload. In this study, we present {\\lambda}-BCE and {\\lambda}-smooth\nL1loss to effectively train 3D-CNN by some slices from 3D seismic data, so that\nthe model can learn the segmentation of 3D seismic data from a few 2D slices.\nIn order to fully extract information from limited data and suppress seismic\nnoise, we propose an attention module that can be used for active supervision\ntraining and embedded in the network. The attention heatmap target is generated\nby the original label, and letting it supervise the attention module using the\n{\\lambda}-smooth L1loss. The experiment proves the effectiveness of our loss\nfunction and attention module, it also shows that our method can extract 3D\nseismic features from a few 2D slices labels, and the segmentation effect\nachieves state-of-the-art. We only use 3.3% of the all labels, and we can\nachieve similar performance as using all labels. This work has been submitted\nto the IEEE for possible publication. Copyright may be transferred without\nnotice, after which this version may no longer be accessible.",
          "link": "http://arxiv.org/abs/2105.03857",
          "publishedOn": "2021-05-23T06:08:18.058Z",
          "wordCount": 724,
          "title": "Seismic Fault Segmentation via 3D-CNN Training by a Few 2D Slices Labels. (arXiv:2105.03857v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yubo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan-Cong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1\">Minh-Quan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh-Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thanh-Toan Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam V. Nguyen</a>",
          "description": "This paper pushes the envelope on camouflaged regions to decompose them into\nmeaningful components, namely, camouflaged instances. To promote the new task\nof camouflaged instance segmentation in-the-wild, we introduce a new dataset,\nnamely CAMO++, by extending our preliminary CAMO dataset (camouflaged object\nsegmentation) in terms of quantity and diversity. The new dataset substantially\nincreases the number of images with hierarchical pixel-wise ground-truths. We\nalso provide a benchmark suite for the task of camouflaged instance\nsegmentation. In particular, we conduct extensive evaluation of\nstate-of-the-art instance segmentation methods on our newly constructed CAMO++\ndataset in various scenarios. We also propose Camouflage Fusion Learning (CFL)\nframework for camouflaged instance segmentation to further improve the\nstate-of-the-art performance. The dataset, model, evaluation suite, and\nbenchmark will be publicly available at our project page.\n\\url{https://sites.google.com/view/ltnghia/research/camo\\_plus\\_plus}",
          "link": "http://arxiv.org/abs/2103.17123",
          "publishedOn": "2021-05-23T06:08:18.024Z",
          "wordCount": 611,
          "title": "Camouflaged Instance Segmentation In-The-Wild: Dataset And Benchmark Suite. (arXiv:2103.17123v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hornauer_S/0/1/0/all/0/1\">Sascha Hornauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffarzadegan_S/0/1/0/all/0/1\">Shabnam Ghaffarzadegan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>",
          "description": "Recent progress in network-based audio event classification has shown the\nbenefit of pre-training models on visual data such as ImageNet. While this\nprocess allows knowledge transfer across different domains, training a model on\nlarge-scale visual datasets is time consuming. On several audio event\nclassification benchmarks, we show a fast and effective alternative that\npre-trains the model unsupervised, only on audio data and yet delivers on-par\nperformance with ImageNet pre-training. Furthermore, we show that our\ndiscriminative audio learning can be used to transfer knowledge across audio\ndatasets and optionally include ImageNet pre-training.",
          "link": "http://arxiv.org/abs/2105.09279",
          "publishedOn": "2021-05-23T06:08:18.017Z",
          "wordCount": 573,
          "title": "Unsupervised Discriminative Learning of Sounds for Audio Event Classification. (arXiv:2105.09279v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03814",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1\">Phairot Autthasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1\">Rattanaphon Chaisaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1\">Thapanun Sudhawiyangkul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1\">Phurin Rangpong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1\">Suktipol Kiatthaveephong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1\">Nat Dilokthanakul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1\">Gun Bhakdisongkhram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>",
          "description": "Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)\nallow control of several applications by decoding neurophysiological phenomena,\nwhich are usually recorded by electroencephalography (EEG) using a non-invasive\ntechnique. Despite great advances in MI-based BCI, EEG rhythms are specific to\na subject and various changes over time. These issues point to significant\nchallenges to enhance the classification performance, especially in a\nsubject-independent manner. To overcome these challenges, we propose MIN2Net, a\nnovel end-to-end multi-task learning to tackle this task. We integrate deep\nmetric learning into a multi-task autoencoder to learn a compact and\ndiscriminative latent representation from EEG and perform classification\nsimultaneously. This approach reduces the complexity in pre-processing, results\nin significant performance improvement on EEG classification. Experimental\nresults in a subject-independent manner show that MIN2Net outperforms the\nstate-of-the-art techniques, achieving an F1-score improvement of 6.72%, and\n2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that\nMIN2Net improves discriminative information in the latent representation. This\nstudy indicates the possibility and practicality of using this model to develop\nMI-based BCI applications for new users without the need for calibration.",
          "link": "http://arxiv.org/abs/2102.03814",
          "publishedOn": "2021-05-23T06:08:18.005Z",
          "wordCount": 664,
          "title": "MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09750",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1\">Zongcai Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "Along with the rapid development of real-world applications, higher\nrequirements on the accuracy and efficiency of image super-resolution (SR) are\nbrought forward. Though existing methods have achieved remarkable success, the\nmajority of them demand plenty of computational resources and large amount of\nRAM, and thus they can not be well applied to mobile device. In this paper, we\naim at designing efficient architecture for 8-bit quantization and deploy it on\nmobile device. First, we conduct an experiment about meta-node latency by\ndecomposing lightweight SR architectures, which determines the portable\noperations we can utilize. Then, we dig deeper into what kind of architecture\nis beneficial to 8-bit quantization and propose anchor-based plain net (ABPN).\nFinally, we adopt quantization-aware training strategy to further boost the\nperformance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in\nterms of PSNR, while satisfying realistic needs at the same time. Code is\navaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.",
          "link": "http://arxiv.org/abs/2105.09750",
          "publishedOn": "2021-05-23T06:08:17.998Z",
          "wordCount": 596,
          "title": "Anchor-based Plain Net for Mobile Image Super-Resolution. (arXiv:2105.09750v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sibo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1\">Sertac Karaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Deep learning has been used to demonstrate end-to-end neural network learning\nfor autonomous vehicle control from raw sensory input. While LiDAR sensors\nprovide reliably accurate information, existing end-to-end driving solutions\nare mainly based on cameras since processing 3D data requires a large memory\nfootprint and computation cost. On the other hand, increasing the robustness of\nthese systems is also critical; however, even estimating the model's\nuncertainty is very challenging due to the cost of sampling-based methods. In\nthis paper, we present an efficient and robust LiDAR-based end-to-end\nnavigation framework. We first introduce Fast-LiDARNet that is based on sparse\nconvolution kernel optimization and hardware-aware model design. We then\npropose Hybrid Evidential Fusion that directly estimates the uncertainty of the\nprediction from only a single forward pass and then fuses the control\npredictions intelligently. We evaluate our system on a full-scale vehicle and\ndemonstrate lane-stable as well as navigation capabilities. In the presence of\nout-of-distribution events (e.g., sensor failures), our system significantly\nimproves robustness and reduces the number of takeovers in the real world.",
          "link": "http://arxiv.org/abs/2105.09932",
          "publishedOn": "2021-05-23T06:08:17.961Z",
          "wordCount": 618,
          "title": "Efficient and Robust LiDAR-Based End-to-End Navigation. (arXiv:2105.09932v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pahwa_R/0/1/0/all/0/1\">Ramanpreet S Pahwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1\">Soon Wee Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ren Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1\">Richard Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_O/0/1/0/all/0/1\">Oo Zaw Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_W/0/1/0/all/0/1\">Wang Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1\">Vempati Srinivasa Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nwe_T/0/1/0/all/0/1\">Tin Lay Nwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanjing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_J/0/1/0/all/0/1\">Jens Timo Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pichumani_R/0/1/0/all/0/1\">Ramani Pichumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregorich_T/0/1/0/all/0/1\">Thomas Gregorich</a>",
          "description": "For over 40 years lithographic silicon scaling has driven circuit integration\nand performance improvement in the semiconductor industry. As silicon scaling\nslows down, the industry is increasingly dependent on IC package technologies\nto contribute to further circuit integration and performance improvements. This\nis a paradigm shift and requires the IC package industry to reduce the size and\nincrease the density of internal interconnects on a scale which has never been\ndone before. Traditional package characterization and process optimization\nrelies on destructive techniques such as physical cross-sections and delayering\nto extract data from internal package features. These destructive techniques\nare not practical with today's advanced packages. In this paper we will\ndemonstrate how data acquired non-destructively with a 3D X-ray microscope can\nbe enhanced and optimized using machine learning, and can then be used to\nmeasure, characterize and optimize the design and production of buried\ninterconnects in advanced IC packages. Test vehicles replicating 2.5D and HBM\nconstruction were designed and fabricated, and digital data was extracted from\nthese test vehicles using 3D X-ray and machine learning techniques. The\nextracted digital data was used to characterize and optimize the design and\nproduction of the interconnects and demonstrates a superior alternative to\ndestructive physical analysis. We report an mAP of 0.96 for 3D object\ndetection, a dice score of 0.92 for 3D segmentation, and an average of 2.1um\nerror for 3D metrology on the test dataset. This paper is the first part of a\nmulti-part report.",
          "link": "http://arxiv.org/abs/2103.04838",
          "publishedOn": "2021-05-23T06:08:17.947Z",
          "wordCount": 754,
          "title": "Machine-learning based methodologies for 3d x-ray measurement, characterization and optimization for buried structures in advanced ic packages. (arXiv:2103.04838v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouhsain_S/0/1/0/all/0/1\">Smail Ait Bouhsain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1\">Saeed Saadatnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>",
          "description": "In order to be globally deployed, autonomous cars must guarantee the safety\nof pedestrians. This is the reason why forecasting pedestrians' intentions\nsufficiently in advance is one of the most critical and challenging tasks for\nautonomous vehicles. This work tries to solve this problem by jointly\npredicting the intention and visual states of pedestrians. In terms of visual\nstates, whereas previous work focused on x-y coordinates, we will also predict\nthe size and indeed the whole bounding box of the pedestrian. The method is a\nrecurrent neural network in a multi-task learning approach. It has one head\nthat predicts the intention of the pedestrian for each one of its future\nposition and another one predicting the visual states of the pedestrian.\nExperiments on the JAAD dataset show the superiority of the performance of our\nmethod compared to previous works for intention prediction. Also, although its\nsimple architecture (more than 2 times faster), the performance of the bounding\nbox prediction is comparable to the ones yielded by much more complex\narchitectures. Our code is available online.",
          "link": "http://arxiv.org/abs/2010.10270",
          "publishedOn": "2021-05-23T06:08:17.928Z",
          "wordCount": 647,
          "title": "Pedestrian Intention Prediction: A Multi-task Perspective. (arXiv:2010.10270v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanli Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden M. Lake</a>",
          "description": "Humans are highly efficient learners, with the ability to grasp the meaning\nof a new concept from just a few examples. Unlike popular computer vision\nsystems, humans can flexibly leverage the compositional structure of the visual\nworld, understanding new concepts as combinations of existing concepts. In the\ncurrent paper, we study how people learn different types of visual\ncompositions, using abstract visual forms with rich relational structure. We\nfind that people can make meaningful compositional generalizations from just a\nfew examples in a variety of scenarios, and we develop a Bayesian program\ninduction model that provides a close fit to the behavioral data. Unlike past\nwork examining special cases of compositionality, our work shows how a single\ncomputational approach can account for many distinct types of compositional\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09848",
          "publishedOn": "2021-05-23T06:08:17.922Z",
          "wordCount": 587,
          "title": "Flexible Compositional Learning of Structured Visual Concepts. (arXiv:2105.09848v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1\">Andrew Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1\">Vicky Kalogeiton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "The objective of this work is person-clustering in videos -- grouping\ncharacters according to their identity. Previous methods focus on the narrower\ntask of face-clustering, and for the most part ignore other cues such as the\nperson's voice, their overall appearance (hair, clothes, posture), and the\nediting structure of the videos. Similarly, most current datasets evaluate only\nthe task of face-clustering, rather than person-clustering. This limits their\napplicability to downstream applications such as story understanding which\nrequire person-level, rather than only face-level, reasoning. In this paper we\nmake contributions to address both these deficiencies: first, we introduce a\nMulti-Modal High-Precision Clustering algorithm for person-clustering in videos\nusing cues from several modalities (face, body, and voice). Second, we\nintroduce a Video Person-Clustering dataset, for evaluating multi-modal\nperson-clustering. It contains body-tracks for each annotated character,\nface-tracks when visible, and voice-tracks when speaking, with their associated\nfeatures. The dataset is by far the largest of its kind, and covers films and\nTV-shows representing a wide range of demographics. Finally, we show the\neffectiveness of using multiple modalities for person-clustering, explore the\nuse of this new broad task for story understanding through character\nco-occurrences, and achieve a new state of the art on all available datasets\nfor face and person-clustering.",
          "link": "http://arxiv.org/abs/2105.09939",
          "publishedOn": "2021-05-23T06:08:17.901Z",
          "wordCount": 633,
          "title": "Face, Body, Voice: Video Person-Clustering with Multiple Modalities. (arXiv:2105.09939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weidler_T/0/1/0/all/0/1\">Tonio Weidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehnen_J/0/1/0/all/0/1\">Julian Lehnen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_Q/0/1/0/all/0/1\">Quinton Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebok_D/0/1/0/all/0/1\">D&#xe1;vid Seb&#x151;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gerhard Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessens_K/0/1/0/all/0/1\">Kurt Driessens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senden_M/0/1/0/all/0/1\">Mario Senden</a>",
          "description": "Lateral connections play an important role for sensory processing in visual\ncortex by supporting discriminable neuronal responses even to highly similar\nfeatures. In the present work, we show that establishing a biologically\ninspired Mexican hat lateral connectivity profile along the filter domain can\nsignificantly improve the classification accuracy of a variety of lightweight\nconvolutional neural networks without the addition of trainable network\nparameters. Moreover, we demonstrate that it is possible to analytically\ndetermine the stationary distribution of modulated filter activations and\nthereby avoid using recurrence for modeling temporal dynamics. We furthermore\nreveal that the Mexican hat connectivity profile has the effect of ordering\nfilters in a sequence resembling the topographic organization of feature\nselectivity in early visual cortex. In an ordered filter sequence, this profile\nthen sharpens the filters' tuning curves.",
          "link": "http://arxiv.org/abs/2105.09830",
          "publishedOn": "2021-05-23T06:08:17.895Z",
          "wordCount": 573,
          "title": "Biologically Inspired Semantic Lateral Connectivity for Convolutional Neural Networks. (arXiv:2105.09830v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipayan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>",
          "description": "Reservoir Computing (RC) offers a viable option to deploy AI algorithms on\nlow-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired\nRC model that mimics the cortical microcircuits and uses spiking neural\nnetworks (SNN) that can be directly realized on neuromorphic hardware. In this\npaper, we present a novel Parallelized LSM (PLSM) architecture that\nincorporates spatio-temporal read-out layer and semantic constraints on model\noutput. To the best of our knowledge, such a formulation has been done for the\nfirst time in literature, and it offers a computationally lighter alternative\nto traditional deep-learning models. Additionally, we also present a\ncomprehensive algorithm for the implementation of parallelizable SNNs and LSMs\nthat are GPU-compatible. We implement the PLSM model to classify\nunintentional/accidental video clips, using the Oops dataset. From the\nexperimental results on detecting unintentional action in video, it can be\nobserved that our proposed model outperforms a self-supervised model and a\nfully supervised traditional deep learning model. All the implemented codes can\nbe found at our repository\nhttps://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.",
          "link": "http://arxiv.org/abs/2105.09909",
          "publishedOn": "2021-05-23T06:08:17.884Z",
          "wordCount": 613,
          "title": "PLSM: A Parallelized Liquid State Machine for Unintentional Action Detection. (arXiv:2105.09909v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1\">Nkechinyere N. Agu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Joy T. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hanqing Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1\">James Hendler</a>",
          "description": "Radiologists usually observe anatomical regions of chest X-ray images as well\nas the overall image before making a decision. However, most existing deep\nlearning models only look at the entire X-ray image for classification, failing\nto utilize important anatomical information. In this paper, we propose a novel\nmulti-label chest X-ray classification model that accurately classifies the\nimage finding and also localizes the findings to their correct anatomical\nregions. Specifically, our model consists of two modules, the detection module\nand the anatomical dependency module. The latter utilizes graph convolutional\nnetworks, which enable our model to learn not only the label dependency but\nalso the relationship between the anatomical regions in the chest X-ray. We\nfurther utilize a method to efficiently create an adjacency matrix for the\nanatomical regions using the correlation of the label across the different\nregions. Detailed experiments and analysis of our results show the\neffectiveness of our method when compared to the current state-of-the-art\nmulti-label chest X-ray image classification methods while also providing\naccurate location information.",
          "link": "http://arxiv.org/abs/2105.09937",
          "publishedOn": "2021-05-23T06:08:17.877Z",
          "wordCount": 619,
          "title": "AnaXNet: Anatomy Aware Multi-label Finding Classification in Chest X-ray. (arXiv:2105.09937v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wangyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from human's visual and intuitive perspective. We take\nthe first step to bridge the gap by proposing a deep learning-based technique\nto automatically classify road networks into four classes on a visual basis.\nThe method is implemented by generating an image of the street network (Colored\nRoad Hierarchy Diagram), which we introduce in this paper, and classifying it\nusing a deep convolutional neural network (ResNet-34). The model achieves an\noverall classification accuracy of 0.875. Nine cities around the world are\nselected as the study areas and their road networks are acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through a\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: the\neffectiveness of our human perception augmentation is examined by a case study\nof urban vitality prediction. An advanced tree-based regression model is for\nthe first time designated to establish the relationship between morphological\nindices and vitality indicators. A positive effect of human perception\naugmentation is detected in the comparative experiment of baseline model and\naugmented model. This work expands the toolkit of quantitative urban morphology\nstudy with new techniques, supporting further studies in the future.",
          "link": "http://arxiv.org/abs/2105.09908",
          "publishedOn": "2021-05-23T06:08:17.864Z",
          "wordCount": 703,
          "title": "Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.01165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ilke Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciftci_U/0/1/0/all/0/1\">Umur A. Ciftci</a>",
          "description": "Following the recent initiatives for the democratization of AI, deep fake\ngenerators have become increasingly popular and accessible, causing dystopian\nscenarios towards social erosion of trust. A particular domain, such as\nbiological signals, attracted attention towards detection methods that are\ncapable of exploiting authenticity signatures in real videos that are not yet\nfaked by generative approaches. In this paper, we first propose several\nprominent eye and gaze features that deep fakes exhibit differently. Second, we\ncompile those features into signatures and analyze and compare those of real\nand fake videos, formulating geometric, visual, metric, temporal, and spectral\nvariations. Third, we generalize this formulation to the deep fake detection\nproblem by a deep neural network, to classify any video in the wild as fake or\nreal. We evaluate our approach on several deep fake datasets, achieving 92.48%\naccuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on\nCelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most\ndeep and biological fake detectors with complex network architectures without\nthe proposed gaze signatures. We conduct ablation studies involving different\nfeatures, architectures, sequence durations, and post-processing artifacts.",
          "link": "http://arxiv.org/abs/2101.01165",
          "publishedOn": "2021-05-23T06:08:17.845Z",
          "wordCount": 660,
          "title": "Where Do Deep Fakes Look? Synthetic Face Detection via Gaze Tracking. (arXiv:2101.01165v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ran Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rujun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhuoling Xiao</a>",
          "description": "The technology for Visual Odometry (VO) that estimates the position and\norientation of the moving object through analyzing the image sequences captured\nby on-board cameras, has been well investigated with the rising interest in\nautonomous driving. This paper studies monocular VO from the perspective of\nDeep Learning (DL). Unlike most current learning-based methods, our approach,\ncalled DeepAVO, is established on the intuition that features contribute\ndiscriminately to different motion patterns. Specifically, we present a novel\nfour-branch network to learn the rotation and translation by leveraging\nConvolutional Neural Networks (CNNs) to focus on different quadrants of optical\nflow input. To enhance the ability of feature selection, we further introduce\nan effective channel-spatial attention mechanism to force each branch to\nexplicitly distill related information for specific Frame to Frame (F2F) motion\nestimation. Experiments on various datasets involving outdoor driving and\nindoor walking scenarios show that the proposed DeepAVO outperforms the\nstate-of-the-art monocular methods by a large margin, demonstrating competitive\nperformance to the stereo VO algorithm and verifying promising potential for\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09899",
          "publishedOn": "2021-05-23T06:08:17.838Z",
          "wordCount": 619,
          "title": "DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry. (arXiv:2105.09899v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.13934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>",
          "description": "Methods to quantify the complexity of trajectory datasets are still a missing\npiece in benchmarking human trajectory prediction models. In order to gain a\nbetter understanding of the complexity of trajectory prediction tasks and\nfollowing the intuition, that more complex datasets contain more information,\nan approach for quantifying the amount of information contained in a dataset\nfrom a prototype-based dataset representation is proposed. The dataset\nrepresentation is obtained by first employing a non-trivial spatial sequence\nalignment, which enables a subsequent learning vector quantization (LVQ) stage.\nA large-scale complexity analysis is conducted on several human trajectory\nprediction benchmarking datasets, followed by a brief discussion on indications\nfor human trajectory prediction and benchmarking.",
          "link": "http://arxiv.org/abs/2005.13934",
          "publishedOn": "2021-05-23T06:08:17.832Z",
          "wordCount": 600,
          "title": "Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction. (arXiv:2005.13934v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.01383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Jan Paul Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangrong Xu</a>",
          "description": "This paper proposes a novel automatically generating image masks method for\nthe state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method\nachieves the best results in object detection until now, however, it is very\ntime-consuming and laborious to get the object Masks for training, the proposed\nmethod is composed by a two-stage design, to automatically generating image\nmasks, the first stage implements a fully convolutional networks (FCN) based\nsegmentation network, the second stage network, a Mask R-CNN based object\ndetection network, which is trained on the object image masks from FCN output,\nthe original input image, and additional label information. Through\nexperimentation, our proposed method can obtain the image masks automatically\nto train Mask R-CNN, and it can achieve very high classification accuracy with\nan over 90% mean of average precision (mAP) for segmentation",
          "link": "http://arxiv.org/abs/2003.01383",
          "publishedOn": "2021-05-23T06:08:17.823Z",
          "wordCount": 603,
          "title": "Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN. (arXiv:2003.01383v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.02161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_N/0/1/0/all/0/1\">Nivedita Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Automated Parking is becoming a standard feature in modern vehicles. Existing\nparking systems build a local map to be able to plan for maneuvering towards a\ndetected slot. Next generation parking systems have an use case where they\nbuild a persistent map of the environment where the car is frequently parked,\nsay for example, home parking or office parking. The pre-built map helps in\nre-localizing the vehicle better when its trying to park the next time. This is\nachieved by augmenting the parking system with a Visual SLAM pipeline and the\nfeature is called trained trajectory parking in the automotive industry. In\nthis paper, we discuss the use cases, design and implementation of a trained\ntrajectory automated parking system. The proposed system is deployed on\ncommercial vehicles and the consumer application is illustrated in\n\\url{https://youtu.be/nRWF5KhyJZU}. The focus of this paper is on the\napplication and the details of vision algorithms are kept at high level.",
          "link": "http://arxiv.org/abs/2001.02161",
          "publishedOn": "2021-05-23T06:08:17.817Z",
          "wordCount": 642,
          "title": "Trained Trajectory based Automated Parking System using Visual SLAM on Surround View Cameras. (arXiv:2001.02161v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.13693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pares_F/0/1/0/all/0/1\">Ferran Par&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_Duart_A/0/1/0/all/0/1\">Anna Arias-Duart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1\">Dario Garcia-Gasulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campo_Frances_G/0/1/0/all/0/1\">Gema Campo-Franc&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viladrich_N/0/1/0/all/0/1\">Nina Viladrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1\">Eduard Ayguad&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1\">Jes&#xfa;s Labarta</a>",
          "description": "In the image classification task, the most common approach is to resize all\nimages in a dataset to a unique shape, while reducing their precision to a size\nwhich facilitates experimentation at scale. This practice has benefits from a\ncomputational perspective, but it entails negative side-effects on performance\ndue to loss of information and image deformation. In this work we introduce the\nMAMe dataset, an image classification dataset with remarkable high resolution\nand variable shape properties. The goal of MAMe is to provide a tool for\nstudying the impact of such properties in image classification, while\nmotivating research in the field. The MAMe dataset contains thousands of\nartworks from three different museums, and proposes a classification task\nconsisting on differentiating between 29 mediums (i.e. materials and\ntechniques) supervised by art experts. After reviewing the singularity of MAMe\nin the context of current image classification tasks, a thorough description of\nthe task is provided, together with dataset statistics. Experiments are\nconducted to evaluate the impact of using high resolution images, variable\nshape inputs and both properties at the same time. Results illustrate the\npositive impact in performance when using high resolution images, while\nhighlighting the lack of solutions to exploit variable shapes. An additional\nexperiment exposes the distinctiveness between the MAMe dataset and the\nprototypical ImageNet dataset. Finally, the baselines are inspected using\nexplainability methods and expert knowledge, to gain insights on the challenges\nthat remain ahead.",
          "link": "http://arxiv.org/abs/2007.13693",
          "publishedOn": "2021-05-23T06:08:17.797Z",
          "wordCount": 725,
          "title": "The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties. (arXiv:2007.13693v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Thai_B/0/1/0/all/0/1\">Binh Nguyen-Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1\">Catherine Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawi_N/0/1/0/all/0/1\">Nadia Badawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "The absence or abnormality of fidgety movements of joints or limbs is\nstrongly indicative of cerebral palsy in infants. Developing computer-based\nmethods for assessing infant movements in videos is pivotal for improved\ncerebral palsy screening. Most existing methods use appearance-based features\nand are thus sensitive to strong but irrelevant signals caused by background\nclutter or a moving camera. Moreover, these features are computed over the\nwhole frame, thus they measure gross whole body movements rather than specific\njoint/limb motion.\n\nAddressing these challenges, we develop and validate a new method for fidgety\nmovement assessment from consumer-grade videos using human poses extracted from\nshort clips. Human poses capture only relevant motion profiles of joints and\nlimbs and are thus free from irrelevant appearance artifacts. The dynamics and\ncoordination between joints are modeled using spatio-temporal graph\nconvolutional networks. Frames and body parts that contain discriminative\ninformation about fidgety movements are selected through a spatio-temporal\nattention mechanism. We validate the proposed model on the cerebral palsy\nscreening task using a real-life consumer-grade video dataset collected at an\nAustralian hospital through the Cerebral Palsy Alliance, Australia. Our\nexperiments show that the proposed method achieves the ROC-AUC score of 81.87%,\nsignificantly outperforming existing competing methods with better\ninterpretability.",
          "link": "http://arxiv.org/abs/2105.09783",
          "publishedOn": "2021-05-23T06:08:17.790Z",
          "wordCount": 657,
          "title": "A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos. (arXiv:2105.09783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xiaoguang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiankun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1\">Wenjie Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "In real-world scenarios, many factors may harm face recognition performance,\ne.g., large pose, bad illumination,low resolution, blur and noise. To address\nthese challenges, previous efforts usually first restore the low-quality faces\nto high-quality ones and then perform face recognition. However, most of these\nmethods are stage-wise, which is sub-optimal and deviates from the reality. In\nthis paper, we address all these challenges jointly for unconstrained face\nrecognition. We propose an Multi-Degradation Face Restoration (MDFR) model to\nrestore frontalized high-quality faces from the given low-quality ones under\narbitrary facial poses, with three distinct novelties. First, MDFR is a\nwell-designed encoder-decoder architecture which extracts feature\nrepresentation from an input face image with arbitrary low-quality factors and\nrestores it to a high-quality counterpart. Second, MDFR introduces a pose\nresidual learning strategy along with a 3D-based Pose Normalization Module\n(PNM), which can perceive the pose gap between the input initial pose and its\nreal-frontal pose to guide the face frontalization. Finally, MDFR can generate\nfrontalized high-quality face images by a single unified network, showing a\nstrong capability of preserving face identity. Qualitative and quantitative\nexperiments on both controlled and in-the-wild benchmarks demonstrate the\nsuperiority of MDFR over state-of-the-art methods on both face frontalization\nand face restoration.",
          "link": "http://arxiv.org/abs/2105.09907",
          "publishedOn": "2021-05-23T06:08:17.782Z",
          "wordCount": 647,
          "title": "Joint Face Image Restoration and Frontalization for Recognition. (arXiv:2105.09907v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clever_H/0/1/0/all/0/1\">Henry M. Clever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grady_P/0/1/0/all/0/1\">Patrick Grady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turk_G/0/1/0/all/0/1\">Greg Turk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1\">Charles C. Kemp</a>",
          "description": "Contact pressure between the human body and its surroundings has important\nimplications. For example, it plays a role in comfort, safety, posture, and\nhealth. We present a method that infers contact pressure between a human body\nand a mattress from a depth image. Specifically, we focus on using a depth\nimage from a downward facing camera to infer pressure on a body at rest in bed\noccluded by bedding, which is directly applicable to the prevention of pressure\ninjuries in healthcare. Our approach involves augmenting a real dataset with\nsynthetic data generated via a soft-body physics simulation of a human body, a\nmattress, a pressure sensing mat, and a blanket. We introduce a novel deep\nnetwork that we trained on an augmented dataset and evaluated with real data.\nThe network contains an embedded human body mesh model and uses a white-box\nmodel of depth and pressure image generation. Our network successfully infers\nbody pose, outperforming prior work. It also infers contact pressure across a\n3D mesh model of the human body, which is a novel capability, and does so in\nthe presence of occlusion from blankets.",
          "link": "http://arxiv.org/abs/2105.09936",
          "publishedOn": "2021-05-23T06:08:17.774Z",
          "wordCount": 633,
          "title": "BodyPressure -- Inferring Body Pose and Contact Pressure from a Depth Image. (arXiv:2105.09936v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.08797",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1\">Soufiane Hayou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1\">Jean-Francois Ton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Overparameterized Neural Networks (NN) display state-of-the-art performance.\nHowever, there is a growing need for smaller, energy-efficient, neural networks\ntobe able to use machine learning applications on devices with limited\ncomputational resources. A popular approach consists of using pruning\ntechniques. While these techniques have traditionally focused on pruning\npre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et\nal. (2018) has shown promising results when pruning at initialization. However,\nfor Deep NNs, such procedures remain unsatisfactory as the resulting pruned\nnetworks can be difficult to train and, for instance, they do not prevent one\nlayer from being fully pruned. In this paper, we provide a comprehensive\ntheoretical analysis of Magnitude and Gradient based pruning at initialization\nand training of sparse architectures. This allows us to propose novel\nprincipled approaches which we validate experimentally on a variety of NN\narchitectures.",
          "link": "http://arxiv.org/abs/2002.08797",
          "publishedOn": "2021-05-23T06:08:17.767Z",
          "wordCount": 619,
          "title": "Robust Pruning at Initialization. (arXiv:2002.08797v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>",
          "description": "Learned networks in the domain of visual recognition and cognition impress in\npart because even though they are trained with datasets many orders of\nmagnitude smaller than the full population of possible images, they exhibit\nsufficient generalization to be applicable to new and previously unseen data.\nAlthough many have examined issues regarding generalization from several\nperspectives, we wondered If a network is trained with a biased dataset that\nmisses particular samples corresponding to some defining domain attribute, can\nit generalize to the full domain from which that training dataset was\nextracted? It is certainly true that in vision, no current training set fully\ncaptures all visual information and this may lead to Selection Bias. Here, we\ntry a novel approach in the tradition of the Thought Experiment. We run this\nthought experiment on a real domain of visual objects that we can fully\ncharacterize and look at specific gaps in training data and their impact on\nperformance requirements. Our thought experiment points to three conclusions:\nfirst, that generalization behavior is dependent on how sufficiently the\nparticular dimensions of the domain are represented during training; second,\nthat the utility of any generalization is completely dependent on the\nacceptable system error; and third, that specific visual features of objects,\nsuch as pose orientations out of the imaging plane or colours, may not be\nrecoverable if not represented sufficiently in a training set. Any currently\nobserved generalization in modern deep learning networks may be more the result\nof coincidental alignments and whose utility needs to be confirmed with respect\nto a system's performance specification. Our Thought Experiment Probe approach,\ncoupled with the resulting Bias Breakdown can be very informative towards\nunderstanding the impact of biases.",
          "link": "http://arxiv.org/abs/2105.09934",
          "publishedOn": "2021-05-23T06:08:17.749Z",
          "wordCount": 730,
          "title": "Probing the Effect of Selection Bias on NN Generalization with a Thought Experiment. (arXiv:2105.09934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1\">Kasra Arnavaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1\">Jelena M. Krivokapic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1\">Silja Heilmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1\">Jakob Andreas B&#xe6;rentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1\">Pia Nyeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>",
          "description": "Motivated by a challenging tubular network segmentation task, this paper\ntackles two commonly encountered problems in biomedical imaging: Topological\nconsistency of the segmentation, and limited annotations. We propose a\ntopological score which measures both topological and geometric consistency\nbetween the predicted and ground truth segmentations, applied for model\nselection and validation. We apply our topological score in three scenarios: i.\na U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised\nU-net architecture, which offers a straightforward approach to jointly training\nthe network both as an autoencoder and a segmentation algorithm. This allows us\nto utilize un-annotated data for training a representation that generalizes\nacross test data variability, in spite of our annotated training data having\nvery limited variation. Our contributions are validated on a challenging\nsegmentation task, locating tubular structures in the fetal pancreas from noisy\nlive imaging confocal microscopy.",
          "link": "http://arxiv.org/abs/2105.09737",
          "publishedOn": "2021-05-23T06:08:17.742Z",
          "wordCount": 590,
          "title": "Semi-supervised, Topology-Aware Segmentation of Tubular Structures from Live Imaging 3D Microscopy. (arXiv:2105.09737v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1\">Rakshit Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1\">Umar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seonwook Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>",
          "description": "A major challenge for physically unconstrained gaze estimation is acquiring\ntraining data with 3D gaze annotations for in-the-wild and outdoor scenarios.\nIn contrast, videos of human interactions in unconstrained environments are\nabundantly available and can be much more easily annotated with frame-level\nactivity labels. In this work, we tackle the previously unexplored problem of\nweakly-supervised gaze estimation from videos of human interactions. We\nleverage the insight that strong gaze-related geometric constraints exist when\npeople perform the activity of \"looking at each other\" (LAEO). To acquire\nviable 3D gaze supervision from LAEO labels, we propose a training algorithm\nalong with several novel loss functions especially designed for the task. With\nweak supervision from two large scale CMU-Panoptic and AVA-LAEO activity\ndatasets, we show significant improvements in (a) the accuracy of\nsemi-supervised gaze estimation and (b) cross-domain generalization on the\nstate-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation\nbenchmark. We open source our code at\nhttps://github.com/NVlabs/weakly-supervised-gaze.",
          "link": "http://arxiv.org/abs/2105.09803",
          "publishedOn": "2021-05-23T06:08:17.735Z",
          "wordCount": 588,
          "title": "Weakly-Supervised Physically Unconstrained Gaze Estimation. (arXiv:2105.09803v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1\">Manav Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1\">Peter Jakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1\">Tobias Schmid-Schirling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Multi-view classification is inspired by the behavior of humans, especially\nwhen fine-grained features or in our case rarely occurring anomalies are to be\ndetected. Current contributions point to the problem of how high-dimensional\ndata can be fused. In this work, we build upon the deep support vector data\ndescription algorithm and address multi-perspective anomaly detection using\nthree different fusion techniques i.e. early fusion, late fusion, and late\nfusion with multiple decoders. We employ different augmentation techniques with\na denoising process to deal with scarce one-class data, which further improves\nthe performance (ROC AUC = 80\\%). Furthermore, we introduce the dices dataset\nthat consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g. drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed approach\nexceeds the state-of-the-art on both the MNIST and dices datasets. To the best\nof our knowledge, this is the first work that focuses on addressing\nmulti-perspective anomaly detection in images by jointly using different\nperspectives together with one single objective function for anomaly detection.",
          "link": "http://arxiv.org/abs/2105.09903",
          "publishedOn": "2021-05-23T06:08:17.679Z",
          "wordCount": 633,
          "title": "Multi-Perspective Anomaly Detection. (arXiv:2105.09903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fonder_M/0/1/0/all/0/1\">Micha&#xeb;l Fonder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>",
          "description": "Getting the distance to objects is crucial for autonomous vehicles. In\ninstances where depth sensors cannot be used, this distance has to be estimated\nfrom RGB cameras. As opposed to cars, the task of estimating depth from\non-board mounted cameras is made complex on drones because of the lack of\nconstrains on motion during flights. %In the case of drones, this task is even\nmore complex than for car-mounted cameras since the camera motion is\nunconstrained. In this paper, we present a method to estimate the distance of\nobjects seen by an on-board mounted camera by using its RGB video stream and\ndrone motion information. Our method is built upon a pyramidal convolutional\nneural network architecture and uses time recurrence in pair with geometric\nconstraints imposed by motion to produce pixel-wise depth maps. %from a RGB\nvideo stream of a camera attached to the drone In our architecture, each level\nof the pyramid is designed to produce its own depth estimate based on past\nobservations and information provided by the previous level in the pyramid. We\nintroduce a spatial reprojection layer to maintain the spatio-temporal\nconsistency of the data between the levels. We analyse the performance of our\napproach on Mid-Air, a public drone dataset featuring synthetic drone\ntrajectories recorded in a wide variety of unstructured outdoor environments.\nOur experiments show that our network outperforms state-of-the-art depth\nestimation methods and that the use of motion information is the main\ncontributing factor for this improvement. The code of our method is publicly\navailable on GitHub; see\n$\\href{https://github.com/michael-fonder/M4Depth}{\\text{https://github.com/michael-fonder/M4Depth}}$",
          "link": "http://arxiv.org/abs/2105.09847",
          "publishedOn": "2021-05-23T06:08:17.646Z",
          "wordCount": 705,
          "title": "M4Depth: A motion-based approach for monocular depth estimation on video sequences. (arXiv:2105.09847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09913",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1\">Shehan Perera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adhikari_S/0/1/0/all/0/1\">Srikar Adhikari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>",
          "description": "The rapid and seemingly endless expansion of COVID-19 can be traced back to\nthe inefficiency and shortage of testing kits that offer accurate results in a\ntimely manner. An emerging popular technique, which adopts improvements made in\nmobile ultrasound technology, allows for healthcare professionals to conduct\nrapid screenings on a large scale. We present an image-based solution that aims\nat automating the testing process which allows for rapid mass testing to be\nconducted with or without a trained medical professional that can be applied to\nrural environments and third world countries. Our contributions towards rapid\nlarge-scale testing include a novel deep learning architecture capable of\nanalyzing ultrasound data that can run in real-time and significantly improve\nthe current state-of-the-art detection accuracies using image-based COVID-19\ndetection.",
          "link": "http://arxiv.org/abs/2105.09913",
          "publishedOn": "2021-05-23T06:08:17.622Z",
          "wordCount": 617,
          "title": "POCFormer: A Lightweight Transformer Architecture for Detection of COVID-19 Using Point of Care Ultrasound. (arXiv:2105.09913v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McNally_W/0/1/0/all/0/1\">William McNally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walters_P/0/1/0/all/0/1\">Pascale Walters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McPhee_J/0/1/0/all/0/1\">John McPhee</a>",
          "description": "Existing multi-camera solutions for automatic scorekeeping in steel-tip darts\nare very expensive and thus inaccessible to most players. Motivated to develop\na more accessible low-cost solution, we present a new approach to keypoint\ndetection and apply it to predict dart scores from a single image taken from\nany camera angle. This problem involves detecting multiple keypoints that may\nbe of the same class and positioned in close proximity to one another. The\nwidely adopted framework for regressing keypoints using heatmaps is not\nwell-suited for this task. To address this issue, we instead propose to model\nkeypoints as objects. We develop a deep convolutional neural network around\nthis idea and use it to predict dart locations and dartboard calibration points\nwithin an overall pipeline for automatic dart scoring, which we call DeepDarts.\nAdditionally, we propose several task-specific data augmentation strategies to\nimprove the generalization of our method. As a proof of concept, two datasets\ncomprising 16k images originating from two different dartboard setups were\nmanually collected and annotated to evaluate the system. In the primary dataset\ncontaining 15k images captured from a face-on view of the dartboard using a\nsmartphone, DeepDarts predicted the total score correctly in 94.7% of the test\nimages. In a second more challenging dataset containing limited training data\n(830 images) and various camera angles, we utilize transfer learning and\nextensive data augmentation to achieve a test accuracy of 84.0%. Because\nDeepDarts relies only on single images, it has the potential to be deployed on\nedge devices, giving anyone with a smartphone access to an automatic dart\nscoring system for steel-tip darts. The code and datasets are available.",
          "link": "http://arxiv.org/abs/2105.09880",
          "publishedOn": "2021-05-23T06:08:17.580Z",
          "wordCount": 718,
          "title": "DeepDarts: Modeling Keypoints as Objects for Automatic Scorekeeping in Darts using a Single Camera. (arXiv:2105.09880v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borkar_J/0/1/0/all/0/1\">Jaydeep Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "There has been a rise in the use of Machine Learning as a Service (MLaaS)\nVision APIs as they offer multiple services including pre-built models and\nalgorithms, which otherwise take a huge amount of resources if built from\nscratch. As these APIs get deployed for high-stakes applications, it's very\nimportant that they are robust to different manipulations. Recent works have\nonly focused on typical adversarial attacks when evaluating the robustness of\nvision APIs. We propose two new aspects of adversarial image generation methods\nand evaluate them on the robustness of Google Cloud Vision API's optical\ncharacter recognition service and object detection APIs deployed in real-world\nsettings such as sightengine.com, picpurify.com, Google Cloud Vision API, and\nMicrosoft Azure's Computer Vision API. Specifically, we go beyond the\nconventional small-noise adversarial attacks and introduce secret embedding and\ntransparent adversarial examples as a simpler way to evaluate robustness. These\nmethods are so straightforward that even non-specialists can craft such\nattacks. As a result, they pose a serious threat where APIs are used for\nhigh-stakes applications. Our transparent adversarial examples successfully\nevade state-of-the art object detections APIs such as Azure Cloud Vision\n(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).\n90% of the images have a secret embedded text that successfully fools the\nvision of time-limited humans but is detected by Google Cloud Vision API's\noptical character recognition. Complementing to current research, our results\nprovide simple but unconventional methods on robustness evaluation.",
          "link": "http://arxiv.org/abs/2105.09685",
          "publishedOn": "2021-05-23T06:08:17.573Z",
          "wordCount": 694,
          "title": "Simple Transparent Adversarial Examples. (arXiv:2105.09685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xianzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jianyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuting He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "This paper introduces our solution for the Track2 in AI City Challenge 2021\n(AICITY21). The Track2 is a vehicle re-identification (ReID) task with both the\nreal-world data and synthetic data. We mainly focus on four points, i.e.\ntraining data, unsupervised domain-adaptive (UDA) training, post-processing,\nmodel ensembling in this challenge. (1) Both cropping training data and using\nsynthetic data can help the model learn more discriminative features. (2) Since\nthere is a new scenario in the test set that dose not appear in the training\nset, UDA methods perform well in the challenge. (3) Post-processing techniques\nincluding re-ranking, image-to-track retrieval, inter-camera fusion, etc,\nsignificantly improve final performance. (4) We ensemble CNN-based models and\ntransformer-based models which provide different representation diversity. With\naforementioned techniques, our method finally achieves 0.7445 mAP score,\nyielding the first place in the competition. Codes are available at\nhttps://github.com/michuanhaohao/AICITY2021_Track2_DMT.",
          "link": "http://arxiv.org/abs/2105.09701",
          "publishedOn": "2021-05-23T06:08:17.562Z",
          "wordCount": 610,
          "title": "An Empirical Study of Vehicle Re-Identification on the AI City Challenge. (arXiv:2105.09701v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yukai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>",
          "description": "Deep convolutional networks have attracted great attention in image\nrestoration and enhancement. Generally, restoration quality has been improved\nby building more and more convolutional block. However, these methods mostly\nlearn a specific model to handle all images and ignore difficulty diversity. In\nother words, an area in the image with high frequency tend to lose more\ninformation during compressing while an area with low frequency tends to lose\nless. In this article, we adrress the efficiency issue in image SR by\nincorporating a patch-wise rolling network(PRN) to content-adaptively recover\nimages according to difficulty levels. In contrast to existing studies that\nignore difficulty diversity, we adopt different stage of a neural network to\nperform image restoration. In addition, we propose a rolling strategy that\nutilizes the parameters of each stage more flexible. Extensive experiments\ndemonstrate that our model not only shows a significant acceleration but also\nmaintain state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2105.09645",
          "publishedOn": "2021-05-23T06:08:17.554Z",
          "wordCount": 572,
          "title": "Content-adaptive Representation Learning for Fast Image Super-resolution. (arXiv:2105.09645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09511",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shaohua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sui_X/0/1/0/all/0/1\">Xiuchao Sui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xinxing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>",
          "description": "Medical image segmentation is important for computer-aided diagnosis. Good\nsegmentation demands the model to see the big picture and fine details\nsimultaneously, i.e., to learn image features that incorporate large context\nwhile keep high spatial resolutions. To approach this goal, the most widely\nused methods -- U-Net and variants, extract and fuse multi-scale features.\nHowever, the fused features still have small \"effective receptive fields\" with\na focus on local image cues, limiting their performance. In this work, we\npropose Segtran, an alternative segmentation framework based on transformers,\nwhich have unlimited \"effective receptive fields\" even at high feature\nresolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer:\na squeezed attention block regularizes the self attention of transformers, and\nan expansion block learns diversified representations. Additionally, we propose\na new positional encoding scheme for transformers, imposing a continuity\ninductive bias for images. Experiments were performed on 2D and 3D medical\nimage segmentation tasks: optic disc/cup segmentation in fundus images\n(REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain\ntumor segmentation in MRI scans (BraTS'19 challenge). Compared with\nrepresentative existing methods, Segtran consistently achieved the highest\nsegmentation accuracy, and exhibited good cross-domain generalization\ncapabilities.",
          "link": "http://arxiv.org/abs/2105.09511",
          "publishedOn": "2021-05-23T06:08:17.533Z",
          "wordCount": 638,
          "title": "Medical Image Segmentation using Squeeze-and-Expansion Transformers. (arXiv:2105.09511v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09720",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mudiyanselage_T/0/1/0/all/0/1\">Thosini Bamunu Mudiyanselage</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Senanayake_N/0/1/0/all/0/1\">Nipuna Senanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1\">Chunyan Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yi Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanqing Zhang</a>",
          "description": "The novel corona virus (Covid-19) has introduced significant challenges due\nto its rapid spreading nature through respiratory transmission. As a result,\nthere is a huge demand for Artificial Intelligence (AI) based quick disease\ndiagnosis methods as an alternative to high demand tests such as Polymerase\nChain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective\nradiography technique due to resource availability and quick screening. But, a\nsufficient and systematic data collection that is required by complex deep\nleaning (DL) models is more difficult and hence there are recent efforts that\nutilize transfer learning to address this issue. Still these transfer learnt\nmodels suffer from lack of generalization and increased bias to the training\ndataset resulting poor performance for unseen data. Limited correlation of the\ntransferred features from the pre-trained model to a specific medical imaging\ndomain like X-ray and overfitting on fewer data can be reasons for this\ncircumstance. In this work, we propose a novel Graph Convolution Neural Network\n(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR\nimages and meta information about patients. The proposed method exploits\nimportant relational knowledge between data instances and their features using\ngraph representation and applies convolution to learn the graph data which is\nnot possible with conventional convolution on Euclidean domain. The results of\nextensive experiments of proposed model on binary (Covid vs normal) and three\nclass (Covid, normal, other pneumonia) classification problems outperform\ndifferent benchmark transfer learnt models, hence overcoming the aforementioned\ndrawbacks.",
          "link": "http://arxiv.org/abs/2105.09720",
          "publishedOn": "2021-05-23T06:08:17.526Z",
          "wordCount": 747,
          "title": "Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks. (arXiv:2105.09720v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09683",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_R/0/1/0/all/0/1\">Ruhui Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Laili Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>",
          "description": "Background and Objective: The new type of coronavirus is also called\nCOVID-19. It began to spread at the end of 2019 and has now spread across the\nworld. Until October 2020, It has infected around 37 million people and claimed\nabout 1 million lives. We propose a deep learning model that can help\nradiologists and clinicians use chest X-rays to diagnose COVID-19 cases and\nshow the diagnostic features of pneumonia. Methods: The approach in this study\nis: 1) we propose a data enhancement method to increase the diversity of the\ndata set, thereby improving the generalization performance of the model. 2) Our\ndeep convolution neural network model DPN-SE adds a self-attention mechanism to\nthe DPN network. The addition of a self-attention mechanism has greatly\nimproved the performance of the network. 3) Use the Lime interpretable library\nto mark the feature regions on the X-ray medical image that helps doctors more\nquickly diagnose COVID-19 in people. Results: Under the same network model, the\ndata with and without data enhancement is put into the model for training\nrespectively. At last, comparing two experimental results: among the 10 network\nmodels with different structures, 7 network models have improved their effects\nafter using data enhancement, with an average improvement of 1% in recognition\naccuracy. We propose that the accuracy and recall rates of the DPN-SE network\nare 93% and 98% of cases (COVID vs. pneumonia bacteria vs. viral pneumonia vs.\nnormal). Compared with the original DPN, the respective accuracy is improved by\n2%. Conclusion: The data augmentation method we used has achieved effective\nresults on a small amount of data set, showing that a reasonable data\naugmentation method can improve the recognition accuracy without changing the\nsample size and model structure. Overall, the proposed method and model can\neffectively become a very useful tool for clinical radiologists.",
          "link": "http://arxiv.org/abs/2105.09683",
          "publishedOn": "2021-05-23T06:08:17.519Z",
          "wordCount": 807,
          "title": "DPN-SENet:A self-attention mechanism neural network for detection and diagnosis of COVID-19 from chest x-ray images. (arXiv:2105.09683v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09600",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1\">Zhihao Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>",
          "description": "Learning based video compression attracts increasing attention in the past\nfew years. The previous hybrid coding approaches rely on pixel space operations\nto reduce spatial and temporal redundancy, which may suffer from inaccurate\nmotion estimation or less effective motion compensation. In this work, we\npropose a feature-space video coding network (FVC) by performing all major\noperations (i.e., motion estimation, motion compression, motion compensation\nand residual compression) in the feature space. Specifically, in the proposed\ndeformable compensation module, we first apply motion estimation in the feature\nspace to produce motion information (i.e., the offset maps), which will be\ncompressed by using the auto-encoder style network. Then we perform motion\ncompensation by using deformable convolution and generate the predicted\nfeature. After that, we compress the residual feature between the feature from\nthe current frame and the predicted feature from our deformable compensation\nmodule. For better frame reconstruction, the reference features from multiple\nprevious reconstructed frames are also fused by using the non-local attention\nmechanism in the multi-frame feature fusion module. Comprehensive experimental\nresults demonstrate that the proposed framework achieves the state-of-the-art\nperformance on four benchmark datasets including HEVC, UVG, VTL and MCL-JCV.",
          "link": "http://arxiv.org/abs/2105.09600",
          "publishedOn": "2021-05-23T06:08:17.511Z",
          "wordCount": 635,
          "title": "FVC: A New Framework towards Deep Video Compression in Feature Space. (arXiv:2105.09600v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kowalczyk_M/0/1/0/all/0/1\">Marcin Kowalczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1\">Tomasz Kryjak</a>",
          "description": "This work describes the hardware implementation of a connected component\nlabelling (CCL) module in reprogammable logic. The main novelty of the design\nis the \"full\", i.e. without any simplifications, support of a 4 pixel per clock\nformat (4 ppc) and real-time processing of a 4K/UltraHD video stream (3840 x\n2160 pixels) at 60 frames per second. To achieve this, a special labelling\nmethod was designed and a functionality that stops the input data stream in\norder to process pixel groups which require writing more than one merger into\nthe equivalence table. The proposed module was verified in simulation and in\nhardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the ZCU104 evaluation\nboard.",
          "link": "http://arxiv.org/abs/2105.09658",
          "publishedOn": "2021-05-23T06:08:17.504Z",
          "wordCount": 559,
          "title": "A Connected Component Labelling algorithm for multi-pixel per clock cycle video strea. (arXiv:2105.09658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09624",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1\">Janek Gr&#xf6;hl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1\">Melanie Schellenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1\">Kris Dreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1\">Niklas Holzwarth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>",
          "description": "Photoacoustic imaging has the potential to revolutionise healthcare due to\nthe valuable information on tissue physiology that is contained in\nmultispectral photoacoustic measurements. Clinical translation of the\ntechnology requires conversion of the high-dimensional acquired data into\nclinically relevant and interpretable information. In this work, we present a\ndeep learning-based approach to semantic segmentation of multispectral\nphotoacoustic images to facilitate the interpretability of recorded images.\nManually annotated multispectral photoacoustic imaging data are used as gold\nstandard reference annotations and enable the training of a deep learning-based\nsegmentation algorithm in a supervised manner. Based on a validation study with\nexperimentally acquired data of healthy human volunteers, we show that\nautomatic tissue segmentation can be used to create powerful analyses and\nvisualisations of multispectral photoacoustic images. Due to the intuitive\nrepresentation of high-dimensional information, such a processing algorithm\ncould be a valuable means to facilitate the clinical translation of\nphotoacoustic imaging.",
          "link": "http://arxiv.org/abs/2105.09624",
          "publishedOn": "2021-05-23T06:08:17.485Z",
          "wordCount": 608,
          "title": "Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barakat_B/0/1/0/all/0/1\">Berat Kurar Barakat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droby_A/0/1/0/all/0/1\">Ahmad Droby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saabni_R/0/1/0/all/0/1\">Raid Saabni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sana_J/0/1/0/all/0/1\">Jihad El-Sana</a>",
          "description": "Despite recent advances in the field of supervised deep learning for text\nline segmentation, unsupervised deep learning solutions are beginning to gain\npopularity. In this paper, we present an unsupervised deep learning method that\nembeds document image patches to a compact Euclidean space where distances\ncorrespond to a coarse text line pattern similarity. Once this space has been\nproduced, text line segmentation can be easily implemented using standard\ntechniques with the embedded feature vectors. To train the model, we extract\nrandom pairs of document image patches with the assumption that neighbour\npatches contain a similar coarse trend of text lines, whereas if one of them is\nrotated, they contain different coarse trends of text lines. Doing well on this\ntask requires the model to learn to recognize the text lines and their salient\nparts. The benefit of our approach is zero manual labelling effort. We evaluate\nthe method qualitatively and quantitatively on several variants of text line\nsegmentation datasets to demonstrate its effectivity.",
          "link": "http://arxiv.org/abs/2105.09405",
          "publishedOn": "2021-05-23T06:08:17.479Z",
          "wordCount": 596,
          "title": "Unsupervised learning of text line segmentationby differentiating coarse patterns. (arXiv:2105.09405v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shijie Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tong Lin</a>",
          "description": "Recently, collaborative learning proposed by Song and Chai has achieved\nremarkable improvements in image classification tasks by simultaneously\ntraining multiple classifier heads. However, huge memory footprints required by\nsuch multi-head structures may hinder the training of large-capacity baseline\nmodels. The natural question is how to achieve collaborative learning within a\nsingle network without duplicating any modules. In this paper, we propose four\nways of collaborative learning among different parts of a single network with\nnegligible engineering efforts. To improve the robustness of the network, we\nleverage the consistency of the output layer and intermediate layers for\ntraining under the collaborative learning framework. Besides, the similarity of\nintermediate representation and convolution kernel is also introduced to reduce\nthe reduce redundant in a neural network. Compared to the method of Song and\nChai, our framework further considers the collaboration inside a single model\nand takes smaller overhead. Extensive experiments on Cifar-10, Cifar-100,\nImageNet32 and STL-10 corroborate the effectiveness of these four ways\nseparately while combining them leads to further improvements. In particular,\ntest errors on the STL-10 dataset are decreased by $9.28\\%$ and $5.45\\%$ for\nResNet-18 and VGG-16 respectively. Moreover, our method is proven to be robust\nto label noise with experiments on Cifar-10 dataset. For example, our method\nhas $3.53\\%$ higher performance under $50\\%$ noise ratio setting.",
          "link": "http://arxiv.org/abs/2105.09590",
          "publishedOn": "2021-05-23T06:08:17.472Z",
          "wordCount": 638,
          "title": "Intra-Model Collaborative Learning of Neural Networks. (arXiv:2105.09590v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhibo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuchen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>",
          "description": "Recently few-shot object detection is widely adopted to deal with\ndata-limited situations. While most previous works merely focus on the\nperformance on few-shot categories, we claim that detecting all classes is\ncrucial as test samples may contain any instances in realistic applications,\nwhich requires the few-shot detector to learn new concepts without forgetting.\nThrough analysis on transfer learning based methods, some neglected but\nbeneficial properties are utilized to design a simple yet effective few-shot\ndetector, Retentive R-CNN. It consists of Bias-Balanced RPN to debias the\npretrained RPN and Re-detector to find few-shot class objects without\nforgetting previous knowledge. Extensive experiments on few-shot detection\nbenchmarks show that Retentive R-CNN significantly outperforms state-of-the-art\nmethods on overall performance among all settings as it can achieve competitive\nresults on few-shot classes and does not degrade the base class performance at\nall. Our approach has demonstrated that the long desired never-forgetting\nlearner is available in object detection.",
          "link": "http://arxiv.org/abs/2105.09491",
          "publishedOn": "2021-05-23T06:08:17.466Z",
          "wordCount": 583,
          "title": "Generalized Few-Shot Object Detection without Forgetting. (arXiv:2105.09491v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Heming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Object goal navigation aims to steer an agent towards a target object based\non observations of the agent. It is of pivotal importance to design effective\nvisual representations of the observed scene in determining navigation actions.\nIn this paper, we introduce a Visual Transformer Network (VTNet) for learning\ninformative visual representation in navigation. VTNet is a highly effective\nstructure that embodies two key properties for visual representations: First,\nthe relationships among all the object instances in a scene are exploited;\nSecond, the spatial locations of objects and image regions are emphasized so\nthat directional navigation signals can be learned. Furthermore, we also\ndevelop a pre-training scheme to associate the visual representations with\nnavigation signals, and thus facilitate navigation policy learning. In a\nnutshell, VTNet embeds object and region features with their location cues as\nspatial-aware descriptors and then incorporates all the encoded descriptors\nthrough attention operations to achieve informative representation for\nnavigation. Given such visual representations, agents are able to explore the\ncorrelations between visual observations and navigation actions. For example,\nan agent would prioritize \"turning right\" over \"turning left\" when the visual\nrepresentation emphasizes on the right side of activation map. Experiments in\nthe artificial environment AI2-Thor demonstrate that VTNet significantly\noutperforms state-of-the-art methods in unseen testing environments.",
          "link": "http://arxiv.org/abs/2105.09447",
          "publishedOn": "2021-05-23T06:08:17.458Z",
          "wordCount": 642,
          "title": "VTNet: Visual Transformer Network for Object Goal Navigation. (arXiv:2105.09447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoyue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Song Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">S.-H. Gary Chan</a>",
          "description": "Labeled crowd scene images are expensive and scarce. To significantly reduce\nthe requirement of the labeled images, we propose ColorCount, a novel CNN-based\napproach by combining self-supervised transfer colorization learning and global\nprior classification to leverage the abundantly available unlabeled data. The\nself-supervised colorization branch learns the semantics and surface texture of\nthe image by using its color components as pseudo labels. The classification\nbranch extracts global group priors by learning correlations among image\nclusters. Their fused resultant discriminative features (global priors,\nsemantics and textures) provide ample priors for counting, hence significantly\nreducing the requirement of labeled images. We conduct extensive experiments on\nfour challenging benchmarks. ColorCount achieves much better performance as\ncompared with other unsupervised approaches. Its performance is close to the\nsupervised baseline with substantially less labeled data (10\\% of the original\none).",
          "link": "http://arxiv.org/abs/2105.09684",
          "publishedOn": "2021-05-23T06:08:17.452Z",
          "wordCount": 570,
          "title": "Crowd Counting by Self-supervised Transfer Colorization Learning and Global Prior Classification. (arXiv:2105.09684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1\">Mayukh Bagchi</a>",
          "description": "We assume that substances in the world are represented by two types of\nconcepts, namely substance concepts and classification concepts, the former\ninstrumental to (visual) perception, the latter to (language based)\nclassification. Based on this distinction, we introduce a general methodology\nfor building lexico-semantic hierarchies of substance concepts, where nodes are\nannotated with the media, e.g.,videos or photos, from which substance concepts\nare extracted, and are associated with the corresponding classification\nconcepts. The methodology is based on Ranganathan's original faceted approach,\ncontextualized to the problem of classifying substance concepts. The key\nnovelty is that the hierarchy is built exploiting the visual properties of\nsubstance concepts, while the linguistically defined properties of\nclassification concepts are only used to describe substance concepts. The\nvalidity of the approach is exemplified by providing some highlights of an\nongoing project whose goal is to build a large scale multimedia multilingual\nconcept hierarchy.",
          "link": "http://arxiv.org/abs/2105.09422",
          "publishedOn": "2021-05-23T06:08:17.426Z",
          "wordCount": 567,
          "title": "Classifying concepts via visual properties. (arXiv:2105.09422v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1\">Pengxiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Joint relation modeling is a curial component in human motion prediction.\nMost existing methods tend to design skeletal-based graphs to build the\nrelations among joints, where local interactions between joint pairs are well\nlearned. However, the global coordination of all joints, which reflects human\nmotion's balance property, is usually weakened because it is learned from part\nto whole progressively and asynchronously. Thus, the final predicted motions\nare sometimes unnatural. To tackle this issue, we learn a medium, called\nbalance attractor (BA), from the spatiotemporal features of motion to\ncharacterize the global motion features, which is subsequently used to build\nnew joint relations. Through the BA, all joints are related synchronously, and\nthus the global coordination of all joints can be better learned. Based on the\nBA, we propose our framework, referred to Attractor-Guided Neural Network,\nmainly including Attractor-Based Joint Relation Extractor (AJRE) and\nMulti-timescale Dynamics Extractor (MTDE). The AJRE mainly includes Global\nCoordination Extractor (GCE) and Local Interaction Extractor (LIE). The former\npresents the global coordination of all joints, and the latter encodes local\ninteractions between joint pairs. The MTDE is designed to extract dynamic\ninformation from raw position information for effective prediction. Extensive\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods in both short and long-term predictions in H3.6M, CMU-Mocap, and 3DPW.",
          "link": "http://arxiv.org/abs/2105.09711",
          "publishedOn": "2021-05-23T06:08:17.418Z",
          "wordCount": 642,
          "title": "An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction. (arXiv:2105.09711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam V. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1\">Zhongliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>",
          "description": "Camouflaged objects attempt to conceal their texture into the background and\ndiscriminating them from the background is hard even for human beings. The main\nobjective of this paper is to explore the camouflaged object segmentation\nproblem, namely, segmenting the camouflaged object(s) for a given image. This\nproblem has not been well studied in spite of a wide range of potential\napplications including the preservation of wild animals and the discovery of\nnew species, surveillance systems, search-and-rescue missions in the event of\nnatural disasters such as earthquakes, floods or hurricanes. This paper\naddresses a new challenging problem of camouflaged object segmentation. To\naddress this problem, we provide a new image dataset of camouflaged objects for\nbenchmarking purposes. In addition, we propose a general end-to-end network,\ncalled the Anabranch Network, that leverages both classification and\nsegmentation tasks. Different from existing networks for segmentation, our\nproposed network possesses the second branch for classification to predict the\nprobability of containing camouflaged object(s) in an image, which is then\nfused into the main branch for segmentation to boost up the segmentation\naccuracy. Extensive experiments conducted on the newly built dataset\ndemonstrate the effectiveness of our network using various fully convolutional\nnetworks. \\url{https://sites.google.com/view/ltnghia/research/camo}",
          "link": "http://arxiv.org/abs/2105.09451",
          "publishedOn": "2021-05-23T06:08:17.411Z",
          "wordCount": 649,
          "title": "Anabranch Network for Camouflaged Object Segmentation. (arXiv:2105.09451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rundi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changxi Zheng</a>",
          "description": "Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation -- describing a shape as\na sequence of computer-aided design (CAD) operations. Unlike meshes and point\nclouds, CAD models encode the user creation process of 3D shapes, widely used\nin numerous industrial and engineering design tasks. However, the sequential\nand irregular structure of CAD operations poses significant challenges for\nexisting 3D generative models. Drawing an analogy between CAD operations and\nnatural language, we propose a CAD generative network based on the Transformer.\nWe demonstrate the performance of our model for both shape autoencoding and\nrandom shape generation. To train our network, we create a new CAD dataset\nconsisting of 179,133 models and their CAD construction sequences. We have made\nthis dataset publicly available to promote future research on this topic.",
          "link": "http://arxiv.org/abs/2105.09492",
          "publishedOn": "2021-05-23T06:08:17.404Z",
          "wordCount": 608,
          "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Dengqiang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shangqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qunlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xinzhe Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Registration networks have shown great application potentials in medical\nimage analysis. However, supervised training methods have a great demand for\nlarge and high-quality labeled datasets, which is time-consuming and sometimes\nimpractical due to data sharing issues. Unsupervised image registration\nalgorithms commonly employ intensity-based similarity measures as loss\nfunctions without any manual annotations. These methods estimate the\nparameterized transformations between pairs of moving and fixed images through\nthe optimization of the network parameters during training. However, these\nmethods become less effective when the image quality varies, e.g., some images\nare corrupted by substantial noise or artifacts. In this work, we propose a\nnovel approach based on a low-rank representation, i.e., Regnet-LRR, to tackle\nthe problem. We project noisy images into a noise-free low-rank space, and then\ncompute the similarity between the images. Based on the low-rank similarity\nmeasure, we train the registration network to predict the dense deformation\nfields of noisy image pairs. We highlight that the low-rank projection is\nreformulated in a way that the registration network can successfully update\ngradients. With two tasks, i.e., cardiac and abdominal intra-modality\nregistration, we demonstrate that the low-rank representation can boost the\ngeneralization ability and robustness of models as well as bring significant\nimprovements in noisy data registration scenarios.",
          "link": "http://arxiv.org/abs/2105.09548",
          "publishedOn": "2021-05-23T06:08:17.397Z",
          "wordCount": 645,
          "title": "A low-rank representation for unsupervised registration of medical images. (arXiv:2105.09548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Rui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>",
          "description": "Attention mechanisms have been widely applied to cross-modal tasks such as\nimage captioning and information retrieval, and have achieved remarkable\nimprovements due to its capability to learn fine-grained relevance across\ndifferent modalities. However, existing attention models could be sub-optimal\nand lack preciseness because there is no direct supervision involved during\ntraining. In this work, we propose Contrastive Content Re-sourcing (CCR) and\nContrastive Content Swapping (CCS) constraints to address such limitation.\nThese constraints supervise the training of attention models in a contrastive\nlearning manner without requiring explicit attention annotations. Additionally,\nwe introduce three metrics, namely Attention Precision, Recall and F1-Score, to\nquantitatively evaluate the attention quality. We evaluate the proposed\nconstraints with cross-modal retrieval (image-text matching) task. The\nexperiments on both Flickr30k and MS-COCO datasets demonstrate that integrating\nthese attention constraints into two state-of-the-art attention-based models\nimproves the model performance in terms of both retrieval accuracy and\nattention metrics.",
          "link": "http://arxiv.org/abs/2105.09597",
          "publishedOn": "2021-05-23T06:08:17.378Z",
          "wordCount": 587,
          "title": "More Than Just Attention: Learning Cross-Modal Attentions with Contrastive Constraints. (arXiv:2105.09597v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "Superpixels are higher-order perceptual groups of pixels in an image, often\ncarrying much more information than raw pixels. There is an inherent relational\nstructure to the relationship among different superpixels of an image. This\nrelational information can convey some form of domain information about the\nimage, e.g. relationship between superpixels representing two eyes in a cat\nimage. Our interest in this paper is to construct computer vision models,\nspecifically those based on Deep Neural Networks (DNNs) to incorporate these\nsuperpixels information. We propose a methodology to construct a hybrid model\nthat leverages (a) Convolutional Neural Network (CNN) to deal with spatial\ninformation in an image, and (b) Graph Neural Network (GNN) to deal with\nrelational superpixel information in the image. The proposed deep model is\nlearned using a generic hybrid loss function that we call a `hybrid' loss. We\nevaluate the predictive performance of our proposed hybrid vision model on four\npopular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.\nMoreover, we evaluate our method on three real-world classification tasks:\nCOVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint\nIdentification. The results demonstrate that the relational superpixel\ninformation provided via a GNN could improve the performance of standard\nCNN-based vision systems.",
          "link": "http://arxiv.org/abs/2105.09448",
          "publishedOn": "2021-05-23T06:08:17.371Z",
          "wordCount": 693,
          "title": "Superpixel-based Domain-Knowledge Infusion in Computer Vision. (arXiv:2105.09448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuncong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>",
          "description": "Recently, plenty of work has tried to introduce transformers into computer\nvision tasks, with good results. Unlike classic convolution networks, which\nextract features within a local receptive field, transformers can adaptively\naggregate similar features from a global view using self-attention mechanism.\nFor object detection, Feature Pyramid Network (FPN) proposes feature\ninteraction across layers and proves its extremely importance. However, its\ninteraction is still in a local manner, which leaves a lot of room for\nimprovement. Since transformer was originally designed for NLP tasks, adapting\nprocessing subject directly from text to image will cause unaffordable\ncomputation and space overhead. In this paper, we utilize a linearized\nattention function to overcome above problems and build a novel architecture,\nnamed Content-Augmented Feature Pyramid Network (CA-FPN), which proposes a\nglobal content extraction module and deeply combines with FPN through light\nlinear transformers. What's more, light transformers can further make the\napplication of multi-head attention mechanism easier. Most importantly, our\nCA-FPN can be readily plugged into existing FPN-based models. Extensive\nexperiments on the challenging COCO object detection dataset demonstrated that\nour CA-FPN significantly outperforms competitive baselines without bells and\nwhistles. Code will be made publicly available.",
          "link": "http://arxiv.org/abs/2105.09464",
          "publishedOn": "2021-05-23T06:08:17.363Z",
          "wordCount": 634,
          "title": "Content-Augmented Feature Pyramid Network with Light Linear Transformers. (arXiv:2105.09464v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1\">Ruhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1\">Kaida Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Laili Zhu</a>",
          "description": "Recently, the anchor-free object detection model has shown great potential\nfor accuracy and speed to exceed anchor-based object detection. Therefore, two\nissues are mainly studied in this article: (1) How to let the backbone network\nin the anchor-free object detection model learn feature extraction? (2) How to\nmake better use of the feature pyramid network? In order to solve the above\nproblems, Experiments show that our model has a certain improvement in accuracy\ncompared with the current popular detection models on the COCO dataset, the\ndesigned attention mechanism module can capture contextual information well,\nimprove detection accuracy, and use sepc network to help balance abstract and\ndetailed information, and reduce the problem of semantic gap in the feature\npyramid network. Whether it is anchor-based network model YOLOv3, Faster RCNN,\nor anchor-free network model Foveabox, FSAF, FCOS. Our optimal model can get\n39.5% COCO AP under the background of ResNet50.",
          "link": "http://arxiv.org/abs/2105.09596",
          "publishedOn": "2021-05-23T06:08:17.355Z",
          "wordCount": 596,
          "title": "AGSFCOS: Based on attention mechanism and Scale-Equalizing pyramid network of object detection. (arXiv:2105.09596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lingni Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somasundaram_K/0/1/0/all/0/1\">Kiran Somasundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>",
          "description": "Given a video captured from a first person perspective and recorded in a\nfamiliar environment, can we recognize what the person is doing and identify\nwhere the action occurs in the 3D space? We address this challenging problem of\njointly recognizing and localizing actions of a mobile user on a known 3D map\nfrom egocentric videos. To this end, we propose a novel deep probabilistic\nmodel. Our model takes the inputs of a Hierarchical Volumetric Representation\n(HVR) of the environment and an egocentric video, infers the 3D action location\nas a latent variable, and recognizes the action based on the video and\ncontextual cues surrounding its potential locations. To evaluate our model, we\nconduct extensive experiments on a newly collected egocentric video dataset, in\nwhich both human naturalistic actions and photo-realistic 3D environment\nreconstructions are captured. Our method demonstrates strong results on both\naction recognition and 3D action localization across seen and unseen\nenvironments. We believe our work points to an exciting research direction in\nthe intersection of egocentric vision, and 3D scene understanding.",
          "link": "http://arxiv.org/abs/2105.09544",
          "publishedOn": "2021-05-23T06:08:17.347Z",
          "wordCount": 612,
          "title": "Egocentric Activity Recognition and Localization on a 3D Map. (arXiv:2105.09544v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangeh_M/0/1/0/all/0/1\">Mehrdad J Gangeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plata_M/0/1/0/all/0/1\">Marcin Plata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motahari_H/0/1/0/all/0/1\">Hamid Motahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel P Duffy</a>",
          "description": "Removing noise from scanned pages is a vital step before their submission to\noptical character recognition (OCR) system. Most available image denoising\nmethods are supervised where the pairs of noisy/clean pages are required.\nHowever, this assumption is rarely met in real settings. Besides, there is no\nsingle model that can remove various noise types from documents. Here, we\npropose a unified end-to-end unsupervised deep learning model, for the first\ntime, that can effectively remove multiple types of noise, including salt \\&\npepper noise, blurred and/or faded text, as well as watermarks from documents\nat various levels of intensity. We demonstrate that the proposed model\nsignificantly improves the quality of scanned images and the OCR of the pages\non several test datasets.",
          "link": "http://arxiv.org/abs/2105.09437",
          "publishedOn": "2021-05-23T06:08:17.320Z",
          "wordCount": 551,
          "title": "End-to-End Unsupervised Document Image Blind Denoising. (arXiv:2105.09437v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yada Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>",
          "description": "With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and characterized with multiple labels,\nthus exhibiting the co-existence of multiple types of heterogeneity. Although\nstate-of-the-art techniques are good at modeling the complex heterogeneity with\nsufficient label information, such label information can be quite expensive to\nobtain in real applications, leading to sub-optimal performance using these\ntechniques. Inspired by the capability of contrastive learning to utilize rich\nunlabeled data for improving performance, in this paper, we propose a unified\nheterogeneous learning framework, which combines both weighted unsupervised\ncontrastive loss and weighted supervised contrastive loss to model multiple\ntypes of heterogeneity. We also provide theoretical analyses showing that the\nproposed weighted supervised contrastive loss is the lower bound of the mutual\ninformation of two samples from the same class and the weighted unsupervised\ncontrastive loss is the lower bound of the mutual information between the\nhidden representation of two views of the same sample. Experimental results on\nreal-world data sets demonstrate the effectiveness and the efficiency of the\nproposed method modeling multiple types of heterogeneity.",
          "link": "http://arxiv.org/abs/2105.09401",
          "publishedOn": "2021-05-23T06:08:17.301Z",
          "wordCount": 613,
          "title": "Heterogeneous Contrastive Learning. (arXiv:2105.09401v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolotouros_N/0/1/0/all/0/1\">Nikos Kolotouros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badger_M/0/1/0/all/0/1\">Marc Badger</a>",
          "description": "Animals are diverse in shape, but building a deformable shape model for a new\nspecies is not always possible due to the lack of 3D data. We present a method\nto capture new species using an articulated template and images of that\nspecies. In this work, we focus mainly on birds. Although birds represent\nalmost twice the number of species as mammals, no accurate shape model is\navailable. To capture a novel species, we first fit the articulated template to\neach training sample. By disentangling pose and shape, we learn a shape space\nthat captures variation both among species and within each species from image\nevidence. We learn models of multiple species from the CUB dataset, and\ncontribute new species-specific and multi-species shape models that are useful\nfor downstream reconstruction tasks. Using a low-dimensional embedding, we show\nthat our learned 3D shape space better reflects the phylogenetic relationships\namong birds than learned perceptual features.",
          "link": "http://arxiv.org/abs/2105.09396",
          "publishedOn": "2021-05-23T06:08:17.274Z",
          "wordCount": 596,
          "title": "Birds of a Feather: Capturing Avian Shape Models from Images. (arXiv:2105.09396v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09378",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gadjimuradov_F/0/1/0/all/0/1\">Fasil Gadjimuradov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benkert_T/0/1/0/all/0/1\">Thomas Benkert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nickel_M/0/1/0/all/0/1\">Marcel Dominik Nickel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>",
          "description": "Purpose: To develop an algorithm for robust partial Fourier (PF)\nreconstruction applicable to diffusion-weighted (DW) images with non-smooth\nphase variations.\n\nMethods: Based on an unrolled proximal splitting algorithm, a neural network\narchitecture is derived which alternates between data consistency operations\nand regularization implemented by recurrent convolutions. In order to exploit\ncorrelations, multiple repetitions of the same slice are jointly reconstructed\nunder consideration of permutation-equivariance. The proposed method is trained\non DW liver data of 60 volunteers and evaluated on retrospectively and\nprospectively sub-sampled data of different anatomies and resolutions. In\naddition, the benefits of using a recurrent network over other unrolling\nstrategies is investigated.\n\nResults: Conventional PF techniques can be significantly outperformed in\nterms of quantitative measures as well as perceptual image quality. The\nproposed method is able to generalize well to brain data with contrasts and\nresolution not present in the training set. The reduction in echo time (TE)\nassociated with prospective PF-sampling enables DW imaging with higher signal.\nAlso, the TE increase in acquisitions with higher resolution can be compensated\nfor. It can be shown that unrolling by means of a recurrent network produced\nbetter results than using a weight-shared network or a cascade of networks.\n\nConclusion: This work demonstrates that robust PF reconstruction of DW data\nis feasible even at strong PF factors in applications with severe phase\nvariations. Since the proposed method does not rely on smoothness priors of the\nphase but uses learned recurrent convolutions instead, artifacts of\nconventional PF methods can be avoided.",
          "link": "http://arxiv.org/abs/2105.09378",
          "publishedOn": "2021-05-23T06:08:17.262Z",
          "wordCount": 705,
          "title": "Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network. (arXiv:2105.09378v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halperin_T/0/1/0/all/0/1\">Tavi Halperin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakim_H/0/1/0/all/0/1\">Hanit Hakim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vantzos_O/0/1/0/all/0/1\">Orestis Vantzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochman_G/0/1/0/all/0/1\">Gershon Hochman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_N/0/1/0/all/0/1\">Netai Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassy_L/0/1/0/all/0/1\">Lior Sassy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupchik_M/0/1/0/all/0/1\">Michael Kupchik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_O/0/1/0/all/0/1\">Ofir Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>",
          "description": "We present an algorithm for producing a seamless animated loop from a single\nimage. The algorithm detects periodic structures, such as the windows of a\nbuilding or the steps of a staircase, and generates a non-trivial displacement\nvector field that maps each segment of the structure onto a neighboring segment\nalong a user- or auto-selected main direction of motion. This displacement\nfield is used, together with suitable temporal and spatial smoothing, to warp\nthe image and produce the frames of a continuous animation loop. Our\ncinemagraphs are created in under a second on a mobile device. Over 140,000\nusers downloaded our app and exported over 350,000 cinemagraphs. Moreover, we\nconducted two user studies that show that users prefer our method for creating\nsurreal and structured cinemagraphs compared to more manual approaches and\ncompared to previous methods.",
          "link": "http://arxiv.org/abs/2105.09374",
          "publishedOn": "2021-05-23T06:08:17.231Z",
          "wordCount": 608,
          "title": "Endless Loops: Detecting and Animating Periodic Patterns in Still Images. (arXiv:2105.09374v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09365",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Uysal_E/0/1/0/all/0/1\">Enes Sadi Uysal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilici_M/0/1/0/all/0/1\">M.&#x15e;afak Bilici</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaza_B/0/1/0/all/0/1\">B. Selin Zaza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozgenc_M/0/1/0/all/0/1\">M. Yi&#x11f;it &#xd6;zgen&#xe7;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyar_O/0/1/0/all/0/1\">Onur Boyar</a>",
          "description": "Retinal Vessel Segmentation is important for diagnosis of various diseases.\nThe research on retinal vessel segmentation focuses mainly on improvement of\nthe segmentation model which is usually based on U-Net architecture. In our\nstudy we use the U-Net architecture and we rely on heavy data augmentation in\norder to achieve better performance. The success of the data augmentation\nrelies on successfully addressing the problem of input images. By analyzing\ninput images and performing the augmentation accordingly we show that the\nperformance of the U-Net model can be increased dramatically. Results are\nreported using the most widely used retina dataset, DRIVE.",
          "link": "http://arxiv.org/abs/2105.09365",
          "publishedOn": "2021-05-23T06:08:17.203Z",
          "wordCount": 564,
          "title": "Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation. (arXiv:2105.09365v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuesu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>",
          "description": "While imitation learning for vision based autonomous mobile robot navigation\nhas recently received a great deal of attention in the research community,\nexisting approaches typically require state action demonstrations that were\ngathered using the deployment platform. However, what if one cannot easily\noutfit their platform to record these demonstration signals or worse yet the\ndemonstrator does not have access to the platform at all? Is imitation learning\nfor vision based autonomous navigation even possible in such scenarios? In this\nwork, we hypothesize that the answer is yes and that recent ideas from the\nImitation from Observation (IfO) literature can be brought to bear such that a\nrobot can learn to navigate using only ego centric video collected by a\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\nintroduce a new algorithm, Visual Observation only Imitation Learning for\nAutonomous navigation (VOILA), that can successfully learn navigation policies\nfrom a single video demonstration collected from a physically different agent.\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\nnot only successfully imitates the expert, but that it also learns navigation\npolicies that can generalize to novel environments. Further, we demonstrate the\neffectiveness of VOILA in a real world setting by showing that it allows a\nwheeled Jackal robot to successfully imitate a human walking in an environment\nusing a video recorded using a mobile phone camera.",
          "link": "http://arxiv.org/abs/2105.09371",
          "publishedOn": "2021-05-23T06:08:17.176Z",
          "wordCount": 671,
          "title": "VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.",
          "link": "http://arxiv.org/abs/2105.09356",
          "publishedOn": "2021-05-23T06:08:17.161Z",
          "wordCount": 621,
          "title": "Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v1 [cs.LG])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/1910.04054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_V/0/1/0/all/0/1\">Viswanath Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delalleau_O/0/1/0/all/0/1\">Olivier Delalleau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1\">Alexander H. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuttler_H/0/1/0/all/0/1\">Heinrich K&#xfc;ttler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nardelli_N/0/1/0/all/0/1\">Nantas Nardelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Mike Rabbat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Effective network congestion control strategies are key to keeping the\nInternet (or any large computer network) operational. Network congestion\ncontrol has been dominated by hand-crafted heuristics for decades. Recently,\nReinforcementLearning (RL) has emerged as an alternative to automatically\noptimize such control strategies. Research so far has primarily considered RL\ninterfaces which block the sender while an agent considers its next action.\nThis is largely an artifact of building on top of frameworks designed for RL in\ngames (e.g. OpenAI Gym). However, this does not translate to real-world\nnetworking environments, where a network sender waiting on a policy without\nsending data leads to under-utilization of bandwidth. We instead propose to\nformulate congestion control with an asynchronous RL agent that handles delayed\nactions. We present MVFST-RL, a scalable framework for congestion control in\nthe QUIC transport protocol that leverages state-of-the-art in asynchronous RL\ntraining with off-policy correction. We analyze modeling improvements to\nmitigate the deviation from Markovian dynamics, and evaluate our method on\nemulated networks from the Pantheon benchmark platform. The source code is\npublicly available at https://github.com/facebookresearch/mvfst-rl.",
          "link": "http://arxiv.org/abs/1910.04054",
          "publishedOn": "2021-05-28T01:42:17.224Z",
          "wordCount": 696,
          "title": "MVFST-RL: An Asynchronous RL Framework for Congestion Control with Delayed Actions. (arXiv:1910.04054v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1\">Zhaobin Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xuan Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Rongye Shi</a>",
          "description": "Car-following behavior has been extensively studied using physics-based\nmodels, such as the Intelligent Driver Model. These models successfully\ninterpret traffic phenomena observed in the real-world but may not fully\ncapture the complex cognitive process of driving. Deep learning models, on the\nother hand, have demonstrated their power in capturing observed traffic\nphenomena but require a large amount of driving data to train. This paper aims\nto develop a family of neural network based car-following models that are\ninformed by physics-based models, which leverage the advantage of both\nphysics-based (being data-efficient and interpretable) and deep learning based\n(being generalizable) models. We design physics-informed deep learning\ncar-following (PIDL-CF) architectures encoded with two popular physics-based\nmodels - IDM and OVM, on which acceleration is predicted for four traffic\nregimes: acceleration, deceleration, cruising, and emergency braking. Two types\nof PIDL-CFM problems are studied, one to predict acceleration only and the\nother to jointly predict acceleration and discover model parameters. We also\ndemonstrate the superior performance of PIDL with the Next Generation\nSIMulation (NGSIM) dataset over baselines, especially when the training data is\nsparse. The results demonstrate the superior performance of neural networks\ninformed by physics over those without. The developed PIDL-CF framework holds\nthe potential for system identification of driving models and for the\ndevelopment of driving-based controls for automated vehicles.",
          "link": "http://arxiv.org/abs/2012.13376",
          "publishedOn": "2021-05-28T01:42:17.217Z",
          "wordCount": 680,
          "title": "A Physics-Informed Deep Learning Paradigm for Car-Following Models. (arXiv:2012.13376v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tsang Ing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Adriano L. I. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "In this paper, we argue that the unsatisfactory out-of-distribution (OOD)\ndetection performance of neural networks is mainly due to the SoftMax loss\nanisotropy and propensity to produce low entropy probability distributions in\ndisagreement with the principle of maximum entropy. Current out-of-distribution\n(OOD) detection approaches usually do not directly fix the SoftMax loss\ndrawbacks but rather build techniques to circumvent it. Unfortunately, those\nmethods usually produce undesired side effects (e.g., classification accuracy\ndrop, additional hyperparameters, slower inferences, and collecting extra\ndata). In the opposite direction, we propose replacing SoftMax loss with a\nnovel loss function that does not suffer from the mentioned weaknesses. The\nproposed IsoMax loss is isotropic (exclusively distance-based) and provides\nhigh entropy posterior probability distributions. Replacing the SoftMax loss by\nIsoMax loss requires no model or training changes. Additionally, the models\ntrained with IsoMax loss produce as fast and energy-efficient inferences as\nthose trained using SoftMax loss. Further, no classification accuracy drop is\nobserved. The proposed method does not rely on outlier/background data,\nhyperparameter tuning, temperature calibration, feature extraction, metric\nlearning, adversarial training, ensemble procedures, or generative models. Our\nexperiments showed that IsoMax loss works as a seamless SoftMax loss drop-in\nreplacement that significantly improves neural networks' OOD detection\nperformance. Therefore, it may be used as a baseline OOD detection approach to\nbe combined with current or future OOD detection techniques to achieve even\nhigher results.",
          "link": "http://arxiv.org/abs/2006.04005",
          "publishedOn": "2021-05-28T01:42:17.210Z",
          "wordCount": 697,
          "title": "Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1\">Somesh Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Joyce An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Bombarelli_R/0/1/0/all/0/1\">Rafael G&#xf3;mez-Bombarelli</a>",
          "description": "Macromolecules are large, complex molecules composed of covalently bonded\nmonomer units, existing in different stereochemical configurations and\ntopologies. As a result of such chemical diversity, representing, comparing,\nand learning over macromolecules emerge as critical challenges. To address\nthis, we developed a macromolecule graph representation, with monomers and\nbonds as nodes and edges, respectively. We captured the inherent chemistry of\nthe macromolecule by using molecular fingerprints for node and edge attributes.\nFor the first time, we demonstrated computation of chemical similarity between\n2 macromolecules of varying chemistry and topology, using exact graph edit\ndistances and graph kernels. We trained interpretable graph neural networks for\na variety of glycan classification tasks, achieving state-of-the-art results.\nOur work has two-fold implications - it provides a general framework for\nrepresentation, comparison, and learning of macromolecules, and it enables\nquantitative chemistry-informed decision-making and iterative design in the\nmacromolecular chemical space.",
          "link": "http://arxiv.org/abs/2103.02565",
          "publishedOn": "2021-05-28T01:42:17.192Z",
          "wordCount": 633,
          "title": "Chemistry-informed Macromolecule Graph Representation for Similarity Computation and Supervised Learning. (arXiv:2103.02565v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12894",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Huang_C/0/1/0/all/0/1\">Chaofan Huang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_S/0/1/0/all/0/1\">Simin Ma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_S/0/1/0/all/0/1\">Shihao Yang</a>",
          "description": "Ordinary differential equations (ODEs), commonly used to characterize the\ndynamic systems, are difficult to propose in closed-form for many complicated\nscientific applications, even with the help of domain expert. We propose a fast\nand accurate data-driven method, MAGI-X, to learn the unknown dynamic from the\nobservation data in a non-parametric fashion, without the need of any domain\nknowledge. Unlike the existing methods that mainly rely on the costly numerical\nintegration, MAGI-X utilizes the powerful functional approximator of neural\nnetwork to learn the unknown nonlinear dynamic within the MAnifold-constrained\nGaussian process Inference (MAGI) framework that completely circumvents the\nnumerical integration. Comparing against the state-of-the-art methods on three\nrealistic examples, MAGI-X achieves competitive accuracy in both fitting and\nforecasting while only taking a fraction of computational time. Moreover,\nMAGI-X provides practical solution for the inference of partial observed\nsystems, which no previous method is able to handle.",
          "link": "http://arxiv.org/abs/2105.12894",
          "publishedOn": "2021-05-28T01:42:17.185Z",
          "wordCount": 579,
          "title": "MAGI-X: Manifold-Constrained Gaussian Process Inference for Unknown System Dynamics. (arXiv:2105.12894v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunfei Yang</a>",
          "description": "This paper studies how well generative adversarial networks (GANs) learn\nprobability distributions from finite samples. Our main results estimate the\nconvergence rates of GANs under a collection of integral probability metrics\ndefined through H\\\"older classes, including the Wasserstein distance as a\nspecial case. We also show that GANs are able to adaptively learn data\ndistributions with low-dimensional structure or have H\\\"older densities, when\nthe network architectures are chosen properly. In particular, for distributions\nconcentrate around a low-dimensional set, it is proved that the learning rates\nof GANs do not depend on the high ambient dimension, but on the lower intrinsic\ndimension. Our analysis is based on a new oracle inequality decomposing the\nestimation error into generator and discriminator approximation error and\nstatistical error, which may be of independent interest.",
          "link": "http://arxiv.org/abs/2105.13010",
          "publishedOn": "2021-05-28T01:42:17.178Z",
          "wordCount": 573,
          "title": "An error analysis of generative adversarial networks for learning distributions. (arXiv:2105.13010v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Briggs_C/0/1/0/all/0/1\">Christopher Briggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andras_P/0/1/0/all/0/1\">Peter Andras</a>",
          "description": "Energy demand forecasting is an essential task performed within the energy\nindustry to help balance supply with demand and maintain a stable load on the\nelectricity grid. As supply transitions towards less reliable renewable energy\ngeneration, smart meters will prove a vital component to aid these forecasting\ntasks. However, smart meter take-up is low among privacy-conscious consumers\nthat fear intrusion upon their fine-grained consumption data. In this work we\npropose and explore a federated learning (FL) based approach for training\nforecasting models in a distributed, collaborative manner whilst retaining the\nprivacy of the underlying data. We compare two approaches: FL, and a clustered\nvariant, FL+HC against a non-private, centralised learning approach and a fully\nprivate, localised learning approach. Within these approaches, we measure model\nperformance using RMSE and computational efficiency via the number of samples\nrequired to train models under each scenario. In addition, we suggest the FL\nstrategies are followed by a personalisation step and show that model\nperformance can be improved by doing so. We show that FL+HC followed by\npersonalisation can achieve a $\\sim$5% improvement in model performance with a\n$\\sim$10x reduction in computation compared to localised learning. Finally we\nprovide advice on private aggregation of predictions for building a private\nend-to-end energy demand forecasting application.",
          "link": "http://arxiv.org/abs/2105.13325",
          "publishedOn": "2021-05-28T01:42:17.171Z",
          "wordCount": 635,
          "title": "Federated Learning for Short-term Residential Energy Demand Forecasting. (arXiv:2105.13325v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.10692",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Harris_K/0/1/0/all/0/1\">Kameron Decker Harris</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yizhe Zhu</a>",
          "description": "We provide a novel analysis of low-rank tensor completion based on hypergraph\nexpanders. As a proxy for rank, we minimize the max-quasinorm of the tensor,\nwhich generalizes the max-norm for matrices. Our analysis is deterministic and\nshows that the number of samples required to approximately recover an order-$t$\ntensor with at most $n$ entries per dimension is linear in $n$, under the\nassumption that the rank and order of the tensor are $O(1)$. As steps in our\nproof, we find a new expander mixing lemma for a $t$-partite, $t$-uniform\nregular hypergraph model, and prove several new properties about tensor\nmax-quasinorm. To the best of our knowledge, this is the first deterministic\nanalysis of tensor completion. We develop a practical algorithm that solves a\nrelaxed version of the max-quasinorm minimization problem, and we demonstrate\nits efficacy with numerical experiments.",
          "link": "http://arxiv.org/abs/1910.10692",
          "publishedOn": "2021-05-28T01:42:17.156Z",
          "wordCount": 601,
          "title": "Deterministic tensor completion with hypergraph expanders. (arXiv:1910.10692v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_R/0/1/0/all/0/1\">Ratna Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmakar_P/0/1/0/all/0/1\">Prasenjit Karmakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumyajit Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spandan_D/0/1/0/all/0/1\">Debaleen Das Spandan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_S/0/1/0/all/0/1\">Shouvit Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sujoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Sandip Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandi_S/0/1/0/all/0/1\">Subrata Nandi</a>",
          "description": "Intelligent city transportation systems are one of the core infrastructures\nof a smart city. The true ingenuity of such an infrastructure lies in providing\nthe commuters with real-time information about citywide transports like public\nbuses, allowing her to pre-plan the travel. However, providing prior\ninformation for transportation systems like public buses in real-time is\ninherently challenging because of the diverse nature of different\nstay-locations that a public bus stops. Although straightforward factors stay\nduration, extracted from unimodal sources like GPS, at these locations look\nerratic, a thorough analysis of public bus GPS trails for 720km of bus travels\nat the city of Durgapur, a semi-urban city in India, reveals that several other\nfine-grained contextual features can characterize these locations accurately.\nAccordingly, we develop BuStop, a system for extracting and characterizing the\nstay locations from multi-modal sensing using commuters' smartphones. Using\nthis multi-modal information BuStop extracts a set of granular contextual\nfeatures that allow the system to differentiate among the different\nstay-location types. A thorough analysis of BuStop using the collected dataset\nindicates that the system works with high accuracy in identifying different\nstay locations like regular bus stops, random ad-hoc stops, stops due to\ntraffic congestion stops at traffic signals, and stops at sharp turns.\nAdditionally, we also develop a proof-of-concept setup on top of BuStop to\nanalyze the potential of the framework in predicting expected arrival time, a\ncritical piece of information required to pre-plan travel, at any given bus\nstop. Subsequent analysis of the PoC framework, through simulation over the\ntest dataset, shows that characterizing the stay-locations indeed helps make\nmore accurate arrival time predictions with deviations less than 60s from the\nground-truth arrival time.",
          "link": "http://arxiv.org/abs/2105.13131",
          "publishedOn": "2021-05-28T01:42:17.150Z",
          "wordCount": 740,
          "title": "Exploiting Multi-modal Contextual Sensing for City-bus's Stay Location Characterization: Towards Sub-60 Seconds Accurate Arrival Time Prediction. (arXiv:2105.13131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grand_Clement_J/0/1/0/all/0/1\">Julien Grand-Cl&#xe9;ment</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroer_C/0/1/0/all/0/1\">Christian Kroer</a>",
          "description": "We develop new parameter and scale-free algorithms for solving convex-concave\nsaddle-point problems. Our results are based on a new simple regret minimizer,\nthe Conic Blackwell Algorithm$^+$ (CBA$^+$), which attains $O(1/\\sqrt{T})$\naverage regret. Intuitively, our approach generalizes to other decision sets of\ninterest ideas from the Counterfactual Regret minimization (CFR$^+$) algorithm,\nwhich has very strong practical performance for solving sequential games on\nsimplexes. We show how to implement CBA$^+$ for the simplex, $\\ell_{p}$ norm\nballs, and ellipsoidal confidence regions in the simplex, and we present\nnumerical experiments for solving matrix games and distributionally robust\noptimization problems. Our empirical results show that CBA$^+$ is a simple\nalgorithm that outperforms state-of-the-art methods on synthetic data and real\ndata instances, without the need for any choice of step sizes or other\nalgorithmic parameters.",
          "link": "http://arxiv.org/abs/2105.13203",
          "publishedOn": "2021-05-28T01:42:17.143Z",
          "wordCount": 553,
          "title": "Conic Blackwell Algorithm: Parameter-Free Convex-Concave Saddle-Point Solving. (arXiv:2105.13203v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woods_W/0/1/0/all/0/1\">Walt Woods</a>",
          "description": "When working to understand usage of a data format, examples of the data\nformat are often more representative than the format's specification. For\nexample, two different applications might use very different JSON\nrepresentations, or two PDF-writing applications might make use of very\ndifferent areas of the PDF specification to realize the same rendered content.\nThe complexity arising from these distinct origins can lead to large,\ndifficult-to-understand attack surfaces, presenting a security concern when\nconsidering both exfiltration and data schizophrenia. Grammar inference can aid\nin describing the practical language generator behind examples of a data\nformat. However, most grammar inference research focuses on natural language,\nnot data formats, and fails to support crucial features such as type recursion.\nWe propose a novel set of mechanisms for grammar inference, RL-GRIT, and apply\nthem to understanding de facto data formats. After reviewing existing grammar\ninference solutions, it was determined that a new, more flexible scaffold could\nbe found in Reinforcement Learning (RL). Within this work, we lay out the many\nalgorithmic changes required to adapt RL from its traditional, sequential-time\nenvironment to the highly interdependent environment of parsing. The result is\nan algorithm which can demonstrably learn recursive control structures in\nsimple data formats, and can extract meaningful structure from fragments of the\nPDF format. Whereas prior work in grammar inference focused on either regular\nlanguages or constituency parsing, we show that RL can be used to surpass the\nexpressiveness of both classes, and offers a clear path to learning\ncontext-sensitive languages. The proposed algorithm can serve as a building\nblock for understanding the ecosystems of de facto data formats.",
          "link": "http://arxiv.org/abs/2105.13114",
          "publishedOn": "2021-05-28T01:42:17.132Z",
          "wordCount": 712,
          "title": "RL-GRIT: Reinforcement Learning for Grammar Inference. (arXiv:2105.13114v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">J&#xf6;rg Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elster_C/0/1/0/all/0/1\">Clemens Elster</a>",
          "description": "We present a Bayesian treatment for deep regression using an\nErrors-in-Variables model which accounts for the uncertainty associated with\nthe input to the employed neural network. It is shown how the treatment can be\ncombined with already existing approaches for uncertainty quantification that\nare based on variational inference. Our approach yields a decomposition of the\npredictive uncertainty into an aleatoric and epistemic part that is more\ncomplete and, in many cases, more consistent from a statistical perspective. We\nillustrate and discuss the approach along various toy and real world examples.",
          "link": "http://arxiv.org/abs/2105.09095",
          "publishedOn": "2021-05-28T01:42:17.124Z",
          "wordCount": 541,
          "title": "Errors-in-Variables for deep learning: rethinking aleatoric uncertainty. (arXiv:2105.09095v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pellegrini_L/0/1/0/all/0/1\">Lorenzo Pellegrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graffieti_G/0/1/0/all/0/1\">Gabriele Graffieti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maltoni_D/0/1/0/all/0/1\">Davide Maltoni</a>",
          "description": "On-device training for personalized learning is a challenging research\nproblem. Being able to quickly adapt deep prediction models at the edge is\nnecessary to better suit personal user needs. However, adaptation on the edge\nposes some questions on both the efficiency and sustainability of the learning\nprocess and on the ability to work under shifting data distributions. Indeed,\nnaively fine-tuning a prediction model only on the newly available data results\nin catastrophic forgetting, a sudden erasure of previously acquired knowledge.\nIn this paper, we detail the implementation and deployment of a hybrid\ncontinual learning strategy (AR1*) on a native Android application for\nreal-time on-device personalization without forgetting. Our benchmark, based on\nan extension of the CORe50 dataset, shows the efficiency and effectiveness of\nour solution.",
          "link": "http://arxiv.org/abs/2105.13127",
          "publishedOn": "2021-05-28T01:42:17.117Z",
          "wordCount": 571,
          "title": "Continual Learning at the Edge: Real-Time Training on Smartphone Devices. (arXiv:2105.13127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>",
          "description": "We propose to measure fine-grained domain relevance - the degree that a term\nis relevant to a broad (e.g., computer science) or narrow (e.g., deep learning)\ndomain. Such measurement is crucial for many downstream tasks in natural\nlanguage processing. To handle long-tail terms, we build a core-anchored\nsemantic graph, which uses core terms with rich description information to\nbridge the vast remaining fringe terms semantically. To support a fine-grained\ndomain without relying on a matching corpus for supervision, we develop\nhierarchical core-fringe learning, which learns core and fringe terms jointly\nin a semi-supervised manner contextualized in the hierarchy of the domain. To\nreduce expensive human efforts, we employ automatic annotation and hierarchical\npositive-unlabeled learning. Our approach applies to big or small domains,\ncovers head or tail terms, and requires little human effort. Extensive\nexperiments demonstrate that our methods outperform strong baselines and even\nsurpass professional human performance.",
          "link": "http://arxiv.org/abs/2105.13255",
          "publishedOn": "2021-05-28T01:42:17.104Z",
          "wordCount": 587,
          "title": "Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach. (arXiv:2105.13255v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.01675",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delingette_H/0/1/0/all/0/1\">Herv&#xe9; Delingette</a>",
          "description": "Variational autoencoder (VAE) is a very popular and well-investigated\ngenerative model in neural learning research. To leverage VAE in practical\ntasks dealing with a massive dataset of large dimensions, it is required to\ndeal with the difficulty of building low variance evidence lower bounds (ELBO).\nMarkov Chain Monte Carlo (MCMC) is an effective approach to tighten the ELBO\nfor approximating the posterior distribution and Hamiltonian Variational\nAutoencoder (HVAE) is an effective MCMC inspired approach for constructing a\nlow-variance ELBO that is amenable to the reparameterization trick. The HVAE\nadapted the Hamiltonian dynamic flow into variational inference that\nsignificantly improves the performance of the posterior estimation. We propose\nin this work a Langevin dynamic flow-based inference approach by incorporating\nthe gradients information in the inference process through the Langevin dynamic\nwhich is a kind of MCMC based method similar to HVAE. Specifically, we employ a\nquasi-symplectic integrator to cope with the prohibit problem of the Hessian\ncomputing in naive Langevin flow. We show the theoretical and practical\neffectiveness of the proposed framework with other gradient flow-based methods.",
          "link": "http://arxiv.org/abs/2009.01675",
          "publishedOn": "2021-05-28T01:42:17.097Z",
          "wordCount": 631,
          "title": "Quasi-symplectic Langevin Variational Autoencoder. (arXiv:2009.01675v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "This paper presents a new Vision Transformer (ViT) architecture Multi-Scale\nVision Longformer, which significantly enhances the ViT of\n\\cite{dosovitskiy2020image} for encoding high-resolution images using two\ntechniques. The first is the multi-scale model structure, which provides image\nencodings at multiple scales with manageable computational cost. The second is\nthe attention mechanism of vision Longformer, which is a variant of Longformer\n\\cite{beltagy2020longformer}, originally developed for natural language\nprocessing, and achieves a linear complexity w.r.t. the number of input tokens.\nA comprehensive empirical study shows that the new ViT significantly\noutperforms several strong baselines, including the existing ViT models and\ntheir ResNet counterparts, and the Pyramid Vision Transformer from a concurrent\nwork \\cite{wang2021pyramid}, on a range of vision tasks, including image\nclassification, object detection, and segmentation. The models and source code\nare released at \\url{https://github.com/microsoft/vision-longformer}.",
          "link": "http://arxiv.org/abs/2103.15358",
          "publishedOn": "2021-05-28T01:42:17.059Z",
          "wordCount": 615,
          "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding. (arXiv:2103.15358v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ingale_V/0/1/0/all/0/1\">Vaishali Ingale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1\">Anush Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_D/0/1/0/all/0/1\">Divit Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Krishna Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>",
          "description": "This paper explores the idea of utilising Long Short-Term Memory neural\nnetworks (LSTMNN) for the generation of musical sequences in ABC notation. The\nproposed approach takes ABC notations from the Nottingham dataset and encodes\nit to beefed as input for the neural networks. The primary objective is to\ninput the neural networks with an arbitrary note, let the network process and\naugment a sequence based on the note until a good piece of music is produced.\nMultiple tunings have been done to amend the parameters of the network for\noptimal generation. The output is assessed on the basis of rhythm, harmony, and\ngrammar accuracy.",
          "link": "http://arxiv.org/abs/2105.09046",
          "publishedOn": "2021-05-28T01:42:17.046Z",
          "wordCount": 554,
          "title": "Music Generation using Three layered LSTM. (arXiv:2105.09046v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Siddharth Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mabsout_B/0/1/0/all/0/1\">Bassel Mabsout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancuso_R/0/1/0/all/0/1\">Renato Mancuso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "A critical problem with the practical utility of controllers trained with\ndeep Reinforcement Learning (RL) is the notable lack of smoothness in the\nactions learned by the RL policies. This trend often presents itself in the\nform of control signal oscillation and can result in poor control, high power\nconsumption, and undue system wear. We introduce Conditioning for Action Policy\nSmoothness (CAPS), an effective yet intuitive regularization on action\npolicies, which offers consistent improvement in the smoothness of the learned\nstate-to-action mappings of neural network controllers, reflected in the\nelimination of high-frequency components in the control signal. Tested on a\nreal system, improvements in controller smoothness on a quadrotor drone\nresulted in an almost 80% reduction in power consumption while consistently\ntraining flight-worthy controllers. Project website: this http URL",
          "link": "http://arxiv.org/abs/2012.06644",
          "publishedOn": "2021-05-28T01:42:17.034Z",
          "wordCount": 598,
          "title": "Regularizing Action Policies for Smooth Control with Reinforcement Learning. (arXiv:2012.06644v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banville_H/0/1/0/all/0/1\">Hubert Banville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_S/0/1/0/all/0/1\">Sean U.N. Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aimone_C/0/1/0/all/0/1\">Chris Aimone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engemann_D/0/1/0/all/0/1\">Denis-Alexander Engemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>",
          "description": "Building machine learning models using EEG recorded outside of the laboratory\nsetting requires methods robust to noisy data and randomly missing channels.\nThis need is particularly great when working with sparse EEG montages (1-6\nchannels), often encountered in consumer-grade or mobile EEG devices. Neither\nclassical machine learning models nor deep neural networks trained end-to-end\non EEG are typically designed or tested for robustness to corruption, and\nespecially to randomly missing channels. While some studies have proposed\nstrategies for using data with missing channels, these approaches are not\npractical when sparse montages are used and computing power is limited (e.g.,\nwearables, cell phones). To tackle this problem, we propose dynamic spatial\nfiltering (DSF), a multi-head attention module that can be plugged in before\nthe first layer of a neural network to handle missing EEG channels by learning\nto focus on good channels and to ignore bad ones. We tested DSF on public EEG\ndata encompassing ~4,000 recordings with simulated channel corruption and on a\nprivate dataset of ~100 at-home recordings of mobile EEG with natural\ncorruption. Our proposed approach achieves the same performance as baseline\nmodels when no noise is applied, but outperforms baselines by as much as 29.4%\naccuracy when significant channel corruption is present. Moreover, DSF outputs\nare interpretable, making it possible to monitor channel importance in\nreal-time. This approach has the potential to enable the analysis of EEG in\nchallenging settings where channel corruption hampers the reading of brain\nsignals.",
          "link": "http://arxiv.org/abs/2105.12916",
          "publishedOn": "2021-05-28T01:42:17.025Z",
          "wordCount": 696,
          "title": "Robust learning from corrupted EEG with dynamic spatial filtering. (arXiv:2105.12916v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Likang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongke Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>",
          "description": "In the online innovation market, the fund-raising performance of the start-up\nproject is a concerning issue for creators, investors and platforms.\nUnfortunately, existing studies always focus on modeling the fund-raising\nprocess after the publishment of a project but the predicting of a project\nattraction in the market before setting up is largely unexploited. Usually,\nthis prediction is always with great challenges to making a comprehensive\nunderstanding of both the start-up project and market environment. To that end,\nin this paper, we present a focused study on this important problem from a\nmarket graph perspective. Specifically, we propose a Graph-based Market\nEnvironment (GME) model for predicting the fund-raising performance of the\nunpublished project by exploiting the market environment. In addition, we\ndiscriminatively model the project competitiveness and market preferences by\ndesigning two graph-based neural network architectures and incorporating them\ninto a joint optimization stage. Furthermore, to explore the information\npropagation problem with dynamic environment in a large-scale market graph, we\nextend the GME model with parallelizing competitiveness quantification and\nhierarchical propagation algorithm. Finally, we conduct extensive experiments\non real-world data. The experimental results clearly demonstrate the\neffectiveness of our proposed model.",
          "link": "http://arxiv.org/abs/2105.12918",
          "publishedOn": "2021-05-28T01:42:17.004Z",
          "wordCount": 631,
          "title": "Estimating Fund-Raising Performance for Start-up Projects from a Market Graph Perspective. (arXiv:2105.12918v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.07226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeliang Liu</a>",
          "description": "Despite the increasing importance of strain localization modeling (e.g.,\nfailure analysis) in computer-aided engineering, there is a lack of effective\napproaches to capturing relevant material behaviors consistently across\nmultiple length scales. We aim to address this gap within the framework of deep\nmaterial networks (DMN) -- a machine learning model with embedded mechanics in\nthe building blocks. A new cell-division scheme is proposed to track the scale\ntransition through the network, and its consistency is ensured by the physics\nof fitting parameters. Essentially, each microscale node in the bottom layer is\ndescribed by an ellipsoidal cell with its dimensions back-propagated from the\nmacroscale material point. New crack surfaces in the cell are modeled by\nenriching cohesive layers, and failure algorithms are developed for crack\ninitiation and evolution in the implicit DMN analysis. Besides studies on a\nsingle material point, we apply the multiscale model to concurrent multiscale\nsimulations for the dynamic crush of a particle-reinforced composite tube and\nvarious tests on carbon fiber reinforced polymer composites. For the latter,\nexperimental validations on an off-axis tensile test specimen are also\nprovided.",
          "link": "http://arxiv.org/abs/2101.07226",
          "publishedOn": "2021-05-28T01:42:16.988Z",
          "wordCount": 652,
          "title": "Cell division in deep material networks applied to multiscale strain localization modeling. (arXiv:2101.07226v2 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leo Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xufei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengshan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jinyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiantao Zhou</a>",
          "description": "Deep learning models are known to be vulnerable to adversarial examples that\nare elaborately designed for malicious purposes and are imperceptible to the\nhuman perceptual system. Autoencoder, when trained solely over benign examples,\nhas been widely used for (self-supervised) adversarial detection based on the\nassumption that adversarial examples yield larger reconstruction error.\nHowever, because lacking adversarial examples in its training and the too\nstrong generalization ability of autoencoder, this assumption does not always\nhold true in practice. To alleviate this problem, we explore to detect\nadversarial examples by disentangled representations of images under the\nautoencoder structure. By disentangling input images as class features and\nsemantic features, we train an autoencoder, assisted by a discriminator\nnetwork, over both correctly paired class/semantic features and incorrectly\npaired class/semantic features to reconstruct benign and counterexamples. This\nmimics the behavior of adversarial examples and can reduce the unnecessary\ngeneralization ability of autoencoder. Compared with the state-of-the-art\nself-supervised detection methods, our method exhibits better performance in\nvarious measurements (i.e., AUC, FPR, TPR) over different datasets (MNIST,\nFashion-MNIST and CIFAR-10), different adversarial attack methods (FGSM, BIM,\nPGD, DeepFool, and CW) and different victim models (8-layer CNN and 16-layer\nVGG). We compare our method with the state-of-the-art self-supervised detection\nmethods under different adversarial attacks and different victim models (30\nattack settings), and it exhibits better performance in various measurements\n(AUC, FPR, TPR) for most attacks settings. Ideally, AUC is $1$ and our method\nachieves $0.99+$ on CIFAR-10 for all attacks. Notably, different from other\nAutoencoder-based detectors, our method can provide resistance to the adaptive\nadversary.",
          "link": "http://arxiv.org/abs/2105.03689",
          "publishedOn": "2021-05-28T01:42:16.969Z",
          "wordCount": 726,
          "title": "Self-Supervised Adversarial Example Detection by Disentangled Representation. (arXiv:2105.03689v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Underhill_N/0/1/0/all/0/1\">Ngaire Underhill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinert_A/0/1/0/all/0/1\">Andrew Weinert</a>",
          "description": "The National Airspace System (NAS) is a complex and evolving system that\nenables safe and efficient aviation. Advanced air mobility concepts and new\nairspace entrants, such as unmanned aircraft, must integrate into the NAS\nwithout degrading overall safety or efficiency. For instance, regulations,\nstandards, and systems are required to mitigate the risk of a midair collision\nbetween aircraft. Monte Carlo simulations have been a foundational capability\nfor decades to develop, assess, and certify aircraft conflict avoidance\nsystems. These are often validated through human-in-the-loop experiments and\nflight testing. For many aviation safety studies, manned aircraft behavior is\nrepresented using dynamic Bayesian networks. The original statistical models\nwere developed from 2008-2013 to support safety simulations for altitudes above\n500 feet Above Ground Level (AGL). However, these models were not sufficient to\nassess the safety of smaller UAS operations below 500 feet AGL. In response,\nnewer models with altitude floors below 500 feet AGL have been in development\nsince 2018. Many of the models assume that aircraft behavior is uncorrelated\nand not dependent on air traffic services or nearby aircraft. Our research\nobjective was to compare the various uncorrelated models of conventional\naircraft and identify how the models differ. Particularly if models of\nrotorcraft were sufficiently different than models of fixed-wing aircraft to\nrequire type specific models. The primary contribution is guidance on which\nuncorrelated models to leverage when evaluating the performance of a collision\navoidance system designed for low altitude operations. We also address which\nmodels can be surrogates for noncooperative aircraft without transponders.",
          "link": "http://arxiv.org/abs/2103.04753",
          "publishedOn": "2021-05-28T01:42:16.963Z",
          "wordCount": 728,
          "title": "Applicability and Surrogacy of Uncorrelated Airspace Encounter Models at Low Altitudes. (arXiv:2103.04753v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Siyuan Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier B. Oliva</a>",
          "description": "Time series imputation is a fundamental task for understanding time series\nwith missing data. Existing methods either do not directly handle\nirregularly-sampled data or degrade severely with sparsely observed data. In\nthis work, we reformulate time series as permutation-equivariant sets and\npropose a novel imputation model NRTSI that does not impose any recurrent\nstructures. Taking advantage of the permutation equivariant formulation, we\ndesign a principled and efficient hierarchical imputation procedure. In\naddition, NRTSI can directly handle irregularly-sampled time series, perform\nmultiple-mode stochastic imputation, and handle data with partially observed\ndimensions. Empirically, we show that NRTSI achieves state-of-the-art\nperformance across a wide range of time series imputation benchmarks.",
          "link": "http://arxiv.org/abs/2102.03340",
          "publishedOn": "2021-05-28T01:42:16.955Z",
          "wordCount": 567,
          "title": "NRTSI: Non-Recurrent Time Series Imputation. (arXiv:2102.03340v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Han-Wei Shen</a>",
          "description": "Analyzing particle data plays an important role in many scientific\napplications such as fluid simulation, cosmology simulation and molecular\ndynamics. While there exist methods that can perform feature extraction and\ntracking for volumetric data, performing those tasks for particle data is more\nchallenging because of the lack of explicit connectivity information. Although\none may convert the particle data to volume first, this approach is at risk of\nincurring error and increasing the size of the data. In this paper, we take a\ndeep learning approach to create feature representations for scientific\nparticle data to assist feature extraction and tracking. We employ a deep\nlearning model, which produces latent vectors to represent the relation between\nspatial locations and physical attributes in a local neighborhood. With the\nlatent vectors, features can be extracted by clustering these vectors. To\nachieve fast feature tracking, the mean-shift tracking algorithm is applied in\nthe feature space, which only requires inference of the latent vector for\nselected regions of interest. We validate our approach using two datasets and\ncompare our method with other existing methods.",
          "link": "http://arxiv.org/abs/2105.13240",
          "publishedOn": "2021-05-28T01:42:16.944Z",
          "wordCount": 609,
          "title": "Time Varying Particle Data Feature Extraction and Tracking with Neural Networks. (arXiv:2105.13240v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2009.02252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plachouras_V/0/1/0/all/0/1\">Vassilis Plachouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Challenging problems such as open-domain question answering, fact checking,\nslot filling and entity linking require access to large, external knowledge\nsources. While some models do well on individual tasks, developing general\nmodels is difficult as each task might require computationally expensive\nindexing of custom knowledge sources, in addition to dedicated infrastructure.\nTo catalyze research on models that condition on specific information in large\ntextual resources, we present a benchmark for knowledge-intensive language\ntasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia,\nreducing engineering turnaround through the re-use of components, as well as\naccelerating research into task-agnostic memory architectures. We test both\ntask-specific and general baselines, evaluating downstream performance in\naddition to the ability of the models to provide provenance. We find that a\nshared dense vector index coupled with a seq2seq model is a strong baseline,\noutperforming more tailor-made approaches for fact checking, open-domain\nquestion answering and dialogue, and yielding competitive results on entity\nlinking and slot filling, by generating disambiguated text. KILT data and code\nare available at https://github.com/facebookresearch/KILT.",
          "link": "http://arxiv.org/abs/2009.02252",
          "publishedOn": "2021-05-28T01:42:16.938Z",
          "wordCount": 689,
          "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks. (arXiv:2009.02252v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atre_M/0/1/0/all/0/1\">Medha Atre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_B/0/1/0/all/0/1\">Birendra Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Ashwini Rao</a>",
          "description": "Use of Deep Learning (DL) in commercial applications such as image\nclassification, sentiment analysis and speech recognition is increasing. When\ntraining DL models with large number of parameters and/or large datasets, cost\nand speed of training can become prohibitive. Distributed DL training solutions\nthat split a training job into subtasks and execute them over multiple nodes\ncan decrease training time. However, the cost of current solutions, built\npredominantly for cluster computing systems, can still be an issue. In contrast\nto cluster computing systems, Volunteer Computing (VC) systems can lower the\ncost of computing, but applications running on VC systems have to handle fault\ntolerance, variable network latency and heterogeneity of compute nodes, and the\ncurrent solutions are not designed to do so. We design a distributed solution\nthat can run DL training on a VC system by using a data parallel approach. We\nimplement a novel asynchronous SGD scheme called VC-ASGD suited for VC systems.\nIn contrast to traditional VC systems that lower cost by using untrustworthy\nvolunteer devices, we lower cost by leveraging preemptible computing instances\non commercial cloud platforms. By using preemptible instances that require\napplications to be fault tolerant, we lower cost by 70-90% and improve data\nsecurity.",
          "link": "http://arxiv.org/abs/2103.08894",
          "publishedOn": "2021-05-28T01:42:16.918Z",
          "wordCount": 690,
          "title": "Distributed Deep Learning Using Volunteer Computing-Like Paradigm. (arXiv:2103.08894v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_W/0/1/0/all/0/1\">Wenjie Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Keyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_S/0/1/0/all/0/1\">Simon J.D. Prince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanshuai Cao</a>",
          "description": "It is a common belief that training deep transformers from scratch requires\nlarge datasets. Consequently, for small datasets, people usually use shallow\nand simple additional layers on top of pre-trained models during fine-tuning.\nThis work shows that this does not always need to be the case: with proper\ninitialization and optimization, the benefits of very deep transformers can\ncarry over to challenging tasks with small datasets, including Text-to-SQL\nsemantic parsing and logical reading comprehension. In particular, we\nsuccessfully train $48$ layers of transformers, comprising $24$ fine-tuned\nlayers from pre-trained RoBERTa and $24$ relation-aware layers trained from\nscratch. With fewer training steps and no task-specific pre-training, we obtain\nthe state-of-the-art performance on the challenging cross-domain Text-to-SQL\nparsing benchmark Spider. We achieve this by deriving a novel Data-dependent\nTransformer Fixed-update initialization scheme (DT-Fixup), inspired by the\nprior T-Fixup work. Further error analysis shows that increasing depth can help\nimprove generalization on small datasets for hard cases that require reasoning\nand structural understanding.",
          "link": "http://arxiv.org/abs/2012.15355",
          "publishedOn": "2021-05-28T01:42:16.911Z",
          "wordCount": 645,
          "title": "Optimizing Deeper Transformers on Small Datasets. (arXiv:2012.15355v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "This paper proposes a novel method of learning by predicting view assignments\nwith support samples (PAWS). The method trains a model to minimize a\nconsistency loss, which ensures that different views of the same unlabeled\ninstance are assigned similar pseudo-labels. The pseudo-labels are generated\nnon-parametrically, by comparing the representations of the image views to\nthose of a set of randomly sampled labeled images. The distance between the\nview representations and labeled representations is used to provide a weighting\nover class labels, which we interpret as a soft pseudo-label. By\nnon-parametrically incorporating labeled samples in this way, PAWS extends the\ndistance-metric loss used in self-supervised methods such as BYOL and SwAV to\nthe semi-supervised setting. Despite the simplicity of the approach, PAWS\noutperforms other semi-supervised methods across architectures, setting a new\nstate-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of\nthe labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to\n12x less training than the previous best methods.",
          "link": "http://arxiv.org/abs/2104.13963",
          "publishedOn": "2021-05-28T01:42:16.905Z",
          "wordCount": 641,
          "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Annaswamy_A/0/1/0/all/0/1\">Anuradha M. Annaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_A/0/1/0/all/0/1\">Anubhav Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yingnan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaudio_J/0/1/0/all/0/1\">Joseph E. Gaudio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreu_J/0/1/0/all/0/1\">Jos&#xe9; M. Moreu</a>",
          "description": "This paper considers the problem of real-time control and learning in dynamic\nsystems subjected to uncertainties. Adaptive approaches are proposed to address\nthe problem, which are combined to with methods and tools in Reinforcement\nLearning (RL) and Machine Learning (ML). Algorithms are proposed in\ncontinuous-time that combine adaptive approaches with RL leading to online\ncontrol policies that guarantee stable behavior in the presence of parametric\nuncertainties that occur in real-time. Algorithms are proposed in discrete-time\nthat combine adaptive approaches proposed for parameter and output estimation\nand ML approaches proposed for accelerated performance that guarantee stable\nestimation even in the presence of time-varying regressors, and for accelerated\nlearning of the parameters with persistent excitation. Numerical validations of\nall algorithms are carried out using a quadrotor landing task on a moving\nplatform and benchmark problems in ML. All results clearly point out the\nadvantage of adaptive approaches for real-time control and learning.",
          "link": "http://arxiv.org/abs/2105.06577",
          "publishedOn": "2021-05-28T01:42:16.727Z",
          "wordCount": 612,
          "title": "Online Algorithms and Policies Using Adaptive and Machine Learning Approaches. (arXiv:2105.06577v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuli Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>",
          "description": "For basic machine learning problems, expected error is used to evaluate model\nperformance. Since the distribution of data is usually unknown, we can make\nsimple hypothesis that the data are sampled independently and identically\ndistributed (i.i.d.) and the mean value of loss function is used as the\nempirical risk by Law of Large Numbers (LLN). This is known as the Monte Carlo\nmethod. However, when LLN is not applicable, such as imbalanced data problems,\nempirical risk will cause overfitting and might decrease robustness and\ngeneralization ability. Inspired by the framework of nonlinear expectation\ntheory, we substitute the mean value of loss function with the maximum value of\nsubgroup mean loss. We call it nonlinear Monte Carlo method. In order to use\nnumerical method of optimization, we linearize and smooth the functional of\nmaximum empirical risk and get the descent direction via quadratic programming.\nWith the proposed method, we achieve better performance than SOTA backbone\nmodels with less training steps, and more robustness for basic regression and\nimbalanced classification tasks.",
          "link": "http://arxiv.org/abs/2010.14060",
          "publishedOn": "2021-05-28T01:42:16.716Z",
          "wordCount": 631,
          "title": "Nonlinear Monte Carlo Method for Imbalanced Data Learning. (arXiv:2010.14060v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gheibi_O/0/1/0/all/0/1\">Omid Gheibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyns_D/0/1/0/all/0/1\">Danny Weyns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quin_F/0/1/0/all/0/1\">Federico Quin</a>",
          "description": "Recently, we witness a rapid increase in the use of machine learning in\nself-adaptive systems. Machine learning has been used for a variety of reasons,\nranging from learning a model of the environment of a system during operation\nto filtering large sets of possible configurations before analysing them. While\na body of work on the use of machine learning in self-adaptive systems exists,\nthere is currently no systematic overview of this area. Such overview is\nimportant for researchers to understand the state of the art and direct future\nresearch efforts. This paper reports the results of a systematic literature\nreview that aims at providing such an overview. We focus on self-adaptive\nsystems that are based on a traditional Monitor-Analyze-Plan-Execute feedback\nloop (MAPE). The research questions are centred on the problems that motivate\nthe use of machine learning in self-adaptive systems, the key engineering\naspects of learning in self-adaptation, and open challenges. The search\nresulted in 6709 papers, of which 109 were retained for data collection.\nAnalysis of the collected data shows that machine learning is mostly used for\nupdating adaptation rules and policies to improve system qualities, and\nmanaging resources to better balance qualities and resources. These problems\nare primarily solved using supervised and interactive learning with\nclassification, regression and reinforcement learning as the dominant methods.\nSurprisingly, unsupervised learning that naturally fits automation is only\napplied in a small number of studies. Key open challenges in this area include\nthe performance of learning, managing the effects of learning, and dealing with\nmore complex types of goals. From the insights derived from this systematic\nliterature review we outline an initial design process for applying machine\nlearning in self-adaptive systems that are based on MAPE feedback loops.",
          "link": "http://arxiv.org/abs/2103.04112",
          "publishedOn": "2021-05-28T01:42:16.697Z",
          "wordCount": 750,
          "title": "Applying Machine Learning in Self-Adaptive Systems: A Systematic Literature Review. (arXiv:2103.04112v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bethune_L/0/1/0/all/0/1\">Louis B&#xe9;thune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Sanz_A/0/1/0/all/0/1\">Alberto Gonz&#xe1;lez-Sanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamalet_F/0/1/0/all/0/1\">Franck Mamalet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrurier_M/0/1/0/all/0/1\">Mathieu Serrurier</a>",
          "description": "Lipschitz constrained models have been used to solve specifics deep learning\nproblems such as the estimation of Wasserstein distance for GAN, or the\ntraining of neural networks robust to adversarial attacks. Regardless the novel\nand effective algorithms to build such 1-Lipschitz networks, their usage\nremains marginal, and they are commonly considered as less expressive and less\nable to fit properly the data than their unconstrained counterpart.\n\nThe goal of the paper is to demonstrate that, despite being empirically\nharder to train, 1-Lipschitz neural networks are theoretically better grounded\nthan unconstrained ones when it comes to classification. To achieve that we\nrecall some results about 1-Lipschitz function in the scope of deep learning\nand we extend and illustrate them to derive general properties for\nclassification.\n\nFirst, we show that 1-Lipschitz neural network can fit arbitrarily difficult\nfrontier making them as expressive as classical ones. When minimizing the log\nloss, we prove that the optimization problem under Lipschitz constraint is well\nposed and have a minimum, whereas regular neural networks can diverge even on\nremarkably simple situations. Then, we study the link between classification\nwith 1-Lipschitz network and optimal transport thanks to regularized versions\nof Kantorovich-Rubinstein duality theory. Last, we derive preliminary bounds on\ntheir VC dimension.",
          "link": "http://arxiv.org/abs/2104.05097",
          "publishedOn": "2021-05-28T01:42:16.689Z",
          "wordCount": 685,
          "title": "The Many Faces of 1-Lipschitz Neural Networks. (arXiv:2104.05097v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farina_G/0/1/0/all/0/1\">Gabriele Farina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celli_A/0/1/0/all/0/1\">Andrea Celli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchesi_A/0/1/0/all/0/1\">Alberto Marchesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatti_N/0/1/0/all/0/1\">Nicola Gatti</a>",
          "description": "The existence of simple uncoupled no-regret learning dynamics that converge\nto correlated equilibria in normal-form games is a celebrated result in the\ntheory of multi-agent systems. Specifically, it has been known for more than 20\nyears that when all players seek to minimize their internal regret in a\nrepeated normal-form game, the empirical frequency of play converges to a\nnormal-form correlated equilibrium. Extensive-form games generalize normal-form\ngames by modeling both sequential and simultaneous moves, as well as imperfect\ninformation. Because of the sequential nature and presence of private\ninformation in the game, correlation in extensive-form games possesses\nsignificantly different properties than its counterpart in normal-form games,\nmany of which are still open research directions. Extensive-form correlated\nequilibrium (EFCE) has been proposed as the natural extensive-form counterpart\nto the classical notion of correlated equilibrium in normal-form games.\nCompared to the latter, the constraints that define the set of EFCEs are\nsignificantly more complex, as the correlation device must keep into account\nthe evolution of beliefs of each player as they make observations throughout\nthe game. Due to that significant added complexity, the existence of uncoupled\nlearning dynamics leading to an EFCE has remained a challenging open research\nquestion for a long time. In this article, we settle that question by giving\nthe first uncoupled no-regret dynamics that converge to the set of EFCEs in\nn-player general-sum extensive-form games with perfect recall. We show that\neach iterate can be computed in time polynomial in the size of the game tree,\nand that, when all players play repeatedly according to our learning dynamics,\nthe empirical frequency of play is proven to be a O(T^-0.5)-approximate EFCE\nwith high probability after T game repetitions, and an EFCE almost surely in\nthe limit.",
          "link": "http://arxiv.org/abs/2104.01520",
          "publishedOn": "2021-05-28T01:42:16.682Z",
          "wordCount": 787,
          "title": "Simple Uncoupled No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium. (arXiv:2104.01520v2 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaoyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Libin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1\">Mikhail Belkin</a>",
          "description": "The success of deep learning is due, to a large extent, to the remarkable\neffectiveness of gradient-based optimization methods applied to large neural\nnetworks. The purpose of this work is to propose a modern view and a general\nmathematical framework for loss landscapes and efficient optimization in\nover-parameterized machine learning models and systems of non-linear equations,\na setting that includes over-parameterized deep neural networks. Our starting\nobservation is that optimization problems corresponding to such systems are\ngenerally not convex, even locally. We argue that instead they satisfy PL$^*$,\na variant of the Polyak-Lojasiewicz condition on most (but not all) of the\nparameter space, which guarantees both the existence of solutions and efficient\noptimization by (stochastic) gradient descent (SGD/GD). The PL$^*$ condition of\nthese systems is closely related to the condition number of the tangent kernel\nassociated to a non-linear system showing how a PL$^*$-based non-linear theory\nparallels classical analyses of over-parameterized linear equations. We show\nthat wide neural networks satisfy the PL$^*$ condition, which explains the\n(S)GD convergence to a global minimum. Finally we propose a relaxation of the\nPL$^*$ condition applicable to \"almost\" over-parameterized systems.",
          "link": "http://arxiv.org/abs/2003.00307",
          "publishedOn": "2021-05-28T01:42:16.669Z",
          "wordCount": 671,
          "title": "Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. (arXiv:2003.00307v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strauss_R/0/1/0/all/0/1\">Ryan R. Strauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier B. Oliva</a>",
          "description": "Modeling distributions of covariates, or density estimation, is a core\nchallenge in unsupervised learning. However, the majority of work only\nconsiders the joint distribution, which has limited utility in practical\nsituations. A more general and useful problem is arbitrary conditional density\nestimation, which aims to model any possible conditional distribution over a\nset of covariates, reflecting the more realistic setting of inference based on\nprior knowledge. We propose a novel method, Arbitrary Conditioning with Energy\n(ACE), that can simultaneously estimate the distribution $p(\\mathbf{x}_u \\mid\n\\mathbf{x}_o)$ for all possible subsets of unobserved features $\\mathbf{x}_u$\nand observed features $\\mathbf{x}_o$. ACE is designed to avoid unnecessary bias\nand complexity -- we specify densities with a highly expressive energy function\nand reduce the problem to only learning one-dimensional conditionals (from\nwhich more complex distributions can be recovered during inference). This\nresults in an approach that is both simpler and higher-performing than prior\nmethods. We show that ACE achieves state-of-the-art for arbitrary conditional\nlikelihood estimation and data imputation on standard benchmarks.",
          "link": "http://arxiv.org/abs/2102.04426",
          "publishedOn": "2021-05-28T01:42:16.651Z",
          "wordCount": 611,
          "title": "Arbitrary Conditional Distributions with Energy. (arXiv:2102.04426v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vijayan_N/0/1/0/all/0/1\">Nithia Vijayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+A_P/0/1/0/all/0/1\">Prashanth L. A</a>",
          "description": "We propose two policy gradient algorithms for solving the problem of control\nin an off-policy reinforcement learning (RL) context. Both algorithms\nincorporate a smoothed functional (SF) based gradient estimation scheme. The\nfirst algorithm is a straightforward combination of importance sampling-based\noff-policy evaluation with SF-based gradient estimation. The second algorithm,\ninspired by the stochastic variance-reduced gradient (SVRG) algorithm,\nincorporates variance reduction in the update iteration. For both algorithms,\nwe derive non-asymptotic bounds that establish convergence to an approximate\nstationary point. From these results, we infer that the first algorithm\nconverges at a rate that is comparable to the well-known REINFORCE algorithm in\nan off-policy RL context, while the second algorithm exhibits an improved rate\nof convergence.",
          "link": "http://arxiv.org/abs/2101.02137",
          "publishedOn": "2021-05-28T01:42:16.644Z",
          "wordCount": 579,
          "title": "Smoothed functional-based gradient algorithms for off-policy reinforcement learning: A non-asymptotic viewpoint. (arXiv:2101.02137v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03194",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1\">Xinyu Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yixian Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_L/0/1/0/all/0/1\">Lijun Sun</a>",
          "description": "Missing value problem in spatiotemporal traffic data has long been a\nchallenging topic, in particular for large-scale and high-dimensional data with\ncomplex missing mechanisms and diverse degrees of missingness. Recent studies\nbased on tensor nuclear norm have demonstrated the superiority of tensor\nlearning in imputation tasks by effectively characterizing the complex\ncorrelations/dependencies in spatiotemporal data. However, despite the\npromising results, these approaches do not scale well to large tensors. In this\npaper, we focus on addressing the missing data imputation problem for\nlarge-scale spatiotemporal traffic data. To achieve both high accuracy and\nefficiency, we develop a scalable autoregressive tensor learning model --\nLow-Tubal-Rank Autoregressive Tensor Completion (LATC-Tubal) -- based on the\nexisting framework of Low-Rank Autoregressive Tensor Completion (LATC), which\nis well-suited for spatiotemporal traffic data that characterized by\nmultidimensional structure of location$\\times$ time of day $\\times$ day. In\nparticular, the proposed LATC-Tubal model involves a scalable tensor nuclear\nnorm minimization scheme by integrating linear unitary transformation.\nTherefore, the tensor nuclear norm minimization can be solved by singular value\nthresholding on the transformed matrix of each day while the day-to-day\ncorrelation can be effectively preserved by the unitary transform matrix.\nBefore setting up the experiment, we consider two large-scale 5-minute traffic\nspeed data sets collected by the California PeMS system with 11160 sensors. We\ncompare LATC-Tubal with state-of-the-art baseline models, and find that\nLATC-Tubal can achieve competitively accuracy with a significantly lower\ncomputational cost. In addition, the LATC-Tubal will also benefit other tasks\nin modeling large-scale spatiotemporal traffic data, such as network-level\ntraffic forecasting.",
          "link": "http://arxiv.org/abs/2008.03194",
          "publishedOn": "2021-05-28T01:42:16.638Z",
          "wordCount": 702,
          "title": "Scalable Low-Rank Autoregressive Tensor Learning for Spatiotemporal Traffic Data Imputation. (arXiv:2008.03194v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ritchie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dotzel_J/0/1/0/all/0/1\">Jordan Dotzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanqiu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_P/0/1/0/all/0/1\">Preslav Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiru Zhang</a>",
          "description": "Outliers in weights and activations pose a key challenge for fixed-point\nquantization of neural networks. While they can be addressed by fine-tuning,\nthis is not practical for ML service providers (e.g., Google or Microsoft) who\noften receive customer models without training data. Specialized hardware for\nhandling activation outliers can enable low-precision neural networks, but at\nthe cost of nontrivial area overhead. We instead propose overwrite quantization\n(OverQ), a lightweight hardware technique that opportunistically increases\nbitwidth for activation outliers by overwriting nearby zeros. It has two major\nmodes of operation: range overwrite and precision overwrite. Range overwrite\nreallocates bits to increase the range of outliers, while precision overwrite\nreuses zeros to increase the precision of non-outlier values. Combining range\noverwrite with a simple cascading logic, we handle the vast majority of\noutliers to significantly improve model accuracy at low bitwidth. Our\nexperiments show that with modest cascading, we can consistently handle over\n90% of outliers and achieve +5% ImageNet Top-1 accuracy on a quantized\nResNet-50 at 4 bits. Our ASIC prototype shows OverQ can be implemented\nefficiently on top of existing weight-stationary systolic arrays with small\narea increases per processing element. We imagine this technique can complement\nmodern DNN accelerator designs to provide small increases in accuracy with\ninsignificant area overhead.",
          "link": "http://arxiv.org/abs/1910.06909",
          "publishedOn": "2021-05-28T01:42:16.632Z",
          "wordCount": 680,
          "title": "OverQ: Opportunistic Outlier Quantization for Neural Network Accelerators. (arXiv:1910.06909v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shaoming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1\">Hyo-Sang Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsourdos_A/0/1/0/all/0/1\">Antonios Tsourdos</a>",
          "description": "This paper investigates the problem of impact-time-control and proposes a\nlearning-based computational guidance algorithm to solve this problem. The\nproposed guidance algorithm is developed based on a general\nprediction-correction concept: the exact time-to-go under proportional\nnavigation guidance with realistic aerodynamic characteristics is estimated by\na deep neural network and a biased command to nullify the impact time error is\ndeveloped by utilizing the emerging reinforcement learning techniques. The deep\nneural network is augmented into the reinforcement learning block to resolve\nthe issue of sparse reward that has been observed in typical reinforcement\nlearning formulation. Extensive numerical simulations are conducted to support\nthe proposed algorithm.",
          "link": "http://arxiv.org/abs/2103.05196",
          "publishedOn": "2021-05-28T01:42:16.626Z",
          "wordCount": 567,
          "title": "Computational Impact Time Guidance: A Learning-Based Prediction-Correction Approach. (arXiv:2103.05196v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1\">Soham De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Samuel L. Smith</a>",
          "description": "In computer vision, it is standard practice to draw a single sample from the\ndata augmentation procedure for each unique image in the mini-batch, however it\nis not clear whether this choice is optimal for generalization. In this work,\nwe provide a detailed empirical evaluation of how the number of augmentation\nsamples per unique image influences performance on held out data. Remarkably,\nwe find that drawing multiple samples per image consistently enhances the test\naccuracy achieved for both small and large batch training, despite reducing the\nnumber of unique training examples in each mini-batch. This benefit arises even\nwhen different augmentation multiplicities perform the same number of parameter\nupdates and gradient evaluations. Our results suggest that, although the\nvariance in the gradient estimate arising from subsampling the dataset has an\nimplicit regularization benefit, the variance which arises from the data\naugmentation process harms test accuracy. By applying augmentation multiplicity\nto the recently proposed NFNet model family, we achieve a new ImageNet state of\nthe art of 86.8$\\%$ top-1 w/o extra data.",
          "link": "http://arxiv.org/abs/2105.13343",
          "publishedOn": "2021-05-28T01:42:16.604Z",
          "wordCount": 613,
          "title": "Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error. (arXiv:2105.13343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.02763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuanzhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Ning Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Ke Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qi Qi</a>",
          "description": "Video anomaly detection is commonly used in many applications such as\nsecurity surveillance and is very challenging.A majority of recent video\nanomaly detection approaches utilize deep reconstruction models, but their\nperformance is often suboptimal because of insufficient reconstruction error\ndifferences between normal and abnormal video frames in practice. Meanwhile,\nframe prediction-based anomaly detection methods have shown promising\nperformance. In this paper, we propose a novel and robust unsupervised video\nanomaly detection method by frame prediction with proper design which is more\nin line with the characteristics of surveillance videos. The proposed method is\nequipped with a multi-path ConvGRU-based frame prediction network that can\nbetter handle semantically informative objects and areas of different scales\nand capture spatial-temporal dependencies in normal videos. A noise tolerance\nloss is introduced during training to mitigate the interference caused by\nbackground noise. Extensive experiments have been conducted on the CUHK Avenue,\nShanghaiTech Campus, and UCSD Pedestrian datasets, and the results show that\nour proposed method outperforms existing state-of-the-art approaches.\nRemarkably, our proposed method obtains the frame-level AUROC score of 88.3% on\nthe CUHK Avenue dataset.",
          "link": "http://arxiv.org/abs/2011.02763",
          "publishedOn": "2021-05-28T01:42:16.595Z",
          "wordCount": 672,
          "title": "Robust Unsupervised Video Anomaly Detection by Multi-Path Frame Prediction. (arXiv:2011.02763v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07397",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bongini_P/0/1/0/all/0/1\">Pietro Bongini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bianchini_M/0/1/0/all/0/1\">Monica Bianchini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scarselli_F/0/1/0/all/0/1\">Franco Scarselli</a>",
          "description": "Drug Discovery is a fundamental and ever-evolving field of research. The\ndesign of new candidate molecules requires large amounts of time and money, and\ncomputational methods are being increasingly employed to cut these costs.\nMachine learning methods are ideal for the design of large amounts of potential\nnew candidate molecules, which are naturally represented as graphs. Graph\ngeneration is being revolutionized by deep learning methods, and molecular\ngeneration is one of its most promising applications. In this paper, we\nintroduce a sequential molecular graph generator based on a set of graph neural\nnetwork modules, which we call MG^2N^2. At each step, a node or a group of\nnodes is added to the graph, along with its connections. The modular\narchitecture simplifies the training procedure, also allowing an independent\nretraining of a single module. Sequentiality and modularity make the generation\nprocess interpretable. The use of graph neural networks maximizes the\ninformation in input at each generative step, which consists of the subgraph\nproduced during the previous steps. Experiments of unconditional generation on\nthe QM9 and Zinc datasets show that our model is capable of generalizing\nmolecular patterns seen during the training phase, without overfitting. The\nresults indicate that our method is competitive, and outperforms challenging\nbaselines for unconditional generation.",
          "link": "http://arxiv.org/abs/2012.07397",
          "publishedOn": "2021-05-28T01:42:16.581Z",
          "wordCount": 676,
          "title": "Molecular graph generation with Graph Neural Networks. (arXiv:2012.07397v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nemirovsky_D/0/1/0/all/0/1\">Daniel Nemirovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiebaut_N/0/1/0/all/0/1\">Nicolas Thiebaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ye Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>",
          "description": "The prevalence of machine learning models in various industries has led to\ngrowing demands for model interpretability and for the ability to provide\nmeaningful recourse to users. For example, patients hoping to improve their\ndiagnoses or loan applicants seeking to increase their chances of approval.\nCounterfactuals can help in this regard by identifying input perturbations that\nwould result in more desirable prediction outcomes. Meaningful counterfactuals\nshould be able to achieve the desired outcome, but also be realistic,\nactionable, and efficient to compute. Current approaches achieve desired\noutcomes with moderate actionability but are severely limited in terms of\nrealism and latency. To tackle these limitations, we apply Generative\nAdversarial Nets (GANs) toward counterfactual search. We also introduce a novel\nResidual GAN (RGAN) that helps to improve counterfactual realism and\nactionability compared to regular GANs. The proposed CounteRGAN method utilizes\nan RGAN and a target classifier to produce counterfactuals capable of providing\nmeaningful recourse. Evaluations on two popular datasets highlight how the\nCounteRGAN is able to overcome the limitations of existing methods, including\nlatency improvements of >50x to >90,000x, making meaningful recourse available\nin real-time and applicable to a wide range of domains.",
          "link": "http://arxiv.org/abs/2009.05199",
          "publishedOn": "2021-05-28T01:42:16.572Z",
          "wordCount": 657,
          "title": "CounteRGAN: Generating Realistic Counterfactuals with Residual Generative Adversarial Nets. (arXiv:2009.05199v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Audio tagging is an active research area and has a wide range of\napplications. Since the release of AudioSet, great progress has been made in\nadvancing model performance, which mostly comes from the development of novel\nmodel architectures and attention modules. However, we find that appropriate\ntraining techniques are equally important for building audio tagging models\nwith AudioSet, but have not received the attention they deserve. To fill the\ngap, in this work, we present PSLA, a collection of training techniques that\ncan noticeably boost the model accuracy including ImageNet pretraining,\nbalanced sampling, data augmentation, label enhancement, model aggregation and\ntheir design choices. By training an EfficientNet with these techniques, we\nobtain a single model (with 13.6M parameters) and an ensemble model that\nachieve mean average precision (mAP) scores of 0.444 and 0.474 on AudioSet,\nrespectively, outperforming the previous best system of 0.439 with 81M\nparameters. In addition, our model also achieves a new state-of-the-art mAP of\n0.567 on FSD50K.",
          "link": "http://arxiv.org/abs/2102.01243",
          "publishedOn": "2021-05-28T01:42:16.562Z",
          "wordCount": 624,
          "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation. (arXiv:2102.01243v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaspschak_B/0/1/0/all/0/1\">Bastian Kaspschak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meissner_U/0/1/0/all/0/1\">Ulf-G. Mei&#xdf;ner</a>",
          "description": "Deep Learning using the eponymous deep neural networks (DNNs) has become an\nattractive approach towards various data-based problems of theoretical physics\nin the past decade. There has been a clear trend to deeper architectures\ncontaining increasingly more powerful and involved layers. Contrarily, Taylor\ncoefficients of DNNs still appear mainly in the light of interpretability\nstudies, where they are computed at most to first order. However, especially in\ntheoretical physics numerous problems benefit from accessing higher orders, as\nwell. This gap motivates a general formulation of neural network (NN) Taylor\nexpansions. Restricting our analysis to multilayer perceptrons (MLPs) and\nintroducing quantities we refer to as propagators and vertices, both depending\non the MLP's weights and biases, we establish a graph-theoretical approach.\nSimilarly to Feynman rules in quantum field theories, we can systematically\nassign diagrams containing propagators and vertices to the corresponding\npartial derivative. Examining this approach for S-wave scattering lengths of\nshallow potentials, we observe NNs to adapt their derivatives mainly to the\nleading order of the target function's Taylor expansion. To circumvent this\nproblem, we propose an iterative NN perturbation theory. During each iteration\nwe eliminate the leading order, such that the next-to-leading order can be\nfaithfully learned during the subsequent iteration. After performing two\niterations, we find that the first- and second-order Born terms are correctly\nadapted during the respective iterations. Finally, we combine both results to\nfind a proxy that acts as a machine-learned second-order Born approximation.",
          "link": "http://arxiv.org/abs/2009.03192",
          "publishedOn": "2021-05-28T01:42:16.555Z",
          "wordCount": 724,
          "title": "A Neural Network Perturbation Theory Based on the Born Series. (arXiv:2009.03192v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13348",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hsieh_Y/0/1/0/all/0/1\">Yu-Guan Hsieh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Iutzeler_F/0/1/0/all/0/1\">Franck Iutzeler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Malick_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Malick</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>",
          "description": "In networks of autonomous agents (e.g., fleets of vehicles, scattered\nsensors), the problem of minimizing the sum of the agents' local functions has\nreceived a lot of interest. We tackle here this distributed optimization\nproblem in the case of open networks when agents can join and leave the network\nat any time. Leveraging recent online optimization techniques, we propose and\nanalyze the convergence of a decentralized asynchronous optimization method for\nopen networks.",
          "link": "http://arxiv.org/abs/2105.13348",
          "publishedOn": "2021-05-28T01:42:16.534Z",
          "wordCount": 504,
          "title": "Optimization in Open Networks via Dual Averaging. (arXiv:2105.13348v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2005.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoshu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jianbin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Rui Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makoto_O/0/1/0/all/0/1\">Onizuka Makoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_Y/0/1/0/all/0/1\">Yoshiharu Ishikawa</a>",
          "description": "Selectivity estimation aims at estimating the number of database objects that\nsatisfy a selection criterion. Answering this problem accurately and\nefficiently is essential to many applications, such as density estimation,\noutlier detection, query optimization, and data integration. The estimation\nproblem is especially challenging for large-scale high-dimensional data due to\nthe curse of dimensionality, the large variance of selectivity across different\nqueries, and the need to make the estimator consistent (i.e., the selectivity\nis non-decreasing in the threshold). We propose a new deep learning-based model\nthat learns a query-dependent piecewise linear function as selectivity\nestimator, which is flexible to fit the selectivity curve of any distance\nfunction and query object, while guaranteeing that the output is non-decreasing\nin the threshold. To improve the accuracy for large datasets, we propose to\npartition the dataset into multiple disjoint subsets and build a local model on\neach of them. We perform experiments on real datasets and show that the\nproposed model consistently outperforms state-of-the-art models in accuracy in\nan efficient way and is useful for real applications.",
          "link": "http://arxiv.org/abs/2005.09908",
          "publishedOn": "2021-05-28T01:42:16.528Z",
          "wordCount": 669,
          "title": "Consistent and Flexible Selectivity Estimation for High-Dimensional Data. (arXiv:2005.09908v4 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>",
          "description": "Pre-trained language models such as BERT have exhibited remarkable\nperformances in many tasks in natural language understanding (NLU). The tokens\nin the models are usually fine-grained in the sense that for languages like\nEnglish they are words or sub-words and for languages like Chinese they are\ncharacters. In English, for example, there are multi-word expressions which\nform natural lexical units and thus the use of coarse-grained tokenization also\nappears to be reasonable. In fact, both fine-grained and coarse-grained\ntokenizations have advantages and disadvantages for learning of pre-trained\nlanguage models. In this paper, we propose a novel pre-trained language model,\nreferred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained\nand coarse-grained tokenizations. For English, AMBERT takes both the sequence\nof words (fine-grained tokens) and the sequence of phrases (coarse-grained\ntokens) as input after tokenization, employs one encoder for processing the\nsequence of words and the other encoder for processing the sequence of the\nphrases, utilizes shared parameters between the two encoders, and finally\ncreates a sequence of contextualized representations of the words and a\nsequence of contextualized representations of the phrases. Experiments have\nbeen conducted on benchmark datasets for Chinese and English, including CLUE,\nGLUE, SQuAD and RACE. The results show that AMBERT can outperform BERT in all\ncases, particularly the improvements are significant for Chinese. We also\ndevelop a method to improve the efficiency of AMBERT in inference, which still\nperforms better than BERT with the same computational cost as BERT.",
          "link": "http://arxiv.org/abs/2008.11869",
          "publishedOn": "2021-05-28T01:42:16.521Z",
          "wordCount": 755,
          "title": "AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization. (arXiv:2008.11869v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.01452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melis_M/0/1/0/all/0/1\">Marco Melis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scalas_M/0/1/0/all/0/1\">Michele Scalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiorca_D/0/1/0/all/0/1\">Davide Maiorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giacinto_G/0/1/0/all/0/1\">Giorgio Giacinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>",
          "description": "While machine-learning algorithms have demonstrated a strong ability in\ndetecting Android malware, they can be evaded by sparse evasion attacks crafted\nby injecting a small set of fake components, e.g., permissions and system\ncalls, without compromising intrusive functionality. Previous work has shown\nthat, to improve robustness against such attacks, learning algorithms should\navoid overemphasizing few discriminant features, providing instead decisions\nthat rely upon a large subset of components. In this work, we investigate\nwhether gradient-based attribution methods, used to explain classifiers'\ndecisions by identifying the most relevant features, can be used to help\nidentify and select more robust algorithms. To this end, we propose to exploit\ntwo different metrics that represent the evenness of explanations, and a new\ncompact security measure called Adversarial Robustness Metric. Our experiments\nconducted on two different datasets and five classification algorithms for\nAndroid malware detection show that a strong connection exists between the\nuniformity of explanations and adversarial robustness. In particular, we found\nthat popular techniques like Gradient*Input and Integrated Gradients are\nstrongly correlated to security when applied to both linear and nonlinear\ndetectors, while more elementary explanation techniques like the simple\nGradient do not provide reliable information about the robustness of such\nclassifiers.",
          "link": "http://arxiv.org/abs/2005.01452",
          "publishedOn": "2021-05-28T01:42:16.512Z",
          "wordCount": 676,
          "title": "Do Gradient-based Explanations Tell Anything About Adversarial Robustness to Android Malware?. (arXiv:2005.01452v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.07790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stacey_J/0/1/0/all/0/1\">Joe Stacey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubossarsky_H/0/1/0/all/0/1\">Haim Dubossarsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>",
          "description": "Natural Language Inference (NLI) datasets contain annotation artefacts\nresulting in spurious correlations between the natural language utterances and\ntheir respective entailment classes. These artefacts are exploited by neural\nnetworks even when only considering the hypothesis and ignoring the premise,\nleading to unwanted biases. Belinkov et al. (2019b) proposed tackling this\nproblem via adversarial training, but this can lead to learned sentence\nrepresentations that still suffer from the same biases. We show that the bias\ncan be reduced in the sentence representations by using an ensemble of\nadversaries, encouraging the model to jointly decrease the accuracy of these\ndifferent adversaries while fitting the data. This approach produces more\nrobust NLI models, outperforming previous de-biasing efforts when generalised\nto 12 other datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In\naddition, we find that the optimal number of adversarial classifiers depends on\nthe dimensionality of the sentence representations, with larger sentence\nrepresentations being more difficult to de-bias while benefiting from using a\ngreater number of adversaries.",
          "link": "http://arxiv.org/abs/2004.07790",
          "publishedOn": "2021-05-28T01:42:16.505Z",
          "wordCount": 674,
          "title": "Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training. (arXiv:2004.07790v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spadon_G/0/1/0/all/0/1\">Gabriel Spadon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Shenda Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandoli_B/0/1/0/all/0/1\">Bruno Brandoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1\">Stan Matwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+F%2E_J/0/1/0/all/0/1\">Jose F. Rodrigues-Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>",
          "description": "Time-series forecasting is one of the most active research topics in\nartificial intelligence. Applications in real-world time series should consider\ntwo factors for achieving reliable predictions: modeling dynamic dependencies\namong multiple variables and adjusting the model's intrinsic hyperparameters. A\nstill open gap in that literature is that statistical and ensemble learning\napproaches systematically present lower predictive performance than deep\nlearning methods. They generally disregard the data sequence aspect entangled\nwith multivariate data represented in more than one time series. Conversely,\nthis work presents a novel neural network architecture for time-series\nforecasting that combines the power of graph evolution with deep recurrent\nlearning on distinct data distributions; we named our method Recurrent Graph\nEvolution Neural Network (ReGENN). The idea is to infer multiple multivariate\nrelationships between co-occurring time-series by assuming that the temporal\ndata depends not only on inner variables and intra-temporal relationships\n(i.e., observations from itself) but also on outer variables and inter-temporal\nrelationships (i.e., observations from other-selves). An extensive set of\nexperiments was conducted comparing ReGENN with dozens of ensemble methods and\nclassical statistical ones, showing sound improvement of up to 64.87% over the\ncompeting algorithms. Furthermore, we present an analysis of the intermediate\nweights arising from ReGENN, showing that by looking at inter and\nintra-temporal relationships simultaneously, time-series forecasting is majorly\nimproved if paying attention to how multiple multivariate data synchronously\nevolve.",
          "link": "http://arxiv.org/abs/2008.12833",
          "publishedOn": "2021-05-28T01:42:16.485Z",
          "wordCount": 750,
          "title": "Pay Attention to Evolution: Time Series Forecasting with Deep Graph-Evolution Learning. (arXiv:2008.12833v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.03276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>",
          "description": "The ability to transfer in reinforcement learning is key towards building an\nagent of general artificial intelligence. In this paper, we consider the\nproblem of learning to simultaneously transfer across both environments (ENV)\nand tasks (TASK), probably more importantly, by learning from only sparse (ENV,\nTASK) pairs out of all the possible combinations. We propose a novel\ncompositional neural network architecture which depicts a meta rule for\ncomposing policies from the environment and task embeddings. Notably, one of\nthe main challenges is to learn the embeddings jointly with the meta rule. We\nfurther propose new training methods to disentangle the embeddings, making them\nboth distinctive signatures of the environments and tasks and effective\nbuilding blocks for composing the policies. Experiments on GridWorld and Thor,\nof which the agent takes as input an egocentric view, show that our approach\ngives rise to high success rates on all the (ENV, TASK) pairs after learning\nfrom only 40% of them.",
          "link": "http://arxiv.org/abs/1904.03276",
          "publishedOn": "2021-05-28T01:42:16.479Z",
          "wordCount": 626,
          "title": "Synthesized Policies for Transfer and Adaptation across Tasks and Environments. (arXiv:1904.03276v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1\">Ashok C. Popat</a>",
          "description": "Paragraphs are an important class of document entities. We propose a new\napproach for paragraph identification by spatial graph convolutional neural\nnetworks (GCN) applied on OCR text boxes. Two steps, namely line splitting and\nline clustering, are performed to extract paragraphs from the lines in OCR\nresults. Each step uses a beta-skeleton graph constructed from bounding boxes,\nwhere the graph edges provide efficient support for graph convolution\noperations. With only pure layout input features, the GCN model size is 3~4\norders of magnitude smaller compared to R-CNN based models, while achieving\ncomparable or better accuracies on PubLayNet and other datasets. Furthermore,\nthe GCN models show good generalization from synthetic training data to\nreal-world images, and good adaptivity for variable document styles.",
          "link": "http://arxiv.org/abs/2101.12741",
          "publishedOn": "2021-05-28T01:42:16.472Z",
          "wordCount": 592,
          "title": "General-Purpose OCR Paragraph Identification by Graph Convolutional Neural Networks. (arXiv:2101.12741v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pelegrina_G/0/1/0/all/0/1\">Guilherme D. Pelegrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brotto_R/0/1/0/all/0/1\">Renan D. B. Brotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_L/0/1/0/all/0/1\">Leonardo T. Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attux_R/0/1/0/all/0/1\">Romis Attux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_J/0/1/0/all/0/1\">Jo&#xe3;o M. T. Romano</a>",
          "description": "In dimension reduction problems, the adopted technique may produce\ndisparities between the representation errors of two or more different groups.\nFor instance, in the projected space, a specific class can be better\nrepresented in comparison with the other ones. Depending on the situation, this\nunfair result may introduce ethical concerns. Aiming at overcoming this\ninconvenience, a fairness measure can be considered when performing dimension\nreduction through Principal Component Analysis. However, a solution that\nincreases fairness tends to increase the reconstruction error. In other words,\nthere is a trade-off between equity and performance. In this context, this\npaper proposes to address this trade-off in Fair Principal Component Analysis\nproblems by means of a multi-objective-based approach. For this purpose, we\nadopt a fairness measure associated with the disparity between the\nrepresentation errors of different groups. Moreover, we investigate if the\nsolution of a classical Principal Component Analysis can be used to find a fair\nprojection. Numerical experiments attest that a fairer result can be achieved\nwith a very small loss in the reconstruction error.",
          "link": "http://arxiv.org/abs/2006.06137",
          "publishedOn": "2021-05-28T01:42:16.465Z",
          "wordCount": 642,
          "title": "A novel multi-objective-based approach to analyze trade-offs in Fair Principal Component Analysis. (arXiv:2006.06137v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cervino_J/0/1/0/all/0/1\">Juan Cervino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazerque_J/0/1/0/all/0/1\">Juan Andres Bazerque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calvo_Fullana_M/0/1/0/all/0/1\">Miguel Calvo-Fullana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "In this paper we consider a problem known as multi-task learning, consisting\nof fitting a set of classifier or regression functions intended for solving\ndifferent tasks. In our novel formulation, we couple the parameters of these\nfunctions, so that they learn in their task specific domains while staying\nclose to each other. This facilitates cross-fertilization in which data\ncollected across different domains help improving the learning performance at\neach other task. First, we present a simplified case in which the goal is to\nestimate the means of two Gaussian variables, for the purpose of gaining some\ninsights on the advantage of the proposed cross-learning strategy. Then we\nprovide a stochastic projected gradient algorithm to perform cross-learning\nover a generic loss function. If the number of parameters is large, then the\nprojection step becomes computationally expensive. To avoid this situation, we\nderive a primal-dual algorithm that exploits the structure of the dual problem,\nachieving a formulation whose complexity only depends on the number of tasks.\nPreliminary numerical experiments for image classification by neural networks\ntrained on a dataset divided in different domains corroborate that the\ncross-learned function outperforms both the task-specific and the consensus\napproaches.",
          "link": "http://arxiv.org/abs/2010.12993",
          "publishedOn": "2021-05-28T01:42:16.458Z",
          "wordCount": 660,
          "title": "Multi-task Supervised Learning via Cross-learning. (arXiv:2010.12993v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.04211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karmakar_S/0/1/0/all/0/1\">Sayar Karmakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Anirbit Mukherjee</a>",
          "description": "In this work, we demonstrate provable guarantees on the training of depth-$2$\nneural networks in new regimes than previously explored. (1) First we give a\nsimple stochastic algorithm that can train a $\\rm ReLU$ gate in the realizable\nsetting in linear time while using significantly milder conditions on the data\ndistribution than previous results. Leveraging some additional distributional\nassumptions we also show approximate recovery of the true label generating\nparameters when training a $\\rm ReLU$ gate while a probabilistic adversary is\nallowed to corrupt the true labels of the training data. Our guarantee on\nrecovering the true weight degrades gracefully with increasing probability of\nattack and it's nearly optimal in the worst case. Additionally, our analysis\nallows for mini-batching and computes how the convergence time scales with the\nmini-batch size. (2) Secondly, we focus on the question of provable\ninterpolation of arbitrary data by finitely large neural nets. We exhibit a\nnon-gradient iterative algorithm \"${\\rm Neuro{-}Tron}$\" which gives a\nfirst-of-its-kind poly-time approximate solving of a neural regression (here in\nthe $\\ell_\\infty$-norm) problem at finite net widths and for non-realizable\ndata.",
          "link": "http://arxiv.org/abs/2005.04211",
          "publishedOn": "2021-05-28T01:42:16.437Z",
          "wordCount": 689,
          "title": "A Study of Neural Training with Iterative Non-Gradient Methods. (arXiv:2005.04211v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.07822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shining Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wanli Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenkun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xianglin Zuo</a>",
          "description": "Mining causality from text is a complex and crucial natural language\nunderstanding task corresponding to the human cognition. Existing studies at\nits solution can be grouped into two primary categories: feature engineering\nbased and neural model based methods. In this paper, we find that the former\nhas incomplete coverage and inherent errors but provide prior knowledge; while\nthe latter leverages context information but causal inference of which is\ninsufficiency. To handle the limitations, we propose a novel causality\ndetection model named MCDN to explicitly model causal reasoning process, and\nfurthermore, to exploit the advantages of both methods. Specifically, we adopt\nmulti-head self-attention to acquire semantic feature at word level and develop\nthe SCRN to infer causality at segment level. To the best of our knowledge,\nwith regards to the causality tasks, this is the first time that the Relation\nNetwork is applied. The experimental results show that: 1) the proposed\napproach performs prominent performance on causality detection; 2) further\nanalysis manifests the effectiveness and robustness of MCDN.",
          "link": "http://arxiv.org/abs/1908.07822",
          "publishedOn": "2021-05-28T01:42:16.427Z",
          "wordCount": 651,
          "title": "A Multi-level Neural Network for Implicit Causality Detection in Web Texts. (arXiv:1908.07822v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Denoeux_T/0/1/0/all/0/1\">Thierry Denoeux</a>",
          "description": "Evidential clustering is an approach to clustering based on the use of\nDempster-Shafer mass functions to represent cluster-membership uncertainty. In\nthis paper, we introduce a neural-network based evidential clustering\nalgorithm, called NN-EVCLUS, which learns a mapping from attribute vectors to\nmass functions, in such a way that more similar inputs are mapped to output\nmass functions with a lower degree of conflict. The neural network can be\npaired with a one-class support vector machine to make it robust to outliers\nand allow for novelty detection. The network is trained to minimize the\ndiscrepancy between dissimilarities and degrees of conflict for all or some\nobject pairs. Additional terms can be added to the loss function to account for\npairwise constraints or labeled data, which can also be used to adapt the\nmetric. Comparative experiments show the superiority of N-EVCLUS over\nstate-of-the-art evidential clustering algorithms for a range of unsupervised\nand constrained clustering tasks involving both attribute and dissimilarity\ndata.",
          "link": "http://arxiv.org/abs/2009.12795",
          "publishedOn": "2021-05-28T01:42:16.419Z",
          "wordCount": 616,
          "title": "NN-EVCLUS: Neural Network-based Evidential Clustering. (arXiv:2009.12795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casado_F/0/1/0/all/0/1\">Fernando E. Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lema_D/0/1/0/all/0/1\">Dylan Lema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Criado_M/0/1/0/all/0/1\">Marcos F. Criado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_R/0/1/0/all/0/1\">Roberto Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regueiro_C/0/1/0/all/0/1\">Carlos V. Regueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barro_S/0/1/0/all/0/1\">Sen&#xe9;n Barro</a>",
          "description": "Smart devices, such as smartphones, wearables, robots, and others, can\ncollect vast amounts of data from their environment. This data is suitable for\ntraining machine learning models, which can significantly improve their\nbehavior, and therefore, the user experience. Federated learning is a young and\npopular framework that allows multiple distributed devices to train deep\nlearning models collaboratively while preserving data privacy. Nevertheless,\nthis approach may not be optimal for scenarios where data distribution is\nnon-identical among the participants or changes over time, causing what is\nknown as concept drift. Little research has yet been done in this field, but\nthis kind of situation is quite frequent in real life and poses new challenges\nto both continual and federated learning. Therefore, in this work, we present a\nnew method, called Concept-Drift-Aware Federated Averaging (CDA-FedAvg). Our\nproposal is an extension of the most popular federated algorithm, Federated\nAveraging (FedAvg), enhancing it for continual adaptation under concept drift.\nWe empirically demonstrate the weaknesses of regular FedAvg and prove that\nCDA-FedAvg outperforms it in this type of scenario.",
          "link": "http://arxiv.org/abs/2105.13309",
          "publishedOn": "2021-05-28T01:42:16.412Z",
          "wordCount": 609,
          "title": "Concept drift detection and adaptation for federated and continual learning. (arXiv:2105.13309v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.14322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhilin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>",
          "description": "In massive multiple-input multiple-output (MIMO) system, user equipment (UE)\nneeds to send downlink channel state information (CSI) back to base station\n(BS). However, the feedback becomes expensive with the growing complexity of\nCSI in massive MIMO system. Recently, deep learning (DL) approaches are used to\nimprove the reconstruction efficiency of CSI feedback. In this paper, a novel\nfeedback network named CRNet is proposed to achieve better performance via\nextracting CSI features on multiple resolutions. An advanced training scheme\nthat further boosts the network performance is also introduced. Simulation\nresults show that the proposed CRNet outperforms the state-of-the-art CsiNet\nunder the same computational complexity without any extra information. The open\nsource codes are available at https://github.com/Kylin9511/CRNet",
          "link": "http://arxiv.org/abs/1910.14322",
          "publishedOn": "2021-05-28T01:42:16.405Z",
          "wordCount": 605,
          "title": "Multi-resolution CSI Feedback with deep learning in Massive MIMO System. (arXiv:1910.14322v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1\">Brandon Amos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanton_S/0/1/0/all/0/1\">Samuel Stanton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1\">Denis Yarats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "For over a decade, model-based reinforcement learning has been seen as a way\nto leverage control-based domain knowledge to improve the sample-efficiency of\nreinforcement learning agents. While model-based agents are conceptually\nappealing, their policies tend to lag behind those of model-free agents in\nterms of final reward, especially in non-trivial environments. In response,\nresearchers have proposed model-based agents with increasingly complex\ncomponents, from ensembles of probabilistic dynamics models, to heuristics for\nmitigating model error. In a reversal of this trend, we show that simple\nmodel-based agents can be derived from existing ideas that not only match, but\noutperform state-of-the-art model-free agents in terms of both\nsample-efficiency and final reward. We find that a model-free soft value\nestimate for policy evaluation and a model-based stochastic value gradient for\npolicy improvement is an effective combination, achieving state-of-the-art\nresults on a high-dimensional humanoid control task, which most model-based\nagents are unable to solve. Our findings suggest that model-based policy\nevaluation deserves closer attention.",
          "link": "http://arxiv.org/abs/2008.12775",
          "publishedOn": "2021-05-28T01:42:16.386Z",
          "wordCount": 642,
          "title": "On the model-based stochastic value gradient for continuous reinforcement learning. (arXiv:2008.12775v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13320",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+DeLise_T/0/1/0/all/0/1\">Timothy DeLise</a>",
          "description": "This research investigates pricing financial options based on the traditional\nmartingale theory of arbitrage pricing applied to neural SDEs. We treat neural\nSDEs as universal It\\^o process approximators. In this way we can lift all\nassumptions on the form of the underlying price process, and compute\ntheoretical option prices numerically. We propose a variation of the SDE-GAN\napproach by implementing the Wasserstein distance metric as a loss function for\ntraining. Furthermore, it is conjectured that the error of the option price\nimplied by the learnt model can be bounded by the very Wasserstein distance\nmetric that was used to fit the empirical data.",
          "link": "http://arxiv.org/abs/2105.13320",
          "publishedOn": "2021-05-28T01:42:16.379Z",
          "wordCount": 532,
          "title": "Neural Options Pricing. (arXiv:2105.13320v1 [q-fin.MF])"
        },
        {
          "id": "http://arxiv.org/abs/1906.04608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kubota_T/0/1/0/all/0/1\">Tomoyuki Kubota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_K/0/1/0/all/0/1\">Kohei Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_H/0/1/0/all/0/1\">Hirokazu Takahashi</a>",
          "description": "A dynamical system can be regarded as an information processing apparatus\nthat encodes input streams from the external environment to its state and\nprocesses them through state transitions. The information processing capacity\n(IPC) is an excellent tool that comprehensively evaluates these processed\ninputs, providing details of unknown information processing in black box\nsystems; however, this measure can be applied to only time-invariant systems.\nThis paper extends the applicable range to time-variant systems and further\nreveals that the IPC is equivalent to coefficients of polynomial chaos (PC)\nexpansion in more general dynamical systems. To achieve this objective, we\ntackle three issues. First, we establish a connection between the IPC for\ntime-invariant systems and PC expansion, which is a type of polynomial\nexpansion using orthogonal functions of input history as bases. We prove that\nthe IPC corresponds to the squared norm of the coefficient vector of the basis\nin the PC expansion. Second, we show that an input following an arbitrary\ndistribution can be used for the IPC, removing previous restrictions to\nspecific input distributions. Third, we extend the conventional orthogonal\nbases to functions of both time and input history and propose the IPC for\ntime-variant systems. To show the significance of our approach, we demonstrate\nthat our measure can reveal information representations in not only machine\nlearning networks but also a real, cultured neural network. Our generalized\nmeasure paves the way for unveiling the information processing capabilities of\na wide variety of physical dynamics which has been left behind in nature.",
          "link": "http://arxiv.org/abs/1906.04608",
          "publishedOn": "2021-05-28T01:42:16.372Z",
          "wordCount": 733,
          "title": "A Unifying Framework for Information Processing in Stochastically Driven Dynamical Systems. (arXiv:1906.04608v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13304",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_P/0/1/0/all/0/1\">Puhan Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chern_G/0/1/0/all/0/1\">Gia-Wei Chern</a>",
          "description": "We show that the celebrated Falicov-Kimball model exhibits rich and\nintriguing phase-ordering dynamics. Applying modern machine learning methods to\nenable large-scale quantum kinetic Monte Carlo simulations, we uncover an\nunusual phase-separation scenario in which the growth of charge checkerboard\nclusters competes with domain coarsening related to a hidden symmetry-breaking.\nA self-trapping mechanism as a result of this competition gives rise to\narrested growth of checkerboard patterns and their super-clusters. Glassy\nbehaviors similar to the one reported in this work could be generic for other\ncorrelated electron systems.",
          "link": "http://arxiv.org/abs/2105.13304",
          "publishedOn": "2021-05-28T01:42:16.364Z",
          "wordCount": 540,
          "title": "Anomalous phase separation and hidden coarsening of super-clusters in the Falicov-Kimball model. (arXiv:2105.13304v1 [cond-mat.str-el])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_A/0/1/0/all/0/1\">Abdallah Moubayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shami_A/0/1/0/all/0/1\">Abdallah Shami</a>",
          "description": "Modern vehicles, including connected vehicles and autonomous vehicles,\nnowadays involve many electronic control units connected through intra-vehicle\nnetworks to implement various functionalities and perform actions. Modern\nvehicles are also connected to external networks through vehicle-to-everything\ntechnologies, enabling their communications with other vehicles,\ninfrastructures, and smart devices. However, the improving functionality and\nconnectivity of modern vehicles also increase their vulnerabilities to\ncyber-attacks targeting both intra-vehicle and external networks due to the\nlarge attack surfaces. To secure vehicular networks, many researchers have\nfocused on developing intrusion detection systems (IDSs) that capitalize on\nmachine learning methods to detect malicious cyber-attacks. In this paper, the\nvulnerabilities of intra-vehicle and external networks are discussed, and a\nmulti-tiered hybrid IDS that incorporates a signature-based IDS and an\nanomaly-based IDS is proposed to detect both known and unknown attacks on\nvehicular networks. Experimental results illustrate that the proposed system\ncan detect various types of known attacks with 99.99% accuracy on the\nCAN-intrusion-dataset representing the intra-vehicle network data and 99.88%\naccuracy on the CICIDS2017 dataset illustrating the external vehicular network\ndata. For the zero-day attack detection, the proposed system achieves high\nF1-scores of 0.963 and 0.800 on the above two datasets, respectively. The\naverage processing time of each data packet on a vehicle-level machine is less\nthan 0.6 ms, which shows the feasibility of implementing the proposed system in\nreal-time vehicle systems. This emphasizes the effectiveness and efficiency of\nthe proposed IDS.",
          "link": "http://arxiv.org/abs/2105.13289",
          "publishedOn": "2021-05-28T01:42:16.192Z",
          "wordCount": 707,
          "title": "MTH-IDS: A Multi-Tiered Hybrid Intrusion Detection System for Internet of Vehicles. (arXiv:2105.13289v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galimberti_C/0/1/0/all/0/1\">Clara Luc&#xed;a Galimberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furieri_L/0/1/0/all/0/1\">Luca Furieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_Trecate_G/0/1/0/all/0/1\">Giancarlo Ferrari-Trecate</a>",
          "description": "Deep Neural Networks (DNNs) training can be difficult due to vanishing and\nexploding gradients during weight optimization through backpropagation. To\naddress this problem, we propose a general class of Hamiltonian DNNs (H-DNNs)\nthat stem from the discretization of continuous-time Hamiltonian systems and\ninclude several existing architectures based on ordinary differential\nequations. Our main result is that a broad set of H-DNNs ensures non-vanishing\ngradients by design for an arbitrary network depth. This is obtained by proving\nthat, using a semi-implicit Euler discretization scheme, the backward\nsensitivity matrices involved in gradient computations are symplectic. We also\nprovide an upper bound to the magnitude of sensitivity matrices, and show that\nexploding gradients can be either controlled through regularization or avoided\nfor special architectures. Finally, we enable distributed implementations of\nbackward and forward propagation algorithms in H-DNNs by characterizing\nappropriate sparsity constraints on the weight matrices. The good performance\nof H-DNNs is demonstrated on benchmark classification problems, including image\nclassification with the MNIST dataset.",
          "link": "http://arxiv.org/abs/2105.13205",
          "publishedOn": "2021-05-28T01:42:16.173Z",
          "wordCount": 597,
          "title": "Hamiltonian Deep Neural Networks Guaranteeing Non-vanishing Gradients by Design. (arXiv:2105.13205v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_L/0/1/0/all/0/1\">Lara Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elster_C/0/1/0/all/0/1\">Clemens Elster</a>",
          "description": "Deep ensembles can be seen as the current state-of-the-art for uncertainty\nquantification in deep learning. While the approach was originally proposed as\nan non-Bayesian technique, arguments towards its Bayesian footing have been put\nforward as well. We show that deep ensembles can be viewed as an approximate\nBayesian method by specifying the corresponding assumptions. Our finding leads\nto an improved approximation which results in an increased epistemic part of\nthe uncertainty. Numerical examples suggest that the improved approximation can\nlead to more reliable uncertainties. Analytical derivations ensure easy\ncalculation of results.",
          "link": "http://arxiv.org/abs/2105.13283",
          "publishedOn": "2021-05-28T01:42:16.166Z",
          "wordCount": 515,
          "title": "Deep Ensembles from a Bayesian Perspective. (arXiv:2105.13283v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Novac_P/0/1/0/all/0/1\">Pierre-Emmanuel Novac</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hacene_G/0/1/0/all/0/1\">Ghouthi Boukli Hacene</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Pegatoquet_A/0/1/0/all/0/1\">Alain Pegatoquet</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Miramond_B/0/1/0/all/0/1\">Beno&#xee;t Miramond</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gripon_V/0/1/0/all/0/1\">Vincent Gripon</a> (2) ((1) Universit&#xe9; C&#xf4;te d&#x27;Azur, CNRS, LEAT, Sophia Antipolis, France, (2) IMT Atlantique, Brest, France, (3) MILA, Montreal, Canada)",
          "description": "Embedding Artificial Intelligence onto low-power devices is a challenging\ntask that has been partly overcome with recent advances in machine learning and\nhardware design. Presently, deep neural networks can be deployed on embedded\ntargets to perform different tasks such as speech recognition,object detection\nor Human Activity Recognition. However, there is still room for optimization of\ndeep neural networks onto embedded devices. These optimizations mainly address\npower consumption,memory and real-time constraints, but also an easier\ndeployment at the edge. Moreover, there is still a need for a better\nunderstanding of what can be achieved for different use cases. This work\nfocuses on quantization and deployment of deep neural networks onto low-power\n32-bit microcontrollers. The quantization methods, relevant in the context of\nan embedded execution onto a microcontroller, are first outlined. Then, a new\nframework for end-to-end deep neural networks training, quantization and\ndeployment is presented. This framework, called MicroAI, is designed as an\nalternative to existing inference engines (TensorFlow Lite for Microcontrollers\nand STM32Cube.AI). Our framework can indeed be easily adjusted and/or extended\nfor specific use cases. Execution using single precision 32-bit floating-point\nas well as fixed-point on 8- and 16-bit integers are supported. The proposed\nquantization method is evaluated with three different datasets (UCI-HAR, Spoken\nMNIST and GTSRB). Finally, a comparison study between MicroAI and both existing\nembedded inference engines is provided in terms of memory and power efficiency.\nOn-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq\nApollo3 and STM32L452RE).",
          "link": "http://arxiv.org/abs/2105.13331",
          "publishedOn": "2021-05-28T01:42:16.159Z",
          "wordCount": 734,
          "title": "Quantization and Deployment of Deep Neural Networks on Microcontrollers. (arXiv:2105.13331v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toyama_D/0/1/0/all/0/1\">Daniel Toyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamel_P/0/1/0/all/0/1\">Philippe Hamel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gergely_A/0/1/0/all/0/1\">Anita Gergely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1\">Gheorghe Comanici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaese_A/0/1/0/all/0/1\">Amelia Glaese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zafarali Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_T/0/1/0/all/0/1\">Tyler Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mourad_S/0/1/0/all/0/1\">Shibl Mourad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "We introduce AndroidEnv, an open-source platform for Reinforcement Learning\n(RL) research built on top of the Android ecosystem. AndroidEnv allows RL\nagents to interact with a wide variety of apps and services commonly used by\nhumans through a universal touchscreen interface. Since agents train on a\nrealistic simulation of an Android device, they have the potential to be\ndeployed on real devices. In this report, we give an overview of the\nenvironment, highlighting the significant features it provides for research,\nand we present an empirical evaluation of some popular reinforcement learning\nagents on a set of tasks built on this platform.",
          "link": "http://arxiv.org/abs/2105.13231",
          "publishedOn": "2021-05-28T01:42:16.153Z",
          "wordCount": 537,
          "title": "AndroidEnv: A Reinforcement Learning Platform for Android. (arXiv:2105.13231v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ungredda_J/0/1/0/all/0/1\">Juan Ungredda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchi_M/0/1/0/all/0/1\">Mariapia Marchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montrone_T/0/1/0/all/0/1\">Teresa Montrone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branke_J/0/1/0/all/0/1\">Juergen Branke</a>",
          "description": "We consider a multi-objective optimization problem with objective functions\nthat are expensive to evaluate. The decision maker (DM) has unknown\npreferences, and so the standard approach is to generate an approximation of\nthe Pareto front and let the DM choose from the generated non-dominated\ndesigns. However, especially for expensive to evaluate problems where the\nnumber of designs that can be evaluated is very limited, the true best solution\naccording to the DM's unknown preferences is unlikely to be among the small set\nof non-dominated solutions found, even if these solutions are truly Pareto\noptimal. We address this issue by using a multi-objective Bayesian optimization\nalgorithm and allowing the DM to select a preferred solution from a predicted\ncontinuous Pareto front just once before the end of the algorithm rather than\nselecting a solution after the end. This allows the algorithm to understand the\nDM's preferences and make a final attempt to identify a more preferred\nsolution. We demonstrate the idea using ParEGO, and show empirically that the\nfound solutions are significantly better in terms of true DM preferences than\nif the DM would simply pick a solution at the end.",
          "link": "http://arxiv.org/abs/2105.13278",
          "publishedOn": "2021-05-28T01:42:16.145Z",
          "wordCount": 614,
          "title": "One Step Preference Elicitation in Multi-Objective Bayesian Optimization. (arXiv:2105.13278v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baumann_D/0/1/0/all/0/1\">Dominik Baumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marco_A/0/1/0/all/0/1\">Alonso Marco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchetta_M/0/1/0/all/0/1\">Matteo Turchetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1\">Sebastian Trimpe</a>",
          "description": "When learning policies for robotic systems from data, safety is a major\nconcern, as violation of safety constraints may cause hardware damage. SafeOpt\nis an efficient Bayesian optimization (BO) algorithm that can learn policies\nwhile guaranteeing safety with high probability. However, its search space is\nlimited to an initially given safe region. We extend this method by exploring\noutside the initial safe area while still guaranteeing safety with high\nprobability. This is achieved by learning a set of initial conditions from\nwhich we can recover safely using a learned backup controller in case of a\npotential failure. We derive conditions for guaranteed convergence to the\nglobal optimum and validate GoSafe in hardware experiments.",
          "link": "http://arxiv.org/abs/2105.13281",
          "publishedOn": "2021-05-28T01:42:16.139Z",
          "wordCount": 542,
          "title": "GoSafe: Globally Optimal Safe Robot Learning. (arXiv:2105.13281v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13302",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Klusowski_J/0/1/0/all/0/1\">Jason Klusowski</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rush_C/0/1/0/all/0/1\">Cynthia Rush</a>, <a href=\"http://arxiv.org/find/math/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "Sorted l1 regularization has been incorporated into many methods for solving\nhigh-dimensional statistical estimation problems, including the SLOPE estimator\nin linear regression. In this paper, we study how this relatively new\nregularization technique improves variable selection by characterizing the\noptimal SLOPE trade-off between the false discovery proportion (FDP) and true\npositive proportion (TPP) or, equivalently, between measures of type I error\nand power. Assuming a regime of linear sparsity and working under Gaussian\nrandom designs, we obtain an upper bound on the optimal trade-off for SLOPE,\nshowing its capability of breaking the Donoho-Tanner power limit. To put it\ninto perspective, this limit is the highest possible power that the Lasso,\nwhich is perhaps the most popular l1-based method, can achieve even with\narbitrarily strong effect sizes. Next, we derive a tight lower bound that\ndelineates the fundamental limit of sorted l1 regularization in optimally\ntrading the FDP off for the TPP. Finally, we show that on any problem instance,\nSLOPE with a certain regularization sequence outperforms the Lasso, in the\nsense of having a smaller FDP, larger TPP and smaller l2 estimation risk\nsimultaneously. Our proofs are based on a novel technique that reduces a\nvariational calculus problem to a class of infinite-dimensional convex\noptimization problems and a very recent result from approximate message passing\ntheory.",
          "link": "http://arxiv.org/abs/2105.13302",
          "publishedOn": "2021-05-28T01:42:16.121Z",
          "wordCount": 667,
          "title": "Characterizing the SLOPE Trade-off: A Variational Perspective and the Donoho-Tanner Limit. (arXiv:2105.13302v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roddenberry_T/0/1/0/all/0/1\">T. Mitchell Roddenberry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1\">Santiago Segarra</a>",
          "description": "With the increasing popularity of graph-based methods for dimensionality\nreduction and representation learning, node embedding functions have become\nimportant objects of study in the literature. In this paper, we take an\naxiomatic approach to understanding node embedding methods, first stating three\nproperties for embedding dissimilarity networks, then proving that all three\ncannot be satisfied simultaneously by any node embedding method. Similar to\nexisting results on the impossibility of clustering under certain axiomatic\nassumptions, this points to fundamental difficulties inherent to node embedding\ntasks. Once these difficulties are identified, we then relax these axioms to\nallow for certain node embedding methods to be admissible in our framework.",
          "link": "http://arxiv.org/abs/2105.13251",
          "publishedOn": "2021-05-28T01:42:16.112Z",
          "wordCount": 536,
          "title": "An Impossibility Theorem for Node Embedding. (arXiv:2105.13251v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tongxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiye Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Songling Zou</a>",
          "description": "Recently, deep learning has been an area of intense researching. However, as\na kind of computing intensive task, deep learning highly relies on the the\nscale of the GPU memory, which is usually expensive and scarce. Although there\nare some extensive works have been proposed for dynamic GPU memory management,\nthey are hard to be applied to systems with multitasking dynamic workloads,\nsuch as in-database machine learning system.\n\nIn this paper, we demonstrated TENSILE, a method of managing GPU memory in\ntensor granularity to reduce the GPU memory peak, with taking the multitasking\ndynamic workloads into consideration. As far as we know, TENSILE is the first\nmethod which is designed to manage multiple workloads' GPU memory using. We\nimplement TENSILE on our own deep learning framework, and evaluated its\nperformance. The experiment results shows that our method can achieve less time\noverhead than prior works with more GPU memory saved.",
          "link": "http://arxiv.org/abs/2105.13336",
          "publishedOn": "2021-05-28T01:42:16.107Z",
          "wordCount": 613,
          "title": "TENSILE: A Tensor granularity dynamic GPU memory scheduler method towards multiple dynamic workloads system. (arXiv:2105.13336v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13284",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Skordilis_E/0/1/0/all/0/1\">Erotokritos Skordilis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1\">Yi Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tripp_C/0/1/0/all/0/1\">Charles Tripp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moniot_M/0/1/0/all/0/1\">Matthew Moniot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graf_P/0/1/0/all/0/1\">Peter Graf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biagioni_D/0/1/0/all/0/1\">David Biagioni</a>",
          "description": "Mobility on demand (MoD) systems show great promise in realizing flexible and\nefficient urban transportation. However, significant technical challenges arise\nfrom operational decision making associated with MoD vehicle dispatch and fleet\nrebalancing. For this reason, operators tend to employ simplified algorithms\nthat have been demonstrated to work well in a particular setting. To help\nbridge the gap between novel and existing methods, we propose a modular\nframework for fleet rebalancing based on model-free reinforcement learning (RL)\nthat can leverage an existing dispatch method to minimize system cost. In\nparticular, by treating dispatch as part of the environment dynamics, a\ncentralized agent can learn to intermittently direct the dispatcher to\nreposition free vehicles and mitigate against fleet imbalance. We formulate RL\nstate and action spaces as distributions over a grid partitioning of the\noperating area, making the framework scalable and avoiding the complexities\nassociated with multiagent RL. Numerical experiments, using real-world trip and\nnetwork data, demonstrate that this approach has several distinct advantages\nover baseline methods including: improved system cost; high degree of\nadaptability to the selected dispatch method; and the ability to perform\nscale-invariant transfer learning between problem instances with similar\nvehicle and request distributions.",
          "link": "http://arxiv.org/abs/2105.13284",
          "publishedOn": "2021-05-28T01:42:16.101Z",
          "wordCount": 649,
          "title": "A Modular and Transferable Reinforcement Learning Framework for the Fleet Rebalancing Problem. (arXiv:2105.13284v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplanis_C/0/1/0/all/0/1\">Christos Kaplanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1\">Jovana Mitrovi&#x107;</a>",
          "description": "We present an architecture that is effective for continual learning in an\nespecially demanding setting, where task boundaries do not exist or are\nunknown. Our architecture comprises an encoder, pre-trained on a separate\ndataset, and an ensemble of simple one-layer classifiers. Two main innovations\nare required to make this combination work. First, the provision of suitably\ngeneric pre-trained encoders has been made possible thanks to recent progress\nin self-supervised training methods. Second, pairing each classifier in the\nensemble with a key, where the key-space is identical to the latent space of\nthe encoder, allows them to be used collectively, yet selectively, via\nk-nearest neighbour lookup. We show that models trained with the\nencoders-and-ensembles architecture are state-of-the-art for the task-free\nsetting on standard image classification continual learning benchmarks, and\nimprove on prior state-of-the-art by a large margin in the most challenging\ncases. We also show that the architecture learns well in a fully incremental\nsetting, where one class is learned at a time, and we demonstrate its\neffectiveness in this setting with up to 100 classes. Finally, we show that the\narchitecture works in a task-free continual learning context where the data\ndistribution changes gradually, and existing approaches requiring knowledge of\ntask boundaries cannot be applied.",
          "link": "http://arxiv.org/abs/2105.13327",
          "publishedOn": "2021-05-28T01:42:16.094Z",
          "wordCount": 624,
          "title": "Encoders and Ensembles for Task-Free Continual Learning. (arXiv:2105.13327v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barda_A/0/1/0/all/0/1\">Amir Barda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erel_Y/0/1/0/all/0/1\">Yotam Erel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>",
          "description": "Mesh-based learning is one of the popular approaches nowadays to learn\nshapes. The most established backbone in this field is MeshCNN. In this paper,\nwe propose infusing MeshCNN with geometric reasoning to achieve higher quality\nlearning. Through careful analysis of the way geometry is represented\nthrough-out the network, we submit that this representation should be rigid\nmotion invariant, and should allow reconstructing the original geometry.\nAccordingly, we introduce the first and second fundamental forms as an\nedge-centric, rotation and translation invariant, reconstructable\nrepresentation. In addition, we update the originally proposed pooling scheme\nto be more geometrically driven. We validate our analysis through\nexperimentation, and present consistent improvement upon the MeshCNN baseline,\nas well as other more elaborate state-of-the-art architectures. Furthermore, we\ndemonstrate this fundamental forms-based representation opens the door to\naccessible generative machine learning over meshes.",
          "link": "http://arxiv.org/abs/2105.13277",
          "publishedOn": "2021-05-28T01:42:16.076Z",
          "wordCount": 564,
          "title": "MeshCNN Fundamentals: Geometric Learning through a Reconstructable Representation. (arXiv:2105.13277v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nair_H/0/1/0/all/0/1\">Harideep Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">John Paul Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James E. Smith</a>",
          "description": "Temporal Neural Networks (TNNs) are spiking neural networks that use time as\na resource to represent and process information, similar to the mammalian\nneocortex. In contrast to compute-intensive Deep Neural Networks that employ\nseparate training and inference phases, TNNs are capable of extremely efficient\nonline incremental/continuous learning and are excellent candidates for\nbuilding edge-native sensory processing units. This work proposes a\nmicroarchitecture framework for implementing TNNs using standard CMOS.\nGate-level implementations of three key building blocks are presented: 1)\nmulti-synapse neurons, 2) multi-neuron columns, and 3) unsupervised and\nsupervised online learning algorithms based on Spike Timing Dependent\nPlasticity (STDP). The TNN microarchitecture is embodied in a set of\ncharacteristic scaling equations for assessing the gate count, area, delay and\npower consumption for any TNN design. Post-synthesis results (in 45nm CMOS) for\nthe proposed designs are presented, and their online incremental learning\ncapability is demonstrated.",
          "link": "http://arxiv.org/abs/2105.13262",
          "publishedOn": "2021-05-28T01:42:16.070Z",
          "wordCount": 603,
          "title": "A Microarchitecture Implementation Framework for Online Learning with Temporal Neural Networks. (arXiv:2105.13262v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durugkar_I/0/1/0/all/0/1\">Ishan Durugkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tec_M/0/1/0/all/0/1\">Mauricio Tec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>",
          "description": "Learning with an objective function that seeks to minimize the mismatch with\na reference distribution has been shown to be useful for generative modeling\nand imitation learning. In this paper, we investigate whether one such\nobjective, the Wasserstein-1 distance between a policy's state visitation\ndistribution and a target distribution, can be utilized effectively for\nreinforcement learning (RL) tasks. Specifically, this paper focuses on\ngoal-conditioned reinforcement learning where the idealized (unachievable)\ntarget distribution has all the probability mass at the goal. We introduce a\nquasimetric specific to Markov Decision Processes (MDPs), and show that the\npolicy that minimizes the Wasserstein-1 distance of its state visitation\ndistribution to this target distribution under this quasimetric is the policy\nthat reaches the goal in as few steps as possible. Our approach, termed\nAdversarial Intrinsic Motivation (AIM), estimates this Wasserstein-1 distance\nthrough its dual objective and uses it to compute a supplemental reward\nfunction. Our experiments show that this reward function changes smoothly with\nrespect to transitions in the MDP and assists the agent in learning.\nAdditionally, we combine AIM with Hindsight Experience Replay (HER) and show\nthat the resulting algorithm accelerates learning significantly on several\nsimulated robotics tasks when compared to HER with a sparse positive reward at\nthe goal state.",
          "link": "http://arxiv.org/abs/2105.13345",
          "publishedOn": "2021-05-28T01:42:16.064Z",
          "wordCount": 627,
          "title": "Adversarial Intrinsic Motivation for Reinforcement Learning. (arXiv:2105.13345v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qi Zheng</a>",
          "description": "One of the most commonly performed manipulation in a human's daily life is\npouring. Many factors have an effect on target accuracy, including pouring\nvelocity, rotation angle, geometric of the source, and the receiving\ncontainers. This paper presents an approach to increase the repeatability and\naccuracy of the robotic manipulator by estimating the change in the amount of\nwater of the pouring cup to a sequence of pouring actions using multiple layers\nof the deep recurrent neural network, especially gated recurrent units (GRU).\nThe proposed GRU model achieved a validation mean squared error as low as 1e-4\n(lbf) for the predicted value of weight f(t). This paper contains a\ncomprehensive evaluation and analysis of numerous experiments with various\ndesigns of recurrent neural networks and hyperparameters fine-tuning.",
          "link": "http://arxiv.org/abs/2105.12828",
          "publishedOn": "2021-05-28T01:42:16.057Z",
          "wordCount": 552,
          "title": "Pouring Dynamics Estimation Using Gated Recurrent Units. (arXiv:2105.12828v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Id_I/0/1/0/all/0/1\">Ibnu Daqiqil Id</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_M/0/1/0/all/0/1\">Masanobu Abe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hara_S/0/1/0/all/0/1\">Sunao Hara</a>",
          "description": "Based on the experimental results, all concepts drift types have their\nrespective hyperparameter configurations. Simple and gradual concept drift have\nsimilar pattern which requires a smaller {\\alpha} value than recurring concept\ndrift because, in this type of drift, a new concept appear continuously, so it\nneeds a high-frequency model adaptation. However, in recurring concepts, the\nnew concept may repeat in the future, so the lower frequency adaptation is\nbetter. Furthermore, high-frequency model adaptation could lead to an\noverfitting problem. Implementing CMGMM component pruning mechanism help to\ncontrol the number of the active component and improve model performance.",
          "link": "http://arxiv.org/abs/2105.13220",
          "publishedOn": "2021-05-28T01:42:16.051Z",
          "wordCount": 554,
          "title": "Evaluation of concept drift adaptation for acoustic scene classifier based on Kernel Density Drift Detection and Combine Merge Gaussian Mixture Model. (arXiv:2105.13220v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+SarcheshmehPour_Y/0/1/0/all/0/1\">Yasmin SarcheshmehPour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1\">Alexander Jung</a>",
          "description": "Many important application domains generate distributed collections of\nheterogeneous local datasets. These local datasets are often related via an\nintrinsic network structure that arises from domain-specific notions of\nsimilarity between local datasets. Different notions of similarity are induced\nby spatiotemporal proximity, statistical dependencies, or functional relations.\nWe use this network structure to adaptively pool similar local datasets into\nnearly homogenous training sets for learning tailored models. Our main\nconceptual contribution is to formulate networked federated learning using the\nconcept of generalized total variation (GTV) minimization as a regularizer.\nThis formulation is highly flexible and can be combined with almost any\nparametric model including Lasso or deep neural networks. We unify and\nconsiderably extend some well-known approaches to federated multi-task\nlearning. Our main algorithmic contribution is a novel federated learning\nalgorithm that is well suited for distributed computing environments such as\nedge computing over wireless networks. This algorithm is robust against model\nmisspecification and numerical errors arising from limited computational\nresources including processing time or wireless channel bandwidth. As our main\ntechnical contribution, we offer precise conditions on the local models as well\non their network structure such that our algorithm learns nearly optimal local\nmodels. Our analysis reveals an interesting interplay between the\n(information-) geometry of local models and the (cluster-) geometry of their\nnetwork.",
          "link": "http://arxiv.org/abs/2105.12769",
          "publishedOn": "2021-05-28T01:42:16.044Z",
          "wordCount": 643,
          "title": "Networked Federated Multi-Task Learning. (arXiv:2105.12769v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastianello_N/0/1/0/all/0/1\">Nicola Bastianello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonetto_A/0/1/0/all/0/1\">Andrea Simonetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DallAnese_E/0/1/0/all/0/1\">Emiliano Dall&#x27;Anese</a>",
          "description": "This paper presents a new regularization approach -- termed OpReg-Boost -- to\nboost the convergence and lessen the asymptotic error of online optimization\nand learning algorithms. In particular, the paper considers online algorithms\nfor optimization problems with a time-varying (weakly) convex composite cost.\nFor a given online algorithm, OpReg-Boost learns the closest algorithmic map\nthat yields linear convergence; to this end, the learning procedure hinges on\nthe concept of operator regression. We show how to formalize the operator\nregression problem and propose a computationally-efficient Peaceman-Rachford\nsolver that exploits a closed-form solution of simple quadratically-constrained\nquadratic programs (QCQPs). Simulation results showcase the superior properties\nof OpReg-Boost w.r.t. the more classical forward-backward algorithm, FISTA, and\nAnderson acceleration, and with respect to its close relative\nconvex-regression-boost (CvxReg-Boost) which is also novel but less performing.",
          "link": "http://arxiv.org/abs/2105.13271",
          "publishedOn": "2021-05-28T01:42:16.025Z",
          "wordCount": 568,
          "title": "OpReg-Boost: Learning to Accelerate Online Algorithms with Operator Regression. (arXiv:2105.13271v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shamsoshoara_A/0/1/0/all/0/1\">Alireza Shamsoshoara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afghah_F/0/1/0/all/0/1\">Fatemeh Afghah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blasch_E/0/1/0/all/0/1\">Erik Blasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashdown_J/0/1/0/all/0/1\">Jonathan Ashdown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>",
          "description": "The damage to cellular towers during natural and man-made disasters can\ndisturb the communication services for cellular users. One solution to the\nproblem is using unmanned aerial vehicles to augment the desired communication\nnetwork. The paper demonstrates the design of a UAV-Assisted Imitation Learning\n(UnVAIL) communication system that relays the cellular users' information to a\nneighbor base station. Since the user equipment (UEs) are equipped with buffers\nwith limited capacity to hold packets, UnVAIL alternates between different UEs\nto reduce the chance of buffer overflow, positions itself optimally close to\nthe selected UE to reduce service time, and uncovers a network pathway by\nacting as a relay node. UnVAIL utilizes Imitation Learning (IL) as a\ndata-driven behavioral cloning approach to accomplish an optimal scheduling\nsolution. Results demonstrate that UnVAIL performs similar to a human expert\nknowledge-based planning in communication timeliness, position accuracy, and\nenergy consumption with an accuracy of 97.52% when evaluated on a developed\nsimulator to train the UAV.",
          "link": "http://arxiv.org/abs/2105.12823",
          "publishedOn": "2021-05-28T01:42:16.019Z",
          "wordCount": 606,
          "title": "UAV-Assisted Communication in Remote Disaster Areas using Imitation Learning. (arXiv:2105.12823v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13031",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Izzo_D/0/1/0/all/0/1\">Dario Izzo</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gomez_P/0/1/0/all/0/1\">Pablo G&#xf3;mez</a>",
          "description": "We present a novel approach based on artificial neural networks, so-called\ngeodesyNets, and present compelling evidence of their ability to serve as\naccurate geodetic models of highly irregular bodies using minimal prior\ninformation on the body. The approach does not rely on the body shape\ninformation but, if available, can harness it. GeodesyNets learn a\nthree-dimensional, differentiable, function representing the body density,\nwhich we call neural density field. The body shape, as well as other geodetic\nproperties, can easily be recovered. We investigate six different shapes\nincluding the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and\n25143 Itokawa for which shape models developed during close proximity surveys\nare available. Both heterogeneous and homogeneous mass distributions are\nconsidered. The gravitational acceleration computed from the trained\ngeodesyNets models, as well as the inferred body shape, show great accuracy in\nall cases with a relative error on the predicted acceleration smaller than 1\\%\neven close to the asteroid surface. When the body shape information is\navailable, geodesyNets can seamlessly exploit it and be trained to represent a\nhigh-fidelity neural density field able to give insights into the internal\nstructure of the body. This work introduces a new unexplored approach to\ngeodesy, adding a powerful tool to consolidated ones based on spherical\nharmonics, mascon models and polyhedral gravity.",
          "link": "http://arxiv.org/abs/2105.13031",
          "publishedOn": "2021-05-28T01:42:16.011Z",
          "wordCount": 649,
          "title": "Geodesy of irregular small bodies via neural density fields: geodesyNets. (arXiv:2105.13031v1 [astro-ph.EP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenyi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wendi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xu Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Text-to-Image generation in the general domain has long been an open problem,\nwhich requires both generative model and cross-modal understanding. We propose\nCogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance\nthis problem. We also demonstrate the finetuning strategies for various\ndownstream tasks, e.g. style learning, super-resolution, text-image ranking and\nfashion design, and methods to stabilize pretraining, e.g. eliminating NaN\nlosses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS\nCOCO, outperforms previous GAN-based models and a recent similar work DALL-E.",
          "link": "http://arxiv.org/abs/2105.13290",
          "publishedOn": "2021-05-28T01:42:16.004Z",
          "wordCount": 526,
          "title": "CogView: Mastering Text-to-Image Generation via Transformers. (arXiv:2105.13290v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yujun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengtian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>",
          "description": "Data-driven, automatic design space exploration of neural accelerator\narchitecture is desirable for specialization and productivity. Previous\nframeworks focus on sizing the numerical architectural hyper-parameters while\nneglect searching the PE connectivities and compiler mappings. To tackle this\nchallenge, we propose Neural Accelerator Architecture Search (NAAS) which\nholistically searches the neural network architecture, accelerator\narchitecture, and compiler mapping in one optimization loop. NAAS composes\nhighly matched architectures together with efficient mapping. As a data-driven\napproach, NAAS rivals the human design Eyeriss by 4.4x EDP reduction with 2.7%\naccuracy improvement on ImageNet under the same computation resource, and\noffers 1.4x to 3.5x EDP reduction than only sizing the architectural\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2105.13258",
          "publishedOn": "2021-05-28T01:42:15.986Z",
          "wordCount": 530,
          "title": "NAAS: Neural Accelerator Architecture Search. (arXiv:2105.13258v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13121",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Zheng_S/0/1/0/all/0/1\">Shuangjia Zheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_T/0/1/0/all/0/1\">Tao Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1\">Chengtao Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_B/0/1/0/all/0/1\">Binghong Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Coley_C/0/1/0/all/0/1\">Connor W. Coley</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_R/0/1/0/all/0/1\">Ruibo Wu</a>",
          "description": "Nature, a synthetic master, creates more than 300,000 natural products (NPs)\nwhich are the major constituents of FDA-proved drugs owing to the vast chemical\nspace of NPs. To date, there are fewer than 30,000 validated NPs compounds\ninvolved in about 33,000 known enzyme catalytic reactions, and even fewer\nbiosynthetic pathways are known with complete cascade-connected enzyme\ncatalysis. Therefore, it is valuable to make computer-aided bio-retrosynthesis\npredictions. Here, we develop BioNavi-NP, a navigable and user-friendly\ntoolkit, which is capable of predicting the biosynthetic pathways for NPs and\nNP-like compounds through a novel (AND-OR Tree)-based planning algorithm, an\nenhanced molecular Transformer neural network, and a training set that combines\ngeneral organic transformations and biosynthetic steps. Extensive evaluations\nreveal that BioNavi-NP generalizes well to identifying the reported\nbiosynthetic pathways for 90% of test compounds and recovering the verified\nbuilding blocks for 73%, significantly outperforming conventional rule-based\napproaches. Moreover, BioNavi-NP also shows an outstanding capacity of\nbiologically plausible pathways enumeration. In this sense, BioNavi-NP is a\nleading-edge toolkit to redesign complex biosynthetic pathways of natural\nproducts with applications to total or semi-synthesis and pathway elucidation\nor reconstruction.",
          "link": "http://arxiv.org/abs/2105.13121",
          "publishedOn": "2021-05-28T01:42:15.980Z",
          "wordCount": 623,
          "title": "BioNavi-NP: Biosynthesis Navigator for Natural Products. (arXiv:2105.13121v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thapa_R/0/1/0/all/0/1\">Rahul Thapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Dongning Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_X/0/1/0/all/0/1\">Xun Jiao</a>",
          "description": "Inspired by the way human brain works, the emerging hyperdimensional\ncomputing (HDC) is getting more and more attention. HDC is an emerging\ncomputing scheme based on the working mechanism of brain that computes with\ndeep and abstract patterns of neural activity instead of actual numbers.\nCompared with traditional ML algorithms such as DNN, HDC is more\nmemory-centric, granting it advantages such as relatively smaller model size,\nless computation cost, and one-shot learning, making it a promising candidate\nin low-cost computing platforms. However, the robustness of HDC models have not\nbeen systematically studied. In this paper, we systematically expose the\nunexpected or incorrect behaviors of HDC models by developing HDXplore, a\nblackbox differential testing-based framework. We leverage multiple HDC models\nwith similar functionality as cross-referencing oracles to avoid manual\nchecking or labeling the original input. We also propose different perturbation\nmechanisms in HDXplore. HDXplore automatically finds thousands of incorrect\ncorner case behaviors of the HDC model. We propose two retraining mechanisms\nand using the corner cases generated by HDXplore to retrain the HDC model, we\ncan improve the model accuracy by up to 9%.",
          "link": "http://arxiv.org/abs/2105.12770",
          "publishedOn": "2021-05-28T01:42:15.974Z",
          "wordCount": 613,
          "title": "HDXplore: Automated Blackbox Testing of Brain-Inspired Hyperdimensional Computing. (arXiv:2105.12770v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13189",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiyong Zhou</a>",
          "description": "In this paper, we propose a novel sparse recovery method based on the\ngeneralized error function. Both the theoretical analysis and the practical\nalgorithms are presented. Numerical experiments are conducted to demonstrate\nthe advantageous performance of the proposed approach over the state-of-the-art\nsparse recovery methods. Its practical application in magnetic resonance\nimaging (MRI) reconstruction is studied as well.",
          "link": "http://arxiv.org/abs/2105.13189",
          "publishedOn": "2021-05-28T01:42:15.968Z",
          "wordCount": 493,
          "title": "Sparse recovery based on the generalized error function. (arXiv:2105.13189v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shenggui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "Within Transformer, self-attention is the key module to learn powerful\ncontext-aware representations. However, self-attention suffers from quadratic\nmemory requirements with respect to the sequence length, which limits us to\nprocess longer sequence on GPU. In this work, we propose sequence parallelism,\na memory efficient parallelism method to help us break input sequence length\nlimitation and train with longer sequence on GPUs. Compared with existing\nparallelism, our approach no longer requires a single device to hold the whole\nsequence. Specifically, we split the input sequence into multiple chunks and\nfeed each chunk into its corresponding device (i.e. GPU). To compute the\nattention output, we communicate attention embeddings among GPUs. Inspired by\nring all-reduce, we integrated ring-style communication with self-attention\ncalculation and proposed Ring Self-Attention (RSA). Our implementation is fully\nbased on PyTorch. Without extra compiler or library changes, our approach is\ncompatible with data parallelism and pipeline parallelism. Experiments show\nthat sequence parallelism performs well when scaling with batch size and\nsequence length. Compared with tensor parallelism, our approach achieved\n$13.7\\times$ and $3.0\\times$ maximum batch size and sequence length\nrespectively when scaling up to 64 NVIDIA P100 GPUs. We plan to integrate our\nsequence parallelism with data, pipeline and tensor parallelism to further\ntrain large-scale models with 4D parallelism in our future work.",
          "link": "http://arxiv.org/abs/2105.13120",
          "publishedOn": "2021-05-28T01:42:15.961Z",
          "wordCount": 639,
          "title": "Sequence Parallelism: Making 4D Parallelism Possible. (arXiv:2105.13120v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kadeethum_T/0/1/0/all/0/1\">Teeratorn Kadeethum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OMalley_D/0/1/0/all/0/1\">Daniel O&#x27;Malley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuhg_J/0/1/0/all/0/1\">Jan Niklas Fuhg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Youngsoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jonghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_H/0/1/0/all/0/1\">Hari S. Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouklas_N/0/1/0/all/0/1\">Nikolaos Bouklas</a>",
          "description": "This work is the first to employ and adapt the image-to-image translation\nconcept based on conditional generative adversarial networks (cGAN) towards\nlearning a forward and an inverse solution operator of partial differential\nequations (PDEs). Even though the proposed framework could be applied as a\nsurrogate model for the solution of any PDEs, here we focus on steady-state\nsolutions of coupled hydro-mechanical processes in heterogeneous porous media.\nStrongly heterogeneous material properties, which translate to the\nheterogeneity of coefficients of the PDEs and discontinuous features in the\nsolutions, require specialized techniques for the forward and inverse solution\nof these problems. Additionally, parametrization of the spatially heterogeneous\ncoefficients is excessively difficult by using standard reduced order modeling\ntechniques. In this work, we overcome these challenges by employing the\nimage-to-image translation concept to learn the forward and inverse solution\noperators and utilize a U-Net generator and a patch-based discriminator. Our\nresults show that the proposed data-driven reduced order model has competitive\npredictive performance capabilities in accuracy and computational efficiency as\nwell as training time requirements compared to state-of-the-art data-driven\nmethods for both forward and inverse problems.",
          "link": "http://arxiv.org/abs/2105.13136",
          "publishedOn": "2021-05-28T01:42:15.955Z",
          "wordCount": 633,
          "title": "A framework for data-driven solution and parameter estimation of PDEs using conditional generative adversarial networks. (arXiv:2105.13136v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sajiv Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>",
          "description": "Modeling of non-rigid object launching and manipulation is complex\nconsidering the wide range of dynamics affecting trajectory, many of which may\nbe unknown. Using physics models can be inaccurate because they cannot account\nfor unknown factors and the effects of the deformation of the object as it is\nlaunched; moreover, deriving force coefficients for these models is not\npossible without extensive experimental testing. Recently, advancements in\ndata-powered artificial intelligence methods have allowed learnable models and\nsystems to emerge. It is desirable to train a model for launch prediction on a\nrobot, as deep neural networks can account for immeasurable dynamics. However,\nthe inability to collect large amounts of experimental data decreases\nperformance of deep neural networks. Through estimating force coefficients, the\naccepted physics models can be leveraged to produce adequate supplemental data\nto artificially increase the size of the training set, yielding improved neural\nnetworks. In this paper, we introduce a new framework for algorithmic\nestimation of force coefficients for non-rigid object launching, which can be\ngeneralized to other domains, in order to generate large datasets. We implement\na novel training algorithm and objective for our deep neural network to\naccurately model launch trajectory of non-rigid objects and predict whether\nthey will hit a series of targets. Our experimental results demonstrate the\neffectiveness of using simulated data from force coefficient estimation and\nshows the importance of simulated data for training an effective neural\nnetwork.",
          "link": "http://arxiv.org/abs/2105.12833",
          "publishedOn": "2021-05-28T01:42:15.949Z",
          "wordCount": 676,
          "title": "Simulated Data Generation Through Algorithmic Force Coefficient Estimation for AI-Based Robotic Projectile Launch Modeling. (arXiv:2105.12833v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ungredda_J/0/1/0/all/0/1\">Juan Ungredda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branke_J/0/1/0/all/0/1\">Juergen Branke</a>",
          "description": "Many real-world optimisation problems such as hyperparameter tuning in\nmachine learning or simulation-based optimisation can be formulated as\nexpensive-to-evaluate black-box functions. A popular approach to tackle such\nproblems is Bayesian optimisation (BO), which builds a response surface model\nbased on the data collected so far, and uses the mean and uncertainty predicted\nby the model to decide what information to collect next. In this paper, we\npropose a novel variant of the well-known Knowledge Gradient acquisition\nfunction that allows it to handle constraints. We empirically compare the new\nalgorithm with four other state-of-the-art constrained Bayesian optimisation\nalgorithms and demonstrate its superior performance. We also prove theoretical\nconvergence in the infinite budget limit.",
          "link": "http://arxiv.org/abs/2105.13245",
          "publishedOn": "2021-05-28T01:42:15.942Z",
          "wordCount": 531,
          "title": "Bayesian Optimisation for Constrained Problems. (arXiv:2105.13245v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Im_J/0/1/0/all/0/1\">Jinbae Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Moonki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hoyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsouk Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Sehee Chung</a>",
          "description": "Recently, opinion summarization, which is the generation of a summary from\nmultiple reviews, has been conducted in a self-supervised manner by considering\na sampled review as a pseudo summary. However, non-text data such as image and\nmetadata related to reviews have been considered less often. To use the\nabundant information contained in non-text data, we propose a self-supervised\nmultimodal opinion summarization framework called MultimodalSum. Our framework\nobtains a representation of each modality using a separate encoder for each\nmodality, and the text decoder generates a summary. To resolve the inherent\nheterogeneity of multimodal data, we propose a multimodal training pipeline. We\nfirst pretrain the text encoder--decoder based solely on text modality data.\nSubsequently, we pretrain the non-text modality encoders by considering the\npretrained text decoder as a pivot for the homogeneous representation of\nmultimodal data. Finally, to fuse multimodal representations, we train the\nentire framework in an end-to-end manner. We demonstrate the superiority of\nMultimodalSum by conducting experiments on Yelp and Amazon datasets.",
          "link": "http://arxiv.org/abs/2105.13135",
          "publishedOn": "2021-05-28T01:42:15.916Z",
          "wordCount": 593,
          "title": "Self-Supervised Multimodal Opinion Summarization. (arXiv:2105.13135v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xingyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiuhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zenan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangcan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Implicit equilibrium models, i.e., deep neural networks (DNNs) defined by\nimplicit equations, have been becoming more and more attractive recently. In\nthis paper, we investigate one emerging question if model's equilibrium point\ncan be regarded as the solution of an optimization problem. Specifically, we\nfirst decompose DNNs into a new class of unit layer that is differential of an\nimplicit convex function while keeping its output unchanged. Then, the\nequilibrium model of the unit layer can be derived, named Optimization Induced\nEquilibrium Networks (OptEq), which can be easily extended to deep layers. The\nequilibrium point of OptEq can be theoretically connected to the solution of\nits corresponding convex optimization problem with explicit objectives. Based\non this, we can flexibly introduce prior properties to the equilibrium points:\n1) modifying the underlying convex problems explicitly so as to change the\narchitectures of OptEq; and 2) merging the information into the fixed point\niteration, which guarantees to choose the desired equilibrium when the fixed\npoint set is non-singleton. This work establishes an important first step\ntowards optimization guided design of deep models.",
          "link": "http://arxiv.org/abs/2105.13228",
          "publishedOn": "2021-05-28T01:42:15.905Z",
          "wordCount": 608,
          "title": "Optimization Induced Equilibrium Networks. (arXiv:2105.13228v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Runzhe Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chengchun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shikai Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>",
          "description": "Order dispatch is one of the central problems to ride-sharing platforms.\nRecently, value-based reinforcement learning algorithms have shown promising\nperformance on this problem. However, in real-world applications, the\nnon-stationarity of the demand-supply system poses challenges to re-utilizing\ndata generated in different time periods to learn the value function. In this\nwork, motivated by the fact that the relative relationship between the values\nof some states is largely stable across various environments, we propose a\npattern transfer learning framework for value-based reinforcement learning in\nthe order dispatch problem. Our method efficiently captures the value patterns\nby incorporating a concordance penalty. The superior performance of the\nproposed method is supported by experiments.",
          "link": "http://arxiv.org/abs/2105.13218",
          "publishedOn": "2021-05-28T01:42:15.894Z",
          "wordCount": 538,
          "title": "Pattern Transfer Learning for Reinforcement Learning in Order Dispatching. (arXiv:2105.13218v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edge_D/0/1/0/all/0/1\">Darren Edge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>",
          "description": "Making evidence based decisions requires data. However for real-world\napplications, the privacy of data is critical. Using synthetic data which\nreflects certain statistical properties of the original data preserves the\nprivacy of the original data. To this end, prior works utilize differentially\nprivate data release mechanisms to provide formal privacy guarantees. However,\nsuch mechanisms have unacceptable privacy vs. utility trade-offs. We propose\nincorporating causal information into the training process to favorably modify\nthe aforementioned trade-off. We theoretically prove that generative models\ntrained with additional causal knowledge provide stronger differential privacy\nguarantees. Empirically, we evaluate our solution comparing different models\nbased on variational auto-encoders (VAEs), and show that causal information\nimproves resilience to membership inference, with improvements in downstream\nutility.",
          "link": "http://arxiv.org/abs/2105.13144",
          "publishedOn": "2021-05-28T01:42:15.866Z",
          "wordCount": 552,
          "title": "Causally Constrained Data Synthesis for Private Data Release. (arXiv:2105.13144v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>",
          "description": "InfoNCE loss is a widely used loss function for contrastive model training.\nIt aims to estimate the mutual information between a pair of variables by\ndiscriminating between each positive pair and its associated $K$ negative\npairs. It is proved that when the sample labels are clean, the lower bound of\nmutual information estimation is tighter when more negative samples are\nincorporated, which usually yields better model performance. However, in many\nreal-world tasks the labels often contain noise, and incorporating too many\nnoisy negative samples for model training may be suboptimal. In this paper, we\nstudy how many negative samples are optimal for InfoNCE in different scenarios\nvia a semi-quantitative theoretical framework. More specifically, we first\npropose a probabilistic model to analyze the influence of the negative sampling\nratio $K$ on training sample informativeness. Then, we design a training\neffectiveness function to measure the overall influence of training samples on\nmodel learning based on their informativeness. We estimate the optimal negative\nsampling ratio using the $K$ value that maximizes the training effectiveness\nfunction. Based on our framework, we further propose an adaptive negative\nsampling method that can dynamically adjust the negative sampling ratio to\nimprove InfoNCE based model training. Extensive experiments on different\nreal-world datasets show our framework can accurately predict the optimal\nnegative sampling ratio in different tasks, and our proposed adaptive negative\nsampling method can achieve better performance than the commonly used fixed\nnegative sampling ratio strategy.",
          "link": "http://arxiv.org/abs/2105.13003",
          "publishedOn": "2021-05-28T01:42:15.849Z",
          "wordCount": 665,
          "title": "Rethinking InfoNCE: How Many Negative Samples Do You Need?. (arXiv:2105.13003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13191",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Machidon_A/0/1/0/all/0/1\">Alina L. Machidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pejovic_V/0/1/0/all/0/1\">Veljko Pejovic</a>",
          "description": "Compressive sensing (CS) is a mathematically elegant tool for reducing the\nsampling rate, potentially bringing context-awareness to a wider range of\ndevices. Nevertheless, practical issues with the sampling and reconstruction\nalgorithms prevent further proliferation of CS in real world domains,\nespecially among heterogeneous ubiquitous devices. Deep learning (DL) naturally\ncomplements CS for adapting the sampling matrix, reconstructing the signal, and\nlearning form the compressed samples. While the CS-DL integration has received\nsubstantial research interest recently, it has not yet been thoroughly\nsurveyed, nor has the light been shed on practical issues towards bringing the\nCS-DL to real world implementations in the ubicomp domain. In this paper we\nidentify main possible ways in which CS and DL can interplay, extract key ideas\nfor making CS-DL efficient, identify major trends in CS-DL research space, and\nderive guidelines for future evolution of CS-DL within the ubicomp domain.",
          "link": "http://arxiv.org/abs/2105.13191",
          "publishedOn": "2021-05-28T01:42:15.832Z",
          "wordCount": 585,
          "title": "Deep Learning Techniques for Compressive Sensing-Based Reconstruction and Inference -- A Ubiquitous Systems Perspective. (arXiv:2105.13191v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>",
          "description": "With the advances of data-driven machine learning research, a wide variety of\nprediction problems have been tackled. It has become critical to explore how\nmachine learning and specifically deep learning methods can be exploited to\nanalyse healthcare data. A major limitation of existing methods has been the\nfocus on grid-like data; however, the structure of physiological recordings are\noften irregular and unordered which makes it difficult to conceptualise them as\na matrix. As such, graph neural networks have attracted significant attention\nby exploiting implicit information that resides in a biological system, with\ninteractive nodes connected by edges whose weights can be either temporal\nassociations or anatomical junctions. In this survey, we thoroughly review the\ndifferent types of graph architectures and their applications in healthcare. We\nprovide an overview of these methods in a systematic manner, organized by their\ndomain of application including functional connectivity, anatomical structure\nand electrical-based analysis. We also outline the limitations of existing\ntechniques and discuss potential directions for future research.",
          "link": "http://arxiv.org/abs/2105.13137",
          "publishedOn": "2021-05-28T01:42:15.826Z",
          "wordCount": 611,
          "title": "Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past, Present and Future. (arXiv:2105.13137v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengjing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Gang Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuomo_S/0/1/0/all/0/1\">Salvatore Cuomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccialli_F/0/1/0/all/0/1\">Francesco Piccialli</a>",
          "description": "Heterogeneous data are commonly adopted as the inputs for some models that\npredict the future trends of some observations. Existing predictive models\ntypically ignore the inconsistencies and imperfections in heterogeneous data\nwhile also failing to consider the (1) spatial correlations among monitoring\npoints or (2) predictions for the entire study area. To address the above\nproblems, this paper proposes a deep learning method for fusing heterogeneous\ndata collected from multiple monitoring points using graph convolutional\nnetworks (GCNs) to predict the future trends of some observations and evaluates\nits effectiveness by applying it in an air quality predictions scenario. The\nessential idea behind the proposed method is to (1) fuse the collected\nheterogeneous data based on the locations of the monitoring points with regard\nto their spatial correlations and (2) perform prediction based on global\ninformation rather than local information. In the proposed method, first, we\nassemble a fusion matrix using the proposed RBF-based fusion approach; second,\nbased on the fused data, we construct spatially and temporally correlated data\nas inputs for the predictive model; finally, we employ the spatiotemporal graph\nconvolutional network (STGCN) to predict the future trends of some\nobservations. In the application scenario of air quality prediction, it is\nobserved that (1) the fused data derived from the RBF-based fusion approach\nachieve satisfactory consistency; (2) the performances of the prediction models\nbased on fused data are better than those based on raw data; and (3) the STGCN\nmodel achieves the best performance when compared to those of all baseline\nmodels. The proposed method is applicable for similar scenarios where\ncontinuous heterogeneous data are collected from multiple monitoring points\nscattered across a study area.",
          "link": "http://arxiv.org/abs/2105.13125",
          "publishedOn": "2021-05-28T01:42:15.820Z",
          "wordCount": 719,
          "title": "Heterogeneous Data Fusion Considering Spatial Correlations using Graph Convolutional Networks and its Application in Air Quality Prediction. (arXiv:2105.13125v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baniecki_H/0/1/0/all/0/1\">Hubert Baniecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kretowicz_W/0/1/0/all/0/1\">Wojciech Kretowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1\">Przemyslaw Biecek</a>",
          "description": "Many methods have been developed to understand complex predictive models and\nhigh expectations are placed on post-hoc model explainability. It turns out\nthat such explanations are not robust nor trustworthy, and they can be fooled.\nThis paper presents techniques for attacking Partial Dependence (plots,\nprofiles, PDP), which are among the most popular methods of explaining any\npredictive model trained on tabular data. We showcase that PD can be\nmanipulated in an adversarial manner, which is alarming, especially in\nfinancial or medical applications where auditability became a must-have trait\nsupporting black-box models. The fooling is performed via poisoning the data to\nbend and shift explanations in the desired direction using genetic and gradient\nalgorithms. To the best of our knowledge, this is the first work performing\nattacks on variable dependence explanations. The novel approach of using a\ngenetic algorithm for doing so is highly transferable as it generalizes both\nways: in a model-agnostic and an explanation-agnostic manner.",
          "link": "http://arxiv.org/abs/2105.12837",
          "publishedOn": "2021-05-28T01:42:15.813Z",
          "wordCount": 579,
          "title": "Fooling Partial Dependence via Data Poisoning. (arXiv:2105.12837v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13099",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1\">Alberto Bietti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vaiter_S/0/1/0/all/0/1\">Samuel Vaiter</a>",
          "description": "We study the approximation power of Graph Neural Networks (GNNs) on latent\nposition random graphs. In the large graph limit, GNNs are known to converge to\ncertain \"continuous\" models known as c-GNNs, which directly enables a study of\ntheir approximation power on random graph models. In the absence of input node\nfeatures however, just as GNNs are limited by the Weisfeiler-Lehman isomorphism\ntest, c-GNNs will be severely limited on simple random graph models. For\ninstance, they will fail to distinguish the communities of a well-separated\nStochastic Block Model (SBM) with constant degree function. Thus, we consider\nrecently proposed architectures that augment GNNs with unique node identifiers,\nsometimes referred to as Graph Wavelets Neural Networks (GWNNs). We study the\nconvergence of GWNNs to their continuous counterpart (c-GWNNs) in the large\nrandom graph limit, under new conditions on the node identifiers. We then show\nthat c-GWNNs are strictly more powerful than c-GNNs in the continuous limit,\nand prove their universality on several random graph models of interest,\nincluding most SBMs and a large class of random geometric graphs. Our results\ncover both permutation-invariant and permutation-equivariant architectures.",
          "link": "http://arxiv.org/abs/2105.13099",
          "publishedOn": "2021-05-28T01:42:15.793Z",
          "wordCount": 616,
          "title": "On the Universality of Graph Neural Networks on Large Random Graphs. (arXiv:2105.13099v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zinc Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hutchin Huang</a>",
          "description": "Hateful content detection is one of the areas where deep learning can and\nshould make a significant difference. The Hateful Memes Challenge from Facebook\nhelps fulfill such potential by challenging the contestants to detect hateful\nspeech in multi-modal memes using deep learning algorithms. In this paper, we\nutilize multi-modal, pre-trained models VilBERT and Visual BERT. We improved\nmodels' performance by adding training datasets generated from data\naugmentation. Enlarging the training data set helped us get a more than 2%\nboost in terms of AUROC with the Visual BERT model. Our approach achieved\n0.7439 AUROC along with an accuracy of 0.7037 on the challenge's test set,\nwhich revealed remarkable progress.",
          "link": "http://arxiv.org/abs/2105.13132",
          "publishedOn": "2021-05-28T01:42:15.785Z",
          "wordCount": 559,
          "title": "Enhance Multimodal Model Performance with Data Augmentation: Facebook Hateful Meme Challenge Solution. (arXiv:2105.13132v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13011",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+De_S/0/1/0/all/0/1\">Subhayan De</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doostan_A/0/1/0/all/0/1\">Alireza Doostan</a>",
          "description": "With the capability of accurately representing a functional relationship\nbetween the inputs of a physical system's model and output quantities of\ninterest, neural networks have become popular for surrogate modeling in\nscientific applications. However, as these networks are over-parameterized,\ntheir training often requires a large amount of data. To prevent overfitting\nand improve generalization error, regularization based on, e.g., $\\ell_1$- and\n$\\ell_2$-norms of the parameters is applied. Similarly, multiple connections of\nthe network may be pruned to increase sparsity in the network parameters. In\nthis paper, we explore the effects of sparsity promoting\n$\\ell_1$-regularization on training neural networks when only a small training\ndataset from a high-fidelity model is available. As opposed to standard\n$\\ell_1$-regularization that is known to be inadequate, we consider two\nvariants of $\\ell_1$-regularization informed by the parameters of an identical\nnetwork trained using data from lower-fidelity models of the problem at hand.\nThese bi-fidelity strategies are generalizations of transfer learning of neural\nnetworks that uses the parameters learned from a large low-fidelity dataset to\nefficiently train networks for a small high-fidelity dataset. We also compare\nthe bi-fidelity strategies with two $\\ell_1$-regularization methods that only\nuse the high-fidelity dataset. Three numerical examples for propagating\nuncertainty through physical systems are used to show that the proposed\nbi-fidelity $\\ell_1$-regularization strategies produce errors that are one\norder of magnitude smaller than those of networks trained only using datasets\nfrom the high-fidelity models.",
          "link": "http://arxiv.org/abs/2105.13011",
          "publishedOn": "2021-05-28T01:42:15.753Z",
          "wordCount": 661,
          "title": "Neural Network Training Using $\\ell_1$-Regularization and Bi-fidelity Data. (arXiv:2105.13011v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Phuong_M/0/1/0/all/0/1\">Mary Phuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1\">Christoph H. Lampert</a>",
          "description": "Knowledge distillation, i.e., one classifier being trained on the outputs of\nanother classifier, is an empirically very successful technique for knowledge\ntransfer between classifiers. It has even been observed that classifiers learn\nmuch faster and more reliably if trained with the outputs of another classifier\nas soft labels, instead of from ground truth data. So far, however, there is no\nsatisfactory theoretical explanation of this phenomenon. In this work, we\nprovide the first insights into the working mechanisms of distillation by\nstudying the special case of linear and deep linear classifiers. Specifically,\nwe prove a generalization bound that establishes fast convergence of the\nexpected risk of a distillation-trained linear classifier. From the bound and\nits proof we extract three key factors that determine the success of\ndistillation: * data geometry -- geometric properties of the data distribution,\nin particular class separation, has a direct influence on the convergence speed\nof the risk; * optimization bias -- gradient descent optimization finds a very\nfavorable minimum of the distillation objective; and * strong monotonicity --\nthe expected risk of the student classifier always decreases when the size of\nthe training set grows.",
          "link": "http://arxiv.org/abs/2105.13093",
          "publishedOn": "2021-05-28T01:42:15.746Z",
          "wordCount": 624,
          "title": "Towards Understanding Knowledge Distillation. (arXiv:2105.13093v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Markgraf_O/0/1/0/all/0/1\">Oliver Markgraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stan_D/0/1/0/all/0/1\">Daniel Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1\">Anthony W. Lin</a>",
          "description": "We study the problem of learning a finite union of integer (axis-aligned)\nhypercubes over the d-dimensional integer lattice, i.e., whose edges are\nparallel to the coordinate axes. This is a natural generalization of the\nclassic problem in the computational learning theory of learning rectangles. We\nprovide a learning algorithm with access to a minimally adequate teacher (i.e.\nmembership and equivalence oracles) that solves this problem in\npolynomial-time, for any fixed dimension d. Over a non-fixed dimension, the\nproblem subsumes the problem of learning DNF boolean formulas, a central open\nproblem in the field. We have also provided extensions to handle infinite\nhypercubes in the union, as well as showing how subset queries could improve\nthe performance of the learning algorithm in practice. Our problem has a\nnatural application to the problem of monadic decomposition of quantifier-free\ninteger linear arithmetic formulas, which has been actively studied in recent\nyears. In particular, a finite union of integer hypercubes correspond to a\nfinite disjunction of monadic predicates over integer linear arithmetic\n(without modulo constraints). Our experiments suggest that our learning\nalgorithms substantially outperform the existing algorithms.",
          "link": "http://arxiv.org/abs/2105.13071",
          "publishedOn": "2021-05-28T01:42:15.740Z",
          "wordCount": 626,
          "title": "Learning Union of Integer Hypercubes with Queries (Technical Report). (arXiv:2105.13071v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>",
          "description": "In label-noise learning, estimating the transition matrix is a hot topic as\nthe matrix plays an important role in building statistically consistent\nclassifiers. Traditionally, the transition from clean distribution to noisy\ndistribution (i.e., clean label transition matrix) has been widely exploited to\nlearn a clean label classifier by employing the noisy data. Motivated by that\nclassifiers mostly output Bayes optimal labels for prediction, in this paper,\nwe study to directly model the transition from Bayes optimal distribution to\nnoisy distribution (i.e., Bayes label transition matrix) and learn a Bayes\noptimal label classifier. Note that given only noisy data, it is ill-posed to\nestimate either the clean label transition matrix or the Bayes label transition\nmatrix. But favorably, Bayes optimal labels are less uncertain compared with\nthe clean labels, i.e., the class posteriors of Bayes optimal labels are\none-hot vectors while those of clean labels are not. This enables two\nadvantages to estimate the Bayes label transition matrix, i.e., (a) we could\ntheoretically recover a set of Bayes optimal labels under mild conditions; (b)\nthe feasible solution space is much smaller. By exploiting the advantages, we\nestimate the Bayes label transition matrix by employing a deep neural network\nin a parameterized way, leading to better generalization and superior\nclassification performance.",
          "link": "http://arxiv.org/abs/2105.13001",
          "publishedOn": "2021-05-28T01:42:15.719Z",
          "wordCount": 637,
          "title": "Estimating Instance-dependent Label-noise Transition Matrix using DNNs. (arXiv:2105.13001v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">Aimon Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Nabeel Mohammed</a>",
          "description": "Pretraining has sparked groundswell of interest in deep learning workflows to\nlearn from limited data and improve generalization. While this is common for 2D\nimage classification tasks, its application to 3D medical imaging tasks like\nchest CT interpretation is limited. We explore the idea of whether pretraining\na model on realistic videos could improve performance rather than training the\nmodel from scratch, intended for tuberculosis type classification from chest CT\nscans. To incorporate both spatial and temporal features, we develop a hybrid\nconvolutional neural network (CNN) and recurrent neural network (RNN) model,\nwhere the features are extracted from each axial slice of the CT scan by a CNN,\nthese sequence of image features are input to a RNN for classification of the\nCT scan. Our model termed as ViPTT-Net, was trained on over 1300 video clips\nwith labels of human activities, and then fine-tuned on chest CT scans with\nlabels of tuberculosis type. We find that pretraining the model on videos lead\nto better representations and significantly improved model validation\nperformance from a kappa score of 0.17 to 0.35, especially for\nunder-represented class samples. Our best method achieved 2nd place in the\nImageCLEF 2021 Tuberculosis - TBT classification task with a kappa score of\n0.20 on the final test set with only image information (without using clinical\nmeta-data). All codes and models are made available.",
          "link": "http://arxiv.org/abs/2105.12810",
          "publishedOn": "2021-05-28T01:42:15.712Z",
          "wordCount": 677,
          "title": "ViPTT-Net: Video pretraining of spatio-temporal model for tuberculosis type classification from chest CT scans. (arXiv:2105.12810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1\">Tri Dung Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guandong Xu</a>",
          "description": "Causal inference methods are widely applied in various decision-making\ndomains such as precision medicine, optimal policy and economics. Central to\nthese applications is the treatment effect estimation of intervention\nstrategies. Current estimation methods are mostly restricted to the\ndeterministic treatment, which however, is unable to address the stochastic\nspace treatment policies. Moreover, previous methods can only make binary\nyes-or-no decisions based on the treatment effect, lacking the capability of\nproviding fine-grained effect estimation degree to explain the process of\ndecision making. In our study, we therefore advance the causal inference\nresearch to estimate stochastic intervention effect by devising a new\nstochastic propensity score and stochastic intervention effect estimator (SIE).\nMeanwhile, we design a customized genetic algorithm specific to stochastic\nintervention effect (Ge-SIO) with the aim of providing causal evidence for\ndecision making. We provide the theoretical analysis and conduct an empirical\nstudy to justify that our proposed measures and algorithms can achieve a\nsignificant performance lift in comparison with state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2105.12898",
          "publishedOn": "2021-05-28T01:42:15.706Z",
          "wordCount": 593,
          "title": "Stochastic Intervention for Causal Effect Estimation. (arXiv:2105.12898v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dopierre_T/0/1/0/all/0/1\">Thomas Dopierre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gravier_C/0/1/0/all/0/1\">Christophe Gravier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logerais_W/0/1/0/all/0/1\">Wilfried Logerais</a>",
          "description": "Recent research considers few-shot intent detection as a meta-learning\nproblem: the model is learning to learn from a consecutive set of small tasks\nnamed episodes. In this work, we propose ProtAugment, a meta-learning algorithm\nfor short texts classification (the intent detection task). ProtAugment is a\nnovel extension of Prototypical Networks, that limits overfitting on the bias\nintroduced by the few-shots classification objective at each episode. It relies\non diverse paraphrasing: a conditional language model is first fine-tuned for\nparaphrasing, and diversity is later introduced at the decoding stage at each\nmeta-learning episode. The diverse paraphrasing is unsupervised as it is\napplied to unlabelled data, and then fueled to the Prototypical Network\ntraining objective as a consistency loss. ProtAugment is the state-of-the-art\nmethod for intent detection meta-learning, at no extra labeling efforts and\nwithout the need to fine-tune a conditional language model on a given\napplication domain.",
          "link": "http://arxiv.org/abs/2105.12995",
          "publishedOn": "2021-05-28T01:42:15.700Z",
          "wordCount": 594,
          "title": "ProtAugment: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning. (arXiv:2105.12995v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13102",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+DAgostino_D/0/1/0/all/0/1\">Danny D&#x27;Agostino</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Serani_A/0/1/0/all/0/1\">Andrea Serani</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Stern_F/0/1/0/all/0/1\">Frederick Stern</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Diez_M/0/1/0/all/0/1\">Matteo Diez</a>",
          "description": "The prediction capability of recurrent-type neural networks is investigated\nfor real-time short-term prediction (nowcasting) of ship motions in high sea\nstate. Specifically, the performance of recurrent neural networks, long-short\nterm memory, and gated recurrent units models are assessed and compared using a\ndata set coming from computational fluid dynamics simulations of a\nself-propelled destroyer-type vessel in stern-quartering sea state 7. Time\nseries of incident wave, ship motions, rudder angle, as well as immersion\nprobes, are used as variables for a nowcasting problem. The objective is to\nobtain about 20 s ahead prediction. Overall, the three methods provide\npromising and comparable results.",
          "link": "http://arxiv.org/abs/2105.13102",
          "publishedOn": "2021-05-28T01:42:15.693Z",
          "wordCount": 567,
          "title": "Recurrent-type Neural Networks for Real-time Short-term Prediction of Ship Motions in High Sea State. (arXiv:2105.13102v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13052",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Boulle_N/0/1/0/all/0/1\">Nicolas Boull&#xe9;</a>, <a href=\"http://arxiv.org/find/math/1/au:+Townsend_A/0/1/0/all/0/1\">Alex Townsend</a>",
          "description": "The randomized singular value decomposition (SVD) is a popular and effective\nalgorithm for computing a near-best rank $k$ approximation of a matrix $A$\nusing matrix-vector products with standard Gaussian vectors. Here, we\ngeneralize the theory of randomized SVD to multivariable Gaussian vectors,\nallowing one to incorporate prior knowledge of $A$ into the algorithm. This\nenables us to explore the continuous analogue of the randomized SVD for\nHilbert--Schmidt (HS) operators using operator-function products with functions\ndrawn from a Gaussian process (GP). We then construct a new covariance kernel\nfor GPs, based on weighted Jacobi polynomials, which allows us to rapidly\nsample the GP and control the smoothness of the randomly generated functions.\nNumerical examples on matrices and HS operators demonstrate the applicability\nof the algorithm.",
          "link": "http://arxiv.org/abs/2105.13052",
          "publishedOn": "2021-05-28T01:42:15.673Z",
          "wordCount": 559,
          "title": "A generalization of the randomized singular value decomposition. (arXiv:2105.13052v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12866",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1\">Xiaoliang Wan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_K/0/1/0/all/0/1\">Kejun Tang</a>",
          "description": "In this work, we have proposed augmented KRnets including both discrete and\ncontinuous models. One difficulty in flow-based generative modeling is to\nmaintain the invertibility of the transport map, which is often a trade-off\nbetween effectiveness and robustness. The exact invertibility has been achieved\nin the real NVP using a specific pattern to exchange information between two\nseparated groups of dimensions. KRnet has been developed to enhance the\ninformation exchange among data dimensions by incorporating the\nKnothe-Rosenblatt rearrangement into the structure of the transport map. Due to\nthe maintenance of exact invertibility, a full nonlinear update of all data\ndimensions needs three iterations in KRnet. To alleviate this issue, we will\nadd augmented dimensions that act as a channel for communications among the\ndata dimensions. In the augmented KRnet, a fully nonlinear update is achieved\nin two iterations. We also show that the augmented KRnet can be reformulated as\nthe discretization of a neural ODE, where the exact invertibility is kept such\nthat the adjoint method can be formulated with respect to the discretized ODE\nto obtain the exact gradient. Numerical experiments have been implemented to\ndemonstrate the effectiveness of our models.",
          "link": "http://arxiv.org/abs/2105.12866",
          "publishedOn": "2021-05-28T01:42:15.666Z",
          "wordCount": 615,
          "title": "Augmented KRnet for density estimation and approximation. (arXiv:2105.12866v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chau_S/0/1/0/all/0/1\">Siu Lun Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouabid_S/0/1/0/all/0/1\">Shahine Bouabid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejdinovic_D/0/1/0/all/0/1\">Dino Sejdinovic</a>",
          "description": "Refining low-resolution (LR) spatial fields with high-resolution (HR)\ninformation is challenging as the diversity of spatial datasets often prevents\ndirect matching of observations. Yet, when LR samples are modeled as aggregate\nconditional means of HR samples with respect to a mediating variable that is\nglobally observed, the recovery of the underlying fine-grained field can be\nframed as taking an \"inverse\" of the conditional expectation, namely a\ndeconditioning problem. In this work, we introduce conditional mean processes\n(CMP), a new class of Gaussian Processes describing conditional means. By\ntreating CMPs as inter-domain features of the underlying field, a posterior for\nthe latent field can be established as a solution to the deconditioning\nproblem. Furthermore, we show that this solution can be viewed as a two-staged\nvector-valued kernel ridge regressor and show that it has a minimax optimal\nconvergence rate under mild assumptions. Lastly, we demonstrate its proficiency\nin a synthetic and a real-world atmospheric field downscaling problem, showing\nsubstantial improvements over existing methods.",
          "link": "http://arxiv.org/abs/2105.12909",
          "publishedOn": "2021-05-28T01:42:15.659Z",
          "wordCount": 583,
          "title": "Deconditional Downscaling with Gaussian Processes. (arXiv:2105.12909v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingchao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_F/0/1/0/all/0/1\">Florian Meyer</a>",
          "description": "Location-aware networks will introduce innovative services and applications\nfor modern convenience, applied ocean sciences, and public safety. In this\npaper, we establish a hybrid method for model-based and data-driven inference.\nWe consider a cooperative localization (CL) scenario where the mobile agents in\na wireless network aim to localize themselves by performing pairwise\nobservations with other agents and by exchanging location information. A\ntraditional method for distributed CL in large agent networks is belief\npropagation (BP) which is completely model-based and is known to suffer from\nproviding inconsistent (overconfident) estimates. The proposed approach\naddresses these limitations by complementing BP with learned information\nprovided by a graph neural network (GNN). We demonstrate numerically that our\nmethod can improve estimation accuracy and avoid overconfident beliefs, while\nits computational complexity remains comparable to BP. Notably, more consistent\nbeliefs are obtained by not explicitly addressing overconfidence in the loss\nfunction used for training of the GNN.",
          "link": "http://arxiv.org/abs/2105.12903",
          "publishedOn": "2021-05-28T01:42:15.651Z",
          "wordCount": 582,
          "title": "Neural Enhanced Belief Propagation for Cooperative Localization. (arXiv:2105.12903v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_P/0/1/0/all/0/1\">Pratik K. Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songlin Liu</a>",
          "description": "Recommender Systems are a subclass of machine learning systems that employ\nsophisticated information filtering strategies to reduce the search time and\nsuggest the most relevant items to any particular user. Hybrid recommender\nsystems combine multiple recommendation strategies in different ways to benefit\nfrom their complementary advantages. Some hybrid recommender systems have\ncombined collaborative filtering and content-based approaches to build systems\nthat are more robust. In this paper, we propose a hybrid recommender system,\nwhich combines Alternative Least Squares (ALS) based collaborative filtering\nwith deep learning to enhance recommendation performance as well as overcome\nthe limitations associated with the collaborative filtering approach,\nespecially concerning its cold start problem. In essence, we use the outputs\nfrom ALS (collaborative filtering) to influence the recommendations from a Deep\nNeural Network (DNN), which combines characteristic, contextual, structural and\nsequential information, in a big data processing framework. We have conducted\nseveral experiments in testing the efficacy of the proposed hybrid architecture\nin recommending smartphones to prospective customers and compared its\nperformance with other open-source recommenders. The results have shown that\nthe proposed system has outperformed several existing hybrid recommender\nsystems.",
          "link": "http://arxiv.org/abs/2105.12876",
          "publishedOn": "2021-05-28T01:42:15.644Z",
          "wordCount": 619,
          "title": "A Hybrid Recommender System for Recommending Smartphones to Prospective Customers. (arXiv:2105.12876v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1\">J. M. de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rovelli_P/0/1/0/all/0/1\">P. Rovelli</a>",
          "description": "Internet and social media offer firms novel ways of managing their marketing\nstrategy and gain competitive advantage. The groups of users expressing\nthemselves on the Internet about a particular topic, product, or brand are\nfrequently called a virtual tribe or E-tribe. However, there are no automatic\ntools for identifying and studying the characteristics of these virtual tribes.\nTowards this aim, this paper presents Tribefinder, a system to reveal Twitter\nusers' tribal affiliations, by analyzing their tweets and language use. To show\nthe potential of this instrument, we provide an example considering three\nspecific tribal macro-categories: alternative realities, lifestyle, and\nrecreation. In addition, we discuss the different characteristics of each\nidentified tribe, in terms of use of language and social interaction metrics.\nTribefinder illustrates the importance of adopting a new lens for studying\nvirtual tribes, which is crucial for firms to properly design their marketing\nstrategy, and for scholars to extend prior marketing research.",
          "link": "http://arxiv.org/abs/2105.13036",
          "publishedOn": "2021-05-28T01:42:15.636Z",
          "wordCount": 622,
          "title": "Put your money where your mouth is: Using deep learning to identify consumer tribes from word usage. (arXiv:2105.13036v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huda_S/0/1/0/all/0/1\">Safeen Huda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Songhori_E/0/1/0/all/0/1\">Ebrahim Songhori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldie_A/0/1/0/all/0/1\">Anna Goldie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirhoseini_A/0/1/0/all/0/1\">Azalia Mirhoseini</a>",
          "description": "The rapidly-changing ML model landscape presents a unique opportunity for\nbuilding hardware accelerators optimized for specific datacenter-scale\nworkloads. We propose Full-stack Accelerator Search Technique (FAST), a\nhardware accelerator search framework that defines a broad optimization\nenvironment covering key design decisions within the hardware-software stack,\nincluding hardware datapath, software scheduling, and compiler passes such as\noperation fusion and tensor padding. Although FAST can be used on any number\nand type of deep learning workload, in this paper we focus on optimizing for a\nsingle or small set of vision models, resulting in significantly faster and\nmore power-efficient designs relative to a general purpose ML accelerator. When\nevaluated on EfficientNet, ResNet50v2, and OCR inference performance relative\nto a TPU-v3, designs generated by FAST optimized for single workloads can\nimprove Perf/TDP (peak power) by over 6x in the best case and 4x on average. On\na limited workload subset, FAST improves Perf/TDP 2.85x on average, with a\nreduction to 2.35x for a single design optimized over the set of workloads. In\naddition, we demonstrate a potential 1.8x speedup opportunity for TPU-v3 with\nimproved scheduling.",
          "link": "http://arxiv.org/abs/2105.12842",
          "publishedOn": "2021-05-28T01:42:15.613Z",
          "wordCount": 616,
          "title": "A Full-stack Accelerator Search Technique for Vision Applications. (arXiv:2105.12842v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1\">Taisuke Kobayashi</a>",
          "description": "This paper addresses a new interpretation of reinforcement learning (RL) as\nreverse Kullback-Leibler (KL) divergence optimization, and derives a new\noptimization method using forward KL divergence. Although RL originally aims to\nmaximize return indirectly through optimization of policy, the recent work by\nLevine has proposed a different derivation process with explicit consideration\nof optimality as stochastic variable. This paper follows this concept and\nformulates the traditional learning laws for both value function and policy as\nthe optimization problems with reverse KL divergence including optimality.\nFocusing on the asymmetry of KL divergence, the new optimization problems with\nforward KL divergence are derived. Remarkably, such new optimization problems\ncan be regarded as optimistic RL. That optimism is intuitively specified by a\nhyperparameter converted from an uncertainty parameter. In addition, it can be\nenhanced when it is integrated with prioritized experience replay and\neligibility traces, both of which accelerate learning. The effects of this\nexpected optimism was investigated through learning tendencies on numerical\nsimulations using Pybullet. As a result, moderate optimism accelerated learning\nand yielded higher rewards. In a realistic robotic simulation, the proposed\nmethod with the moderate optimism outperformed one of the state-of-the-art RL\nmethod.",
          "link": "http://arxiv.org/abs/2105.12991",
          "publishedOn": "2021-05-28T01:42:15.606Z",
          "wordCount": 616,
          "title": "Optimistic Reinforcement Learning by Forward Kullback-Leibler Divergence Optimization. (arXiv:2105.12991v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12893",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bai_Y/0/1/0/all/0/1\">Yuanlu Bai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Balch_T/0/1/0/all/0/1\">Tucker Balch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_H/0/1/0/all/0/1\">Haoxian Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dervovic_D/0/1/0/all/0/1\">Danial Dervovic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lam_H/0/1/0/all/0/1\">Henry Lam</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vyetrenko_S/0/1/0/all/0/1\">Svitlana Vyetrenko</a>",
          "description": "Stochastic simulation aims to compute output performance for complex models\nthat lack analytical tractability. To ensure accurate prediction, the model\nneeds to be calibrated and validated against real data. Conventional methods\napproach these tasks by assessing the model-data match via simple hypothesis\ntests or distance minimization in an ad hoc fashion, but they can encounter\nchallenges arising from non-identifiability and high dimensionality. In this\npaper, we investigate a framework to develop calibration schemes that satisfy\nrigorous frequentist statistical guarantees, via a basic notion that we call\neligibility set designed to bypass non-identifiability via a set-based\nestimation. We investigate a feature extraction-then-aggregation approach to\nconstruct these sets that target at multivariate outputs. We demonstrate our\nmethodology on several numerical examples, including an application to\ncalibration of a limit order book market simulator (ABIDES).",
          "link": "http://arxiv.org/abs/2105.12893",
          "publishedOn": "2021-05-28T01:42:15.599Z",
          "wordCount": 571,
          "title": "Calibrating Over-Parametrized Simulation Models: A Framework via Eligibility Set. (arXiv:2105.12893v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stosic_D/0/1/0/all/0/1\">Darko Stosic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stosic_D/0/1/0/all/0/1\">Dusan Stosic</a>",
          "description": "While larger neural models are pushing the boundaries of what deep learning\ncan do, often more weights are needed to train models rather than to run\ninference for tasks. This paper seeks to understand this behavior using search\nspaces -- adding weights creates extra degrees of freedom that form new paths\nfor optimization (or wider search spaces) rendering neural model training more\neffective. We then show how we can augment search spaces to train sparse models\nattaining competitive scores across dozens of deep learning workloads. They are\nalso are tolerant of structures targeting current hardware, opening avenues for\ntraining and inference acceleration. Our work encourages research to explore\nbeyond massive neural models being used today.",
          "link": "http://arxiv.org/abs/2105.12920",
          "publishedOn": "2021-05-28T01:42:15.591Z",
          "wordCount": 531,
          "title": "Search Spaces for Neural Model Training. (arXiv:2105.12920v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dong Eui Chang</a>",
          "description": "This paper presents a vision-based modularized drone racing navigation system\nthat uses a customized convolutional neural network (CNN) for the perception\nmodule to produce high-level navigation commands and then leverages a\nstate-of-the-art planner and controller to generate low-level control commands,\nthus exploiting the advantages of both data-based and model-based approaches.\nUnlike the state-of-the-art method which only takes the current camera image as\nthe CNN input, we further add the latest three drone states as part of the\ninputs. Our method outperforms the state-of-the-art method in various track\nlayouts and offers two switchable navigation behaviors with a single trained\nnetwork. The CNN-based perception module is trained to imitate an expert policy\nthat automatically generates ground truth navigation commands based on the\npre-computed global trajectories. Owing to the extensive randomization and our\nmodified dataset aggregation (DAgger) policy during data collection, our\nnavigation system, which is purely trained in simulation with synthetic\ntextures, successfully operates in environments with randomly-chosen\nphotorealistic textures without further fine-tuning.",
          "link": "http://arxiv.org/abs/2105.12923",
          "publishedOn": "2021-05-28T01:42:15.584Z",
          "wordCount": 613,
          "title": "Robust Navigation for Racing Drones based on Imitation Learning and Modularization. (arXiv:2105.12923v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carrasco_J/0/1/0/all/0/1\">Jacinto Carrasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markova_I/0/1/0/all/0/1\">Irina Markova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_D/0/1/0/all/0/1\">David L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilera_I/0/1/0/all/0/1\">Ignacio Aguilera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_D/0/1/0/all/0/1\">Diego Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Barzana_M/0/1/0/all/0/1\">Marta Garc&#xed;a-Barzana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_Rodil_M/0/1/0/all/0/1\">Manuel Arias-Rodil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luengo_J/0/1/0/all/0/1\">Juli&#xe1;n Luengo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1\">Francisco Herrera</a>",
          "description": "The research in anomaly detection lacks a unified definition of what\nrepresents an anomalous instance. Discrepancies in the nature itself of an\nanomaly lead to multiple paradigms of algorithms design and experimentation.\nPredictive maintenance is a special case, where the anomaly represents a\nfailure that must be prevented. Related time-series research as outlier and\nnovelty detection or time-series classification does not apply to the concept\nof an anomaly in this field, because they are not single points which have not\nbeen seen previously and may not be precisely annotated. Moreover, due to the\nlack of annotated anomalous data, many benchmarks are adapted from supervised\nscenarios.\n\nTo address these issues, we generalise the concept of positive and negative\ninstances to intervals to be able to evaluate unsupervised anomaly detection\nalgorithms. We also preserve the imbalance scheme for evaluation through the\nproposal of the Preceding Window ROC, a generalisation for the calculation of\nROC curves for time-series scenarios. We also adapt the mechanism from a\nestablished time-series anomaly detection benchmark to the proposed\ngeneralisations to reward early detection. Therefore, the proposal represents a\nflexible evaluation framework for the different scenarios. To show the\nusefulness of this definition, we include a case study of Big Data algorithms\nwith a real-world time-series problem provided by the company ArcelorMittal,\nand compare the proposal with an evaluation method.",
          "link": "http://arxiv.org/abs/2105.12818",
          "publishedOn": "2021-05-28T01:42:15.564Z",
          "wordCount": 677,
          "title": "Anomaly Detection in Predictive Maintenance: A New Evaluation Framework for Temporal Unsupervised Anomaly Detection Algorithms. (arXiv:2105.12818v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allamanis_M/0/1/0/all/0/1\">Miltiadis Allamanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_Flux_H/0/1/0/all/0/1\">Henry Jackson-Flux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1\">Marc Brockschmidt</a>",
          "description": "Machine learning-based program analyses have recently shown the promise of\nintegrating formal and probabilistic reasoning towards aiding software\ndevelopment. However, in the absence of large annotated corpora, training these\nanalyses is challenging. Towards addressing this, we present BugLab, an\napproach for self-supervised learning of bug detection and repair. BugLab\nco-trains two models: (1) a detector model that learns to detect and repair\nbugs in code, (2) a selector model that learns to create buggy code for the\ndetector to use as training data. A Python implementation of BugLab improves by\nup to 30% upon baseline methods on a test dataset of 2374 real-life bugs and\nfinds 19 previously unknown bugs in open-source software.",
          "link": "http://arxiv.org/abs/2105.12787",
          "publishedOn": "2021-05-28T01:42:15.555Z",
          "wordCount": 534,
          "title": "Self-Supervised Bug Detection and Repair. (arXiv:2105.12787v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chun-Ta Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1\">Da-Cheng Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yicheng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dlabal_J/0/1/0/all/0/1\">Jan Dlabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_A/0/1/0/all/0/1\">Arjun Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heydon_A/0/1/0/all/0/1\">Allan Heydon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferng_C/0/1/0/all/0/1\">Chun-Sung Ferng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyara_R/0/1/0/all/0/1\">Reah Miyara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuxman_A/0/1/0/all/0/1\">Ariel Fuxman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_F/0/1/0/all/0/1\">Futang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duerig_T/0/1/0/all/0/1\">Tom Duerig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomkins_A/0/1/0/all/0/1\">Andrew Tomkins</a>",
          "description": "In this work, we propose CARLS, a novel framework for augmenting the capacity\nof existing deep learning frameworks by enabling multiple components -- model\ntrainers, knowledge makers and knowledge banks -- to concertedly work together\nin an asynchronous fashion across hardware platforms. The proposed CARLS is\nparticularly suitable for learning paradigms where model training benefits from\nadditional knowledge inferred or discovered during training, such as node\nembeddings for graph neural networks or reliable pseudo labels from model\npredictions. We also describe three learning paradigms -- semi-supervised\nlearning, curriculum learning and multimodal learning -- as examples that can\nbe scaled up efficiently by CARLS. One version of CARLS has been open-sourced\nand available for download at:\nhttps://github.com/tensorflow/neural-structured-learning/tree/master/research/carls",
          "link": "http://arxiv.org/abs/2105.12849",
          "publishedOn": "2021-05-28T01:42:15.545Z",
          "wordCount": 562,
          "title": "CARLS: Cross-platform Asynchronous Representation Learning System. (arXiv:2105.12849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruoming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>",
          "description": "Recently, linear regression models, such as EASE and SLIM, have shown to\noften produce rather competitive results against more sophisticated deep\nlearning models. On the other side, the (weighted) matrix factorization\napproaches have been popular choices for recommendation in the past and widely\nadopted in the industry. In this work, we aim to theoretically understand the\nrelationship between these two approaches, which are the cornerstones of\nmodel-based recommendations. Through the derivation and analysis of the\nclosed-form solutions for two basic regression and matrix factorization\napproaches, we found these two approaches are indeed inherently related but\nalso diverge in how they \"scale-down\" the singular values of the original\nuser-item interaction matrix. This analysis also helps resolve the questions\nrelated to the regularization parameter range and model complexities. We\nfurther introduce a new learning algorithm in searching (hyper)parameters for\nthe closed-form solution and utilize it to discover the nearby models of the\nexisting solutions. The experimental results demonstrate that the basic models\nand their closed-form solutions are indeed quite competitive against the\nstate-of-the-art models, thus, confirming the validity of studying the basic\nmodels. The effectiveness of exploring the nearby models are also\nexperimentally validated.",
          "link": "http://arxiv.org/abs/2105.12937",
          "publishedOn": "2021-05-28T01:42:15.538Z",
          "wordCount": 630,
          "title": "Towards a Better Understanding of Linear Models for Recommendation. (arXiv:2105.12937v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12941",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1\">Jilei Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Negoescu_D/0/1/0/all/0/1\">Diana Negoescu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ahammad_P/0/1/0/all/0/1\">Parvez Ahammad</a>",
          "description": "Predictive machine learning models often lack interpretability, resulting in\nlow trust from model end users despite having high predictive performance.\nWhile many model interpretation approaches return top important features to\nhelp interpret model predictions, these top features may not be well-organized\nor intuitive to end users, which limits model adoption rates. In this paper, we\npropose Intellige, a user-facing model explainer that creates user-digestible\ninterpretations and insights reflecting the rationale behind model predictions.\nIntellige builds an end-to-end pipeline from machine learning platforms to end\nuser platforms, and provides users with an interface for implementing model\ninterpretation approaches and for customizing narrative insights. Intellige is\na platform consisting of four components: Model Importer, Model Interpreter,\nNarrative Generator, and Narrative Exporter. We describe these components, and\nthen demonstrate the effectiveness of Intellige through use cases at LinkedIn.\nQuantitative performance analyses indicate that Intellige's narrative insights\nlead to lifts in adoption rates of predictive model recommendations, as well as\nto increases in downstream key metrics such as revenue when compared to\nprevious approaches, while qualitative analyses indicate positive feedback from\nend users.",
          "link": "http://arxiv.org/abs/2105.12941",
          "publishedOn": "2021-05-28T01:42:15.530Z",
          "wordCount": 604,
          "title": "Intellige: A User-Facing Model Explainer for Narrative Explanations. (arXiv:2105.12941v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shriver_D/0/1/0/all/0/1\">David Shriver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbaum_S/0/1/0/all/0/1\">Sebastian Elbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwyer_M/0/1/0/all/0/1\">Matthew B. Dwyer</a>",
          "description": "Despite the large number of sophisticated deep neural network (DNN)\nverification algorithms, DNN verifier developers, users, and researchers still\nface several challenges. First, verifier developers must contend with the\nrapidly changing DNN field to support new DNN operations and property types.\nSecond, verifier users have the burden of selecting a verifier input format to\nspecify their problem. Due to the many input formats, this decision can greatly\nrestrict the verifiers that a user may run. Finally, researchers face\ndifficulties in re-using benchmarks to evaluate and compare verifiers, due to\nthe large number of input formats required to run different verifiers. Existing\nbenchmarks are rarely in formats supported by verifiers other than the one for\nwhich the benchmark was introduced. In this work we present DNNV, a framework\nfor reducing the burden on DNN verifier researchers, developers, and users.\nDNNV standardizes input and output formats, includes a simple yet expressive\nDSL for specifying DNN properties, and provides powerful simplification and\nreduction operations to facilitate the application, development, and comparison\nof DNN verifiers. We show how DNNV increases the support of verifiers for\nexisting benchmarks from 30% to 74%.",
          "link": "http://arxiv.org/abs/2105.12841",
          "publishedOn": "2021-05-28T01:42:15.503Z",
          "wordCount": 613,
          "title": "DNNV: A Framework for Deep Neural Network Verification. (arXiv:2105.12841v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shivanandamurthy_S/0/1/0/all/0/1\">Supreeth Mysore Shivanandamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_I/0/1/0/all/0/1\">Ishan. G. Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_S/0/1/0/all/0/1\">Sayed Ahmad Salehi</a>",
          "description": "With the rapidly growing use of Convolutional Neural Networks (CNNs) in\nreal-world applications related to machine learning and Artificial Intelligence\n(AI), several hardware accelerator designs for CNN inference and training have\nbeen proposed recently. In this paper, we present ATRIA, a novel bit-pArallel\nsTochastic aRithmetic based In-DRAM Accelerator for energy-efficient and\nhigh-speed inference of CNNs. ATRIA employs light-weight modifications in DRAM\ncell arrays to implement bit-parallel stochastic arithmetic based acceleration\nof multiply-accumulate (MAC) operations inside DRAM. ATRIA significantly\nimproves the latency, throughput, and efficiency of processing CNN inferences\nby performing 16 MAC operations in only five consecutive memory operation\ncycles. We mapped the inference tasks of four benchmark CNNs on ATRIA to\ncompare its performance with five state-of-the-art in-DRAM CNN accelerators\nfrom prior work. The results of our analysis show that ATRIA exhibits only 3.5%\ndrop in CNN inference accuracy and still achieves improvements of up to 3.2x in\nframes-per-second (FPS) and up to 10x in efficiency (FPS/W/mm2), compared to\nthe best-performing in-DRAM accelerator from prior work.",
          "link": "http://arxiv.org/abs/2105.12781",
          "publishedOn": "2021-05-28T01:42:15.496Z",
          "wordCount": 617,
          "title": "ATRIA: A Bit-Parallel Stochastic Arithmetic Based Accelerator for In-DRAM CNN Processing. (arXiv:2105.12781v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lambeta_M/0/1/0/all/0/1\">Mike Lambeta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huazhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_P/0/1/0/all/0/1\">Po-Wei Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaoxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1\">Roberto Calandra</a>",
          "description": "With the increased availability of rich tactile sensors, there is an equally\nproportional need for open-source and integrated software capable of\nefficiently and effectively processing raw touch measurements into high-level\nsignals that can be used for control and decision-making. In this paper, we\npresent PyTouch -- the first machine learning library dedicated to the\nprocessing of touch sensing signals. PyTouch, is designed to be modular,\neasy-to-use and provides state-of-the-art touch processing capabilities as a\nservice with the goal of unifying the tactile sensing community by providing a\nlibrary for building scalable, proven, and performance-validated modules over\nwhich applications and research can be built upon. We evaluate PyTouch on\nreal-world data from several tactile sensors on touch processing tasks such as\ntouch detection, slip and object pose estimations. PyTouch is open-sourced at\nhttps://github.com/facebookresearch/pytouch .",
          "link": "http://arxiv.org/abs/2105.12791",
          "publishedOn": "2021-05-28T01:42:15.489Z",
          "wordCount": 578,
          "title": "PyTouch: A Machine Learning Library for Touch Processing. (arXiv:2105.12791v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1\">Mark Sellke</a>",
          "description": "Classically, data interpolation with a parametrized model class is possible\nas long as the number of parameters is larger than the number of equations to\nbe satisfied. A puzzling phenomenon in deep learning is that models are trained\nwith many more parameters than what this classical theory would suggest. We\npropose a theoretical explanation for this phenomenon. We prove that for a\nbroad class of data distributions and model classes, overparametrization is\nnecessary if one wants to interpolate the data smoothly. Namely we show that\nsmooth interpolation requires $d$ times more parameters than mere\ninterpolation, where $d$ is the ambient data dimension. We prove this universal\nlaw of robustness for any smoothly parametrized function class with polynomial\nsize weights, and any covariate distribution verifying isoperimetry. In the\ncase of two-layers neural networks and Gaussian covariates, this law was\nconjectured in prior work by Bubeck, Li and Nagaraj.",
          "link": "http://arxiv.org/abs/2105.12806",
          "publishedOn": "2021-05-28T01:42:15.483Z",
          "wordCount": 570,
          "title": "A Universal Law of Robustness via Isoperimetry. (arXiv:2105.12806v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>",
          "description": "We study the problem of best arm identification in linear bandits in the\nfixed-budget setting. By leveraging properties of the G-optimal design and\nincorporating it into the arm allocation rule, we design a parameter-free\nalgorithm, Optimal Design-based Linear Best Arm Identification (OD-LinBAI). We\nprovide a theoretical analysis of the failure probability of OD-LinBAI. While\nthe performances of existing methods (e.g., BayesGap) depend on all the\noptimality gaps, OD-LinBAI depends on the gaps of the top $d$ arms, where $d$\nis the effective dimension of the linear bandit instance. Furthermore, we\npresent a minimax lower bound for this problem. The upper and lower bounds show\nthat OD-LinBAI is minimax optimal up to multiplicative factors in the exponent.\nFinally, numerical experiments corroborate our theoretical findings.",
          "link": "http://arxiv.org/abs/2105.13017",
          "publishedOn": "2021-05-28T01:42:15.477Z",
          "wordCount": 558,
          "title": "Towards Minimax Optimal Best Arm Identification in Linear Bandits. (arXiv:2105.13017v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Prashant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Sabyasachi Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1\">Vanshil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondameedi_V/0/1/0/all/0/1\">Vineetha Kondameedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Abhinav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Akshaj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_C/0/1/0/all/0/1\">Chiranjib Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vinay Viswanathan</a>",
          "description": "Accurate reconstruction of static environments from LiDAR scans of scenes\ncontaining dynamic objects, which we refer to as Dynamic to Static Translation\n(DST), is an important area of research in Autonomous Navigation. This problem\nhas been recently explored for visual SLAM, but to the best of our knowledge no\nwork has been attempted to address DST for LiDAR scans. The problem is of\ncritical importance due to wide-spread adoption of LiDAR in Autonomous\nVehicles. We show that state-of the art methods developed for the visual domain\nwhen adapted for LiDAR scans perform poorly.\n\nWe develop DSLR, a deep generative model which learns a mapping between\ndynamic scan to its static counterpart through an adversarially trained\nautoencoder. Our model yields the first solution for DST on LiDAR that\ngenerates static scans without using explicit segmentation labels. DSLR cannot\nalways be applied to real world data due to lack of paired dynamic-static\nscans. Using Unsupervised Domain Adaptation, we propose DSLR-UDA for transfer\nto real world data and experimentally show that this performs well in real\nworld settings. Additionally, if segmentation information is available, we\nextend DSLR to DSLR-Seg to further improve the reconstruction quality.\n\nDSLR gives the state of the art performance on simulated and real-world\ndatasets and also shows at least 4x improvement. We show that DSLR, unlike the\nexisting baselines, is a practically viable model with its reconstruction\nquality within the tolerable limits for tasks pertaining to autonomous\nnavigation like SLAM in dynamic environments.",
          "link": "http://arxiv.org/abs/2105.12774",
          "publishedOn": "2021-05-28T01:42:15.457Z",
          "wordCount": 708,
          "title": "DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially Trained Autoencoder. (arXiv:2105.12774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12807",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Withnell_E/0/1/0/all/0/1\">Eloise Withnell</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>",
          "description": "Deep learning based approaches have proven promising to model omics data.\nHowever, one of the current limitations compared to statistical and traditional\nmachine learning approaches is the lack of explainability, which not only\nreduces the reliability, but limits the potential for acquiring novel knowledge\nfrom unpicking the \"black-box\" models. Here we present XOmiVAE, a novel\ninterpretable deep learning model for cancer classification using\nhigh-dimensional omics data. XOmiVAE is able to obtain contribution values of\neach gene and latent dimension for a specific prediction, and the correlation\nbetween genes and the latent dimensions. It is also revealed that XOmiVAE can\nexplain both the supervised classification and the unsupervised clustering\nresults from the deep learning network. To the best of our knowledge, XOmiVAE\nis one of the first activated-based deep learning interpretation method to\nexplain novel clusters generated by variational autoencoders. The results\ngenerated by XOmiVAE were validated by both the biomedical knowledge and the\nperformance of downstream tasks. XOmiVAE explanations of deep learning based\ncancer classification and clustering aligned with current domain knowledge\nincluding biological annotation and literature, which shows great potential for\nnovel biomedical knowledge discovery from deep learning models. The top XOmiVAE\nselected genes and dimensions shown significant influence to the performance of\ncancer classification. Additionally, we offer important steps to consider when\ninterpreting deep learning models for tumour classification. For instance, we\ndemonstrate the importance of choosing background samples that makes biological\nsense and the limitations of connection weight based methods to explain latent\ndimensions.",
          "link": "http://arxiv.org/abs/2105.12807",
          "publishedOn": "2021-05-28T01:42:15.395Z",
          "wordCount": 700,
          "title": "XOmiVAE: an interpretable deep learning model for cancer classification using high-dimensional omics data. (arXiv:2105.12807v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1\">Steven Schockaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>",
          "description": "Analogies play a central role in human commonsense reasoning. The ability to\nrecognize analogies such as \"eye is to seeing what ear is to hearing\",\nsometimes referred to as analogical proportions, shape how we structure\nknowledge and understand language. Surprisingly, however, the task of\nidentifying such analogies has not yet received much attention in the language\nmodel era. In this paper, we analyze the capabilities of transformer-based\nlanguage models on this unsupervised task, using benchmarks obtained from\neducational settings, as well as more commonly used datasets. We find that\noff-the-shelf language models can identify analogies to a certain extent, but\nstruggle with abstract and complex relations, and results are highly sensitive\nto model architecture and hyperparameters. Overall the best results were\nobtained with GPT-2 and RoBERTa, while configurations using BERT were not able\nto outperform word embedding models. Our results raise important questions for\nfuture work about how, and to what extent, pre-trained language models capture\nknowledge about abstract semantic relations.",
          "link": "http://arxiv.org/abs/2105.04949",
          "publishedOn": "2021-05-27T01:32:29.916Z",
          "wordCount": 628,
          "title": "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?. (arXiv:2105.04949v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02596",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_H/0/1/0/all/0/1\">Huan Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Decentralized optimization over time-varying graphs has been increasingly\ncommon in modern machine learning with massive data stored on millions of\nmobile devices, such as in federated learning. This paper revisits the widely\nused accelerated gradient tracking and extends it to time-varying graphs. We\nprove the $O((\\frac{\\gamma}{1-\\sigma_{\\gamma}})^2\\sqrt{\\frac{L}{\\epsilon}})$\nand\n$O((\\frac{\\gamma}{1-\\sigma_{\\gamma}})^{1.5}\\sqrt{\\frac{L}{\\mu}}\\log\\frac{1}{\\epsilon})$\ncomplexities for the practical single loop accelerated gradient tracking over\ntime-varying graphs when the problems are nonstrongly convex and strongly\nconvex, respectively, where $\\gamma$ and $\\sigma_{\\gamma}$ are two common\nconstants charactering the network connectivity, $\\epsilon$ is the desired\nprecision, and $L$ and $\\mu$ are the smoothness and strong convexity constants,\nrespectively. Our complexities improve significantly over the ones of\n$O(\\frac{1}{\\epsilon^{5/7}})$ and\n$O((\\frac{L}{\\mu})^{5/7}\\frac{1}{(1-\\sigma)^{1.5}}\\log\\frac{1}{\\epsilon})$,\nrespectively, which were proved in the original literature only for static\ngraphs, where $\\frac{1}{1-\\sigma}$ equals $\\frac{\\gamma}{1-\\sigma_{\\gamma}}$\nwhen the network is time-invariant. When combining with a multiple consensus\nsubroutine, the dependence on the network connectivity constants can be further\nimproved to $O(1)$ and $O(\\frac{\\gamma}{1-\\sigma_{\\gamma}})$ for the\ncomputation and communication complexities, respectively. When the network is\nstatic, by employing the Chebyshev acceleration, our complexities exactly match\nthe lower bounds without hiding any poly-logarithmic factor for both\nnonstrongly convex and strongly convex problems.",
          "link": "http://arxiv.org/abs/2104.02596",
          "publishedOn": "2021-05-27T01:32:29.890Z",
          "wordCount": 652,
          "title": "Accelerated Gradient Tracking over Time-varying Graphs for Decentralized Optimization. (arXiv:2104.02596v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huaizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shanshan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yong Luo</a>",
          "description": "AI engineering has emerged as a crucial discipline to democratize deep neural\nnetwork (DNN) models among software developers with a diverse background. In\nparticular, altering these DNN models in the deployment stage posits a\ntremendous challenge. In this research, we propose and develop a low-code\nsolution, ModelPS (an acronym for \"Model Photoshop\"), to enable and empower\ncollaborative DNN model editing and intelligent model serving. The ModelPS\nsolution embodies two transformative features: 1) a user-friendly web interface\nfor a developer team to share and edit DNN models pictorially, in a low-code\nfashion, and 2) a model genie engine in the backend to aid developers in\ncustomizing model editing configurations for given deployment requirements or\nconstraints. Our case studies with a wide range of deep learning (DL) models\nshow that the system can tremendously reduce both development and communication\noverheads with improved productivity. The code has been released as an\nopen-source package at GitHub.",
          "link": "http://arxiv.org/abs/2105.08275",
          "publishedOn": "2021-05-27T01:32:29.881Z",
          "wordCount": 620,
          "title": "ModelPS: An Interactive and Collaborative Platform for Editing Pre-trained Models at Scale. (arXiv:2105.08275v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12404",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Tegin_U/0/1/0/all/0/1\">U&#x11f;ur Te&#x11f;in</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yildirim_M/0/1/0/all/0/1\">Mustafa Y&#x131;ld&#x131;r&#x131;m</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Oguz_I/0/1/0/all/0/1\">&#x130;lker O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Moser_C/0/1/0/all/0/1\">Christophe Moser</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Psaltis_D/0/1/0/all/0/1\">Demetri Psaltis</a>",
          "description": "Today's heavy machine learning tasks are fueled by large datasets. Computing\nis performed with power hungry processors whose performance is ultimately\nlimited by the data transfer to and from memory. Optics is one of the powerful\nmeans of communicating and processing information and there is intense current\ninterest in optical information processing for realizing high-speed\ncomputations. Here we present and experimentally demonstrate an optical\ncomputing framework based on spatiotemporal effects in multimode fibers for a\nrange of learning tasks from classifying COVID-19 X-ray lung images and speech\nrecognition to predicting age from face images. The presented framework\novercomes the energy scaling problem of existing systems without compromising\nspeed. We leveraged simultaneous, linear, and nonlinear interaction of spatial\nmodes as a computation engine. We numerically and experimentally showed the\nability of the method to execute several different tasks with accuracy\ncomparable to a digital implementation.",
          "link": "http://arxiv.org/abs/2012.12404",
          "publishedOn": "2021-05-27T01:32:29.872Z",
          "wordCount": 646,
          "title": "Scalable Optical Learning Operator. (arXiv:2012.12404v2 [physics.optics] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_A/0/1/0/all/0/1\">Athresh Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1\">Sriraam Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "While probabilistic models are an important tool for studying causality,\ndoing so suffers from the intractability of inference. As a step towards\ntractable causal models, we consider the problem of learning interventional\ndistributions using sum-product networks (SPNs) that are over-parameterized by\ngate functions, e.g., neural networks. Providing an arbitrarily intervened\ncausal graph as input, effectively subsuming Pearl's do-operator, the gate\nfunction predicts the parameters of the SPN. The resulting interventional SPNs\nare motivated and illustrated by a structural causal model themed around\npersonal health. Our empirical evaluation on three benchmark data sets as well\nas a synthetic health data set clearly demonstrates that interventional SPNs\nindeed are both expressive in modelling and flexible in adapting to the\ninterventions.",
          "link": "http://arxiv.org/abs/2102.10440",
          "publishedOn": "2021-05-27T01:32:29.854Z",
          "wordCount": 603,
          "title": "Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models. (arXiv:2102.10440v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haibin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dapeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1\">Bryan Kian Hsiang Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1\">Patrick Jaillet</a>",
          "description": "Deep Gaussian processes (DGPs), a hierarchical composition of GP models, have\nsuccessfully boosted the expressive power of their single-layer counterpart.\nHowever, it is impossible to perform exact inference in DGPs, which has\nmotivated the recent development of variational inference-based methods.\nUnfortunately, either these methods yield a biased posterior belief or it is\ndifficult to evaluate their convergence. This paper introduces a new approach\nfor specifying flexible, arbitrarily complex, and scalable approximate\nposterior distributions. The posterior distribution is constructed through a\nnormalizing flow (NF) which transforms a simple initial probability into a more\ncomplex one through a sequence of invertible transformations. Moreover, a novel\nconvolutional normalizing flow (CNF) is developed to improve the time\nefficiency and capture dependency between layers. Empirical evaluation shows\nthat CNF DGP outperforms the state-of-the-art approximation methods for DGPs.",
          "link": "http://arxiv.org/abs/2104.08472",
          "publishedOn": "2021-05-27T01:32:29.838Z",
          "wordCount": 616,
          "title": "Convolutional Normalizing Flows for Deep Gaussian Processes. (arXiv:2104.08472v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lizhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Guojin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lei Zhao</a>",
          "description": "Which samples should be labelled in a large data set is one of the most\nimportant problems for trainingof deep learning. So far, a variety of active\nsample selection strategies related to deep learning havebeen proposed in many\nliteratures. We defined them as Active Deep Learning (ADL) only if\ntheirpredictor is deep model, where the basic learner is called as predictor\nand the labeling schemes iscalled selector. In this survey, three fundamental\nfactors in selector designation were summarized. Wecategory ADL into\nmodel-driven ADL and data-driven ADL, by whether its selector is model-drivenor\ndata-driven. The different characteristics of the two major type of ADL were\naddressed in indetail respectively. Furthermore, different sub-classes of\ndata-driven and model-driven ADL are alsosummarized and discussed emphatically.\nThe advantages and disadvantages between data-driven ADLand model-driven ADL\nare thoroughly analyzed. We pointed out that, with the development of\ndeeplearning, the selector in ADL also is experiencing the stage from\nmodel-driven to data-driven. Finally,we make discussion on ADL about its\nuncertainty, explanatory, foundations of cognitive science etc.and survey on\nthe trend of ADL from model-driven to data-driven.",
          "link": "http://arxiv.org/abs/2101.09933",
          "publishedOn": "2021-05-27T01:32:29.833Z",
          "wordCount": 634,
          "title": "A Survey on Active Deep Learning: From Model-driven to Data-driven. (arXiv:2101.09933v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dornheim_J/0/1/0/all/0/1\">Johannes Dornheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morand_L/0/1/0/all/0/1\">Lukas Morand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeitvogel_S/0/1/0/all/0/1\">Samuel Zeitvogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iraki_T/0/1/0/all/0/1\">Tarek Iraki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Link_N/0/1/0/all/0/1\">Norbert Link</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helm_D/0/1/0/all/0/1\">Dirk Helm</a>",
          "description": "A major goal of materials design is to find material structures with desired\nproperties and in a second step to find a processing path to reach one of these\nstructures. In this paper, we propose and investigate a deep reinforcement\nlearning approach for the optimization of processing paths. The goal is to find\noptimal processing paths in the material structure space that lead to\ntarget-structures, which have been identified beforehand to result in desired\nmaterial properties. As the relation between properties and structures is\ngenerally non-unique, typically a whole set of target-structures can be\nidentified, that lead to desired properties. Our proposed method optimizes\nprocessing paths from a start structure to one of these equivalent\ntarget-structures. The algorithm learns to find near-optimal paths by\ninteracting with the structure-generating process. It is guided by structure\ndescriptors as process state features and a reward signal, which is formulated\nbased on a distance function in the structure space. The model-free\nreinforcement learning algorithm learns through trial and error while\ninteracting with the process and does not rely on a priori sampled processing\ndata. We instantiate and evaluate the proposed methods by optimizing paths of a\ngeneric metal forming process.",
          "link": "http://arxiv.org/abs/2009.09706",
          "publishedOn": "2021-05-27T01:32:29.811Z",
          "wordCount": 684,
          "title": "Deep Reinforcement Learning Methods for Structure-Guided Processing Path Optimization. (arXiv:2009.09706v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghazadeh_A/0/1/0/all/0/1\">Amirali Aghazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vipul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeWeese_A/0/1/0/all/0/1\">Alex DeWeese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyluoglu_O/0/1/0/all/0/1\">O. Ozan Koyluoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>",
          "description": "We consider feature selection for applications in machine learning where the\ndimensionality of the data is so large that it exceeds the working memory of\nthe (local) computing machine. Unfortunately, current large-scale sketching\nalgorithms show poor memory-accuracy trade-off due to the irreversible\ncollision and accumulation of the stochastic gradient noise in the sketched\ndomain. Here, we develop a second-order ultra-high dimensional feature\nselection algorithm, called BEAR, which avoids the extra collisions by storing\nthe second-order gradients in the celebrated Broyden-Fletcher-Goldfarb-Shannon\n(BFGS) algorithm in Count Sketch, a sublinear memory data structure from the\nstreaming literature. Experiments on real-world data sets demonstrate that BEAR\nrequires up to three orders of magnitude less memory space to achieve the same\nclassification accuracy compared to the first-order sketching algorithms.\nTheoretical analysis proves convergence of BEAR with rate O(1/t) in t\niterations of the sketched algorithm. Our algorithm reveals an unexplored\nadvantage of second-order optimization for memory-constrained sketching of\nmodels trained on ultra-high dimensional data sets.",
          "link": "http://arxiv.org/abs/2010.13829",
          "publishedOn": "2021-05-27T01:32:29.805Z",
          "wordCount": 625,
          "title": "BEAR: Sketching BFGS Algorithm for Ultra-High Dimensional Feature Selection in Sublinear Memory. (arXiv:2010.13829v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenhui Li</a>",
          "description": "The heavy traffic and related issues have always been concerns for modern\ncities. With the help of deep learning and reinforcement learning, people have\nproposed various policies to solve these traffic-related problems, such as\nsmart traffic signal control systems and taxi dispatching systems. People\nusually validate these policies in a city simulator, since directly applying\nthem in the real city introduces real cost. However, these policies validated\nin the city simulator may fail in the real city if the simulator is\nsignificantly different from the real world. To tackle this problem, we need to\nbuild a real-like traffic simulation system. Therefore, in this paper, we\npropose to learn the human routing model, which is one of the most essential\npart in the traffic simulator. This problem has two major challenges. First,\nhuman routing decisions are determined by multiple factors, besides the common\ntime and distance factor. Second, current historical routes data usually covers\njust a small portion of vehicles, due to privacy and device availability\nissues. To address these problems, we propose a theory-guided residual network\nmodel, where the theoretical part can emphasize the general principles for\nhuman routing decisions (e.g., fastest route), and the residual part can\ncapture drivable condition preferences (e.g., local road or highway). Since the\ntheoretical part is composed of traditional shortest path algorithms that do\nnot need data to train, our residual network can learn human routing models\nfrom limited data. We have conducted extensive experiments on multiple\nreal-world datasets to show the superior performance of our model, especially\nwith small data. Besides, we have also illustrated why our model is better at\nrecovering real routes through case studies.",
          "link": "http://arxiv.org/abs/2105.08279",
          "publishedOn": "2021-05-27T01:32:29.767Z",
          "wordCount": 719,
          "title": "Learning to Route via Theory-Guided Residual Network. (arXiv:2105.08279v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianjin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1\">Yulong Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>",
          "description": "Recent years have witnessed an upsurge of interest in the problem of anomaly\ndetection on attributed networks due to its importance in both research and\npractice. Although various approaches have been proposed to solve this problem,\ntwo major limitations exist: (1) unsupervised approaches usually work much less\nefficiently due to the lack of supervisory signal, and (2) existing anomaly\ndetection methods only use local contextual information to detect anomalous\nnodes, e.g., one- or two-hop information, but ignore the global contextual\ninformation. Since anomalous nodes differ from normal nodes in structures and\nattributes, it is intuitive that the distance between anomalous nodes and their\nneighbors should be larger than that between normal nodes and their neighbors\nif we remove the edges connecting anomalous and normal nodes. Thus, hop counts\nbased on both global and local contextual information can be served as the\nindicators of anomaly. Motivated by this intuition, we propose a hop-count\nbased model (HCM) to detect anomalies by modeling both local and global\ncontextual information. To make better use of hop counts for anomaly\nidentification, we propose to use hop counts prediction as a self-supervised\ntask. We design two anomaly scores based on the hop counts prediction via HCM\nmodel to identify anomalies. Besides, we employ Bayesian learning to train HCM\nmodel for capturing uncertainty in learned parameters and avoiding overfitting.\nExtensive experiments on real-world attributed networks demonstrate that our\nproposed model is effective in anomaly detection.",
          "link": "http://arxiv.org/abs/2104.07917",
          "publishedOn": "2021-05-27T01:32:29.761Z",
          "wordCount": 706,
          "title": "Hop-Count Based Self-Supervised Anomaly Detection on Attributed Networks. (arXiv:2104.07917v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06868",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>",
          "description": "While the choice of prior is one of the most critical parts of the Bayesian\ninference workflow, recent Bayesian deep learning models have often fallen back\non vague priors, such as standard Gaussians. In this review, we highlight the\nimportance of prior choices for Bayesian deep learning and present an overview\nof different priors that have been proposed for (deep) Gaussian processes,\nvariational autoencoders, and Bayesian neural networks. We also outline\ndifferent methods of learning priors for these models from data. We hope to\nmotivate practitioners in Bayesian deep learning to think more carefully about\nthe prior specification for their models and to provide them with some\ninspiration in this regard.",
          "link": "http://arxiv.org/abs/2105.06868",
          "publishedOn": "2021-05-27T01:32:29.756Z",
          "wordCount": 549,
          "title": "Priors in Bayesian Deep Learning: A Review. (arXiv:2105.06868v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yihang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael K. Ng</a>",
          "description": "In this paper, we study Wasserstein Generative Adversarial Networks (WGANs)\nusing GroupSort neural networks as discriminators. We show that the error bound\nfor the approximation of target distribution depends on both the width/depth\n(capacity) of generators and discriminators, as well as the number of samples\nin training. A quantified generalization bound is established for Wasserstein\ndistance between the generated distribution and the target distribution.\nAccording to our theoretical results, WGANs have higher requirement for the\ncapacity of discriminators than that of generators, which is consistent with\nsome existing theories. More importantly, overly deep and wide (high capacity)\ngenerators may cause worse results (after training) than low capacity\ngenerators if discriminators are not strong enough. Numerical results on the\nsynthetic data (swiss roll) and MNIST data confirm our theoretical results, and\ndemonstrate that the performance by using GroupSort neural networks as\ndiscriminators is better than that of the original WGAN.",
          "link": "http://arxiv.org/abs/2103.10060",
          "publishedOn": "2021-05-27T01:32:29.751Z",
          "wordCount": 605,
          "title": "Approximation Capabilities of Wasserstein Generative Adversarial Networks. (arXiv:2103.10060v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1\">Celia Cintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quanz_B/0/1/0/all/0/1\">Brian Quanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1\">Skyler Speakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinwande_V/0/1/0/all/0/1\">Victor Akinwande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "Deep generative models, such as Variational Autoencoders (VAEs), have been\nemployed widely in computational creativity research. However, such models\ndiscourage out-of-distribution generation to avoid spurious sample generation,\nlimiting their creativity. Thus, incorporating research on human creativity\ninto generative deep learning techniques presents an opportunity to make their\noutputs more compelling and human-like. As we see the emergence of generative\nmodels directed to creativity research, a need for machine learning-based\nsurrogate metrics to characterize creative output from these models is\nimperative. We propose group-based subset scanning to quantify, detect, and\ncharacterize creative processes by detecting a subset of anomalous\nnode-activations in the hidden layers of generative models. Our experiments on\noriginal, typically decoded, and \"creatively decoded\" (Das et al 2020) image\ndatasets reveal that the proposed subset scores distribution is more useful for\ndetecting creative processes in the activation space rather than the pixel\nspace. Further, we found that creative samples generate larger subsets of\nanomalies than normal or non-creative samples across datasets. The node\nactivations highlighted during the creative decoding process are different from\nthose responsible for normal sample generation.",
          "link": "http://arxiv.org/abs/2104.00479",
          "publishedOn": "2021-05-27T01:32:29.746Z",
          "wordCount": 652,
          "title": "Towards creativity characterization of generative models via group-based subset scanning. (arXiv:2104.00479v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10064",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Weber_M/0/1/0/all/0/1\">Maurice Weber</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liu_N/0/1/0/all/0/1\">Nana Liu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhikuan Zhao</a>",
          "description": "Quantum machine learning models have the potential to offer speedups and\nbetter predictive accuracy compared to their classical counterparts. However,\nthese quantum algorithms, like their classical counterparts, have been shown to\nalso be vulnerable to input perturbations, in particular for classification\nproblems. These can arise either from noisy implementations or, as a worst-case\ntype of noise, adversarial attacks. In order to develop defence mechanisms and\nto better understand the reliability of these algorithms, it is crucial to\nunderstand their robustness properties in presence of natural noise sources or\nadversarial manipulation. From the observation that measurements involved in\nquantum classification algorithms are naturally probabilistic, we uncover and\nformalize a fundamental link between binary quantum hypothesis testing and\nprovably robust quantum classification. This link leads to a tight robustness\ncondition which puts constraints on the amount of noise a classifier can\ntolerate, independent of whether the noise source is natural or adversarial.\nBased on this result, we develop practical protocols to optimally certify\nrobustness. Finally, since this is a robustness condition against worst-case\ntypes of noise, our result naturally extends to scenarios where the noise\nsource is known. Thus, we also provide a framework to study the reliability of\nquantum classification protocols beyond the adversarial, worst-case noise\nscenarios.",
          "link": "http://arxiv.org/abs/2009.10064",
          "publishedOn": "2021-05-27T01:32:29.731Z",
          "wordCount": 681,
          "title": "Optimal Provable Robustness of Quantum Classification via Quantum Hypothesis Testing. (arXiv:2009.10064v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhizhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1\">Pengfei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>",
          "description": "Community detection, a fundamental task for network analysis, aims to\npartition a network into multiple sub-structures to help reveal their latent\nfunctions. Community detection has been extensively studied in and broadly\napplied to many real-world network problems. Classical approaches to community\ndetection typically utilize probabilistic graphical models and adopt a variety\nof prior knowledge to infer community structures. As the problems that network\nmethods try to solve and the network data to be analyzed become increasingly\nmore sophisticated, new approaches have also been proposed and developed,\nparticularly those that utilize deep learning and convert networked data into\nlow dimensional representation. Despite all the recent advancement, there is\nstill a lack of insightful understanding of the theoretical and methodological\nunderpinning of community detection, which will be critically important for\nfuture development of the area of network analysis. In this paper, we develop\nand present a unified architecture of network community-finding methods to\ncharacterize the state-of-the-art of the field of community detection.\nSpecifically, we provide a comprehensive review of the existing community\ndetection methods and introduce a new taxonomy that divides the existing\nmethods into two categories, namely probabilistic graphical model and deep\nlearning. We then discuss in detail the main idea behind each method in the two\ncategories. Furthermore, to promote future development of community detection,\nwe release several benchmark datasets from several problem domains and\nhighlight their applications to various network analysis tasks. We conclude\nwith discussions of the challenges of the field and suggestions of possible\ndirections for future research.",
          "link": "http://arxiv.org/abs/2101.01669",
          "publishedOn": "2021-05-27T01:32:29.726Z",
          "wordCount": 738,
          "title": "A Survey of Community Detection Approaches: From Statistical Modeling to Deep Representation. (arXiv:2101.01669v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chu-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaech_A/0/1/0/all/0/1\">Aaron Jaech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>",
          "description": "Standard autoregressive language models perform only polynomial-time\ncomputation to compute the probability of the next symbol. While this is\nattractive, it means they cannot model distributions whose next-symbol\nprobability is hard to compute. Indeed, they cannot even model them well enough\nto solve associated easy decision problems for which an engineer might want to\nconsult a language model. These limitations apply no matter how much\ncomputation and data are used to train the model, unless the model is given\naccess to oracle parameters that grow superpolynomially in sequence length.\n\nThus, simply training larger autoregressive language models is not a panacea\nfor NLP. Alternatives include energy-based models (which give up efficient\nsampling) and latent-variable autoregressive models (which give up efficient\nscoring of a given string). Both are powerful enough to escape the above\nlimitations.",
          "link": "http://arxiv.org/abs/2010.11939",
          "publishedOn": "2021-05-27T01:32:29.720Z",
          "wordCount": 601,
          "title": "Limitations of Autoregressive Models and Their Alternatives. (arXiv:2010.11939v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1\">Xiang-Rong Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liqin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guorui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinyao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Binding Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Siran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jingshan Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hongbo Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>",
          "description": "Traditional industrial recommenders are usually trained on a single business\ndomain and then serve for this domain. However, in large commercial platforms,\nit is often the case that the recommenders need to make click-through rate\n(CTR) predictions for multiple business domains. Different domains have\noverlapping user groups and items. Thus, there exist commonalities. Since the\nspecific user groups have disparity and the user behaviors may change in\nvarious business domains, there also have distinctions. The distinctions result\nin domain-specific data distributions, making it hard for a single shared model\nto work well on all domains. To learn an effective and efficient CTR model to\nhandle multiple domains simultaneously, we present Star Topology Adaptive\nRecommender (STAR). Concretely, STAR has the star topology, which consists of\nthe shared centered parameters and domain-specific parameters. The shared\nparameters are applied to learn commonalities of all domains, and the\ndomain-specific parameters capture domain distinction for more refined\nprediction. Given requests from different business domains, STAR can adapt its\nparameters conditioned on the domain characteristics. The experimental result\nfrom production data validates the superiority of the proposed STAR model.\nSince 2020, STAR has been deployed in the display advertising system of\nAlibaba, obtaining averaging 8.0% improvement on CTR and 6.0% on RPM (Revenue\nPer Mille).",
          "link": "http://arxiv.org/abs/2101.11427",
          "publishedOn": "2021-05-27T01:32:29.715Z",
          "wordCount": 689,
          "title": "One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. (arXiv:2101.11427v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangtong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seijen_H/0/1/0/all/0/1\">Harm van Seijen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Combes_R/0/1/0/all/0/1\">Remi Tachet des Combes</a>",
          "description": "We investigate the discounting mismatch in actor-critic algorithm\nimplementations from a representation learning perspective. Theoretically,\nactor-critic algorithms usually have discounting for both actor and critic,\ni.e., there is a $\\gamma^t$ term in the actor update for the transition\nobserved at time $t$ in a trajectory and the critic is a discounted value\nfunction. Practitioners, however, usually ignore the discounting ($\\gamma^t$)\nfor the actor while using a discounted critic. We investigate this mismatch in\ntwo scenarios. In the first scenario, we consider optimizing an undiscounted\nobjective $(\\gamma = 1)$ where $\\gamma^t$ disappears naturally $(1^t = 1)$. We\nthen propose to interpret the discounting in critic in terms of a\nbias-variance-representation trade-off and provide supporting empirical\nresults. In the second scenario, we consider optimizing a discounted objective\n($\\gamma < 1$) and propose to interpret the omission of the discounting in the\nactor update from an auxiliary task perspective and provide supporting\nempirical results.",
          "link": "http://arxiv.org/abs/2010.01069",
          "publishedOn": "2021-05-27T01:32:29.710Z",
          "wordCount": 628,
          "title": "A Deeper Look at Discounting Mismatch in Actor-Critic Algorithms. (arXiv:2010.01069v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1\">Caglar Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>",
          "description": "Knowledge graph embedding techniques are key to making knowledge graphs\namenable to the plethora of machine learning approaches based on vector\nrepresentations. Link prediction is often used as a proxy to evaluate the\nquality of these embeddings. Given that the creation of benchmarks for link\nprediction is a time-consuming endeavor, most work on the subject matter uses\nonly a few benchmarks. As benchmarks are crucial for the fair comparison of\nalgorithms, ensuring their quality is tantamount to providing a solid ground\nfor developing better solutions to link prediction and ipso facto embedding\nknowledge graphs. First studies of benchmarks pointed to limitations pertaining\nto information leaking from the development to the test fragments of some\nbenchmark datasets. We spotted a further common limitation of three of the\nbenchmarks commonly used for evaluating link prediction approaches:\nout-of-vocabulary entities in the test and validation sets. We provide an\nimplementation of an approach for spotting and removing such entities and\nprovide corrected versions of the datasets WN18RR, FB15K-237, and YAGO3-10. Our\nexperiments on the corrected versions of WN18RR, FB15K-237, and YAGO3-10\nsuggest that the measured performance of state-of-the-art approaches is altered\nsignificantly with p-values <1%, <1.4%, and <1%, respectively. Overall,\nstate-of-the-art approaches gain on average absolute $3.29 \\pm 0.24\\%$ in all\nmetrics on WN18RR. This means that some of the conclusions achieved in previous\nworks might need to be revisited. We provide an open-source implementation of\nour experiments and corrected datasets at at\nhttps://github.com/dice-group/OOV-In-Link-Prediction.",
          "link": "http://arxiv.org/abs/2105.12524",
          "publishedOn": "2021-05-27T01:32:29.694Z",
          "wordCount": 663,
          "title": "Out-of-Vocabulary Entities in Link Prediction. (arXiv:2105.12524v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Changnan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haosen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiajun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shihong Deng</a>",
          "description": "This paper introduces a novel design of model-free reinforcement learning,\nCASA, Critic AS an Actor. CASA follows the actor-critic framework that\nestimates state-value, state-action-value and policy simultaneously. We prove\nthat CASA integrates a consistent path for the policy evaluation and the policy\nimprovement, which completely eliminates the gradient conflict between the\npolicy improvement and the policy evaluation. The policy evaluation is\nequivalent to a compensational policy improvement, which alleviates the\nfunction approximation error, and is also equivalent to an entropy-regularized\npolicy improvement, which prevents the policy from being trapped into a\nsuboptimal solution. Building on this design, an expectation-correct Doubly\nRobust Trace is introduced to learn state-value and state-action-value, and the\nconvergence is guaranteed. Our experiments show that the design achieves\nState-Of-The-Art on Arcade Learning Environment.",
          "link": "http://arxiv.org/abs/2105.03923",
          "publishedOn": "2021-05-27T01:32:29.689Z",
          "wordCount": 577,
          "title": "CASA: A Bridge Between Gradient of Policy Improvement and Policy Evaluation. (arXiv:2105.03923v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Keyulu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mozhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1\">Stefanie Jegelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>",
          "description": "Graph Neural Networks (GNNs) have been studied through the lens of expressive\npower and generalization. However, their optimization properties are less well\nunderstood. We take the first step towards analyzing GNN training by studying\nthe gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that\ndespite the non-convexity of training, convergence to a global minimum at a\nlinear rate is guaranteed under mild assumptions that we validate on real-world\ngraphs. Second, we study what may affect the GNNs' training speed. Our results\nshow that the training of GNNs is implicitly accelerated by skip connections,\nmore depth, and/or a good label distribution. Empirical results confirm that\nour theoretical results for linearized GNNs align with the training behavior of\nnonlinear GNNs. Our results provide the first theoretical support for the\nsuccess of GNNs with skip connections in terms of optimization, and suggest\nthat deep GNNs with skip connections would be promising in practice.",
          "link": "http://arxiv.org/abs/2105.04550",
          "publishedOn": "2021-05-27T01:32:29.678Z",
          "wordCount": 622,
          "title": "Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth. (arXiv:2105.04550v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaoub_A/0/1/0/all/0/1\">Alaaeddine Chaoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voisin_A/0/1/0/all/0/1\">Alexandre Voisin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerisara_C/0/1/0/all/0/1\">Christophe Cerisara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iung_B/0/1/0/all/0/1\">Beno&#xee;t Iung</a>",
          "description": "The remaining Useful Life (RUL) of equipment is defined as the duration\nbetween the current time and its failure. An accurate and reliable prognostic\nof the remaining useful life provides decision-makers with valuable information\nto adopt an appropriate maintenance strategy to maximize equipment utilization\nand avoid costly breakdowns. In this work, we propose an end-to-end deep\nlearning model based on multi-layer perceptron and long short-term memory\nlayers (LSTM) to predict the RUL. After normalization of all data, inputs are\nfed directly to an MLP layers for feature learning, then to an LSTM layer to\ncapture temporal dependencies, and finally to other MLP layers for RUL\nprognostic. The proposed architecture is tested on the NASA commercial modular\naero-propulsion system simulation (C-MAPSS) dataset. Despite its simplicity\nwith respect to other recently proposed models, the model developed outperforms\nthem with a significant decrease in the competition score and in the root mean\nsquare error score between the predicted and the gold value of the RUL. In this\npaper, we will discuss how the proposed end-to-end model is able to achieve\nsuch good results and compare it to other deep learning and state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2104.05049",
          "publishedOn": "2021-05-27T01:32:29.673Z",
          "wordCount": 651,
          "title": "Learning representations with end-to-end models for improved remaining useful life prognostics. (arXiv:2104.05049v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raziei_Z/0/1/0/all/0/1\">Zohreh Raziei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghaddam_M/0/1/0/all/0/1\">Mohsen Moghaddam</a>",
          "description": "Recent advances in deep Reinforcement Learning (RL) have created\nunprecedented opportunities for intelligent automation, where a machine can\nautonomously learn an optimal policy for performing a given task. However,\ncurrent deep RL algorithms predominantly specialize in a narrow range of tasks,\nare sample inefficient, and lack sufficient stability, which in turn hinder\ntheir industrial adoption. This article tackles this limitation by developing\nand testing a Hyper-Actor Soft Actor-Critic (HASAC) RL framework based on the\nnotions of task modularization and transfer learning. The goal of the proposed\nHASAC is to enhance the adaptability of an agent to new tasks by transferring\nthe learned policies of former tasks to the new task via a \"hyper-actor\". The\nHASAC framework is tested on a new virtual robotic manipulation benchmark,\nMeta-World. Numerical experiments show superior performance by HASAC over\nstate-of-the-art deep RL algorithms in terms of reward value, success rate, and\ntask completion time.",
          "link": "http://arxiv.org/abs/2012.01934",
          "publishedOn": "2021-05-27T01:32:29.657Z",
          "wordCount": 608,
          "title": "Adaptable Automation with Modular Deep Reinforcement Learning and Policy Transfer. (arXiv:2012.01934v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1\">Vivien Cabannes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "Machine learning approached through supervised learning requires expensive\nannotation of data. This motivates weakly supervised learning, where data are\nannotated with incomplete yet discriminative information. In this paper, we\nfocus on partial labelling, an instance of weak supervision where, from a given\ninput, we are given a set of potential targets. We review a disambiguation\nprinciple to recover full supervision from weak supervision, and propose an\nempirical disambiguation algorithm. We prove exponential convergence rates of\nour algorithm under classical learnability assumptions, and we illustrate the\nusefulness of our method on practical examples.",
          "link": "http://arxiv.org/abs/2102.02789",
          "publishedOn": "2021-05-27T01:32:29.645Z",
          "wordCount": 577,
          "title": "Disambiguation of weak supervision with exponential convergence rates. (arXiv:2102.02789v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yunzhe Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genc_S/0/1/0/all/0/1\">Sahika Genc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jonathan Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallya_S/0/1/0/all/0/1\">Sunil Mallya</a>",
          "description": "Accelerating learning processes for complex tasks by leveraging previously\nlearned tasks has been one of the most challenging problems in reinforcement\nlearning, especially when the similarity between source and target tasks is\nlow. This work proposes REPresentation And INstance Transfer (REPAINT)\nalgorithm for knowledge transfer in deep reinforcement learning. REPAINT not\nonly transfers the representation of a pre-trained teacher policy in the\non-policy learning, but also uses an advantage-based experience selection\napproach to transfer useful samples collected following the teacher policy in\nthe off-policy learning. Our experimental results on several benchmark tasks\nshow that REPAINT significantly reduces the total training time in generic\ncases of task similarity. In particular, when the source tasks are dissimilar\nto, or sub-tasks of, the target tasks, REPAINT outperforms other baselines in\nboth training-time reduction and asymptotic performance of return scores.",
          "link": "http://arxiv.org/abs/2011.11827",
          "publishedOn": "2021-05-27T01:32:29.637Z",
          "wordCount": 609,
          "title": "REPAINT: Knowledge Transfer in Deep Reinforcement Learning. (arXiv:2011.11827v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05712",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Nazaria_M/0/1/0/all/0/1\">Massimo Nazaria</a>",
          "description": "This report presents a preliminary analysis of an LSTM neural network\ndesigned to predict the accuracy of magnitude estimates computed by Early-est\nduring the first minutes after an earthquake occurs.",
          "link": "http://arxiv.org/abs/2104.05712",
          "publishedOn": "2021-05-27T01:32:29.630Z",
          "wordCount": 498,
          "title": "Predicting the Accuracy of Early-est Earthquake Magnitude Estimates with an LSTM Neural Network: A Preliminary Analysis. (arXiv:2104.05712v2 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_A/0/1/0/all/0/1\">Aurick Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_S/0/1/0/all/0/1\">Sang Keun Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanya_S/0/1/0/all/0/1\">Suhas Jayaram Subramanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_Q/0/1/0/all/0/1\">Qirong Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganger_G/0/1/0/all/0/1\">Gregory R. Ganger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>",
          "description": "Pollux improves scheduling performance in deep learning (DL) clusters by\nadaptively co-optimizing inter-dependent factors both at the per-job level and\nat the cluster-wide level. Most existing schedulers expect users to specify the\nnumber of resources for each job, often leading to inefficient resource use.\nSome recent schedulers choose job resources for users, but do so without\nawareness of how DL training can be re-optimized to better utilize the provided\nresources.\n\nPollux simultaneously considers both aspects. By monitoring the status of\neach job during training, Pollux models how their goodput (a novel metric we\nintroduce that combines system throughput with statistical efficiency) would\nchange by adding or removing resources. Leveraging these information, Pollux\ndynamically (re-)assigns resources to improve cluster-wide goodput, while\nrespecting fairness and continually optimizing each DL job to better utilize\nthose resources.\n\nIn experiments with real DL jobs and with trace-driven simulations, Pollux\nreduces average job completion times by 37-50% relative to state-of-the-art DL\nschedulers, even when they are provided with ideal resource and training\nconfigurations for every job. Pollux promotes fairness among DL jobs competing\nfor resources based on a more meaningful measure of useful job progress, and\nreveals a new opportunity for reducing DL cost in cloud environments. Pollux is\nimplemented and publicly available as part of an open-source project at\nhttps://github.com/petuum/adaptdl.",
          "link": "http://arxiv.org/abs/2008.12260",
          "publishedOn": "2021-05-27T01:32:29.604Z",
          "wordCount": 695,
          "title": "Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning. (arXiv:2008.12260v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun-Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1\">Jacob Abernethy</a>",
          "description": "Over-parametrization has become a popular technique in deep learning. It is\nobserved that by over-parametrization, a larger neural network needs a fewer\ntraining iterations than a smaller one to achieve a certain level of\nperformance -- namely, over-parametrization leads to acceleration in\noptimization. However, despite that over-parametrization is widely used\nnowadays, little theory is available to explain the acceleration due to\nover-parametrization. In this paper, we propose understanding it by studying a\nsimple problem first. Specifically, we consider the setting that there is a\nsingle teacher neuron with quadratic activation, where over-parametrization is\nrealized by having multiple student neurons learn the data generated from the\nteacher neuron. We provably show that over-parametrization helps the iterate\ngenerated by gradient descent to enter the neighborhood of a global optimal\nsolution that achieves zero testing error faster. On the other hand, we also\npoint out an issue regarding the necessity of over-parametrization and study\nhow the scaling of the output neurons affects the convergence time.",
          "link": "http://arxiv.org/abs/2010.01637",
          "publishedOn": "2021-05-27T01:32:29.568Z",
          "wordCount": 627,
          "title": "Understanding How Over-Parametrization Leads to Acceleration: A case of learning a single teacher neuron. (arXiv:2010.01637v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1\">Mingyang Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhi-Ming Ma</a>",
          "description": "We establish upper bounds for the expected excess risk of models trained by\nproper iterative algorithms which approximate the global minima (resp. local\nminima) under convex (resp. non-convex) loss functions. In contrast to the\nexisting bounds, our results are not limited to a specific algorithm e.g.,\nstochastic gradient descent, and the bounds remain small when the sample size\n$n$ is large for an arbitrary number of iterations. In concrete, after a\ncertain number of iterations, the bound under convex loss functions is of order\n$\\tilde{\\mathcal{O}}(1/n)$. Under non-convex loss functions with $d$ model\nparameters such that $d/n$ is smaller than a threshold independent of $n$, the\norder of $\\tilde{\\mathcal{O}}(1/n)$ can be maintained if the empirical risk has\nno spurious local minima with high probability. The bound becomes\n$\\tilde{\\mathcal{O}}(1/\\sqrt{n})$ if we discard the assumption on the empirical\nlocal minima. Technically, we assume the Hessian of the population risk is\nnon-degenerate at each local minima. Under this and some other mild smoothness\nand boundedness assumptions, we establish our results via algorithmic stability\n\\citep{bousquet2002stability} and characterization of the empirical risk\nlandscape. Our bounds are dimensional insensitive and fast converges to zero as\n$n$ goes to infinity. These underscore that with locally strongly convex\npopulation risk, the models trained by proper iterative algorithms generalize\nwell on unseen data even when the loss function is non-convex and $d$ is large.",
          "link": "http://arxiv.org/abs/2012.02456",
          "publishedOn": "2021-05-27T01:32:29.553Z",
          "wordCount": 697,
          "title": "Characterization of Excess Risk for Locally Strongly Convex Population Risk. (arXiv:2012.02456v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12638",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Panapitiya_G/0/1/0/all/0/1\">Gihan Panapitiya</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Girard_M/0/1/0/all/0/1\">Michael Girard</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Hollas_A/0/1/0/all/0/1\">Aaron Hollas</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Murugesan_V/0/1/0/all/0/1\">Vijay Murugesan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Saldanha_E/0/1/0/all/0/1\">Emily Saldanha</a>",
          "description": "Determining the aqueous solubility of molecules is a vital step in many\npharmaceutical, environmental, and energy storage applications. Despite efforts\nmade over decades, there are still challenges associated with developing a\nsolubility prediction model with satisfactory accuracy for many of these\napplications. The goal of this study is to develop a general model capable of\npredicting the solubility of a broad range of organic molecules. Using the\nlargest currently available solubility dataset, we implement deep\nlearning-based models to predict solubility from molecular structure and\nexplore several different molecular representations including molecular\ndescriptors, simplified molecular-input line-entry system (SMILES) strings,\nmolecular graphs, and three-dimensional (3D) atomic coordinates using four\ndifferent neural network architectures - fully connected neural networks\n(FCNNs), recurrent neural networks (RNNs), graph neural networks (GNNs), and\nSchNet. We find that models using molecular descriptors achieve the best\nperformance, with GNN models also achieving good performance. We perform\nextensive error analysis to understand the molecular properties that influence\nmodel performance, perform feature analysis to understand which information\nabout molecular structure is most valuable for prediction, and perform a\ntransfer learning and data size study to understand the impact of data\navailability on model performance.",
          "link": "http://arxiv.org/abs/2105.12638",
          "publishedOn": "2021-05-27T01:32:29.417Z",
          "wordCount": 644,
          "title": "Predicting Aqueous Solubility of Organic Molecules Using Deep Learning Models with Varied Molecular Representations. (arXiv:2105.12638v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Arnab Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1\">Sutanu Gayen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandasamy_S/0/1/0/all/0/1\">Saravanan Kandasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinodchandran_N/0/1/0/all/0/1\">N. V. Vinodchandran</a>",
          "description": "We study the problems of identity and closeness testing of $n$-dimensional\nproduct distributions. Prior works by Canonne, Diakonikolas, Kane and Stewart\n(COLT 2017) and Daskalakis and Pan (COLT 2017) have established tight sample\ncomplexity bounds for non-tolerant testing over a binary alphabet: given two\nproduct distributions $P$ and $Q$ over a binary alphabet, distinguish between\nthe cases $P = Q$ and $d_{\\mathrm{TV}}(P, Q) > \\epsilon$. We build on this\nprior work to give a more comprehensive map of the complexity of testing of\nproduct distributions by investigating tolerant testing with respect to several\nnatural distance measures and over an arbitrary alphabet. Our study gives a\nfine-grained understanding of how the sample complexity of tolerant testing\nvaries with the distance measures for product distributions. In addition, we\nalso extend one of our upper bounds on product distributions to bounded-degree\nBayes nets.",
          "link": "http://arxiv.org/abs/2012.14632",
          "publishedOn": "2021-05-27T01:32:29.411Z",
          "wordCount": 611,
          "title": "Testing Product Distributions: A Closer Look. (arXiv:2012.14632v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>",
          "description": "With the success of the graph embedding model in both academic and industry\nareas, the robustness of graph embedding against adversarial attack inevitably\nbecomes a crucial problem in graph learning. Existing works usually perform the\nattack in a white-box fashion: they need to access the predictions/labels to\nconstruct their adversarial loss. However, the inaccessibility of\npredictions/labels makes the white-box attack impractical to a real graph\nlearning system. This paper promotes current frameworks in a more general and\nflexible sense -- we demand to attack various kinds of graph embedding models\nwith black-box driven. We investigate the theoretical connections between graph\nsignal processing and graph embedding models and formulate the graph embedding\nmodel as a general graph signal process with a corresponding graph filter.\nTherefore, we design a generalized adversarial attacker: GF-Attack. Without\naccessing any labels and model predictions, GF-Attack can perform the attack\ndirectly on the graph filter in a black-box fashion. We further prove that\nGF-Attack can perform an effective attack without knowing the number of layers\nof graph embedding models. To validate the generalization of GF-Attack, we\nconstruct the attacker on four popular graph embedding models. Extensive\nexperiments validate the effectiveness of GF-Attack on several benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2105.12419",
          "publishedOn": "2021-05-27T01:32:29.391Z",
          "wordCount": 660,
          "title": "Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge. (arXiv:2105.12419v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohoney_J/0/1/0/all/0/1\">Jason Mohoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waleffe_R/0/1/0/all/0/1\">Roger Waleffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1\">Theodoros Rekatsinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1\">Shivaram Venkataraman</a>",
          "description": "We propose a new framework for computing the embeddings of large-scale graphs\non a single machine. A graph embedding is a fixed length vector representation\nfor each node (and/or edge-type) in a graph and has emerged as the de-facto\napproach to apply modern machine learning on graphs. We identify that current\nsystems for learning the embeddings of large-scale graphs are bottlenecked by\ndata movement, which results in poor resource utilization and inefficient\ntraining. These limitations require state-of-the-art systems to distribute\ntraining across multiple machines. We propose Marius, a system for efficient\ntraining of graph embeddings that leverages partition caching and buffer-aware\ndata orderings to minimize disk access and interleaves data movement with\ncomputation to maximize utilization. We compare Marius against two\nstate-of-the-art industrial systems on a diverse array of benchmarks. We\ndemonstrate that Marius achieves the same level of accuracy but is up to one\norder of magnitude faster. We also show that Marius can scale training to\ndatasets an order of magnitude beyond a single machine's GPU and CPU memory\ncapacity, enabling training of configurations with more than a billion edges\nand 550 GB of total parameters on a single machine with 16 GB of GPU memory and\n64 GB of CPU memory. Marius is open-sourced at www.marius-project.org.",
          "link": "http://arxiv.org/abs/2101.08358",
          "publishedOn": "2021-05-27T01:32:29.385Z",
          "wordCount": 681,
          "title": "Marius: Learning Massive Graph Embeddings on a Single Machine. (arXiv:2101.08358v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08748",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Castellano_A/0/1/0/all/0/1\">Agustin Castellano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Min_H/0/1/0/all/0/1\">Hancheng Min</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bazerque_J/0/1/0/all/0/1\">Juan Bazerque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mallada_E/0/1/0/all/0/1\">Enrique Mallada</a>",
          "description": "This paper aims to put forward the concept that learning to take safe actions\nin unknown environments, even with probability one guarantees, can be achieved\nwithout the need for an unbounded number of exploratory trials, provided that\none is willing to navigate trade-offs between optimality, level of exposure to\nunsafe events, and the maximum detection time of unsafe actions. We illustrate\nthis concept in two complementary settings. We first focus on the canonical\nmulti-armed bandit problem and seek to study the intrinsic trade-offs of\nlearning safety in the presence of uncertainty. Under mild assumptions on\nsufficient exploration, we provide an algorithm that provably detects all\nunsafe machines in an (expected) finite number of rounds. The analysis also\nunveils a trade-off between the number of rounds needed to secure the\nenvironment and the probability of discarding safe machines. We then consider\nthe problem of finding optimal policies for a Markov Decision Process (MDP)\nwith almost sure constraints. We show that the (action) value function\nsatisfies a barrier-based decomposition which allows for the identification of\nfeasible policies independently of the reward process. Using this\ndecomposition, we develop a Barrier-learning algorithm, that identifies such\nunsafe state-action pairs in a finite expected number of steps. Our analysis\nfurther highlights a trade-off between the time lag for the underlying MDP\nnecessary to detect unsafe actions, and the level of exposure to unsafe events.\nSimulations corroborate our theoretical findings, further illustrating the\naforementioned trade-offs, and suggesting that safety constraints can further\nspeed up the learning process.",
          "link": "http://arxiv.org/abs/2105.08748",
          "publishedOn": "2021-05-27T01:32:29.380Z",
          "wordCount": 715,
          "title": "Learning to Act Safely with Limited Exposure and Almost Sure Certainty. (arXiv:2105.08748v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1\">Celia Cintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1\">Skyler Speakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1\">Girmaw Abebe Tadesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinwande_V/0/1/0/all/0/1\">Victor Akinwande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McFowland_E/0/1/0/all/0/1\">Edward McFowland III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weldemariam_K/0/1/0/all/0/1\">Komminist Weldemariam</a>",
          "description": "Generative Adversarial Networks (GANs) have recently achieved unprecedented\nsuccess in photo-realistic image synthesis from low-dimensional random noise.\nThe ability to synthesize high-quality content at a large scale brings\npotential risks as the generated samples may lead to misinformation that can\ncreate severe social, political, health, and business hazards. We propose\nSubsetGAN to identify generated content by detecting a subset of anomalous\nnode-activations in the inner layers of pre-trained neural networks. These\nnodes, as a group, maximize a non-parametric measure of divergence away from\nthe expected distribution of activations created from real data. This enable us\nto identify synthesised images without prior knowledge of their distribution.\nSubsetGAN efficiently scores subsets of nodes and returns the group of nodes\nwithin the pre-trained classifier that contributed to the maximum score. The\nclassifier can be a general fake classifier trained over samples from multiple\nsources or the discriminator network from different GANs. Our approach shows\nconsistently higher detection power than existing detection methods across\nseveral state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different\nproportions of generated content.",
          "link": "http://arxiv.org/abs/2105.12479",
          "publishedOn": "2021-05-27T01:32:29.374Z",
          "wordCount": 632,
          "title": "Pattern Detection in the Activation Space for Identifying Synthesized Content. (arXiv:2105.12479v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bergamini_L/0/1/0/all/0/1\">Luca Bergamini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yawei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheel_O/0/1/0/all/0/1\">Oliver Scheel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chih Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Blazej Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "In this work, we present a simple end-to-end trainable machine learning\nsystem capable of realistically simulating driving experiences. This can be\nused for the verification of self-driving system performance without relying on\nexpensive and time-consuming road testing. In particular, we frame the\nsimulation problem as a Markov Process, leveraging deep neural networks to\nmodel both state distribution and transition function. These are trainable\ndirectly from the existing raw observations without the need for any\nhandcrafting in the form of plant or kinematic models. All that is needed is a\ndataset of historical traffic episodes. Our formulation allows the system to\nconstruct never seen scenes that unfold realistically reacting to the\nself-driving car's behaviour. We train our system directly from 1,000 hours of\ndriving logs and measure both realism, reactivity of the simulation as the two\nkey properties of the simulation. At the same time, we apply the method to\nevaluate the performance of a recently proposed state-of-the-art ML planning\nsystem trained from human driving logs. We discover this planning system is\nprone to previously unreported causal confusion issues that are difficult to\ntest by non-reactive simulation. To the best of our knowledge, this is the\nfirst work that directly merges highly realistic data-driven simulations with a\nclosed-loop evaluation for self-driving vehicles. We make the data, code, and\npre-trained models publicly available to further stimulate simulation\ndevelopment.",
          "link": "http://arxiv.org/abs/2105.12332",
          "publishedOn": "2021-05-27T01:32:29.369Z",
          "wordCount": 680,
          "title": "SimNet: Learning Reactive Self-driving Simulations from Real-world Observations. (arXiv:2105.12332v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+SM_A/0/1/0/all/0/1\">Akash SM</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph Weischedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>",
          "description": "With the recent advances of open-domain story generation, the lack of\nreliable automatic evaluation metrics becomes an increasingly imperative issue\nthat hinders the fast development of story generation. According to conducted\nresearches in this regard, learnable evaluation metrics have promised more\naccurate assessments by having higher correlations with human judgments. A\ncritical bottleneck of obtaining a reliable learnable evaluation metric is the\nlack of high-quality training data for classifiers to efficiently distinguish\nplausible and implausible machine-generated stories. Previous works relied on\n\\textit{heuristically manipulated} plausible examples to mimic possible system\ndrawbacks such as repetition, contradiction, or irrelevant content in the text\nlevel, which can be \\textit{unnatural} and \\textit{oversimplify} the\ncharacteristics of implausible machine-generated stories. We propose to tackle\nthese issues by generating a more comprehensive set of implausible stories\nusing {\\em plots}, which are structured representations of controllable factors\nused to generate stories. Since these plots are compact and structured, it is\neasier to manipulate them to generate text with targeted undesirable\nproperties, while at the same time maintain the grammatical correctness and\nnaturalness of the generated sentences. To improve the quality of generated\nimplausible stories, we further apply the adversarial filtering procedure\npresented by \\citet{zellers2018swag} to select a more nuanced set of\nimplausible texts. Experiments show that the evaluation metrics trained on our\ngenerated data result in more reliable automatic assessments that correlate\nremarkably better with human judgments compared to the baselines.",
          "link": "http://arxiv.org/abs/2104.05801",
          "publishedOn": "2021-05-27T01:32:29.351Z",
          "wordCount": 702,
          "title": "Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation. (arXiv:2104.05801v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05793",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cai_X/0/1/0/all/0/1\">Xu Cai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gomes_S/0/1/0/all/0/1\">Selwyn Gomes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1\">Jonathan Scarlett</a>",
          "description": "In this paper, we study the problem of Gaussian process (GP) bandits under\nrelaxed optimization criteria stating that any function value above a certain\nthreshold is \"good enough\". On the theoretical side, we study various {\\em\nlenient regret} notions in which all near-optimal actions incur zero penalty,\nand provide upper bounds on the lenient regret for GP-UCB and an elimination\nalgorithm, circumventing the usual $O(\\sqrt{T})$ term (with time horizon $T$)\nresulting from zooming extremely close towards the function maximum. In\naddition, we complement these upper bounds with algorithm-independent lower\nbounds. On the practical side, we consider the problem of finding a single\n\"good action\" according to a known pre-specified threshold, and introduce\nseveral good-action identification algorithms that exploit knowledge of the\nthreshold. We experimentally find that such algorithms can often find a good\naction faster than standard optimization-based approaches.",
          "link": "http://arxiv.org/abs/2102.05793",
          "publishedOn": "2021-05-27T01:32:29.344Z",
          "wordCount": 598,
          "title": "Lenient Regret and Good-Action Identification in Gaussian Process Bandits. (arXiv:2102.05793v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12700",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1\">Luka Murn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1\">Marc Gorriz Blanch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santamaria_M/0/1/0/all/0/1\">Maria Santamaria</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivera_F/0/1/0/all/0/1\">Fiona Rivera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>",
          "description": "Machine learning techniques for more efficient video compression and video\nenhancement have been developed thanks to breakthroughs in deep learning. The\nnew techniques, considered as an advanced form of Artificial Intelligence (AI),\nbring previously unforeseen capabilities. However, they typically come in the\nform of resource-hungry black-boxes (overly complex with little transparency\nregarding the inner workings). Their application can therefore be unpredictable\nand generally unreliable for large-scale use (e.g. in live broadcast). The aim\nof this work is to understand and optimise learned models in video processing\napplications so systems that incorporate them can be used in a more trustworthy\nmanner. In this context, the presented work introduces principles for\nsimplification of learned models targeting improved transparency in\nimplementing machine learning for video production and distribution\napplications. These principles are demonstrated on video compression examples,\nshowing how bitrate savings and reduced complexity can be achieved by\nsimplifying relevant deep learning models.",
          "link": "http://arxiv.org/abs/2105.12700",
          "publishedOn": "2021-05-27T01:32:29.339Z",
          "wordCount": 613,
          "title": "Towards Transparent Application of Machine Learning in Video Processing. (arXiv:2105.12700v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1\">Shan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanzhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paris_C/0/1/0/all/0/1\">Cecile Paris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1\">Surya Nepal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Quan Z. Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "A community reveals the features and connections of its members that are\ndifferent from those in other communities in a network. Detecting communities\nis of great significance in network analysis. Despite the classical spectral\nclustering and statistical inference methods, we notice a significant\ndevelopment of deep learning techniques for community detection in recent years\nwith their advantages in handling high dimensional network data. Hence, a\ncomprehensive overview of community detection's latest progress through deep\nlearning is timely to both academics and practitioners. This survey devises and\nproposes a new taxonomy covering different categories of the state-of-the-art\nmethods, including deep learning-based models upon deep neural networks, deep\nnonnegative matrix factorization and deep sparse filtering. The main category,\ni.e., deep neural networks, is further divided into convolutional networks,\ngraph attention networks, generative adversarial networks and autoencoders. The\nsurvey also summarizes the popular benchmark data sets, model evaluation\nmetrics, and open-source implementations to address experimentation settings.\nWe then discuss the practical applications of community detection in various\ndomains and point to implementation scenarios. Finally, we outline future\ndirections by suggesting challenging topics in this fast-growing deep learning\nfield.",
          "link": "http://arxiv.org/abs/2105.12584",
          "publishedOn": "2021-05-27T01:32:29.327Z",
          "wordCount": 641,
          "title": "A Comprehensive Survey on Community Detection with Deep Learning. (arXiv:2105.12584v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2007.11354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Sin Kit Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1\">Hye-Young Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>",
          "description": "Federated learning is an emerging machine learning paradigm where clients\ntrain models locally and formulate a global model based on the local model\nupdates. To identify the state-of-the-art in federated learning and explore how\nto develop federated learning systems, we perform a systematic literature\nreview from a software engineering perspective, based on 231 primary studies.\nOur data synthesis covers the lifecycle of federated learning system\ndevelopment that includes background understanding, requirement analysis,\narchitecture design, implementation, and evaluation. We highlight and summarise\nthe findings from the results, and identify future trends to encourage\nresearchers to advance their current work.",
          "link": "http://arxiv.org/abs/2007.11354",
          "publishedOn": "2021-05-27T01:32:29.322Z",
          "wordCount": 636,
          "title": "A Systematic Literature Review on Federated Machine Learning: From A Software Engineering Perspective. (arXiv:2007.11354v8 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1810.10368",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dresdner_G/0/1/0/all/0/1\">Gideon Dresdner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Strathmann_H/0/1/0/all/0/1\">Heiko Strathmann</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1\">Gunnar R&#xe4;tsch</a>",
          "description": "Kernel methods on discrete domains have shown great promise for many\nchallenging data types, for instance, biological sequence data and molecular\nstructure data. Scalable kernel methods like Support Vector Machines may offer\ngood predictive performances but do not intrinsically provide uncertainty\nestimates. In contrast, probabilistic kernel methods like Gaussian Processes\noffer uncertainty estimates in addition to good predictive performance but fall\nshort in terms of scalability. While the scalability of Gaussian processes can\nbe improved using sparse inducing point approximations, the selection of these\ninducing points remains challenging. We explore different techniques for\nselecting inducing points on discrete domains, including greedy selection,\ndeterminantal point processes, and simulated annealing. We find that simulated\nannealing, which can select inducing points that are not in the training set,\ncan perform competitively with support vector machines and full Gaussian\nprocesses on synthetic data, as well as on challenging real-world DNA sequence\ndata.",
          "link": "http://arxiv.org/abs/1810.10368",
          "publishedOn": "2021-05-27T01:32:29.317Z",
          "wordCount": 609,
          "title": "Scalable Gaussian Processes on Discrete Domains. (arXiv:1810.10368v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.12430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsakiris_M/0/1/0/all/0/1\">Manolis C. Tsakiris</a>",
          "description": "Despite the popularity of low-rank matrix completion, the majority of its\ntheory has been developed under the assumption of random observation patterns,\nwhereas very little is known about the practically relevant case of non-random\npatterns. Specifically, a fundamental yet largely open question is to describe\npatterns that allow for unique or finitely many completions. This paper\nprovides two such families of patterns for any rank. A key to achieving this is\na novel formulation of low-rank matrix completion in terms of Plucker\ncoordinates, the latter a traditional tool in computer vision. This connection\nis of potential significance to a wide family of matrix and subspace learning\nproblems with incomplete data.",
          "link": "http://arxiv.org/abs/2004.12430",
          "publishedOn": "2021-05-27T01:32:29.312Z",
          "wordCount": 601,
          "title": "Low-rank matrix completion theory via Plucker coordinates. (arXiv:2004.12430v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12204",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Massiani_P/0/1/0/all/0/1\">Pierre-Fran&#xe7;ois Massiani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heim_S/0/1/0/all/0/1\">Steve Heim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Solowjow_F/0/1/0/all/0/1\">Friedrich Solowjow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trimpe_S/0/1/0/all/0/1\">Sebastian Trimpe</a>",
          "description": "The relationship between safety and optimality in control is not well\nunderstood, and they are often seen as important yet conflicting objectives.\nThere is a pressing need to formalize this relationship, especially given the\ngrowing prominence of learning-based methods. Indeed, it is common practice in\nreinforcement learning to simply modify reward functions by penalizing\nfailures, with the penalty treated as a mere heuristic. We rigorously examine\nthis relationship, and formalize the requirements for safe value functions:\nvalue functions that are both optimal for a given task, and enforce safety. We\nreveal the structure of this relationship through a proof of strong duality,\nshowing that there always exists a finite penalty that induces a safe value\nfunction. This penalty is not unique, but upper-unbounded: larger penalties do\nnot harm optimality. Although it is often not possible to compute the minimum\nrequired penalty, we reveal clear structure of how the penalty, rewards,\ndiscount factor, and dynamics interact. This insight suggests practical,\ntheory-guided heuristics to design reward functions for control problems where\nsafety is important.",
          "link": "http://arxiv.org/abs/2105.12204",
          "publishedOn": "2021-05-27T01:32:29.307Z",
          "wordCount": 599,
          "title": "Safe Value Functions. (arXiv:2105.12204v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2005.11890",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Perry_R/0/1/0/all/0/1\">Ronan Perry</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mischler_G/0/1/0/all/0/1\">Gavin Mischler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_R/0/1/0/all/0/1\">Richard Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_T/0/1/0/all/0/1\">Theodore Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chang_A/0/1/0/all/0/1\">Alexander Chang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koul_A/0/1/0/all/0/1\">Arman Koul</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Franz_C/0/1/0/all/0/1\">Cameron Franz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Richard_H/0/1/0/all/0/1\">Hugo Richard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carmichael_I/0/1/0/all/0/1\">Iain Carmichael</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1\">Pierre Ablin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>",
          "description": "As data are generated more and more from multiple disparate sources,\nmultiview data sets, where each sample has features in distinct views, have\nballooned in recent years. However, no comprehensive package exists that\nenables non-specialists to use these methods easily. mvlearn is a Python\nlibrary which implements the leading multiview machine learning methods. Its\nsimple API closely follows that of scikit-learn for increased ease-of-use. The\npackage can be installed from Python Package Index (PyPI) and the conda package\nmanager and is released under the MIT open-source license. The documentation,\ndetailed examples, and all releases are available at\nhttps://mvlearn.github.io/.",
          "link": "http://arxiv.org/abs/2005.11890",
          "publishedOn": "2021-05-27T01:32:29.238Z",
          "wordCount": 585,
          "title": "mvlearn: Multiview Machine Learning in Python. (arXiv:2005.11890v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Alain-Sam Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cont_R/0/1/0/all/0/1\">Rama Cont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossier_A/0/1/0/all/0/1\">Alain Rossier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>",
          "description": "Residual networks (ResNets) have displayed impressive results in pattern\nrecognition and, recently, have garnered considerable theoretical interest due\nto a perceived link with neural ordinary differential equations (neural ODEs).\nThis link relies on the convergence of network weights to a smooth function as\nthe number of layers increases. We investigate the properties of weights\ntrained by stochastic gradient descent and their scaling with network depth\nthrough detailed numerical experiments. We observe the existence of scaling\nregimes markedly different from those assumed in neural ODE literature.\nDepending on certain features of the network architecture, such as the\nsmoothness of the activation function, one may obtain an alternative ODE limit,\na stochastic differential equation or neither of these. These findings cast\ndoubts on the validity of the neural ODE model as an adequate asymptotic\ndescription of deep ResNets and point to an alternative class of differential\nequations as a better description of the deep network limit.",
          "link": "http://arxiv.org/abs/2105.12245",
          "publishedOn": "2021-05-27T01:32:29.231Z",
          "wordCount": 596,
          "title": "Scaling Properties of Deep Residual Networks. (arXiv:2105.12245v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1\">Francesco Croce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>",
          "description": "Adversarial training (AT) in order to achieve adversarial robustness wrt\nsingle $l_p$-threat models has been discussed extensively. However, for\nsafety-critical systems adversarial robustness should be achieved wrt all\n$l_p$-threat models simultaneously. In this paper we develop a simple and\nefficient training scheme to achieve adversarial robustness against the union\nof $l_p$-threat models. Our novel $l_1+l_\\infty$-AT scheme is based on\ngeometric considerations of the different $l_p$-balls and costs as much as\nnormal adversarial training against a single $l_p$-threat model. Moreover, we\nshow that using our $l_1+l_\\infty$-AT scheme one can fine-tune with just 3\nepochs any $l_p$-robust model (for $p \\in \\{1,2,\\infty\\}$) and achieve multiple\nnorm adversarial robustness. In this way we boost the previous state-of-the-art\nreported for multiple-norm robustness by more than $6\\%$ on CIFAR-10 and report\nup to our knowledge the first ImageNet models with multiple norm robustness.\nMoreover, we study the general transfer of adversarial robustness between\ndifferent threat models and in this way boost the previous SOTA\n$l_1$-robustness on CIFAR-10 by almost $10\\%$.",
          "link": "http://arxiv.org/abs/2105.12508",
          "publishedOn": "2021-05-27T01:32:29.194Z",
          "wordCount": 625,
          "title": "Adversarial robustness against multiple $l_p$-threat models at the price of one and how to quickly fine-tune robust models to another threat model. (arXiv:2105.12508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_M/0/1/0/all/0/1\">Mathew Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_T/0/1/0/all/0/1\">Tomer Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataer_Cansizoglu_E/0/1/0/all/0/1\">Esra Ataer-Cansizoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jae-Woo Choi</a>",
          "description": "Matching and recommending products is beneficial for both customers and\ncompanies. With the rapid increase in home goods e-commerce, there is an\nincreasing demand for quantitative methods for providing such recommendations\nfor millions of products. This approach is facilitated largely by online stores\nsuch as Amazon and Wayfair, in which the goal is to maximize overall sales.\nInstead of focusing on overall sales, we take a product design perspective, by\nemploying big-data analysis for determining the design qualities of a highly\nrecommended product. Specifically, we focus on the visual style compatibility\nof such products. We build off previous work which implemented a style-based\nsimilarity metric for thousands of furniture products. Using analysis and\nvisualization, we extract attributes of furniture products that are highly\ncompatible style-wise. We propose a designer in-the-loop workflow that mirrors\nmethods of displaying similar products to consumers browsing e-commerce\nwebsites. Our findings are useful when designing new products, since they\nprovide insight regarding what furniture will be strongly compatible across\nmultiple styles, and hence, more likely to be recommended.",
          "link": "http://arxiv.org/abs/2105.12256",
          "publishedOn": "2021-05-27T01:32:29.172Z",
          "wordCount": 644,
          "title": "Style Similarity as Feedback for Product Design. (arXiv:2105.12256v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Srishti Yadav</a>",
          "description": "Unlike deep learning which requires large training datasets, correlation\nfilter-based trackers like Kernelized Correlation Filter (KCF) uses implicit\nproperties of tracked images (circulant matrices) for training in real-time.\nDespite their practical application in tracking, a need for a better\nunderstanding of the fundamentals associated with KCF in terms of\ntheoretically, mathematically, and experimentally exists. This thesis first\ndetails the workings prototype of the tracker and investigates its\neffectiveness in real-time applications and supporting visualizations. We\nfurther address some of the drawbacks of the tracker in cases of occlusions,\nscale changes, object rotation, out-of-view and model drift with our novel\nRGB-D Kernel Correlation tracker. We also study the use of particle filters to\nimprove trackers' accuracy. Our results are experimentally evaluated using a)\nstandard dataset and b) real-time using the Microsoft Kinect V2 sensor. We\nbelieve this work will set the basis for a better understanding of the\neffectiveness of kernel-based correlation filter trackers and to further define\nsome of its possible advantages in tracking.",
          "link": "http://arxiv.org/abs/2105.12161",
          "publishedOn": "2021-05-27T01:32:29.151Z",
          "wordCount": 598,
          "title": "Occlusion Aware Kernel Correlation Filter Tracker using RGB-D. (arXiv:2105.12161v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Namuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songkuk Kim</a>",
          "description": "Bayesian neural networks (BNNs) have shown success in the areas of\nuncertainty estimation and robustness. However, a crucial challenge prohibits\ntheir use in practice: Bayesian NNs require a large number of predictions to\nproduce reliable results, leading to a significant increase in computational\ncost. To alleviate this issue, we propose spatial smoothing, a method that\nensembles neighboring feature map points of CNNs. By simply adding a few blur\nlayers to the models, we empirically show that the spatial smoothing improves\naccuracy, uncertainty estimation, and robustness of BNNs across a whole range\nof ensemble sizes. In particular, BNNs incorporating the spatial smoothing\nachieve high predictive performance merely with a handful of ensembles.\nMoreover, this method also can be applied to canonical deterministic neural\nnetworks to improve the performances. A number of evidences suggest that the\nimprovements can be attributed to the smoothing and flattening of the loss\nlandscape. In addition, we provide a fundamental explanation for prior works -\nnamely, global average pooling, pre-activation, and ReLU6 - by addressing to\nthem as special cases of the spatial smoothing. These not only enhance\naccuracy, but also improve uncertainty estimation and robustness by making the\nloss landscape smoother in the same manner as the spatial smoothing. The code\nis available at https://github.com/xxxnell/spatial-smoothing.",
          "link": "http://arxiv.org/abs/2105.12639",
          "publishedOn": "2021-05-27T01:32:29.136Z",
          "wordCount": 654,
          "title": "Blurs Make Results Clearer: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoke_W/0/1/0/all/0/1\">Willis Hoke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shea_D/0/1/0/all/0/1\">Daniel Shea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casey_S/0/1/0/all/0/1\">Stephen Casey</a>",
          "description": "Molecular dynamics simulations produce data with complex nonlinear dynamics.\nIf the timestep behavior of such a dynamic system can be represented by a\nlinear operator, future states can be inferred directly without expensive\nsimulations. The use of an autoencoder in combination with a physical timestep\noperator allows both the relevant structural characteristics of the molecular\ngraphs and the underlying physics of the system to be isolated during the\ntraining process. In this work, we develop a pipeline for establishing\ngraph-structured representations of time-series volumetric data from molecular\ndynamics simulations. We then train an autoencoder to find nonlinear mappings\nto a latent space where future timesteps can be predicted through application\nof a linear operator trained in tandem with the autoencoder. Increasing the\ndimensionality of the autoencoder output is shown to improve the accuracy of\nthe physical timestep operator.",
          "link": "http://arxiv.org/abs/2105.12295",
          "publishedOn": "2021-05-27T01:32:29.131Z",
          "wordCount": 566,
          "title": "Operator Autoencoders: Learning Physical Operations on Encoded Molecular Graphs. (arXiv:2105.12295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Rushabh Patel</a>",
          "description": "Invasive ductal carcinoma is a prevalent, potentially deadly disease\nassociated with a high rate of morbidity and mortality. Its malignancy is the\nsecond leading cause of death from cancer in women. The mammogram is an\nextremely useful resource for mass detection and invasive ductal carcinoma\ndiagnosis. We are proposing a method for Invasive ductal carcinoma that will\nuse convolutional neural networks (CNN) on mammograms to assist radiologists in\ndiagnosing the disease. Due to the varying image clarity and structure of\ncertain mammograms, it is difficult to observe major cancer characteristics\nsuch as microcalcification and mass, and it is often difficult to interpret and\ndiagnose these attributes. The aim of this study is to establish a novel method\nfor fully automated feature extraction and classification in invasive ductal\ncarcinoma computer-aided diagnosis (CAD) systems. This article presents a tumor\nclassification algorithm that makes novel use of convolutional neural networks\non breast mammogram images to increase feature extraction and training speed.\nThe algorithm makes two contributions.",
          "link": "http://arxiv.org/abs/2105.12564",
          "publishedOn": "2021-05-27T01:32:29.127Z",
          "wordCount": 611,
          "title": "Predicting invasive ductal carcinoma using a Reinforcement Sample Learning Strategy using Deep Learning. (arXiv:2105.12564v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.03357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_M/0/1/0/all/0/1\">Michael K. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catt_E/0/1/0/all/0/1\">Elliot Catt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1\">Marcus Hutter</a>",
          "description": "Reinforcement learners are agents that learn to pick actions that lead to\nhigh reward. Ideally, the value of a reinforcement learner's policy approaches\noptimality--where the optimal informed policy is the one which maximizes\nreward. Unfortunately, we show that if an agent is guaranteed to be\n\"asymptotically optimal\" in any (stochastically computable) environment, then\nsubject to an assumption about the true environment, this agent will be either\n\"destroyed\" or \"incapacitated\" with probability 1. Much work in reinforcement\nlearning uses an ergodicity assumption to avoid this problem. Often, doing\ntheoretical research under simplifying assumptions prepares us to provide\npractical solutions even in the absence of those assumptions, but the\nergodicity assumption in reinforcement learning may have led us entirely astray\nin preparing safe and effective exploration strategies for agents in dangerous\nenvironments. Rather than assuming away the problem, we present an agent,\nMentee, with the modest guarantee of approaching the performance of a mentor,\ndoing safe exploration instead of reckless exploration. Critically, Mentee's\nexploration probability depends on the expected information gain from\nexploring. In a simple non-ergodic environment with a weak mentor, we find\nMentee outperforms existing asymptotically optimal agents and its mentor.",
          "link": "http://arxiv.org/abs/2006.03357",
          "publishedOn": "2021-05-27T01:32:29.122Z",
          "wordCount": 676,
          "title": "Curiosity Killed or Incapacitated the Cat and the Asymptotically Optimal Agent. (arXiv:2006.03357v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1\">Ali Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Kim Phuc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehl_L/0/1/0/all/0/1\">Ludovic Koehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shujun Li</a>",
          "description": "Deep learning play a vital role in classifying different arrhythmias using\nthe electrocardiography (ECG) data. Nevertheless, training deep learning models\nnormally requires a large amount of data and it can lead to privacy concerns.\nUnfortunately, a large amount of healthcare data cannot be easily collected\nfrom a single silo. Additionally, deep learning models are like black-box, with\nno explainability of the predicted results, which is often required in clinical\nhealthcare. This limits the application of deep learning in real-world health\nsystems. In this paper, we design a new explainable artificial intelligence\n(XAI) based deep learning framework in a federated setting for ECG-based\nhealthcare applications. The federated setting is used to solve issues such as\ndata availability and privacy concerns. Furthermore, the proposed framework\nsetting effectively classifies arrhythmia's using an autoencoder and a\nclassifier, both based on a convolutional neural network (CNN). Additionally,\nwe propose an XAI-based module on top of the proposed classifier to explain the\nclassification results, which help clinical practitioners make quick and\nreliable decisions. The proposed framework was trained and tested using the\nMIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98%\nfor arrhythmia detection using noisy and clean data, respectively, with\nfive-fold cross-validation.",
          "link": "http://arxiv.org/abs/2105.12497",
          "publishedOn": "2021-05-27T01:32:29.117Z",
          "wordCount": 642,
          "title": "Designing ECG Monitoring Healthcare System with Federated Transfer Learning and Explainable AI. (arXiv:2105.12497v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Serban_A/0/1/0/all/0/1\">Alex Serban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poll_E/0/1/0/all/0/1\">Erik Poll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visser_J/0/1/0/all/0/1\">Joost Visser</a>",
          "description": "While many defences against adversarial examples have been proposed, finding\nrobust machine learning models is still an open problem. The most compelling\ndefence to date is adversarial training and consists of complementing the\ntraining data set with adversarial examples. Yet adversarial training severely\nimpacts training time and depends on finding representative adversarial\nsamples. In this paper we propose to train models on output spaces with large\nclass separation in order to gain robustness without adversarial training. We\nintroduce a method to partition the output space into class prototypes with\nlarge separation and train models to preserve it. Experimental results shows\nthat models trained with these prototypes -- which we call deep repulsive\nprototypes -- gain robustness competitive with adversarial training, while also\npreserving more accuracy on natural samples. Moreover, the models are more\nresilient to large perturbation sizes. For example, we obtained over 50%\nrobustness for CIFAR-10, with 92% accuracy on natural samples and over 20%\nrobustness for CIFAR-100, with 71% accuracy on natural samples without\nadversarial training. For both data sets, the models preserved robustness\nagainst large perturbations better than adversarially trained models.",
          "link": "http://arxiv.org/abs/2105.12427",
          "publishedOn": "2021-05-27T01:32:29.111Z",
          "wordCount": 601,
          "title": "Deep Repulsive Prototypes for Adversarial Robustness. (arXiv:2105.12427v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1\">Sajad Khodadadian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1\">Siva Theja Maguluri</a>",
          "description": "In this paper, we develop a novel variant of off-policy natural actor-critic\nalgorithm with linear function approximation and we establish a sample\ncomplexity of $\\mathcal{O}(\\epsilon^{-3})$, outperforming all the previously\nknown convergence bounds of such algorithms. In order to overcome the\ndivergence due to deadly triad in off-policy policy evaluation under function\napproximation, we develop a critic that employs $n$-step TD-learning algorithm\nwith a properly chosen $n$. We present finite-sample convergence bounds on this\ncritic under both constant and diminishing step sizes, which are of independent\ninterest. Furthermore, we develop a variant of natural policy gradient under\nfunction approximation, with an improved convergence rate of $\\mathcal{O}(1/T)$\nafter $T$ iterations. Combining the finite sample error bounds of actor and the\ncritic, we obtain the $\\mathcal{O}(\\epsilon^{-3})$ sample complexity. We derive\nour sample complexity bounds solely based on the assumption that the behavior\npolicy sufficiently explores all the states and actions, which is a much\nlighter assumption compared to the related literature.",
          "link": "http://arxiv.org/abs/2105.12540",
          "publishedOn": "2021-05-27T01:32:29.096Z",
          "wordCount": 591,
          "title": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic with Linear Function Approximation. (arXiv:2105.12540v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolinski_P/0/1/0/all/0/1\">Pierre Wolinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpiat_G/0/1/0/all/0/1\">Guillaume Charpiat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1\">Yann Ollivier</a>",
          "description": "In machine learning, it is common to optimize the parameters of a\nprobabilistic model, modulated by an ad hoc regularization term that penalizes\nsome values of the parameters. Regularization terms appear naturally in\nVariational Inference (VI), a tractable way to approximate Bayesian posteriors:\nthe loss to optimize contains a Kullback--Leibler divergence term between the\napproximate posterior and a Bayesian prior. We fully characterize which\nregularizers can arise this way, and provide a systematic way to compute the\ncorresponding prior. This viewpoint also provides a prediction for useful\nvalues of the regularization factor in neural networks. We apply this framework\nto regularizers such as L2, L1 or group-Lasso.",
          "link": "http://arxiv.org/abs/2002.00178",
          "publishedOn": "2021-05-27T01:32:29.091Z",
          "wordCount": 587,
          "title": "An Equivalence between Bayesian Priors and Penalties in Variational Inference. (arXiv:2002.00178v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhaoxia/0/1/0/all/0/1\">Zhaoxia</a> (Summer) <a href=\"http://arxiv.org/find/cs/1/au:+Deng/0/1/0/all/0/1\">Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1\">Ping Tak Peter Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haixin Liu</a>, Jie (Amy) <a href=\"http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1\">Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuen_H/0/1/0/all/0/1\">Hector Yuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khudia_D/0/1/0/all/0/1\">Daya Khudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaohan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1\">Ellie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1\">Dhruv Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1\">Raghuraman Krishnamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadathur_S/0/1/0/all/0/1\">Satish Nadathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changkyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumov_M/0/1/0/all/0/1\">Maxim Naumov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naghshineh_S/0/1/0/all/0/1\">Sam Naghshineh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smelyanskiy_M/0/1/0/all/0/1\">Mikhail Smelyanskiy</a>",
          "description": "Tremendous success of machine learning (ML) and the unabated growth in ML\nmodel complexity motivated many ML-specific designs in both CPU and accelerator\narchitectures to speed up the model inference. While these architectures are\ndiverse, highly optimized low-precision arithmetic is a component shared by\nmost. Impressive compute throughputs are indeed often exhibited by these\narchitectures on benchmark ML models. Nevertheless, production models such as\nrecommendation systems important to Facebook's personalization services are\ndemanding and complex: These systems must serve billions of users per month\nresponsively with low latency while maintaining high prediction accuracy,\nnotwithstanding computations with many tens of billions parameters per\ninference. Do these low-precision architectures work well with our production\nrecommendation systems? They do. But not without significant effort. We share\nin this paper our search strategies to adapt reference recommendation models to\nlow-precision hardware, our optimization of low-precision compute kernels, and\nthe design and development of tool chain so as to maintain our models' accuracy\nthroughout their lifespan during which topic trends and users' interests\ninevitably evolve. Practicing these low-precision technologies helped us save\ndatacenter capacities while deploying models with up to 5X complexity that\nwould otherwise not be deployed on traditional general-purpose CPUs. We believe\nthese lessons from the trenches promote better co-design between hardware\narchitecture and software engineering and advance the state of the art of ML in\nindustry.",
          "link": "http://arxiv.org/abs/2105.12676",
          "publishedOn": "2021-05-27T01:32:29.085Z",
          "wordCount": 697,
          "title": "Low-Precision Hardware Architectures Meet Recommendation Model Inference at Scale. (arXiv:2105.12676v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1912.02580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1\">Francesco Farina</a>",
          "description": "In this paper, we introduce the concept of collective learning (CL) which\nexploits the notion of collective intelligence in the field of distributed\nsemi-supervised learning. The proposed framework draws inspiration from the\nlearning behavior of human beings, who alternate phases involving\ncollaboration, confrontation and exchange of views with other consisting of\nstudying and learning on their own. On this regard, CL comprises two main\nphases: a self-training phase in which learning is performed on local private\n(labeled) data only and a collective training phase in which proxy-labels are\nassigned to shared (unlabeled) data by means of a consensus-based algorithm. In\nthe considered framework, heterogeneous systems can be connected over the same\nnetwork, each with different computational capabilities and resources and\neveryone in the network may take advantage of the cooperation and will\neventually reach higher performance with respect to those it can reach on its\nown. An extensive experimental campaign on an image classification problem\nemphasizes the properties of CL by analyzing the performance achieved by the\ncooperating agents.",
          "link": "http://arxiv.org/abs/1912.02580",
          "publishedOn": "2021-05-27T01:32:29.078Z",
          "wordCount": 619,
          "title": "Collective Learning. (arXiv:1912.02580v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "In recent years there has been a lot of focus on adversarial attacks,\nespecially on deep neural networks. Here, we argue that they are more general\nin nature and can easily affect a larger class of models, e.g., any\ndifferentiable perturbed optimizers. We further show that such attacks can be\ndetermined by the hidden confounders in a domain, thus drawing a novel\nconnection between such attacks and causality. Establishing this causal\nperspective is characterized by the influence of the structural causal model's\ndata generating process on the subsequent optimization thereby exhibiting\nintriguing parameters of the former. We reveal the existence of such parameters\nfor three combinatorial optimization problems, namely linear assignment,\nshortest path and a real world problem of energy systems. Our empirical\nexamination also unveils worrisome consequences of these attacks on\ndifferentiable perturbed optimizers thereby highlighting the criticality of our\nfindings.",
          "link": "http://arxiv.org/abs/2105.12697",
          "publishedOn": "2021-05-27T01:32:29.044Z",
          "wordCount": 584,
          "title": "Intriguing Parameters of Structural Causal Models. (arXiv:2105.12697v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09165",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Moosmuller_C/0/1/0/all/0/1\">Caroline Moosm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cloninger_A/0/1/0/all/0/1\">Alexander Cloninger</a>",
          "description": "Discriminating between distributions is an important problem in a number of\nscientific fields. This motivated the introduction of Linear Optimal\nTransportation (LOT), which embeds the space of distributions into an\n$L^2$-space. The transform is defined by computing the optimal transport of\neach distribution to a fixed reference distribution, and has a number of\nbenefits when it comes to speed of computation and to determining\nclassification boundaries. In this paper, we characterize a number of settings\nin which LOT embeds families of distributions into a space in which they are\nlinearly separable. This is true in arbitrary dimension, and for families of\ndistributions generated through perturbations of shifts and scalings of a fixed\ndistribution.We also prove conditions under which the $L^2$ distance of the LOT\nembedding between two distributions in arbitrary dimension is nearly isometric\nto Wasserstein-2 distance between those distributions. This is of significant\ncomputational benefit, as one must only compute $N$ optimal transport maps to\ndefine the $N^2$ pairwise distances between $N$ distributions. We demonstrate\nthe benefits of LOT on a number of distribution classification problems.",
          "link": "http://arxiv.org/abs/2008.09165",
          "publishedOn": "2021-05-27T01:32:29.037Z",
          "wordCount": 653,
          "title": "Linear Optimal Transport Embedding: Provable Wasserstein classification for certain rigid transformations and perturbations. (arXiv:2008.09165v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.10309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fyffe_G/0/1/0/all/0/1\">Graham Fyffe</a>",
          "description": "We prove that the evidence lower bound (ELBO) employed by variational\nauto-encoders (VAEs) admits non-trivial solutions having constant posterior\nvariances under certain mild conditions, removing the need to learn variances\nin the encoder. The proof follows from an unexpected journey through an array\nof topics: the closed form optimal decoder for Gaussian VAEs, a proof that the\ndecoder is always smooth, a proof that the ELBO at its stationary points is\nequal to the exact log evidence, and the posterior variance is merely part of a\nstochastic estimator of the decoder Hessian. The penalty incurred from using a\nconstant posterior variance is small under mild conditions, and otherwise\ndiscourages large variations in the decoder Hessian. From here we derive a\nsimplified formulation of the ELBO as an expectation over a batch, which we\ncall the Batch Information Lower Bound (BILBO). Despite the use of Gaussians,\nour analysis is broadly applicable -- it extends to any likelihood function\nthat induces a Riemannian metric. Regarding learned likelihoods, we show that\nthe ELBO is optimal in the limit as the likelihood variances approach zero,\nwhere it is equivalent to the change of variables formulation employed in\nnormalizing flow networks. Standard optimization procedures are unstable in\nthis limit, so we propose a bounded Gaussian likelihood that is invariant to\nthe scale of the data using a measure of the aggregate information in a batch,\nwhich we call Bounded Aggregate Information Sampling (BAGGINS). Combining the\ntwo formulations, we construct VAE networks with only half the outputs of\nordinary VAEs (no learned variances), yielding improved ELBO scores and scale\ninvariance in experiments. As we perform our analyses irrespective of any\nparticular network architecture, our reformulations may apply to any VAE\nimplementation.",
          "link": "http://arxiv.org/abs/1912.10309",
          "publishedOn": "2021-05-27T01:32:29.031Z",
          "wordCount": 747,
          "title": "There and Back Again: Unraveling the Variational Auto-Encoder. (arXiv:1912.10309v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuoran Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chen Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lei Lyu</a>",
          "description": "Source code can be parsed into the abstract syntax tree (AST) based on\ndefined syntax rules. However, in pre-training, little work has considered the\nincorporation of tree structure into the learning process. In this paper, we\npresent TreeBERT, a tree-based pre-trained model for improving programming\nlanguage-oriented generation tasks. To utilize tree structure, TreeBERT\nrepresents the AST corresponding to the code as a set of composition paths and\nintroduces node position embedding. The model is trained by tree masked\nlanguage modeling (TMLM) and node order prediction (NOP) with a hybrid\nobjective. TMLM uses a novel masking strategy designed according to the tree's\ncharacteristics to help the model understand the AST and infer the missing\nsemantics of the AST. With NOP, TreeBERT extracts the syntactical structure by\nlearning the order constraints of nodes in AST. We pre-trained TreeBERT on\ndatasets covering multiple programming languages. On code summarization and\ncode documentation tasks, TreeBERT outperforms other pre-trained models and\nstate-of-the-art models designed for these tasks. Furthermore, TreeBERT\nperforms well when transferred to the pre-trained unseen programming language.",
          "link": "http://arxiv.org/abs/2105.12485",
          "publishedOn": "2021-05-27T01:32:29.025Z",
          "wordCount": 607,
          "title": "TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. (arXiv:2105.12485v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.07105",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shin_Y/0/1/0/all/0/1\">Yuyol Shin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yoon_Y/0/1/0/all/0/1\">Yoonjin Yoon</a>",
          "description": "Traffic forecasting problem remains a challenging task in the intelligent\ntransportation system due to its spatio-temporal complexity. Although temporal\ndependency has been well studied and discussed, spatial dependency is\nrelatively less explored due to its large variations, especially in the urban\nenvironment. In this study, a novel graph convolutional network model,\nMulti-Weight Traffic Graph Convolutional (MW-TGC) network, is proposed and\napplied to two urban networks with contrasting geometric constraints. The model\nconducts graph convolution operations on speed data with multi-weighted\nadjacency matrices to combine the features, including speed limit, distance,\nand angle. The spatially isolated dimension reduction operation is conducted on\nthe combined features to learn the dependencies among the features and reduce\nthe size of the output to a computationally feasible level. The output of\nmulti-weight graph convolution is applied to the sequence-to-sequence model\nwith Long Short-Term Memory units to learn temporal dependencies. When applied\nto two urban sites, urban-core and urban-mix, MW-TGC network not only\noutperformed the comparative models in both sites but also reduced variance in\nthe heterogeneous urban-mix network. We conclude that MW-TGC network can\nprovide a robust traffic forecasting performance across the variations in\nspatial complexity, which can be a strong advantage in urban traffic\nforecasting.",
          "link": "http://arxiv.org/abs/1909.07105",
          "publishedOn": "2021-05-27T01:32:29.007Z",
          "wordCount": 692,
          "title": "Incorporating dynamicity of transportation network with multi-weight traffic graph convolutional network for traffic forecasting. (arXiv:1909.07105v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuhang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lianfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sara Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1\">Hod Lipson</a>",
          "description": "Ability to generate intelligent and generalizable facial expressions is\nessential for building human-like social robots. At present, progress in this\nfield is hindered by the fact that each facial expression needs to be\nprogrammed by humans. In order to adapt robot behavior in real time to\ndifferent situations that arise when interacting with human subjects, robots\nneed to be able to train themselves without requiring human labels, as well as\nmake fast action decisions and generalize the acquired knowledge to diverse and\nnew contexts. We addressed this challenge by designing a physical animatronic\nrobotic face with soft skin and by developing a vision-based self-supervised\nlearning framework for facial mimicry. Our algorithm does not require any\nknowledge of the robot's kinematic model, camera calibration or predefined\nexpression set. By decomposing the learning process into a generative model and\nan inverse model, our framework can be trained using a single motor babbling\ndataset. Comprehensive evaluations show that our method enables accurate and\ndiverse face mimicry across diverse human subjects. The project website is at\nthis http URL",
          "link": "http://arxiv.org/abs/2105.12724",
          "publishedOn": "2021-05-27T01:32:28.820Z",
          "wordCount": 632,
          "title": "Smile Like You Mean It: Driving Animatronic Robotic Face with Learned Models. (arXiv:2105.12724v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conserva_M/0/1/0/all/0/1\">Michelangelo Conserva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deisenroth_M/0/1/0/all/0/1\">Marc Peter Deisenroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">K S Sesh Kumar</a>",
          "description": "Many algorithms for ranked data become computationally intractable as the\nnumber of objects grows due to complex geometric structure induced by rankings.\nAn additional challenge is posed by partial rankings, i.e. rankings in which\nthe preference is only known for a subset of all objects. For these reasons,\nstate-of-the-art methods cannot scale to real-world applications, such as\nrecommender systems. We address this challenge by exploiting geometric\nstructure of ranked data and additional available information about the objects\nto derive a submodular kernel for ranking. The submodular kernel combines the\nefficiency of submodular optimization with the theoretical properties of\nkernel-based methods. We demonstrate that the submodular kernel drastically\nreduces the computational cost compared to state-of-the-art kernels and scales\nwell to large datasets while attaining good empirical performance.",
          "link": "http://arxiv.org/abs/2105.12356",
          "publishedOn": "2021-05-27T01:32:28.814Z",
          "wordCount": 549,
          "title": "Submodular Kernels for Efficient Rankings. (arXiv:2105.12356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Carabarin_L/0/1/0/all/0/1\">Lizeth Gonzalez-Carabarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huijben_I/0/1/0/all/0/1\">Iris A.M. Huijben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1\">Bastiaan S. Veeling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_A/0/1/0/all/0/1\">Alexandre Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1\">Ruud J.G. van Sloun</a>",
          "description": "Unstructured neural network pruning algorithms have achieved impressive\ncompression rates. However, the resulting - typically irregular - sparse\nmatrices hamper efficient hardware implementations, leading to additional\nmemory usage and complex control logic that diminishes the benefits of\nunstructured pruning. This has spurred structured coarse-grained pruning\nsolutions that prune entire filters or even layers, enabling efficient\nimplementation at the expense of reduced flexibility. Here we propose a\nflexible new pruning mechanism that facilitates pruning at different\ngranularities (weights, kernels, filters/feature maps), while retaining\nefficient memory organization (e.g. pruning exactly k-out-of-n weights for\nevery output neuron, or pruning exactly k-out-of-n kernels for every feature\nmap). We refer to this algorithm as Dynamic Probabilistic Pruning (DPP). DPP\nleverages the Gumbel-softmax relaxation for differentiable k-out-of-n sampling,\nfacilitating end-to-end optimization. We show that DPP achieves competitive\ncompression rates and classification accuracy when pruning common deep learning\nmodels trained on different benchmark datasets for image classification.\nRelevantly, the non-magnitude-based nature of DPP allows for joint optimization\nof pruning and weight quantization in order to even further compress the\nnetwork, which we show as well. Finally, we propose novel information theoretic\nmetrics that show the confidence and pruning diversity of pruning masks within\na layer.",
          "link": "http://arxiv.org/abs/2105.12686",
          "publishedOn": "2021-05-27T01:32:28.803Z",
          "wordCount": 644,
          "title": "Dynamic Probabilistic Pruning: A general framework for hardware-constrained pruning at different granularities. (arXiv:2105.12686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_B/0/1/0/all/0/1\">Benjamin Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildreth_D/0/1/0/all/0/1\">Dalton Hildreth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Duowen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baran_I/0/1/0/all/0/1\">Ilya Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vova Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Adriana Schulz</a>",
          "description": "Assembly modeling is a core task of computer aided design (CAD), comprising\naround one third of the work in a CAD workflow. Optimizing this process\ntherefore represents a huge opportunity in the design of a CAD system, but\ncurrent research of assembly based modeling is not directly applicable to\nmodern CAD systems because it eschews the dominant data structure of modern\nCAD: parametric boundary representations (BREPs). CAD assembly modeling defines\nassemblies as a system of pairwise constraints, called mates, between parts,\nwhich are defined relative to BREP topology rather than in world coordinates\ncommon to existing work. We propose SB-GCN, a representation learning scheme on\nBREPs that retains the topological structure of parts, and use these learned\nrepresentations to predict CAD type mates. To train our system, we compiled the\nfirst large scale dataset of BREP CAD assemblies, which we are releasing along\nwith benchmark mate prediction tasks. Finally, we demonstrate the compatibility\nof our model with an existing commercial CAD system by building a tool that\nassists users in mate creation by suggesting mate completions, with 72.2%\naccuracy.",
          "link": "http://arxiv.org/abs/2105.12238",
          "publishedOn": "2021-05-27T01:32:28.786Z",
          "wordCount": 638,
          "title": "SB-GCN: Structured BREP Graph Convolutional Network for Automatic Mating of CAD Assemblies. (arXiv:2105.12238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poklukar_P/0/1/0/all/0/1\">Petra Poklukar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varava_A/0/1/0/all/0/1\">Anastasia Varava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1\">Danica Kragic</a>",
          "description": "Evaluating the quality of learned representations without relying on a\ndownstream task remains one of the challenges in representation learning. In\nthis work, we present Geometric Component Analysis (GeomCA) algorithm that\nevaluates representation spaces based on their geometric and topological\nproperties. GeomCA can be applied to representations of any dimension,\nindependently of the model that generated them. We demonstrate its\napplicability by analyzing representations obtained from a variety of\nscenarios, such as contrastive learning models, generative models and\nsupervised learning models.",
          "link": "http://arxiv.org/abs/2105.12486",
          "publishedOn": "2021-05-27T01:32:28.774Z",
          "wordCount": 504,
          "title": "GeomCA: Geometric Evaluation of Data Representations. (arXiv:2105.12486v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Randel_R/0/1/0/all/0/1\">Rodrigo Randel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloise_D/0/1/0/all/0/1\">Daniel Aloise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1\">Alain Hertz</a>",
          "description": "Distance metric learning algorithms aim to appropriately measure similarities\nand distances between data points. In the context of clustering, metric\nlearning is typically applied with the assist of side-information provided by\nexperts, most commonly expressed in the form of cannot-link and must-link\nconstraints. In this setting, distance metric learning algorithms move closer\npairs of data points involved in must-link constraints, while pairs of points\ninvolved in cannot-link constraints are moved away from each other. For these\nalgorithms to be effective, it is important to use a distance metric that\nmatches the expert knowledge, beliefs, and expectations, and the\ntransformations made to stick to the side-information should preserve\ngeometrical properties of the dataset. Also, it is interesting to filter the\nconstraints provided by the experts to keep only the most useful and reject\nthose that can harm the clustering process. To address these issues, we propose\nto exploit the dual information associated with the pairwise constraints of the\nsemi-supervised clustering problem. Experiments clearly show that distance\nmetric learning algorithms benefit from integrating this dual information.",
          "link": "http://arxiv.org/abs/2105.12703",
          "publishedOn": "2021-05-27T01:32:28.769Z",
          "wordCount": 603,
          "title": "Exploring dual information in distance metric learning for clustering. (arXiv:2105.12703v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We propose Predict then Interpolate (PI), a simple algorithm for learning\ncorrelations that are stable across environments. The algorithm follows from\nthe intuition that when using a classifier trained on one environment to make\npredictions on examples from another environment, its mistakes are informative\nas to which correlations are unstable. In this work, we prove that by\ninterpolating the distributions of the correct predictions and the wrong\npredictions, we can uncover an oracle distribution where the unstable\ncorrelation vanishes. Since the oracle interpolation coefficients are not\naccessible, we use group distributionally robust optimization to minimize the\nworst-case risk across all such interpolations. We evaluate our method on both\ntext classification and image classification. Empirical results demonstrate\nthat our algorithm is able to learn robust classifiers (outperforms IRM by\n23.85% on synthetic environments and 12.41% on natural environments). Our code\nand data are available at https://github.com/YujiaBao/Predict-then-Interpolate.",
          "link": "http://arxiv.org/abs/2105.12628",
          "publishedOn": "2021-05-27T01:32:28.763Z",
          "wordCount": 591,
          "title": "Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers. (arXiv:2105.12628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">An Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wu Luo</a>",
          "description": "We propose a successive convex approximation based off-policy optimization\n(SCAOPO) algorithm to solve the general constrained reinforcement learning\nproblem, which is formulated as a constrained Markov decision process (CMDP) in\nthe context of average cost. The SCAOPO is based on solving a sequence of\nconvex objective/feasibility optimization problems obtained by replacing the\nobjective and constraint functions in the original problems with convex\nsurrogate functions. At each iteration, the convex surrogate problem can be\nefficiently solved by Lagrange dual method even the policy is parameterized by\na high-dimensional function. Moreover, the SCAOPO enables to reuse old\nexperiences from previous updates, thereby significantly reducing the\nimplementation cost when deployed in the real-world engineering systems that\nneed to online learn the environment. In spite of the time-varying state\ndistribution and the stochastic bias incurred by the off-policy learning, the\nSCAOPO with a feasible initial point can still provably converge to a\nKarush-Kuhn-Tucker (KKT) point of the original problem almost surely.",
          "link": "http://arxiv.org/abs/2105.12545",
          "publishedOn": "2021-05-27T01:32:28.747Z",
          "wordCount": 591,
          "title": "Successive Convex Approximation Based Off-Policy Optimization for Constrained Reinforcement Learning. (arXiv:2105.12545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simsek_B/0/1/0/all/0/1\">Berfin &#x15e;im&#x15f;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ged_F/0/1/0/all/0/1\">Fran&#xe7;ois Ged</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacot_A/0/1/0/all/0/1\">Arthur Jacot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spadaro_F/0/1/0/all/0/1\">Francesco Spadaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongler_C/0/1/0/all/0/1\">Cl&#xe9;ment Hongler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1\">Wulfram Gerstner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brea_J/0/1/0/all/0/1\">Johanni Brea</a>",
          "description": "We study how permutation symmetries in overparameterized multi-layer neural\nnetworks generate `symmetry-induced' critical points. Assuming a network with $\nL $ layers of minimal widths $ r_1^*, \\ldots, r_{L-1}^* $ reaches a zero-loss\nminimum at $ r_1^*! \\cdots r_{L-1}^*! $ isolated points that are permutations\nof one another, we show that adding one extra neuron to each layer is\nsufficient to connect all these previously discrete minima into a single\nmanifold. For a two-layer overparameterized network of width $ r^*+ h =: m $ we\nexplicitly describe the manifold of global minima: it consists of $ T(r^*, m) $\naffine subspaces of dimension at least $ h $ that are connected to one another.\nFor a network of width $m$, we identify the number $G(r,m)$ of affine subspaces\ncontaining only symmetry-induced critical points that are related to the\ncritical points of a smaller network of width $r<r^*$. Via a combinatorial\nanalysis, we derive closed-form formulas for $ T $ and $ G $ and show that the\nnumber of symmetry-induced critical subspaces dominates the number of affine\nsubspaces forming the global minima manifold in the mildly overparameterized\nregime (small $ h $) and vice versa in the vastly overparameterized regime ($h\n\\gg r^*$). Our results provide new insights into the minimization of the\nnon-convex loss function of overparameterized neural networks.",
          "link": "http://arxiv.org/abs/2105.12221",
          "publishedOn": "2021-05-27T01:32:28.733Z",
          "wordCount": 664,
          "title": "Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances. (arXiv:2105.12221v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1\">Mohamad Wehbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamann_T/0/1/0/all/0/1\">Tim Hamann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_J/0/1/0/all/0/1\">Jens Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaempf_P/0/1/0/all/0/1\">Peter Kaempf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1\">Dario Zanca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1\">Bjoern Eskofier</a>",
          "description": "Most online handwriting recognition systems require the use of specific\nwriting surfaces to extract positional data. In this paper we present a online\nhandwriting recognition system for word recognition which is based on inertial\nmeasurement units (IMUs) for digitizing text written on paper. This is obtained\nby means of a sensor-equipped pen that provides acceleration, angular velocity,\nand magnetic forces streamed via Bluetooth. Our model combines convolutional\nand bidirectional LSTM networks, and is trained with the Connectionist Temporal\nClassification loss that allows the interpretation of raw sensor data into\nwords without the need of sequence segmentation. We use a dataset of words\ncollected using multiple sensor-enhanced pens and evaluate our model on\ndistinct test sets of seen and unseen words achieving a character error rate of\n17.97% and 17.08%, respectively, without the use of a dictionary or language\nmodel",
          "link": "http://arxiv.org/abs/2105.12434",
          "publishedOn": "2021-05-27T01:32:28.722Z",
          "wordCount": 576,
          "title": "Towards an IMU-based Pen Online Handwriting Recognizer. (arXiv:2105.12434v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scholkemper_M/0/1/0/all/0/1\">Michael Scholkemper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1\">Michael T. Schaub</a>",
          "description": "This paper re-examines the concept of node equivalences like structural\nequivalence or automorphic equivalence, which have originally emerged in social\nnetwork analysis to characterize the role an actor plays within a social\nsystem, but have since then been of independent interest for graph-based\nlearning tasks. Traditionally, such exact node equivalences have been defined\neither in terms of the one hop neighborhood of a node, or in terms of the\nglobal graph structure. Here we formalize exact node roles with a\nscale-parameter, describing up to what distance the ego network of a node\nshould be considered when assigning node roles - motivated by the idea that\nthere can be local roles of a node that should not be determined by nodes\narbitrarily far away in the network. We present numerical experiments that show\nhow already \"shallow\" roles of depth 3 or 4 carry sufficient information to\nperform node classification tasks with high accuracy. These findings\ncorroborate the success of recent graph-learning approaches that compute\napproximate node roles in terms of embeddings, by nonlinearly aggregating node\nfeatures in an (un)supervised manner over relatively small neighborhood sizes.\nIndeed, based on our ideas we can construct a shallow classifier achieving on\npar results with recent graph neural network architectures.",
          "link": "http://arxiv.org/abs/2105.12598",
          "publishedOn": "2021-05-27T01:32:28.717Z",
          "wordCount": 630,
          "title": "Local, global and scale-dependent node roles. (arXiv:2105.12598v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1\">Ofir Nachum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>",
          "description": "In imitation learning, it is common to learn a behavior policy to match an\nunknown target policy via max-likelihood training on a collected set of target\ndemonstrations. In this work, we consider using offline experience datasets -\npotentially far from the target distribution - to learn low-dimensional state\nrepresentations that provably accelerate the sample-efficiency of downstream\nimitation learning. A central challenge in this setting is that the unknown\ntarget policy itself may not exhibit low-dimensional behavior, and so there is\na potential for the representation learning objective to alias states in which\nthe target policy acts differently. Circumventing this challenge, we derive a\nrepresentation learning objective which provides an upper bound on the\nperformance difference between the target policy and a lowdimensional policy\ntrained with max-likelihood, and this bound is tight regardless of whether the\ntarget policy itself exhibits low-dimensional structure. Moving to the\npracticality of our method, we show that our objective can be implemented as\ncontrastive learning, in which the transition dynamics are approximated by\neither an implicit energy-based model or, in some special cases, an implicit\nlinear model with representations given by random Fourier features. Experiments\non both tabular environments and high-dimensional Atari games provide\nquantitative evidence for the practical benefits of our proposed objective.",
          "link": "http://arxiv.org/abs/2105.12272",
          "publishedOn": "2021-05-27T01:32:28.701Z",
          "wordCount": 634,
          "title": "Provable Representation Learning for Imitation with Contrastive Fourier Features. (arXiv:2105.12272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12478",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+McCormick_T/0/1/0/all/0/1\">Tyler McCormick</a>",
          "description": "Breiman organizes \"Statistical modeling: The two cultures\" around a simple\nvisual. Data, to the far right, are compelled into a \"black box\" with an arrow\nand then catapulted left by a second arrow, having been transformed into an\noutput. Breiman then posits two interpretations of this visual as encapsulating\na distinction between two cultures in statistics. The divide, he argues is\nabout what happens in the \"black box.\" In this comment, I argue for a broader\nperspective on statistics and, in doing so, elevate questions from \"before\" and\n\"after\" the box as fruitful areas for statistical innovation and practice.",
          "link": "http://arxiv.org/abs/2105.12478",
          "publishedOn": "2021-05-27T01:32:28.696Z",
          "wordCount": 519,
          "title": "The \"given data\" paradigm undermines both cultures. (arXiv:2105.12478v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shih_P/0/1/0/all/0/1\">Po-Kan Shih</a>",
          "description": "With the formation of next generation wireless communication, a growing\nnumber of new applications like internet of things, autonomous car, and drone\nis crowding the unlicensed spectrum. Licensed network such as LTE also comes to\nthe unlicensed spectrum for better providing high-capacity contents with low\ncost. However, LTE was not designed for sharing spectrum with others. A\ncooperation center for these networks is costly because they possess\nheterogeneous properties and everyone can enter and leave the spectrum\nunrestrictedly, so the design will be challenging. Since it is infeasible to\nincorporate potentially infinite scenarios with one unified design, an\nalternative solution is to let each network learn its own coexistence policy.\nPrevious solutions only work on fixed scenarios. In this work a reinforcement\nlearning algorithm is presented to cope with the coexistence between Wi-Fi and\nLTE-LAA agents in 5 GHz unlicensed spectrum. The coexistence problem was\nmodeled as a Dec-POMDP and Bayesian approach was adopted for policy learning\nwith nonparametric prior to accommodate the uncertainty of policy for different\nagents. A fairness measure was introduced in the reward function to encourage\nfair sharing between agents. The reinforcement learning was turned into an\noptimization problem by transforming the value function as likelihood and\nvariational inference for posterior approximation. Simulation results\ndemonstrate that this algorithm can reach high value with compact policy\nrepresentations, and stay computationally efficient when applying to agent set.",
          "link": "http://arxiv.org/abs/2105.12249",
          "publishedOn": "2021-05-27T01:32:28.691Z",
          "wordCount": 654,
          "title": "Bayesian Nonparametric Reinforcement Learning in LTE and Wi-Fi Coexistence. (arXiv:2105.12249v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1\">Pak-Hei Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1\">Ana I.L. Namburete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>",
          "description": "The objective of this work is to segment any arbitrary structures of interest\n(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D\nsegmentation). We show that high accuracy can be achieved by simply propagating\nthe 2D slice segmentation with an affinity matrix between consecutive slices,\nwhich can be learnt in a self-supervised manner, namely slice reconstruction.\nSpecifically, we compare the proposed framework, termed as Sli2Vol, with\nsupervised approaches and two other unsupervised/ self-supervised slice\nregistration approaches, on 8 public datasets (both CT and MRI scans), spanning\n9 different SOIs. Without any parameter-tuning, the same model achieves\nsuperior performance with Dice scores (0-100 scale) of over 80 for most of the\nbenchmarks, including the ones that are unseen during training. Our results\nshow generalizability of the proposed approach across data from different\nmachines and with different SOIs: a major use case of semi-automatic\nsegmentation methods where fully supervised approaches would normally struggle.\nThe source code will be made publicly available at\nhttps://github.com/pakheiyeung/Sli2Vol.",
          "link": "http://arxiv.org/abs/2105.12722",
          "publishedOn": "2021-05-27T01:32:28.685Z",
          "wordCount": 613,
          "title": "Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Michael Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_P/0/1/0/all/0/1\">Peter Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_I/0/1/0/all/0/1\">Ingemar J. Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampos_V/0/1/0/all/0/1\">Vasileios Lampos</a>",
          "description": "Influenza is an infectious disease with the potential to become a pandemic,\nand hence, forecasting its prevalence is an important undertaking for planning\nan effective response. Research has found that web search activity can be used\nto improve influenza models. Neural networks (NN) can provide state-of-the-art\nforecasting accuracy but do not commonly incorporate uncertainty in their\nestimates, something essential for using them effectively during decision\nmaking. In this paper, we demonstrate how Bayesian Neural Networks (BNNs) can\nbe used to both provide a forecast and a corresponding uncertainty without\nsignificant loss in forecasting accuracy compared to traditional NNs. Our\nmethod accounts for two sources of uncertainty: data and model uncertainty,\narising due to measurement noise and model specification, respectively.\nExperiments are conducted using 14 years of data for England, assessing the\nmodel's accuracy over the last 4 flu seasons in this dataset. We evaluate the\nperformance of different models including competitive baselines with\nconventional metrics as well as error functions that incorporate uncertainty\nestimates. Our empirical analysis indicates that considering both sources of\nuncertainty simultaneously is superior to considering either one separately. We\nalso show that a BNN with recurrent layers that models both sources of\nuncertainty yields superior accuracy for these metrics for forecasting horizons\ngreater than 7 days.",
          "link": "http://arxiv.org/abs/2105.12433",
          "publishedOn": "2021-05-27T01:32:28.680Z",
          "wordCount": 646,
          "title": "Estimating the Uncertainty of Neural Network Forecasts for Influenza Prevalence Using Web Search Activity. (arXiv:2105.12433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christian Berger</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dozza_M/0/1/0/all/0/1\">Marco Dozza</a> (2) ((1) Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden, (2) Department of Maritime Sciences and Mechanics, Chalmers University of Technology, Gothenburg, Sweden)",
          "description": "Pedestrian trajectory prediction in urban scenarios is essential for\nautomated driving. This task is challenging because the behavior of pedestrians\nis influenced by both their own history paths and the interactions with others.\nPrevious research modeled these interactions with pooling mechanisms or\naggregating with hand-crafted attention weights. In this paper, we present the\nSocial Interaction-Weighted Spatio-Temporal Convolutional Neural Network\n(Social-IWSTCNN), which includes both the spatial and the temporal features. We\npropose a novel design, namely the Social Interaction Extractor, to learn the\nspatial and social interaction features of pedestrians. Most previous works\nused ETH and UCY datasets which include five scenes but do not cover urban\ntraffic scenarios extensively for training and evaluation. In this paper, we\nuse the recently released large-scale Waymo Open Dataset in urban traffic\nscenarios, which includes 374 urban training scenes and 76 urban testing scenes\nto analyze the performance of our proposed algorithm in comparison to the\nstate-of-the-art (SOTA) models. The results show that our algorithm outperforms\nSOTA algorithms such as Social-LSTM, Social-GAN, and Social-STGCNN on both\nAverage Displacement Error (ADE) and Final Displacement Error (FDE).\nFurthermore, our Social-IWSTCNN is 54.8 times faster in data pre-processing\nspeed, and 4.7 times faster in total test speed than the current best SOTA\nalgorithm Social-STGCNN.",
          "link": "http://arxiv.org/abs/2105.12436",
          "publishedOn": "2021-05-27T01:32:28.665Z",
          "wordCount": 699,
          "title": "Social-IWSTCNN: A Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network for Pedestrian Trajectory Prediction in Urban Traffic Scenarios. (arXiv:2105.12436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerer_M/0/1/0/all/0/1\">Matthias K&#xfc;mmerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ori Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>",
          "description": "Since 2014 transfer learning has become the key driver for the improvement of\nspatial saliency prediction; however, with stagnant progress in the last 3-5\nyears. We conduct a large-scale transfer learning study which tests different\nImageNet backbones, always using the same read out architecture and learning\nprotocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze\nII with ResNet50 features we improve the performance on saliency prediction\nfrom 78% to 85%. However, as we continue to test better ImageNet models as\nbackbones (such as EfficientNetB5) we observe no additional improvement on\nsaliency prediction. By analyzing the backbones further, we find that\ngeneralization to other datasets differs substantially, with models being\nconsistently overconfident in their fixation predictions. We show that by\ncombining multiple backbones in a principled manner a good confidence\ncalibration on unseen datasets can be achieved. This yields a significant leap\nin benchmark performance in and out-of-domain with a 15 percent point\nimprovement over DeepGaze II to 93% on MIT1003, marking a new state of the art\non the MIT/Tuebingen Saliency Benchmark in all available metrics (AUC: 88.3%,\nsAUC: 79.4%, CC: 82.4%).",
          "link": "http://arxiv.org/abs/2105.12441",
          "publishedOn": "2021-05-27T01:32:28.659Z",
          "wordCount": 621,
          "title": "Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling. (arXiv:2105.12441v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>",
          "description": "In this paper, we present empirical analysis on basic and depression specific\nmulti-emotion mining in Tweets with the help of state of the art multi-label\nclassifiers. We choose our basic emotions from a hybrid emotion model\nconsisting of the common emotions from four highly regarded psychological\nmodels of emotions. Moreover, we augment that emotion model with new emotion\ncategories because of their importance in the analysis of depression. Most of\nthose additional emotions have not been used in previous emotion mining\nresearch. Our experimental analyses show that a cost sensitive RankSVM\nalgorithm and a Deep Learning model are both robust, measured by both Macro\nF-measures and Micro F-measures. This suggests that these algorithms are\nsuperior in addressing the widely known data imbalance problem in multi-label\nlearning. Moreover, our application of Deep Learning performs the best, giving\nit an edge in modeling deep semantic features of our extended emotional\ncategories.",
          "link": "http://arxiv.org/abs/2105.12364",
          "publishedOn": "2021-05-27T01:32:28.653Z",
          "wordCount": 587,
          "title": "Basic and Depression Specific Emotion Identification in Tweets: Multi-label Classification Experiments. (arXiv:2105.12364v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasy_B/0/1/0/all/0/1\">Brittany Terese Fasy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenk_C/0/1/0/all/0/1\">Carola Wenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summa_B/0/1/0/all/0/1\">Brian Summa</a>",
          "description": "Persistence diagrams have been widely used to quantify the underlying\nfeatures of filtered topological spaces in data visualization. In many\napplications, computing distances between diagrams is essential; however,\ncomputing these distances has been challenging due to the computational cost.\nIn this paper, we propose a persistence diagram hashing framework that learns a\nbinary code representation of persistence diagrams, which allows for fast\ncomputation of distances. This framework is built upon a generative adversarial\nnetwork (GAN) with a diagram distance loss function to steer the learning\nprocess. Instead of attempting to transform diagrams into vectorized\nrepresentations, we hash diagrams into binary codes, which have natural\nadvantages in large-scale tasks. The training of this model is domain-oblivious\nin that it can be computed purely from synthetic, randomly created diagrams. As\na consequence, our proposed method is directly applicable to various datasets\nwithout the need of retraining the model. These binary codes, when compared\nusing fast Hamming distance, better maintain topological similarity properties\nbetween datasets than other vectorized representations. To evaluate this\nmethod, we apply our framework to the problem of diagram clustering and we\ncompare the quality and performance of our approach to the state-of-the-art. In\naddition, we show the scalability of our approach on a dataset with 10k\npersistence diagrams, which is not possible with current techniques. Moreover,\nour experimental results demonstrate that our method is significantly faster\nwith less memory usage, while retaining comparable or better quality\ncomparisons.",
          "link": "http://arxiv.org/abs/2105.12208",
          "publishedOn": "2021-05-27T01:32:28.647Z",
          "wordCount": 682,
          "title": "A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces. (arXiv:2105.12208v1 [cs.CG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1\">Andrew Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkpen_D/0/1/0/all/0/1\">Diana Inkpen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonie_R/0/1/0/all/0/1\">R&#x103;zvan Andonie</a>",
          "description": "The introduction of Transformer neural networks has changed the landscape of\nNatural Language Processing (NLP) during the last years. So far, none of the\nvisualization systems has yet managed to examine all the facets of the\nTransformers. This gave us the motivation of the current work. We propose a new\nNLP Transformer context-sensitive visualization method that leverages existing\nNLP tools to find the most significant groups of tokens (words) that have the\ngreatest effect on the output, thus preserving some context from the original\ntext. First, we use a sentence-level dependency parser to highlight promising\nword groups. The dependency parser creates a tree of relationships between the\nwords in the sentence. Next, we systematically remove adjacent and non-adjacent\ntuples of \\emph{n} tokens from the input text, producing several new texts with\nthose tokens missing. The resulting texts are then passed to a pre-trained BERT\nmodel. The classification output is compared with that of the full text, and\nthe difference in the activation strength is recorded. The modified texts that\nproduce the largest difference in the target classification output neuron are\nselected, and the combination of removed words are then considered to be the\nmost influential on the model's output. Finally, the most influential word\ncombinations are visualized in a heatmap.",
          "link": "http://arxiv.org/abs/2105.12202",
          "publishedOn": "2021-05-27T01:32:28.641Z",
          "wordCount": 644,
          "title": "Context-Sensitive Visualization of Deep Learning Natural Language Processing Models. (arXiv:2105.12202v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1\">Yijiang Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chaobing Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">YanFeng Zhu</a>",
          "description": "Synonymous keyword retrieval has become an important problem for sponsored\nsearch ever since major search engines relax the exact match product's matching\nrequirement to a synonymous level. Since the synonymous relations between\nqueries and keywords are quite scarce, the traditional information retrieval\nframework is inefficient in this scenario. In this paper, we propose a novel\nquotient space-based retrieval framework to address this problem. Considering\nthe synonymy among keywords as a mathematical equivalence relation, we can\ncompress the synonymous keywords into one representative, and the corresponding\nquotient space would greatly reduce the size of the keyword repository. Then an\nembedding-based retrieval is directly conducted between queries and the keyword\nrepresentatives. To mitigate the semantic gap of the quotient space-based\nretrieval, a single semantic siamese model is utilized to detect both the\nkeyword--keyword and query-keyword synonymous relations. The experiments show\nthat with our quotient space-based retrieval method, the synonymous keyword\nretrieving performance can be greatly improved in terms of memory cost and\nrecall efficiency. This method has been successfully implemented in Baidu's\nonline sponsored search system and has yielded a significant improvement in\nrevenue.",
          "link": "http://arxiv.org/abs/2105.12371",
          "publishedOn": "2021-05-27T01:32:28.626Z",
          "wordCount": 607,
          "title": "Quotient Space-Based Keyword Retrieval in Sponsored Search. (arXiv:2105.12371v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12358",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Du_Z/0/1/0/all/0/1\">Zhe Du</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sattar_Y/0/1/0/all/0/1\">Yahya Sattar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tarzanagh_D/0/1/0/all/0/1\">Davoud Ataee Tarzanagh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Balzano_L/0/1/0/all/0/1\">Laura Balzano</a>, <a href=\"http://arxiv.org/find/math/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ozay_N/0/1/0/all/0/1\">Necmiye Ozay</a>",
          "description": "Real-world control applications often involve complex dynamics subject to\nabrupt changes or variations. Markov jump linear systems (MJS) provide a rich\nframework for modeling such dynamics. Despite an extensive history, theoretical\nunderstanding of parameter sensitivities of MJS control is somewhat lacking.\nMotivated by this, we investigate robustness aspects of certainty equivalent\nmodel-based optimal control for MJS with quadratic cost function. Given the\nuncertainty in the system matrices and in the Markov transition matrix is\nbounded by $\\epsilon$ and $\\eta$ respectively, robustness results are\nestablished for (i) the solution to coupled Riccati equations and (ii) the\noptimal cost, by providing explicit perturbation bounds which decay as\n$\\mathcal{O}(\\epsilon + \\eta)$ and $\\mathcal{O}((\\epsilon + \\eta)^2)$\nrespectively.",
          "link": "http://arxiv.org/abs/2105.12358",
          "publishedOn": "2021-05-27T01:32:28.620Z",
          "wordCount": 560,
          "title": "Certainty Equivalent Quadratic Control for Markov Jump Systems. (arXiv:2105.12358v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1\">Allan Gr&#xf8;nlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tranberg_J/0/1/0/all/0/1\">Jonas Tranberg</a>",
          "description": "High resolution data models like grid terrain models made from LiDAR data are\na prerequisite for modern day Geographic Information Systems applications.\nBesides providing the foundation for the very accurate digital terrain models,\nLiDAR data is also extensively used to classify which parts of the considered\nsurface comprise relevant elements like water, buildings and vegetation. In\nthis paper we consider the problem of classifying which areas of a given\nsurface are fortified by for instance, roads, sidewalks, parking spaces, paved\ndriveways and terraces. We consider using LiDAR data and orthophotos, combined\nand alone, to show how well the modern machine learning algorithms Gradient\nBoosted Trees and Convolutional Neural Networks are able to detect fortified\nareas on large real world data. The LiDAR data features, in particular the\nintensity feature that measures the signal strength of the return, that we\nconsider in this project are heavily dependent on the actual LiDAR sensor that\nmade the measurement. This is highly problematic, in particular for the\ngeneralisation capability of pattern matching algorithms, as this means that\ndata features for test data may be very different from the data the model is\ntrained on. We propose an algorithmic solution to this problem by designing a\nneural net embedding architecture that transforms data from all the different\nsensor systems into a new common representation that works as well as if the\ntraining data and test data originated from the same sensor. The final\nalgorithm result has an accuracy above 96 percent, and an AUC score above 0.99.",
          "link": "http://arxiv.org/abs/2105.12385",
          "publishedOn": "2021-05-27T01:32:28.611Z",
          "wordCount": 676,
          "title": "Learning to Detect Fortified Areas. (arXiv:2105.12385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koutini_K/0/1/0/all/0/1\">Khaled Koutini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eghbal_zadeh_H/0/1/0/all/0/1\">Hamid Eghbal-zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1\">Gerhard Widmer</a>",
          "description": "In this paper, we study the performance of variants of well-known\nConvolutional Neural Network (CNN) architectures on different audio tasks. We\nshow that tuning the Receptive Field (RF) of CNNs is crucial to their\ngeneralization. An insufficient RF limits the CNN's ability to fit the training\ndata. In contrast, CNNs with an excessive RF tend to over-fit the training data\nand fail to generalize to unseen testing data. As state-of-the-art CNN\narchitectures-in computer vision and other domains-tend to go deeper in terms\nof number of layers, their RF size increases and therefore they degrade in\nperformance in several audio classification and tagging tasks. We study\nwell-known CNN architectures and how their building blocks affect their\nreceptive field. We propose several systematic approaches to control the RF of\nCNNs and systematically test the resulting architectures on different audio\nclassification and tagging tasks and datasets. The experiments show that\nregularizing the RF of CNNs using our proposed approaches can drastically\nimprove the generalization of models, out-performing complex architectures and\npre-trained models on larger datasets. The proposed CNNs achieve\nstate-of-the-art results in multiple tasks, from acoustic scene classification\nto emotion and theme detection in music to instrument recognition, as\ndemonstrated by top ranks in several pertinent challenges (DCASE, MediaEval).",
          "link": "http://arxiv.org/abs/2105.12395",
          "publishedOn": "2021-05-27T01:32:28.605Z",
          "wordCount": 673,
          "title": "Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks. (arXiv:2105.12395v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platinsky_L/0/1/0/all/0/1\">Lukas Platinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speichert_S/0/1/0/all/0/1\">Stefanie Speichert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Blazej Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheel_O/0/1/0/all/0/1\">Oliver Scheel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yawei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "We investigate what grade of sensor data is required for training an\nimitation-learning-based AV planner on human expert demonstration.\nMachine-learned planners are very hungry for training data, which is usually\ncollected using vehicles equipped with the same sensors used for autonomous\noperation. This is costly and non-scalable. If cheaper sensors could be used\nfor collection instead, data availability would go up, which is crucial in a\nfield where data volume requirements are large and availability is small. We\npresent experiments using up to 1000 hours worth of expert demonstration and\nfind that training with 10x lower-quality data outperforms 1x AV-grade data in\nterms of planner performance. The important implication of this is that cheaper\nsensors can indeed be used. This serves to improve data access and democratize\nthe field of imitation-based motion planning. Alongside this, we perform a\nsensitivity analysis of planner performance as a function of perception range,\nfield-of-view, accuracy, and data volume, and the reason why lower-quality data\nstill provide good planning results.",
          "link": "http://arxiv.org/abs/2105.12337",
          "publishedOn": "2021-05-27T01:32:28.578Z",
          "wordCount": 625,
          "title": "What data do we need for training an AV motion planner?. (arXiv:2105.12337v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12342",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Gotoh_J/0/1/0/all/0/1\">Jun-ya Gotoh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kim_M/0/1/0/all/0/1\">Michael Jong Kim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lim_A/0/1/0/all/0/1\">Andrew E.B. Lim</a>",
          "description": "While solutions of Distributionally Robust Optimization (DRO) problems can\nsometimes have a higher out-of-sample expected reward than the Sample Average\nApproximation (SAA), there is no guarantee. In this paper, we introduce the\nclass of Distributionally Optimistic Optimization (DOO) models, and show that\nit is always possible to \"beat\" SAA out-of-sample if we consider not just\nworst-case (DRO) models but also best-case (DOO) ones. We also show, however,\nthat this comes at a cost: Optimistic solutions are more sensitive to model\nerror than either worst-case or SAA optimizers, and hence are less robust.",
          "link": "http://arxiv.org/abs/2105.12342",
          "publishedOn": "2021-05-27T01:32:28.572Z",
          "wordCount": 553,
          "title": "A data-driven approach to beating SAA out-of-sample. (arXiv:2105.12342v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12315",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saito_K/0/1/0/all/0/1\">Koichi Saito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uhlich_S/0/1/0/all/0/1\">Stefan Uhlich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fabbro_G/0/1/0/all/0/1\">Giorgio Fabbro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mitsufuji_Y/0/1/0/all/0/1\">Yuki Mitsufuji</a>",
          "description": "Recently, deep neural network (DNN)-based speech enhancement (SE) systems\nhave been used with great success. During training, such systems require clean\nspeech data - ideally, in large quantity with a variety of acoustic conditions,\nmany different speaker characteristics and for a given sampling rate (e.g.,\n48kHz for fullband SE). However, obtaining such clean speech data is not\nstraightforward - especially, if only considering publicly available datasets.\nAt the same time, a lot of material for automatic speech recognition (ASR) with\nthe desired acoustic/speaker/sampling rate characteristics is publicly\navailable except being clean, i.e., it also contains background noise as this\nis even often desired in order to have ASR systems that are noise-robust.\nHence, using such data to train SE systems is not straightforward. In this\npaper, we propose two improvements to train SE systems on noisy speech data.\nFirst, we propose several modifications of the loss functions, which make them\nrobust against noisy speech targets. In particular, computing the median over\nthe sample axis before averaging over time-frequency bins allows to use such\ndata. Furthermore, we propose a noise augmentation scheme for mixture-invariant\ntraining (MixIT), which allows using it also in such scenarios. For our\nexperiments, we use the Mozilla Common Voice dataset and we show that using our\nrobust loss function improves PESQ by up to 0.19 compared to a system trained\nin the traditional way. Similarly, for MixIT we can see an improvement of up to\n0.27 in PESQ when using our proposed noise augmentation.",
          "link": "http://arxiv.org/abs/2105.12315",
          "publishedOn": "2021-05-27T01:32:28.567Z",
          "wordCount": 693,
          "title": "Training Speech Enhancement Systems with Noisy Speech Datasets. (arXiv:2105.12315v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laugros_A/0/1/0/all/0/1\">Alfred Laugros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caplier_A/0/1/0/all/0/1\">Alice Caplier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ospici_M/0/1/0/all/0/1\">Matthieu Ospici</a>",
          "description": "Neural Networks are sensitive to various corruptions that usually occur in\nreal-world applications such as blurs, noises, low-lighting conditions, etc. To\nestimate the robustness of neural networks to these common corruptions, we\ngenerally use a group of modeled corruptions gathered into a benchmark.\nUnfortunately, no objective criterion exists to determine whether a benchmark\nis representative of a large diversity of independent corruptions. In this\npaper, we propose a metric called corruption overlapping score, which can be\nused to reveal flaws in corruption benchmarks. Two corruptions overlap when the\nrobustnesses of neural networks to these corruptions are correlated. We argue\nthat taking into account overlappings between corruptions can help to improve\nexisting benchmarks or build better ones.",
          "link": "http://arxiv.org/abs/2105.12357",
          "publishedOn": "2021-05-27T01:32:28.562Z",
          "wordCount": 545,
          "title": "Using the Overlapping Score to Improve Corruption Benchmarks. (arXiv:2105.12357v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Horvat_C/0/1/0/all/0/1\">Christian Horvat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_J/0/1/0/all/0/1\">Jean-Pascal Pfister</a>",
          "description": "Normalizing Flows (NFs) are universal density estimators based on Neuronal\nNetworks. However, this universality is limited: the density's support needs to\nbe diffeomorphic to a Euclidean space. In this paper, we propose a novel method\nto overcome this limitation without sacrificing universality. The proposed\nmethod inflates the data manifold by adding noise in the normal space, trains\nan NF on this inflated manifold, and, finally, deflates the learned density.\nOur main result provides sufficient conditions on the manifold and the specific\nchoice of noise under which the corresponding estimator is exact. Our method\nhas the same computational complexity as NFs and does not require computing an\ninverse flow. We also show that, if the embedding dimension is much larger than\nthe manifold dimension, noise in the normal space can be well approximated by\nGaussian noise. This allows to use our method for approximating arbitrary\ndensities on non-flat manifolds provided that the manifold dimension is known.",
          "link": "http://arxiv.org/abs/2105.12152",
          "publishedOn": "2021-05-27T01:32:28.548Z",
          "wordCount": 573,
          "title": "Density estimation: an inflation-deflation approach. (arXiv:2105.12152v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayan Nag</a>",
          "description": "Self-supervised learning and pre-training strategies have developed over the\nlast few years especially for Convolutional Neural Networks (CNNs). Recently\napplication of such methods can also be noticed for Graph Neural Networks\n(GNNs). In this paper, we have used a graph based self-supervised learning\nstrategy with different loss functions (Barlow Twins[ 7], HSIC[ 4], VICReg[ 1])\nwhich have shown promising results when applied with CNNs previously. We have\nalso proposed a hybrid loss function combining the advantages of VICReg and\nHSIC and called it as VICRegHSIC. The performance of these aforementioned\nmethods have been compared when applied to two different datasets namely MUTAG\nand PROTEINS. Moreover, the impact of different batch sizes, projector\ndimensions and data augmentation strategies have also been explored. The\nresults are preliminary and we will be continuing to explore with other\ndatasets.",
          "link": "http://arxiv.org/abs/2105.12247",
          "publishedOn": "2021-05-27T01:32:28.537Z",
          "wordCount": 578,
          "title": "Graph Self Supervised Learning: the BT, the HSIC, and the VICReg. (arXiv:2105.12247v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaheen_K/0/1/0/all/0/1\">Khadija Shaheen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanif_M/0/1/0/all/0/1\">Muhammad Abdullah Hanif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_O/0/1/0/all/0/1\">Osman Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "Continual learning is essential for all real-world applications, as frozen\npre-trained models cannot effectively deal with non-stationary data\ndistributions. The purpose of this study is to review the state-of-the-art\nmethods that allow continuous learning of computational models over time. We\nprimarily focus on the learning algorithms that perform continuous learning in\nan online fashion from considerably large (or infinite) sequential data and\nrequire substantially low computational and memory resources. We critically\nanalyze the key challenges associated with continual learning for autonomous\nreal-world systems and compare current methods in terms of computations,\nmemory, and network/model complexity. We also briefly describe the\nimplementations of continuous learning algorithms under three main autonomous\nsystems, i.e., self-driving vehicles, unmanned aerial vehicles, and robotics.\nThe learning methods of these autonomous systems and their strengths and\nlimitations are extensively explored in this article.",
          "link": "http://arxiv.org/abs/2105.12374",
          "publishedOn": "2021-05-27T01:32:28.531Z",
          "wordCount": 564,
          "title": "Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks. (arXiv:2105.12374v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yatong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_T/0/1/0/all/0/1\">Tanmay Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gai_Y/0/1/0/all/0/1\">Yu Gai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1\">Somayeh Sojoudi</a>",
          "description": "Recent work has shown that the training of a one-hidden-layer, scalar-output\nfully-connected ReLU neural network can be reformulated as a finite-dimensional\nconvex program. Unfortunately, the scale of such a convex program grows\nexponentially in data size. In this work, we prove that a stochastic procedure\nwith a linear complexity well approximates the exact formulation. Moreover, we\nderive a convex optimization approach to efficiently solve the \"adversarial\ntraining\" problem, which trains neural networks that are robust to adversarial\ninput perturbations. Our method can be applied to binary classification and\nregression, and provides an alternative to the current adversarial training\nmethods, such as Fast Gradient Sign Method (FGSM) and Projected Gradient\nDescent (PGD). We demonstrate in experiments that the proposed method achieves\na noticeably better adversarial robustness and performance than the existing\nmethods.",
          "link": "http://arxiv.org/abs/2105.12237",
          "publishedOn": "2021-05-27T01:32:28.509Z",
          "wordCount": 566,
          "title": "Practical Convex Formulation of Robust One-hidden-layer Neural Network Training. (arXiv:2105.12237v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Philipp_G/0/1/0/all/0/1\">George Philipp</a>",
          "description": "In essence, a neural network is an arbitrary differentiable, parametrized\nfunction. Choosing a neural network architecture for any task is as complex as\nsearching the space of those functions. For the last few years, 'neural\narchitecture design' has been largely synonymous with 'neural architecture\nsearch' (NAS), i.e. brute-force, large-scale search. NAS has yielded\nsignificant gains on practical tasks. However, NAS methods end up searching for\na local optimum in architecture space in a small neighborhood around\narchitectures that often go back decades, based on CNN or LSTM.\n\nIn this work, we present a different and complementary approach to\narchitecture design, which we term 'zero-shot architecture design' (ZSAD). We\ndevelop methods that can predict, without any training, whether an archi",
          "link": "http://arxiv.org/abs/2105.12210",
          "publishedOn": "2021-05-27T01:32:28.497Z",
          "wordCount": 957,
          "title": "The Nonlinearity Coefficient -- A Practical Guide to Neural Architecture Design. (arXiv:2105.12210v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12271",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hero_A/0/1/0/all/0/1\">Alfred Hero</a>",
          "description": "We propose a new graphical model inference procedure, called SG-PALM, for\nlearning conditional dependency structure of high-dimensional tensor-variate\ndata. Unlike most other tensor graphical models the proposed model is\ninterpretable and computationally scalable to high dimension. Physical\ninterpretability follows from the Sylvester generative (SG) model on which\nSG-PALM is based: the model is exact for any observation process that is a\nsolution of a partial differential equation of Poisson type. Scalability\nfollows from the fast proximal alternating linearized minimization (PALM)\nprocedure that SG-PALM uses during training. We establish that SG-PALM\nconverges linearly (i.e., geometric convergence rate) to a global optimum of\nits objective function. We demonstrate the scalability and accuracy of SG-PALM\nfor an important but challenging climate prediction problem: spatio-temporal\nforecasting of solar flares from multimodal imaging data.",
          "link": "http://arxiv.org/abs/2105.12271",
          "publishedOn": "2021-05-27T01:32:28.486Z",
          "wordCount": 563,
          "title": "SG-PALM: a Fast Physically Interpretable Tensor Graphical Model. (arXiv:2105.12271v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1\">Deepak-George Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olshanskyi_D/0/1/0/all/0/1\">Daniil Olshanskyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_K/0/1/0/all/0/1\">Karter Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1\">Ali Jannesari</a>",
          "description": "The major components of any successful autonomous flight system are task\ncompletion and collision avoidance. Most deep learning algorithms are\nsuccessful while executing these aspects under the environment and conditions\nin which they have been trained. However, they fail when subjected to novel\nenvironments. In this paper we present autonomous UAV flight using Deep\nReinforcement Learning augmented with Self-Attention Models that can\neffectively reason when subjected to varying inputs. In addition to their\nreasoning ability, they also are interpretable which enables it to be used\nunder real-world conditions. We have tested our algorithm under different\nweather and environments and found it to be robust compared to conventional\nDeep Reinforcement Learning algorithms.",
          "link": "http://arxiv.org/abs/2105.12254",
          "publishedOn": "2021-05-27T01:32:28.454Z",
          "wordCount": 536,
          "title": "Interpretable UAV Collision Avoidance using Deep Reinforcement Learning. (arXiv:2105.12254v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1\">Michael Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashkirov_S/0/1/0/all/0/1\">Sergey Bashkirov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rico_J/0/1/0/all/0/1\">Javier Fernandez Rico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toriyama_I/0/1/0/all/0/1\">Ike Toriyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyada_N/0/1/0/all/0/1\">Naoyuki Miyada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanagisawa_H/0/1/0/all/0/1\">Hideki Yanagisawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishizuka_K/0/1/0/all/0/1\">Kensaku Ishizuka</a>",
          "description": "Teaching an anthropomorphic robot from human example offers the opportunity\nto impart humanlike qualities on its movement. In this work we present a\nreinforcement learning based method for teaching a real world bipedal robot to\nperform movements directly from human motion capture data. Our method\nseamlessly transitions from training in a simulation environment to executing\non a physical robot without requiring any real world training iterations or\noffline steps. To overcome the disparity in joint configurations between the\nrobot and the motion capture actor, our method incorporates motion re-targeting\ninto the training process. Domain randomization techniques are used to\ncompensate for the differences between the simulated and physical systems. We\ndemonstrate our method on an internally developed humanoid robot with movements\nranging from a dynamic walk cycle to complex balancing and waving. Our\ncontroller preserves the style imparted by the motion capture data and exhibits\ngraceful failure modes resulting in safe operation for the robot. This work was\nperformed for research purposes only.",
          "link": "http://arxiv.org/abs/2105.12277",
          "publishedOn": "2021-05-27T01:32:28.442Z",
          "wordCount": 616,
          "title": "Learning Bipedal Robot Locomotion from Human Movement. (arXiv:2105.12277v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12257",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bodin_A/0/1/0/all/0/1\">Antoine Bodin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Macris_N/0/1/0/all/0/1\">Nicolas Macris</a>",
          "description": "We consider a rank-one symmetric matrix corrupted by additive noise. The\nrank-one matrix is formed by an $n$-component unknown vector on the sphere of\nradius $\\sqrt{n}$, and we consider the problem of estimating this vector from\nthe corrupted matrix in the high dimensional limit of $n$ large, by gradient\ndescent for a quadratic cost function on the sphere. Explicit formulas for the\nwhole time evolution of the overlap between the estimator and unknown vector,\nas well as the cost, are rigorously derived. In the long time limit we recover\nthe well known spectral phase transition, as a function of the signal-to-noise\nratio. The explicit formulas also allow to point out interesting transient\nfeatures of the time evolution. Our analysis technique is based on recent\nprogress in random matrix theory and uses local versions of the semi-circle\nlaw.",
          "link": "http://arxiv.org/abs/2105.12257",
          "publishedOn": "2021-05-27T01:32:28.436Z",
          "wordCount": 569,
          "title": "Rank-one matrix estimation: analytic time evolution of gradient descent dynamics. (arXiv:2105.12257v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lutter_M/0/1/0/all/0/1\">Michael Lutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jan Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>",
          "description": "When transferring a control policy from simulation to a physical system, the\npolicy needs to be robust to variations in the dynamics to perform well.\nCommonly, the optimal policy overfits to the approximate model and the\ncorresponding state-distribution, often resulting in failure to trasnfer\nunderlying distributional shifts. In this paper, we present Robust Fitted Value\nIteration, which uses dynamic programming to compute the optimal value function\non the compact state domain and incorporates adversarial perturbations of the\nsystem dynamics. The adversarial perturbations encourage a optimal policy that\nis robust to changes in the dynamics. Utilizing the continuous-time perspective\nof reinforcement learning, we derive the optimal perturbations for the states,\nactions, observations and model parameters in closed-form. Notably, the\nresulting algorithm does not require discretization of states or actions.\nTherefore, the optimal adversarial perturbations can be efficiently\nincorporated in the min-max value function update. We apply the resulting\nalgorithm to the physical Furuta pendulum and cartpole. By changing the masses\nof the systems we evaluate the quantitative and qualitative performance across\ndifferent model parameters. We show that robust value iteration is more robust\ncompared to deep reinforcement learning algorithm and the non-robust version of\nthe algorithm. Videos of the experiments are shown at\nhttps://sites.google.com/view/rfvi",
          "link": "http://arxiv.org/abs/2105.12189",
          "publishedOn": "2021-05-27T01:32:28.431Z",
          "wordCount": 647,
          "title": "Robust Value Iteration for Continuous Control Tasks. (arXiv:2105.12189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12290",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Leinwand_B/0/1/0/all/0/1\">Benjamin Leinwand</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pipiras_V/0/1/0/all/0/1\">Vladas Pipiras</a>",
          "description": "Dense networks with weighted connections often exhibit a community like\nstructure, where although most nodes are connected to each other, different\npatterns of edge weights may emerge depending on each node's community\nmembership. We propose a new framework for generating and estimating dense\nweighted networks with potentially different connectivity patterns across\ndifferent communities. The proposed model relies on a particular class of\nfunctions which map individual node characteristics to the edges connecting\nthose nodes, allowing for flexibility while requiring a small number of\nparameters relative to the number of edges. By leveraging the estimation\ntechniques, we also develop a bootstrap methodology for generating new networks\non the same set of vertices, which may be useful in circumstances where\nmultiple data sets cannot be collected. Performance of these methods are\nanalyzed in theory, simulations, and real data.",
          "link": "http://arxiv.org/abs/2105.12290",
          "publishedOn": "2021-05-27T01:32:28.413Z",
          "wordCount": 564,
          "title": "Block Dense Weighted Networks with Augmented Degree Correction. (arXiv:2105.12290v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_J/0/1/0/all/0/1\">Joymallya Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1\">Suvodeep Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1\">Tim Menzies</a>",
          "description": "Increasingly, software is making autonomous decisions in case of criminal\nsentencing, approving credit cards, hiring employees, and so on. Some of these\ndecisions show bias and adversely affect certain social groups (e.g. those\ndefined by sex, race, age, marital status). Many prior works on bias mitigation\ntake the following form: change the data or learners in multiple ways, then see\nif any of that improves fairness. Perhaps a better approach is to postulate\nroot causes of bias and then applying some resolution strategy. This paper\npostulates that the root causes of bias are the prior decisions that affect-\n(a) what data was selected and (b) the labels assigned to those examples. Our\nFair-SMOTE algorithm removes biased labels; and rebalances internal\ndistributions such that based on sensitive attribute, examples are equal in\nboth positive and negative classes. On testing, it was seen that this method\nwas just as effective at reducing bias as prior approaches. Further, models\ngenerated via Fair-SMOTE achieve higher performance (measured in terms of\nrecall and F1) than other state-of-the-art fairness improvement algorithms. To\nthe best of our knowledge, measured in terms of number of analyzed learners and\ndatasets, this study is one of the largest studies on bias mitigation yet\npresented in the literature.",
          "link": "http://arxiv.org/abs/2105.12195",
          "publishedOn": "2021-05-27T01:32:28.407Z",
          "wordCount": 662,
          "title": "Bias in Machine Learning Software: Why? How? What to do?. (arXiv:2105.12195v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Dayal Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreejith_G/0/1/0/all/0/1\">G J Sreejith</a>",
          "description": "Using a mean-field theory of signal propagation, we analyze the evolution of\ncorrelations between two signals propagating forward through a deep ReLU\nnetwork with correlated weights. Signals become highly correlated in deep ReLU\nnetworks with uncorrelated weights. We show that ReLU networks with\nanti-correlated weights can avoid this fate and have a chaotic phase where the\nsignal correlations saturate below unity. Consistent with this analysis, we\nfind that networks initialized with anti-correlated weights can train faster\n(in a teacher-student setting) by taking advantage of the increased\nexpressivity in the chaotic phase. Combining this with a previously proposed\nstrategy of using an asymmetric initialization to reduce dead node probability,\nwe propose an initialization scheme that allows faster training and learning\nthan the best-known initializations.",
          "link": "http://arxiv.org/abs/2103.12499",
          "publishedOn": "2021-05-26T01:22:12.162Z",
          "wordCount": 591,
          "title": "Initializing ReLU networks in an expressive subspace of weights. (arXiv:2103.12499v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wrzalik_M/0/1/0/all/0/1\">Marco Wrzalik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krechel_D/0/1/0/all/0/1\">Dirk Krechel</a>",
          "description": "Many recent approaches towards neural information retrieval mitigate their\ncomputational costs by using a multi-stage ranking pipeline. In the first\nstage, a number of potentially relevant candidates are retrieved using an\nefficient retrieval model such as BM25. Although BM25 has proven decent\nperformance as a first-stage ranker, it tends to miss relevant passages. In\nthis context we propose CoRT, a simple neural first-stage ranking model that\nleverages contextual representations from pretrained language models such as\nBERT to complement term-based ranking functions while causing no significant\ndelay at query time. Using the MS MARCO dataset, we show that CoRT\nsignificantly increases the candidate recall by complementing BM25 with missing\ncandidates. Consequently, we find subsequent re-rankers achieve superior\nresults with less candidates. We further demonstrate that passage retrieval\nusing CoRT can be realized with surprisingly low latencies.",
          "link": "http://arxiv.org/abs/2010.10252",
          "publishedOn": "2021-05-26T01:22:12.020Z",
          "wordCount": 619,
          "title": "CoRT: Complementary Rankings from Transformers. (arXiv:2010.10252v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1\">Christian Anti&#x107;</a>",
          "description": "Analogy-making is at the core of human intelligence and creativity with\napplications to such diverse tasks as commonsense reasoning, learning, language\nacquisition, and story telling. This paper contributes to the foundations of\nartificial general intelligence by introducing from first principles an\nabstract algebraic framework of analogical proportions of the form `$a$ is to\n$b$ what $c$ is to $d$' in the general setting of universal algebra. This\nenables us to compare mathematical objects possibly across different domains in\na uniform way which is crucial for AI-systems. The main idea is to define\nsolutions to analogical equations in terms of maximal sets of algebraic\njustifications, which amounts to deriving abstract terms of concrete elements\nfrom a `known' source domain which can then be instantiated in an `unknown'\ntarget domain to obtain analogous elements. It turns out that our notion of\nanalogical proportions has appealing mathematical properties. For example, we\nshow that analogical proportions preserve functional dependencies across\ndifferent domains, which is desirable. We study Lepage's axioms of analogical\nproportions and argue why we disagree with his symmetry, central permutation,\nstrong reflexivity, and strong determinism axioms. We compare our framework\nwith two prominent and recently introduced frameworks of analogical proportions\nfrom the literature in the concrete domains of sets and numbers, and we show\nthat in each case we either disagree with the notion from the literature\njustified by some plausible counter-example or we can show that our model\nyields strictly more reasonable solutions. This provides evidence for its\napplicability. In a broader sense, this paper is a first step towards a theory\nof analogical reasoning and learning systems with potential applications to\nfundamental AI-problems like commonsense reasoning and computational learning\nand creativity.",
          "link": "http://arxiv.org/abs/2006.02854",
          "publishedOn": "2021-05-26T01:22:12.005Z",
          "wordCount": 765,
          "title": "Analogical Proportions. (arXiv:2006.02854v6 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.10165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jinwen Ma</a>",
          "description": "Tensor completion and robust principal component analysis have been widely\nused in machine learning while the key problem relies on the minimization of a\ntensor rank that is very challenging. A common way to tackle this difficulty is\nto approximate the tensor rank with the $\\ell_1-$norm of singular values based\non its Tensor Singular Value Decomposition (T-SVD). Besides, the sparsity of a\ntensor is also measured by its $\\ell_1-$norm. However, the $\\ell_1$ penalty is\nessentially biased and thus the result will deviate. In order to sidestep the\nbias, we propose a novel non-convex tensor rank surrogate function and a novel\nnon-convex sparsity measure. In this new setting by using the concavity instead\nof the convexity, a majorization minimization algorithm is further designed for\ntensor completion and robust principal component analysis. Furthermore, we\nanalyze its theoretical properties. Finally, the experiments on natural and\nhyperspectral images demonstrate the efficacy and efficiency of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/1904.10165",
          "publishedOn": "2021-05-26T01:22:11.999Z",
          "wordCount": 616,
          "title": "T-SVD Based Non-convex Tensor Completion and Robust Principal Component Analysis. (arXiv:1904.10165v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xiaolong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhong Yin</a>",
          "description": "Use of the electroencephalogram (EEG) and machine learning approaches to\nrecognize emotions can facilitate affective human computer interactions.\nHowever, the type of EEG data constitutes an obstacle for cross-individual EEG\nfeature modelling and classification. To address this issue, we propose a\ndeep-learning framework denoted as a dynamic entropy-based pattern learning\n(DEPL) to abstract informative indicators pertaining to the neurophysiological\nfeatures among multiple individuals. DEPL enhanced the capability of\nrepresentations generated by a deep convolutional neural network by modelling\nthe interdependencies between the cortical locations of dynamical entropy based\nfeatures. The effectiveness of the DEPL has been validated with two public\ndatabases, commonly referred to as the DEAP and MAHNOB-HCI multimodal tagging\ndatabases. Specifically, the leave one subject out training and testing\nparadigm has been applied. Numerous experiments on EEG emotion recognition\ndemonstrate that the proposed DEPL is superior to those traditional machine\nlearning (ML) methods, and could learn between electrode dependencies w.r.t.\ndifferent emotions, which is meaningful for developing the effective\nhuman-computer interaction systems by adapting to human emotions in the real\nworld applications.",
          "link": "http://arxiv.org/abs/2009.12525",
          "publishedOn": "2021-05-26T01:22:11.994Z",
          "wordCount": 636,
          "title": "Cross-individual Recognition of Emotions by a Dynamic Entropy based on Pattern Learning with EEG features. (arXiv:2009.12525v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05468",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kapoor_S/0/1/0/all/0/1\">Sanyam Kapoor</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1\">Theofanis Karaletsos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bui_T/0/1/0/all/0/1\">Thang D. Bui</a>",
          "description": "Through sequential construction of posteriors on observing data online,\nBayes' theorem provides a natural framework for continual learning. We develop\nVariational Auto-Regressive Gaussian Processes (VAR-GPs), a principled\nposterior updating mechanism to solve sequential tasks in continual learning.\nBy relying on sparse inducing point approximations for scalable posteriors, we\npropose a novel auto-regressive variational distribution which reveals two\nfruitful connections to existing results in Bayesian inference, expectation\npropagation and orthogonal inducing points. Mean predictive entropy estimates\nshow VAR-GPs prevent catastrophic forgetting, which is empirically supported by\nstrong performance on modern continual learning benchmarks against competitive\nbaselines. A thorough ablation study demonstrates the efficacy of our modeling\nchoices.",
          "link": "http://arxiv.org/abs/2006.05468",
          "publishedOn": "2021-05-26T01:22:11.988Z",
          "wordCount": 552,
          "title": "Variational Auto-Regressive Gaussian Processes for Continual Learning. (arXiv:2006.05468v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.12197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>",
          "description": "We present a method to search for a probe (or query) image representation\nagainst a large gallery in the encrypted domain. We require that the probe and\ngallery images be represented in terms of a fixed-length representation, which\nis typical for representations obtained from learned networks. Our encryption\nscheme is agnostic to how the fixed-length representation is obtained and can\ntherefore be applied to any fixed-length representation in any application\ndomain. Our method, dubbed HERS (Homomorphically Encrypted Representation\nSearch), operates by (i) compressing the representation towards its estimated\nintrinsic dimensionality with minimal loss of accuracy (ii) encrypting the\ncompressed representation using the proposed fully homomorphic encryption\nscheme, and (iii) efficiently searching against a gallery of encrypted\nrepresentations directly in the encrypted domain, without decrypting them.\nNumerical results on large galleries of face, fingerprint, and object datasets\nsuch as ImageNet show that, for the first time, accurate and fast image search\nwithin the encrypted domain is feasible at scale (500 seconds; $275\\times$\nspeed up over state-of-the-art for encrypted search against a gallery of 100\nmillion).",
          "link": "http://arxiv.org/abs/2003.12197",
          "publishedOn": "2021-05-26T01:22:11.983Z",
          "wordCount": 640,
          "title": "HERS: Homomorphically Encrypted Representation Search. (arXiv:2003.12197v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush K. Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holness_G/0/1/0/all/0/1\">Gary Holness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_P/0/1/0/all/0/1\">Poopalasingam Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markushin_Y/0/1/0/all/0/1\">Yuri Markushin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melikechi_N/0/1/0/all/0/1\">Noureddine Melikechi</a>",
          "description": "Classification and identification of amino acids in aqueous solutions is\nimportant in the study of biomacromolecules. Laser Induced Breakdown\nSpectroscopy (LIBS) uses high energy laser-pulses for ablation of chemical\ncompounds whose radiated spectra are captured and recorded to reveal molecular\nstructure. Spectral peaks and noise from LIBS are impacted by experimental\nprotocols. Current methods for LIBS spectral analysis achieves promising\nresults using PCA, a linear method. It is well-known that the underlying\nphysical processes behind LIBS are highly nonlinear. Our work set out to\nunderstand the impact of LIBS spectra on suitable neighborhood size over which\nto consider pattern phenomena, if nonlinear methods capture pattern phenomena\nwith increased efficacy, and how they improve classification and identification\nof compounds. We analyzed four amino acids, polysaccharide, and a control\ngroup, water. We developed an information theoretic method for measurement of\nLIBS energy spectra, implemented manifold methods for nonlinear dimensionality\nreduction, and found while clustering results were not statistically\nsignificantly different, nonlinear methods lead to increased classification\naccuracy. Moreover, our approach uncovered the contribution of micro-wells\n(experimental protocol) in LIBS spectra. To the best of our knowledge, ours is\nthe first application of Manifold methods to LIBS amino-acid analysis in the\nresearch literature.",
          "link": "http://arxiv.org/abs/2105.12089",
          "publishedOn": "2021-05-26T01:22:11.977Z",
          "wordCount": 659,
          "title": "Investigating Manifold Neighborhood size for Nonlinear Analysis of LIBS Amino Acid Spectra. (arXiv:2105.12089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>",
          "description": "Deep neural network (DNN) usually learns the target function from low to high\nfrequency, which is called frequency principle or spectral bias. This frequency\nprinciple sheds light on a high-frequency curse of DNNs -- difficult to learn\nhigh-frequency information. Inspired by the frequency principle, a series of\nworks are devoted to develop algorithms for overcoming the high-frequency\ncurse. A natural question arises: what is the upper limit of the decaying rate\nw.r.t. frequency when one trains a DNN? In this work, our theory, confirmed by\nnumerical experiments, suggests that there is a critical decaying rate w.r.t.\nfrequency in DNN training. Below the upper limit of the decaying rate, the DNN\ninterpolates the training data by a function with a certain regularity.\nHowever, above the upper limit, the DNN interpolates the training data by a\ntrivial function, i.e., a function is only non-zero at training data points.\nOur results indicate a better way to overcome the high-frequency curse is to\ndesign a proper pre-condition approach to shift high-frequency information to\nlow-frequency one, which coincides with several previous developed algorithms\nfor fast learning high-frequency information. More importantly, this work\nrigorously proves that the high-frequency curse is an intrinsic difficulty of\nDNNs.",
          "link": "http://arxiv.org/abs/2105.11675",
          "publishedOn": "2021-05-26T01:22:11.971Z",
          "wordCount": 652,
          "title": "An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural Network. (arXiv:2105.11675v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malekzadeh_M/0/1/0/all/0/1\">Mohammad Malekzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borovykh_A/0/1/0/all/0/1\">Anastasia Borovykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz G&#xfc;nd&#xfc;z</a>",
          "description": "It is known that deep neural networks, trained for the classification of a\nnon-sensitive target attribute, can reveal sensitive attributes of their input\ndata; through features of different granularity extracted by the classifier.\nWe, taking a step forward, show that deep classifiers can be trained to\nsecretly encode a sensitive attribute of users' input data, at inference time,\ninto the classifier's outputs for the target attribute. An attack that works\neven if users have a white-box view of the classifier, and can keep all\ninternal representations hidden except for the classifier's estimation of the\ntarget attribute. We introduce an information-theoretical formulation of such\nadversaries and present efficient empirical implementations for training\nhonest-but-curious (HBC) classifiers based on this formulation: deep models\nthat can be accurate in predicting the target attribute, but also can utilize\ntheir outputs to secretly encode a sensitive attribute. Our evaluations on\nseveral tasks in real-world datasets show that a semi-trusted server can build\na classifier that is not only perfectly honest but also accurately curious. Our\nwork highlights a vulnerability that can be exploited by malicious machine\nlearning service providers to attack their user's privacy in several seemingly\nsafe scenarios; such as encrypted inferences, computations at the edge, or\nprivate knowledge distillation. We conclude by showing the difficulties in\ndistinguishing between standard and HBC classifiers and discussing potential\nproactive defenses against this vulnerability of deep classifiers.",
          "link": "http://arxiv.org/abs/2105.12049",
          "publishedOn": "2021-05-26T01:22:11.965Z",
          "wordCount": 675,
          "title": "Honest-but-Curious Nets: Sensitive Attributes of Private Inputs can be Secretly Coded into the Entropy of Classifiers' Outputs. (arXiv:2105.12049v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Killedar_V/0/1/0/all/0/1\">Vinayak Killedar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokala_P/0/1/0/all/0/1\">Praveen Kumar Pokala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seelamantula_C/0/1/0/all/0/1\">Chandra Sekhar Seelamantula</a>",
          "description": "We address the problem of compressed sensing using a deep generative prior\nmodel and consider both linear and learned nonlinear sensing mechanisms, where\nthe nonlinear one involves either a fully connected neural network or a\nconvolutional neural network. Recently, it has been argued that the\ndistribution of natural images do not lie in a single manifold but rather lie\nin a union of several submanifolds. We propose a sparsity-driven latent space\nsampling (SDLSS) framework and develop a proximal meta-learning (PML) algorithm\nto enforce sparsity in the latent space. SDLSS allows the range-space of the\ngenerator to be considered as a union-of-submanifolds. We also derive the\nsample complexity bounds within the SDLSS framework for the linear measurement\nmodel. The results demonstrate that for a higher degree of compression, the\nSDLSS method is more efficient than the state-of-the-art method. We first\nconsider a comparison between linear and nonlinear sensing mechanisms on\nFashion-MNIST dataset and show that the learned nonlinear version is superior\nto the linear one. Subsequent comparisons with the deep compressive sensing\n(DCS) framework proposed in the literature are reported. We also consider the\neffect of the dimension of the latent space and the sparsity factor in\nvalidating the SDLSS framework. Performance quantification is carried out by\nemploying three objective metrics: peak signal-to-noise ratio (PSNR),\nstructural similarity index metric (SSIM), and reconstruction error (RE).",
          "link": "http://arxiv.org/abs/2105.11956",
          "publishedOn": "2021-05-26T01:22:11.948Z",
          "wordCount": 666,
          "title": "Learning Generative Prior with Latent Space Sparsity Constraints. (arXiv:2105.11956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan T&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-T&#xfc;r</a>",
          "description": "Interactive robots navigating photo-realistic environments face challenges\nunderlying vision-and-language navigation (VLN), but in addition, they need to\nbe trained to handle the dynamic nature of dialogue. However, research in\nCooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts\nwith a guide in natural language in order to reach a goal, treats the dialogue\nhistory as a VLN-style static instruction. In this paper, we present VISITRON,\na navigator better suited to the interactive regime inherent to CVDN by being\ntrained to: i) identify and associate object-level concepts and semantics\nbetween the environment and dialogue history, ii) identify when to interact vs.\nnavigate via imitation learning of a binary classification head. We perform\nextensive ablations with VISITRON to gain empirical insights and improve\nperformance on CVDN. VISITRON is competitive with models on the static CVDN\nleaderboard. We also propose a generalized interactive regime to fine-tune and\nevaluate VISITRON and future such models with pre-trained guides for\nadaptability.",
          "link": "http://arxiv.org/abs/2105.11589",
          "publishedOn": "2021-05-26T01:22:11.936Z",
          "wordCount": 617,
          "title": "VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator. (arXiv:2105.11589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">S. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">L. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">J. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>",
          "description": "The Graph Neural Network (GNN) has achieved remarkable success in graph data\nrepresentation. However, the previous work only considered the ideal balanced\ndataset, and the practical imbalanced dataset was rarely considered, which, on\nthe contrary, is of more significance for the application of GNN. Traditional\nmethods such as resampling, reweighting and synthetic samples that deal with\nimbalanced datasets are no longer applicable in GNN. Ensemble models can handle\nimbalanced datasets better compared with single estimator. Besides, ensemble\nlearning can achieve higher estimation accuracy and has better reliability\ncompared with the single estimator. In this paper, we propose an ensemble model\ncalled AdaGCN, which uses a Graph Convolutional Network (GCN) as the base\nestimator during adaptive boosting. In AdaGCN, a higher weight will be set for\nthe training samples that are not properly classified by the previous\nclassifier, and transfer learning is used to reduce computational cost and\nincrease fitting capability. Experiments show that the AdaGCN model we proposed\nachieves better performance than GCN, GraphSAGE, GAT, N-GCN and the most of\nadvanced reweighting and resampling methods on synthetic imbalanced datasets,\nwith an average improvement of 4.3%. Our model also improves state-of-the-art\nbaselines on all of the challenging node classification tasks we consider:\nCora, Citeseer, Pubmed, and NELL.",
          "link": "http://arxiv.org/abs/2105.11625",
          "publishedOn": "2021-05-26T01:22:11.905Z",
          "wordCount": 656,
          "title": "AdaGCN:Adaptive Boosting Algorithm for Graph Convolutional Networks on Imbalanced Node Classification. (arXiv:2105.11625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_N/0/1/0/all/0/1\">Nidhi Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sharmishtha Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christian_R/0/1/0/all/0/1\">Ryan Christian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gridley_J/0/1/0/all/0/1\">Jared Gridley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammad Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1\">Alex Gittens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>",
          "description": "Large amounts of threat intelligence information about malware attacks are\navailable in disparate, typically unstructured, formats. Knowledge graphs can\ncapture this information and its context using RDF triples represented by\nentities and relations. Sparse or inaccurate threat information, however, leads\nto challenges such as incomplete or erroneous triples. Generic information\nextraction (IE) models used to populate the knowledge graph cannot fully\nguarantee domain-specific context. This paper proposes a system to generate a\nMalware Knowledge Graph called MalKG, the first open-source automated knowledge\ngraph for malware threat intelligence. MalKG dataset (MT40K\\footnote{ Anonymous\nGitHub link: https://github.com/malkg-researcher/MalKG}) contains approximately\n40,000 triples generated from 27,354 unique entities and 34 relations. For\nground truth, we manually curate a knowledge graph called MT3K, with 3,027\ntriples generated from 5,741 unique entities and 22 relations. We demonstrate\nthe intelligence prediction of MalKG using two use cases. Predicting malware\nthreat information using the benchmark model achieves 80.4 for the hits@10\nmetric (predicts the top 10 options for an information class), and 0.75 for the\nMRR (mean reciprocal rank). We also propose an automated, contextual framework\nfor information extraction, both manually and automatically, at the sentence\nlevel from 1,100 malware threat reports and from the common vulnerabilities and\nexposures (CVE) database.",
          "link": "http://arxiv.org/abs/2102.05571",
          "publishedOn": "2021-05-26T01:22:11.900Z",
          "wordCount": 686,
          "title": "Predicting malware threat intelligence using KGs. (arXiv:2102.05571v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1\">Nikolay O. Nikitin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revin_I/0/1/0/all/0/1\">Ilia Revin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hvatov_A/0/1/0/all/0/1\">Alexander Hvatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vychuzhanin_P/0/1/0/all/0/1\">Pavel Vychuzhanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1\">Anna V. Kalyuzhnaya</a>",
          "description": "The paper describes the usage of intelligent approaches for field development\ntasks that may assist a decision-making process. We focused on the problem of\nwells location optimization and two tasks within it: improving the quality of\noil production estimation and estimation of reservoir characteristics for\nappropriate wells allocation and parametrization, using machine learning\nmethods. For oil production estimation, we implemented and investigated the\nquality of forecasting models: physics-based, pure data-driven, and hybrid one.\nThe CRMIP model was chosen as a physics-based approach. We compare it with the\nmachine learning and hybrid methods in a frame of oil production forecasting\ntask. In the investigation of reservoir characteristics for wells location\nchoice, we automated the seismic analysis using evolutionary identification of\nconvolutional neural network for the reservoir detection. The Volve oil field\ndataset was used as a case study to conduct the experiments. The implemented\napproaches can be used to analyze different oil fields or adapted to similar\nphysics-related problems.",
          "link": "http://arxiv.org/abs/2103.02598",
          "publishedOn": "2021-05-26T01:22:11.893Z",
          "wordCount": 644,
          "title": "Hybrid and Automated Machine Learning Approaches for Oil Fields Development: the Case Study of Volve Field, North Sea. (arXiv:2103.02598v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarbu_S/0/1/0/all/0/1\">Septimia Sarbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_A/0/1/0/all/0/1\">Abdellatif Zaidi</a>",
          "description": "We consider the problem of learning parametric distributions from their\nquantized samples in a network. Specifically, $n$ agents or sensors observe\nindependent samples of an unknown parametric distribution; and each of them\nuses $k$ bits to describe its observed sample to a central processor whose goal\nis to estimate the unknown distribution. First, we establish a generalization\nof the well-known van Trees inequality to general $L_p$-norms, with $p > 1$, in\nterms of Generalized Fisher information. Then, we develop minimax lower bounds\non the estimation error for two losses: general $L_p$-norms and the related\nWasserstein loss from optimal transport.",
          "link": "http://arxiv.org/abs/2105.12019",
          "publishedOn": "2021-05-26T01:22:11.888Z",
          "wordCount": 540,
          "title": "On learning parametric distributions from quantized samples. (arXiv:2105.12019v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10919",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Olmschenk_G/0/1/0/all/0/1\">Greg Olmschenk</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Silva_S/0/1/0/all/0/1\">Stela Ishitani Silva</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Rau_G/0/1/0/all/0/1\">Gioia Rau</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Barry_R/0/1/0/all/0/1\">Richard K. Barry</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kruse_E/0/1/0/all/0/1\">Ethan Kruse</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Cacciapuoti_L/0/1/0/all/0/1\">Luca Cacciapuoti</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kostov_V/0/1/0/all/0/1\">Veselin Kostov</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Powell_B/0/1/0/all/0/1\">Brian P. Powell</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wyrwas_E/0/1/0/all/0/1\">Edward Wyrwas</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Schnittman_J/0/1/0/all/0/1\">Jeremy D. Schnittman</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Barclay_T/0/1/0/all/0/1\">Thomas Barclay</a>",
          "description": "The Transiting Exoplanet Survey Satellite (TESS) mission measured light from\nstars in ~75% of the sky throughout its two year primary mission, resulting in\nmillions of TESS 30-minute cadence light curves to analyze in the search for\ntransiting exoplanets. To search this vast data trove for transit signals, we\naim to provide an approach that is both computationally efficient and produces\nhighly performant predictions. This approach minimizes the required human\nsearch effort. We present a convolutional neural network, which we train to\nidentify planetary transit signals and dismiss false positives. To make a\nprediction for a given light curve, our network requires no prior transit\nparameters identified using other methods. Our network performs inference on a\nTESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large\nscale archival searches. We present 181 new planet candidates identified by our\nnetwork, which pass subsequent human vetting designed to rule out false\npositives. Our neural network model is additionally provided as open-source\ncode for public use and extension.",
          "link": "http://arxiv.org/abs/2101.10919",
          "publishedOn": "2021-05-26T01:22:11.883Z",
          "wordCount": 672,
          "title": "Identifying Planetary Transit Candidates in TESS Full-Frame Image Light Curves via Convolutional Neural Networks. (arXiv:2101.10919v2 [astro-ph.EP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07562",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Yang_S/0/1/0/all/0/1\">Seong-Gyu Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kim_B/0/1/0/all/0/1\">Beom Jun Kim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Son_S/0/1/0/all/0/1\">Seung-Woo Son</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kim_H/0/1/0/all/0/1\">Heetae Kim</a>",
          "description": "Complex network analyses have provided clues to improve power-grid stability\nwith the help of numerical models. The high computational cost of numerical\nsimulations, however, has inhibited the approach especially when it deals with\nthe dynamic properties of power grids such as frequency synchronization. In\nthis study, we investigate machine learning techniques to estimate the\nstability of power grid synchronization. We test three different machine\nlearning algorithms -- random forest, support vector machine, and artificial\nneural network -- training them with two different types of synthetic power\ngrids consisting of homogeneous and heterogeneous input-power distribution,\nrespectively. We find that the three machine learning models better predict the\nsynchronization stability of power-grid nodes when they are trained with the\nheterogeneous input-power distribution than the homogeneous one. With the\nreal-world power grids of Great Britain, Spain, France, and Germany, we also\ndemonstrate that the machine learning algorithms trained on synthetic power\ngrids are transferable to the stability prediction of the real-world power\ngrids, which implies the prospective applicability of machine learning\ntechniques on power-grid studies.",
          "link": "http://arxiv.org/abs/2105.07562",
          "publishedOn": "2021-05-26T01:22:11.875Z",
          "wordCount": 631,
          "title": "Power-grid stability predictions using transferable machine learning. (arXiv:2105.07562v2 [physics.soc-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doumanidis_C/0/1/0/all/0/1\">Constantine C. Doumanidis</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostou_C/0/1/0/all/0/1\">Christina Anagnostou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Arvaniti_E/0/1/0/all/0/1\">Evangelia-Sofia Arvaniti</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulou_A/0/1/0/all/0/1\">Anthi Papadopoulou</a> (1) ((1) Aristotle University of Thessaloniki)",
          "description": "Recent interest in exploiting Deep Learning techniques for Noise Suppression,\nhas led to the creation of Hybrid Denoising Systems that combine classic Signal\nProcessing with Deep Learning. In this paper, we concentrated our efforts on\nextending the RNNoise denoising system (arXiv:1709.08243) with the inclusion of\ncomplementary features during the training phase. We present a comprehensive\nexplanation of the set-up process of a modified system and present the\ncomparative results derived from a performance evaluation analysis, using a\nreference version of RNNoise as control.",
          "link": "http://arxiv.org/abs/2105.11813",
          "publishedOn": "2021-05-26T01:22:11.857Z",
          "wordCount": 547,
          "title": "RNNoise-Ex: Hybrid Speech Enhancement System based on RNN and Spectral Features. (arXiv:2105.11813v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Amartya Mukherjee</a>",
          "description": "Growing advancements in reinforcement learning has led to advancements in\ncontrol theory. Reinforcement learning has effectively solved the inverted\npendulum problem and more recently the double inverted pendulum problem. In\nreinforcement learning, our agents learn by interacting with the control system\nwith the goal of maximizing rewards. In this paper, we explore three such\nreward functions in the cart position problem. This paper concludes that a\ndiscontinuous reward function that gives non-zero rewards to agents only if\nthey are within a given distance from the desired position gives the best\nresults.",
          "link": "http://arxiv.org/abs/2105.11617",
          "publishedOn": "2021-05-26T01:22:11.852Z",
          "wordCount": 536,
          "title": "A Comparison of Reward Functions in Q-Learning Applied to a Cart Position Problem. (arXiv:2105.11617v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12062",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tian_L/0/1/0/all/0/1\">Lai Tian</a>, <a href=\"http://arxiv.org/find/math/1/au:+So_A/0/1/0/all/0/1\">Anthony Man-Cho So</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cheng_J/0/1/0/all/0/1\">James Cheng</a>",
          "description": "The problem of finding near-stationary points in convex optimization has not\nbeen adequately studied yet, unlike other optimality measures such as\nminimizing function value. Even in the deterministic case, the optimal method\n(OGM-G, due to Kim and Fessler (2021)) has just been discovered recently. In\nthis work, we conduct a systematic study of the algorithmic techniques in\nfinding near-stationary points of convex finite-sums. Our main contributions\nare several algorithmic discoveries: (1) we discover a memory-saving variant of\nOGM-G based on the performance estimation problem approach (Drori and Teboulle,\n2014); (2) we design a new accelerated SVRG variant that can simultaneously\nachieve fast rates for both minimizing gradient norm and function value; (3) we\npropose an adaptively regularized accelerated SVRG variant, which does not\nrequire the knowledge of some unknown initial constants and achieves\nnear-optimal complexities. We put an emphasis on the simplicity and\npracticality of the new schemes, which could facilitate future developments.",
          "link": "http://arxiv.org/abs/2105.12062",
          "publishedOn": "2021-05-26T01:22:11.846Z",
          "wordCount": 596,
          "title": "Practical Schemes for Finding Near-Stationary Points of Convex Finite-Sums. (arXiv:2105.12062v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mingu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung Quang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seungju Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "Adversarial attack is aimed at fooling the target classifier with\nimperceptible perturbation. Adversarial examples, which are carefully crafted\nwith a malicious purpose, can lead to erroneous predictions, resulting in\ncatastrophic accidents. To mitigate the effects of adversarial attacks, we\npropose a novel purification model called CAP-GAN. CAP-GAN takes account of the\nidea of pixel-level and feature-level consistency to achieve reasonable\npurification under cycle-consistent learning. Specifically, we utilize the\nguided attention module and knowledge distillation to convey meaningful\ninformation to the purification model. Once a model is fully trained, inputs\nwould be projected into the purification model and transformed into clean-like\nimages. We vary the capacity of the adversary to argue the robustness against\nvarious types of attack strategies. On the CIFAR-10 dataset, CAP-GAN\noutperforms other pre-processing based defenses under both black-box and\nwhite-box settings.",
          "link": "http://arxiv.org/abs/2102.07304",
          "publishedOn": "2021-05-26T01:22:11.840Z",
          "wordCount": 607,
          "title": "CAP-GAN: Towards Adversarial Robustness with Cycle-consistent Attentional Purification. (arXiv:2102.07304v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sacha_M/0/1/0/all/0/1\">Miko&#x142;aj Sacha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaz_M/0/1/0/all/0/1\">Miko&#x142;aj B&#x142;a&#x17c;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrski_P/0/1/0/all/0/1\">Piotr Byrski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabrowski_Tumanski_P/0/1/0/all/0/1\">Pawe&#x142; D&#x105;browski-Tuma&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrominski_M/0/1/0/all/0/1\">Miko&#x142;aj Chromi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loska_R/0/1/0/all/0/1\">Rafa&#x142; Loska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wlodarczyk_Pruszynski_P/0/1/0/all/0/1\">Pawe&#x142; W&#x142;odarczyk-Pruszy&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jastrzebski_S/0/1/0/all/0/1\">Stanis&#x142;aw Jastrz&#x119;bski</a>",
          "description": "The central challenge in automated synthesis planning is to be able to\ngenerate and predict outcomes of a diverse set of chemical reactions. In\nparticular, in many cases, the most likely synthesis pathway cannot be applied\ndue to additional constraints, which requires proposing alternative chemical\nreactions. With this in mind, we present Molecule Edit Graph Attention Network\n(MEGAN), an end-to-end encoder-decoder neural model. MEGAN is inspired by\nmodels that express a chemical reaction as a sequence of graph edits, akin to\nthe arrow pushing formalism. We extend this model to retrosynthesis prediction\n(predicting substrates given the product of a chemical reaction) and scale it\nup to large datasets. We argue that representing the reaction as a sequence of\nedits enables MEGAN to efficiently explore the space of plausible chemical\nreactions, maintaining the flexibility of modeling the reaction in an\nend-to-end fashion, and achieving state-of-the-art accuracy in standard\nbenchmarks. Code and trained models are made available online at\nhttps://github.com/molecule-one/megan.",
          "link": "http://arxiv.org/abs/2006.15426",
          "publishedOn": "2021-05-26T01:22:11.835Z",
          "wordCount": 638,
          "title": "Molecule Edit Graph Attention Network: Modeling Chemical Reactions as Sequences of Graph Edits. (arXiv:2006.15426v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13881",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1\">Jason M. Klusowski</a>",
          "description": "This paper shows that decision trees constructed with Classification and\nRegression Trees (CART) methodology are universally consistent in an additive\nmodel context, even when the number of predictor variables scales exponentially\nwith the sample size, under certain $1$-norm sparsity constraints. The\nconsistency is universal in the sense that there are no a priori assumptions on\nthe distribution of the predictor variables. Amazingly, this adaptivity to\n(approximate or exact) sparsity is achieved with a single tree, as opposed to\nwhat might be expected for an ensemble. Finally, we show that these qualitative\nproperties of individual trees are inherited by Breiman's random forests.\nAnother surprise is that consistency holds even when the \"mtry\" tuning\nparameter vanishes as a fraction of the number of predictor variables, thus\nspeeding up computation of the forest. A key step in the analysis is the\nestablishment of an oracle inequality, which precisely characterizes the\ngoodness-of-fit and complexity tradeoff for a misspecified model.",
          "link": "http://arxiv.org/abs/2104.13881",
          "publishedOn": "2021-05-26T01:22:11.819Z",
          "wordCount": 611,
          "title": "Universal Consistency of Decision Trees in High Dimensions. (arXiv:2104.13881v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jarboui_F/0/1/0/all/0/1\">Firas Jarboui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1\">Vianney Perchet</a>",
          "description": "The gloabal objective of inverse Reinforcement Learning (IRL) is to estimate\nthe unknown cost function of some MDP base on observed trajectories generated\nby (approximate) optimal policies. The classical approach consists in tuning\nthis cost function so that associated optimal trajectories (that minimise the\ncumulative discounted cost, i.e. the classical RL loss) are 'similar' to the\nobserved ones. Prior contributions focused on penalising degenerate solutions\nand improving algorithmic scalability. Quite orthogonally to them, we question\nthe pertinence of characterising optimality with respect to the cumulative\ndiscounted cost as it induces an implicit bias against policies with longer\nmixing times. State of the art value based RL algorithms circumvent this issue\nby solving for the fixed point of the Bellman optimality operator, a stronger\ncriterion that is not well defined for the inverse problem. To alleviate this\nbias in IRL, we introduce an alternative training loss that puts more weights\non future states which yields a reformulation of the (maximum entropy) IRL\nproblem. The algorithms we devised exhibit enhanced performances (and similar\ntractability) than off-the-shelf ones in multiple OpenAI gym environments.",
          "link": "http://arxiv.org/abs/2105.11812",
          "publishedOn": "2021-05-26T01:22:11.812Z",
          "wordCount": 596,
          "title": "A Generalised Inverse Reinforcement Learning Framework. (arXiv:2105.11812v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sumengen_B/0/1/0/all/0/1\">Baris Sumengen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">Anand Rajagopalan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Citovsky_G/0/1/0/all/0/1\">Gui Citovsky</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Simcha_D/0/1/0/all/0/1\">David Simcha</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Pradipta Mitra</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Blasiak_S/0/1/0/all/0/1\">Sam Blasiak</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mason Liang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a> (1) ((1) Google Research, (2) 0x Labs)",
          "description": "Hierarchical Agglomerative Clustering (HAC) is one of the oldest but still\nmost widely used clustering methods. However, HAC is notoriously hard to scale\nto large data sets as the underlying complexity is at least quadratic in the\nnumber of data points and many algorithms to solve HAC are inherently\nsequential. In this paper, we propose {Reciprocal Agglomerative Clustering\n(RAC)}, a distributed algorithm for HAC, that uses a novel strategy to\nefficiently merge clusters in parallel. We prove theoretically that RAC\nrecovers the exact solution of HAC. Furthermore, under clusterability and\nbalancedness assumption we show provable speedups in total runtime due to the\nparallelism. We also show that these speedups are achievable for certain\nprobabilistic data models. In extensive experiments, we show that this\nparallelism is achieved on real world data sets and that the proposed RAC\nalgorithm can recover the HAC hierarchy on billions of data points connected by\ntrillions of edges in less than an hour.",
          "link": "http://arxiv.org/abs/2105.11653",
          "publishedOn": "2021-05-26T01:22:11.806Z",
          "wordCount": 608,
          "title": "Scaling Hierarchical Agglomerative Clustering to Billion-sized Datasets. (arXiv:2105.11653v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11765",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thebille_A/0/1/0/all/0/1\">Ann-Katrin Thebille</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dietrich_E/0/1/0/all/0/1\">Esther Dietrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klaus_M/0/1/0/all/0/1\">Martin Klaus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gernhold_L/0/1/0/all/0/1\">Lukas Gernhold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lennartz_M/0/1/0/all/0/1\">Maximilian Lennartz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuppe_C/0/1/0/all/0/1\">Christoph Kuppe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kramann_R/0/1/0/all/0/1\">Rafael Kramann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huber_T/0/1/0/all/0/1\">Tobias B. Huber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sauter_G/0/1/0/all/0/1\">Guido Sauter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puelles_V/0/1/0/all/0/1\">Victor G. Puelles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmermann_M/0/1/0/all/0/1\">Marina Zimmermann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonn_S/0/1/0/all/0/1\">Stefan Bonn</a>",
          "description": "The automated analysis of medical images is currently limited by technical\nand biological noise and bias. The same source tissue can be represented by\nvastly different images if the image acquisition or processing protocols vary.\nFor an image analysis pipeline, it is crucial to compensate such biases to\navoid misinterpretations. Here, we evaluate, compare, and improve existing\ngenerative model architectures to overcome domain shifts for immunofluorescence\n(IF) and Hematoxylin and Eosin (H&E) stained microscopy images. To determine\nthe performance of the generative models, the original and transformed images\nwere segmented or classified by deep neural networks that were trained only on\nimages of the target bias. In the scope of our analysis, U-Net cycleGANs\ntrained with an additional identity and an MS-SSIM-based loss and Fixed-Point\nGANs trained with an additional structure loss led to the best results for the\nIF and H&E stained samples, respectively. Adapting the bias of the samples\nsignificantly improved the pixel-level segmentation for human kidney glomeruli\nand podocytes and improved the classification accuracy for human prostate\nbiopsies by up to 14%.",
          "link": "http://arxiv.org/abs/2105.11765",
          "publishedOn": "2021-05-26T01:22:11.801Z",
          "wordCount": 651,
          "title": "Deep learning-based bias transfer for overcoming laboratory differences of microscopic images. (arXiv:2105.11765v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sazanovich_M/0/1/0/all/0/1\">Mikita Sazanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolskaya_A/0/1/0/all/0/1\">Anastasiya Nikolskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belousov_Y/0/1/0/all/0/1\">Yury Belousov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shpilman_A/0/1/0/all/0/1\">Aleksei Shpilman</a>",
          "description": "Black-box optimization is one of the vital tasks in machine learning, since\nit approximates real-world conditions, in that we do not always know all the\nproperties of a given system, up to knowing almost nothing but the results.\nThis paper describes our approach to solving the black-box optimization\nchallenge at NeurIPS 2020 through learning search space partition for local\nBayesian optimization. We describe the task of the challenge as well as our\nalgorithm for low budget optimization that we named \\texttt{SPBOpt}. We\noptimize the hyper-parameters of our algorithm for the competition finals using\nmulti-task Bayesian optimization on results from the first two evaluation\nsettings. Our approach has ranked third in the competition finals.",
          "link": "http://arxiv.org/abs/2012.10335",
          "publishedOn": "2021-05-26T01:22:11.795Z",
          "wordCount": 586,
          "title": "Solving Black-Box Optimization Challenge via Learning Search Space Partition for Local Bayesian Optimization. (arXiv:2012.10335v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Steffen Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "Contrastive learning has recently seen tremendous success in self-supervised\nlearning. So far, however, it is largely unclear why the learned\nrepresentations generalize so effectively to a large variety of downstream\ntasks. We here prove that feedforward models trained with objectives belonging\nto the commonly used InfoNCE family learn to implicitly invert the underlying\ngenerative model of the observed data. While the proofs make certain\nstatistical assumptions about the generative model, we observe empirically that\nour findings hold even if these assumptions are severely violated. Our theory\nhighlights a fundamental connection between contrastive learning, generative\nmodeling, and nonlinear independent component analysis, thereby furthering our\nunderstanding of the learned representations as well as providing a theoretical\nfoundation to derive more effective contrastive losses.",
          "link": "http://arxiv.org/abs/2102.08850",
          "publishedOn": "2021-05-26T01:22:11.774Z",
          "wordCount": 605,
          "title": "Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+dOrsi_T/0/1/0/all/0/1\">Tommaso d&#x27;Orsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novikov_G/0/1/0/all/0/1\">Gleb Novikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steurer_D/0/1/0/all/0/1\">David Steurer</a>",
          "description": "We consider a robust linear regression model $y=X\\beta^* + \\eta$, where an\nadversary oblivious to the design $X\\in \\mathbb{R}^{n\\times d}$ may choose\n$\\eta$ to corrupt all but an $\\alpha$ fraction of the observations $y$ in an\narbitrary way. Prior to our work, even for Gaussian $X$, no estimator for\n$\\beta^*$ was known to be consistent in this model except for quadratic sample\nsize $n \\gtrsim (d/\\alpha)^2$ or for logarithmic inlier fraction $\\alpha\\ge\n1/\\log n$. We show that consistent estimation is possible with nearly linear\nsample size and inverse-polynomial inlier fraction. Concretely, we show that\nthe Huber loss estimator is consistent for every sample size $n=\n\\omega(d/\\alpha^2)$ and achieves an error rate of $O(d/\\alpha^2n)^{1/2}$. Both\nbounds are optimal (up to constant factors). Our results extend to designs far\nbeyond the Gaussian case and only require the column span of $X$ to not contain\napproximately sparse vectors). (similar to the kind of assumption commonly made\nabout the kernel space for compressed sensing). We provide two technically\nsimilar proofs. One proof is phrased in terms of strong convexity, extending\nwork of [Tsakonas et al.'14], and particularly short. The other proof\nhighlights a connection between the Huber loss estimator and high-dimensional\nmedian computations. In the special case of Gaussian designs, this connection\nleads us to a strikingly simple algorithm based on computing coordinate-wise\nmedians that achieves optimal guarantees in nearly-linear time, and that can\nexploit sparsity of $\\beta^*$. The model studied here also captures\nheavy-tailed noise distributions that may not even have a first moment.",
          "link": "http://arxiv.org/abs/2009.14774",
          "publishedOn": "2021-05-26T01:22:11.765Z",
          "wordCount": 707,
          "title": "Consistent regression when oblivious outliers overwhelm. (arXiv:2009.14774v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burgess_C/0/1/0/all/0/1\">Cassandra Burgess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neisinger_C/0/1/0/all/0/1\">Cordelia Neisinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinner_R/0/1/0/all/0/1\">Rafael Dinner</a>",
          "description": "We present a novel convolutional neural network that learns to match images\nof an object taken from different viewpoints or by different optical sensors.\nOur Re-Identification Across Domain Network (RADON) scores pairs of input\nimages from different domains on similarity. Our approach extends previous work\non Siamese networks and modifies them to more challenging use cases, including\nlow- and no-shot learning, in which few images of a specific target are\navailable for training. RADON shows strong performance on cross-view vehicle\nmatching and cross-domain person identification in a no-shot learning\nenvironment.",
          "link": "http://arxiv.org/abs/2105.12056",
          "publishedOn": "2021-05-26T01:22:11.757Z",
          "wordCount": 524,
          "title": "Matching Targets Across Domains with RADON, the Re-Identification Across Domain Network. (arXiv:2105.12056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhuangwei Shi</a>",
          "description": "Kalman Filter requires the true parameters of the model and solves optimal\nstate estimation recursively. Expectation Maximization (EM) algorithm is\napplicable for estimating the parameters of the model that are not available\nbefore Kalman filtering, which is EM-KF algorithm. To improve the preciseness\nof EM-KF algorithm, the author presents a state estimation method by combining\nthe Long-Short Term Memory network (LSTM), Transformer and EM-KF algorithm in\nthe framework of Encoder-Decoder in Sequence to Sequence (seq2seq). Simulation\non a linear mobile robot model demonstrates that the new method is more\naccurate. Source code of this paper is available at\nhttps://github.com/zshicode/Deep-Learning-Based-State-Estimation.",
          "link": "http://arxiv.org/abs/2105.00250",
          "publishedOn": "2021-05-26T01:22:11.752Z",
          "wordCount": 545,
          "title": "Incorporating Transformer and LSTM to Kalman Filter with EM algorithm for state estimation. (arXiv:2105.00250v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_A/0/1/0/all/0/1\">Adam Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Yuwei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher G. Brinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanhua Li</a>",
          "description": "Existing topic modeling and text segmentation methodologies generally require\nlarge datasets for training, limiting their capabilities when only small\ncollections of text are available. In this work, we reexamine the inter-related\nproblems of \"topic identification\" and \"text segmentation\" for sparse document\nlearning, when there is a single new text of interest. In developing a\nmethodology to handle single documents, we face two major challenges. First is\nsparse information: with access to only one document, we cannot train\ntraditional topic models or deep learning algorithms. Second is significant\nnoise: a considerable portion of words in any single document will produce only\nnoise and not help discern topics or segments. To tackle these issues, we\ndesign an unsupervised, computationally efficient methodology called BATS:\nBiclustering Approach to Topic modeling and Segmentation. BATS leverages three\nkey ideas to simultaneously identify topics and segment text: (i) a new\nmechanism that uses word order information to reduce sample complexity, (ii) a\nstatistically sound graph-based biclustering technique that identifies latent\nstructures of words and sentences, and (iii) a collection of effective\nheuristics that remove noise words and award important words to further improve\nperformance. Experiments on four datasets show that our approach outperforms\nseveral state-of-the-art baselines when considering topic coherence, topic\ndiversity, segmentation, and runtime comparison metrics.",
          "link": "http://arxiv.org/abs/2008.02218",
          "publishedOn": "2021-05-26T01:22:11.746Z",
          "wordCount": 706,
          "title": "BATS: A Spectral Biclustering Approach to Single Document Topic Modeling and Segmentation. (arXiv:2008.02218v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01815",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Everett_M/0/1/0/all/0/1\">Michael Everett</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Habibi_G/0/1/0/all/0/1\">Golnaz Habibi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+How_J/0/1/0/all/0/1\">Jonathan P. How</a>",
          "description": "Neural Networks (NNs) can provide major empirical performance improvements\nfor robotic systems, but they also introduce challenges in formally analyzing\nthose systems' safety properties. In particular, this work focuses on\nestimating the forward reachable set of closed-loop systems with NN\ncontrollers. Recent work provides bounds on these reachable sets, yet the\ncomputationally efficient approaches provide overly conservative bounds (thus\ncannot be used to verify useful properties), whereas tighter methods are too\nintensive for online computation. This work bridges the gap by formulating a\nconvex optimization problem for reachability analysis for closed-loop systems\nwith NN controllers. While the solutions are less tight than prior semidefinite\nprogram-based methods, they are substantially faster to compute, and some of\nthe available computation time can be used to refine the bounds through input\nset partitioning, which more than overcomes the tightness gap. The proposed\nframework further considers systems with measurement and process noise, thus\nbeing applicable to realistic systems with uncertainty. Finally, numerical\ncomparisons show $10\\times$ reduction in conservatism in $\\frac{1}{2}$ of the\ncomputation time compared to the state-of-the-art, and the ability to handle\nvarious sources of uncertainty is highlighted on a quadrotor model.",
          "link": "http://arxiv.org/abs/2101.01815",
          "publishedOn": "2021-05-26T01:22:11.740Z",
          "wordCount": 646,
          "title": "Efficient Reachability Analysis of Closed-Loop Systems with Neural Network Controllers. (arXiv:2101.01815v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussenot_L/0/1/0/all/0/1\">Leonard Hussenot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrychowicz_M/0/1/0/all/0/1\">Marcin Andrychowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_D/0/1/0/all/0/1\">Damien Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadashi_R/0/1/0/all/0/1\">Robert Dadashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raichuk_A/0/1/0/all/0/1\">Anton Raichuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stafiniak_L/0/1/0/all/0/1\">Lukasz Stafiniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girgin_S/0/1/0/all/0/1\">Sertan Girgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinier_R/0/1/0/all/0/1\">Raphael Marinier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momchev_N/0/1/0/all/0/1\">Nikola Momchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_S/0/1/0/all/0/1\">Sabela Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orsini_M/0/1/0/all/0/1\">Manu Orsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1\">Matthieu Geist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1\">Olivier Pietquin</a>",
          "description": "We address the issue of tuning hyperparameters (HPs) for imitation learning\nalgorithms in the context of continuous-control, when the underlying reward\nfunction of the demonstrating expert cannot be observed at any time. The vast\nliterature in imitation learning mostly considers this reward function to be\navailable for HP selection, but this is not a realistic setting. Indeed, would\nthis reward function be available, it could then directly be used for policy\ntraining and imitation would not be necessary. To tackle this mostly ignored\nproblem, we propose a number of possible proxies to the external reward. We\nevaluate them in an extensive empirical study (more than 10'000 agents across 9\nenvironments) and make practical recommendations for selecting HPs. Our results\nshow that while imitation learning algorithms are sensitive to HP choices, it\nis often possible to select good enough HPs through a proxy to the reward\nfunction.",
          "link": "http://arxiv.org/abs/2105.12034",
          "publishedOn": "2021-05-26T01:22:11.722Z",
          "wordCount": 586,
          "title": "Hyperparameter Selection for Imitation Learning. (arXiv:2105.12034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiangzhu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chonghui Guo</a>",
          "description": "In recent years, multi-view learning technologies for various applications\nhave attracted a surge of interest. Due to more compatible and complementary\ninformation from multiple views, existing multi-view methods could achieve more\npromising performance than conventional single-view methods in most situations.\nHowever, there are still no sufficient researches on the unified framework in\nexisting multi-view works. Meanwhile, how to efficiently integrate multi-view\ninformation is still full of challenges. In this paper, we propose a novel\nmulti-view learning framework, which aims to leverage most existing graph\nembedding works into a unified formula via introducing the graph consensus\nterm. In particular, our method explores the graph structure in each view\nindependently to preserve the diversity property of graph embedding methods.\nMeanwhile, we choose heterogeneous graphs to construct the graph consensus term\nto explore the correlations among multiple views jointly. To this end, the\ndiversity and complementary information among different views could be\nsimultaneously considered. Furthermore, the proposed framework is utilized to\nimplement the multi-view extension of Locality Linear Embedding, named\nMulti-view Locality Linear Embedding (MvLLE), which could be efficiently solved\nby applying the alternating optimization strategy. Empirical validations\nconducted on six benchmark datasets can show the effectiveness of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/2105.11781",
          "publishedOn": "2021-05-26T01:22:11.716Z",
          "wordCount": 625,
          "title": "A unified framework based on graph consensus term for multi-view learning. (arXiv:2105.11781v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying-Jun Angela Zhang</a>",
          "description": "We study over-the-air model aggregation in federated edge learning (FEEL)\nsystems, where channel state information at the transmitters (CSIT) is assumed\nto be unavailable. We leverage the reconfigurable intelligent surface (RIS)\ntechnology to align the cascaded channel coefficients for CSIT-free model\naggregation. To this end, we jointly optimize the RIS and the receiver by\nminimizing the aggregation error under the channel alignment constraint. We\nthen develop a difference-of-convex algorithm for the resulting non-convex\noptimization. Numerical experiments on image classification show that the\nproposed method is able to achieve a similar learning accuracy as the\nstate-of-the-art CSIT-based solution, demonstrating the efficiency of our\napproach in combating the lack of CSIT.",
          "link": "http://arxiv.org/abs/2102.10749",
          "publishedOn": "2021-05-26T01:22:11.710Z",
          "wordCount": 610,
          "title": "CSIT-Free Federated Edge Learning via Reconfigurable Intelligent Surface. (arXiv:2102.10749v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12725",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yadlowsky_S/0/1/0/all/0/1\">Steve Yadlowsky</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yun_T/0/1/0/all/0/1\">Taedong Yun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+McLean_C/0/1/0/all/0/1\">Cory McLean</a>, <a href=\"http://arxiv.org/find/stat/1/au:+DAmour_A/0/1/0/all/0/1\">Alexander D&#x27;Amour</a>",
          "description": "Logistic regression remains one of the most widely used tools in applied\nstatistics, machine learning and data science. However, in moderately\nhigh-dimensional problems, where the number of features $d$ is a non-negligible\nfraction of the sample size $n$, the logistic regression maximum likelihood\nestimator (MLE), and statistical procedures based the large-sample\napproximation of its distribution, behave poorly. Recently, Sur and Cand\\`es\n(2019) showed that these issues can be corrected by applying a new\napproximation of the MLE's sampling distribution in this high-dimensional\nregime. Unfortunately, these corrections are difficult to implement in\npractice, because they require an estimate of the \\emph{signal strength}, which\nis a function of the underlying parameters $\\beta$ of the logistic regression.\nTo address this issue, we propose SLOE, a fast and straightforward approach to\nestimate the signal strength in logistic regression. The key insight of SLOE is\nthat the Sur and Cand\\`es (2019) correction can be reparameterized in terms of\nthe \\emph{corrupted signal strength}, which is only a function of the estimated\nparameters $\\widehat \\beta$. We propose an estimator for this quantity, prove\nthat it is consistent in the relevant high-dimensional regime, and show that\ndimensionality correction using SLOE is accurate in finite samples. Compared to\nthe existing ProbeFrontier heuristic, SLOE is conceptually simpler and orders\nof magnitude faster, making it suitable for routine use. We demonstrate the\nimportance of routine dimensionality correction in the Heart Disease dataset\nfrom the UCI repository, and a genomics application using data from the UK\nBiobank. We provide an open source package for this method, available at\n\\url{https://github.com/google-research/sloe-logistic}.",
          "link": "http://arxiv.org/abs/2103.12725",
          "publishedOn": "2021-05-26T01:22:11.705Z",
          "wordCount": 716,
          "title": "SLOE: A Faster Method for Statistical Inference in High-Dimensional Logistic Regression. (arXiv:2103.12725v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.08787",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Huynh_V/0/1/0/all/0/1\">Viet Huynh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dam_N/0/1/0/all/0/1\">Nhan Dam</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_X/0/1/0/all/0/1\">XuanLong Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yurochkin_M/0/1/0/all/0/1\">Mikhail Yurochkin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bui_H/0/1/0/all/0/1\">Hung Bui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Phung_a/0/1/0/all/0/1\">and Dinh Phung</a>",
          "description": "We propose a novel approach to the problem of multilevel clustering, which\naims to simultaneously partition data in each group and discover grouping\npatterns among groups in a potentially large hierarchically structured corpus\nof data. Our method involves a joint optimization formulation over several\nspaces of discrete probability measures, which are endowed with Wasserstein\ndistance metrics. We propose several variants of this problem, which admit fast\noptimization algorithms, by exploiting the connection to the problem of finding\nWasserstein barycenters. Consistency properties are established for the\nestimates of both local and global clusters. Finally, experimental results with\nboth synthetic and real data are presented to demonstrate the flexibility and\nscalability of the proposed approach.",
          "link": "http://arxiv.org/abs/1909.08787",
          "publishedOn": "2021-05-26T01:22:11.699Z",
          "wordCount": 577,
          "title": "On Efficient Multilevel Clustering via Wasserstein Distances. (arXiv:1909.08787v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.14376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahluwalia_A/0/1/0/all/0/1\">Aalok Ahluwalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Penta_M/0/1/0/all/0/1\">Massimiliano Di Penta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falessi_D/0/1/0/all/0/1\">Davide Falessi</a>",
          "description": "To develop and train defect prediction models, researchers rely on datasets\nin which a defect is attributed to an artifact, e.g., a class of a given\nrelease. However, the creation of such datasets is far from being perfect. It\ncan happen that a defect is discovered several releases after its introduction:\nthis phenomenon has been called \"dormant defects\". This means that, if we\nobserve today the status of a class in its current version, it can be\nconsidered as defect-free while this is not the case. We call \"snoring\" the\nnoise consisting of such classes, affected by dormant defects only. We\nconjecture that the presence of snoring negatively impacts the classifiers'\naccuracy and their evaluation. Moreover, earlier releases likely contain more\nsnoring classes than older releases, thus, removing the most recent releases\nfrom a dataset could reduce the snoring effect and improve the accuracy of\nclassifiers. In this paper we investigate the impact of the snoring noise on\nclassifiers' accuracy and their evaluation, and the effectiveness of a possible\ncountermeasure consisting in removing the last releases of data. We analyze the\naccuracy of 15 machine learning defect prediction classifiers on data from more\nthan 4,000 bugs and 600 releases of 19 open source projects from the Apache\necosystem. Our results show that, on average across projects: (i) the presence\nof snoring decreases the recall of defect prediction classifiers; (ii)\nevaluations affected by snoring are likely unable to identify the best\nclassifiers, and (iii) removing data from recent releases helps to\nsignificantly improve the accuracy of the classifiers. On summary, this paper\nprovides insights on how to create a software defect dataset by mitigating the\neffect of snoring.",
          "link": "http://arxiv.org/abs/2003.14376",
          "publishedOn": "2021-05-26T01:22:11.683Z",
          "wordCount": 763,
          "title": "On the Need of Removing Last Releases of Data When Using or Validating Defect Prediction Models. (arXiv:2003.14376v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sie_S/0/1/0/all/0/1\">Syuan-Hao Sie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jye-Luen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ren Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chih-Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chih-Cheng Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Meng-Fan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kea-Tiong Tang</a>",
          "description": "Convolutional neural networks (CNNs) play a key role in deep learning\napplications. However, the large storage overheads and the substantial\ncomputation cost of CNNs are problematic in hardware accelerators.\nComputing-in-memory (CIM) architecture has demonstrated great potential to\neffectively compute large-scale matrix-vector multiplication. However, the\nintensive multiply and accumulation (MAC) operations executed at the crossbar\narray and the limited capacity of CIM macros remain bottlenecks for further\nimprovement of energy efficiency and throughput. To reduce computation costs,\nnetwork pruning and quantization are two widely studied compression methods to\nshrink the model size. However, most of the model compression algorithms can\nonly be implemented in digital-based CNN accelerators. For implementation in a\nstatic random access memory (SRAM) CIM-based accelerator, the model compression\nalgorithm must consider the hardware limitations of CIM macros, such as the\nnumber of word lines and bit lines that can be turned on at the same time, as\nwell as how to map the weight to the SRAM CIM macro. In this study, a software\nand hardware co-design approach is proposed to design an SRAM CIM-based CNN\naccelerator and an SRAM CIM-aware model compression algorithm. To lessen the\nhigh-precision MAC required by batch normalization (BN), a quantization\nalgorithm that can fuse BN into the weights is proposed. Furthermore, to reduce\nthe number of network parameters, a sparsity algorithm that considers a CIM\narchitecture is proposed. Last, MARS, a CIM-based CNN accelerator that can\nutilize multiple SRAM CIM macros as processing units and support a sparsity\nneural network, is proposed.",
          "link": "http://arxiv.org/abs/2010.12861",
          "publishedOn": "2021-05-26T01:22:11.675Z",
          "wordCount": 734,
          "title": "MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with Co-designed Compressed Neural Networks. (arXiv:2010.12861v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.08721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Simon-Martin Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>",
          "description": "While deep learning strategies achieve outstanding results in computer vision\ntasks, one issue remains: The current strategies rely heavily on a huge amount\nof labeled data. In many real-world problems, it is not feasible to create such\nan amount of labeled training data. Therefore, it is common to incorporate\nunlabeled data into the training process to reach equal results with fewer\nlabels. Due to a lot of concurrent research, it is difficult to keep track of\nrecent developments. In this survey, we provide an overview of often used ideas\nand methods in image classification with fewer labels. We compare 34 methods in\ndetail based on their performance and their commonly used ideas rather than a\nfine-grained taxonomy. In our analysis, we identify three major trends that\nlead to future research opportunities. 1. State-of-the-art methods are\nscaleable to real-world applications in theory but issues like class imbalance,\nrobustness, or fuzzy labels are not considered. 2. The degree of supervision\nwhich is needed to achieve comparable results to the usage of all labels is\ndecreasing and therefore methods need to be extended to settings with a\nvariable number of classes. 3. All methods share some common ideas but we\nidentify clusters of methods that do not share many ideas. We show that\ncombining ideas from different clusters can lead to better performance.",
          "link": "http://arxiv.org/abs/2002.08721",
          "publishedOn": "2021-05-26T01:22:11.670Z",
          "wordCount": 720,
          "title": "A survey on Semi-, Self- and Unsupervised Learning for Image Classification. (arXiv:2002.08721v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dodel_D/0/1/0/all/0/1\">David Dodel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schotz_M/0/1/0/all/0/1\">Michael Sch&#xf6;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vodisch_N/0/1/0/all/0/1\">Niclas V&#xf6;disch</a>",
          "description": "This paper presents the FSOCO dataset, a collaborative dataset for\nvision-based cone detection systems in Formula Student Driverless competitions.\nIt contains human annotated ground truth labels for both bounding boxes and\ninstance-wise segmentation masks. The data buy-in philosophy of FSOCO asks\nstudent teams to contribute to the database first before being granted access\nensuring continuous growth. By providing clear labeling guidelines and tools\nfor a sophisticated raw image selection, new annotations are guaranteed to meet\nthe desired quality. The effectiveness of the approach is shown by comparing\nprediction results of a network trained on FSOCO and its unregulated\npredecessor. The FSOCO dataset can be found at fsoco-dataset.com.",
          "link": "http://arxiv.org/abs/2012.07139",
          "publishedOn": "2021-05-26T01:22:11.663Z",
          "wordCount": 577,
          "title": "FSOCO: The Formula Student Objects in Context Dataset. (arXiv:2012.07139v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Suhas S Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1\">Dheeraj Nagaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1\">Praneeth Netrapalli</a>",
          "description": "We consider the setting of vector valued non-linear dynamical systems\n$X_{t+1} = \\phi(A^* X_t) + \\eta_t$, where $\\eta_t$ is unbiased noise and $\\phi\n: \\mathbb{R} \\to \\mathbb{R}$ is a known link function that satisfies certain\n{\\em expansivity property}. The goal is to learn $A^*$ from a single trajectory\n$X_1,\\cdots,X_T$ of {\\em dependent or correlated} samples. While the problem is\nwell-studied in the linear case, where $\\phi$ is identity, with optimal error\nrates even for non-mixing systems, existing results in the non-linear case hold\nonly for mixing systems. In this work, we improve existing results for learning\nnonlinear systems in a number of ways: a) we provide the first offline\nalgorithm that can learn non-linear dynamical systems without the mixing\nassumption, b) we significantly improve upon the sample complexity of existing\nresults for mixing systems, c) in the much harder one-pass, streaming setting\nwe study a SGD with Reverse Experience Replay ($\\mathsf{SGD-RER}$) method, and\ndemonstrate that for mixing systems, it achieves the same sample complexity as\nour offline algorithm, d) we justify the expansivity assumption by showing that\nfor the popular ReLU link function -- a non-expansive but easy to learn link\nfunction with i.i.d. samples -- any method would require exponentially many\nsamples (with respect to dimension of $X_t$) from the dynamical system. We\nvalidate our results via. simulations and demonstrate that a naive application\nof SGD can be highly sub-optimal. Indeed, our work demonstrates that for\ncorrelated data, specialized methods designed for the dependency structure in\ndata can significantly outperform standard SGD based methods.",
          "link": "http://arxiv.org/abs/2105.11558",
          "publishedOn": "2021-05-26T01:22:11.657Z",
          "wordCount": 696,
          "title": "Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems. (arXiv:2105.11558v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1\">Wen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhihua Tao</a>",
          "description": "Few-shot learning has been proposed and rapidly emerging as a viable means\nfor completing various tasks. Many few-shot models have been widely used for\nrelation learning tasks. However, each of these models has a shortage of\ncapturing a certain aspect of semantic features, for example, CNN on long-range\ndependencies part, Transformer on local features. It is difficult for a single\nmodel to adapt to various relation learning, which results in the high variance\nproblem. Ensemble strategy could be competitive on improving the accuracy of\nfew-shot relation extraction and mitigating high variance risks. This paper\nexplores an ensemble approach to reduce the variance and introduces fine-tuning\nand feature attention strategies to calibrate relation-level features. Results\non several few-shot relation learning tasks show that our model significantly\noutperforms the previous state-of-the-art models.",
          "link": "http://arxiv.org/abs/2105.11904",
          "publishedOn": "2021-05-26T01:22:11.638Z",
          "wordCount": 559,
          "title": "Ensemble Making Few-Shot Learning Stronger. (arXiv:2105.11904v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.10293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sater_R/0/1/0/all/0/1\">Raed Abdel Sater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "Internet of Things (IoT) sensors in smart buildings are becoming increasingly\nubiquitous, making buildings more livable, energy efficient, and sustainable.\nThese devices sense the environment and generate multivariate temporal data of\nparamount importance for detecting anomalies and improving the prediction of\nenergy usage in smart buildings. However, detecting these anomalies in\ncentralized systems is often plagued by a huge delay in response time. To\novercome this issue, we formulate the anomaly detection problem in a federated\nlearning setting by leveraging the multi-task learning paradigm, which aims at\nsolving multiple tasks simultaneously while taking advantage of the\nsimilarities and differences across tasks. We propose a novel privacy-by-design\nfederated learning model using a stacked long short-time memory (LSTM) model,\nand we demonstrate that it is more than twice as fast during training\nconvergence compared to the centralized LSTM. The effectiveness of our\nfederated learning approach is demonstrated on three real-world datasets\ngenerated by the IoT production system at General Electric Current smart\nbuilding, achieving state-of-the-art performance compared to baseline methods\nin both classification and regression tasks. Our experimental results\ndemonstrate the effectiveness of the proposed framework in reducing the overall\ntraining cost without compromising the prediction performance.",
          "link": "http://arxiv.org/abs/2010.10293",
          "publishedOn": "2021-05-26T01:22:11.633Z",
          "wordCount": 649,
          "title": "A Federated Learning Approach to Anomaly Detection in Smart Buildings. (arXiv:2010.10293v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lorch_L/0/1/0/all/0/1\">Lars Lorch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothfuss_J/0/1/0/all/0/1\">Jonas Rothfuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "Bayesian structure learning allows inferring Bayesian network structure from\ndata while reasoning about the epistemic uncertainty -- a key element towards\nenabling active causal discovery and designing interventions in real world\nsystems. In this work, we propose a general, fully differentiable framework for\nBayesian structure learning (DiBS) that operates in the continuous space of a\nlatent probabilistic graph representation. Building on recent advances in\nvariational inference, we use DiBS to devise an efficient method for\napproximating posteriors over structural models. Contrary to existing work,\nDiBS is agnostic to the form of the local conditional distributions and allows\nfor joint posterior inference of both the graph structure and the conditional\ndistribution parameters. This makes our method directly applicable to posterior\ninference of nonstandard Bayesian network models, e.g., with nonlinear\ndependencies encoded by neural networks. In evaluations on simulated and\nreal-world data, DiBS significantly outperforms related approaches to joint\nposterior inference.",
          "link": "http://arxiv.org/abs/2105.11839",
          "publishedOn": "2021-05-26T01:22:11.627Z",
          "wordCount": 572,
          "title": "DiBS: Differentiable Bayesian Structure Learning. (arXiv:2105.11839v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11748",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Weiyi Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1\">Colin Jacobs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>",
          "description": "Automatic lesion segmentation on thoracic CT enables rapid quantitative\nanalysis of lung involvement in COVID- 19 infections. Obtaining voxel-level\nannotations for training segmentation networks is prohibitively expensive.\nTherefore we propose a weakly-supervised segmentation method based on dense\nregression activation maps (dRAM). Most advanced weakly supervised segmentation\napproaches exploit class activation maps (CAMs) to localize objects generated\nfrom high-level semantic features at a coarse resolution. As a result, CAMs\nprovide coarse outlines that do not align precisely with the object\nsegmentations. Instead, we exploit dense features from a segmentation network\nto compute dense regression activation maps (dRAMs) for preserving local\ndetails. During training, dRAMs are pooled lobe-wise to regress the per-lobe\nlesion percentage. In such a way, the network achieves additional information\nregarding the lesion quantification in comparison with the classification\napproach. Furthermore, we refine dRAMs based on an attention module and dense\nconditional random field trained together with the main regression task. The\nrefined dRAMs are served as the pseudo labels for training a final segmentation\nnetwork. When evaluated on 69 CT scans, our method substantially improves the\nintersection over union from 0.335 in the CAM-based weakly supervised\nsegmentation method to 0.495.",
          "link": "http://arxiv.org/abs/2105.11748",
          "publishedOn": "2021-05-26T01:22:11.621Z",
          "wordCount": 688,
          "title": "Dense Regression Activation Maps For Lesion Segmentation in CT scans of COVID-19 patients. (arXiv:2105.11748v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>",
          "description": "Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.11866",
          "publishedOn": "2021-05-26T01:22:11.615Z",
          "wordCount": 580,
          "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11802",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1\">Johannes Kirschner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "We consider Bayesian optimization in settings where observations can be\nadversarially biased, for example by an uncontrolled hidden confounder. Our\nfirst contribution is a reduction of the confounded setting to the dueling\nbandit model. Then we propose a novel approach for dueling bandits based on\ninformation-directed sampling (IDS). Thereby, we obtain the first efficient\nkernelized algorithm for dueling bandits that comes with cumulative regret\nguarantees. Our analysis further generalizes a previously proposed\nsemi-parametric linear bandit model to non-linear reward functions, and\nuncovers interesting links to doubly-robust estimation.",
          "link": "http://arxiv.org/abs/2105.11802",
          "publishedOn": "2021-05-26T01:22:11.602Z",
          "wordCount": 507,
          "title": "Bias-Robust Bayesian Optimization via Dueling Bandit. (arXiv:2105.11802v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1908.07009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_I/0/1/0/all/0/1\">Ivan Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuesta_Infante_A/0/1/0/all/0/1\">Alfredo Cuesta-Infante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1\">Kalyan Veeramachaneni</a>",
          "description": "In many real life situations, including job and loan applications,\ngatekeepers must make justified and fair real-time decisions about a person's\nfitness for a particular opportunity. In this paper, we aim to accomplish\napproximate group fairness in an online stochastic decision-making process,\nwhere the fairness metric we consider is equalized odds. Our work follows from\nthe classical learning-from-experts scheme, assuming a finite set of\nclassifiers (human experts, rules, options, etc) that cannot be modified. We\nrun separate instances of the algorithm for each label class as well as\nsensitive groups, where the probability of choosing each instance is optimized\nfor both fairness and regret. Our theoretical results show that approximately\nequalized odds can be achieved without sacrificing much regret. We also\ndemonstrate the performance of the algorithm on real data sets commonly used by\nthe fairness community.",
          "link": "http://arxiv.org/abs/1908.07009",
          "publishedOn": "2021-05-26T01:22:11.597Z",
          "wordCount": 618,
          "title": "Towards Reducing Biases in Combining Multiple Experts Online. (arXiv:1908.07009v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Maoguo Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1\">A. K. Qin</a>",
          "description": "Multi-party learning provides solutions for training joint models with\ndecentralized data under legal and practical constraints. However, traditional\nmulti-party learning approaches are confronted with obstacles such as system\nheterogeneity, statistical heterogeneity, and incentive design. How to deal\nwith these challenges and further improve the efficiency and performance of\nmulti-party learning has become an urgent problem to be solved. In this paper,\nwe propose a novel contrastive multi-party learning framework for knowledge\nrefinement and sharing with an accountable incentive mechanism. Since the\nexisting naive model parameter averaging method is contradictory to the\nlearning paradigm of neural networks, we simulate the process of human\ncognition and communication, and analogy multi-party learning as a many-to-one\nknowledge sharing problem. The approach is capable of integrating the acquired\nexplicit knowledge of each client in a transparent manner without privacy\ndisclosure, and it reduces the dependence on data distribution and\ncommunication environments. The proposed scheme achieves significant\nimprovement in model performance in a variety of scenarios, as we demonstrated\nthrough experiments on several real-world datasets.",
          "link": "http://arxiv.org/abs/2104.06670",
          "publishedOn": "2021-05-26T01:22:11.590Z",
          "wordCount": 634,
          "title": "Towards Explainable Multi-Party Learning: A Contrastive Knowledge Sharing Framework. (arXiv:2104.06670v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Liyao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xinyue Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinazzi_M/0/1/0/all/0/1\">Matteo Chinazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vespignani_A/0/1/0/all/0/1\">Alessandro Vespignani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi-An Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Rose Yu</a>",
          "description": "Deep learning is gaining increasing popularity for spatiotemporal\nforecasting. However, prior works have mostly focused on point estimates\nwithout quantifying the uncertainty of the predictions. In high stakes domains,\nbeing able to generate probabilistic forecasts with confidence intervals is\ncritical to risk assessment and decision making. Hence, a systematic study of\nuncertainty quantification (UQ) methods for spatiotemporal forecasting is\nmissing in the community. In this paper, we describe two types of\nspatiotemporal forecasting problems: regular grid-based and graph-based. Then\nwe analyze UQ methods from both the Bayesian and the frequentist point of view,\ncasting in a unified framework via statistical decision theory. Through\nextensive experiments on real-world road network traffic, epidemics, and air\nquality forecasting tasks, we reveal the statistical and computational\ntrade-offs for different UQ methods: Bayesian methods are typically more robust\nin mean prediction, while confidence levels obtained from frequentist methods\nprovide more extensive coverage over data variations. Computationally, quantile\nregression type methods are cheaper for a single confidence interval but\nrequire re-training for different intervals. Sampling based methods generate\nsamples that can form multiple confidence intervals, albeit at a higher\ncomputational cost.",
          "link": "http://arxiv.org/abs/2105.11982",
          "publishedOn": "2021-05-26T01:22:11.576Z",
          "wordCount": 631,
          "title": "Quantifying Uncertainty in Deep Spatiotemporal Forecasting. (arXiv:2105.11982v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sen_N/0/1/0/all/0/1\">Nirmalya Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahidullah_M/0/1/0/all/0/1\">Md Sahidullah</a> (MULTISPEECH), <a href=\"http://arxiv.org/find/cs/1/au:+Patil_H/0/1/0/all/0/1\">Hemant Patil</a> (DA-IICT), <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1\">Shyamal Kumar das Mandal</a> (IIT Kharagpur), <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Sreenivasa Krothapalli Rao</a> (IIT Kharagpur), <a href=\"http://arxiv.org/find/cs/1/au:+Basu_T/0/1/0/all/0/1\">Tapan Kumar Basu</a> (IIT Kharagpur)",
          "description": "The performance of speaker recognition system is highly dependent on the\namount of speech used in enrollment and test. This work presents a detailed\nexperimental review and analysis of the GMM-SVM based speaker recognition\nsystem in presence of duration variability. This article also reports a\ncomparison of the performance of GMM-SVM classifier with its precursor\ntechnique Gaussian mixture model-universal background model (GMM-UBM)\nclassifier in presence of duration variability. The goal of this research work\nis not to propose a new algorithm for improving speaker recognition performance\nin presence of duration variability. However, the main focus of this work is on\nutterance partitioning (UP), a commonly used strategy to compensate the\nduration variability issue. We have analysed in detailed the impact of training\nutterance partitioning in speaker recognition performance under GMM-SVM\nframework. We further investigate the reason why the utterance partitioning is\nimportant for boosting speaker recognition performance. We have also shown in\nwhich case the utterance partitioning could be useful and where not. Our study\nhas revealed that utterance partitioning does not reduce the data imbalance\nproblem of the GMM-SVM classifier as claimed in earlier study. Apart from\nthese, we also discuss issues related to the impact of parameters such as\nnumber of Gaussians, supervector length, amount of splitting required for\nobtaining better performance in short and long duration test conditions from\nspeech duration perspective. We have performed the experiments with telephone\nspeech from POLYCOST corpus consisting of 130 speakers.",
          "link": "http://arxiv.org/abs/2105.11728",
          "publishedOn": "2021-05-26T01:22:11.563Z",
          "wordCount": 713,
          "title": "Utterance partitioning for speaker recognition: an experimental review and analysis with new findings under GMM-SVM framework. (arXiv:2105.11728v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weihong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qifang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhuoyao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1\">Qin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1\">Qiang Huo</a>",
          "description": "Recent grid-based document representations like BERTgrid allow the\nsimultaneous encoding of the textual and layout information of a document in a\n2D feature map so that state-of-the-art image segmentation and/or object\ndetection models can be straightforwardly leveraged to extract key information\nfrom documents. However, such methods have not achieved comparable performance\nto state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK\nyet. In this paper, we propose a new multi-modal backbone network by\nconcatenating a BERTgrid to an intermediate layer of a CNN model, where the\ninput of CNN is a document image and the BERTgrid is a grid of word embeddings,\nto generate a more powerful grid-based document representation, named\nViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal\nbackbone network are trained jointly. Our experimental results demonstrate that\nthis joint training strategy improves significantly the representation ability\nof ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction\napproach has achieved state-of-the-art performance on real-world datasets.",
          "link": "http://arxiv.org/abs/2105.11672",
          "publishedOn": "2021-05-26T01:22:11.550Z",
          "wordCount": 620,
          "title": "ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for Key Information Extraction from Documents. (arXiv:2105.11672v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12022",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Vreugdenhil_R/0/1/0/all/0/1\">Robbie Vreugdenhil</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_V/0/1/0/all/0/1\">Viet Anh Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Eftekhari_A/0/1/0/all/0/1\">Armin Eftekhari</a>, <a href=\"http://arxiv.org/find/math/1/au:+Esfahani_P/0/1/0/all/0/1\">Peyman Mohajerin Esfahani</a>",
          "description": "We propose a novel approximation hierarchy for cardinality-constrained,\nconvex quadratic programs that exploits the rank-dominating eigenvectors of the\nquadratic matrix. Each level of approximation admits a min-max characterization\nwhose objective function can be optimized over the binary variables\nanalytically, while preserving convexity in the continuous variables.\nExploiting this property, we propose two scalable optimization algorithms,\ncoined as the \"best response\" and the \"dual program\", that can efficiently\nscreen the potential indices of the nonzero elements of the original program.\nWe show that the proposed methods are competitive with the existing screening\nmethods in the current sparse regression literature, and it is particularly\nfast on instances with high number of measurements in experiments with both\nsynthetic and real datasets.",
          "link": "http://arxiv.org/abs/2105.12022",
          "publishedOn": "2021-05-26T01:22:11.543Z",
          "wordCount": 556,
          "title": "Principal Component Hierarchy for Sparse Quadratic Programs. (arXiv:2105.12022v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11964",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hellkvist_M/0/1/0/all/0/1\">Martin Hellkvist</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozcelikkale_A/0/1/0/all/0/1\">Ay&#xe7;a &#xd6;z&#xe7;elikkale</a>",
          "description": "We consider a linear minimum mean squared error (LMMSE) estimation framework\nwith model mismatch where the assumed model order is smaller than that of the\nunderlying linear system which generates the data used in the estimation\nprocess. By modelling the regressors of the underlying system as random\nvariables, we analyze the average behaviour of the mean squared error (MSE).\nOur results quantify how the MSE depends on the interplay between the number of\nsamples and the number of parameters in the underlying system and in the\nassumed model. In particular, if the number of samples is not sufficiently\nlarge, neither increasing the number of samples nor the assumed model\ncomplexity is sufficient to guarantee a performance improvement.",
          "link": "http://arxiv.org/abs/2105.11964",
          "publishedOn": "2021-05-26T01:22:11.527Z",
          "wordCount": 546,
          "title": "Model Mismatch Trade-offs in LMMSE Estimation. (arXiv:2105.11964v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15106",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bellot_A/0/1/0/all/0/1\">Alexis Bellot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "Unobserved confounding is one of the greatest challenges for causal\ndiscovery. The case in which unobserved variables have a widespread effect on\nmany of the observed ones is particularly difficult because most pairs of\nvariables are conditionally dependent given any other subset, rendering the\ncausal effect unidentifiable. In this paper we show that beyond conditional\nindependencies, under the principle of independent mechanisms, unobserved\nconfounding in this setting leaves a statistical footprint in the observed data\ndistribution that allows for disentangling spurious and causal effects. Using\nthis insight, we demonstrate that a sparse linear Gaussian directed acyclic\ngraph among observed variables may be recovered approximately and propose an\nadjusted score-based causal discovery algorithm that may be implemented with\ngeneral purpose solvers and scales to high-dimensional problems. We find, in\naddition, that despite the conditions we pose to guarantee causal recovery,\nperformance in practice is robust to large deviations in model assumptions.",
          "link": "http://arxiv.org/abs/2103.15106",
          "publishedOn": "2021-05-26T01:22:11.463Z",
          "wordCount": null,
          "title": "Deconfounded Score Method: Scoring DAGs with Dense Unobserved Confounding. (arXiv:2103.15106v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12018",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Arganda_E/0/1/0/all/0/1\">Ernesto Arganda</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Medina_A/0/1/0/all/0/1\">Anibal D. Medina</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Perez_A/0/1/0/all/0/1\">Andres D. Perez</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Szynkman_A/0/1/0/all/0/1\">Alejandro Szynkman</a>",
          "description": "We study several simplified dark matter (DM) models and their signatures at\nthe LHC using neural networks. We focus on the usual monojet plus missing\ntransverse energy channel, but to train the algorithms we organize the data in\n2D histograms instead of event-by-event arrays. This results in a large\nperformance boost to distinguish between standard model (SM) only and SM plus\nnew physics signals. We use the kinematic monojet features as input data which\nallow us to describe families of models with a single data sample. We found\nthat the neural network performance does not depend on the simulated number of\nbackground events if they are presented as a function of $S/\\sqrt{B}$, where\n$S$ and $B$ are the number of signal and background events per histogram,\nrespectively. This provides flexibility to the method, since testing a\nparticular model in that case only requires knowing the new physics monojet\ncross section. Furthermore, we also discuss the network performance under\nincorrect assumptions about the true DM nature. Finally, we propose multimodel\nclassifiers to search and identify new signals in a more general way, for the\nnext LHC run.",
          "link": "http://arxiv.org/abs/2105.12018",
          "publishedOn": "2021-05-26T01:22:11.460Z",
          "wordCount": null,
          "title": "Towards a method to anticipate dark matter signals with deep learning at the LHC. (arXiv:2105.12018v1 [hep-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_N/0/1/0/all/0/1\">Nelly Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaghloul_Z/0/1/0/all/0/1\">Zaghloul Saad Zaghloul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azumah_S/0/1/0/all/0/1\">Sylvia Worlali Azumah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengcheng Li</a>",
          "description": "Internet of Things (IoT) allowed smart homes to improve the quality and the\ncomfort of our daily lives. However, these conveniences introduced several\nsecurity concerns that increase rapidly. IoT devices, smart home hubs, and\ngateway raise various security risks. The smart home gateways act as a\ncentralized point of communication between the IoT devices, which can create a\nbackdoor into network data for hackers. One of the common and effective ways to\ndetect such attacks is intrusion detection in the network traffic. In this\npaper, we proposed an intrusion detection system (IDS) to detect anomalies in a\nsmart home network using a bidirectional long short-term memory (BiLSTM) and\nconvolutional neural network (CNN) hybrid model. The BiLSTM recurrent behavior\nprovides the intrusion detection model to preserve the learned information\nthrough time, and the CNN extracts perfectly the data features. The proposed\nmodel can be applied to any smart home network gateway.",
          "link": "http://arxiv.org/abs/2105.12096",
          "publishedOn": "2021-05-26T01:22:11.452Z",
          "wordCount": null,
          "title": "Intrusion Detection System in Smart Home Network Using Bidirectional LSTM and Convolutional Neural Networks Hybrid Model. (arXiv:2105.12096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kosmider_M/0/1/0/all/0/1\">Micha&#x142; Ko&#x15b;mider</a>",
          "description": "Machine learning algorithms, when trained on audio recordings from a limited\nset of devices, may not generalize well to samples recorded using other devices\nwith different frequency responses. In this work, a relatively straightforward\nmethod is introduced to address this problem. Two variants of the approach are\npresented. First requires aligned examples from multiple devices, the second\napproach alleviates this requirement. This method works for both time and\nfrequency domain representations of audio recordings. Further, a relation to\nstandardization and Cepstral Mean Subtraction is analysed. The proposed\napproach becomes effective even when very few examples are provided. This\nmethod was developed during the Detection and Classification of Acoustic Scenes\nand Events (DCASE) 2019 challenge and won the 1st place in the scenario with\nmis-matched recording devices with the accuracy of 75%. Source code for the\nexperiments can be found online.",
          "link": "http://arxiv.org/abs/2105.11856",
          "publishedOn": "2021-05-26T01:22:11.433Z",
          "wordCount": null,
          "title": "Spectrum Correction: Acoustic Scene Classification with Mismatched Recording Devices. (arXiv:2105.11856v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11724",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Benard_C/0/1/0/all/0/1\">Cl&#xe9;ment B&#xe9;nard</a> (LPSM), <a href=\"http://arxiv.org/find/stat/1/au:+Biau_G/0/1/0/all/0/1\">G&#xe9;rard Biau</a> (LPSM), <a href=\"http://arxiv.org/find/stat/1/au:+Veiga_S/0/1/0/all/0/1\">S&#xe9;bastien da Veiga</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scornet_E/0/1/0/all/0/1\">Erwan Scornet</a> (CMAP)",
          "description": "Interpretability of learning algorithms is crucial for applications involving\ncritical decisions, and variable importance is one of the main interpretation\ntools. Shapley effects are now widely used to interpret both tree ensembles and\nneural networks, as they can efficiently handle dependence and interactions in\nthe data, as opposed to most other variable importance measures. However,\nestimating Shapley effects is a challenging task, because of the computational\ncomplexity and the conditional expectation estimates. Accordingly, existing\nShapley algorithms have flaws: a costly running time, or a bias when input\nvariables are dependent. Therefore, we introduce SHAFF, SHApley eFfects via\nrandom Forests, a fast and accurate Shapley effect estimate, even when input\nvariables are dependent. We show SHAFF efficiency through both a theoretical\nanalysis of its consistency, and the practical performance improvements over\ncompetitors with extensive experiments. An implementation of SHAFF in C++ and R\nis available online.",
          "link": "http://arxiv.org/abs/2105.11724",
          "publishedOn": "2021-05-26T01:22:11.429Z",
          "wordCount": null,
          "title": "SHAFF: Fast and consistent SHApley eFfect estimates via random Forests. (arXiv:2105.11724v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vahidi_C/0/1/0/all/0/1\">Cyrus Vahidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "Convolutional Neural Networks have been extensively explored in the task of\nautomatic music tagging. The problem can be approached by using either\nengineered time-frequency features or raw audio as input. Modulation filter\nbank representations that have been actively researched as a basis for timbre\nperception have the potential to facilitate the extraction of perceptually\nsalient features. We explore end-to-end learned front-ends for audio\nrepresentation learning, ModNet and SincModNet, that incorporate a temporal\nmodulation processing block. The structure is effectively analogous to a\nmodulation filter bank, where the FIR filter center frequencies are learned in\na data-driven manner. The expectation is that a perceptually motivated filter\nbank can provide a useful representation for identifying music features. Our\nexperimental results provide a fully visualisable and interpretable front-end\ntemporal modulation decomposition of raw audio. We evaluate the performance of\nour model against the state-of-the-art of music tagging on the MagnaTagATune\ndataset. We analyse the impact on performance for particular tags when\ntime-frequency bands are subsampled by the modulation filters at a\nprogressively reduced rate. We demonstrate that modulation filtering provides\npromising results for music tagging and feature representation, without using\nextensive musical domain knowledge in the design of this front-end.",
          "link": "http://arxiv.org/abs/2105.11836",
          "publishedOn": "2021-05-26T01:22:11.402Z",
          "wordCount": null,
          "title": "A Modulation Front-End for Music Audio Tagging. (arXiv:2105.11836v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pitombeira_Neto_A/0/1/0/all/0/1\">Anselmo R. Pitombeira-Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_H/0/1/0/all/0/1\">Helano P. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_T/0/1/0/all/0/1\">Ticiana L. Coelho da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macedo_J/0/1/0/all/0/1\">Jos&#xe9; Antonio F. de Macedo</a>",
          "description": "We consider the problem of modeling trajectories of drivers in a road network\nfrom the perspective of inverse reinforcement learning. As rational agents,\ndrivers are trying to maximize some reward function unknown to an external\nobserver as they make up their trajectories. We apply the concept of random\nutility from microeconomic theory to model the unknown reward function as a\nfunction of observable features plus an error term which represents features\nknown only to the driver. We develop a parameterized generative model for the\ntrajectories based on a random utility Markov decision process formulation of\ndrivers decisions. We show that maximum entropy inverse reinforcement learning\nis a particular case of our proposed formulation when we assume a Gumbel\ndensity function for the unobserved reward error terms. We illustrate Bayesian\ninference on model parameters through a case study with real trajectory data\nfrom a large city obtained from sensors placed on sparsely distributed points\non the street network.",
          "link": "http://arxiv.org/abs/2105.12092",
          "publishedOn": "2021-05-26T01:22:11.397Z",
          "wordCount": null,
          "title": "Trajectory Modeling via Random Utility Inverse Reinforcement Learning. (arXiv:2105.12092v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minshuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>",
          "description": "The Lottery Ticket Hypothesis suggests that an over-parametrized network\nconsists of \"lottery tickets\", and training a certain collection of them (i.e.,\na subnetwork) can match the performance of the full model. In this paper, we\nstudy such a collection of tickets, which is referred to as \"winning tickets\",\nin extremely over-parametrized models, e.g., pre-trained language models. We\nobserve that at certain compression ratios, generalization performance of the\nwinning tickets can not only match, but also exceed that of the full model. In\nparticular, we observe a phase transition phenomenon: As the compression ratio\nincreases, generalization performance of the winning tickets first improves\nthen deteriorates after a certain threshold. We refer to the tickets on the\nthreshold as \"super tickets\". We further show that the phase transition is task\nand model dependent -- as model size becomes larger and training data set\nbecomes smaller, the transition becomes more pronounced. Our experiments on the\nGLUE benchmark show that the super tickets improve single task fine-tuning by\n$0.9$ points on BERT-base and $1.0$ points on BERT-large, in terms of\ntask-average score. We also demonstrate that adaptively sharing the super\ntickets across tasks benefits multi-task learning.",
          "link": "http://arxiv.org/abs/2105.12002",
          "publishedOn": "2021-05-26T01:22:11.317Z",
          "wordCount": null,
          "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization. (arXiv:2105.12002v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.00381",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dalmaijer_E/0/1/0/all/0/1\">E. S. Dalmaijer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nord_C/0/1/0/all/0/1\">C. L. Nord</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Astle_D/0/1/0/all/0/1\">D. E. Astle</a>",
          "description": "Cluster algorithms are increasingly popular in biomedical research due to\ntheir compelling ability to identify discrete subgroups in data, and their\nincreasing accessibility in mainstream software. While guidelines exist for\nalgorithm selection and outcome evaluation, there are no firmly established\nways of computing a priori statistical power for cluster analysis. Here, we\nestimated power and accuracy for common analysis pipelines through simulation.\nWe varied subgroup size, number, separation (effect size), and covariance\nstructure. We then subjected generated datasets to dimensionality reduction\n(none, multidimensional scaling, or UMAP) and cluster algorithms (k-means,\nagglomerative hierarchical clustering with Ward or average linkage and\nEuclidean or cosine distance, HDBSCAN). Finally, we compared the statistical\npower of discrete (k-means), \"fuzzy\" (c-means), and finite mixture modelling\napproaches (which include latent profile and latent class analysis). We found\nthat outcomes were driven by large effect sizes or the accumulation of many\nsmaller effects across features, and were unaffected by differences in\ncovariance structure. Sufficient statistical power was achieved with relatively\nsmall samples (N=20 per subgroup), provided cluster separation is large\n({\\Delta}=4). Fuzzy clustering provided a more parsimonious and powerful\nalternative for identifying separable multivariate normal distributions,\nparticularly those with slightly lower centroid separation ({\\Delta}=3).\nOverall, we recommend that researchers 1) only apply cluster analysis when\nlarge subgroup separation is expected, 2) aim for sample sizes of N=20 to N=30\nper expected subgroup, 3) use multidimensional scaling to improve cluster\nseparation, and 4) use fuzzy clustering or finite mixture modelling approaches\nthat are more powerful and more parsimonious with partially overlapping\nmultivariate normal distributions.",
          "link": "http://arxiv.org/abs/2003.00381",
          "publishedOn": "2021-05-26T01:22:11.307Z",
          "wordCount": null,
          "title": "Statistical power for cluster analysis. (arXiv:2003.00381v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Rushabh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>",
          "description": "Background The learning of genotype-phenotype associations and history of\nhuman disease by doing detailed and precise analysis of phenotypic\nabnormalities can be defined as deep phenotyping. To understand and detect this\ninteraction between phenotype and genotype is a fundamental step when\ntranslating precision medicine to clinical practice. The recent advances in the\nfield of machine learning is efficient to predict these interactions between\nabnormal human phenotypes and genes.\n\nMethods In this study, we developed a framework to predict links between\nhuman phenotype ontology (HPO) and genes. The annotation data from the\nheterogeneous knowledge resources i.e., orphanet, is used to parse human\nphenotype-gene associations. To generate the embeddings for the nodes (HPO &\ngenes), an algorithm called node2vec was used. It performs node sampling on\nthis graph based on random walks, then learns features over these sampled nodes\nto generate embeddings. These embeddings were used to perform the downstream\ntask to predict the presence of the link between these nodes using 5 different\nsupervised machine learning algorithms.\n\nResults: The downstream link prediction task shows that the Gradient Boosting\nDecision Tree based model (LightGBM) achieved an optimal AUROC 0.904 and AUCPR\n0.784. In addition, LightGBM achieved an optimal weighted F1 score of 0.87.\nCompared to the other 4 methods LightGBM is able to find more accurate\ninteraction/link between human phenotype & gene pairs.",
          "link": "http://arxiv.org/abs/2105.11989",
          "publishedOn": "2021-05-26T01:22:11.306Z",
          "wordCount": null,
          "title": "Graph Based Link Prediction between Human Phenotypes and Genes. (arXiv:2105.11989v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mosser_L/0/1/0/all/0/1\">Lukas Mosser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeini_E/0/1/0/all/0/1\">Ehsan Zabihi Naeini</a>",
          "description": "Deep neural networks offer numerous potential applications across geoscience,\nfor example, one could argue that they are the state-of-the-art method for\npredicting faults in seismic datasets. In quantitative reservoir\ncharacterization workflows, it is common to incorporate the uncertainty of\npredictions thus such subsurface models should provide calibrated probabilities\nand the associated uncertainties in their predictions. It has been shown that\npopular Deep Learning-based models are often miscalibrated, and due to their\ndeterministic nature, provide no means to interpret the uncertainty of their\npredictions. We compare three different approaches to obtaining probabilistic\nmodels based on convolutional neural networks in a Bayesian formalism, namely\nDeep Ensembles, Concrete Dropout, and Stochastic Weight Averaging-Gaussian\n(SWAG). These methods are consistently applied to fault detection case studies\nwhere Deep Ensembles use independently trained models to provide fault\nprobabilities, Concrete Dropout represents an extension to the popular Dropout\ntechnique to approximate Bayesian neural networks, and finally, we apply SWAG,\na recent method that is based on the Bayesian inference equivalence of\nmini-batch Stochastic Gradient Descent. We provide quantitative results in\nterms of model calibration and uncertainty representation, as well as\nqualitative results on synthetic and real seismic datasets. Our results show\nthat the approximate Bayesian methods, Concrete Dropout and SWAG, both provide\nwell-calibrated predictions and uncertainty attributes at a lower computational\ncost when compared to the baseline Deep Ensemble approach. The resulting\nuncertainties also offer a possibility to further improve the model performance\nas well as enhancing the interpretability of the models.",
          "link": "http://arxiv.org/abs/2105.12115",
          "publishedOn": "2021-05-26T01:22:11.300Z",
          "wordCount": null,
          "title": "Calibration and Uncertainty Quantification of Bayesian Convolutional Neural Networks for Geophysical Applications. (arXiv:2105.12115v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blakseth_S/0/1/0/all/0/1\">Sindre Stenen Blakseth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_A/0/1/0/all/0/1\">Adil Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kvamsdal_T/0/1/0/all/0/1\">Trond Kvamsdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+San_O/0/1/0/all/0/1\">Omer San</a>",
          "description": "Hybrid Analysis and Modeling (HAM) is an emerging modeling paradigm which\naims to combine physics-based modeling (PBM) and data-driven modeling (DDM) to\ncreate generalizable, trustworthy, accurate, computationally efficient and\nself-evolving models. Here, we introduce, justify and demonstrate a novel\napproach to HAM -- the Corrective Source Term Approach (CoSTA) -- which\naugments the governing equation of a PBM model with a corrective source term\ngenerated by a deep neural network (DNN). In a series of numerical experiments\non one-dimensional heat diffusion, CoSTA is generally found to outperform\ncomparable DDM and PBM models in terms of accuracy -- often reducing predictive\nerrors by several orders of magnitude -- while also generalizing better than\npure DDM. Due to its flexible but solid theoretical foundation, CoSTA provides\na modular framework for leveraging novel developments within both PBM and DDM,\nand due to the interpretability of the DNN-generated source term within the PBM\nparadigm, CoSTA can be a potential door-opener for data-driven techniques to\nenter high-stakes applications previously reserved for pure PBM.",
          "link": "http://arxiv.org/abs/2105.11521",
          "publishedOn": "2021-05-26T01:22:11.291Z",
          "wordCount": null,
          "title": "Deep neural network enabled corrective source term approach to hybrid analysis and modeling. (arXiv:2105.11521v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/1908.05569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tsang Ing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Adriano L. I. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Out-of-distribution (OOD) detection approaches usually present special\nrequirements (e.g., hyperparameter validation, collection of outlier data) and\nproduce side effects (e.g., classification accuracy drop, slower\nenergy-inefficient inferences). We argue that these issues are a consequence of\nthe SoftMax loss anisotropy and disagreement with the maximum entropy\nprinciple. Thus, we propose the IsoMax loss and the entropic score. The\nseamless drop-in replacement of the SoftMax loss by IsoMax loss requires\nneither additional data collection nor hyperparameter validation. The trained\nmodels do not exhibit classification accuracy drop and produce fast\nenergy-efficient inferences. Moreover, our experiments show that training\nneural networks with IsoMax loss significantly improves their OOD detection\nperformance. The IsoMax loss exhibits state-of-the-art performance under the\nmentioned conditions (fast energy-efficient inference, no classification\naccuracy drop, no collection of outlier data, and no hyperparameter\nvalidation), which we call the seamless OOD detection task. In future work,\ncurrent OOD detection methods may replace the SoftMax loss with the IsoMax loss\nto improve their performance on the commonly studied non-seamless OOD detection\nproblem.",
          "link": "http://arxiv.org/abs/1908.05569",
          "publishedOn": "2021-05-26T01:22:11.291Z",
          "wordCount": null,
          "title": "Entropic Out-of-Distribution Detection. (arXiv:1908.05569v13 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunshan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yujuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Wai Keung Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jinyoung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>",
          "description": "This companion paper supports the replication of the fashion trend\nforecasting experiments with the KERN (Knowledge Enhanced Recurrent Network)\nmethod that we presented in the ICMR 2020. We provide an artifact that allows\nthe replication of the experiments using a Python implementation. The artifact\nis easy to deploy with simple installation, training and evaluation. We\nreproduce the experiments conducted in the original paper and obtain similar\nperformance as previously reported. The replication results of the experiments\nsupport the main claims in the original paper.",
          "link": "http://arxiv.org/abs/2105.11826",
          "publishedOn": "2021-05-26T01:22:11.290Z",
          "wordCount": null,
          "title": "Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend Forecasting. (arXiv:2105.11826v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panknin_D/0/1/0/all/0/1\">Danny Panknin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shinichi Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus Robert M&#xfc;ller</a>",
          "description": "We propose a novel active learning strategy for regression, which is\nmodel-agnostic, robust against model mismatch, and interpretable. Assuming that\na small number of initial samples are available, we derive the optimal training\ndensity that minimizes the generalization error of local polynomial smoothing\n(LPS) with its kernel bandwidth tuned locally: We adopt the mean integrated\nsquared error (MISE) as a generalization criterion, and use the asymptotic\nbehavior of the MISE as well as thelocally optimal bandwidths (LOB) -- the\nbandwidth function that minimizes MISE in the asymptotic limit. The asymptotic\nexpression of our objective then reveals the dependence of the MISE on the\ntraining density, enabling analytic minimization. As a result, we obtain the\noptimal training density in a closed-form. The almost model-free nature of our\napproach should encode raw properties of the target problem, and thus provide a\nrobust and model-agnostic active learning strategy. Furthermore, the obtained\ntraining density factorizes the influence of local function complexity, noise\nleveland test density in a transparent and interpretable way. We validate our\ntheory in numerical simulations, and show that the proposed active learning\nmethod outperforms the existing state-of-the-art model-agnostic approaches.",
          "link": "http://arxiv.org/abs/2105.11990",
          "publishedOn": "2021-05-26T01:22:11.290Z",
          "wordCount": null,
          "title": "Optimal Sampling Density for Nonparametric Regression. (arXiv:2105.11990v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poorheravi_P/0/1/0/all/0/1\">Parisa Abdolrahim Poorheravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaudet_V/0/1/0/all/0/1\">Vincent Gaudet</a>",
          "description": "Manifold learning is used for dimensionality reduction, with the goal of\nfinding a projection subspace to increase and decrease the inter- and\nintraclass variances, respectively. However, a bottleneck for subspace learning\nmethods often arises from the high dimensionality of datasets. In this paper, a\nhierarchical approach is proposed to scale subspace learning methods, with the\ngoal of improving classification in large datasets by a range of 3% to 10%.\nDifferent combinations of methods are studied. We assess the proposed method on\nfive publicly available large datasets, for different eigen-value based\nsubspace learning methods such as linear discriminant analysis, principal\ncomponent analysis, generalized discriminant analysis, and reconstruction\nindependent component analysis. To further examine the effect of the proposed\nmethod on various classification methods, we fed the generated result to linear\ndiscriminant analysis, quadratic linear analysis, k-nearest neighbor, and\nrandom forest classifiers. The resulting classification accuracies are compared\nto show the effectiveness of the hierarchical approach, reporting results of an\naverage of 5% increase in classification accuracy.",
          "link": "http://arxiv.org/abs/2105.12005",
          "publishedOn": "2021-05-26T01:22:11.290Z",
          "wordCount": null,
          "title": "Hierarchical Subspace Learning for Dimensionality Reduction to Improve Classification Accuracy in Large Data Sets. (arXiv:2105.12005v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1905.05022",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Huang_W/0/1/0/all/0/1\">Weipeng Huang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Laitonjam_N/0/1/0/all/0/1\">Nishma Laitonjam</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Piao_G/0/1/0/all/0/1\">Guangyuan Piao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hurley_N/0/1/0/all/0/1\">Neil Hurley</a>",
          "description": "This paper focuses on the problem of hierarchical non-overlapping clustering\nof a dataset. In such a clustering, each data item is associated with exactly\none leaf node and each internal node is associated with all the data items\nstored in the sub-tree beneath it, so that each level of the hierarchy\ncorresponds to a partition of the dataset. We develop a novel Bayesian\nnonparametric method combining the nested Chinese Restaurant Process (nCRP) and\nthe Hierarchical Dirichlet Process (HDP). Compared with other existing Bayesian\napproaches, our solution tackles data with complex latent mixture features\nwhich has not been previously explored in the literature. We discuss the\ndetails of the model and the inference procedure. Furthermore, experiments on\nthree datasets show that our method achieves solid empirical results in\ncomparison with existing algorithms.",
          "link": "http://arxiv.org/abs/1905.05022",
          "publishedOn": "2021-05-26T01:22:11.290Z",
          "wordCount": null,
          "title": "Inferring Hierarchical Mixture Structures: A Bayesian Nonparametric Approach. (arXiv:1905.05022v6 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_O/0/1/0/all/0/1\">Ozioma Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McSharry_P/0/1/0/all/0/1\">Patrick McSharry</a>",
          "description": "Modelling, simulation, and forecasting offer a means of facilitating better\nplanning and decision-making. These quantitative approaches can add value\nbeyond traditional methods that do not rely on data and are particularly\nrelevant for public transportation. Lagos is experiencing rapid urbanization\nand currently has a population of just under 15 million. Both long waiting\ntimes and uncertain travel times has driven many people to acquire their own\nvehicle or use alternative modes of transport. This has significantly increased\nthe number of vehicles on the roads leading to even more traffic and greater\ntraffic congestion. This paper investigates urban travel demand in Lagos and\nexplores passenger dynamics in time and space. Using individual commuter trip\ndata from tickets purchased from the Lagos State Bus Rapid Transit (BRT), the\ndemand patterns through the hours of the day, days of the week and bus stations\nare analysed. This study aims to quantify demand from actual passenger trips\nand estimate the impact that dynamic scheduling could have on passenger waiting\ntimes. Station segmentation is provided to cluster stations by their demand\ncharacteristics in order to tailor specific bus schedules. Intra-day public\ntransportation demand in Lagos BRT is analysed and predictions are compared.\nSimulations using fixed and dynamic bus scheduling demonstrate that the average\nwaiting time could be reduced by as much as 80%. The load curves, insights and\nthe approach developed will be useful for informing policymaking in Lagos and\nsimilar African cities facing the challenges of rapid urbanization.",
          "link": "http://arxiv.org/abs/2105.11816",
          "publishedOn": "2021-05-26T01:22:11.289Z",
          "wordCount": null,
          "title": "Public Transportation Demand Analysis: A Case Study of Metropolitan Lagos. (arXiv:2105.11816v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11818",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Leluc_R/0/1/0/all/0/1\">R&#xe9;mi Leluc</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Portier_F/0/1/0/all/0/1\">Fran&#xe7;ois Portier</a>",
          "description": "While classical forms of stochastic gradient descent algorithm treat the\ndifferent coordinates in the same way, a framework allowing for adaptive (non\nuniform) coordinate sampling is developed to leverage structure in data. In a\nnon-convex setting and including zeroth order gradient estimate, almost sure\nconvergence as well as non-asymptotic bounds are established. Within the\nproposed framework, we develop an algorithm, MUSKETEER, based on a\nreinforcement strategy: after collecting information on the noisy gradients, it\nsamples the most promising coordinate (all for one); then it moves along the\none direction yielding an important decrease of the objective (one for all).\nNumerical experiments on both synthetic and real data examples confirm the\neffectiveness of MUSKETEER in large scale problems.",
          "link": "http://arxiv.org/abs/2105.11818",
          "publishedOn": "2021-05-26T01:22:11.289Z",
          "wordCount": null,
          "title": "SGD with Coordinate Sampling: Theory and Practice. (arXiv:2105.11818v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "Steerable CNN imposes the prior knowledge of transformation invariance or\nequivariance in the network architecture to enhance the the network robustness\non geometry transformation of data and reduce overfitting. It has been an\nintuitive and widely used technique to construct a steerable filter by\naugmenting a filter with its transformed copies in the past decades, which is\nnamed as filter transform in this paper. Recently, the problem of steerable CNN\nhas been studied from aspect of group representation theory, which reveals the\nfunction space structure of a steerable kernel function. However, it is not yet\nclear on how this theory is related to the filter transform technique. In this\npaper, we show that kernel constructed by filter transform can also be\ninterpreted in the group representation theory. This interpretation help\ncomplete the puzzle of steerable CNN theory and provides a novel and simple\napproach to implement steerable convolution operators. Experiments are executed\non multiple datasets to verify the feasibility of the proposed approach.",
          "link": "http://arxiv.org/abs/2105.11636",
          "publishedOn": "2021-05-26T01:22:11.288Z",
          "wordCount": null,
          "title": "FILTRA: Rethinking Steerable CNN by Filter Transform. (arXiv:2105.11636v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1905.10687",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kruse_J/0/1/0/all/0/1\">Jakob Kruse</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Detommaso_G/0/1/0/all/0/1\">Gianluca Detommaso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kothe_U/0/1/0/all/0/1\">Ullrich K&#xf6;the</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scheichl_R/0/1/0/all/0/1\">Robert Scheichl</a>",
          "description": "Many recent invertible neural architectures are based on coupling block\ndesigns where variables are divided in two subsets which serve as inputs of an\neasily invertible (usually affine) triangular transformation. While such a\ntransformation is invertible, its Jacobian is very sparse and thus may lack\nexpressiveness. This work presents a simple remedy by noting that subdivision\nand (affine) coupling can be repeated recursively within the resulting subsets,\nleading to an efficiently invertible block with dense, triangular Jacobian. By\nformulating our recursive coupling scheme via a hierarchical architecture, HINT\nallows sampling from a joint distribution p(y,x) and the corresponding\nposterior p(x|y) using a single invertible network. We evaluate our method on\nsome standard data sets and benchmark its full power for density estimation and\nBayesian inference on a novel data set of 2D shapes in Fourier\nparameterization, which enables consistent visualization of samples for\ndifferent dimensionalities.",
          "link": "http://arxiv.org/abs/1905.10687",
          "publishedOn": "2021-05-26T01:22:11.288Z",
          "wordCount": null,
          "title": "HINT: Hierarchical Invertible Neural Transport for Density Estimation and Bayesian Inference. (arXiv:1905.10687v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shields_J/0/1/0/all/0/1\">Jackson Shields</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizarro_O/0/1/0/all/0/1\">Oscar Pizarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_S/0/1/0/all/0/1\">Stefan B. Williams</a>",
          "description": "Special-purpose Autonomous Underwater Vehicles (AUVs) are utilised for\nbenthic (seafloor) surveys, where the vehicle collects optical imagery of near\nthe seafloor. Due to the small-sensor footprint of the cameras and the vast\nareas to be surveyed, these AUVs can not feasibly full coverage of areas larger\nthan a few tens of thousands of square meters. Therefore AUV paths which sample\nsparsely, yet effectively, the survey areas are necessary. Broad scale acoustic\nbathymetric data is ready available over large areas, and often is a useful\nprior of seafloor cover. As such, prior bathymetry can be used to guide AUV\ndata collection. This research proposes methods for planning initial AUV\nsurveys that efficiently explore a feature space representation of the\nbathymetry, in order to sample from a diverse set of bathymetric terrain. This\nwill enable the AUV to visit areas that likely contain unique habitats and are\nrepresentative of the entire survey site. The suitability of these methods to\nplan AUV surveys is evaluated based on the coverage of the feature space and\nalso the ability to visit all classes of benthic habitat on the initial dive.\nThis is a valuable tool for AUV surveys as it increases the utility of initial\ndives. It also delivers a comprehensive training set to learn a relationship\nbetween acoustic bathymetry and visually-derived seafloor classifications.",
          "link": "http://arxiv.org/abs/2105.11598",
          "publishedOn": "2021-05-26T01:22:11.287Z",
          "wordCount": null,
          "title": "Feature Space Exploration For Planning Initial Benthic AUV Surveys. (arXiv:2105.11598v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aasi_E/0/1/0/all/0/1\">Erfan Aasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasile_C/0/1/0/all/0/1\">Cristian Ioan Vasile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahreinian_M/0/1/0/all/0/1\">Mahroo Bahreinian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belta_C/0/1/0/all/0/1\">Calin Belta</a>",
          "description": "Many autonomous systems, such as robots and self-driving cars, involve\nreal-time decision making in complex environments, and require prediction of\nfuture outcomes from limited data. Moreover, their decisions are increasingly\nrequired to be interpretable to humans for safe and trustworthy co-existence.\nThis paper is a first step towards interpretable learning-based robot control.\nWe introduce a novel learning problem, called incremental formula and predictor\nlearning, to generate binary classifiers with temporal logic structure from\ntime-series data. The classifiers are represented as pairs of Signal Temporal\nLogic (STL) formulae and predictors for their satisfaction. The incremental\nproperty provides prediction of labels for prefix signals that are revealed\nover time. We propose a boosted decision-tree algorithm that leverages weak,\nbut computationally inexpensive, learners to increase prediction and runtime\nperformance. The effectiveness and classification accuracy of our algorithms\nare evaluated on autonomous-driving and naval surveillance case studies.",
          "link": "http://arxiv.org/abs/2105.11508",
          "publishedOn": "2021-05-26T01:22:11.284Z",
          "wordCount": 580,
          "title": "Inferring Temporal Logic Properties from Data using Boosted Decision Trees. (arXiv:2105.11508v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>",
          "description": "Most existing deep learning-based pan-sharpening methods have several widely\nrecognized issues, such as spectral distortion and insufficient spatial texture\nenhancement, we propose a novel pan-sharpening convolutional neural network\nbased on a high-pass modification block. Different from existing methods, the\nproposed block is designed to learn the high-pass information, leading to\nenhance spatial information in each band of the multi-spectral-resolution\nimages. To facilitate the generation of visually appealing pan-sharpened\nimages, we propose a perceptual loss function and further optimize the model\nbased on high-level features in the near-infrared space. Experiments\ndemonstrate the superior performance of the proposed method compared to the\nstate-of-the-art pan-sharpening methods, both quantitatively and qualitatively.\nThe proposed model is open-sourced at https://github.com/jiaming-wang/HMB.",
          "link": "http://arxiv.org/abs/2105.11576",
          "publishedOn": "2021-05-26T01:22:11.279Z",
          "wordCount": 575,
          "title": "Pan-sharpening via High-pass Modification Convolutional Neural Network. (arXiv:2105.11576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11627",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiqiang Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_J/0/1/0/all/0/1\">Jingshuang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>",
          "description": "We introduced the least-squares ReLU neural network (LSNN) method for solving\nthe linear advection-reaction problem with discontinuous solution and showed\nthat the method outperforms mesh-based numerical methods in terms of the number\nof degrees of freedom. This paper studies the LSNN method for scalar nonlinear\nhyperbolic conservation law. The method is a discretization of an equivalent\nleast-squares (LS) formulation in the set of neural network functions with the\nReLU activation function. Evaluation of the LS functional is done by using\nnumerical integration and conservative finite volume scheme. Numerical results\nof some test problems show that the method is capable of approximating the\ndiscontinuous interface of the underlying problem automatically through the\nfree breaking lines of the ReLU neural network. Moreover, the method does not\nexhibit the common Gibbs phenomena along the discontinuous interface.",
          "link": "http://arxiv.org/abs/2105.11627",
          "publishedOn": "2021-05-26T01:22:11.262Z",
          "wordCount": 568,
          "title": "Least-Squares ReLU Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Law. (arXiv:2105.11627v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_R/0/1/0/all/0/1\">Ross C. C. Leon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laucht_A/0/1/0/all/0/1\">Arne Laucht</a>",
          "description": "In the electronics industry, introducing Machine Learning (ML)-based\ntechniques can enhance Technology Computer-Aided Design (TCAD) methods.\nHowever, the performance of ML models is highly dependent on their training\ndatasets. Particularly in the semiconductor industry, given the fact that the\nfabrication process of semiconductor devices is complicated and expensive, it\nis of great difficulty to obtain datasets with sufficient size and good\nquality. In this paper, we propose a strategy for improving ML-based device\nmodeling by data self-augmentation using variational autoencoder-based\ntechniques, where initially only a few experimental data points are required\nand TCAD tools are not essential. Taking a deep neural network-based prediction\ntask of the Ohmic resistance value in Gallium Nitride devices as an example, we\napply our proposed strategy to augment data points and achieve a reduction in\nthe mean absolute error of predicting the experimental results by up to 70%.\nThe proposed method could be easily modified for different tasks, rendering it\nof high interest to the semiconductor industry in general.",
          "link": "http://arxiv.org/abs/2105.11453",
          "publishedOn": "2021-05-26T01:22:11.256Z",
          "wordCount": 596,
          "title": "Improving Machine Learning-Based Modeling of Semiconductor Devices by Data Self-Augmentation. (arXiv:2105.11453v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11544",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Stephenson_O/0/1/0/all/0/1\">Oliver L. Stephenson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kohne_T/0/1/0/all/0/1\">Tobias K&#xf6;hne</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhan_E/0/1/0/all/0/1\">Eric Zhan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cahill_B/0/1/0/all/0/1\">Brent E. Cahill</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yun_S/0/1/0/all/0/1\">Sang-Ho Yun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ross_Z/0/1/0/all/0/1\">Zachary E. Ross</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Simons_M/0/1/0/all/0/1\">Mark Simons</a>",
          "description": "Satellite remote sensing is playing an increasing role in the rapid mapping\nof damage after natural disasters. In particular, synthetic aperture radar\n(SAR) can image the Earth's surface and map damage in all weather conditions,\nday and night. However, current SAR damage mapping methods struggle to separate\ndamage from other changes in the Earth's surface. In this study, we propose a\nnovel approach to damage mapping, combining deep learning with the full time\nhistory of SAR observations of an impacted region in order to detect anomalous\nvariations in the Earth's surface properties due to a natural disaster. We\nquantify Earth surface change using time series of Interferometric SAR\ncoherence, then use a recurrent neural network (RNN) as a probabilistic anomaly\ndetector on these coherence time series. The RNN is first trained on pre-event\ncoherence time series, and then forecasts a probability distribution of the\ncoherence between pre- and post-event SAR images. The difference between the\nforecast and observed co-event coherence provides a measure of the confidence\nin the identification of damage. The method allows the user to choose a damage\ndetection threshold that is customized for each location, based on the local\nbehavior of coherence through time before the event. We apply this method to\ncalculate estimates of damage for three earthquakes using multi-year time\nseries of Sentinel-1 SAR acquisitions. Our approach shows good agreement with\nobserved damage and quantitative improvement compared to using pre- to co-event\ncoherence loss as a damage proxy.",
          "link": "http://arxiv.org/abs/2105.11544",
          "publishedOn": "2021-05-26T01:22:11.245Z",
          "wordCount": 688,
          "title": "Deep Learning-based Damage Mapping with InSAR Coherence Time Series. (arXiv:2105.11544v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11622",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Feng_S/0/1/0/all/0/1\">Shihang Feng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_X/0/1/0/all/0/1\">Xitong Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wohlberg_B/0/1/0/all/0/1\">Brendt Wohlberg</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Symons_N/0/1/0/all/0/1\">Neill Symons</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1\">Youzuo Lin</a>",
          "description": "4D seismic imaging has been widely used in CO$_2$ sequestration projects to\nmonitor the fluid flow in the volumetric subsurface region that is not sampled\nby wells. Ideally, real-time monitoring and near-future forecasting would\nprovide site operators with great insights to understand the dynamics of the\nsubsurface reservoir and assess any potential risks. However, due to obstacles\nsuch as high deployment cost, availability of acquisition equipment, exclusion\nzones around surface structures, only very sparse seismic imaging data can be\nobtained during monitoring. That leads to an unavoidable and growing knowledge\ngap over time. The operator needs to understand the fluid flow throughout the\nproject lifetime and the seismic data are only available at a limited number of\ntimes, this is insufficient for understanding the reservoir behavior. To\novercome those challenges, we have developed spatio-temporal\nneural-network-based models that can produce high-fidelity interpolated or\nextrapolated images effectively and efficiently. Specifically, our models are\nbuilt on an autoencoder, and incorporate the long short-term memory (LSTM)\nstructure with a new loss function regularized by optical flow. We validate the\nperformance of our models using real 4D post-stack seismic imaging data\nacquired at the Sleipner CO$_2$ sequestration field. We employ two different\nstrategies in evaluating our models. Numerically, we compare our models with\ndifferent baseline approaches using classic pixel-based metrics. We also\nconduct a blind survey and collect a total of 20 responses from domain experts\nto evaluate the quality of data generated by our models. Via both numerical and\nexpert evaluation, we conclude that our models can produce high-quality 2D/3D\nseismic imaging data at a reasonable cost, offering the possibility of\nreal-time monitoring or even near-future forecasting of the CO$_2$ storage\nreservoir.",
          "link": "http://arxiv.org/abs/2105.11622",
          "publishedOn": "2021-05-26T01:22:11.227Z",
          "wordCount": 722,
          "title": "Connect the Dots: In Situ 4D Seismic Monitoring of CO$_2$ Storage with Spatio-temporal CNNs. (arXiv:2105.11622v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1\">Olivier Sigaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caselles_Dupre_H/0/1/0/all/0/1\">Hugo Caselles-Dupr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1\">C&#xe9;dric Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akakzia_A/0/1/0/all/0/1\">Ahmed Akakzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetouani_M/0/1/0/all/0/1\">Mohamed Chetouani</a>",
          "description": "Autonomous discovery and direct instruction are two extreme sources of\nlearning in children, but educational sciences have shown that intermediate\napproaches such as assisted discovery or guided play resulted in better\nacquisition of skills. When turning to Artificial Intelligence, the above\ndichotomy is translated into the distinction between autonomous agents which\nlearn in isolation and interactive learning agents which can be taught by\nsocial partners but generally lack autonomy. In between should stand teachable\nautonomous agents: agents learning from both internal and teaching signals to\nbenefit from the higher efficiency of assisted discovery. Such agents could\nlearn on their own in the real world, but non-expert users could drive their\nlearning behavior towards their expectations. More fundamentally, combining\nboth capabilities might also be a key step towards general intelligence. In\nthis paper we elucidate obstacles along this research line. First, we build on\na seminal work of Bruner to extract relevant features of the assisted discovery\nprocesses. Second, we describe current research on autotelic agents, i.e.\nagents equipped with forms of intrinsic motivations that enable them to\nrepresent, self-generate and pursue their own goals. We argue that autotelic\ncapabilities are paving the way towards teachable and autonomous agents.\nFinally, we adopt a social learning perspective on tutoring interactions and we\nhighlight some components that are currently missing to autotelic agents before\nthey can be taught by ordinary people using natural pedagogy, and we provide a\nlist of specific research questions that emerge from this perspective.",
          "link": "http://arxiv.org/abs/2105.11977",
          "publishedOn": "2021-05-26T01:22:11.214Z",
          "wordCount": null,
          "title": "Towards Teachable Autonomous Agents. (arXiv:2105.11977v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11590",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Miller_N/0/1/0/all/0/1\">Nathan Eli Miller</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Mukhopadhyay_S/0/1/0/all/0/1\">Saibal Mukhopadhyay</a>",
          "description": "In this work, we present a Quantum Hopfield Associative Memory (QHAM) and\ndemonstrate its capabilities in simulation and hardware using IBM Quantum\nExperience. The QHAM is based on a quantum neuron design which can be utilized\nfor many different machine learning applications and can be implemented on real\nquantum hardware without requiring mid-circuit measurement or reset operations.\nWe analyze the accuracy of the neuron and the full QHAM considering hardware\nerrors via simulation with hardware noise models as well as with implementation\non the 15-qubit ibmq_16_melbourne device. The quantum neuron and the QHAM are\nshown to be resilient to noise and require low qubit and time overhead. We\nbenchmark the QHAM by testing its effective memory capacity against qubit- and\ncircuit-level errors and demonstrate its capabilities in the NISQ-era of\nquantum hardware. This demonstration of the first functional QHAM to be\nimplemented in NISQ-era quantum hardware is a significant step in machine\nlearning at the leading edge of quantum computing.",
          "link": "http://arxiv.org/abs/2105.11590",
          "publishedOn": "2021-05-26T01:22:11.213Z",
          "wordCount": null,
          "title": "A Quantum Hopfield Associative Memory Implemented on an Actual Quantum Processor. (arXiv:2105.11590v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11535",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jankowiak_M/0/1/0/all/0/1\">Martin Jankowiak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>",
          "description": "We introduce a simple and scalable method for training Gaussian process (GP)\nmodels that exploits cross-validation and nearest neighbor truncation. To\naccommodate binary and multi-class classification we leverage P\\`olya-Gamma\nauxiliary variables and variational inference. In an extensive empirical\ncomparison with a number of alternative methods for scalable GP regression and\nclassification, we find that our method offers fast training and excellent\npredictive performance. We argue that the good predictive performance can be\ntraced to the non-parametric nature of the resulting predictive distributions\nas well as to the cross-validation loss, which provides robustness against\nmodel mis-specification.",
          "link": "http://arxiv.org/abs/2105.11535",
          "publishedOn": "2021-05-26T01:22:11.212Z",
          "wordCount": null,
          "title": "Scalable Cross Validation Losses for Gaussian Process Models. (arXiv:2105.11535v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Liyue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capaldi_D/0/1/0/all/0/1\">Dante Capaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>",
          "description": "Deep learning affords enormous opportunities to augment the armamentarium of\nbiomedical imaging, albeit its design and implementation have potential flaws.\nFundamentally, most deep learning models are driven entirely by data without\nconsideration of any prior knowledge, which dramatically increases the\ncomplexity of neural networks and limits the application scope and model\ngeneralizability. Here we establish a geometry-informed deep learning framework\nfor ultra-sparse 3D tomographic image reconstruction. We introduce a novel\nmechanism for integrating geometric priors of the imaging system. We\ndemonstrate that the seamless inclusion of known priors is essential to enhance\nthe performance of 3D volumetric computed tomography imaging with ultra-sparse\nsampling. The study opens new avenues for data-driven biomedical imaging and\npromises to provide substantially improved imaging tools for various clinical\nimaging and image-guided interventions.",
          "link": "http://arxiv.org/abs/2105.11692",
          "publishedOn": "2021-05-26T01:22:11.212Z",
          "wordCount": null,
          "title": "A Geometry-Informed Deep Learning Framework for Ultra-Sparse 3D Tomographic Image Reconstruction. (arXiv:2105.11692v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yangting Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "Reinforcement learning (RL)-based neural architecture search (NAS) generally\nguarantees better convergence yet suffers from the requirement of huge\ncomputational resources compared with gradient-based approaches, due to the\nrollout bottleneck -- exhaustive training for each sampled generation on proxy\ntasks. In this paper, we propose a general pipeline to accelerate the\nconvergence of the rollout process as well as the RL process in NAS. It is\nmotivated by the interesting observation that both the architecture and the\nparameter knowledge can be transferred between different experiments and even\ndifferent tasks. We first introduce an uncertainty-aware critic (value\nfunction) in Proximal Policy Optimization (PPO) to utilize the architecture\nknowledge in previous experiments, which stabilizes the training process and\nreduces the searching time by 4 times. Further, an architecture knowledge pool\ntogether with a block similarity function is proposed to utilize parameter\nknowledge and reduces the searching time by 2 times. It is the first to\nintroduce block-level weight sharing in RLbased NAS. The block similarity\nfunction guarantees a 100% hitting ratio with strict fairness. Besides, we show\nthat a simply designed off-policy correction factor used in \"replay buffer\" in\nRL optimization can further reduce half of the searching time. Experiments on\nthe Mobile Neural Architecture Search (MNAS) search space show the proposed\nFast Neural Architecture Search (FNAS) accelerates standard RL-based NAS\nprocess by ~10x (e.g. ~256 2x2 TPUv2 x days / 20,000 GPU x hour -> 2,000 GPU x\nhour for MNAS), and guarantees better performance on various vision tasks.",
          "link": "http://arxiv.org/abs/2105.11694",
          "publishedOn": "2021-05-26T01:22:11.212Z",
          "wordCount": null,
          "title": "FNAS: Uncertainty-Aware Fast Neural Architecture Search. (arXiv:2105.11694v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaigh_C/0/1/0/all/0/1\">Cheikh Brahim El Vaigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renoust_B/0/1/0/all/0/1\">Benjamin Renoust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1\">Hajime Nagahara</a>",
          "description": "The rise of digitization of cultural documents offers large-scale contents,\nopening the road for development of AI systems in order to preserve, search,\nand deliver cultural heritage. To organize such cultural content also means to\nclassify them, a task that is very familiar to modern computer science.\nContextual information is often the key to structure such real world data, and\nwe propose to use it in form of a knowledge graph. Such a knowledge graph,\ncombined with content analysis, enhances the notion of proximity between\nartworks so it improves the performances in classification tasks. In this\npaper, we propose a novel use of a knowledge graph, that is constructed on\nannotated data and pseudo-labeled data. With label propagation, we boost\nartwork classification by training a model using a graph convolutional network,\nrelying on the relationships between entities of the knowledge graph. Following\na transductive learning framework, our experiments show that relying on a\nknowledge graph modeling the relations between labeled data and unlabeled data\nallows to achieve state-of-the-art results on multiple classification tasks on\na dataset of paintings, and on a dataset of Buddha statues. Additionally, we\nshow state-of-the-art results for the difficult case of dealing with unbalanced\ndata, with the limitation of disregarding classes with extremely low degrees in\nthe knowledge graph.",
          "link": "http://arxiv.org/abs/2105.11852",
          "publishedOn": "2021-05-26T01:22:11.211Z",
          "wordCount": null,
          "title": "GCNBoost: Artwork Classification by Label Propagation through a Knowledge Graph. (arXiv:2105.11852v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaakoubi_Y/0/1/0/all/0/1\">Yassine Yaakoubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soumis_F/0/1/0/all/0/1\">Fran&#xe7;ois Soumis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1\">Simon Lacoste-Julien</a>",
          "description": "Motivated by the needs from an airline crew scheduling application, we\nintroduce structured convolutional kernel networks (Struct-CKN), which combine\nCKNs from Mairal et al. (2014) in a structured prediction framework that\nsupports constraints on the outputs. CKNs are a particular kind of\nconvolutional neural networks that approximate a kernel feature map on training\ndata, thus combining properties of deep learning with the non-parametric\nflexibility of kernel methods. Extending CKNs to structured outputs allows us\nto obtain useful initial solutions on a flight-connection dataset that can be\nfurther refined by an airline crew scheduling solver. More specifically, we use\na flight-based network modeled as a general conditional random field capable of\nincorporating local constraints in the learning process. Our experiments\ndemonstrate that this approach yields significant improvements for the\nlarge-scale crew pairing problem (50,000 flights per month) over standard\napproaches, reducing the solution cost by 17% (a gain of millions of dollars)\nand the cost of global constraints by 97%.",
          "link": "http://arxiv.org/abs/2105.11646",
          "publishedOn": "2021-05-26T01:22:11.205Z",
          "wordCount": 584,
          "title": "Structured Convolutional Kernel Networks for Airline Crew Scheduling. (arXiv:2105.11646v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintao Wu</a>",
          "description": "The underlying assumption of many machine learning algorithms is that the\ntraining data and test data are drawn from the same distributions. However, the\nassumption is often violated in real world due to the sample selection bias\nbetween the training and test data. Previous research works focus on reweighing\nbiased training data to match the test data and then building classification\nmodels on the reweighed training data. However, how to achieve fairness in the\nbuilt classification models is under-explored. In this paper, we propose a\nframework for robust and fair learning under sample selection bias. Our\nframework adopts the reweighing estimation approach for bias correction and the\nminimax robust estimation approach for achieving robustness on prediction\naccuracy. Moreover, during the minimax optimization, the fairness is achieved\nunder the worst case, which guarantees the model's fairness on test data. We\nfurther develop two algorithms to handle sample selection bias when test data\nis both available and unavailable. We conduct experiments on two real-world\ndatasets and the experimental results demonstrate its effectiveness in terms of\nboth utility and fairness metrics.",
          "link": "http://arxiv.org/abs/2105.11570",
          "publishedOn": "2021-05-26T01:22:11.198Z",
          "wordCount": null,
          "title": "Robust Fairness-aware Learning Under Sample Selection Bias. (arXiv:2105.11570v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1\">Dobrik Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Franscesco Giannini</a>",
          "description": "LENs is a Python module integrating a variety of state-of-the-art approaches\nto provide logic explanations from neural networks. This package focuses on\nbringing these methods to non-specialists. It has minimal dependencies and it\nis distributed under the Apache 2.0 licence allowing both academic and\ncommercial use. Source code and documentation can be downloaded from the github\nrepository: https://github.com/pietrobarbiero/logic_explainer_networks.",
          "link": "http://arxiv.org/abs/2105.11697",
          "publishedOn": "2021-05-26T01:22:11.198Z",
          "wordCount": null,
          "title": "LENs: a Python library for Logic Explained Networks. (arXiv:2105.11697v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1\">Sheng Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dingwen Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1\">Franck Cappello</a>",
          "description": "Error-bounded lossy compression is becoming an indispensable technique for\nthe success of today's scientific projects with vast volumes of data produced\nduring the simulations or instrument data acquisitions. Not only can it\nsignificantly reduce data size, but it also can control the compression errors\nbased on user-specified error bounds. Autoencoder (AE) models have been widely\nused in image compression, but few AE-based compression approaches support\nerror-bounding features, which are highly required by scientific applications.\nTo address this issue, we explore using convolutional autoencoders to improve\nerror-bounded lossy compression for scientific data, with the following three\nkey contributions. (1) We provide an in-depth investigation of the\ncharacteristics of various autoencoder models and develop an error-bounded\nautoencoder-based framework in terms of the SZ model. (2) We optimize the\ncompression quality for main stages in our designed AE-based error-bounded\ncompression framework, fine-tuning the block sizes and latent sizes and also\noptimizing the compression efficiency of latent vectors. (3) We evaluate our\nproposed solution using five real-world scientific datasets and comparing them\nwith six other related works. Experiments show that our solution exhibits a\nvery competitive compression quality from among all the compressors in our\ntests. In absolute terms, it can obtain a much better compression quality (100%\n~ 800% improvement in compression ratio with the same data distortion) compared\nwith SZ2.1 and ZFP in cases with a high compression ratio.",
          "link": "http://arxiv.org/abs/2105.11730",
          "publishedOn": "2021-05-26T01:22:11.198Z",
          "wordCount": null,
          "title": "Exploring Autoencoder-Based Error-Bounded Compression for Scientific Data. (arXiv:2105.11730v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaoxuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pivaro_N/0/1/0/all/0/1\">Nicola Pivaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shobhit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canova_M/0/1/0/all/0/1\">Marcello Canova</a>",
          "description": "Connected and Automated Hybrid Electric Vehicles have the potential to reduce\nfuel consumption and travel time in real-world driving conditions. The\neco-driving problem seeks to design optimal speed and power usage profiles\nbased upon look-ahead information from connectivity and advanced mapping\nfeatures. Recently, Deep Reinforcement Learning (DRL) has been applied to the\neco-driving problem. While the previous studies synthesize simulators and\nmodel-free DRL to reduce online computation, this work proposes a Safe\nOff-policy Model-Based Reinforcement Learning algorithm for the eco-driving\nproblem. The advantages over the existing literature are three-fold. First, the\ncombination of off-policy learning and the use of a physics-based model\nimproves the sample efficiency. Second, the training does not require any\nextrinsic rewarding mechanism for constraint satisfaction. Third, the\nfeasibility of trajectory is guaranteed by using a safe set approximated by\ndeep generative models.\n\nThe performance of the proposed method is benchmarked against a baseline\ncontroller representing human drivers, a previously designed model-free DRL\nstrategy, and the wait-and-see optimal solution. In simulation, the proposed\nalgorithm leads to a policy with a higher average speed and a better fuel\neconomy compared to the model-free agent. Compared to the baseline controller,\nthe learned strategy reduces the fuel consumption by more than 21\\% while\nkeeping the average speed comparable.",
          "link": "http://arxiv.org/abs/2105.11640",
          "publishedOn": "2021-05-26T01:22:11.197Z",
          "wordCount": null,
          "title": "Safe Model-based Off-policy Reinforcement Learning for Eco-Driving in Connected and Automated Hybrid Electric Vehicles. (arXiv:2105.11640v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12033",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai V. Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bui_Thanh_T/0/1/0/all/0/1\">Tan Bui-Thanh</a>",
          "description": "Deep Learning (DL), in particular deep neural networks (DNN), by design is\npurely data-driven and in general does not require physics. This is the\nstrength of DL but also one of its key limitations when applied to science and\nengineering problems in which underlying physical properties (such as\nstability, conservation, and positivity) and desired accuracy need to be\nachieved. DL methods in their original forms are not capable of respecting the\nunderlying mathematical models or achieving desired accuracy even in big-data\nregimes. On the other hand, many data-driven science and engineering problems,\nsuch as inverse problems, typically have limited experimental or observational\ndata, and DL would overfit the data in this case. Leveraging information\nencoded in the underlying mathematical models, we argue, not only compensates\nmissing information in low data regimes but also provides opportunities to\nequip DL methods with the underlying physics and hence obtaining higher\naccuracy. This short communication introduces several model-constrained DL\napproaches (including both feed-forward DNN and autoencoders) that are capable\nof learning not only information hidden in the training data but also in the\nunderlying mathematical models to solve inverse problems. We present and\nprovide intuitions for our formulations for general nonlinear problems. For\nlinear inverse problems and linear networks, the first order optimality\nconditions show that our model-constrained DL approaches can learn information\nencoded in the underlying mathematical models, and thus can produce consistent\nor equivalent inverse solutions, while naive purely data-based counterparts\ncannot.",
          "link": "http://arxiv.org/abs/2105.12033",
          "publishedOn": "2021-05-26T01:22:11.197Z",
          "wordCount": null,
          "title": "Model-Constrained Deep Learning Approaches for Inverse Problems. (arXiv:2105.12033v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amir_G/0/1/0/all/0/1\">Guy Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schapira_M/0/1/0/all/0/1\">Michael Schapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Guy Katz</a>",
          "description": "Deep neural networks (DNNs) have gained significant popularity in recent\nyears, becoming the state of the art in a variety of domains. In particular,\ndeep reinforcement learning (DRL) has recently been employed to train DNNs that\nact as control policies for various types of real-world systems. In this work,\nwe present the whiRL 2.0 tool, which implements a new approach for verifying\ncomplex properties of interest for such DRL systems. To demonstrate the\nbenefits of whiRL 2.0, we apply it to case studies from the communication\nnetworks domain that have recently been used to motivate formal verification of\nDRL systems, and which exhibit characteristics that are conducive for scalable\nverification. We propose techniques for performing k-induction and automated\ninvariant inference on such systems, and use these techniques for proving\nsafety and liveness properties of interest that were previously impossible to\nverify due to the scalability barriers of prior approaches. Furthermore, we\nshow how our proposed techniques provide insights into the inner workings and\nthe generalizability of DRL systems. whiRL 2.0 is publicly available online.",
          "link": "http://arxiv.org/abs/2105.11931",
          "publishedOn": "2021-05-26T01:22:11.196Z",
          "wordCount": null,
          "title": "Towards Scalable Verification of RL-Driven Systems. (arXiv:2105.11931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1\">Shuai Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kaihao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Suting Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qing Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>",
          "description": "Predicting the start-ups that will eventually succeed is essentially\nimportant for the venture capital business and worldwide policy makers,\nespecially at an early stage such that rewards can possibly be exponential.\n\nThough various empirical studies and data-driven modeling work have been\ndone, the predictive power of the complex networks of stakeholders including\nventure capital investors, start-ups, and start-ups' managing members has not\nbeen thoroughly explored. We design an incremental representation learning\nmechanism and a sequential learning model, utilizing the network structure\ntogether with the rich attributes of the nodes. In general, our method achieves\nthe state-of-the-art prediction performance on a comprehensive dataset of\nglobal venture capital investments and surpasses human investors by large\nmargins. Specifically, it excels at predicting the outcomes for start-ups in\nindustries such as healthcare and IT. Meanwhile, we shed light on impacts on\nstart-up success from observable factors including gender, education, and\nnetworking, which can be of value for practitioners as well as policy makers\nwhen they screen ventures of high growth potentials.",
          "link": "http://arxiv.org/abs/2105.11537",
          "publishedOn": "2021-05-26T01:22:11.193Z",
          "wordCount": 614,
          "title": "Graph Neural Network Based VC Investment Success Prediction. (arXiv:2105.11537v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11452",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kopru_B/0/1/0/all/0/1\">Berkay K&#xf6;pr&#xfc;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aslan_M/0/1/0/all/0/1\">Murat Aslan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kholmatov_A/0/1/0/all/0/1\">Alisher Kholmatov</a>",
          "description": "Sleep is restoration process of the body. The efficiency of this restoration\nprocess is directly correlated to the amount of time spent at each sleep phase.\nHence, automatic tracking of sleep via wearable devices has attracted both the\nresearchers and industry. Current state-of-the-art sleep tracking solutions are\nmemory and processing greedy and they require cloud or mobile phone\nconnectivity. We propose a memory efficient sleep tracking architecture which\ncan work in the embedded environment without needing any cloud or mobile phone\nconnection. In this study, a novel architecture is proposed that consists of a\nfeature extraction and Artificial Neural Networks based stacking classifier.\nBesides, we discussed how to tackle with sequential nature of the sleep staging\nfor the memory constraint environments through the proposed framework. To\nverify the system, a dataset is collected from 24 different subjects for 31\nnights with a wrist worn device having 3-axis accelerometer (ACC) and\nphotoplethysmogram (PPG) sensors. Over the collected dataset, the proposed\nclassification architecture achieves 20\\% and 14\\% better F1 scores than its\ncompetitors. Apart from the superior performance, proposed architecture is a\npromising solution for resource constraint embedded systems by allocating only\n4.2 kilobytes of memory (RAM).",
          "link": "http://arxiv.org/abs/2105.11452",
          "publishedOn": "2021-05-26T01:22:11.167Z",
          "wordCount": 632,
          "title": "Neural Network Based Sleep Phases Classification for Resource Constraint Environments. (arXiv:2105.11452v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawi_D/0/1/0/all/0/1\">Diaa Badawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyuncu_E/0/1/0/all/0/1\">Erdem Koyuncu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1\">A. Enis Cetin</a>",
          "description": "We consider a family of vector dot products that can be implemented using\nsign changes and addition operations only. The dot products are\nenergy-efficient as they avoid the multiplication operation entirely. Moreover,\nthe dot products induce the $\\ell_1$-norm, thus providing robustness to\nimpulsive noise. First, we analytically prove that the dot products yield\nsymmetric, positive semi-definite generalized covariance matrices, thus\nenabling principal component analysis (PCA). Moreover, the generalized\ncovariance matrices can be constructed in an Energy Efficient (EEF) manner due\nto the multiplication-free property of the underlying vector products. We\npresent image reconstruction examples in which our EEF PCA method result in the\nhighest peak signal-to-noise ratios compared to the ordinary $\\ell_2$-PCA and\nthe recursive $\\ell_1$-PCA.",
          "link": "http://arxiv.org/abs/2105.11634",
          "publishedOn": "2021-05-26T01:22:11.161Z",
          "wordCount": 563,
          "title": "Robust Principal Component Analysis Using a Novel Kernel Related with the L1-Norm. (arXiv:2105.11634v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rim_D/0/1/0/all/0/1\">Daniela N. Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_I/0/1/0/all/0/1\">Inseon Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>",
          "description": "Recent achievements in end-to-end deep learning have encouraged the\nexploration of tasks dealing with highly structured data with unified deep\nnetwork models. Having such models for compressing audio signals has been\nchallenging since it requires discrete representations that are not easy to\ntrain with end-to-end backpropagation. In this paper, we present an end-to-end\ndeep learning approach that combines recurrent neural networks (RNNs) within\nthe training strategy of variational autoencoders (VAEs) with a binary\nrepresentation of the latent space. We apply a reparametrization trick for the\nBernoulli distribution for the discrete representations, which allows smooth\nbackpropagation. In addition, our approach allows the separation of the encoder\nand decoder, which is necessary for compression tasks. To our best knowledge,\nthis is the first end-to-end learning for a single audio compression model with\nRNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.",
          "link": "http://arxiv.org/abs/2105.11681",
          "publishedOn": "2021-05-26T01:22:11.155Z",
          "wordCount": 579,
          "title": "Deep Neural Networks and End-to-End Learning for Audio Compression. (arXiv:2105.11681v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11603",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Hasan_A/0/1/0/all/0/1\">Areeq I. Hasan</a>",
          "description": "We propose a novel paradigm of integration of Grover's algorithm in a machine\nlearning framework: the inductive Grover oracular quantum neural network\n(IGO-QNN). The model defines a variational quantum circuit with hidden layers\nof parameterized quantum neurons densely connected via entangle synapses to\nencode a dynamic Grover's search oracle that can be trained from a set of\ndatabase-hit training examples. This widens the range of problem applications\nof Grover's unstructured search algorithm to include the vast majority of\nproblems lacking analytic descriptions of solution verifiers, allowing for\nquadratic speed-up in unstructured search for the set of search problems with\nrelationships between input and output spaces that are tractably underivable\ndeductively. This generalization of Grover's oracularization may prove\nparticularly effective in deep reinforcement learning, computer vision, and,\nmore generally, as a feature vector classifier at the top of an existing model.",
          "link": "http://arxiv.org/abs/2105.11603",
          "publishedOn": "2021-05-26T01:22:11.134Z",
          "wordCount": 563,
          "title": "IGO-QNN: Quantum Neural Network Architecture for Inductive Grover Oracularization. (arXiv:2105.11603v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basto_C/0/1/0/all/0/1\">Carlos Basto</a>",
          "description": "A data-centric approach with Natural Language Processing (NLP) to predict\npersonality types based on the MBTI (an introspective self-assessment\nquestionnaire that indicates different psychological preferences about how\npeople perceive the world and make decisions) through systematic enrichment of\ntext representation, based on the domain of the area, under the generation of\nfeatures based on three types of analysis: sentimental, grammatical and\naspects. The experimentation had a robust baseline of stacked models, with\npremature optimization of hyperparameters through grid search, with gradual\nfeedback, for each of the four classifiers (dichotomies) of MBTI. The results\nshowed that attention to the data iteration loop focused on quality,\nexplanatory power and representativeness for the abstraction of more\nrelevant/important resources for the studied phenomenon made it possible to\nimprove the evaluation metrics results more quickly and less costly than\ncomplex models such as the LSTM or state of the art ones as BERT, as well as\nthe importance of these results by comparisons made from various perspectives.\nIn addition, the study demonstrated a broad spectrum for the evolution and\ndeepening of the task and possible approaches for a greater extension of the\nabstraction of personality types.",
          "link": "http://arxiv.org/abs/2105.11798",
          "publishedOn": "2021-05-26T01:22:11.123Z",
          "wordCount": 642,
          "title": "Extending the Abstraction of Personality Types based on MBTI with Machine Learning and Natural Language Processing. (arXiv:2105.11798v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hyekang Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1\">Calvin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_I/0/1/0/all/0/1\">Ishan Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battle_L/0/1/0/all/0/1\">Leilani Battle</a>",
          "description": "For deep learning practitioners, hyperparameter tuning for optimizing model\nperformance can be a computationally expensive task. Though visualization can\nhelp practitioners relate hyperparameter settings to overall model performance,\nsignificant manual inspection is still required to guide the hyperparameter\nsettings in the next batch of experiments. In response, we present a\nstreamlined visualization system enabling deep learning practitioners to more\nefficiently explore, tune, and optimize hyperparameters in a batch of\nexperiments. A key idea is to directly suggest more optimal hyperparameter\nvalues using a predictive mechanism. We then integrate this mechanism with\ncurrent visualization practices for deep learning. Moreover, an analysis on the\nvariance in a selected performance metric in the context of the model\nhyperparameters shows the impact that certain hyperparameters have on the\nperformance metric. We evaluate the tool with a user study on deep learning\nmodel builders, finding that our participants have little issue adopting the\ntool and working with it as part of their workflow.",
          "link": "http://arxiv.org/abs/2105.11516",
          "publishedOn": "2021-05-26T01:22:11.116Z",
          "wordCount": 586,
          "title": "Guided Hyperparameter Tuning Through Visualization and Inference. (arXiv:2105.11516v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Michael Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Seyed Ali Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansch_R/0/1/0/all/0/1\">Ronny H&#xe4;nsch</a>",
          "description": "Annotated datasets have become one of the most crucial preconditions for the\ndevelopment and evaluation of machine learning-based methods designed for the\nautomated interpretation of remote sensing data. In this paper, we review the\nhistoric development of such datasets, discuss their features based on a few\nselected examples, and address open issues for future developments.",
          "link": "http://arxiv.org/abs/2105.11726",
          "publishedOn": "2021-05-26T01:22:11.061Z",
          "wordCount": 518,
          "title": "There is no data like more data -- current status of machine learning datasets in remote sensing. (arXiv:2105.11726v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hanxu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>",
          "description": "It is important to study what implicit regularization is imposed on the loss\nfunction during the training that leads over-parameterized neural networks\n(NNs) to good performance on real dataset. Empirically, existing works have\nshown that weights of NNs condense on isolated orientations with small\ninitialization. The condensation implies that the NN learns features from the\ntraining data and is effectively a much smaller network. In this work, we show\nthat the singularity of the activation function at original point is a key\nfactor to understanding the condensation at initial training stage. Our\nexperiments suggest that the maximal number of condensed orientations is twice\nof the singularity order. Our theoretical analysis confirms experiments for two\ncases, one is for the first-order singularity activation function and the other\nis for the one-dimensional input. This work takes a step towards understanding\nhow small initialization implicitly leads NNs to condensation at initial\ntraining, which is crucial to understand the training and the learning of deep\nNNs.",
          "link": "http://arxiv.org/abs/2105.11686",
          "publishedOn": "2021-05-26T01:22:11.053Z",
          "wordCount": 596,
          "title": "Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training. (arXiv:2105.11686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francisco_P/0/1/0/all/0/1\">P&#xe9;rez-Hern&#xe1;ndez Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_R/0/1/0/all/0/1\">Rodr&#xed;guez-Ortega Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yassir_B/0/1/0/all/0/1\">Benhammou Yassir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francisco_H/0/1/0/all/0/1\">Herrera Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siham_T/0/1/0/all/0/1\">Tabik Siham</a>",
          "description": "The detection of critical infrastructures is of high importance in several\nfields such as security, anomaly detection, land use planning and land use\nchange detection. However, critical infrastructures detection in aerial and\nsatellite images is still a challenge as each one has completely different size\nand requires different spacial resolution to be identified correctly.\nHeretofore, there are no special datasets for training critical infrastructures\ndetectors. This paper presents a smart dataset as well as a\nresolution-independent critical infrastructure detection system. In particular,\nguided by the performance of the detection model, we built a dataset organized\ninto two scales, small and large scale, and designed a two-stage deep learning\ndetection of different scale critical infrastructures (DetDSCI) methodology in\northo-images. DetDSCI methodology first determines the input image zoom level\nusing a classification model, then analyses the input image with the\nappropriate scale detection model. Our experiments show that DetDSCI\nmethodology achieves up to 37,53% F1 improvement with respect to the baseline\ndetector.",
          "link": "http://arxiv.org/abs/2105.11844",
          "publishedOn": "2021-05-26T01:22:11.034Z",
          "wordCount": null,
          "title": "Small and large scale critical infrastructures detection based on deep learning using high resolution orthogonal images. (arXiv:2105.11844v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jianhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>",
          "description": "Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural\nnetworks, have attracted great attentions from researchers and industry. The\nmost efficient way to train deep SNNs is through ANN-SNN conversion. However,\nthe conversion usually suffers from accuracy loss and long inference time,\nwhich impede the practical application of SNN. In this paper, we theoretically\nanalyze ANN-SNN conversion and derive sufficient conditions of the optimal\nconversion. To better correlate ANN-SNN and get greater accuracy, we propose\nRate Norm Layer to replace the ReLU activation function in source ANN training,\nenabling direct conversion from a trained ANN to an SNN. Moreover, we propose\nan optimal fit curve to quantify the fit between the activation value of source\nANN and the actual firing rate of target SNN. We show that the inference time\ncan be reduced by optimizing the upper bound of the fit curve in the revised\nANN to achieve fast inference. Our theory can explain the existing work on fast\nreasoning and get better results. The experimental results show that the\nproposed method achieves near loss less conversion with VGG-16,\nPreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster\nreasoning performance under 0.265x energy consumption of the typical method.\nThe code is available at\nhttps://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.",
          "link": "http://arxiv.org/abs/2105.11654",
          "publishedOn": "2021-05-26T01:22:11.021Z",
          "wordCount": 679,
          "title": "Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep Spiking Neural Networks. (arXiv:2105.11654v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zijian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Hongda Jia</a>",
          "description": "Recently, deep reinforcement learning (RL) algorithms have made great\nprogress in multi-agent domain. However, due to characteristics of RL, training\nfor complex tasks would be resource-intensive and time-consuming. To meet this\nchallenge, mutual learning strategy between homogeneous agents is essential,\nwhich is under-explored in previous studies, because most existing methods do\nnot consider to use the knowledge of agent models. In this paper, we present an\nadaptation method of the majority of multi-agent reinforcement learning (MARL)\nalgorithms called KnowSR which takes advantage of the differences in learning\nbetween agents. We employ the idea of knowledge distillation (KD) to share\nknowledge among agents to shorten the training phase. To empirically\ndemonstrate the robustness and effectiveness of KnowSR, we performed extensive\nexperiments on state-of-the-art MARL algorithms in collaborative and\ncompetitive scenarios. The results demonstrate that KnowSR outperforms recently\nreported methodologies, emphasizing the importance of the proposed knowledge\nsharing for MARL.",
          "link": "http://arxiv.org/abs/2105.11611",
          "publishedOn": "2021-05-26T01:22:10.972Z",
          "wordCount": 589,
          "title": "KnowSR: Knowledge Sharing among Homogeneous Agents in Multi-agent Reinforcement Learning. (arXiv:2105.11611v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1\">Etienne Bennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1\">Victor Bouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1\">Antoine Toubhans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>",
          "description": "Few-Shot Learning (FSL) algorithms have made substantial progress in learning\nnovel concepts with just a handful of labelled data. To classify query\ninstances from novel classes encountered at test-time, they only require a\nsupport set composed of a few labelled samples. FSL benchmarks commonly assume\nthat those queries come from the same distribution as instances in the support\nset. However, in a realistic set-ting, data distribution is plausibly subject\nto change, a situation referred to as Distribution Shift (DS). The present work\naddresses the new and challenging problem of Few-Shot Learning under\nSupport/Query Shift (FSQS) i.e., when support and query instances are sampled\nfrom related but different distributions. Our contributions are the following.\nFirst, we release a testbed for FSQS, including datasets, relevant baselines\nand a protocol for a rigorous and reproducible evaluation. Second, we observe\nthat well-established FSL algorithms unsurprisingly suffer from a considerable\ndrop in accuracy when facing FSQS, stressing the significance of our study.\nFinally, we show that transductive algorithms can limit the inopportune effect\nof DS. In particular, we study both the role of Batch-Normalization and Optimal\nTransport (OT) in aligning distributions, bridging Unsupervised Domain\nAdaptation with FSL. This results in a new method that efficiently combines OT\nwith the celebrated Prototypical Networks. We bring compelling experiments\ndemonstrating the advantage of our method. Our work opens an exciting line of\nresearch by providing a testbed and strong baselines. Our code is available at\nhttps://github.com/ebennequin/meta-domain-shift.",
          "link": "http://arxiv.org/abs/2105.11804",
          "publishedOn": "2021-05-26T01:22:10.943Z",
          "wordCount": 683,
          "title": "Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11632",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiqiang Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_J/0/1/0/all/0/1\">Jingshuang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>",
          "description": "This paper studies least-squares ReLU neural network method for solving the\nlinear advection-reaction problem with discontinuous solution. The method is a\ndiscretization of an equivalent least-squares formulation in the set of neural\nnetwork functions with the ReLU activation function. The method is capable of\napproximating the discontinuous interface of the underlying problem\nautomatically through the free hyper-planes of the ReLU neural network and,\nhence, outperforms mesh-based numerical methods in terms of the number of\ndegrees of freedom. Numerical results of some benchmark test problems show that\nthe method can not only approximate the solution with the least number of\nparameters, but also avoid the common Gibbs phenomena along the discontinuous\ninterface. Moreover, a three-layer ReLU neural network is necessary and\nsufficient in order to well approximate a discontinuous solution with an\ninterface in $\\mathbb{R}^2$ that is not a straight line.",
          "link": "http://arxiv.org/abs/2105.11632",
          "publishedOn": "2021-05-26T01:22:10.922Z",
          "wordCount": 578,
          "title": "Least-Squares ReLU Neural Network (LSNN) Method For Linear Advection-Reaction Equation. (arXiv:2105.11632v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fort_G/0/1/0/all/0/1\">Gersende Fort</a> (IMT), <a href=\"http://arxiv.org/find/cs/1/au:+Moulines_E/0/1/0/all/0/1\">Eric Moulines</a> (X-DEP-MATHAPP, XPOP)",
          "description": "Incremental Expectation Maximization (EM) algorithms were introduced to\ndesign EM for the large scale learning framework by avoiding the full data set\nto be processed at each iteration. Nevertheless, these algorithms all assume\nthat the conditional expectations of the sufficient statistics are explicit. In\nthis paper, we propose a novel algorithm named Perturbed Prox-Preconditioned\nSPIDER (3P-SPIDER), which builds on the Stochastic Path Integral Differential\nEstimatoR EM (SPIDER-EM) algorithm. The 3P-SPIDER algorithm addresses many\nintractabilities of the E-step of EM; it also deals with non-smooth\nregularization and convex constraint set. Numerical experiments show that\n3P-SPIDER outperforms other incremental EM methods and discuss the role of some\ndesign parameters.",
          "link": "http://arxiv.org/abs/2105.11732",
          "publishedOn": "2021-05-26T01:22:10.916Z",
          "wordCount": 556,
          "title": "The Perturbed Prox-Preconditioned SPIDER algorithm for EM-based large scale learning. (arXiv:2105.11732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheibani_M/0/1/0/all/0/1\">Mohamadreza Sheibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_G/0/1/0/all/0/1\">Ge Ou</a>",
          "description": "The abundance of training data is not guaranteed in various supervised\nlearning applications. One of these situations is the post-earthquake regional\ndamage assessment of buildings. Querying the damage label of each building\nrequires a thorough inspection by experts, and thus, is an expensive task. A\npractical approach is to sample the most informative buildings in a sequential\nlearning scheme. Active learning methods recommend the most informative cases\nthat are able to maximally reduce the generalization error. The information\ntheoretic measure of mutual information (MI) is one of the most effective\ncriteria to evaluate the effectiveness of the samples in a pool-based sample\nselection scenario. However, the computational complexity of the standard MI\nalgorithm prevents the utilization of this method on large datasets. A local\nkernels strategy was proposed to reduce the computational costs, but the\nadaptability of the kernels to the observed labels was not considered in the\noriginal formulation of this strategy. In this article, an adaptive local\nkernels methodology is developed that allows for the conformability of the\nkernels to the observed output data while enhancing the computational\ncomplexity of the standard MI algorithm. The proposed algorithm is developed to\nwork on a Gaussian process regression (GPR) framework, where the kernel\nhyperparameters are updated after each label query using the maximum likelihood\nestimation. In the sequential learning procedure, the updated hyperparameters\ncan be used in the MI kernel matrices to improve the sample suggestion\nperformance. The advantages are demonstrated on a simulation of the 2018\nAnchorage, AK, earthquake. It is shown that while the proposed algorithm\nenables GPR to reach acceptable performance with fewer training data, the\ncomputational demands remain lower than the standard local kernels strategy.",
          "link": "http://arxiv.org/abs/2105.11492",
          "publishedOn": "2021-05-26T01:22:10.900Z",
          "wordCount": 732,
          "title": "Adaptive Local Kernels Formulation of Mutual Information with Application to Active Post-Seismic Building Damage Inference. (arXiv:2105.11492v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1\">Mike Preuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>",
          "description": "Transfer learning can speed up training in machine learning and is regularly\nused in classification tasks. It reuses prior knowledge from other tasks to\npre-train networks for new tasks. In reinforcement learning, learning actions\nfor a behavior policy that can be applied to new environments is still a\nchallenge, especially for tasks that involve much planning. Sokoban is a\nchallenging puzzle game. It has been used widely as a benchmark in\nplanning-based reinforcement learning. In this paper, we show how prior\nknowledge improves learning in Sokoban tasks. We find that reusing feature\nrepresentations learned previously can accelerate learning new, more complex,\ninstances. In effect, we show how curriculum learning, from simple to complex\ntasks, works in Sokoban. Furthermore, feature representations learned in\nsimpler instances are more general, and thus lead to positive transfers towards\nmore complex tasks, but not vice versa. We have also studied which part of the\nknowledge is most important for transfer to succeed, and identify which layers\nshould be used for pre-training.",
          "link": "http://arxiv.org/abs/2105.11702",
          "publishedOn": "2021-05-26T01:22:10.887Z",
          "wordCount": null,
          "title": "Transfer Learning and Curriculum Learning in Sokoban. (arXiv:2105.11702v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>",
          "description": "We present the Topology Transformation Equivariant Representation learning, a\ngeneral paradigm of self-supervised learning for node representations of graph\ndata to enable the wide applicability of Graph Convolutional Neural Networks\n(GCNNs). We formalize the proposed model from an information-theoretic\nperspective, by maximizing the mutual information between topology\ntransformations and node representations before and after the transformations.\nWe derive that maximizing such mutual information can be relaxed to minimizing\nthe cross entropy between the applied topology transformation and its\nestimation from node representations. In particular, we seek to sample a subset\nof node pairs from the original graph and flip the edge connectivity between\neach pair to transform the graph topology. Then, we self-train a representation\nencoder to learn node representations by reconstructing the topology\ntransformations from the feature representations of the original and\ntransformed graphs. In experiments, we apply the proposed model to the\ndownstream node and graph classification tasks, and results show that the\nproposed method outperforms the state-of-the-art unsupervised approaches.",
          "link": "http://arxiv.org/abs/2105.11689",
          "publishedOn": "2021-05-26T01:22:10.790Z",
          "wordCount": null,
          "title": "Self-Supervised Graph Representation Learning via Topology Transformations. (arXiv:2105.11689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baisero_A/0/1/0/all/0/1\">Andrea Baisero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1\">Christopher Amato</a>",
          "description": "In partially observable reinforcement learning, offline training gives access\nto latent information which is not available during online training and/or\nexecution, such as the system state. Asymmetric actor-critic methods exploit\nsuch information by training a history-based policy via a state-based critic.\nHowever, many asymmetric methods lack theoretical foundation, and are only\nevaluated on limited domains. We examine the theory of asymmetric actor-critic\nmethods which use state-based critics, and expose fundamental issues which\nundermine the validity of a common variant, and its ability to address high\npartial observability. We propose an unbiased asymmetric actor-critic variant\nwhich is able to exploit state information while remaining theoretically sound,\nmaintaining the validity of the policy gradient theorem, and introducing no\nbias and relatively low variance into the training process. An empirical\nevaluation performed on domains which exhibit significant partial observability\nconfirms our analysis, and shows the unbiased asymmetric actor-critic converges\nto better policies and/or faster than symmetric actor-critic and standard\nasymmetric actor-critic baselines.",
          "link": "http://arxiv.org/abs/2105.11674",
          "publishedOn": "2021-05-26T01:22:10.788Z",
          "wordCount": null,
          "title": "Unbiased Asymmetric Actor-Critic for Partially Observable Reinforcement Learning. (arXiv:2105.11674v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11853",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Nguyen_N/0/1/0/all/0/1\">Nam Nguyen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_K/0/1/0/all/0/1\">Kwang-Chen Chen</a>",
          "description": "This paper introduces a novel quantum embedding search algorithm (QES,\npronounced as \"quest\"), enabling search for optimal quantum embedding design\nfor a specific dataset of interest. First, we establish the connection between\nthe structures of quantum embedding and the representations of directed\nmulti-graphs, enabling a well-defined search space. Second, we instigate the\nentanglement level to reduce the cardinality of the search space to a feasible\nsize for practical implementations. Finally, we mitigate the cost of evaluating\nthe true loss function by using surrogate models via sequential model-based\noptimization. We demonstrate the feasibility of our proposed approach on\nsynthesis and Iris datasets, which empirically shows that found quantum\nembedding architecture by QES outperforms manual designs whereas achieving\ncomparable performance to classical machine learning models.",
          "link": "http://arxiv.org/abs/2105.11853",
          "publishedOn": "2021-05-26T01:22:10.761Z",
          "wordCount": null,
          "title": "Quantum Embedding Search for Quantum Machine Learning. (arXiv:2105.11853v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yawen Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Recent breakthroughs of Neural Architecture Search (NAS) extend the field's\nresearch scope towards a broader range of vision tasks and more diversified\nsearch spaces. While existing NAS methods mostly design architectures on a\nsingle task, algorithms that look beyond single-task search are surging to\npursue a more efficient and universal solution across various tasks. Many of\nthem leverage transfer learning and seek to preserve, reuse, and refine network\ndesign knowledge to achieve higher efficiency in future tasks. However, the\nenormous computational cost and experiment complexity of cross-task NAS are\nimposing barriers for valuable research in this direction. Existing NAS\nbenchmarks all focus on one type of vision task, i.e., classification. In this\nwork, we propose TransNAS-Bench-101, a benchmark dataset containing network\nperformance across seven tasks, covering classification, regression,\npixel-level prediction, and self-supervised tasks. This diversity provides\nopportunities to transfer NAS methods among tasks and allows for more complex\ntransfer schemes to evolve. We explore two fundamentally different types of\nsearch space: cell-level search space and macro-level search space. With 7,352\nbackbones evaluated on seven tasks, 51,464 trained models with detailed\ntraining information are provided. With TransNAS-Bench-101, we hope to\nencourage the advent of exceptional NAS algorithms that raise cross-task search\nefficiency and generalizability to the next level. Our dataset file will be\navailable at Mindspore, VEGA.",
          "link": "http://arxiv.org/abs/2105.11871",
          "publishedOn": "2021-05-26T01:22:10.751Z",
          "wordCount": null,
          "title": "TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search. (arXiv:2105.11871v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11486",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nalwade_A/0/1/0/all/0/1\">Ashwin Nalwade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kisa_J/0/1/0/all/0/1\">Jackie Kisa</a>",
          "description": "Multi-modal magnetic resonance imaging (MRI) is a crucial method for\nanalyzing the human brain. It is usually used for diagnosing diseases and for\nmaking valuable decisions regarding the treatments - for instance, checking for\ngliomas in the human brain. With varying degrees of severity and detection,\nproperly diagnosing gliomas is one of the most daunting and significant\nanalysis tasks in modern-day medicine. Our primary focus is on working with\ndifferent approaches to perform the segmentation of brain tumors in multimodal\nMRI scans. Now, the quantity, variability of the data used for training has\nalways been considered to be crucial for developing excellent models. Hence, we\nalso want to experiment with Knowledge Distillation techniques.",
          "link": "http://arxiv.org/abs/2105.11486",
          "publishedOn": "2021-05-26T01:22:10.725Z",
          "wordCount": null,
          "title": "Experimenting with Knowledge Distillation techniques for performing Brain Tumor Segmentation. (arXiv:2105.11486v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongjing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_I/0/1/0/all/0/1\">Ian Davidson</a>",
          "description": "Recent work on explainable clustering allows describing clusters when the\nfeatures are interpretable. However, much modern machine learning focuses on\ncomplex data such as images, text, and graphs where deep learning is used but\nthe raw features of data are not interpretable. This paper explores a novel\nsetting for performing clustering on complex data while simultaneously\ngenerating explanations using interpretable tags. We propose deep descriptive\nclustering that performs sub-symbolic representation learning on complex data\nwhile generating explanations based on symbolic data. We form good clusters by\nmaximizing the mutual information between empirical distribution on the inputs\nand the induced clustering labels for clustering objectives. We generate\nexplanations by solving an integer linear programming that generates concise\nand orthogonal descriptions for each cluster. Finally, we allow the explanation\nto inform better clustering by proposing a novel pairwise loss with\nself-generated constraints to maximize the clustering and explanation module's\nconsistency. Experimental results on public data demonstrate that our model\noutperforms competitive baselines in clustering performance while offering\nhigh-quality cluster-level explanations.",
          "link": "http://arxiv.org/abs/2105.11549",
          "publishedOn": "2021-05-26T01:22:10.722Z",
          "wordCount": null,
          "title": "Deep Descriptive Clustering. (arXiv:2105.11549v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Corujo_L/0/1/0/all/0/1\">Luis A. Corujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">Peter A. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kieson_E/0/1/0/all/0/1\">Emily Kieson</a>",
          "description": "Creating intelligent systems capable of recognizing emotions is a difficult\ntask, especially when looking at emotions in animals. This paper describes the\nprocess of designing a \"proof of concept\" system to recognize emotions in\nhorses. This system is formed by two elements, a detector and a model. The\ndetector is a faster region-based convolutional neural network that detects\nhorses in an image. The second one, the model, is a convolutional neural\nnetwork that predicts the emotion of those horses. These two models were\ntrained with multiple images of horses until they achieved high accuracy in\ntheir tasks, creating therefore the desired system. 400 images of horses were\nused to train both the detector and the model while 80 were used to validate\nthe system. Once the two components were validated they were combined into a\ntestable system that would detect equine emotions based on established\nbehavioral ethograms indicating emotional affect through head, neck, ear,\nmuzzle, and eye position. The system showed an accuracy of between 69% and 74%\non the validation set, demonstrating that it is possible to predict emotions in\nanimals using autonomous intelligent systems. It is a first \"proof of concept\"\napproach that can be enhanced in many ways. Such a system has multiple\napplications including further studies in the growing field of animal emotions\nas well as in the veterinary field to determine the physical welfare of horses\nor other livestock.",
          "link": "http://arxiv.org/abs/2105.11953",
          "publishedOn": "2021-05-26T01:22:10.692Z",
          "wordCount": null,
          "title": "Emotion Recognition in Horses with Convolutional Neural Networks. (arXiv:2105.11953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziatdinov_M/0/1/0/all/0/1\">Maxim Ziatdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_M/0/1/0/all/0/1\">Muammer Yusuf Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginger_D/0/1/0/all/0/1\">David Ginger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinin_S/0/1/0/all/0/1\">Sergei V. Kalinin</a>",
          "description": "The proliferation of optical, electron, and scanning probe microscopies gives\nrise to large volumes of imaging data of objects as diversified as cells,\nbacteria, pollen, to nanoparticles and atoms and molecules. In most cases, the\nexperimental data streams contain images having arbitrary rotations and\ntranslations within the image. At the same time, for many cases, small amounts\nof labeled data are available in the form of prior published results, image\ncollections, and catalogs, or even theoretical models. Here we develop an\napproach that allows generalizing from a small subset of labeled data with a\nweak orientational disorder to a large unlabeled dataset with a much stronger\norientational (and positional) disorder, i.e., it performs a classification of\nimage data given a small number of examples even in the presence of a\ndistribution shift between the labeled and unlabeled parts. This approach is\nbased on the semi-supervised rotationally invariant variational autoencoder\n(ss-rVAE) model consisting of the encoder-decoder \"block\" that learns a\nrotationally (and translationally) invariant continuous latent representation\nof data and a classifier that encodes data into a finite number of discrete\nclasses. The classifier part of the trained ss-rVAE inherits the rotational\n(and translational) invariances and can be deployed independently of the other\nparts of the model. The performance of the ss-rVAE is illustrated using the\nsynthetic data sets with known factors of variation. We further demonstrate its\napplication for experimental data sets of nanoparticles, creating nanoparticle\nlibraries and disentangling the representations defining the physical factors\nof variation in the data. The code reproducing the results is available at\nhttps://github.com/ziatdinovmax/Semi-Supervised-VAE-nanoparticles.",
          "link": "http://arxiv.org/abs/2105.11475",
          "publishedOn": "2021-05-26T01:22:10.680Z",
          "wordCount": null,
          "title": "Semi-supervised learning of images with strong rotational disorder: assembling nanoparticle libraries. (arXiv:2105.11475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Archibald_T/0/1/0/all/0/1\">Taylor Archibald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggemann_M/0/1/0/all/0/1\">Mason Poggemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_T/0/1/0/all/0/1\">Tony Martinez</a>",
          "description": "Stroke order and velocity are helpful features in the fields of signature\nverification, handwriting recognition, and handwriting synthesis. Recovering\nthese features from offline handwritten text is a challenging and well-studied\nproblem. We propose a new model called TRACE (Trajectory Recovery by an\nAdaptively-trained Convolutional Encoder). TRACE is a differentiable approach\nthat uses a convolutional recurrent neural network (CRNN) to infer temporal\nstroke information from long lines of offline handwritten text with many\ncharacters and dynamic time warping (DTW) to align predictions and ground truth\npoints. TRACE is perhaps the first system to be trained end-to-end on entire\nlines of text of arbitrary width and does not require the use of dynamic\nexemplars. Moreover, the system does not require images to undergo any\npre-processing, nor do the predictions require any post-processing.\nConsequently, the recovered trajectory is differentiable and can be used as a\nloss function for other tasks, including synthesizing offline handwritten text.\n\nWe demonstrate that temporal stroke information recovered by TRACE from\noffline data can be used for handwriting synthesis and establish the first\nbenchmarks for a stroke trajectory recovery system trained on the IAM online\nhandwriting dataset.",
          "link": "http://arxiv.org/abs/2105.11559",
          "publishedOn": "2021-05-26T01:22:10.676Z",
          "wordCount": null,
          "title": "TRACE: A Differentiable Approach to Line-level Stroke Recovery for Offline Handwritten Text. (arXiv:2105.11559v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11863",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Avetisian_M/0/1/0/all/0/1\">Manvel Avetisian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burenko_I/0/1/0/all/0/1\">Ilya Burenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egorov_K/0/1/0/all/0/1\">Konstantin Egorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nesterov_A/0/1/0/all/0/1\">Aleksandr Nesterov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nikolaev_A/0/1/0/all/0/1\">Aleksandr Nikolaev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponomarchuk_A/0/1/0/all/0/1\">Alexander Ponomarchuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sokolova_E/0/1/0/all/0/1\">Elena Sokolova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tuzhilin_A/0/1/0/all/0/1\">Alex Tuzhilin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umerenkov_D/0/1/0/all/0/1\">Dmitry Umerenkov</a>",
          "description": "Analysis of chest CT scans can be used in detecting parts of lungs that are\naffected by infectious diseases such as COVID-19.Determining the volume of\nlungs affected by lesions is essential for formulating treatment\nrecommendations and prioritizingpatients by severity of the disease. In this\npaper we adopted an approach based on using an ensemble of deep\nconvolutionalneural networks for segmentation of slices of lung CT scans. Using\nour models we are able to segment the lesions, evaluatepatients dynamics,\nestimate relative volume of lungs affected by lesions and evaluate the lung\ndamage stage. Our modelswere trained on data from different medical centers. We\ncompared predictions of our models with those of six experiencedradiologists\nand our segmentation model outperformed most of them. On the task of\nclassification of disease severity, ourmodel outperformed all the radiologists.",
          "link": "http://arxiv.org/abs/2105.11863",
          "publishedOn": "2021-05-26T01:22:10.573Z",
          "wordCount": 647,
          "title": "CoRSAI: A System for Robust Interpretation of CT Scans of COVID-19 Patients Using Deep Learning. (arXiv:2105.11863v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hickmann_M/0/1/0/all/0/1\">M. Lautaro Hickmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wurzberger_F/0/1/0/all/0/1\">Fabian Wurzberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoxhalli_M/0/1/0/all/0/1\">Megi Hoxhalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lochner_A/0/1/0/all/0/1\">Arne Lochner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tollich_J/0/1/0/all/0/1\">Jessica T&#xf6;llich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>",
          "description": "Modern multi-document summarization (MDS) methods are based on transformer\narchitectures. They generate state of the art summaries, but lack\nexplainability. We focus on graph-based transformer models for MDS as they\ngained recent popularity. We aim to improve the explainability of the\ngraph-based MDS by analyzing their attention weights. In a graph-based MDS such\nas GraphSum, vertices represent the textual units, while the edges form some\nsimilarity graph over the units. We compare GraphSum's performance utilizing\ndifferent textual units, i. e., sentences versus paragraphs, on two news\nbenchmark datasets, namely WikiSum and MultiNews. Our experiments show that\nparagraph-level representations provide the best summarization performance.\nThus, we subsequently focus oAnalysisn analyzing the paragraph-level attention\nweights of GraphSum's multi-heads and decoding layers in order to improve the\nexplainability of a transformer-based MDS model. As a reference metric, we\ncalculate the ROUGE scores between the input paragraphs and each sentence in\nthe generated summary, which indicate source origin information via text\nsimilarity. We observe a high correlation between the attention weights and\nthis reference metric, especially on the the later decoding layers of the\ntransformer architecture. Finally, we investigate if the generated summaries\nfollow a pattern of positional bias by extracting which paragraph provided the\nmost information for each generated summary. Our results show that there is a\nhigh correlation between the position in the summary and the source origin.",
          "link": "http://arxiv.org/abs/2105.11908",
          "publishedOn": "2021-05-26T01:22:10.567Z",
          "wordCount": 669,
          "title": "Analysis of GraphSum's Attention Weights to Improve the Explainability of Multi-Document Summarization. (arXiv:2105.11908v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00075",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Raymond_S/0/1/0/all/0/1\">Samuel J. Raymond</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Camarillo_D/0/1/0/all/0/1\">David B. Camarillo</a>",
          "description": "Physics-Informed Machine Learning (PIML) has gained momentum in the last 5\nyears with scientists and researchers aiming to utilize the benefits afforded\nby advances in machine learning, particularly in deep learning. With large\nscientific data sets with rich spatio-temporal data and high-performance\ncomputing providing large amounts of data to be inferred and interpreted, the\ntask of PIML is to ensure that these predictions, categorizations, and\ninferences are enforced by, and conform to the limits imposed by physical laws.\nIn this work a new approach to utilizing PIML is discussed that deals with the\nuse of physics-based loss functions. While typical usage of physical equations\nin the loss function requires complex layers of derivatives and other functions\nto ensure that the known governing equation is satisfied, here we show that a\nsimilar level of enforcement can be found by implementing more simpler loss\nfunctions on specific kinds of output data. The generalizability that this\napproach affords is shown using examples of simple mechanical models that can\nbe thought of as sufficiently simplified surrogate models for a wide class of\nproblems.",
          "link": "http://arxiv.org/abs/2105.00075",
          "publishedOn": "2021-05-26T01:22:10.550Z",
          "wordCount": 645,
          "title": "Applying physics-based loss functions to neural networks for improved generalizability in mechanics problems. (arXiv:2105.00075v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shehnepoor_S/0/1/0/all/0/1\">Saeedreza Shehnepoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togneri_R/0/1/0/all/0/1\">Roberto Togneri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>",
          "description": "Social reviews are indispensable resources for modern consumers' decision\nmaking. For financial gain, companies pay fraudsters preferably in groups to\ndemote or promote products and services since consumers are more likely to be\nmisled by a large number of similar reviews from groups. Recent approaches on\nfraudster group detection employed handcrafted features of group behaviors\nwithout considering the semantic relation between reviews from the reviewers in\na group. In this paper, we propose the first neural approach, HIN-RNN, a\nHeterogeneous Information Network (HIN) Compatible RNN for fraudster group\ndetection that requires no handcrafted features. HIN-RNN provides a unifying\narchitecture for representation learning of each reviewer, with the initial\nvector as the sum of word embeddings of all review text written by the same\nreviewer, concatenated by the ratio of negative reviews. Given a co-review\nnetwork representing reviewers who have reviewed the same items with the same\nratings and the reviewers' vector representation, a collaboration matrix is\nacquired through HIN-RNN training. The proposed approach is confirmed to be\neffective with marked improvement over state-of-the-art approaches on both the\nYelp (22% and 12% in terms of recall and F1-value, respectively) and Amazon (4%\nand 2% in terms of recall and F1-value, respectively) datasets.",
          "link": "http://arxiv.org/abs/2105.11602",
          "publishedOn": "2021-05-26T01:22:10.534Z",
          "wordCount": 650,
          "title": "HIN-RNN: A Graph Representation Learning Neural Network for Fraudster Group Detection With No Handcrafted Features. (arXiv:2105.11602v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1810.10625",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gopalakrishnan_S/0/1/0/all/0/1\">Soorya Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Marzi_Z/0/1/0/all/0/1\">Zhinus Marzi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cekic_M/0/1/0/all/0/1\">Metehan Cekic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Madhow_U/0/1/0/all/0/1\">Upamanyu Madhow</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pedarsani_R/0/1/0/all/0/1\">Ramtin Pedarsani</a>",
          "description": "It is by now well-known that small adversarial perturbations can induce\nclassification errors in deep neural networks. In this paper, we take a\nbottom-up signal processing perspective to this problem and show that a\nsystematic exploitation of sparsity in natural data is a promising tool for\ndefense. For linear classifiers, we show that a sparsifying front end is\nprovably effective against $\\ell_{\\infty}$-bounded attacks, reducing output\ndistortion due to the attack by a factor of roughly $K/N$ where $N$ is the data\ndimension and $K$ is the sparsity level. We then extend this concept to deep\nnetworks, showing that a \"locally linear\" model can be used to develop a\ntheoretical foundation for crafting attacks and defenses. We also devise\nattacks based on the locally linear model that outperform the well-known FGSM\nattack. We supplement our theoretical results with experiments on the MNIST and\nCIFAR-10 datasets, showing the efficacy of the proposed sparsity-based defense\nschemes.",
          "link": "http://arxiv.org/abs/1810.10625",
          "publishedOn": "2021-05-26T01:22:10.517Z",
          "wordCount": 620,
          "title": "Robust Adversarial Learning via Sparsifying Front Ends. (arXiv:1810.10625v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1\">Tedo Vrbanec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1\">Ana Mestrovic</a>",
          "description": "The article gives an overview of the plagiarism domain, with focus on\nacademic plagiarism. The article defines plagiarism, explains the origin of the\nterm, as well as plagiarism related terms. It identifies the extent of the\nplagiarism domain and then focuses on the plagiarism subdomain of text\ndocuments, for which it gives an overview of current classifications and\ntaxonomies and then proposes a more comprehensive classification according to\nseveral criteria: their origin and purpose, technical implementation,\nconsequence, complexity of detection and according to the number of linguistic\nsources. The article suggests the new classification of academic plagiarism,\ndescribes sorts and methods of plagiarism, types and categories, approaches and\nphases of plagiarism detection, the classification of methods and algorithms\nfor plagiarism detection. The title of the article explicitly targets the\nacademic community, but it is sufficiently general and interdisciplinary, so it\ncan be useful for many other professionals like software developers, linguists\nand librarians.",
          "link": "http://arxiv.org/abs/2105.12068",
          "publishedOn": "2021-05-26T01:22:10.511Z",
          "wordCount": 612,
          "title": "Taxonomy of academic plagiarism methods. (arXiv:2105.12068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.10653",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bellot_A/0/1/0/all/0/1\">Alexis Bellot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "The ability to generalize from observed to new related environments is\ncentral to any form of reliable machine learning, yet most methods fail when\nmoving beyond i.i.d data. This work argues that in some cases the reason lies\nin a misapreciation of the causal structure in data; and in particular due to\nthe influence of unobserved confounders which void many of the invariances and\nprinciples of minimum error between environments presently used for the problem\nof domain generalization. This observation leads us to study generalization in\nthe context of a broader class of interventions in an underlying causal model\n(including changes in observed, unobserved and target variable distributions)\nand to connect this causal intuition with an explicit distributionally robust\noptimization problem. From this analysis derives a new proposal for model\nlearning with explicit generalization guarantees that is based on the partial\nequality of error derivatives with respect to model parameters. We demonstrate\nthe empirical performance of our approach on healthcare data from different\nmodalities, including image, speech and tabular data.",
          "link": "http://arxiv.org/abs/2007.10653",
          "publishedOn": "2021-05-26T01:22:10.495Z",
          "wordCount": 638,
          "title": "Accounting for Unobserved Confounding in Domain Generalization. (arXiv:2007.10653v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mengxiang Lin</a>",
          "description": "The application of reinforcement learning (RL) in robotic control is still\nlimited in the environments with sparse and delayed rewards. In this paper, we\npropose a practical self-imitation learning method named Self-Imitation\nLearning with Constant Reward (SILCR). Instead of requiring hand-defined\nimmediate rewards from environments, our method assigns the immediate rewards\nat each timestep with constant values according to their final episodic\nrewards. In this way, even if the dense rewards from environments are\nunavailable, every action taken by the agents would be guided properly. We\ndemonstrate the effectiveness of our method in some challenging continuous\nrobotics control tasks in MuJoCo simulation and the results show that our\nmethod significantly outperforms the alternative methods in tasks with sparse\nand delayed rewards. Even compared with alternatives with dense rewards\navailable, our method achieves competitive performance. The ablation\nexperiments also show the stability and reproducibility of our method.",
          "link": "http://arxiv.org/abs/2010.06962",
          "publishedOn": "2021-05-26T01:22:10.474Z",
          "wordCount": 614,
          "title": "Self-Imitation Learning for Robot Tasks with Sparse and Delayed Rewards. (arXiv:2010.06962v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09001",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hellkvist_M/0/1/0/all/0/1\">Martin Hellkvist</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ozcelikkale_A/0/1/0/all/0/1\">Ay&#xe7;a &#xd6;z&#xe7;elikkale</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ahlen_A/0/1/0/all/0/1\">Anders Ahl&#xe9;n</a>",
          "description": "Distributed learning provides an attractive framework for scaling the\nlearning task by sharing the computational load over multiple nodes in a\nnetwork. Here, we investigate the performance of distributed learning for\nlarge-scale linear regression where the model parameters, i.e., the unknowns,\nare distributed over the network. We adopt a statistical learning approach. In\ncontrast to works that focus on the performance on the training data, we focus\non the generalization error, i.e., the performance on unseen data. We provide\nhigh-probability bounds on the generalization error for both isotropic and\ncorrelated Gaussian data as well as sub-gaussian data. These results reveal the\ndependence of the generalization performance on the partitioning of the model\nover the network. In particular, our results show that the generalization error\nof the distributed solution can be substantially higher than that of the\ncentralized solution even when the error on the training data is at the same\nlevel for both the centralized and distributed approaches. Our numerical\nresults illustrate the performance with both real-world image data as well as\nsynthetic data.",
          "link": "http://arxiv.org/abs/2101.09001",
          "publishedOn": "2021-05-26T01:22:10.419Z",
          "wordCount": 625,
          "title": "Linear Regression with Distributed Learning: A Generalization Error Perspective. (arXiv:2101.09001v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yumeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Long Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "Federated learning (FL), as a distributed machine learning paradigm, promotes\npersonal privacy by local data processing at each client. However, relying on a\ncentralized server for model aggregation, standard FL is vulnerable to server\nmalfunctions, untrustworthy server, and external attacks. To address this\nissue, we propose a decentralized FL framework by integrating blockchain into\nFL, namely, blockchain assisted decentralized federated learning (BLADE-FL). In\na round of the proposed BLADE-FL, each client broadcasts the trained model to\nother clients, aggregates its own model with received ones, and then competes\nto generate a block before its local training of the next round. We evaluate\nthe learning performance of BLADE-FL, and develop an upper bound on the global\nloss function. Then we verify that this bound is convex with respect to the\nnumber of overall aggregation rounds K, and optimize the computing resource\nallocation for minimizing the upper bound. We also note that there is a\ncritical problem of training deficiency, caused by lazy clients who plagiarize\nothers' trained models and add artificial noises to disguise their cheating\nbehaviors. Focusing on this problem, we explore the impact of lazy clients on\nthe learning performance of BLADE-FL, and characterize the relationship among\nthe optimal K, the learning parameters, and the proportion of lazy clients.\nBased on MNIST and Fashion-MNIST datasets, we show that the experimental\nresults are consistent with the analytical ones. To be specific, the gap\nbetween the developed upper bound and experimental results is lower than 5%,\nand the optimized K based on the upper bound can effectively minimize the loss\nfunction.",
          "link": "http://arxiv.org/abs/2101.06905",
          "publishedOn": "2021-05-26T01:22:10.401Z",
          "wordCount": 730,
          "title": "Blockchain Assisted Decentralized Federated Learning (BLADE-FL): Performance Analysis and Resource Allocation. (arXiv:2101.06905v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mallasto_A/0/1/0/all/0/1\">Anton Mallasto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arndt_K/0/1/0/all/0/1\">Karol Arndt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1\">Markus Heinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>",
          "description": "Sample-efficient domain adaptation is an open problem in robotics. In this\npaper, we present affine transport -- a variant of optimal transport, which\nmodels the mapping between state transition distributions between the source\nand target domains with an affine transformation. First, we derive the affine\ntransport framework; then, we extend the basic framework with Procrustes\nalignment to model arbitrary affine transformations. We evaluate the method in\na number of OpenAI Gym sim-to-sim experiments with simulation environments, as\nwell as on a sim-to-real domain adaptation task of a robot hitting a hockeypuck\nsuch that it slides and stops at a target position. In each experiment, we\nevaluate the results when transferring between each pair of dynamics domains.\nThe results show that affine transport can significantly reduce the model\nadaptation error in comparison to using the original, non-adapted dynamics\nmodel.",
          "link": "http://arxiv.org/abs/2105.11739",
          "publishedOn": "2021-05-26T01:22:10.394Z",
          "wordCount": 562,
          "title": "Affine Transport for Sim-to-Real Domain Adaptation. (arXiv:2105.11739v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11522",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ballesio_M/0/1/0/all/0/1\">Marco Ballesio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jasra_A/0/1/0/all/0/1\">Ajay Jasra</a>",
          "description": "In this paper, we consider static parameter estimation for a class of\ncontinuous-time state-space models. Our goal is to obtain an unbiased estimate\nof the gradient of the log-likelihood (score function), which is an estimate\nthat is unbiased even if the stochastic processes involved in the model must be\ndiscretized in time. To achieve this goal, we apply a \\emph{doubly randomized\nscheme} (see, e.g.,~\\cite{ub_mcmc, ub_grad}), that involves a novel coupled\nconditional particle filter (CCPF) on the second level of randomization\n\\cite{jacob2}. Our novel estimate helps facilitate the application of\ngradient-based estimation algorithms, such as stochastic-gradient Langevin\ndescent. We illustrate our methodology in the context of stochastic gradient\ndescent (SGD) in several numerical examples and compare with the Rhee \\& Glynn\nestimator \\cite{rhee,vihola}.",
          "link": "http://arxiv.org/abs/2105.11522",
          "publishedOn": "2021-05-26T01:22:10.381Z",
          "wordCount": 568,
          "title": "Unbiased Estimation of the Gradient of the Log-Likelihood for a Class of Continuous-Time State-Space Models. (arXiv:2105.11522v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armi_L/0/1/0/all/0/1\">Laleh Armi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_E/0/1/0/all/0/1\">Elham Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarepour_Ahmadabadi_J/0/1/0/all/0/1\">Jamal Zarepour-Ahmadabadi</a>",
          "description": "Mixture of experts method is a neural network based ensemble learning that\nhas great ability to improve the overall classification accuracy. This method\nis based on the divide and conquer principle, in which the problem space is\ndivided between several experts by supervisition of gating network. In this\npaper, we propose an ensemble learning method based on mixture of experts which\nis named mixture of ELM based experts with trainable gating network (MEETG) to\nimprove the computing cost and to speed up the learning process of ME. The\nstructure of ME consists of multi layer perceptrons (MLPs) as base experts and\ngating network, in which gradient-based learning algorithm is applied for\ntraining the MLPs which is an iterative and time consuming process. In order to\novercome on these problems, we use the advantages of extreme learning machine\n(ELM) for designing the structure of ME. ELM as a learning algorithm for single\nhidden-layer feed forward neural networks provides much faster learning process\nand better generalization ability in comparision with some other traditional\nlearning algorithms. Also, in the proposed method a trainable gating network is\napplied to aggregate the outputs of the experts dynamically according to the\ninput sample. Our experimental results and statistical analysis on 11 benchmark\ndatasets confirm that MEETG has an acceptable performance in classification\nproblems. Furthermore, our experimental results show that the proposed approach\noutperforms the original ELM on prediction stability and classification\naccuracy.",
          "link": "http://arxiv.org/abs/2105.11706",
          "publishedOn": "2021-05-26T01:22:09.886Z",
          "wordCount": null,
          "title": "Mixture of ELM based experts with trainable gating network. (arXiv:2105.11706v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajalan_S/0/1/0/all/0/1\">Saeed Bajalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajalan_N/0/1/0/all/0/1\">Nastaran Bajalan</a>",
          "description": "The main aim of this study is to introduce a 2-layered Artificial Neural\nNetwork (ANN) for solving the Black-Scholes partial differential equation (PDE)\nof either fractional or ordinary orders. Firstly, a discretization method is\nemployed to change the model into a sequence of Ordinary Differential Equations\n(ODE). Then each of these ODEs is solved with the aid of an ANN. Adam\noptimization is employed as the learning paradigm since it can add the\nforeknowledge of slowing down the process of optimization when getting close to\nthe actual optimum solution. The model also takes advantage of fine tuning for\nspeeding up the process and domain mapping to confront infinite domain issue.\nFinally, the accuracy, speed, and convergence of the method for solving several\ntypes of Black-Scholes model are reported.",
          "link": "http://arxiv.org/abs/2105.11240",
          "publishedOn": "2021-05-25T01:56:12.680Z",
          "wordCount": 557,
          "title": "Novel ANN method for solving ordinary and fractional Black-Scholes equation. (arXiv:2105.11240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">ChangQing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">XinFang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">JiaLiang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">YinTang Yang</a>",
          "description": "In this paper, a novel machine learning regression based single event\ntransient (SET) modeling method is proposed. The proposed method can obtain a\nreasonable and accurate model without considering the complex physical\nmechanism. We got plenty of SET current data of SMIC 130nm bulk CMOS by TCAD\nsimulation under different conditions (e.g. different LET and different drain\nbias voltage). A multilayer feedfordward neural network is used to build the\nSET pulse current model by learning the data from TCAD simulation. The proposed\nmodel is validated with the simulation results from TCAD simulation. The\ntrained SET pulse current model is implemented as a Verilog-A current source in\nthe Cadence Spectre circuit simulator and an inverter with five fan-outs is\nused to show the practicability and reasonableness of the proposed SET pulse\ncurrent model for circuit-level single-event effect (SEE) simulation.",
          "link": "http://arxiv.org/abs/2105.10723",
          "publishedOn": "2021-05-25T01:56:12.648Z",
          "wordCount": 572,
          "title": "Machine Learning Regression based Single Event Transient Modeling Method for Circuit-Level Simulation. (arXiv:2105.10723v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wenhao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_S/0/1/0/all/0/1\">Shicong Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1\">Yuejie Chi</a>",
          "description": "Policy optimization, which learns the policy of interest by maximizing the\nvalue function via large-scale optimization techniques, lies at the heart of\nmodern reinforcement learning (RL). In addition to value maximization, other\npractical considerations arise commonly as well, including the need of\nencouraging exploration, and that of ensuring certain structural properties of\nthe learned policy due to safety, resource and operational constraints. These\nconsiderations can often be accounted for by resorting to regularized RL, which\naugments the target value function with a structure-promoting regularization\nterm.\n\nFocusing on an infinite-horizon discounted Markov decision process, this\npaper proposes a generalized policy mirror descent (GPMD) algorithm for solving\nregularized RL. As a generalization of policy mirror descent Lan (2021), the\nproposed algorithm accommodates a general class of convex regularizers as well\nas a broad family of Bregman divergence in cognizant of the regularizer in use.\nWe demonstrate that our algorithm converges linearly over an entire range of\nlearning rates, in a dimension-free fashion, to the global solution, even when\nthe regularizer lacks strong convexity and smoothness. In addition, this linear\nconvergence feature is provably stable in the face of inexact policy evaluation\nand imperfect policy updates. Numerical experiments are provided to corroborate\nthe applicability and appealing performance of GPMD.",
          "link": "http://arxiv.org/abs/2105.11066",
          "publishedOn": "2021-05-25T01:56:12.425Z",
          "wordCount": 661,
          "title": "Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence. (arXiv:2105.11066v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11418",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruijiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saar_tsechansky_M/0/1/0/all/0/1\">Maytal Saar-tsechansky</a>",
          "description": "Conventional active learning algorithms assume a single labeler that produces\nnoiseless label at a given, fixed cost, and aim to achieve the best\ngeneralization performance for given classifier under a budget constraint.\nHowever, in many real settings, different labelers have different labeling\ncosts and can yield different labeling accuracies. Moreover, a given labeler\nmay exhibit different labeling accuracies for different instances. This setting\ncan be referred to as active learning with diverse labelers with varying costs\nand accuracies, and it arises in many important real settings. It is therefore\nbeneficial to understand how to effectively trade-off between labeling accuracy\nfor different instances, labeling costs, as well as the informativeness of\ntraining instances, so as to achieve the best generalization performance at the\nlowest labeling cost. In this paper, we propose a new algorithm for selecting\ninstances, labelers (and their corresponding costs and labeling accuracies),\nthat employs generalization bound of learning with label noise to select\ninformative instances and labelers so as to achieve higher generalization\naccuracy at a lower cost. Our proposed algorithm demonstrates state-of-the-art\nperformance on five UCI and a real crowdsourcing dataset.",
          "link": "http://arxiv.org/abs/2105.11418",
          "publishedOn": "2021-05-25T01:56:12.417Z",
          "wordCount": 611,
          "title": "Cost-Accuracy Aware Adaptive Labeling for Active Learning. (arXiv:2105.11418v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>",
          "description": "Pretrained language models (LMs) perform well on many tasks even when\nlearning from a few examples, but prior work uses many held-out examples to\ntune various aspects of learning, such as hyperparameters, training objectives,\nand natural language templates (\"prompts\"). Here, we evaluate the few-shot\nability of LMs when such held-out examples are unavailable, a setting we call\ntrue few-shot learning. We test two model selection criteria, cross-validation\nand minimum description length, for choosing LM prompts and hyperparameters in\nthe true few-shot setting. On average, both marginally outperform random\nselection and greatly underperform selection based on held-out examples.\nMoreover, selection criteria often prefer models that perform significantly\nworse than randomly-selected ones. We find similar results even when taking\ninto account our uncertainty in a model's true performance during selection, as\nwell as when varying the amount of computation and number of examples used for\nselection. Overall, our findings suggest that prior work significantly\noverestimated the true few-shot ability of LMs given the difficulty of few-shot\nmodel selection.",
          "link": "http://arxiv.org/abs/2105.11447",
          "publishedOn": "2021-05-25T01:56:12.347Z",
          "wordCount": 601,
          "title": "True Few-Shot Learning with Language Models. (arXiv:2105.11447v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Tung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kui_W/0/1/0/all/0/1\">Wu Kui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aw_A/0/1/0/all/0/1\">Ai Ti Aw</a>",
          "description": "Recent unsupervised machine translation (UMT) systems usually employ three\nmain principles: initialization, language modeling and iterative\nback-translation, though they may apply them differently. Crucially, iterative\nback-translation and denoising auto-encoding for language modeling provide data\ndiversity to train the UMT systems. However, the gains from these\ndiversification processes has seemed to plateau. We introduce a novel component\nto the standard UMT framework called Cross-model Back-translated Distillation\n(CBD), that is aimed to induce another level of data diversification that\nexisting principles lack. CBD is applicable to all previous UMT approaches. In\nour experiments, CBD achieves the state of the art in the WMT'14\nEnglish-French, WMT'16 English-German and English-Romanian bilingual\nunsupervised translation tasks, with 38.2, 30.1, and 36.3 BLEU respectively. It\nalso yields 1.5-3.3 BLEU improvements in IWSLT English-French and\nEnglish-German tasks. Through extensive experimental analyses, we show that CBD\nis effective because it embraces data diversity while other similar variants do\nnot.",
          "link": "http://arxiv.org/abs/2006.02163",
          "publishedOn": "2021-05-25T01:56:12.339Z",
          "wordCount": 636,
          "title": "Cross-model Back-translated Distillation for Unsupervised Machine Translation. (arXiv:2006.02163v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1\">Yaron Shoham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elidan_G/0/1/0/all/0/1\">Gal Elidan</a>",
          "description": "Despite seminal advances in reinforcement learning in recent years, many\ndomains where the rewards are sparse, e.g. given only at task completion,\nremain quite challenging. In such cases, it can be beneficial to tackle the\ntask both from its beginning and end, and make the two ends meet. Existing\napproaches that do so, however, are not effective in the common scenario where\nthe strategy needed near the end goal is very different from the one that is\neffective earlier on.\n\nIn this work we propose a novel RL approach for such settings. In short, we\nfirst train a backward-looking agent with a simple relaxed goal, and then\naugment the state representation of the forward-looking agent with\nstraightforward hint features. This allows the learned forward agent to\nleverage information from backward plans, without mimicking their policy.\n\nWe demonstrate the efficacy of our approach on the challenging game of\nSokoban, where we substantially surpass learned solvers that generalize across\nlevels, and are competitive with SOTA performance of the best highly-crafted\nsystems. Impressively, we achieve these results while learning from a small\nnumber of practice levels and using simple RL techniques.",
          "link": "http://arxiv.org/abs/2105.01904",
          "publishedOn": "2021-05-25T01:56:12.333Z",
          "wordCount": 635,
          "title": "Solving Sokoban with forward-backward reinforcement learning. (arXiv:2105.01904v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11241",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mann_P/0/1/0/all/0/1\">Prerak Mann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jain_S/0/1/0/all/0/1\">Sahaj Jain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mittal_S/0/1/0/all/0/1\">Saurabh Mittal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhat_A/0/1/0/all/0/1\">Aruna Bhat</a>",
          "description": "SARS-CoV-2, also known as COVID-19 or Coronavirus, is a viral contagious\ndisease that is infected by a novel coronavirus, and has been rapidly spreading\nacross the globe. It is very important to test and isolate people to reduce\nspread, and from here comes the need to do this quickly and efficiently.\nAccording to some studies, Chest-CT outperforms RT-PCR lab testing, which is\nthe current standard, when diagnosing COVID-19 patients. Due to this, computer\nvision researchers have developed various deep learning systems that can\npredict COVID-19 using a Chest-CT scan correctly to a certain degree. The\naccuracy of these systems is limited since deep learning neural networks such\nas CNNs (Convolutional Neural Networks) need a significantly large quantity of\ndata for training in order to produce good quality results. Since the disease\nis relatively recent and more focus has been on CXR (Chest XRay) images, the\navailable chest CT Scan image dataset is much less. We propose a method, by\nutilizing GANs, to generate synthetic chest CT images of both positive and\nnegative COVID-19 patients. Using a pre-built predictive model, we concluded\nthat around 40% of the generated images are correctly predicted as COVID-19\npositive. The dataset thus generated can be used to train a CNN-based\nclassifier which can help determine COVID-19 in a patient with greater\naccuracy.",
          "link": "http://arxiv.org/abs/2105.11241",
          "publishedOn": "2021-05-25T01:56:12.325Z",
          "wordCount": 709,
          "title": "Generation of COVID-19 Chest CT Scan Images using Generative Adversarial Networks. (arXiv:2105.11241v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Andr&#xe9;s C. Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan D.Wegner</a>",
          "description": "Accurate mapping of oil palm is important for understanding its past and\nfuture impact on the environment. We propose to map and count oil palms by\nestimating tree densities per pixel for large-scale analysis. This allows for\nfine-grained analysis, for example regarding different planting patterns. To\nthat end, we propose a new, active deep learning method to estimate oil palm\ndensity at large scale from Sentinel-2 satellite images, and apply it to\ngenerate complete maps for Malaysia and Indonesia. What makes the regression of\noil palm density challenging is the need for representative reference data that\ncovers all relevant geographical conditions across a large territory.\nSpecifically for density estimation, generating reference data involves\ncounting individual trees. To keep the associated labelling effort low we\npropose an active learning (AL) approach that automatically chooses the most\nrelevant samples to be labelled. Our method relies on estimates of the\nepistemic model uncertainty and of the diversity among samples, making it\npossible to retrieve an entire batch of relevant samples in a single iteration.\nMoreover, our algorithm has linear computational complexity and is easily\nparallelisable to cover large areas. We use our method to compute the first oil\npalm density map with $10\\,$m Ground Sampling Distance (GSD) , for all of\nIndonesia and Malaysia and for two different years, 2017 and 2019. The maps\nhave a mean absolute error of $\\pm$7.3 trees/$ha$, estimated from an\nindependent validation set. We also analyse density variations between\ndifferent states within a country and compare them to official estimates.\nAccording to our estimates there are, in total, $>1.2$ billion oil palms in\nIndonesia covering $>$15 million $ha$, and $>0.5$ billion oil palms in Malaysia\ncovering $>6$ million $ha$.",
          "link": "http://arxiv.org/abs/2105.11207",
          "publishedOn": "2021-05-25T01:56:12.311Z",
          "wordCount": 735,
          "title": "Mapping oil palm density at country scale: An active learning approach. (arXiv:2105.11207v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hannah Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1\">Girmaw Abebe Tadesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1\">Celia Cintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1\">Skyler Speakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1\">Kush Varshney</a>",
          "description": "Recent advances in deep learning have led to breakthroughs in the development\nof automated skin disease classification. As we observe an increasing interest\nin these models in the dermatology space, it is crucial to address aspects such\nas the robustness towards input data distribution shifts. Current skin disease\nmodels could make incorrect inferences for test samples from different hardware\ndevices and clinical settings or unknown disease samples, which are\nout-of-distribution (OOD) from the training samples.To this end, we propose a\nsimple yet effective approach that detect these OOD samples prior to making any\ndecision. The detection is performed via scanning in the latent space\nrepresentation (e.g., activations of the inner layers of any pre-trained skin\ndisease classifier). The input samples could also perturbed to maximise\ndivergence of OOD samples. We validate our ODD detection approach in two use\ncases: 1) identify samples collected from different protocols, and 2) detect\nsamples from unknown disease classes. Additionally, we evaluate the performance\nof the proposed approach and compare it with other state-of-the-art methods.\nFurthermore, data-driven dermatology applications may deepen the disparity in\nclinical care across racial and ethnic groups since most datasets are reported\nto suffer from bias in skin tone distribution. Therefore, we also evaluate the\nfairness of these OOD detection methods across different skin tones. Our\nexperiments resulted in competitive performance across multiple datasets in\ndetecting OOD samples, which could be used (in the future) to design more\neffective transfer learning techniques prior to inferring on these samples.",
          "link": "http://arxiv.org/abs/2105.11160",
          "publishedOn": "2021-05-25T01:56:12.287Z",
          "wordCount": 699,
          "title": "Out-of-Distribution Detection in Dermatology using Input Perturbation and Subset Scanning. (arXiv:2105.11160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1909.03824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yongqiang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1\">Ming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yepang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1\">Shing-Chi Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>",
          "description": "Deep learning models are widely used for image analysis. While they offer\nhigh performance in terms of accuracy, people are concerned about if these\nmodels inappropriately make inferences using irrelevant features that are not\nencoded from the target object in a given image. To address the concern, we\npropose a metamorphic testing approach that assesses if a given inference is\nmade based on irrelevant features. Specifically, we propose two novel\nmetamorphic relations to detect such inappropriate inferences. We applied our\napproach to 10 image classification models and 10 object detection models, with\nthree large datasets, i.e., ImageNet, COCO, and Pascal VOC. Over 5.3% of the\ntop-5 correct predictions made by the image classification models are subject\nto inappropriate inferences using irrelevant features. The corresponding rate\nfor the object detection models is over 8.5%. Based on the findings, we further\ndesigned a new image generation strategy that can effectively attack existing\nmodels. Comparing with a baseline approach, our strategy can double the success\nrate of attacks.",
          "link": "http://arxiv.org/abs/1909.03824",
          "publishedOn": "2021-05-25T01:56:12.274Z",
          "wordCount": 693,
          "title": "Testing Deep Learning Models for Image Analysis Using Object-Relevant Metamorphic Relations. (arXiv:1909.03824v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Ting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shiping Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_A/0/1/0/all/0/1\">Aidong Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiying Wang</a>",
          "description": "Video prediction is a challenging task with wide application prospects in\nmeteorology and robot systems. Existing works fail to trade off short-term and\nlong-term prediction performances and extract robust latent dynamics laws in\nvideo frames. We propose a two-branch seq-to-seq deep model to disentangle the\nTaylor feature and the residual feature in video frames by a novel recurrent\nprediction module (TaylorCell) and residual module. TaylorCell can expand the\nvideo frames' high-dimensional features into the finite Taylor series to\ndescribe the latent laws. In TaylorCell, we propose the Taylor prediction unit\n(TPU) and the memory correction unit (MCU). TPU employs the first input frame's\nderivative information to predict the future frames, avoiding error\naccumulation. MCU distills all past frames' information to correct the\npredicted Taylor feature from TPU. Correspondingly, the residual module\nextracts the residual feature complementary to the Taylor feature. On three\ngeneralist datasets (Moving MNIST, TaxiBJ, Human 3.6), our model outperforms or\nreaches state-of-the-art models, and ablation experiments demonstrate the\neffectiveness of our model in long-term prediction.",
          "link": "http://arxiv.org/abs/2105.11062",
          "publishedOn": "2021-05-25T01:56:12.266Z",
          "wordCount": 612,
          "title": "Taylor saves for later: disentanglement for video prediction using Taylor representation. (arXiv:2105.11062v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Caomingzhe Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhua Zhao</a>",
          "description": "Non-intrusive load monitoring (NILM) decomposes the total load reading into\nappliance-level load signals. Many deep learning-based methods have been\ndeveloped to accomplish NILM, and the training of deep neural networks (DNN)\nrequires massive load data containing different types of appliances. For local\ndata owners with inadequate load data but expect to accomplish a promising\nmodel performance, the conduction of effective NILM co-modelling is\nincreasingly significant. While during the cooperation of local data owners,\ndata exchange and centralized data storage may increase the risk of power\nconsumer privacy breaches. To eliminate the potential risks, a novel NILM\nmethod named Fed-NILM ap-plying Federated Learning (FL) is proposed in this\npaper. In Fed-NILM, local parameters instead of load data are shared among\nlocal data owners. The global model is obtained by weighted averaging the\nparameters. In the experiments, Fed-NILM is validated on two real-world\ndatasets. Besides, a comparison of Fed-NILM with locally-trained NILMs and the\ncentrally-trained one is conducted in both residential and industrial\nscenarios. The experimental results show that Fed-NILM outperforms\nlocally-trained NILMs and approximate the centrally-trained NILM which is\ntrained on the entire load dataset without privacy preservation.",
          "link": "http://arxiv.org/abs/2105.11085",
          "publishedOn": "2021-05-25T01:56:12.232Z",
          "wordCount": 619,
          "title": "Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring Method for Privacy-Protection. (arXiv:2105.11085v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08721",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1\">Pujan Pokhrel</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ioup_E/0/1/0/all/0/1\">Elias Ioup</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1\">Md Tamjidul Hoque</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1\">Mahdi Abdelguerfi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Simeonov_J/0/1/0/all/0/1\">Julian Simeonov</a>",
          "description": "In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast\ndominant wave periods in oceanic waters. First, we use the data collected from\nCDIP buoys and apply various data filtering methods. The data filtering methods\nallow us to obtain a high-quality dataset for training and validation purposes.\nWe then extract various wave-based features like wave heights, periods,\nskewness, kurtosis, etc., and atmospheric features like humidity, pressure, and\nair temperature for the buoys. Afterward, we train algorithms that use LightGBM\nand Extra Trees through a hv-block cross-validation scheme to forecast dominant\nwave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,\nand 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,\nExtra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,\n15-day ahead, and 30 day ahead prediction. In case of the test dataset,\nLightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and\n30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day\nahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both\ntraining and the test dataset suggests that the machine learning models\ndeveloped in this paper are robust. Since the LightGBM algorithm outperforms ET\nfor all the windows tested, it is taken as the final algorithm. Note that the\nperformance of both methods does not decrease significantly as the forecast\nhorizon increases. Likewise, the proposed method outperforms the numerical\napproaches included in this paper in the test dataset. For 1 day ahead\nprediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,\n0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre\nfor Medium-range Weather Forecasts (ECMWF) model, which outperforms all the\nother methods in the test dataset.",
          "link": "http://arxiv.org/abs/2105.08721",
          "publishedOn": "2021-05-25T01:56:12.110Z",
          "wordCount": 768,
          "title": "A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v2 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Junhao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingchun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_N/0/1/0/all/0/1\">Ning Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bochun Yang</a>",
          "description": "The development of intelligent tutoring system has greatly influenced the way\nstudents learn and practice, which increases their learning efficiency. The\nintelligent tutoring system must model learners' mastery of the knowledge\nbefore providing feedback and advices to learners, so one class of algorithm\ncalled \"knowledge tracing\" is surely important. This paper proposed Deep\nSelf-Attentive Knowledge Tracing (DSAKT) based on the data of PTA, an online\nassessment system used by students in many universities in China, to help these\nstudents learn more efficiently. Experimentation on the data of PTA shows that\nDSAKT outperforms the other models for knowledge tracing an improvement of AUC\nby 2.1% on average, and this model also has a good performance on the ASSIST\ndataset.",
          "link": "http://arxiv.org/abs/2105.07909",
          "publishedOn": "2021-05-25T01:56:12.027Z",
          "wordCount": null,
          "title": "Application of Deep Self-Attention in Knowledge Tracing. (arXiv:2105.07909v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10759",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Manjunath_G/0/1/0/all/0/1\">G Manjunath</a>, <a href=\"http://arxiv.org/find/math/1/au:+Clercq_A/0/1/0/all/0/1\">A de Clercq</a>",
          "description": "Obtaining repeated measurements from physical and natural systems for\nbuilding a more informative dynamical model of such systems is engraved in\nmodern science. Results in reconstructing equivalent chaotic dynamical systems\nthrough delay coordinate mappings, Koopman operator based data-driven approach\nand reservoir computing methods have shown the possibility of finding model\nequations on a new phase space that is relatable to the dynamical system\ngenerating the data. Recently, rigorous results that point to reducing the\nfunctional complexity of the map that describes the dynamics in the new phase\nhave made the Koopman operator based approach very attractive for data-driven\nmodeling. However, choosing a set of nonlinear observable functions that can\nwork for different data sets is an open challenge. We use driven dynamical\nsystems comparable to that in reservoir computing with the \\emph{causal\nembedding property} to obtain the right set of observables through which the\ndynamics in the new space is made equivalent or topologically conjugate to the\noriginal system. Deep learning methods are used to learn a map that emerges as\na consequence of the topological conjugacy. Besides stability, amenability for\nhardware implementations, causal embedding based models provide long-term\nconsistency even for maps that have failed under previously reported\ndata-driven or machine learning methods.",
          "link": "http://arxiv.org/abs/2105.10759",
          "publishedOn": "2021-05-25T01:56:12.011Z",
          "wordCount": 636,
          "title": "Universal set of Observables for the Koopman Operator through Causal Embedding. (arXiv:2105.10759v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01714",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Cabral_J/0/1/0/all/0/1\">J. B. Cabral</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lares_M/0/1/0/all/0/1\">M. Lares</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gurovich_S/0/1/0/all/0/1\">S. Gurovich</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Minniti_D/0/1/0/all/0/1\">D. Minniti</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Granitto_P/0/1/0/all/0/1\">P. M. Granitto</a>",
          "description": "As most of the modern astronomical sky surveys produce data faster than\nhumans can analyze it, Machine Learning (ML) has become a central tool in\nAstronomy. Modern ML methods can be characterized as highly resistant to some\nexperimental errors. However, small changes on the data over long distances or\nlong periods of time, which cannot be easily detected by statistical methods,\ncan be harmful to these methods. We develop a new strategy to cope with this\nproblem, also using ML methods in an innovative way, to identify these\npotentially harmful features. We introduce and discuss the notion of Drifting\nFeatures, related with small changes in the properties as measured in the data\nfeatures. We use the identification of RRLs in VVV based on an earlier work and\nintroduce a method for detecting Drifting Features. Our method forces a\nclassifier to learn the tile of origin of diverse sources (mostly stellar\n'point sources'), and select the features more relevant to the task of finding\ncandidates to Drifting Features. We show that this method can efficiently\nidentify a reduced set of features that contains useful information about the\ntile of origin of the sources. For our particular example of detecting RRLs in\nVVV, we find that Drifting Features are mostly related to color indices. On the\nother hand, we show that, even if we have a clear set of Drifting Features in\nour problem, they are mostly insensitive to the identification of RRLs.\nDrifting Features can be efficiently identified using ML methods. However, in\nour example, removing Drifting Features does not improve the identification of\nRRLs.",
          "link": "http://arxiv.org/abs/2105.01714",
          "publishedOn": "2021-05-25T01:56:11.951Z",
          "wordCount": 744,
          "title": "Drifting Features: Detection and evaluation in the context of automatic RRLs identification in VVV. (arXiv:2105.01714v3 [astro-ph.IM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haijin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Si_C/0/1/0/all/0/1\">Caomingzhe Si</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_J/0/1/0/all/0/1\">Junhua Zhao</a>",
          "description": "Non-intrusive load monitoring (NILM) aims at decomposing the total reading of\nthe household power consumption into appliance-wise ones, which is beneficial\nfor consumer behavior analysis as well as energy conservation. NILM based on\ndeep learning has been a focus of research. To train a better neural network,\nit is necessary for the network to be fed with massive data containing various\nappliances and reflecting consumer behavior habits. Therefore, data cooperation\namong utilities and DNOs (distributed network operators) who own the NILM data\nhas been increasingly significant. During the cooperation, however, risks of\nconsumer privacy leakage and losses of data control rights arise. To deal with\nthe problems above, a framework to improve the performance of NILM with\nfederated learning (FL) has been set up. In the framework, model weights\ninstead of the local data are shared among utilities. The global model is\ngenerated by weighted averaging the locally-trained model weights to gather the\nlocally-trained model information. Optimal model selection help choose the\nmodel which adapts to the data from different domains best. Experiments show\nthat this proposal improves the performance of local NILM runners. The\nperformance of this framework is close to that of the centrally-trained model\nobtained by the convergent data without privacy protection.",
          "link": "http://arxiv.org/abs/2104.01618",
          "publishedOn": "2021-05-25T01:56:11.862Z",
          "wordCount": 641,
          "title": "A Federated Learning Framework for Non-Intrusive Load Monitoring. (arXiv:2104.01618v1 [eess.SP] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gongxu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "Graph representation learning has achieved great success in many areas,\nincluding e-commerce, chemistry, biology, etc. However, the fundamental problem\nof choosing the appropriate dimension of node embedding for a given graph still\nremains unsolved. The commonly used strategies for Node Embedding Dimension\nSelection (NEDS) based on grid search or empirical knowledge suffer from heavy\ncomputation and poor model performance. In this paper, we revisit NEDS from the\nperspective of minimum entropy principle. Subsequently, we propose a novel\nMinimum Graph Entropy (MinGE) algorithm for NEDS with graph data. To be\nspecific, MinGE considers both feature entropy and structure entropy on graphs,\nwhich are carefully designed according to the characteristics of the rich\ninformation in them. The feature entropy, which assumes the embeddings of\nadjacent nodes to be more similar, connects node features and link topology on\ngraphs. The structure entropy takes the normalized degree as basic unit to\nfurther measure the higher-order structure of graphs. Based on them, we design\nMinGE to directly calculate the ideal node embedding dimension for any graph.\nFinally, comprehensive experiments with popular Graph Neural Networks (GNNs) on\nbenchmark datasets demonstrate the effectiveness and generalizability of our\nproposed MinGE.",
          "link": "http://arxiv.org/abs/2105.03178",
          "publishedOn": "2021-05-25T01:56:11.822Z",
          "wordCount": 673,
          "title": "Graph Entropy Guided Node Embedding Dimension Selection for Graph Neural Networks. (arXiv:2105.03178v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen-Xin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ru-Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mao-Cai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chi-Chun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi-Liua/0/1/0/all/0/1\">Yi-Liua</a>",
          "description": "An effective and efficient method that solves the high-order and the\nnon-linear ordinary differential equations is provided. The method is based on\nthe ratio net. By comparing the method with existing methods such as the\npolynomial based method and the multilayer perceptron network based method, we\nshow that the ratio net gives good results and has higher efficiency.",
          "link": "http://arxiv.org/abs/2105.11309",
          "publishedOn": "2021-05-25T01:56:11.550Z",
          "wordCount": 509,
          "title": "An Effective and Efficient Method to Solve the High-Order and the Non-Linear Ordinary Differential Equations: the Ratio Net. (arXiv:2105.11309v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.10078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>",
          "description": "Modern neural networks often contain significantly more parameters than the\nsize of their training data. We show that this excess capacity provides an\nopportunity for embedding secret machine learning models within a trained\nneural network. Our novel framework hides the existence of a secret neural\nnetwork with arbitrary desired functionality within a carrier network. We prove\ntheoretically that the secret network's detection is computationally infeasible\nand demonstrate empirically that the carrier network does not compromise the\nsecret network's disguise. Our paper introduces a previously unknown\nsteganographic technique that can be exploited by adversaries if left\nunchecked.",
          "link": "http://arxiv.org/abs/2002.10078",
          "publishedOn": "2021-05-25T01:56:11.525Z",
          "wordCount": 559,
          "title": "On Hiding Neural Networks Inside Neural Networks. (arXiv:2002.10078v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuankai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1\">Dingyi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lijun Sun</a>",
          "description": "This paper studies the traffic state estimation (TSE) problem using sparse\nobservations from mobile sensors. TSE can be considered a spatiotemporal\ninterpolation problem in which the evolution of traffic variables (e.g.,\nspeed/density) is governed by traffic flow dynamics (e.g., partial differential\nequations). Most existing TSE methods either rely on well-defined physical\ntraffic flow models or require large amounts of simulation data as input to\ntrain machine learning models. Different from previous studies, in this paper\nwe propose a purely data-driven and model-free solution. We consider TSE as a\nspatiotemporal matrix completion/interpolation problem, and apply\nspatiotemporal Hankel delay embedding to transforms the original incomplete\nmatrix to a fourth-order tensor. By imposing a low-rank assumption on this\ntensor structure, we can approximate and characterize both global patterns and\nthe unknown and complex local spatiotemporal dynamics in a data-driven manner.\nWe use the truncated nuclear norm of the spatiotemporal unfolding (i.e., square\nnorm) to approximate the tensor rank and develop an efficient solution\nalgorithm based on the Alternating Direction Method of Multipliers (ADMM). The\nproposed framework only involves two hyperparameters -- spatial and temporal\nwindow lengths, which are easy to set given the degree of data sparsity. We\nconduct numerical experiments on both synthetic simulation data and real-world\nhigh-resolution trajectory data, and our results demonstrate the effectiveness\nand superiority of the proposed model in some challenging scenarios.",
          "link": "http://arxiv.org/abs/2105.11335",
          "publishedOn": "2021-05-25T01:56:11.511Z",
          "wordCount": 654,
          "title": "Low-Rank Hankel Tensor Completion for Traffic Speed Estimation. (arXiv:2105.11335v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Huifeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiuqiang He</a>",
          "description": "Click-Through Rate (CTR) prediction is critical for industrial recommender\nsystems, where most deep CTR models follow an Embedding \\& Feature Interaction\nparadigm. However, the majority of methods focus on designing network\narchitectures to better capture feature interactions while the feature\nembedding, especially for numerical features, has been overlooked. Existing\napproaches for numerical features are difficult to capture informative\nknowledge because of the low capacity or hard discretization based on the\noffline expertise feature engineering. In this paper, we propose a novel\nembedding learning framework for numerical features in CTR prediction (AutoDis)\nwith high model capacity, end-to-end training and unique representation\nproperties preserved. AutoDis consists of three core components:\nmeta-embeddings, automatic discretization and aggregation. Specifically, we\npropose meta-embeddings for each numerical field to learn global knowledge from\nthe perspective of field with a manageable number of parameters. Then the\ndifferentiable automatic discretization performs soft discretization and\ncaptures the correlations between the numerical features and meta-embeddings.\nFinally, distinctive and informative embeddings are learned via an aggregation\nfunction. Comprehensive experiments on two public and one industrial datasets\nare conducted to validate the effectiveness of AutoDis. Moreover, AutoDis has\nbeen deployed onto a mainstream advertising platform, where online A/B test\ndemonstrates the improvement over the base model by 2.1% and 2.7% in terms of\nCTR and eCPM, respectively. In addition, the code of our framework is publicly\navailable in\nMindSpore(https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/recommend/autodis).",
          "link": "http://arxiv.org/abs/2012.08986",
          "publishedOn": "2021-05-25T01:56:11.498Z",
          "wordCount": 702,
          "title": "An Embedding Learning Framework for Numerical Features in CTR Prediction. (arXiv:2012.08986v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.12127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_S/0/1/0/all/0/1\">Shariq Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>",
          "description": "Solving tasks with sparse rewards is one of the most important challenges in\nreinforcement learning. In the single-agent setting, this challenge is\naddressed by introducing intrinsic rewards that motivate agents to explore\nunseen regions of their state spaces; however, applying these techniques\nnaively to the multi-agent setting results in agents exploring independently,\nwithout any coordination among themselves. Exploration in cooperative\nmulti-agent settings can be accelerated and improved if agents coordinate their\nexploration. In this paper we introduce a framework for designing intrinsic\nrewards which consider what other agents have explored such that the agents can\ncoordinate. Then, we develop an approach for learning how to dynamically select\nbetween several exploration modalities to maximize extrinsic rewards.\nConcretely, we formulate the approach as a hierarchical policy where a\nhigh-level controller selects among sets of policies trained on diverse\nintrinsic rewards and the low-level controllers learn the action policies of\nall agents under these specific rewards. We demonstrate the effectiveness of\nthe proposed approach in cooperative domains with sparse rewards where\nstate-of-the-art methods fail and challenging multi-stage tasks that\nnecessitate changing modes of coordination.",
          "link": "http://arxiv.org/abs/1905.12127",
          "publishedOn": "2021-05-25T01:56:11.488Z",
          "wordCount": 652,
          "title": "Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning. (arXiv:1905.12127v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Djuric_N/0/1/0/all/0/1\">Nemanja Djuric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Henggang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huahua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_F/0/1/0/all/0/1\">Fang-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Luisa San Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Song Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Rui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayan_A/0/1/0/all/0/1\">Alyssa Dayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sidney Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_B/0/1/0/all/0/1\">Brian C. Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_G/0/1/0/all/0/1\">Gregory P. Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallespi_Gonzalez_C/0/1/0/all/0/1\">Carlos Vallespi-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wellington_C/0/1/0/all/0/1\">Carl K. Wellington</a>",
          "description": "One of the critical pieces of the self-driving puzzle is understanding the\nsurroundings of a self-driving vehicle (SDV) and predicting how these\nsurroundings will change in the near future. To address this task we propose\nMultiXNet, an end-to-end approach for detection and motion prediction based\ndirectly on lidar sensor data. This approach builds on prior work by handling\nmultiple classes of traffic actors, adding a jointly trained second-stage\ntrajectory refinement step, and producing a multimodal probability distribution\nover future actor motion that includes both multiple discrete traffic behaviors\nand calibrated continuous position uncertainties. The method was evaluated on\nlarge-scale, real-world data collected by a fleet of SDVs in several cities,\nwith the results indicating that it outperforms existing state-of-the-art\napproaches.",
          "link": "http://arxiv.org/abs/2006.02000",
          "publishedOn": "2021-05-25T01:56:11.481Z",
          "wordCount": 645,
          "title": "MultiXNet: Multiclass Multistage Multimodal Motion Prediction. (arXiv:2006.02000v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06792",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nikolakakis_K/0/1/0/all/0/1\">Kontantinos E. Nikolakakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kalogerias_D/0/1/0/all/0/1\">Dionysios S. Kalogerias</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sheffet_O/0/1/0/all/0/1\">Or Sheffet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sarwate_A/0/1/0/all/0/1\">Anand D. Sarwate</a>",
          "description": "We study the best-arm identification problem in multi-armed bandits with\nstochastic, potentially private rewards, when the goal is to identify the arm\nwith the highest quantile at a fixed, prescribed level. First, we propose a\n(non-private) successive elimination algorithm for strictly optimal best-arm\nidentification, we show that our algorithm is $\\delta$-PAC and we characterize\nits sample complexity. Further, we provide a lower bound on the expected number\nof pulls, showing that the proposed algorithm is essentially optimal up to\nlogarithmic factors. Both upper and lower complexity bounds depend on a special\ndefinition of the associated suboptimality gap, designed in particular for the\nquantile bandit problem, as we show when the gap approaches zero, best-arm\nidentification is impossible. Second, motivated by applications where the\nrewards are private, we provide a differentially private successive elimination\nalgorithm whose sample complexity is finite even for distributions with\ninfinite support-size, and we characterize its sample complexity. Our\nalgorithms do not require prior knowledge of either the suboptimality gap or\nother statistical information related to the bandit problem at hand.",
          "link": "http://arxiv.org/abs/2006.06792",
          "publishedOn": "2021-05-25T01:56:11.460Z",
          "wordCount": 641,
          "title": "Quantile Multi-Armed Bandits: Optimal Best-Arm Identification and a Differentially Private Scheme. (arXiv:2006.06792v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zongyan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhenyong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Zero-shot object recognition or zero-shot learning aims to transfer the\nobject recognition ability among the semantically related categories, such as\nfine-grained animal or bird species. However, the images of different\nfine-grained objects tend to merely exhibit subtle differences in appearance,\nwhich will severely deteriorate zero-shot object recognition. To reduce the\nsuperfluous information in the fine-grained objects, in this paper, we propose\nto learn the redundancy-free features for generalized zero-shot learning. We\nachieve our motivation by projecting the original visual features into a new\n(redundancy-free) feature space and then restricting the statistical dependence\nbetween these two feature spaces. Furthermore, we require the projected\nfeatures to keep and even strengthen the category relationship in the\nredundancy-free feature space. In this way, we can remove the redundant\ninformation from the visual features without losing the discriminative\ninformation. We extensively evaluate the performance on four benchmark\ndatasets. The results show that our redundancy-free feature based generalized\nzero-shot learning (RFF-GZSL) approach can achieve competitive results compared\nwith the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2006.08939",
          "publishedOn": "2021-05-25T01:56:11.347Z",
          "wordCount": 706,
          "title": "Learning the Redundancy-free Features for Generalized Zero-Shot Object Recognition. (arXiv:2006.08939v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruili Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Deli Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhengjun Zha</a>",
          "description": "Noise injection has been proved to be one of the key technique advances in\ngenerating high-fidelity images. Despite its successful usage in GANs, the\nmechanism of its validity is still unclear. In this paper, we propose a\ngeometric framework to theoretically analyze the role of noise injection in\nGANs. Based on Riemannian geometry, we successfully model the noise injection\nframework as fuzzy equivalence on the geodesic normal coordinates. Guided by\nour theories, we find that the existing method is incomplete and a new strategy\nfor noise injection is devised. Experiments on image generation and GAN\ninversion demonstrate the superiority of our method.",
          "link": "http://arxiv.org/abs/2006.05891",
          "publishedOn": "2021-05-25T01:56:11.334Z",
          "wordCount": 572,
          "title": "On Noise Injection in Generative Adversarial Networks. (arXiv:2006.05891v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eustratiadis_P/0/1/0/all/0/1\">Panagiotis Eustratiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouk_H/0/1/0/all/0/1\">Henry Gouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>",
          "description": "Stochastic Neural Networks (SNNs) that inject noise into their hidden layers\nhave recently been shown to achieve strong robustness against adversarial\nattacks. However, existing SNNs are usually heuristically motivated, and often\nrely on adversarial training, which is computationally costly. We propose a new\nSNN that achieves state-of-the-art performance without relying on adversarial\ntraining, and enjoys solid theoretical justification. Specifically, while\nexisting SNNs inject learned or hand-tuned isotropic noise, our SNN learns an\nanisotropic noise distribution to optimize a learning-theoretic bound on\nadversarial robustness. We evaluate our method on a number of popular\nbenchmarks, show that it can be applied to different architectures, and that it\nprovides robustness to a variety of white-box and black-box attacks, while\nbeing simple and fast to train compared to existing alternatives.",
          "link": "http://arxiv.org/abs/2010.08852",
          "publishedOn": "2021-05-25T01:56:11.327Z",
          "wordCount": 589,
          "title": "Weight-Covariance Alignment for Adversarially Robust Neural Networks. (arXiv:2010.08852v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.08996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cornelio_C/0/1/0/all/0/1\">Cristina Cornelio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1\">Michele Donini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loreggia_A/0/1/0/all/0/1\">Andrea Loreggia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pini_M/0/1/0/all/0/1\">Maria Silvia Pini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Francesca Rossi</a>",
          "description": "In many machine learning scenarios, looking for the best classifier that fits\na particular dataset can be very costly in terms of time and resources.\nMoreover, it can require deep knowledge of the specific domain. We propose a\nnew technique which does not require profound expertise in the domain and\navoids the commonly used strategy of hyper-parameter tuning and model\nselection. Our method is an innovative ensemble technique that uses voting\nrules over a set of randomly-generated classifiers. Given a new input sample,\nwe interpret the output of each classifier as a ranking over the set of\npossible classes. We then aggregate these output rankings using a voting rule,\nwhich treats them as preferences over the classes. We show that our approach\nobtains good results compared to the state-of-the-art, both providing a\ntheoretical analysis and an empirical evaluation of the approach on several\ndatasets.",
          "link": "http://arxiv.org/abs/1909.08996",
          "publishedOn": "2021-05-25T01:56:11.319Z",
          "wordCount": 649,
          "title": "Voting with Random Classifiers (VORACE): Theoretical and Experimental Analysis. (arXiv:1909.08996v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahadori_M/0/1/0/all/0/1\">Mohammad Taha Bahadori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David E. Heckerman</a>",
          "description": "Concept-based explanation approach is a popular model interpertability tool\nbecause it expresses the reasons for a model's predictions in terms of concepts\nthat are meaningful for the domain experts. In this work, we study the problem\nof the concepts being correlated with confounding information in the features.\nWe propose a new causal prior graph for modeling the impacts of unobserved\nvariables and a method to remove the impact of confounding information and\nnoise using a two-stage regression technique borrowed from the instrumental\nvariable literature. We also model the completeness of the concepts set and\nshow that our debiasing method works when the concepts are not complete. Our\nsynthetic and real-world experiments demonstrate the success of our method in\nremoving biases and improving the ranking of the concepts in terms of their\ncontribution to the explanation of the predictions.",
          "link": "http://arxiv.org/abs/2007.11500",
          "publishedOn": "2021-05-25T01:56:11.284Z",
          "wordCount": 612,
          "title": "Debiasing Concept-based Explanations with Causal Analysis. (arXiv:2007.11500v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1902.11104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaquier_N/0/1/0/all/0/1\">No&#xe9;mie Jaquier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haschke_R/0/1/0/all/0/1\">Robert Haschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calinon_S/0/1/0/all/0/1\">Sylvain Calinon</a>",
          "description": "When data are organized in matrices or arrays of higher dimensions (tensors),\nclassical regression methods first transform these data into vectors, therefore\nignoring the underlying structure of the data and increasing the dimensionality\nof the problem. This flattening operation typically leads to overfitting when\nonly few training data is available. In this paper, we present a\nmixture-of-experts model that exploits tensorial representations for regression\nof tensor-valued data. The proposed formulation takes into account the\nunderlying structure of the data and remains efficient when few training data\nare available. Evaluation on artificially generated data, as well as offline\nand real-time experiments recognizing hand movements from tactile myography\nprove the effectiveness of the proposed approach.",
          "link": "http://arxiv.org/abs/1902.11104",
          "publishedOn": "2021-05-25T01:56:11.277Z",
          "wordCount": 607,
          "title": "Tensor-variate Mixture of Experts for Proportional Myographic Control of a Robotic Hand. (arXiv:1902.11104v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halterman_A/0/1/0/all/0/1\">Andrew Halterman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_B/0/1/0/all/0/1\">Benjamin J. Radford</a>",
          "description": "We propose a new task and dataset for a common problem in social science\nresearch: \"upsampling\" coarse document labels to fine-grained labels or spans.\nWe pose the problem in a question answering format, with the answers providing\nthe fine-grained labels. We provide a benchmark dataset and baselines on a\nsocially impactful task: identifying the exact crowd size at protests and\ndemonstrations in the United States given only order-of-magnitude information\nabout protest attendance, a very small sample of fine-grained examples, and\nEnglish-language news text. We evaluate several baseline models, including\nzero-shot results from rule-based and question-answering models, few-shot\nmodels fine-tuned on a small set of documents, and weakly supervised models\nusing a larger set of coarsely-labeled documents. We find that our rule-based\nmodel initially outperforms a zero-shot pre-trained transformer language model\nbut that further fine-tuning on a very small subset of 25 examples\nsubstantially improves out-of-sample performance. We also demonstrate a method\nfor fine-tuning the transformer span on only the coarse labels that performs\nsimilarly to our rule-based approach. This work will contribute to social\nscientists' ability to generate data to understand the causes and successes of\ncollective action.",
          "link": "http://arxiv.org/abs/2105.11260",
          "publishedOn": "2021-05-25T01:56:11.270Z",
          "wordCount": 617,
          "title": "Few-Shot Upsampling for Protest Size Detection. (arXiv:2105.11260v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_K/0/1/0/all/0/1\">Kasra Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beelen_K/0/1/0/all/0/1\">Kaspar Beelen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colavizza_G/0/1/0/all/0/1\">Giovanni Colavizza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardanuy_M/0/1/0/all/0/1\">Mariona Coll Ardanuy</a>",
          "description": "We present four types of neural language models trained on a large historical\ndataset of books in English, published between 1760-1900 and comprised of ~5.1\nbillion tokens. The language model architectures include static (word2vec and\nfastText) and contextualized models (BERT and Flair). For each architecture, we\ntrained a model instance using the whole dataset. Additionally, we trained\nseparate instances on text published before 1850 for the two static models, and\nfour instances considering different time slices for BERT. Our models have\nalready been used in various downstream tasks where they consistently improved\nperformance. In this paper, we describe how the models have been created and\noutline their reuse potential.",
          "link": "http://arxiv.org/abs/2105.11321",
          "publishedOn": "2021-05-25T01:56:11.255Z",
          "wordCount": 540,
          "title": "Neural Language Models for Nineteenth-Century English. (arXiv:2105.11321v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2002.03388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arakelyan_S/0/1/0/all/0/1\">Shushan Arakelyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arasteh_S/0/1/0/all/0/1\">Sima Arasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauser_C/0/1/0/all/0/1\">Christophe Hauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_E/0/1/0/all/0/1\">Erik Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "Tackling binary program analysis problems has traditionally implied manually\ndefining rules and heuristics, a tedious and time-consuming task for human\nanalysts. In order to improve automation and scalability, we propose an\nalternative direction based on distributed representations of binary programs\nwith applicability to a number of downstream tasks. We introduce Bin2vec, a new\napproach leveraging Graph Convolutional Networks (GCN) along with computational\nprogram graphs in order to learn a high dimensional representation of binary\nexecutable programs. We demonstrate the versatility of this approach by using\nour representations to solve two semantically different binary analysis tasks -\nfunctional algorithm classification and vulnerability discovery. We compare the\nproposed approach to our own strong baseline as well as published results and\ndemonstrate improvement over state-of-the-art methods for both tasks. We\nevaluated Bin2vec on 49191 binaries for the functional algorithm classification\ntask, and on 30 different CWE-IDs including at least 100 CVE entries each for\nthe vulnerability discovery task. We set a new state-of-the-art result by\nreducing the classification error by 40% compared to the source-code-based\ninst2vec approach, while working on binary code. For almost every vulnerability\nclass in our dataset, our prediction accuracy is over 80% (and over 90% in\nmultiple classes).",
          "link": "http://arxiv.org/abs/2002.03388",
          "publishedOn": "2021-05-25T01:56:11.227Z",
          "wordCount": 668,
          "title": "Bin2vec: Learning Representations of Binary Executable Programs for Security Tasks. (arXiv:2002.03388v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yuankai Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1\">Lili Ju</a>",
          "description": "Partial differential equations are often used to model various physical\nphenomena, such as heat diffusion, wave propagation, fluid dynamics,\nelasticity, electrodynamics and image processing, and many analytic approaches\nor traditional numerical methods have been developed and widely used for their\nsolutions. Inspired by rapidly growing impact of deep learning on scientific\nand engineering research, in this paper we propose a novel neural network,\nGF-Net, for learning the Green's functions of linear reaction-diffusion\nequations in an unsupervised fashion. The proposed method overcomes the\nchallenges for finding the Green's functions of the equations on arbitrary\ndomains by utilizing physics-informed approach and the symmetry of the Green's\nfunction. As a consequence, it particularly leads to an efficient way for\nsolving the target equations under different boundary conditions and sources.\nWe also demonstrate the effectiveness of the proposed approach by experiments\nin square, annular and L-shape domains.",
          "link": "http://arxiv.org/abs/2105.11045",
          "publishedOn": "2021-05-25T01:56:11.211Z",
          "wordCount": 588,
          "title": "Learning Green's Functions of Linear Reaction-Diffusion Equations with Application to Fast Numerical Solver. (arXiv:2105.11045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1908.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1\">M.G. Sarwar Murshed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_C/0/1/0/all/0/1\">Christopher Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1\">Daqing Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Nazar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananthanarayanan_G/0/1/0/all/0/1\">Ganesh Ananthanarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1\">Faraz Hussain</a>",
          "description": "Resource-constrained IoT devices, such as sensors and actuators, have become\nubiquitous in recent years. This has led to the generation of large quantities\nof data in real-time, which is an appealing target for AI systems. However,\ndeploying machine learning models on such end-devices is nearly impossible. A\ntypical solution involves offloading data to external computing systems (such\nas cloud servers) for further processing but this worsens latency, leads to\nincreased communication costs, and adds to privacy concerns. To address this\nissue, efforts have been made to place additional computing devices at the edge\nof the network, i.e close to the IoT devices where the data is generated.\nDeploying machine learning systems on such edge computing devices alleviates\nthe above issues by allowing computations to be performed close to the data\nsources. This survey describes major research efforts where machine learning\nsystems have been deployed at the edge of computer networks, focusing on the\noperational aspects including compression techniques, tools, frameworks, and\nhardware used in successful applications of intelligent edge systems.",
          "link": "http://arxiv.org/abs/1908.00080",
          "publishedOn": "2021-05-25T01:56:11.190Z",
          "wordCount": null,
          "title": "Machine Learning at the Network Edge: A Survey. (arXiv:1908.00080v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11135",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Holland_M/0/1/0/all/0/1\">Matthew J. Holland</a>",
          "description": "Under data distributions which may be heavy-tailed, many stochastic\ngradient-based learning algorithms are driven by feedback queried at points\nwith almost no performance guarantees on their own. Here we explore a modified\n\"anytime online-to-batch\" mechanism which for smooth objectives admits\nhigh-probability error bounds while requiring only lower-order moment bounds on\nthe stochastic gradients. Using this conversion, we can derive a wide variety\nof \"anytime robust\" procedures, for which the task of performance analysis can\nbe effectively reduced to regret control, meaning that existing regret bounds\n(for the bounded gradient case) can be robustified and leveraged in a\nstraightforward manner. As a direct takeaway, we obtain an easily implemented\nstochastic gradient-based algorithm for which all queried points formally enjoy\nsub-Gaussian error bounds, and in practice show noteworthy gains on real-world\ndata applications.",
          "link": "http://arxiv.org/abs/2105.11135",
          "publishedOn": "2021-05-25T01:56:11.166Z",
          "wordCount": 549,
          "title": "Robust learning with anytime-guaranteed feedback. (arXiv:2105.11135v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2009.02731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1\">Nghi D. Q. Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yijun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lingxiao Jiang</a>",
          "description": "We propose Corder, a self-supervised contrastive learning framework for\nsource code model. Corder is designed to alleviate the need of labeled data for\ncode retrieval and code summarization tasks. The pre-trained model of Corder\ncan be used in two ways: (1) it can produce vector representation of code which\ncan be applied to code retrieval tasks that do not have labeled data; (2) it\ncan be used in a fine-tuning process for tasks that might still require label\ndata such as code summarization. The key innovation is that we train the source\ncode model by asking it to recognize similar and dissimilar code snippets\nthrough a contrastive learning objective. To do so, we use a set of\nsemantic-preserving transformation operators to generate code snippets that are\nsyntactically diverse but semantically equivalent. Through extensive\nexperiments, we have shown that the code models pretrained by Corder\nsubstantially outperform the other baselines for code-to-code retrieval,\ntext-to-code retrieval, and code-to-text summarization tasks.",
          "link": "http://arxiv.org/abs/2009.02731",
          "publishedOn": "2021-05-25T01:56:11.166Z",
          "wordCount": null,
          "title": "Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations. (arXiv:2009.02731v8 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11376",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>",
          "description": "This paper presents a framework of imitating the price behavior of the\nunderlying stock for reinforcement learning option price. We use accessible\nfeatures of the equities pricing data to construct a non-deterministic Markov\ndecision process for modeling stock price behavior driven by principal\ninvestor's decision making. However, low signal-to-noise ratio and instability\nthat appear immanent in equity markets pose challenges to determine the state\ntransition (price change) after executing an action (principal investor's\ndecision) as well as decide an action based on current state (spot price). In\norder to conquer these challenges, we resort to a Bayesian deep neural network\nfor computing the predictive distribution of the state transition led by an\naction. Additionally, instead of exploring a state-action relationship to\nformulate a policy, we seek for an episode based visible-hidden state-action\nrelationship to probabilistically imitate principal investor's successive\ndecision making. Our algorithm then maps imitative principal investor's\ndecisions to simulated stock price paths by a Bayesian deep neural network.\nEventually the optimal option price is reinforcement learned through maximizing\nthe cumulative risk-adjusted return of a dynamically hedged portfolio over\nsimulated price paths of the underlying.",
          "link": "http://arxiv.org/abs/2105.11376",
          "publishedOn": "2021-05-25T01:56:11.158Z",
          "wordCount": 617,
          "title": "Can we imitate stock price behavior to reinforcement learn option price?. (arXiv:2105.11376v1 [q-fin.PR])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Human keypoint detection from a single image is very challenging due to\nocclusion, blur, illumination and scale variance. In this paper, we address\nthis problem from three aspects by devising an efficient network structure,\nproposing three effective training strategies, and exploiting four useful\npostprocessing techniques. First, we find that context information plays an\nimportant role in reasoning human body configuration and invisible keypoints.\nInspired by this, we propose a cascaded context mixer (CCM), which efficiently\nintegrates spatial and channel context information and progressively refines\nthem. Then, to maximize CCM's representation capability, we develop a\nhard-negative person detection mining strategy and a joint-training strategy by\nexploiting abundant unlabeled data. It enables CCM to learn discriminative\nfeatures from massive diverse poses. Third, we present several sub-pixel\nrefinement techniques for postprocessing keypoint predictions to improve\ndetection accuracy. Extensive experiments on the MS COCO keypoint detection\nbenchmark demonstrate the superiority of the proposed method over\nrepresentative state-of-the-art (SOTA) methods. Our single model achieves\ncomparable performance with the winner of the 2018 COCO Keypoint Detection\nChallenge. The final ensemble model sets a new SOTA on this benchmark.",
          "link": "http://arxiv.org/abs/2002.00537",
          "publishedOn": "2021-05-25T01:56:11.150Z",
          "wordCount": null,
          "title": "Towards High Performance Human Keypoint Detection. (arXiv:2002.00537v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1\">Michael F. Zimmer</a>",
          "description": "It has long been a goal to efficiently compute and use second order\ninformation on a function ($f$) to assist in numerical approximations. Here it\nis shown how, using only basic physics and a numerical approximation, such\ninformation can be accurately obtained at a cost of ${\\cal O}(N)$ complexity,\nwhere $N$ is the dimensionality of the parameter space of $f$. In this paper,\nan algorithm ({\\em VA-Flow}) is developed to exploit this second order\ninformation, and pseudocode is presented. It is applied to two classes of\nproblems, that of inverse kinematics (IK) and gradient descent (GD). In the IK\napplication, the algorithm is fast and robust, and is shown to lead to smooth\nbehavior even near singularities. For GD the algorithm also works very well,\nprovided the cost function is locally well-described by a polynomial.",
          "link": "http://arxiv.org/abs/2105.11439",
          "publishedOn": "2021-05-25T01:56:11.148Z",
          "wordCount": null,
          "title": "2nd-order Updates with 1st-order Complexity. (arXiv:2105.11439v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1906.09613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabi_D/0/1/0/all/0/1\">Daniel Alabi</a>",
          "description": "Through the lens of information-theoretic reductions, we examine a reductions\napproach to fair optimization and learning where a black-box optimizer is used\nto learn a fair model for classification or regression. Quantifying the\ncomplexity, both statistically and computationally, of making such models\nsatisfy the rigorous definition of differential privacy is our end goal. We\nresolve a few open questions and show applicability to fair machine learning,\nhypothesis testing, and to optimizing non-standard measures of classification\nloss. Furthermore, our sample complexity bounds are tight amongst all\nstrategies that jointly minimize a composition of functions.\n\nThe reductions approach to fair optimization can be abstracted as the\nconstrained group-objective optimization problem where we aim to optimize an\nobjective that is a function of losses of individual groups, subject to some\nconstraints. We give the first polynomial-time algorithms to solve the problem\nwith $(\\epsilon, 0)$ or $(\\epsilon, \\delta)$ differential privacy guarantees\nwhen defined on a convex decision set (for example, the $\\ell_P$ unit ball)\nwith convex constraints and losses. Accompanying information-theoretic lower\nbounds for the problem are presented. In addition, compared to a previous\nmethod for ensuring differential privacy subject to a relaxed form of the\nequalized odds fairness constraint, the $(\\epsilon, \\delta)$-differentially\nprivate algorithm we present provides asymptotically better sample complexity\nguarantees, resulting in an exponential improvement in certain parameter\nregimes. We introduce a class of bounded divergence linear optimizers, which\ncould be of independent interest, and specialize to pure and approximate\ndifferential privacy.",
          "link": "http://arxiv.org/abs/1906.09613",
          "publishedOn": "2021-05-25T01:56:11.147Z",
          "wordCount": null,
          "title": "The Cost of a Reductions Approach to Private Fair Optimization. (arXiv:1906.09613v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oladosu_A/0/1/0/all/0/1\">Ademola Oladosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tony Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekfeldt_P/0/1/0/all/0/1\">Philip Ekfeldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_B/0/1/0/all/0/1\">Brian A. Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cranmer_M/0/1/0/all/0/1\">Miles Cranmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1\">Shirley Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_Whelan_A/0/1/0/all/0/1\">Adrian M. Price-Whelan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contardo_G/0/1/0/all/0/1\">Gabriella Contardo</a>",
          "description": "This paper presents a meta-learning framework for few-shots One-Class\nClassification (OCC) at test-time, a setting where labeled examples are only\navailable for the positive class, and no supervision is given for the negative\nexample. We consider that we have a set of `one-class classification'\nobjective-tasks with only a small set of positive examples available for each\ntask, and a set of training tasks with full supervision (i.e. highly imbalanced\nclassification). We propose an approach using order-equivariant networks to\nlearn a 'meta' binary-classifier. The model will take as input an example to\nclassify from a given task, as well as the corresponding supervised set of\npositive examples for this OCC task. Thus, the output of the model will be\n'conditioned' on the available positive example of a given task, allowing to\npredict on new tasks and new examples without labeled negative examples. In\nthis paper, we are motivated by an astronomy application. Our goal is to\nidentify if stars belong to a specific stellar group (the 'one-class' for a\ngiven task), called \\textit{stellar streams}, where each stellar stream is a\ndifferent OCC-task. We show that our method transfers well on unseen (test)\nsynthetic streams, and outperforms the baselines even though it is not\nretrained and accesses a much smaller part of the data per task to predict\n(only positive supervision). We see however that it doesn't transfer as well on\nthe real stream GD-1. This could come from intrinsic differences from the\nsynthetic and real stream, highlighting the need for consistency in the\n'nature' of the task for this method. However, light fine-tuning improve\nperformances and outperform our baselines. Our experiments show encouraging\nresults to further explore meta-learning methods for OCC tasks.",
          "link": "http://arxiv.org/abs/2007.04459",
          "publishedOn": "2021-05-25T01:56:11.146Z",
          "wordCount": 768,
          "title": "Meta-Learning for One-Class Classification with Few Examples using Order-Equivariant Network. (arXiv:2007.04459v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11363",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xiaojing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1\">Suman Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1\">David Wagner</a>",
          "description": "Recent works have proposed methods to train classifiers with local robustness\nproperties, which can provably eliminate classes of evasion attacks for most\ninputs, but not all inputs. Since data distribution shift is very common in\nsecurity applications, e.g., often observed for malware detection, local\nrobustness cannot guarantee that the property holds for unseen inputs at the\ntime of deploying the classifier. Therefore, it is more desirable to enforce\nglobal robustness properties that hold for all inputs, which is strictly\nstronger than local robustness.\n\nIn this paper, we present a framework and tools for training classifiers that\nsatisfy global robustness properties. We define new notions of global\nrobustness that are more suitable for security classifiers. We design a novel\nbooster-fixer training framework to enforce global robustness properties. We\nstructure our classifier as an ensemble of logic rules and design a new\nverifier to verify the properties. In our training algorithm, the booster\nincreases the classifier's capacity, and the fixer enforces verified global\nrobustness properties following counterexample guided inductive synthesis.\n\nTo the best of our knowledge, the only global robustness property that has\nbeen previously achieved is monotonicity. Several previous works have defined\nglobal robustness properties, but their training techniques failed to achieve\nverified global robustness. In comparison, we show that we can train\nclassifiers to satisfy different global robustness properties for three\nsecurity datasets, and even multiple properties at the same time, with modest\nimpact on the classifier's performance. For example, we train a Twitter spam\naccount classifier to satisfy five global robustness properties, with 5.4%\ndecrease in true positive rate, and 0.1% increase in false positive rate,\ncompared to a baseline XGBoost model that doesn't satisfy any property.",
          "link": "http://arxiv.org/abs/2105.11363",
          "publishedOn": "2021-05-25T01:56:11.124Z",
          "wordCount": 712,
          "title": "Learning Security Classifiers with Verified Global Robustness Properties. (arXiv:2105.11363v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1\">Mark Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1\">Elad Hazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1\">Blake Woodworth</a>",
          "description": "We investigate the computational complexity of several basic linear algebra\nprimitives, including largest eigenvector computation and linear regression, in\nthe computational model that allows access to the data via a matrix-vector\nproduct oracle. We show that for polynomial accuracy, $\\Theta(d)$ calls to the\noracle are necessary and sufficient even for a randomized algorithm.\n\nOur lower bound is based on a reduction to estimating the least eigenvalue of\na random Wishart matrix. This simple distribution enables a concise proof,\nleveraging a few key properties of the random Wishart ensemble.",
          "link": "http://arxiv.org/abs/1911.02212",
          "publishedOn": "2021-05-25T01:56:11.103Z",
          "wordCount": 564,
          "title": "The gradient complexity of linear regression. (arXiv:1911.02212v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1\">Da-Cheng Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Che Zheng</a>",
          "description": "The dot product self-attention is known to be central and indispensable to\nstate-of-the-art Transformer models. But is it really required? This paper\ninvestigates the true importance and contribution of the dot product-based\nself-attention mechanism on the performance of Transformer models. Via\nextensive experiments, we find that (1) random alignment matrices surprisingly\nperform quite competitively and (2) learning attention weights from token-token\n(query-key) interactions is useful but not that important after all. To this\nend, we propose \\textsc{Synthesizer}, a model that learns synthetic attention\nweights without token-token interactions. In our experiments, we first show\nthat simple Synthesizers achieve highly competitive performance when compared\nagainst vanilla Transformer models across a range of tasks, including machine\ntranslation, language modeling, text generation and GLUE/SuperGLUE benchmarks.\nWhen composed with dot product attention, we find that Synthesizers\nconsistently outperform Transformers. Moreover, we conduct additional\ncomparisons of Synthesizers against Dynamic Convolutions, showing that simple\nRandom Synthesizer is not only $60\\%$ faster but also improves perplexity by a\nrelative $3.5\\%$. Finally, we show that simple factorized Synthesizers can\noutperform Linformers on encoding only tasks.",
          "link": "http://arxiv.org/abs/2005.00743",
          "publishedOn": "2021-05-25T01:56:11.096Z",
          "wordCount": 657,
          "title": "Synthesizer: Rethinking Self-Attention in Transformer Models. (arXiv:2005.00743v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11425",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Avanesov_V/0/1/0/all/0/1\">Valeriy Avanesov</a>",
          "description": "The ever-growing size of the datasets renders well-studied learning\ntechniques, such as Kernel Ridge Regression, inapplicable, posing a serious\ncomputational challenge. Divide-and-conquer is a common remedy, suggesting to\nsplit the dataset into disjoint partitions, obtain the local estimates and\naverage them, it allows to scale-up an otherwise ineffective base approach. In\nthe current study we suggest a fully data-driven approach to quantify\nuncertainty of the averaged estimator. Namely, we construct simultaneous\nelement-wise confidence bands for the predictions yielded by the averaged\nestimator on a given deterministic prediction set. The novel approach features\nrigorous theoretical guaranties for a wide class of base learners with Kernel\nRidge regression being a special case. As a by-product of our analysis we also\nobtain a sup-norm consistency result for the divide-and-conquer Kernel Ridge\nRegression. The simulation study supports the theoretical findings.",
          "link": "http://arxiv.org/abs/2105.11425",
          "publishedOn": "2021-05-25T01:56:11.076Z",
          "wordCount": null,
          "title": "Uncertainty quantification for distributed regression. (arXiv:2105.11425v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jia-Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shuguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1\">Tao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_B/0/1/0/all/0/1\">Bin Tong</a>",
          "description": "Conversion rate (CVR) prediction is one of the most critical tasks for\ndigital display advertising. Commercial systems often require to update models\nin an online learning manner to catch up with the evolving data distribution.\nHowever, conversions usually do not happen immediately after a user click. This\nmay result in inaccurate labeling, which is called delayed feedback problem. In\nprevious studies, delayed feedback problem is handled either by waiting\npositive label for a long period of time, or by consuming the negative sample\non its arrival and then insert a positive duplicate when a conversion happens\nlater. Indeed, there is a trade-off between waiting for more accurate labels\nand utilizing fresh data, which is not considered in existing works. To strike\na balance in this trade-off, we propose Elapsed-Time Sampling Delayed Feedback\nModel (ES-DFM), which models the relationship between the observed conversion\ndistribution and the true conversion distribution. Then we optimize the\nexpectation of true conversion distribution via importance sampling under the\nelapsed-time sampling distribution. We further estimate the importance weight\nfor each instance, which is used as the weight of loss function in CVR\nprediction. To demonstrate the effectiveness of ES-DFM, we conduct extensive\nexperiments on a public data and a private industrial dataset. Experimental\nresults confirm that our method consistently outperforms the previous\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2012.03245",
          "publishedOn": "2021-05-25T01:56:11.075Z",
          "wordCount": null,
          "title": "Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling. (arXiv:2012.03245v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikkelsen_K/0/1/0/all/0/1\">Kaare Mikkelsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_O/0/1/0/all/0/1\">Oliver Y. Ch&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Philipp Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertins_A/0/1/0/all/0/1\">Alfred Mertins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1\">Maarten De Vos</a>",
          "description": "Black-box skepticism is one of the main hindrances impeding\ndeep-learning-based automatic sleep scoring from being used in clinical\nenvironments. Towards interpretability, this work proposes a\nsequence-to-sequence sleep-staging model, namely SleepTransformer. It is based\non the transformer backbone whose self-attention scores offer interpretability\nof the model's decisions at both the epoch and sequence level. At the epoch\nlevel, the attention scores can be encoded as a heat map to highlight\nsleep-relevant features captured from the input EEG signal. At the sequence\nlevel, the attention scores are visualized as the influence of different\nneighboring epochs in an input sequence (i.e. the context) to recognition of a\ntarget epoch, mimicking the way manual scoring is done by human experts. We\nfurther propose a simple yet efficient method to quantify uncertainty in the\nmodel's decisions. The method, which is based on entropy, can serve as a metric\nfor deferring low-confidence epochs to a human expert for further inspection.\nAdditionally, we demonstrate that the proposed SleepTransformer outperforms\nexisting methods at a lower computational cost and achieves state-of-the-art\nperformance on two experimental databases of different sizes.",
          "link": "http://arxiv.org/abs/2105.11043",
          "publishedOn": "2021-05-25T01:56:11.073Z",
          "wordCount": 622,
          "title": "SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification. (arXiv:2105.11043v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daniel Wontae Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Y. Park</a>",
          "description": "In this paper, we devise a distributional framework on actor-critic as a\nsolution to distributional instability, action type restriction, and conflation\nbetween samples and statistics. We propose a new method that minimizes the\nCram\\'er distance with the multi-step Bellman target distribution generated\nfrom a novel Sample-Replacement algorithm denoted SR($\\lambda$), which learns\nthe correct value distribution under multiple Bellman operations.\nParameterizing a value distribution with Gaussian Mixture Model further\nimproves the efficiency and the performance of the method, which we name GMAC.\nWe empirically show that GMAC captures the correct representation of value\ndistributions and improves the performance of a conventional actor-critic\nmethod with low computational cost, in both discrete and continuous action\nspaces using Arcade Learning Environment (ALE) and PyBullet environment.",
          "link": "http://arxiv.org/abs/2105.11366",
          "publishedOn": "2021-05-25T01:56:11.065Z",
          "wordCount": null,
          "title": "GMAC: A Distributional Perspective on Actor-Critic Framework. (arXiv:2105.11366v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thanh_Tung_H/0/1/0/all/0/1\">Hoang Thanh-Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>",
          "description": "Measuring the generalization capacity of Deep Generative Models (DGMs) is\ndifficult because of the curse of dimensionality. Evaluation metrics for DGMs\nsuch as Inception Score, Fr\\'echet Inception Distance, Precision-Recall, and\nNeural Net Divergence try to estimate the distance between the generated\ndistribution and the target distribution using a polynomial number of samples.\nThese metrics are the target of researchers when designing new models. Despite\nthe claims, it is still unclear how well can they measure the generalization\ncapacity of a generative model. In this paper, we investigate the capacity of\nthese metrics in measuring the generalization capacity. We introduce a\nframework for comparing the robustness of evaluation metrics. We show that\nbetter scores in these metrics do not imply better generalization. They can be\nfooled easily by a generator that memorizes a small subset of the training set.\nWe propose a fix to the NND metric to make it more robust to noise in the\ngenerated data. Toward building a robust metric for generalization, we propose\nto apply the Minimum Description Length principle to the problem of evaluating\nDGMs. We develop an efficient method for estimating the complexity of\nGenerative Latent Variable Models (GLVMs). Experimental results show that our\nmetric can effectively detect training set memorization and distinguish GLVMs\nof different generalization capacities. Source code is available at\nhttps://github.com/htt210/GeneralizationMetricGAN.",
          "link": "http://arxiv.org/abs/2011.00754",
          "publishedOn": "2021-05-25T01:56:11.057Z",
          "wordCount": null,
          "title": "Toward a Generalization Metric for Deep Generative Models. (arXiv:2011.00754v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.12632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zennaro_F/0/1/0/all/0/1\">Fabio Massimo Zennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdodi_L/0/1/0/all/0/1\">Laszlo Erdodi</a>",
          "description": "Penetration testing is a security exercise aimed at assessing the security of\na system by simulating attacks against it. So far, penetration testing has been\ncarried out mainly by trained human attackers and its success critically\ndepended on the available expertise. Automating this practice constitutes a\nnon-trivial problem, as the range of actions that a human expert may attempts\nagainst a system and the range of knowledge she relies on to take her decisions\nare hard to capture. In this paper, we focus our attention on simplified\npenetration testing problems expressed in the form of capture the flag hacking\nchallenges, and we analyze how model-free reinforcement learning algorithms may\nhelp to solve them. In modeling these capture the flag competitions as\nreinforcement learning problems we highlight that a specific challenge that\ncharacterize penetration testing is the problem of discovering the structure of\nthe problem at hand. We then show how this challenge may be eased by relying on\ndifferent forms of prior knowledge that may be provided to the agent. In this\nway we demonstrate how the feasibility of tackling penetration testing using\nreinforcement learning may rest on a careful trade-off between model-free and\nmodel-based algorithms. By using techniques to inject a priori knowledge, we\nshow it is possible to better direct the agent and restrict the space of its\nexploration problem, thus achieving solutions more efficiently.",
          "link": "http://arxiv.org/abs/2005.12632",
          "publishedOn": "2021-05-25T01:56:11.056Z",
          "wordCount": null,
          "title": "Modeling Penetration Testing with Reinforcement Learning Using Capture-the-Flag Challenges: Trade-offs between Model-free Learning and A Priori Knowledge. (arXiv:2005.12632v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Boxin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Changhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barton_R/0/1/0/all/0/1\">Robert Barton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiman_T/0/1/0/all/0/1\">Tal Neiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "Despite the prevalence of hypergraphs in a variety of high-impact\napplications, there are relatively few works on hypergraph representation\nlearning, most of which primarily focus on hyperlink prediction, often\nrestricted to the transductive learning setting. Among others, a major hurdle\nfor effective hypergraph representation learning lies in the label scarcity of\nnodes and/or hyperedges. To address this issue, this paper presents an\nend-to-end, bi-level pre-training strategy with Graph Neural Networks for\nhypergraphs. The proposed framework named HyperGene bears three distinctive\nadvantages. First, it is capable of ingesting the labeling information when\navailable, but more importantly, it is mainly designed in the self-supervised\nfashion which significantly broadens its applicability. Second, at the heart of\nthe proposed HyperGene are two carefully designed pretexts, one on the node\nlevel and the other on the hyperedge level, which enable us to encode both the\nlocal and the global context in a mutually complementary way. Third, the\nproposed framework can work in both transductive and inductive settings. When\napplying the two proposed pretexts in tandem, it can accelerate the adaptation\nof the knowledge from the pre-trained model to downstream applications in the\ntransductive setting, thanks to the bi-level nature of the proposed method. The\nextensive experimental results demonstrate that: (1) HyperGene achieves up to\n5.69% improvements in hyperedge classification, and (2) improves pre-training\nefficiency by up to 42.80% on average.",
          "link": "http://arxiv.org/abs/2105.10862",
          "publishedOn": "2021-05-25T01:56:11.033Z",
          "wordCount": null,
          "title": "Hypergraph Pre-training with Graph Neural Networks. (arXiv:2105.10862v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bellotti_A/0/1/0/all/0/1\">Anthony Bellotti</a>",
          "description": "Conformal predictors are an important class of algorithms that allow\npredictions to be made with a user-defined confidence level. They are able to\ndo this by outputting prediction sets, rather than simple point predictions.\nThe conformal predictor is valid in the sense that the accuracy of its\npredictions is guaranteed to meet the confidence level, only assuming\nexchangeability in the data. Since accuracy is guaranteed, the performance of a\nconformal predictor is measured through the efficiency of the prediction sets.\nTypically, a conformal predictor is built on an underlying machine learning\nalgorithm and hence its predictive power is inherited from this algorithm.\nHowever, since the underlying machine learning algorithm is not trained with\nthe objective of minimizing predictive efficiency it means that the resulting\nconformal predictor may be sub-optimal and not aligned sufficiently to this\nobjective. Hence, in this study we consider an approach to train the conformal\npredictor directly with maximum predictive efficiency as the optimization\nobjective, and we focus specifically on the inductive conformal predictor for\nclassification. To do this, the conformal predictor is approximated by a\ndifferentiable objective function and gradient descent used to optimize it. The\nresulting parameter estimates are then passed to a proper inductive conformal\npredictor to give valid prediction sets. We test the method on several real\nworld data sets and find that the method is promising and in most cases gives\nimproved predictive efficiency against a baseline conformal predictor.",
          "link": "http://arxiv.org/abs/2105.11255",
          "publishedOn": "2021-05-25T01:56:11.023Z",
          "wordCount": 659,
          "title": "Optimized conformal classification using gradient descent approximation. (arXiv:2105.11255v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Charlesworth_H/0/1/0/all/0/1\">Henry Charlesworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millea_A/0/1/0/all/0/1\">Adrian Millea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pottrill_E/0/1/0/all/0/1\">Eddie Pottrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riley_R/0/1/0/all/0/1\">Rich Riley</a>",
          "description": "Reinforcement learning (RL) is a general framework that allows systems to\nlearn autonomously through trial-and-error interaction with their environment.\nIn recent years combining RL with expressive, high-capacity neural network\nmodels has led to impressive performance in a diverse range of domains.\nHowever, dealing with the large state and action spaces often required for\nproblems in the real world still remains a significant challenge. In this paper\nwe introduce a new simulation environment, \"Gambit\", designed as a tool to\nbuild scenarios that can drive RL research in a direction useful for military\nanalysis. Using this environment we focus on an abstracted and simplified room\nclearance scenario, where a team of blue agents have to make their way through\na building and ensure that all rooms are cleared of (and remain clear) of enemy\nred agents. We implement a multi-agent version of feudal hierarchical RL that\nintroduces a command hierarchy where a commander at the higher level sends\norders to multiple agents at the lower level who simply have to learn to follow\nthese orders. We find that breaking the task down in this way allows us to\nsolve a number of non-trivial floorplans that require the coordination of\nmultiple agents much more efficiently than the standard baseline RL algorithms\nwe compare with. We then go on to explore how qualitatively different behaviour\ncan emerge depending on what we prioritise in the agent's reward function (e.g.\nclearing the building quickly vs. prioritising rescuing civilians).",
          "link": "http://arxiv.org/abs/2105.11328",
          "publishedOn": "2021-05-25T01:56:11.016Z",
          "wordCount": 668,
          "title": "Room Clearance with Feudal Hierarchical Reinforcement Learning. (arXiv:2105.11328v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "Training convolutional neural networks with a Lipschitz constraint under the\n$l_{2}$ norm is useful for provable adversarial robustness, interpretable\ngradients, stable training, etc. While 1-Lipschitz networks can be designed by\nimposing a 1-Lipschitz constraint on each layer, training such networks\nrequires each layer to be gradient norm preserving (GNP) to prevent gradients\nfrom vanishing. However, existing GNP convolutions suffer from slow training,\nlead to significant reduction in accuracy and provide no guarantees on their\napproximations. In this work, we propose a GNP convolution layer called\n\\methodnamebold\\ (\\methodabv) that uses the following mathematical property:\nwhen a matrix is {\\it Skew-Symmetric}, its exponential function is an {\\it\northogonal} matrix. To use this property, we first construct a convolution\nfilter whose Jacobian is Skew-Symmetric. Then, we use the Taylor series\nexpansion of the Jacobian exponential to construct the \\methodabv\\ layer that\nis orthogonal. To efficiently implement \\methodabv, we keep a finite number of\nterms from the Taylor series and provide a provable guarantee on the\napproximation error. Our experiments on CIFAR-10 and CIFAR-100 show that\n\\methodabv\\ allows us to train provably Lipschitz, large convolutional neural\nnetworks significantly faster than prior works while achieving significant\nimprovements for both standard and certified robust accuracies.",
          "link": "http://arxiv.org/abs/2105.11417",
          "publishedOn": "2021-05-25T01:56:10.999Z",
          "wordCount": null,
          "title": "Skew Orthogonal Convolutions. (arXiv:2105.11417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "The development of practical applications, such as autonomous driving and\nrobotics, has brought increasing attention to 3D point cloud understanding.\nWhile deep learning has achieved remarkable success on image-based tasks, there\nare many unique challenges faced by deep neural networks in processing massive,\nunstructured and noisy 3D points. To demonstrate the latest progress of deep\nlearning for 3D point cloud understanding, this paper summarizes recent\nremarkable research contributions in this area from several different\ndirections (classification, segmentation, detection, tracking, flow estimation,\nregistration, augmentation and completion), together with commonly used\ndatasets, metrics and state-of-the-art performances. More information regarding\nthis survey can be found at:\nhttps://github.com/SHI-Labs/3D-Point-Cloud-Learning.",
          "link": "http://arxiv.org/abs/2009.08920",
          "publishedOn": "2021-05-25T01:56:10.998Z",
          "wordCount": 562,
          "title": "Deep Learning for 3D Point Cloud Understanding: A Survey. (arXiv:2009.08920v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.04971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daniel_T/0/1/0/all/0/1\">Tal Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurutach_T/0/1/0/all/0/1\">Thanard Kurutach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1\">Aviv Tamar</a>",
          "description": "In anomaly detection (AD), one seeks to identify whether a test sample is\nabnormal, given a data set of normal samples. A recent and promising approach\nto AD relies on deep generative models, such as variational autoencoders\n(VAEs), for unsupervised learning of the normal data distribution. In\nsemi-supervised AD (SSAD), the data also includes a small sample of labeled\nanomalies. In this work, we propose two variational methods for training VAEs\nfor SSAD. The intuitive idea in both methods is to train the encoder to\n`separate' between latent vectors for normal and outlier data. We show that\nthis idea can be derived from principled probabilistic formulations of the\nproblem, and propose simple and effective algorithms. Our methods can be\napplied to various data types, as we demonstrate on SSAD datasets ranging from\nnatural images to astronomy and medicine, can be combined with any VAE model\narchitecture, and are naturally compatible with ensembling. When comparing to\nstate-of-the-art SSAD methods that are not specific to particular data types,\nwe obtain marked improvement in outlier detection.",
          "link": "http://arxiv.org/abs/1911.04971",
          "publishedOn": "2021-05-25T01:56:10.974Z",
          "wordCount": 631,
          "title": "Deep Variational Semi-Supervised Novelty Detection. (arXiv:1911.04971v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11612",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1\">Murat A. Erdogdu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseinzadeh_R/0/1/0/all/0/1\">Rasa Hosseinzadeh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1\">Matthew S. Zhang</a>",
          "description": "We study sampling from a target distribution $\\nu_* = e^{-f}$ using the\nunadjusted Langevin Monte Carlo (LMC) algorithm when the potential $f$\nsatisfies a strong dissipativity condition and it is first-order smooth with\nLipschitz gradient. We prove that, initialized with a Gaussian that has\nsufficiently small variance, $\\widetilde{\\mathcal{O}}(\\lambda d\\epsilon^{-1})$\nsteps of the LMC algorithm are sufficient to reach $\\epsilon$-neighborhood of\nthe target in Chi-square divergence, where $\\lambda$ is the log-Sobolev\nconstant of $\\nu_*$. Our results do not require warm-start to deal with\nexponential dimension dependency in Chi-square divergence at initialization. In\nparticular, for strongly convex and first-order smooth potentials, we show that\nthe LMC algorithm achieves the rate estimate\n$\\widetilde{\\mathcal{O}}(d\\epsilon^{-1})$ which improves the previously known\nrates in this metric, under the same assumptions. Translating to other metrics,\nour result also recovers the best-known rate estimates in KL divergence, total\nvariation and $2$-Wasserstein distance in the same setup. Finally, as we rely\non the log-Sobolev inequality, our framework covers a wide range of non-convex\npotentials that are first-order smooth and that exhibit strong convexity\noutside of a compact region.",
          "link": "http://arxiv.org/abs/2007.11612",
          "publishedOn": "2021-05-25T01:56:10.967Z",
          "wordCount": 691,
          "title": "Convergence of Langevin Monte Carlo in Chi-Square Divergence. (arXiv:2007.11612v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Obando_Ceron_J/0/1/0/all/0/1\">Johan S. Obando-Ceron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>",
          "description": "Since the introduction of DQN, a vast majority of reinforcement learning\nresearch has focused on reinforcement learning with deep neural networks as\nfunction approximators. New methods are typically evaluated on a set of\nenvironments that have now become standard, such as Atari 2600 games. While\nthese benchmarks help standardize evaluation, their computational cost has the\nunfortunate side effect of widening the gap between those with ample access to\ncomputational resources, and those without. In this work we argue that, despite\nthe community's emphasis on large-scale environments, the traditional\nsmall-scale environments can still yield valuable scientific insights and can\nhelp reduce the barriers to entry for underprivileged communities. To\nsubstantiate our claims, we empirically revisit the paper which introduced the\nRainbow algorithm [Hessel et al., 2018] and present some new insights into the\nalgorithms used by Rainbow.",
          "link": "http://arxiv.org/abs/2011.14826",
          "publishedOn": "2021-05-25T01:56:10.959Z",
          "wordCount": null,
          "title": "Revisiting Rainbow: Promoting more Insightful and Inclusive Deep Reinforcement Learning Research. (arXiv:2011.14826v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chaoqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "This paper explores semi-supervised anomaly detection, a more practical\nsetting for anomaly detection where a small additional set of labeled samples\nare provided. Based on the analysis of Deep SAD, the state-of-the-art for\nsemi-supervised anomaly detection, we propose a new KL-divergence based\nobjective function and show that two factors: the mutual information between\nthe data and latent representations, and the entropy of latent representations,\nconstitute an integral objective function for anomaly detection. To resolve the\ncontradiction in simultaneously optimizing the two factors, we propose a novel\nencoder-decoder-encoder structure, with the first encoder focusing on\noptimizing the mutual information and the second encoder focusing on optimizing\nthe entropy. The two encoders are enforced to share similar encoding with a\nconsistent constraint on their latent representations. Extensive experiments\nhave revealed that the proposed method significantly outperforms several\nstate-of-the-arts on multiple benchmark datasets, including medical diagnosis\nand several classic anomaly detection benchmarks.",
          "link": "http://arxiv.org/abs/2012.04905",
          "publishedOn": "2021-05-25T01:56:10.952Z",
          "wordCount": null,
          "title": "ESAD: End-to-end Deep Semi-supervised Anomaly Detection. (arXiv:2012.04905v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shivam Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasukurthi_N/0/1/0/all/0/1\">Nikhil Kasukurthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1\">Harshit Pande</a>",
          "description": "Screening for the diagnosis of glaucoma through a fundus image can be\ndetermined by the optic cup to disc diameter ratio (CDR), which requires the\nsegmentation of the cup and disc regions. In this paper, we propose two novel\napproaches, namely Parameter-Shared Branched Network (PSBN) andWeak Region of\nInterest Model-based segmentation (WRoIM) to identify disc and cup boundaries.\nUnlike the previous approaches, the proposed methods are trained end-to-end\nthrough a single neural network architecture and use dynamic cropping instead\nof manual or traditional computer vision-based cropping. We are able to achieve\nsimilar performance as that of state-of-the-art approaches with less number of\nnetwork parameters. Our experiments include comparison with different best\nknown methods on publicly available Drishti-GS1 and RIM-ONE v3 datasets. With\n$7.8 \\times 10^6$ parameters our approach achieves a Dice score of 0.96/0.89\nfor disc/cup segmentation on Drishti-GS1 data whereas the existing\nstate-of-the-art approach uses $19.8\\times 10^6$ parameters to achieve a dice\nscore of 0.97/0.89.",
          "link": "http://arxiv.org/abs/2105.11364",
          "publishedOn": "2021-05-25T01:56:10.948Z",
          "wordCount": null,
          "title": "Dynamic region proposal networks for semantic segmentation in automated glaucoma screening. (arXiv:2105.11364v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.03667",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Senzhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "Node representation learning for directed graphs is critically important to\nfacilitate many graph mining tasks. To capture the directed edges between\nnodes, existing methods mostly learn two embedding vectors for each node,\nsource vector and target vector. However, these methods learn the source and\ntarget vectors separately. For the node with very low indegree or outdegree,\nthe corresponding target vector or source vector cannot be effectively learned.\nIn this paper, we propose a novel Directed Graph embedding framework based on\nGenerative Adversarial Network, called DGGAN. The main idea is to use\nadversarial mechanisms to deploy a discriminator and two generators that\njointly learn each node's source and target vectors. For a given node, the two\ngenerators are trained to generate its fake target and source neighbor nodes\nfrom the same underlying distribution, and the discriminator aims to\ndistinguish whether a neighbor node is real or fake. The two generators are\nformulated into a unified framework and could mutually reinforce each other to\nlearn more robust source and target vectors. Extensive experiments show that\nDGGAN consistently and significantly outperforms existing state-of-the-art\nmethods across multiple graph mining tasks on directed graphs.",
          "link": "http://arxiv.org/abs/2008.03667",
          "publishedOn": "2021-05-25T01:56:10.948Z",
          "wordCount": null,
          "title": "Adversarial Directed Graph Embedding. (arXiv:2008.03667v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.13654",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1\">Soufiane Hayou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rousseau_J/0/1/0/all/0/1\">Judith Rousseau</a>",
          "description": "Recent work by Jacot et al. (2018) has shown that training a neural network\nof any kind with gradient descent in parameter space is strongly related to\nkernel gradient descent in function space with respect to the Neural Tangent\nKernel (NTK). Lee et al. (2019) built on this result by establishing that the\noutput of a neural network trained using gradient descent can be approximated\nby a linear model for wide networks. In parallel, a recent line of studies\n(Schoenholz et al. 2017; Hayou et al. 2019) has suggested that a special\ninitialization, known as the Edge of Chaos, improves training. In this paper,\nwe bridge the gap between these two concepts by quantifying the impact of the\ninitialization and the activation function on the NTK when the network depth\nbecomes large. In particular, we show that the performance of wide deep neural\nnetworks cannot be explained by the NTK regime and we provide experiments\nillustrating our theoretical results.",
          "link": "http://arxiv.org/abs/1905.13654",
          "publishedOn": "2021-05-25T01:56:10.947Z",
          "wordCount": 683,
          "title": "Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks. (arXiv:1905.13654v10 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohenecker_P/0/1/0/all/0/1\">Patrick Hohenecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "In this paper, we introduce the new task of controllable text edition, in\nwhich we take as input a long text, a question, and a target answer, and the\noutput is a minimally modified text, so that it fits the target answer. This\ntask is very important in many situations, such as changing some conditions,\nconsequences, or properties in a legal document, or changing some key\ninformation of an event in a news text. This is very challenging, as it is hard\nto obtain a parallel corpus for training, and we need to first find all text\npositions that should be changed and then decide how to change them. We\nconstructed the new dataset WikiBioCTE for this task based on the existing\ndataset WikiBio (originally created for table-to-text generation). We use\nWikiBioCTE for training, and manually labeled a test set for testing. We also\npropose novel evaluation metrics and a novel method for solving the new task.\nExperimental results on the test set show that our proposed method is a good\nfit for this novel NLP task.",
          "link": "http://arxiv.org/abs/2105.11018",
          "publishedOn": "2021-05-25T01:56:10.908Z",
          "wordCount": 616,
          "title": "Controlling Text Edition by Changing Answers of Specific Questions. (arXiv:2105.11018v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1\">Gang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lia_X/0/1/0/all/0/1\">Xinde Lia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khyam_M/0/1/0/all/0/1\">Mohammad Omar Khyam</a>",
          "description": "(This work has been submitted to the IEEE for possible publication. Copyright\nmay be transferred without notice, after which this version may no longer be\naccessible.)\n\nTo improve the efficiency of deep reinforcement learning (DRL)-based methods\nfor robot manipulator trajectory planning in random working environments, we\npresent three dense reward functions. These rewards differ from the traditional\nsparse reward. First, a posture reward function is proposed to speed up the\nlearning process with a more reasonable trajectory by modeling the distance and\ndirection constraints, which can reduce the blindness of exploration. Second, a\nstride reward function is proposed to improve the stability of the learning\nprocess by modeling the distance and movement distance of joint constraints.\nFinally, in order to further improve learning efficiency, we are inspired by\nthe cognitive process of human behavior and propose a stage incentive\nmechanism, including a hard stage incentive reward function and a soft stage\nincentive reward function. Extensive experiments show that the soft stage\nincentive reward function is able to improve the convergence rate by up to\n46.9% with the state-of-the-art DRL methods. The percentage increase in the\nconvergence mean reward was 4.4-15.5% and the percentage decreases with respect\nto standard deviation were 21.9-63.2%. In the evaluation experiments, the\nsuccess rate of trajectory planning for a robot manipulator reached 99.6%.",
          "link": "http://arxiv.org/abs/2009.12068",
          "publishedOn": "2021-05-25T01:56:10.839Z",
          "wordCount": 689,
          "title": "Deep Reinforcement Learning with a Stage Incentive Mechanism of Dense Reward for Robotic Trajectory Planning. (arXiv:2009.12068v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiantao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fanqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunxiuzi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jin Zhou</a>",
          "description": "Disentanglement is a highly desirable property of representation owing to its\nsimilarity to human understanding and reasoning. Many works achieve\ndisentanglement upon information bottlenecks (IB). Despite their elegant\nmathematical foundations, the IB branch usually exhibits lower performance. In\norder to provide an insight into the problem, we develop an annealing test to\ncalculate the information freezing point (IFP), which is a transition state to\nfreeze information into the latent variables. We also explore these clues or\ninductive biases for separating the entangled factors according to the\ndifferences in the IFP distributions. We found the existing approaches suffer\nfrom the information diffusion problem, according to which the increased\ninformation diffuses in all latent variables.\n\nBased on this insight, we propose a novel disentanglement framework, termed\nthe distilling entangled factor (DEFT), to address the information diffusion\nproblem by scaling backward information. DEFT applies a multistage training\nstrategy, including multigroup encoders with different learning rates and\npiecewise disentanglement pressure, to disentangle the factors stage by stage.\nWe evaluate DEFT on three variants of dSprite and SmallNORB, which show\nlow-variance and high-level disentanglement scores. Furthermore, the experiment\nunder the correlative factors shows incapable of TC-based approaches. DEFT also\nexhibits a competitive performance in the unsupervised setting.",
          "link": "http://arxiv.org/abs/2102.03986",
          "publishedOn": "2021-05-25T01:56:10.697Z",
          "wordCount": 669,
          "title": "DEFT: Distilling Entangled Factors by Preventing Information Diffusion. (arXiv:2102.03986v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yanying Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhu-Jun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silven_O/0/1/0/all/0/1\">Olli Silv&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>",
          "description": "Inspired by the success of classical neural networks, there has been\ntremendous effort to develop classical effective neural networks into quantum\nconcept. In this paper, a novel hybrid quantum-classical neural network with\ndeep residual learning (Res-HQCNN) is proposed. We firstly analysis how to\nconnect residual block structure with a quantum neural network, and give the\ncorresponding training algorithm. At the same time, the advantages and\ndisadvantages of transforming deep residual learning into quantum concept are\nprovided. As a result, the model can be trained in an end-to-end fashion,\nanalogue to the backpropagation in classical neural networks.\n\nTo explore the effectiveness of Res-HQCNN , we perform extensive experiments\nfor quantum data with or without noisy on classical computer. The experimental\nresults show the Res-HQCNN performs better to learn an unknown unitary\ntransformation and has stronger robustness for noisy data, when compared to\nstate of the arts. Moreover, the possible methods of combining residual\nlearning with quantum neural networks are also discussed.",
          "link": "http://arxiv.org/abs/2012.07772",
          "publishedOn": "2021-05-25T01:56:10.685Z",
          "wordCount": 630,
          "title": "A hybrid quantum-classical neural network with deep residual learning. (arXiv:2012.07772v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08590",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abdar_M/0/1/0/all/0/1\">Moloud Abdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salari_S/0/1/0/all/0/1\">Soorena Salari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qahremani_S/0/1/0/all/0/1\">Sina Qahremani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_H/0/1/0/all/0/1\">Hak-Keung Lam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1\">Sadiq Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1\">U. Rajendra Acharya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>",
          "description": "The COVID-19 (Coronavirus disease 2019) has infected more than 151 million\npeople and caused approximately 3.17 million deaths around the world up to the\npresent. The rapid spread of COVID-19 is continuing to threaten human's life\nand health. Therefore, the development of computer-aided detection (CAD)\nsystems based on machine and deep learning methods which are able to accurately\ndifferentiate COVID-19 from other diseases using chest computed tomography (CT)\nand X-Ray datasets is essential and of immediate priority. Different from most\nof the previous studies which used either one of CT or X-ray images, we\nemployed both data types with sufficient samples in implementation. On the\nother hand, due to the extreme sensitivity of this pervasive virus, model\nuncertainty should be considered, while most previous studies have overlooked\nit. Therefore, we propose a novel powerful fusion model named\n$UncertaintyFuseNet$ that consists of an uncertainty module: Ensemble Monte\nCarlo (EMC) dropout. The obtained results prove the effectiveness of our\nproposed fusion for COVID-19 detection using CT scan and X-Ray datasets. Also,\nour proposed $UncertaintyFuseNet$ model is significantly robust to noise and\nperforms well with the previously unseen data. The source codes and models of\nthis study are available at:\nhttps://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.",
          "link": "http://arxiv.org/abs/2105.08590",
          "publishedOn": "2021-05-25T01:56:10.675Z",
          "wordCount": 733,
          "title": "UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion with Ensemble Monte Carlo Dropout for COVID-19 Detection. (arXiv:2105.08590v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Co_K/0/1/0/all/0/1\">Kenneth T. Co</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Gonzalez_L/0/1/0/all/0/1\">Luis Mu&#xf1;oz-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanthan_L/0/1/0/all/0/1\">Leslie Kanthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1\">Emil C. Lupu</a>",
          "description": "Universal Adversarial Perturbations (UAPs) are a prominent class of\nadversarial examples that exploit the systemic vulnerabilities and enable\nphysically realizable and robust attacks against Deep Neural Networks (DNNs).\nUAPs generalize across many different inputs; this leads to realistic and\neffective attacks that can be applied at scale. In this paper we propose\nHyperNeuron, an efficient and scalable algorithm that allows for the real-time\ndetection of UAPs by identifying suspicious neuron hyper-activations. Our\nresults show the effectiveness of HyperNeuron on multiple tasks (image\nclassification, object detection), against a wide variety of universal attacks,\nand in realistic scenarios, like perceptual ad-blocking and adversarial\npatches. HyperNeuron is able to simultaneously detect both adversarial mask and\npatch UAPs with comparable or better performance than existing UAP defenses\nwhilst introducing a significantly reduced latency of only 0.86 milliseconds\nper image. This suggests that many realistic and practical universal attacks\ncan be reliably mitigated in real-time, which shows promise for the robust\ndeployment of machine learning systems.",
          "link": "http://arxiv.org/abs/2105.07334",
          "publishedOn": "2021-05-25T01:56:10.668Z",
          "wordCount": 617,
          "title": "Real-time Detection of Practical Universal Adversarial Perturbations. (arXiv:2105.07334v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Briceno_Mena_L/0/1/0/all/0/1\">Luis A. Briceno-Mena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arges_C/0/1/0/all/0/1\">Christopher G. Arges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romagnoli_J/0/1/0/all/0/1\">Jose A. Romagnoli</a>",
          "description": "Widespread adoption of high-temperature polymer electrolyte membrane fuel\ncells (HT-PEMFCs) and HT-PEM electrochemical hydrogen pumps (HT-PEM ECHPs)\nrequires models and computational tools that provide accurate scale-up and\noptimization. Knowledge-based modeling has limitations as it is time consuming\nand requires information about the system that is not always available (e.g.,\nmaterial properties and interfacial behavior between different materials).\nData-driven modeling on the other hand, is easier to implement, but often\nnecessitates large datasets that could be difficult to obtain. In this\ncontribution, knowledge-based modeling and data-driven modeling are uniquely\ncombined by implementing a Few-Shot Learning (FSL) approach. A knowledge-based\nmodel originally developed for a HT-PEMFC was used to generate simulated data\n(887,735 points) and used to pretrain a neural network source model.\nFurthermore, the source model developed for HT-PEMFCs was successfully applied\nto HT-PEM ECHPs - a different electrochemical system that utilizes similar\nmaterials to the fuel cell. Experimental datasets from both HT-PEMFCs and\nHT-PEM ECHPs with different materials and operating conditions (~50 points\neach) were used to train 8 target models via FSL. Models for the unseen data\nreached high accuracies in all cases (rRMSE between 1.04 and 3.73% for HT-PEMCs\nand between 6.38 and 8.46% for HT-PEM ECHPs).",
          "link": "http://arxiv.org/abs/2105.03057",
          "publishedOn": "2021-05-25T01:56:10.649Z",
          "wordCount": 664,
          "title": "PEMNET: A Transfer Learning-based Modeling Approach of High-Temperature Polymer Electrolyte Membrane Electrochemical Systems. (arXiv:2105.03057v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yuejia Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Semantic embedding has been widely investigated for aligning knowledge graph\n(KG) entities. Current methods have explored and utilized the graph structure,\nthe entity names and attributes, but ignore the ontology (or ontological\nschema) which contains critical meta information such as classes and their\nmembership relationships with entities. In this paper, we propose an\nontology-guided entity alignment method named OntoEA, where both KGs and their\nontologies are jointly embedded, and the class hierarchy and the class\ndisjointness are utilized to avoid false mappings. Extensive experiments on\nseven public and industrial benchmarks have demonstrated the state-of-the-art\nperformance of OntoEA and the effectiveness of the ontologies.",
          "link": "http://arxiv.org/abs/2105.07688",
          "publishedOn": "2021-05-25T01:56:10.642Z",
          "wordCount": 566,
          "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding. (arXiv:2105.07688v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1\">Jeff Ichnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "Searching for objects in indoor organized environments such as homes or\noffices is part of our everyday activities. When looking for a target object,\nwe jointly reason about the rooms and containers the object is likely to be in;\nthe same type of container will have a different probability of having the\ntarget depending on the room it is in. We also combine geometric and semantic\ninformation to infer what container is best to search, or what other objects\nare best to move, if the target object is hidden from view. We propose to use a\n3D scene graph representation to capture the hierarchical, semantic, and\ngeometric aspects of this problem. To exploit this representation in a search\nprocess, we introduce Hierarchical Mechanical Search (HMS), a method that\nguides an agent's actions towards finding a target object specified with a\nnatural language description. HMS is based on a novel neural network\narchitecture that uses neural message passing of vectors with visual,\ngeometric, and linguistic information to allow HMS to reason across layers of\nthe graph while combining semantic and geometric cues. HMS is evaluated on a\nnovel dataset of 500 3D scene graphs with dense placements of semantically\nrelated objects in storage locations, and is shown to be significantly better\nthan several baselines at finding objects and close to the oracle policy in\nterms of the median number of actions required. Additional qualitative results\ncan be found at https://ai.stanford.edu/mech-search/hms.",
          "link": "http://arxiv.org/abs/2012.04060",
          "publishedOn": "2021-05-25T01:56:10.550Z",
          "wordCount": 728,
          "title": "Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search. (arXiv:2012.04060v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tony Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zmyslony_S/0/1/0/all/0/1\">Szymon Zmyslony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozdrenkov_S/0/1/0/all/0/1\">Sergei Nozdrenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1\">Matthew Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopkins_B/0/1/0/all/0/1\">Brandon Hopkins</a>",
          "description": "Honey bees are critical to our ecosystem and food security as a pollinator,\ncontributing 35% of our global agriculture yield. In spite of their importance,\nbeekeeping is exclusively dependent on human labor and experience-derived\nheuristics, while requiring frequent human checkups to ensure the colony is\nhealthy, which can disrupt the colony. Increasingly, pollinator populations are\ndeclining due to threats from climate change, pests, environmental toxicity,\nmaking their management even more critical than ever before in order to ensure\nsustained global food security. To start addressing this pressing challenge, we\ndeveloped an integrated hardware sensing system for beehive monitoring through\naudio and environment measurements, and a hierarchical semi-supervised deep\nlearning model, composed of an audio modeling module and a predictor, to model\nthe strength of beehives. The model is trained jointly on audio reconstruction\nand prediction losses based on human inspections, in order to model both\nlow-level audio features and circadian temporal dynamics. We show that this\nmodel performs well despite limited labels, and can learn an audio embedding\nthat is useful for characterizing different sound profiles of beehives. This is\nthe first instance to our knowledge of applying audio-based deep learning to\nmodel beehives and population size in an observational setting across a large\nnumber of hives.",
          "link": "http://arxiv.org/abs/2105.10536",
          "publishedOn": "2021-05-25T01:56:10.537Z",
          "wordCount": 642,
          "title": "Semi-Supervised Audio Representation Learning for Modeling Beehive Strengths. (arXiv:2105.10536v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11397",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ji_W/0/1/0/all/0/1\">Weiqi Ji</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Richter_F/0/1/0/all/0/1\">Franz Richter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gollner_M/0/1/0/all/0/1\">Michael J. Gollner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Deng_S/0/1/0/all/0/1\">Sili Deng</a>",
          "description": "Modeling the burning processes of biomass such as wood, grass, and crops is\ncrucial for the modeling and prediction of wildland and urban fire behavior.\nDespite its importance, the burning of solid fuels remains poorly understood,\nwhich can be partly attributed to the unknown chemical kinetics of most solid\nfuels. Most available kinetic models were built upon expert knowledge, which\nrequires chemical insights and years of experience. This work presents a\nframework for autonomously discovering biomass pyrolysis kinetic models from\nthermogravimetric analyzer (TGA) experimental data using the recently developed\nchemical reaction neural networks (CRNN). The approach incorporated the CRNN\nmodel into the framework of neural ordinary differential equations to predict\nthe residual mass in TGA data. In addition to the flexibility of\nneural-network-based models, the learned CRNN model is fully interpretable, by\nincorporating the fundamental physics laws, such as the law of mass action and\nArrhenius law, into the neural network structure. The learned CRNN model can\nthen be translated into the classical forms of biomass chemical kinetic models,\nwhich facilitates the extraction of chemical insights and the integration of\nthe kinetic model into large-scale fire simulations. We demonstrated the\neffectiveness of the framework in predicting the pyrolysis and oxidation of\ncellulose. This successful demonstration opens the possibility of rapid and\nautonomous chemical kinetic modeling of solid fuels, such as wildfire fuels and\nindustrial polymers.",
          "link": "http://arxiv.org/abs/2105.11397",
          "publishedOn": "2021-05-25T01:56:10.529Z",
          "wordCount": 658,
          "title": "Autonomous Kinetic Modeling of Biomass Pyrolysis using Chemical Reaction Neural Networks. (arXiv:2105.11397v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Le Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Leilei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bowen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanren Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1\">Weifeng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>",
          "description": "Representation learning on heterogeneous graphs aims to obtain meaningful\nnode representations to facilitate various downstream tasks, such as node\nclassification and link prediction. Existing heterogeneous graph learning\nmethods are primarily developed by following the propagation mechanism of node\nrepresentations. There are few efforts on studying the role of relations for\nimproving the learning of more fine-grained node representations. Indeed, it is\nimportant to collaboratively learn the semantic representations of relations\nand discern node representations with respect to different relation types. To\nthis end, in this paper, we propose a novel Relation-aware Heterogeneous Graph\nNeural Network, namely R-HGNN, to learn node representations on heterogeneous\ngraphs at a fine-grained level by considering relation-aware characteristics.\nSpecifically, a dedicated graph convolution component is first designed to\nlearn unique node representations from each relation-specific graph separately.\nThen, a cross-relation message passing module is developed to improve the\ninteractions of node representations across different relations. Also, the\nrelation representations are learned in a layer-wise manner to capture relation\nsemantics, which are used to guide the node representation learning process.\nMoreover, a semantic fusing module is presented to aggregate relation-aware\nnode representations into a compact representation with the learned relation\nrepresentations. Finally, we conduct extensive experiments on a variety of\ngraph learning tasks, and experimental results demonstrate that our approach\nconsistently outperforms existing methods among all the tasks.",
          "link": "http://arxiv.org/abs/2105.11122",
          "publishedOn": "2021-05-25T01:56:10.410Z",
          "wordCount": 658,
          "title": "Heterogeneous Graph Representation Learning with Relation Awareness. (arXiv:2105.11122v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04525",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1\">Michael Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1\">Evis Sala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1\">Leonardo Rundo</a>",
          "description": "Automatic segmentation methods are an important advancement in medical image\nanalysis. Machine learning techniques, and deep neural networks in particular,\nare the state-of-the-art for most medical image segmentation tasks. Issues with\nclass imbalance pose a significant challenge in medical datasets, with lesions\noften occupying a considerably smaller volume relative to the background. Loss\nfunctions used in the training of deep learning algorithms differ in their\nrobustness to class imbalance, with direct consequences for model convergence.\nThe most commonly used loss functions for segmentation are based on either the\ncross entropy loss, Dice loss or a combination of the two. We propose a Unified\nFocal loss, a new framework that generalises Dice and cross entropy-based\nlosses for handling class imbalance. We evaluate our proposed loss function on\nthree highly class imbalanced, publicly available medical imaging datasets:\nBreast Ultrasound 2017 (BUS2017), Brain Tumour Segmentation 2020 (BraTS20) and\nKidney Tumour Segmentation 2019 (KiTS19). We compare our loss function\nperformance against six Dice or cross entropy-based loss functions, and\ndemonstrate that our proposed loss function is robust to class imbalance,\noutperforming the other loss functions across datasets. Finally, we use the\nUnified Focal loss together with deep supervision to achieve state-of-the-art\nresults without modification of the original U-Net architecture, with a mean\nDice similarity coefficient (DSC)=0.948 on BUS2017, enhancing tumour region\nDSC=0.800 on BraTS20 and kidney tumour DSC=0.758 on KiTS19. This highlights the\nimportance of carefully selecting a suitable loss function prior to the use of\nmore complex architectures.",
          "link": "http://arxiv.org/abs/2102.04525",
          "publishedOn": "2021-05-25T01:56:10.386Z",
          "wordCount": 734,
          "title": "Unified Focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation. (arXiv:2102.04525v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yan Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhiwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuaiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>",
          "description": "We present a new practical framework based on deep reinforcement learning and\ndecision-time planning for real-world vehicle repositioning on ride-hailing (a\ntype of mobility-on-demand, MoD) platforms. Our approach learns the\nspatiotemporal state-value function using a batch training algorithm with deep\nvalue networks. The optimal repositioning action is generated on-demand through\nvalue-based policy search, which combines planning and bootstrapping with the\nvalue networks. For the large-fleet problems, we develop several algorithmic\nfeatures that we incorporate into our framework and that we demonstrate to\ninduce coordination among the algorithmically-guided vehicles. We benchmark our\nalgorithm with baselines in a ride-hailing simulation environment to\ndemonstrate its superiority in improving income efficiency meausred by\nincome-per-hour. We have also designed and run a real-world experiment program\nwith regular drivers on a major ride-hailing platform. We have observed\nsignificantly positive results on key metrics comparing our method with\nexperienced drivers who performed idle-time repositioning based on their own\nexpertise.",
          "link": "http://arxiv.org/abs/2103.04555",
          "publishedOn": "2021-05-25T01:56:10.340Z",
          "wordCount": 620,
          "title": "Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement Learning. (arXiv:2103.04555v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yun-Shiuan Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuezhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuzhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1\">Mark K. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austerweil_J/0/1/0/all/0/1\">Joseph L. Austerweil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaojin Zhu</a>",
          "description": "Successful teaching requires an assumption of how the learner learns - how\nthe learner uses experiences from the world to update their internal states. We\ninvestigate what expectations people have about a learner when they teach them\nin an online manner using rewards and punishment. We focus on a common\nreinforcement learning method, Q-learning, and examine what assumptions people\nhave using a behavioral experiment. To do so, we first establish a normative\nstandard, by formulating the problem as a machine teaching optimization\nproblem. To solve the machine teaching optimization problem, we use a deep\nlearning approximation method which simulates learners in the environment and\nlearns to predict how feedback affects the learner's internal states. What do\npeople assume about a learner's learning and discount rates when they teach\nthem an idealized exploration-exploitation task? In a behavioral experiment, we\nfind that people can teach the task to Q-learners in a relatively efficient and\neffective manner when the learner uses a small value for its discounting rate\nand a large value for its learning rate. However, they still are suboptimal. We\nalso find that providing people with real-time updates of how possible feedback\nwould affect the Q-learner's internal states weakly helps them teach. Our\nresults reveal how people teach using evaluative feedback and provide guidance\nfor how engineers should design machine agents in a manner that is intuitive\nfor people.",
          "link": "http://arxiv.org/abs/2009.02476",
          "publishedOn": "2021-05-25T01:56:10.332Z",
          "wordCount": 707,
          "title": "Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners. (arXiv:2009.02476v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Teng-Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamal_A/0/1/0/all/0/1\">Aly El Gamal</a>",
          "description": "The Information bottleneck (IB) method enables optimizing over the trade-off\nbetween compression of data and prediction accuracy of learned representations,\nand has successfully and robustly been applied to both supervised and\nunsupervised representation learning problems. However, IB has several\nlimitations. First, the IB problem is hard to optimize. The IB Lagrangian\n$\\mathcal{L}_{IB}:=I(X;Z)-\\beta I(Y;Z)$ is non-convex and existing solutions\nguarantee only local convergence. As a result, the obtained solutions depend on\ninitialization. Second, the evaluation of a solution is also a challenging\ntask. Conventionally, it resorts to characterizing the information plane, that\nis, plotting $I(Y;Z)$ versus $I(X;Z)$ for all solutions obtained from different\ninitial points. Furthermore, the IB Lagrangian has phase transitions while\nvarying the multiplier $\\beta$. At phase transitions, both $I(X;Z)$ and\n$I(Y;Z)$ increase abruptly and the rate of convergence becomes significantly\nslow for existing solutions. Recent works with IB adopt variational surrogate\nbounds to the IB Lagrangian. Although allowing efficient optimization, how\nclose are these surrogates to the IB Lagrangian is not clear. In this work, we\nsolve the IB Lagrangian using augmented Lagrangian methods. With augmented\nvariables, we show that the IB objective can be solved with the alternating\ndirection method of multipliers (ADMM). Different from prior works, we prove\nthat the proposed algorithm is consistently convergent, regardless of the value\nof $\\beta$. Empirically, our gradient-descent-based method results in\ninformation plane points that are comparable to those obtained through the\nconventional Blahut-Arimoto-based solvers and is convergent for a wider range\nof the penalty coefficient than previous ADMM solvers.",
          "link": "http://arxiv.org/abs/2102.04729",
          "publishedOn": "2021-05-25T01:56:10.318Z",
          "wordCount": 711,
          "title": "A Provably Convergent Information Bottleneck Solution via ADMM. (arXiv:2102.04729v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04855",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhou_Y/0/1/0/all/0/1\">Yifan Zhou</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>",
          "description": "Transient stability assessment (TSA) is a cornerstone for resilient\noperations of today's interconnected power grids. This paper is a confluence of\nquantum computing, data science and machine learning to potentially address the\npower system TSA challenge. We devise a quantum TSA (qTSA) method to enable\nscalable and efficient data-driven transient stability prediction for bulk\npower systems, which is the first attempt to tackle the TSA issue with quantum\ncomputing. Our contributions are three-fold: 1) A low-depth, high\nexpressibility quantum neural network for accurate and noise-resilient TSA; 2)\nA quantum natural gradient descent algorithm for efficient qTSA training; 3) A\nsystematical analysis on qTSA's performance under various quantum factors. qTSA\nunderpins a foundation of quantum-enabled and data-driven power grid stability\nanalytics. It renders the intractable TSA straightforward and effortless in the\nHilbert space, and therefore provides stability information for power system\noperations. Extensive experiments on quantum simulators and real quantum\ncomputers verify the accuracy, noise-resilience, scalability and universality\nof qTSA.",
          "link": "http://arxiv.org/abs/2104.04855",
          "publishedOn": "2021-05-25T01:56:10.295Z",
          "wordCount": 610,
          "title": "Noise-Resilient Quantum Machine Learning for Stability Assessment of Power Systems. (arXiv:2104.04855v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xucheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanxing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Ce Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>",
          "description": "Weakly-supervised anomaly detection aims at learning an anomaly detector from\na limited amount of labeled data and abundant unlabeled data. Recent works\nbuild deep neural networks for anomaly detection by discriminatively mapping\nthe normal samples and abnormal samples to different regions in the feature\nspace or fitting different distributions. However, due to the limited number of\nannotated anomaly samples, directly training networks with the discriminative\nloss may not be sufficient. To overcome this issue, this paper proposes a novel\nstrategy to transform the input data into a more meaningful representation that\ncould be used for anomaly detection. Specifically, we leverage an autoencoder\nto encode the input data and utilize three factors, hidden representation,\nreconstruction residual vector, and reconstruction error, as the new\nrepresentation for the input data. This representation amounts to encode a test\nsample with its projection on the training data manifold, its direction to its\nprojection and its distance to its projection. In addition to this encoding, we\nalso propose a novel network architecture to seamlessly incorporate those three\nfactors. From our extensive experiments, the benefits of the proposed strategy\nare clearly demonstrated by its superior performance over the competitive\nmethods.",
          "link": "http://arxiv.org/abs/2105.10500",
          "publishedOn": "2021-05-25T01:56:10.261Z",
          "wordCount": 644,
          "title": "Feature Encoding with AutoEncoders for Weakly-supervised Anomaly Detection. (arXiv:2105.10500v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corney_D/0/1/0/all/0/1\">David Corney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1\">Maram Hasanain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1\">Tamer Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_Cedeno_A/0/1/0/all/0/1\">Alberto Barr&#xf3;n-Cede&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papotti_P/0/1/0/all/0/1\">Paolo Papotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>",
          "description": "The reporting and the analysis of current events around the globe has\nexpanded from professional, editor-lead journalism all the way to citizen\njournalism. Nowadays, politicians and other key players enjoy direct access to\ntheir audiences through social media, bypassing the filters of official cables\nor traditional media. However, the multiple advantages of free speech and\ndirect communication are dimmed by the misuse of media to spread inaccurate or\nmisleading claims. These phenomena have led to the modern incarnation of the\nfact-checker -- a professional whose main aim is to examine claims using\navailable evidence and to assess their veracity. As in other text forensics\ntasks, the amount of information available makes the work of the fact-checker\nmore difficult. With this in mind, starting from the perspective of the\nprofessional fact-checker, we survey the available intelligent technologies\nthat can support the human expert in the different steps of her fact-checking\nendeavor. These include identifying claims worth fact-checking, detecting\nrelevant previously fact-checked claims, retrieving relevant evidence to\nfact-check a claim, and actually verifying a claim. In each case, we pay\nattention to the challenges in future work and the potential impact on\nreal-world fact-checking.",
          "link": "http://arxiv.org/abs/2103.07769",
          "publishedOn": "2021-05-25T01:56:10.220Z",
          "wordCount": 692,
          "title": "Automated Fact-Checking for Assisting Human Fact-Checkers. (arXiv:2103.07769v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09313",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sener_O/0/1/0/all/0/1\">Ozan Sener</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1\">George Deligiannidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1\">Murat A. Erdogdu</a>",
          "description": "Despite its success in a wide range of applications, characterizing the\ngeneralization properties of stochastic gradient descent (SGD) in non-convex\ndeep learning problems is still an important challenge. While modeling the\ntrajectories of SGD via stochastic differential equations (SDE) under\nheavy-tailed gradient noise has recently shed light over several peculiar\ncharacteristics of SGD, a rigorous treatment of the generalization properties\nof such SDEs in a learning theoretical framework is still missing. Aiming to\nbridge this gap, in this paper, we prove generalization bounds for SGD under\nthe assumption that its trajectories can be well-approximated by a \\emph{Feller\nprocess}, which defines a rich class of Markov processes that include several\nrecent SDE representations (both Brownian or heavy-tailed) as its special case.\nWe show that the generalization error can be controlled by the \\emph{Hausdorff\ndimension} of the trajectories, which is intimately linked to the tail behavior\nof the driving process. Our results imply that heavier-tailed processes should\nachieve better generalization; hence, the tail-index of the process can be used\nas a notion of \"capacity metric\". We support our theory with experiments on\ndeep neural networks illustrating that the proposed capacity metric accurately\nestimates the generalization error, and it does not necessarily grow with the\nnumber of parameters unlike the existing capacity metrics in the literature.",
          "link": "http://arxiv.org/abs/2006.09313",
          "publishedOn": "2021-05-25T01:56:10.208Z",
          "wordCount": 696,
          "title": "Hausdorff Dimension, Heavy Tails, and Generalization in Neural Networks. (arXiv:2006.09313v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sean Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>",
          "description": "Graph neural networks (GNNs) have been extensively studied for prediction\ntasks on graphs. As pointed out by recent studies, most GNNs assume local\nhomophily, i.e., strong similarities in local neighborhoods. This assumption\nhowever limits the generalizability power of GNNs. To address this limitation,\nwe propose a flexible GNN model, which is capable of handling any graphs\nwithout being restricted by their underlying homophily. At its core, this model\nadopts a node attention mechanism based on multiple learnable spectral filters;\ntherefore, the aggregation scheme is learned adaptively for each graph in the\nspectral domain. We evaluated the proposed model on node classification tasks\nover eight benchmark datasets. The proposed model is shown to generalize well\nto both homophilic and heterophilic graphs. Further, it outperforms all\nstate-of-the-art baselines on heterophilic graphs and performs comparably with\nthem on homophilic graphs.",
          "link": "http://arxiv.org/abs/2103.14187",
          "publishedOn": "2021-05-25T01:56:10.189Z",
          "wordCount": 599,
          "title": "Beyond Low-Pass Filters: Adaptive Feature Propagation on Graphs. (arXiv:2103.14187v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sowmen Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Arup Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Md. Ruhul Amin</a>",
          "description": "The creation of altered and manipulated faces has become more common due to\nthe improvement of DeepFake generation methods. Simultaneously, we have seen\ndetection models' development for differentiating between a manipulated and\noriginal face from image or video content. We have observed that most publicly\navailable DeepFake detection datasets have limited variations, where a single\nface is used in many videos, resulting in an oversampled training dataset. Due\nto this, deep neural networks tend to overfit to the facial features instead of\nlearning to detect manipulation features of DeepFake content. As a result, most\ndetection architectures perform poorly when tested on unseen data. In this\npaper, we provide a quantitative analysis to investigate this problem and\npresent a solution to prevent model overfitting due to the high volume of\nsamples generated from a small number of actors. We introduce Face-Cutout, a\ndata augmentation method for training Convolutional Neural Networks (CNN), to\nimprove DeepFake detection. In this method, training images with various\nocclusions are dynamically generated using face landmark information\nirrespective of orientation. Unlike other general-purpose augmentation methods,\nit focuses on the facial information that is crucial for DeepFake detection.\nOur method achieves a reduction in LogLoss of 15.2% to 35.3% on different\ndatasets, compared to other occlusion-based augmentation techniques. We show\nthat Face-Cutout can be easily integrated with any CNN-based recognition model\nand improve detection performance.",
          "link": "http://arxiv.org/abs/2102.09603",
          "publishedOn": "2021-05-25T01:56:10.143Z",
          "wordCount": 690,
          "title": "Improving DeepFake Detection Using Dynamic Face Augmentation. (arXiv:2102.09603v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhavan_A/0/1/0/all/0/1\">Arya Akhavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1\">Massimiliano Pontil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsybakov_A/0/1/0/all/0/1\">Alexandre B. Tsybakov</a>",
          "description": "We study the problem of zero-order optimization of a strongly convex\nfunction. The goal is to find the minimizer of the function by a sequential\nexploration of its values, under measurement noise. We study the impact of\nhigher order smoothness properties of the function on the optimization error\nand on the cumulative regret. To solve this problem we consider a randomized\napproximation of the projected gradient descent algorithm. The gradient is\nestimated by a randomized procedure involving two function evaluations and a\nsmoothing kernel. We derive upper bounds for this algorithm both in the\nconstrained and unconstrained settings and prove minimax lower bounds for any\nsequential search method. Our results imply that the zero-order algorithm is\nnearly optimal in terms of sample complexity and the problem parameters. Based\non this algorithm, we also propose an estimator of the minimum value of the\nfunction achieving almost sharp oracle behavior. We compare our results with\nthe state-of-the-art, highlighting a number of key improvements.",
          "link": "http://arxiv.org/abs/2006.07862",
          "publishedOn": "2021-05-25T01:56:10.126Z",
          "wordCount": 628,
          "title": "Exploiting Higher Order Smoothness in Derivative-free Optimization and Continuous Bandits. (arXiv:2006.07862v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ohtsubo_Y/0/1/0/all/0/1\">Yusuke Ohtsubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsukawa_T/0/1/0/all/0/1\">Tetsu Matsukawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_E/0/1/0/all/0/1\">Einoshin Suzuki</a>",
          "description": "In this paper, we propose a deep invertible hybrid model which integrates\ndiscriminative and generative learning at a latent space level for\nsemi-supervised few-shot classification. Various tasks for classifying new\nspecies from image data can be modeled as a semi-supervised few-shot\nclassification, which assumes a labeled and unlabeled training examples and a\nsmall support set of the target classes. Predicting target classes with a few\nsupport examples per class makes the learning task difficult for existing\nsemi-supervised classification methods, including selftraining, which\niteratively estimates class labels of unlabeled training examples to learn a\nclassifier for the training classes. To exploit unlabeled training examples\neffectively, we adopt as the objective function the composite likelihood, which\nintegrates discriminative and generative learning and suits better with deep\nneural networks than the parameter coupling prior, the other popular integrated\nlearning approach. In our proposed model, the discriminative and generative\nmodels are respectively Prototypical Networks, which have shown excellent\nperformance in various kinds of few-shot learning, and Normalizing Flow a deep\ninvertible model which returns the exact marginal likelihood unlike the other\nthree major methods, i.e., VAE, GAN, and autoregressive model. Our main\noriginality lies in our integration of these components at a latent space\nlevel, which is effective in preventing overfitting. Experiments using\nmini-ImageNet and VGG-Face datasets show that our method outperforms\nselftraining based Prototypical Networks.",
          "link": "http://arxiv.org/abs/2105.10644",
          "publishedOn": "2021-05-25T01:56:10.118Z",
          "wordCount": 656,
          "title": "Semi-Supervised Few-Shot Classification with Deep Invertible Hybrid Models. (arXiv:2105.10644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06544",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chuanlong Li</a>",
          "description": "Cerebrovascular accident, or commonly known as stroke, is an acute disease\nwith extreme impact on patients and healthcare systems and is the second\nlargest cause of death worldwide. Fast and precise stroke lesion detection and\nlocation is an extreme important process with regards to stroke diagnosis,\ntreatment, and prognosis. Except from the manual segmentation approach, machine\nlearning based segmentation methods are the most promising ones when\nconsidering efficiency and accuracy, and convolutional neural network based\nmodels are the first of its kind. However, most of these neural network models\ndo not really align with the brain anatomical structures. Intuitively, this\nwork presents a more brain alike model which mimics the anatomical structure of\nthe human visual cortex. Through the preliminary experiments on the stroke\nlesion segmentation task, the proposed model is found to be able to perform\nequally well or better to the de-facto standard U-Net. Part of the\nimplementation will be made available at https://github.com/DarkoBomer/VCA-Net.",
          "link": "http://arxiv.org/abs/2105.06544",
          "publishedOn": "2021-05-25T01:56:10.110Z",
          "wordCount": 630,
          "title": "Stroke Lesion Segmentation with Visual Cortex Anatomy Alike Neural Nets. (arXiv:2105.06544v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingqi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1\">Man-On Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>",
          "description": "Extrapolating beyond-demonstrator (BD) through the inverse reinforcement\nlearning (IRL) algorithm aims to learn from and outperform the demonstrator. In\nsharp contrast to the conventional reinforcement learning (RL) algorithms,\nBD-IRL can overcome the dilemma incurred in the reward function design and\nimprove the exploration mechanism of RL, which opens new avenues to building\nsuperior expert systems. Most existing BD-IRL algorithms are performed in two\nstages by first inferring a reward function before learning a policy via RL.\nHowever, such two-stage BD-IRL algorithms suffer from high computational\ncomplexity, weak robustness, and large performance variations. In particular, a\npoor reward function derived in the first stage will inevitably incur severe\nperformance loss in the second stage. In this work, we propose a hybrid\nadversarial inverse reinforcement learning (HAIRL) algorithm that is one-stage,\nmodel-free, generative-adversarial (GA) fashion and curiosity-driven. Thanks to\nthe one-stage design, the HAIRL can integrate both the reward function learning\nand the policy optimization into one procedure, which leads to many advantages\nsuch as low computational complexity, high robustness, and strong adaptability.\nMore specifically, HAIRL simultaneously imitates the demonstrator and explores\nBD performance by utilizing hybrid rewards. In particular, the Wasserstein-1\ndistance (WD) is introduced into HAIRL to stabilize the imitation procedure\nwhile a novel end-to-end curiosity module (ECM) is developed to improve the\nexploration. Finally, extensive simulation results confirm that HAIRL can\nachieve higher performance as compared to other similar BD-IRL algorithms. Our\ncode is available at our GitHub website\n\\footnote{\\url{https://github.com/yuanmingqi/HAIRL}}.",
          "link": "http://arxiv.org/abs/2102.02454",
          "publishedOn": "2021-05-25T01:56:10.077Z",
          "wordCount": 740,
          "title": "Hybrid Adversarial Inverse Reinforcement Learning. (arXiv:2102.02454v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13384",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Budhathoki_K/0/1/0/all/0/1\">Kailash Budhathoki</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Janzing_D/0/1/0/all/0/1\">Dominik Janzing</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bloebaum_P/0/1/0/all/0/1\">Patrick Bloebaum</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ng_H/0/1/0/all/0/1\">Hoiyi Ng</a>",
          "description": "We describe a formal approach based on graphical causal models to identify\nthe \"root causes\" of the change in the probability distribution of variables.\nAfter factorizing the joint distribution into conditional distributions of each\nvariable, given its parents (the \"causal mechanisms\"), we attribute the change\nto changes of these causal mechanisms. This attribution analysis accounts for\nthe fact that mechanisms often change independently and sometimes only some of\nthem change. Through simulations, we study the performance of our distribution\nchange attribution method. We then present a real-world case study identifying\nthe drivers of the difference in the income distribution between men and women.",
          "link": "http://arxiv.org/abs/2102.13384",
          "publishedOn": "2021-05-25T01:56:10.070Z",
          "wordCount": 559,
          "title": "Why did the distribution change?. (arXiv:2102.13384v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beheshti_A/0/1/0/all/0/1\">Amin Beheshti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benatallah_B/0/1/0/all/0/1\">Boualem Benatallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motahari_Nezhad_H/0/1/0/all/0/1\">Hamid Reza Motahari-Nezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodratnama_S/0/1/0/all/0/1\">Samira Ghodratnama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amouzgar_F/0/1/0/all/0/1\">Farhad Amouzgar</a>",
          "description": "In modern enterprises, Business Processes (BPs) are realized over a mix of\nworkflows, IT systems, Web services and direct collaborations of people.\nAccordingly, process data (i.e., BP execution data such as logs containing\nevents, interaction messages and other process artifacts) is scattered across\nseveral systems and data sources, and increasingly show all typical properties\nof the Big Data. Understanding the execution of process data is challenging as\nkey business insights remain hidden in the interactions among process entities:\nmost objects are interconnected, forming complex, heterogeneous but often\nsemi-structured networks. In the context of business processes, we consider the\nBig Data problem as a massive number of interconnected data islands from\npersonal, shared and business data. We present a framework to model process\ndata as graphs, i.e., Process Graph, and present abstractions to summarize the\nprocess graph and to discover concept hierarchies for entities based on both\ndata objects and their interactions in process graphs. We present a language,\nnamely BP-SPARQL, for the explorative querying and understanding of process\ngraphs from various user perspectives. We have implemented a scalable\narchitecture for querying, exploration and analysis of process graphs. We\nreport on experiments performed on both synthetic and real-world datasets that\nshow the viability and efficiency of the approach.",
          "link": "http://arxiv.org/abs/2105.10911",
          "publishedOn": "2021-05-25T01:56:10.063Z",
          "wordCount": 651,
          "title": "A Query Language for Summarizing and Analyzing Business Process Data. (arXiv:2105.10911v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erdodi_L/0/1/0/all/0/1\">Laszlo Erdodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommervoll_%7B/0/1/0/all/0/1\">&#xc5;vald &#xc5;slaugson Sommervoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zennaro_F/0/1/0/all/0/1\">Fabio Massimo Zennaro</a>",
          "description": "In this paper, we propose a formalization of the process of exploitation of\nSQL injection vulnerabilities. We consider a simplification of the dynamics of\nSQL injection attacks by casting this problem as a security capture-the-flag\nchallenge. We model it as a Markov decision process, and we implement it as a\nreinforcement learning problem. We then deploy reinforcement learning agents\ntasked with learning an effective policy to perform SQL injection; we design\nour training in such a way that the agent learns not just a specific strategy\nto solve an individual challenge but a more generic policy that may be applied\nto perform SQL injection attacks against any system instantiated randomly by\nour problem generator. We analyze the results in terms of the quality of the\nlearned policy and in terms of convergence time as a function of the complexity\nof the challenge and the learning agent's complexity. Our work fits in the\nwider research on the development of intelligent agents for autonomous\npenetration testing and white-hat hacking, and our results aim to contribute to\nunderstanding the potential and the limits of reinforcement learning in a\nsecurity environment.",
          "link": "http://arxiv.org/abs/2101.03118",
          "publishedOn": "2021-05-25T01:56:10.056Z",
          "wordCount": 654,
          "title": "Simulating SQL Injection Vulnerability Exploitation Using Q-Learning Reinforcement Learning Agents. (arXiv:2101.03118v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tirer_T/0/1/0/all/0/1\">Tom Tirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>",
          "description": "A major factor in the success of deep neural networks is the use of\nsophisticated architectures rather than the classical multilayer perceptron\n(MLP). Residual networks (ResNets) stand out among these powerful modern\narchitectures. Previous works focused on the optimization advantages of deep\nResNets over deep MLPs. In this paper, we show another distinction between the\ntwo models, namely, a tendency of ResNets to promote smoother interpolations\nthan MLPs. We analyze this phenomenon via the neural tangent kernel (NTK)\napproach. First, we compute the NTK for a considered ResNet model and prove its\nstability during gradient descent training. Then, we show by various evaluation\nmethodologies that for ReLU activations the NTK of ResNet, and its kernel\nregression results, are smoother than the ones of MLP. The better smoothness\nobserved in our analysis may explain the better generalization ability of\nResNets and the practice of moderately attenuating the residual blocks.",
          "link": "http://arxiv.org/abs/2009.10008",
          "publishedOn": "2021-05-25T01:56:10.034Z",
          "wordCount": 604,
          "title": "Kernel-Based Smoothness Analysis of Residual Networks. (arXiv:2009.10008v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1\">David B. Lindell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1\">Julien N. P. Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>",
          "description": "Numerical integration is a foundational technique in scientific computing and\nis at the core of many computer vision applications. Among these applications,\nneural volume rendering has recently been proposed as a new paradigm for view\nsynthesis, achieving photorealistic image quality. However, a fundamental\nobstacle to making these methods practical is the extreme computational and\nmemory requirements caused by the required volume integrations along the\nrendered rays during training and inference. Millions of rays, each requiring\nhundreds of forward passes through a neural network are needed to approximate\nthose integrations with Monte Carlo sampling. Here, we propose automatic\nintegration, a new framework for learning efficient, closed-form solutions to\nintegrals using coordinate-based neural networks. For training, we instantiate\nthe computational graph corresponding to the derivative of the network. The\ngraph is fitted to the signal to integrate. After optimization, we reassemble\nthe graph to obtain a network that represents the antiderivative. By the\nfundamental theorem of calculus, this enables the calculation of any definite\nintegral in two evaluations of the network. Applying this approach to neural\nrendering, we improve a tradeoff between rendering speed and image quality:\nimproving render times by greater than 10 times with a tradeoff of slightly\nreduced image quality.",
          "link": "http://arxiv.org/abs/2012.01714",
          "publishedOn": "2021-05-25T01:56:10.022Z",
          "wordCount": 670,
          "title": "AutoInt: Automatic Integration for Fast Neural Volume Rendering. (arXiv:2012.01714v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamama_M/0/1/0/all/0/1\">Mohamed Hamama</a>",
          "description": "The purpose of this research report is to present the our learning curve and\nthe exposure to the Machine Learning life cycle, with the use of a Kaggle\nbinary classification data set and taking to explore various techniques from\npre-processing to the final optimization and model evaluation, also we\nhighlight on the data imbalance issue and we discuss the different methods of\nhandling that imbalance on the data level by over-sampling and under sampling\nnot only to reach a balanced class representation but to improve the overall\nperformance. This work also opens some gaps for future work.",
          "link": "http://arxiv.org/abs/2105.10959",
          "publishedOn": "2021-05-25T01:56:09.956Z",
          "wordCount": 522,
          "title": "A Study imbalance handling by various data sampling methods in binary classification. (arXiv:2105.10959v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Behmanesh_M/0/1/0/all/0/1\">Maysam Behmanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adibi_P/0/1/0/all/0/1\">Peyman Adibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karshenas_H/0/1/0/all/0/1\">Hossein Karshenas</a>",
          "description": "Support vector machines (SVMs) are powerful supervised learning tools\ndeveloped to solve classification problems. However, SVMs are likely to perform\npoorly in the classification of imbalanced data. The rough set theory presents\na mathematical tool for inference in nondeterministic cases that provides\nmethods for removing irrelevant information from data. In this work, we propose\nan approach that efficiently used fuzzy rough set theory in weighted least\nsquares twin support vector machine called FRLSTSVM for classification of\nimbalanced data. The first innovation is introducing a new fuzzy rough\nset-based under-sampling strategy to make the classifier robust in terms of the\nimbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM,\ndata points from the minority class remain unchanged while a subset of data\npoints in the majority class are selected using a new method. In this model, we\nembed the weight biases in the LSTSVM formulations to overcome the bias\nphenomenon in the original twin SVM for the classification of imbalanced data.\nIn order to determine these weights in this formulation, we introduce a new\nstrategy that uses fuzzy rough set theory as the second innovation.\nExperimental results on the famous imbalanced datasets, compared to the related\ntraditional SVM-based methods, demonstrate the superiority of the proposed\nFRLSTSVM model in the imbalanced data classification.",
          "link": "http://arxiv.org/abs/2105.01198",
          "publishedOn": "2021-05-25T01:56:09.949Z",
          "wordCount": 676,
          "title": "Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set Theory for Imbalanced Data Classification. (arXiv:2105.01198v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sumeet S. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karayev_S/0/1/0/all/0/1\">Sergey Karayev</a>",
          "description": "We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.",
          "link": "http://arxiv.org/abs/2103.06450",
          "publishedOn": "2021-05-25T01:56:09.934Z",
          "wordCount": 629,
          "title": "Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhangyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan.Z.Li</a>",
          "description": "Deep learning on graphs has recently achieved remarkable success on a variety\nof tasks while such success relies heavily on the massive and carefully labeled\ndata. However, precise annotations are generally very expensive and\ntime-consuming. To address this problem, self-supervised learning (SSL) is\nemerging as a new paradigm for extracting informative knowledge through\nwell-designed pretext tasks without relying on manual labels. In this survey,\nwe extend the concept of SSL, which first emerged in the fields of computer\nvision and natural language processing, to present a timely and comprehensive\nreview of the existing SSL techniques for graph data. Specifically, we divide\nexisting graph SSL methods into three categories: contrastive, generative, and\npredictive. More importantly, unlike many other surveys that only provide a\nhigh-level description of published research, we present an additional\nmathematical summary of the existing works in a unified framework. Furthermore,\nto facilitate methodological development and empirical comparisons, we also\nsummarize the commonly used datasets, evaluation metrics, downstream tasks, and\nopen-source implementations of various algorithms. Finally, we discuss the\ntechnical challenges and potential future directions for improving graph\nself-supervised learning.",
          "link": "http://arxiv.org/abs/2105.07342",
          "publishedOn": "2021-05-25T01:56:09.913Z",
          "wordCount": 623,
          "title": "Self-supervised on Graphs: Contrastive, Generative,or Predictive. (arXiv:2105.07342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiawei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lijun Sun</a>",
          "description": "The bus system is a critical component of sustainable urban transportation.\nHowever, due to the significant uncertainties in passenger demand and traffic\nconditions, bus operation is unstable in nature and bus bunching has become a\ncommon phenomenon that undermines the reliability and efficiency of bus\nservices. Despite recent advances in multi-agent reinforcement learning (MARL)\non traffic control, little research has focused on bus fleet control due to the\ntricky asynchronous characteristic -- control actions only happen when a bus\narrives at a bus stop and thus agents do not act simultaneously. In this study,\nwe formulate route-level bus fleet control as an asynchronous multi-agent\nreinforcement learning (ASMR) problem and extend the classical actor-critic\narchitecture to handle the asynchronous issue. Specifically, we design a novel\ncritic network to effectively approximate the marginal contribution for other\nagents, in which graph attention neural network is used to conduct inductive\nlearning for policy evaluation. The critic structure also helps the ego agent\noptimize its policy more efficiently. We evaluate the proposed framework on\nreal-world bus services and actual passenger demand derived from smart card\ndata. Our results show that the proposed model outperforms both traditional\nheadway-based control methods and existing MARL methods.",
          "link": "http://arxiv.org/abs/2105.00376",
          "publishedOn": "2021-05-25T01:56:09.905Z",
          "wordCount": 642,
          "title": "Reducing Bus Bunching with Asynchronous Multi-Agent Reinforcement Learning. (arXiv:2105.00376v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manomaisaowapak_P/0/1/0/all/0/1\">Parinthorn Manomaisaowapak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Songsiri_J/0/1/0/all/0/1\">Jitkomut Songsiri</a>",
          "description": "This paper considers joint learning of multiple sparse Granger graphical\nmodels to discover underlying common and differential Granger causality (GC)\nstructures across multiple time series. This can be applied to drawing\ngroup-level brain connectivity inferences from a homogeneous group of subjects\nor discovering network differences among groups of signals collected under\nheterogeneous conditions. By recognizing that the GC of a single multivariate\ntime series can be characterized by common zeros of vector autoregressive (VAR)\nlag coefficients, a group sparse prior is included in joint regularized\nleast-squares estimations of multiple VAR models. Group-norm regularizations\nbased on group- and fused-lasso penalties encourage a decomposition of multiple\nnetworks into a common GC structure, with other remaining parts defined in\nindividual-specific networks. Prior information about sparseness and sparsity\npatterns of desired GC networks are incorporated as relative weights, while a\nnon-convex group norm in the penalty is proposed to enhance the accuracy of\nnetwork estimation in low-sample settings. Extensive numerical results on\nsimulations illustrated our method's improvements over existing sparse\nestimation approaches on GC network sparsity recovery. Our methods were also\napplied to available resting-state fMRI time series from the ADHD-200 data sets\nto learn the differences of causality mechanisms, called effective brain\nconnectivity, between adolescents with ADHD and typically developing children.\nOur analysis revealed that parts of the causality differences between the two\ngroups often resided in the orbitofrontal region and areas associated with the\nlimbic system, which agreed with clinical findings and data-driven results in\nprevious studies.",
          "link": "http://arxiv.org/abs/2105.07196",
          "publishedOn": "2021-05-25T01:56:09.897Z",
          "wordCount": 738,
          "title": "Joint learning of multiple Granger causal networks via non-convex regularizations: Inference of group-level brain connectivity. (arXiv:2105.07196v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingyun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yuanxing Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Phillip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "Graph representation learning has attracted increasing research attention.\nHowever, most existing studies fuse all structural features and node attributes\nto provide an overarching view of graphs, neglecting finer substructures'\nsemantics, and suffering from interpretation enigmas. This paper presents a\nnovel hierarchical subgraph-level selection and embedding based graph neural\nnetwork for graph classification, namely SUGAR, to learn more discriminative\nsubgraph representations and respond in an explanatory way. SUGAR reconstructs\na sketched graph by extracting striking subgraphs as the representative part of\nthe original graph to reveal subgraph-level patterns. To adaptively select\nstriking subgraphs without prior knowledge, we develop a reinforcement pooling\nmechanism, which improves the generalization ability of the model. To\ndifferentiate subgraph representations among graphs, we present a\nself-supervised mutual information mechanism to encourage subgraph embedding to\nbe mindful of the global graph structural properties by maximizing their mutual\ninformation. Extensive experiments on six typical bioinformatics datasets\ndemonstrate a significant and consistent improvement in model quality with\ncompetitive performance and interpretability.",
          "link": "http://arxiv.org/abs/2101.08170",
          "publishedOn": "2021-05-25T01:56:09.889Z",
          "wordCount": 652,
          "title": "SUGAR: Subgraph Neural Network with Reinforcement Pooling and Self-Supervised Mutual Information Mechanism. (arXiv:2101.08170v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chamzas_C/0/1/0/all/0/1\">Constantinos Chamzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingston_Z/0/1/0/all/0/1\">Zachary Kingston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quintero_Pena_C/0/1/0/all/0/1\">Carlos Quintero-Pe&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavraki_L/0/1/0/all/0/1\">Lydia E. Kavraki</a>",
          "description": "Earlier work has shown that reusing experience from prior motion planning\nproblems can improve the efficiency of similar, future motion planning queries.\nHowever, for robots with many degrees-of-freedom, these methods exhibit poor\ngeneralization across different environments and often require large datasets\nthat are impractical to gather. We present SPARK and FLAME , two\nexperience-based frameworks for sampling-based planning applicable to complex\nmanipulators in 3 D environments. Both combine samplers associated with\nfeatures from a workspace decomposition into a global biased sampling\ndistribution. SPARK decomposes the environment based on exact geometry while\nFLAME is more general, and uses an octree-based decomposition obtained from\nsensor data. We demonstrate the effectiveness of SPARK and FLAME on a Fetch\nrobot tasked with challenging pick-and-place manipulation problems. Our\napproaches can be trained incrementally and significantly improve performance\nwith only a handful of examples, generalizing better over diverse tasks and\nenvironments as compared to prior approaches.",
          "link": "http://arxiv.org/abs/2010.15335",
          "publishedOn": "2021-05-25T01:56:09.874Z",
          "wordCount": 635,
          "title": "Learning Sampling Distributions Using Local 3D Workspace Decompositions for Motion Planning in High Dimensions. (arXiv:2010.15335v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsukamoto_H/0/1/0/all/0/1\">Hiroyasu Tsukamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Soon-Jo Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1\">Jean-Jacques Slotine</a>",
          "description": "We present a deep learning-based adaptive control framework for nonlinear\nsystems with multiplicatively separable parametrization, called aNCM - for\nadaptive Neural Contraction Metric. The framework utilizes a deep neural\nnetwork to approximate a stabilizing adaptive control law parameterized by an\noptimal contraction metric. The use of deep networks permits real-time\nimplementation of the control law and broad applicability to a variety of\nsystems, including systems modeled with basis function approximation methods.\nWe show using contraction theory that aNCM ensures exponential boundedness of\nthe distance between the target and controlled trajectories even under the\npresence of the parametric uncertainty, robustly to the learning errors caused\nby aNCM approximation as well as external additive disturbances. Its\nsuperiority to the existing robust and adaptive control methods is demonstrated\nin a simple cart-pole balancing task.",
          "link": "http://arxiv.org/abs/2103.02987",
          "publishedOn": "2021-05-25T01:56:09.853Z",
          "wordCount": 603,
          "title": "Learning-based Adaptive Control using Contraction Theory. (arXiv:2103.02987v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>",
          "description": "Image-to-image translation aims to preserve source contents while translating\nto discriminative target styles between two visual domains. Most works apply\nadversarial learning in the ambient image space, which could be computationally\nexpensive and challenging to train. In this paper, we propose to deploy an\nenergy-based model (EBM) in the latent space of a pretrained autoencoder for\nthis task. The pretrained autoencoder serves as both a latent code extractor\nand an image reconstruction worker. Our model, LETIT, is based on the\nassumption that two domains share the same latent space, where latent\nrepresentation is implicitly decomposed as a content code and a domain-specific\nstyle code. Instead of explicitly extracting the two codes and applying\nadaptive instance normalization to combine them, our latent EBM can implicitly\nlearn to transport the source style code to the target style code while\npreserving the content code, an advantage over existing image translation\nmethods. This simplified solution is also more efficient in the one-sided\nunpaired image translation setting. Qualitative and quantitative comparisons\ndemonstrate superior translation quality and faithfulness for content\npreservation. Our model is the first to be applicable to\n1024$\\times$1024-resolution unpaired image translation to the best of our\nknowledge.",
          "link": "http://arxiv.org/abs/2012.00649",
          "publishedOn": "2021-05-25T01:56:09.828Z",
          "wordCount": 669,
          "title": "Unpaired Image-to-Image Translation via Latent Energy Transport. (arXiv:2012.00649v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1\">Brian Kenji Iwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "In recent times, deep artificial neural networks have achieved many successes\nin pattern recognition. Part of this success can be attributed to the reliance\non big data to increase generalization. However, in the field of time series\nrecognition, many datasets are often very small. One method of addressing this\nproblem is through the use of data augmentation. In this paper, we survey data\naugmentation techniques for time series and their application to time series\nclassification with neural networks. We outline four families of time series\ndata augmentation, including transformation-based methods, pattern mixing,\ngenerative models, and decomposition methods, and detail their taxonomy.\nFurthermore, we empirically evaluate 12 time series data augmentation methods\non 128 time series classification datasets with 6 different types of neural\nnetworks. Through the results, we are able to analyze the characteristics,\nadvantages and disadvantages, and recommendations of each data augmentation\nmethod. This survey aims to help in the selection of time series data\naugmentation for neural network applications.",
          "link": "http://arxiv.org/abs/2007.15951",
          "publishedOn": "2021-05-25T01:56:09.820Z",
          "wordCount": 636,
          "title": "An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks. (arXiv:2007.15951v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08757",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cai_X/0/1/0/all/0/1\">Xu Cai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1\">Jonathan Scarlett</a>",
          "description": "In this paper, we consider algorithm-independent lower bounds for the problem\nof black-box optimization of functions having a bounded norm is some\nReproducing Kernel Hilbert Space (RKHS), which can be viewed as a non-Bayesian\nGaussian process bandit problem. In the standard noisy setting, we provide a\nnovel proof technique for deriving lower bounds on the regret, with benefits\nincluding simplicity, versatility, and an improved dependence on the error\nprobability. In a robust setting in which every sampled point may be perturbed\nby a suitably-constrained adversary, we provide a novel lower bound for\ndeterministic strategies, demonstrating an inevitable joint dependence of the\ncumulative regret on the corruption level and the time horizon, in contrast\nwith existing lower bounds that only characterize the individual dependencies.\nFurthermore, in a distinct robust setting in which the final point is perturbed\nby an adversary, we strengthen an existing lower bound that only holds for\ntarget success probabilities very close to one, by allowing for arbitrary\nsuccess probabilities above $\\frac{2}{3}$.",
          "link": "http://arxiv.org/abs/2008.08757",
          "publishedOn": "2021-05-25T01:56:09.811Z",
          "wordCount": 625,
          "title": "On Lower Bounds for Standard and Robust Gaussian Process Bandit Optimization. (arXiv:2008.08757v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1\">Ziang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Penghang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>",
          "description": "Deep neural networks (DNNs) are quantized for efficient inference on\nresource-constrained platforms. However, training deep learning models with\nlow-precision weights and activations involves a demanding optimization task,\nwhich calls for minimizing a stage-wise loss function subject to a discrete\nset-constraint. While numerous training methods have been proposed, existing\nstudies for full quantization of DNNs are mostly empirical. From a theoretical\npoint of view, we study practical techniques for overcoming the combinatorial\nnature of network quantization. Specifically, we investigate a simple yet\npowerful projected gradient-like algorithm for quantizing two-linear-layer\nnetworks, which proceeds by repeatedly moving one step at float weights in the\nnegation of a heuristic \\emph{fake} gradient of the loss function (so-called\ncoarse gradient) evaluated at quantized weights. For the first time, we prove\nthat under mild conditions, the sequence of quantized weights recurrently\nvisits the global optimum of the discrete minimization problem for training\nfully quantized network. We also show numerical evidence of the recurrence\nphenomenon of weight evolution in training quantized deep networks.",
          "link": "http://arxiv.org/abs/2012.05529",
          "publishedOn": "2021-05-25T01:56:09.793Z",
          "wordCount": 618,
          "title": "Recurrence of Optimum for Training Weight and Activation Quantized Networks. (arXiv:2012.05529v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Existing methods for vision-and-language learning typically require designing\ntask-specific architectures and objectives for each task. For example, a\nmulti-label answer classifier for visual question answering, a region scorer\nfor referring expression comprehension, and a language decoder for image\ncaptioning, etc. To alleviate these hassles, in this work, we propose a unified\nframework that learns different tasks in a single architecture with the same\nlanguage modeling objective, i.e., multimodal conditional text generation,\nwhere our models learn to generate labels in text based on the visual and\ntextual inputs. On 7 popular vision-and-language benchmarks, including visual\nquestion answering, referring expression comprehension, visual commonsense\nreasoning, most of which have been previously modeled as discriminative tasks,\nour generative approach (with a single unified architecture) reaches comparable\nperformance to recent task-specific state-of-the-art vision-and-language\nmodels. Moreover, our generative approach shows better generalization ability\non questions that have rare answers. Also, we show that our framework allows\nmulti-task learning in a single architecture with a single set of parameters,\nachieving similar performance to separately optimized single-task models. Our\ncode is publicly available at: https://github.com/j-min/VL-T5",
          "link": "http://arxiv.org/abs/2102.02779",
          "publishedOn": "2021-05-25T01:56:09.786Z",
          "wordCount": 654,
          "title": "Unifying Vision-and-Language Tasks via Text Generation. (arXiv:2102.02779v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1\">Jay Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "We empirically demonstrate that test-time adaptive batch normalization, which\nre-estimates the batch-normalization statistics during inference, can provide\n$\\ell_2$-certification as well as improve the commonly occurring corruption\nrobustness of adversarially trained models while maintaining their\nstate-of-the-art empirical robustness against adversarial attacks. Furthermore,\nwe obtain similar $\\ell_2$-certification as the current state-of-the-art\ncertification models for CIFAR-10 by learning our adversarially trained model\nusing larger $\\ell_2$-bounded adversaries. Therefore our work is a step towards\nbridging the gap between the state-of-the-art certification and empirical\nrobustness. Our results also indicate that improving the empirical adversarial\nrobustness may be sufficient as we achieve certification and corruption\nrobustness as a by-product using test-time adaptive batch normalization.",
          "link": "http://arxiv.org/abs/2102.05096",
          "publishedOn": "2021-05-25T01:56:09.780Z",
          "wordCount": 595,
          "title": "Adversarially Trained Models with Test-Time Covariate Shift Adaptation. (arXiv:2102.05096v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10356",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_M/0/1/0/all/0/1\">Muyuan Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ludtke_S/0/1/0/all/0/1\">Steven Ludtke</a>",
          "description": "Structural flexibility and/or dynamic interactions with other molecules is a\ncritical aspect of protein function. CryoEM provides direct visualization of\nindividual macromolecules sampling different conformational and compositional\nstates. While numerous methods are available for computational classification\nof discrete states, characterization of continuous conformational changes or\nlarge numbers of discrete state without human supervision remains challenging.\nHere we present e2gmm, a machine learning algorithm to determine a\nconformational landscape for proteins or complexes using a 3-D Gaussian mixture\nmodel mapped onto 2-D particle images in known orientations. Using a deep\nneural network architecture, e2gmm can automatically resolve the structural\nheterogeneity within the protein complex and map particles onto a small latent\nspace describing conformational and compositional changes. This system presents\na more intuitive and flexible representation than other manifold methods\ncurrently in use. We demonstrate this method on both simulated data as well as\nthree biological systems, to explore compositional and conformational changes\nat a range of scales. The software is distributed as part of EMAN2.",
          "link": "http://arxiv.org/abs/2101.10356",
          "publishedOn": "2021-05-25T01:56:09.773Z",
          "wordCount": 617,
          "title": "Deep learning based mixed-dimensional GMM for characterizing variability in CryoEM. (arXiv:2101.10356v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05363",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhou Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>",
          "description": "It is well-known that standard neural networks, even with a high\nclassification accuracy, are vulnerable to small $\\ell_\\infty$-norm bounded\nadversarial perturbations. Although many attempts have been made, most previous\nworks either can only provide empirical verification of the defense to a\nparticular attack method, or can only develop a certified guarantee of the\nmodel robustness in limited scenarios. In this paper, we seek for a new\napproach to develop a theoretically principled neural network that inherently\nresists $\\ell_\\infty$ perturbations. In particular, we design a novel neuron\nthat uses $\\ell_\\infty$-distance as its basic operation (which we call\n$\\ell_\\infty$-dist neuron), and show that any neural network constructed with\n$\\ell_\\infty$-dist neurons (called $\\ell_{\\infty}$-dist net) is naturally a\n1-Lipschitz function with respect to $\\ell_\\infty$-norm. This directly provides\na rigorous guarantee of the certified robustness based on the margin of\nprediction outputs. We also prove that such networks have enough expressive\npower to approximate any 1-Lipschitz function with robust generalization\nguarantee. Our experimental results show that the proposed network is\npromising. Using $\\ell_{\\infty}$-dist nets as the basic building blocks, we\nconsistently achieve state-of-the-art performance on commonly used datasets:\n93.09% certified accuracy on MNIST ($\\epsilon=0.3$), 79.23% on Fashion MNIST\n($\\epsilon=0.1$) and 35.10% on CIFAR-10 ($\\epsilon=8/255$).",
          "link": "http://arxiv.org/abs/2102.05363",
          "publishedOn": "2021-05-25T01:56:09.766Z",
          "wordCount": 672,
          "title": "Towards Certifying L-infinity Robustness using Neural Networks with L-inf-dist Neurons. (arXiv:2102.05363v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03909",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mingyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_C/0/1/0/all/0/1\">Chenghong Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hun-Seok Kim</a>",
          "description": "We present a deep learning based joint source channel coding (JSCC) scheme\nfor wireless image transmission over multipath fading channels with non-linear\nsignal clipping. The proposed encoder and decoder use convolutional neural\nnetworks (CNN) and directly map the source images to complex-valued baseband\nsamples for orthogonal frequency division multiplexing (OFDM) transmission. The\nproposed model-driven machine learning approach eliminates the need for\nseparate source and channel coding while integrating an OFDM datapath to cope\nwith multipath fading channels. The end-to-end JSCC communication system\ncombines trainable CNN layers with non-trainable but differentiable layers\nrepresenting the multipath channel model and OFDM signal processing blocks. Our\nresults show that injecting domain expert knowledge by incorporating OFDM\nbaseband processing blocks into the machine learning framework significantly\nenhances the overall performance compared to an unstructured CNN. Our method\noutperforms conventional schemes that employ state-of-the-art but separate\nsource and channel coding such as BPG and LDPC with OFDM. Moreover, our method\nis shown to be robust against non-linear signal clipping in OFDM for various\nchannel conditions that do not match the model parameter used during the\ntraining.",
          "link": "http://arxiv.org/abs/2101.03909",
          "publishedOn": "2021-05-25T01:56:09.758Z",
          "wordCount": 643,
          "title": "Deep Joint Source Channel Coding for WirelessImage Transmission with OFDM. (arXiv:2101.03909v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1\">Anastasis Kratsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamanlooy_B/0/1/0/all/0/1\">Behnoosh Zamanlooy</a>",
          "description": "Most stochastic gradient descent algorithms can optimize neural networks that\nare sub-differentiable in their parameters, which requires their activation\nfunction to exhibit a degree of continuity. However, this continuity constraint\non the activation function prevents these neural models from uniformly\napproximating discontinuous functions. This paper focuses on the case where the\ndiscontinuities arise from distinct sub-patterns, each defined on different\nparts of the input space. We propose a new discontinuous deep neural network\nmodel trainable via a decoupled two-step procedure that avoids passing gradient\nupdates through the network's non-differentiable unit. We provide universal\napproximation guarantees for our architecture in the space of bounded\ncontinuous functions and in the space of piecewise continuous functions, which\nwe introduced herein. We present a novel semi-supervised two-step training\nprocedure for our discontinuous deep learning model, and we provide theoretical\nsupport for its effectiveness. The performance of our architecture is evaluated\nexperimentally on two real-world datasets and one synthetic dataset.",
          "link": "http://arxiv.org/abs/2010.15571",
          "publishedOn": "2021-05-25T01:56:09.734Z",
          "wordCount": 643,
          "title": "Learning Sub-Patterns in Piecewise Continuous Functions. (arXiv:2010.15571v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddila_C/0/1/0/all/0/1\">Chandra Maddila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagappan_N/0/1/0/all/0/1\">Nachiappan Nagappan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bird_C/0/1/0/all/0/1\">Christian Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gousios_G/0/1/0/all/0/1\">Georgios Gousios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1\">Arie van Deursen</a>",
          "description": "Modern, complex software systems are being continuously extended and\nadjusted. The developers responsible for this may come from different teams or\norganizations, and may be distributed over the world. This may make it\ndifficult to keep track of what other developers are doing, which may result in\nmultiple developers concurrently editing the same code areas. This, in turn,\nmay lead to hard-to-merge changes or even merge conflicts, logical bugs that\nare difficult to detect, duplication of work, and wasted developer\nproductivity. To address this, we explore the extent of this problem in the\npull request based software development model. We study half a year of changes\nmade to six large repositories in Microsoft in which at least 1,000 pull\nrequests are created each month. We find that files concurrently edited in\ndifferent pull requests are more likely to introduce bugs. Motivated by these\nfindings, we design, implement, and deploy a service named ConE (Concurrent\nEdit Detector) that proactively detects pull requests containing concurrent\nedits, to help mitigate the problems caused by them. ConE has been designed to\nscale, and to minimize false alarms while still flagging relevant concurrently\nedited files. Key concepts of ConE include the detection of the Extent of\nOverlap between pull requests, and the identification of Rarely Concurrently\nEdited Files. To evaluate ConE, we report on its operational deployment on 234\nrepositories inside Microsoft. ConE assessed 26,000 pull requests and made 775\nrecommendations about conflicting changes, which were rated as useful in over\n70% (554) of the cases. From interviews with 48 users we learned that they\nbelieved ConE would save time in conflict resolution and avoiding duplicate\nwork, and that over 90% intend to keep using the service on a daily basis.",
          "link": "http://arxiv.org/abs/2101.06542",
          "publishedOn": "2021-05-25T01:56:09.692Z",
          "wordCount": 752,
          "title": "ConE: A Concurrent Edit Detection Tool for Large ScaleSoftware Development. (arXiv:2101.06542v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Fan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinwei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mosharaf Chowdhury</a>",
          "description": "We present FedScale, a diverse set of challenging and realistic benchmark\ndatasets to facilitate scalable, comprehensive, and reproducible federated\nlearning (FL) research. FedScale datasets are large-scale, encompassing a\ndiverse range of important FL tasks, such as image classification, object\ndetection, language modeling, speech recognition, and reinforcement learning.\nFor each dataset, we provide a unified evaluation protocol using realistic data\nsplits and evaluation metrics. To meet the pressing need for reproducing\nrealistic FL at scale, we have also built an efficient evaluation platform to\nsimplify and standardize the process of FL experimental setup and model\nevaluation. Our evaluation platform provides flexible APIs to implement new FL\nalgorithms and include new execution backends with minimal developer efforts.\nFinally, we perform indepth benchmark experiments on these datasets. Our\nexperiments suggest that FedScale presents significant challenges of\nheterogeneity-aware co-optimizations of the system and statistical efficiency\nunder realistic FL characteristics, indicating fruitful opportunities for\nfuture research. FedScale is open-source with permissive licenses and actively\nmaintained, and we welcome feedback and contributions from the community.",
          "link": "http://arxiv.org/abs/2105.11367",
          "publishedOn": "2021-05-25T01:56:09.535Z",
          "wordCount": 609,
          "title": "FedScale: Benchmarking Model and System Performance of Federated Learning. (arXiv:2105.11367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>",
          "description": "When training control policies for robot manipulation via deep learning,\nsim-to-real transfer can help satisfy the large data requirements. In this\npaper, we study the problem of zero-shot sim-to-real when the task requires\nboth highly precise control, with sub-millimetre error tolerance, and full\nworkspace generalisation. Our framework involves a coarse-to-fine controller,\nwhere trajectories initially begin with classical motion planning based on pose\nestimation, and transition to an end-to-end controller which maps images to\nactions and is trained in simulation with domain randomisation. In this way, we\nachieve precise control whilst also generalising the controller across the\nworkspace and keeping the generality and robustness of vision-based, end-to-end\ncontrol. Real-world experiments on a range of different tasks show that, by\nexploiting the best of both worlds, our framework significantly outperforms\npurely motion planning methods, and purely learning-based methods. Furthermore,\nwe answer a range of questions on best practices for precise sim-to-real\ntransfer, such as how different image sensor modalities and image feature\nrepresentations perform.",
          "link": "http://arxiv.org/abs/2105.11283",
          "publishedOn": "2021-05-25T01:56:09.525Z",
          "wordCount": 598,
          "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across the Workspace. (arXiv:2105.11283v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiabin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiquan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingjie Tian</a>",
          "description": "Learning from label proportions (LLP) aims at learning an instance-level\nclassifier with label proportions in grouped training data. Existing deep\nlearning based LLP methods utilize end-to-end pipelines to obtain the\nproportional loss with Kullback-Leibler divergence between the bag-level prior\nand posterior class distributions. However, the unconstrained optimization on\nthis objective can hardly reach a solution in accordance with the given\nproportions. Besides, concerning the probabilistic classifier, this strategy\nunavoidably results in high-entropy conditional class distributions at the\ninstance level. These issues further degrade the performance of the\ninstance-level classification. In this paper, we regard these problems as noisy\npseudo labeling, and instead impose the strict proportion consistency on the\nclassifier with a constrained optimization as a continuous training stage for\nexisting LLP classifiers. In addition, we introduce the mixup strategy and\nsymmetric crossentropy to further reduce the label noise. Our framework is\nmodel-agnostic, and demonstrates compelling performance improvement in\nextensive experiments, when incorporated into other deep LLP models as a\npost-hoc phase.",
          "link": "http://arxiv.org/abs/2105.10635",
          "publishedOn": "2021-05-25T01:56:09.506Z",
          "wordCount": 601,
          "title": "Two-stage Training for Learning from Label Proportions. (arXiv:2105.10635v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1\">Payam Karisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>",
          "description": "We present an algorithm based on multi-layer transformers for identifying\nAdverse Drug Reactions (ADR) in social media data. Our model relies on the\nproperties of the problem and the characteristics of contextual word embeddings\nto extract two views from documents. Then a classifier is trained on each view\nto label a set of unlabeled documents to be used as an initializer for a new\nclassifier in the other view. Finally, the initialized classifier in each view\nis further trained using the initial training examples. We evaluated our model\nin the largest publicly available ADR dataset. The experiments testify that our\nmodel significantly outperforms the transformer-based models pretrained on\ndomain-specific data.",
          "link": "http://arxiv.org/abs/2105.11354",
          "publishedOn": "2021-05-25T01:56:09.499Z",
          "wordCount": 553,
          "title": "View Distillation with Unlabeled Data for Extracting Adverse Drug Effects from User-Generated Data. (arXiv:2105.11354v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Most existing graph neural networks (GNNs) learn node embeddings using the\nframework of message passing and aggregation. Such GNNs are incapable of\nlearning relative positions between graph nodes within a graph. To empower GNNs\nwith the awareness of node positions, some nodes are set as anchors. Then,\nusing the distances from a node to the anchors, GNNs can infer relative\npositions between nodes. However, P-GNNs arbitrarily select anchors, leading to\ncompromising position-awareness and feature extraction. To eliminate this\ncompromise, we demonstrate that selecting evenly distributed and asymmetric\nanchors is essential. On the other hand, we show that choosing anchors that can\naggregate embeddings of all the nodes within a graph is NP-hard. Therefore,\ndevising efficient optimal algorithms in a deterministic approach is\npractically not feasible. To ensure position-awareness and bypass\nNP-completeness, we propose Position-Sensing Graph Neural Networks (PSGNNs),\nlearning how to choose anchors in a back-propagatable fashion. Experiments\nverify the effectiveness of PSGNNs against state-of-the-art GNNs, substantially\nimproving performance on various synthetic and real-world graph datasets while\nenjoying stable scalability. Specifically, PSGNNs on average boost AUC more\nthan 14% for pairwise node classification and 18% for link prediction over the\nexisting state-of-the-art position-aware methods. Our source code is publicly\navailable at: https://github.com/ZhenyueQin/PSGNN",
          "link": "http://arxiv.org/abs/2105.11346",
          "publishedOn": "2021-05-25T01:56:09.490Z",
          "wordCount": 634,
          "title": "Position-Sensing Graph Neural Networks: Proactively Learning Nodes Relative Positions. (arXiv:2105.11346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jamali_M/0/1/0/all/0/1\">Maedeh Jamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nader Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadivi_P/0/1/0/all/0/1\">Pejman Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirani_S/0/1/0/all/0/1\">Shahram Shirani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>",
          "description": "Digital contents have grown dramatically in recent years, leading to\nincreased attention to copyright. Image watermarking has been considered one of\nthe most popular methods for copyright protection. With the recent advancements\nin applying deep neural networks in image processing, these networks have also\nbeen used in image watermarking. Robustness and imperceptibility are two\nchallenging features of watermarking methods that the trade-off between them\nshould be satisfied. In this paper, we propose to use an end-to-end network for\nwatermarking. We use a convolutional neural network (CNN) to control the\nembedding strength based on the image content. Dynamic embedding helps the\nnetwork to have the lowest effect on the visual quality of the watermarked\nimage. Different image processing attacks are simulated as a network layer to\nimprove the robustness of the model. Our method is a blind watermarking\napproach that replicates the watermark string to create a matrix of the same\nsize as the input image. Instead of diffusing the watermark data into the input\nimage, we inject the data into the feature space and force the network to do\nthis in regions that increase the robustness against various attacks.\nExperimental results show the superiority of the proposed method in terms of\nimperceptibility and robustness compared to the state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2105.11095",
          "publishedOn": "2021-05-25T01:56:09.406Z",
          "wordCount": 645,
          "title": "Robust Watermarking using Diffusion of Logo into Autoencoder Feature Maps. (arXiv:2105.11095v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boon_M/0/1/0/all/0/1\">Marcus N. Boon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Euler_H/0/1/0/all/0/1\">Hans-Christian Ruiz Euler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ven_B/0/1/0/all/0/1\">Bram van de Ven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibarra_U/0/1/0/all/0/1\">Unai Alegre Ibarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobbert_P/0/1/0/all/0/1\">Peter A. Bobbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiel_W/0/1/0/all/0/1\">Wilfred G. van der Wiel</a>",
          "description": "Deep learning, a multi-layered neural network approach inspired by the brain,\nhas revolutionized machine learning. One of its key enablers has been\nbackpropagation, an algorithm that computes the gradient of a loss function\nwith respect to the weights in the neural network model, in combination with\nits use in gradient descent. However, the implementation of deep learning in\ndigital computers is intrinsically wasteful, with energy consumption becoming\nprohibitively high for many applications. This has stimulated the development\nof specialized hardware, ranging from neuromorphic CMOS integrated circuits and\nintegrated photonic tensor cores to unconventional, material-based computing\nsystems. The learning process in these material systems, taking place, e.g., by\nartificial evolution or surrogate neural network modelling, is still a\ncomplicated and time-consuming process. Here, we demonstrate an efficient and\naccurate homodyne gradient extraction method for performing gradient descent on\nthe loss function directly in the material system. We demonstrate the method in\nour recently developed dopant network processing units, where we readily\nrealize all Boolean gates. This shows that gradient descent can in principle be\nfully implemented in materio using simple electronics, opening up the way to\nautonomously learning material systems.",
          "link": "http://arxiv.org/abs/2105.11233",
          "publishedOn": "2021-05-25T01:56:09.386Z",
          "wordCount": 631,
          "title": "Gradient Descent in Materio. (arXiv:2105.11233v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1\">Avinash Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_A/0/1/0/all/0/1\">Arpan Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatsa_S/0/1/0/all/0/1\">Shivam Vinayak Vatsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>",
          "description": "We consider a system of several collocated nodes sharing a time slotted\nwireless channel, and seek a MAC that (i) provides low mean delay, (ii) has\ndistributed control (i.e., there is no central scheduler), and (iii) does not\nrequire explicit exchange of state information or control signals. The design\nof such MAC protocols must keep in mind the need for contention access at light\ntraffic, and scheduled access in heavy traffic, leading to the long-standing\ninterest in hybrid, adaptive MACs.\n\nWe first propose EZMAC, a simple extension of an existing decentralized,\nhybrid MAC called ZMAC. Next, motivated by our results on delay and throughput\noptimality in partially observed, constrained queuing networks, we develop\nanother decentralized MAC protocol that we term QZMAC. A method to improve the\nshort-term fairness of QZMAC is proposed and analysed, and the resulting\nmodified algorithm is shown to possess better fairness properties than QZMAC.\nThe theory developed to reduce delay is also shown to work %with different\ntraffic types (batch arrivals, for example) and even in the presence of\ntransmission errors and fast fading.\n\nExtensions to handle time critical traffic (alarms, for example) and hidden\nnodes are also discussed. Practical implementation issues, such as handling\nClear Channel Assessment (CCA) errors, are outlined. We implement and\ndemonstrate the performance of QZMAC on a test bed consisting of CC2420 based\nCrossbow telosB motes, running the 6TiSCH communication stack on the Contiki\noperating system over the 2.4GHz ISM band.\n\nFinally, using simulations, we show that both protocols achieve mean delays\nmuch lower than those achieved by ZMAC, and QZMAC provides mean delays very\nclose to the minimum achievable in this setting, i.e., that of the centralized\ncomplete knowledge scheduler.",
          "link": "http://arxiv.org/abs/2105.11213",
          "publishedOn": "2021-05-25T01:56:09.373Z",
          "wordCount": 731,
          "title": "Decentralized, Hybrid MAC Design with Reduced State Information Exchange for Low-Delay IoT Applications. (arXiv:2105.11213v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nori_M/0/1/0/all/0/1\">Milad Khademi Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangseok Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Il-Min Kim</a>",
          "description": "Federated Learning (FL) has recently received a lot of attention for\nlarge-scale privacy-preserving machine learning. However, high communication\noverheads due to frequent gradient transmissions decelerate FL. To mitigate the\ncommunication overheads, two main techniques have been studied: (i) local\nupdate of weights characterizing the trade-off between communication and\ncomputation and (ii) gradient compression characterizing the trade-off between\ncommunication and precision. To the best of our knowledge, studying and\nbalancing those two trade-offs jointly and dynamically while considering their\nimpacts on convergence has remained unresolved even though it promises\nsignificantly faster FL. In this paper, we first formulate our problem to\nminimize learning error with respect to two variables: local update\ncoefficients and sparsity budgets of gradient compression who characterize\ntrade-offs between communication and computation/precision, respectively. We\nthen derive an upper bound of the learning error in a given wall-clock time\nconsidering the interdependency between the two variables. Based on this\ntheoretical analysis, we propose an enhanced FL scheme, namely Fast FL (FFL),\nthat jointly and dynamically adjusts the two variables to minimize the learning\nerror. We demonstrate that FFL consistently achieves higher accuracies faster\nthan similar schemes existing in the literature.",
          "link": "http://arxiv.org/abs/2105.11028",
          "publishedOn": "2021-05-25T01:56:09.357Z",
          "wordCount": 641,
          "title": "Fast Federated Learning by Balancing Communication Trade-Offs. (arXiv:2105.11028v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrikanth_N/0/1/0/all/0/1\">N.C. Shrikanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1\">Tim Menzies</a>",
          "description": "Before researchers rush to reason across all available data, they should\nfirst check if the information is densest within some small region. We say this\nsince, in 240 GitHub projects, we find that the information in that data\n``clumps'' towards the earliest parts of the project. In fact, a defect\nprediction model learned from just the first 150 commits works as well, or\nbetter than state-of-the-art alternatives. Using just this early life cycle\ndata, we can build models very quickly (using weeks, not months, of CPU time).\nAlso, we can find simple models (with just two features) that generalize to\nhundreds of software projects. Based on this experience, we warn that prior\nwork on generalizing software engineering defect prediction models may have\nneedlessly complicated an inherently simple process. Further, prior work that\nfocused on later-life cycle data now needs to be revisited since their\nconclusions were drawn from relatively uninformative regions. Replication note:\nall our data and scripts are online at\nhttps://github.com/snaraya7/early-defect-prediction-tse.",
          "link": "http://arxiv.org/abs/2105.11082",
          "publishedOn": "2021-05-25T01:56:09.349Z",
          "wordCount": 605,
          "title": "The Early Bird Catches the Worm: Better Early Life Cycle Defect Predictors. (arXiv:2105.11082v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1\">Mingyang Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiacheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhi-Ming Ma</a>",
          "description": "Recently, learning a model that generalizes well on out-of-distribution (OOD)\ndata has attracted great attention in the machine learning community. In this\npaper, after defining OOD generalization via Wasserstein distance, we\ntheoretically show that a model robust to input perturbation generalizes well\non OOD data. Inspired by previous findings that adversarial training helps\nimprove input-robustness, we theoretically show that adversarially trained\nmodels have converged excess risk on OOD data, and empirically verify it on\nboth image classification and natural language understanding tasks. Besides, in\nthe paradigm of first pre-training and then fine-tuning, we theoretically show\nthat a pre-trained model that is more robust to input perturbation provides a\nbetter initialization for generalization on downstream OOD data. Empirically,\nafter fine-tuning, this better-initialized model from adversarial pre-training\nalso has better OOD generalization.",
          "link": "http://arxiv.org/abs/2105.11144",
          "publishedOn": "2021-05-25T01:56:09.332Z",
          "wordCount": 564,
          "title": "Improved OOD Generalization via Adversarial Training and Pre-training. (arXiv:2105.11144v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jian Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tiankai Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maciejewski_R/0/1/0/all/0/1\">Ross Maciejewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "Algorithmic fairness is becoming increasingly important in data mining and\nmachine learning, and one of the most fundamental notions is group fairness.\nThe vast majority of the existing works on group fairness, with a few\nexceptions, primarily focus on debiasing with respect to a single sensitive\nattribute, despite the fact that the co-existence of multiple sensitive\nattributes (e.g., gender, race, marital status, etc.) in the real-world is\ncommonplace. As such, methods that can ensure a fair learning outcome with\nrespect to all sensitive attributes of concern simultaneously need to be\ndeveloped. In this paper, we study multi-group fairness in machine learning\n(MultiFair), where statistical parity, a representative group fairness measure,\nis guaranteed among demographic groups formed by multiple sensitive attributes\nof interest. We formulate it as a mutual information minimization problem and\npropose a generic end-to-end algorithmic framework to solve it. The key idea is\nto leverage a variational representation of mutual information, which considers\nthe variational distribution between learning outcomes and sensitive\nattributes, as well as the density ratio between the variational and the\noriginal distributions. Our proposed framework is generalizable to many\ndifferent settings, including other statistical notions of fairness, and could\nhandle any type of learning task equipped with a gradient-based optimizer.\nEmpirical evaluations in the fair classification task on three real-world\ndatasets demonstrate that our proposed framework can effectively debias the\nclassification results with minimal impact to the classification accuracy.",
          "link": "http://arxiv.org/abs/2105.11069",
          "publishedOn": "2021-05-25T01:56:09.325Z",
          "wordCount": 666,
          "title": "MultiFair: Multi-Group Fairness in Machine Learning. (arXiv:2105.11069v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouhollahi_Z/0/1/0/all/0/1\">Zeinab Rouhollahi</a>",
          "description": "Recently, financial institutes have been dealing with an increase in\nfinancial crimes. In this context, financial services firms started to improve\ntheir vigilance and use new technologies and approaches to identify and predict\nfinancial fraud and crime possibilities. This task is challenging as\ninstitutions need to upgrade their data and analytics capabilities to enable\nnew technologies such as Artificial Intelligence (AI) to predict and detect\nfinancial crimes. In this paper, we put a step towards AI-enabled financial\ncrime detection in general and money laundering detection in particular to\naddress this challenge. We study and analyse the recent works done in financial\ncrime detection and present a novel model to detect money laundering cases with\nminimum human intervention needs.",
          "link": "http://arxiv.org/abs/2105.10866",
          "publishedOn": "2021-05-25T01:56:09.318Z",
          "wordCount": 543,
          "title": "Towards Artificial Intelligence Enabled Financial Crime Detection. (arXiv:2105.10866v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thorpe_J/0/1/0/all/0/1\">John Thorpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yifan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyolfson_J/0/1/0/all/0/1\">Jonathan Eyolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Shen Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guanzhou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhihao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jinliang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vora_K/0/1/0/all/0/1\">Keval Vora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1\">Ravi Netravali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Miryung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoqing Harry Xu</a>",
          "description": "A graph neural network (GNN) enables deep learning on structured graph data.\nThere are two major GNN training obstacles: 1) it relies on high-end servers\nwith many GPUs which are expensive to purchase and maintain, and 2) limited\nmemory on GPUs cannot scale to today's billion-edge graphs. This paper presents\nDorylus: a distributed system for training GNNs. Uniquely, Dorylus can take\nadvantage of serverless computing to increase scalability at a low cost.\n\nThe key insight guiding our design is computation separation. Computation\nseparation makes it possible to construct a deep, bounded-asynchronous pipeline\nwhere graph and tensor parallel tasks can fully overlap, effectively hiding the\nnetwork latency incurred by Lambdas. With the help of thousands of Lambda\nthreads, Dorylus scales GNN training to billion-edge graphs. Currently, for\nlarge graphs, CPU servers offer the best performance-per-dollar over GPU\nservers. Just using Lambdas on top of CPU servers offers up to 2.75x more\nperformance-per-dollar than training only with CPU servers. Concretely, Dorylus\nis 1.22x faster and 4.83x cheaper than GPU servers for massive sparse graphs.\nDorylus is up to 3.8x faster and 10.7x cheaper compared to existing\nsampling-based systems.",
          "link": "http://arxiv.org/abs/2105.11118",
          "publishedOn": "2021-05-25T01:56:09.311Z",
          "wordCount": 645,
          "title": "Dorylus: Affordable, Scalable, and Accurate GNN Training over Billion-Edge Graphs. (arXiv:2105.11118v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okawa_M/0/1/0/all/0/1\">Maya Okawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_Y/0/1/0/all/0/1\">Yusuke Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_H/0/1/0/all/0/1\">Hiroyuki Toda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurashima_T/0/1/0/all/0/1\">Takeshi Kurashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>",
          "description": "Sequences of events including infectious disease outbreaks, social network\nactivities, and crimes are ubiquitous and the data on such events carry\nessential information about the underlying diffusion processes between\ncommunities (e.g., regions, online user groups). Modeling diffusion processes\nand predicting future events are crucial in many applications including\nepidemic control, viral marketing, and predictive policing. Hawkes processes\noffer a central tool for modeling the diffusion processes, in which the\ninfluence from the past events is described by the triggering kernel. However,\nthe triggering kernel parameters, which govern how each community is influenced\nby the past events, are assumed to be static over time. In the real world, the\ndiffusion processes depend not only on the influences from the past, but also\nthe current (time-evolving) states of the communities, e.g., people's awareness\nof the disease and people's current interests. In this paper, we propose a\nnovel Hawkes process model that is able to capture the underlying dynamics of\ncommunity states behind the diffusion processes and predict the occurrences of\nevents based on the dynamics. Specifically, we model the latent dynamic\nfunction that encodes these hidden dynamics by a mixture of neural networks.\nThen we design the triggering kernel using the latent dynamic function and its\nintegral. The proposed method, termed DHP (Dynamic Hawkes Processes), offers a\nflexible way to learn complex representations of the time-evolving communities'\nstates, while at the same time it allows to computing the exact likelihood,\nwhich makes parameter learning tractable. Extensive experiments on four\nreal-world event datasets show that DHP outperforms five widely adopted methods\nfor event prediction.",
          "link": "http://arxiv.org/abs/2105.11152",
          "publishedOn": "2021-05-25T01:56:09.304Z",
          "wordCount": 725,
          "title": "Dynamic Hawkes Processes for Discovering Time-evolving Communities' States behind Diffusion Processes. (arXiv:2105.11152v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haimin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hongwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Rui Deng</a>",
          "description": "With the continuous development of the petroleum industry, long-distance\ntransportation of oil and gas has been the norm. Due to gravity differentiation\nin horizontal wells and highly deviated wells (non-vertical wells), the water\nphase at the bottom of the pipeline will cause scaling and corrosion in the\npipeline. Scaling and corrosion will make the transportation process difficult,\nand transportation costs will be considerably increased. Therefore, the study\nof the oil-water two-phase flow pattern is of great importance to oil\nproduction. In this paper, a fuzzy inference system is used to predict the flow\npattern of the fluid, get the prediction result, and compares it with the\nprediction result of the BP neural network. From the comparison of the results,\nwe found that the prediction results of the fuzzy inference system are more\naccurate and reliable than the prediction results of the BP neural network. At\nthe same time, it can realize real-time monitoring and has less error control.\nExperimental results demonstrate that in the entire production logging process\nof non-vertical wells, the use of a fuzzy inference system to predict fluid\nflow patterns can greatly save production costs while ensuring the safe\noperation of production equipment.",
          "link": "http://arxiv.org/abs/2105.11181",
          "publishedOn": "2021-05-25T01:56:09.297Z",
          "wordCount": 628,
          "title": "Fuzzy inference system application for oil-water flow patterns identification. (arXiv:2105.11181v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1\">Mikolaj Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>",
          "description": "State-of-the-art performance for many emerging edge applications is achieved\nby deep neural networks (DNNs). Often, these DNNs are location and time\nsensitive, and the parameters of a specific DNN must be delivered from an edge\nserver to the edge device rapidly and efficiently to carry out time-sensitive\ninference tasks. We introduce AirNet, a novel training and analog transmission\nmethod that allows efficient wireless delivery of DNNs. We first train the DNN\nwith noise injection to counter the wireless channel noise. We also employ\npruning to reduce the channel bandwidth necessary for transmission, and perform\nknowledge distillation from a larger model to achieve satisfactory performance,\ndespite the channel perturbations. We show that AirNet achieves significantly\nhigher test accuracy compared to digital alternatives under the same bandwidth\nand power constraints. It also exhibits graceful degradation with channel\nquality, which reduces the requirement for accurate channel estimation.",
          "link": "http://arxiv.org/abs/2105.11166",
          "publishedOn": "2021-05-25T01:56:09.273Z",
          "wordCount": 579,
          "title": "AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1\">Maciej Wo&#x142;czyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zajac_M/0/1/0/all/0/1\">Micha&#x142; Zaj&#x105;c</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucinski_L/0/1/0/all/0/1\">&#x141;ukasz Kuci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1\">Piotr Mi&#x142;o&#x15b;</a>",
          "description": "Continual learning (CL) -- the ability to continuously learn, building on\npreviously acquired knowledge -- is a natural requirement for long-lived\nautonomous reinforcement learning (RL) agents. While building such agents, one\nneeds to balance opposing desiderata, such as constraints on capacity and\ncompute, the ability to not catastrophically forget, and to exhibit positive\ntransfer on new tasks. Understanding the right trade-off is conceptually and\ncomputationally challenging, which we argue has led the community to overly\nfocus on catastrophic forgetting. In response to these issues, we advocate for\nthe need to prioritize forward transfer and propose Continual World, a\nbenchmark consisting of realistic and meaningfully diverse robotic tasks built\non top of Meta-World as a testbed. Following an in-depth empirical evaluation\nof existing CL methods, we pinpoint their limitations and highlight unique\nalgorithmic challenges in the RL setting. Our benchmark aims to provide a\nmeaningful and computationally inexpensive challenge for the community and thus\nhelp better understand the performance of existing and future solutions.",
          "link": "http://arxiv.org/abs/2105.10919",
          "publishedOn": "2021-05-25T01:56:09.264Z",
          "wordCount": 598,
          "title": "Continual World: A Robotic Benchmark For Continual Reinforcement Learning. (arXiv:2105.10919v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jouhari_M/0/1/0/all/0/1\">Mohammed Jouhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ali_A/0/1/0/all/0/1\">Abdulla Al-Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1\">Emna Baccour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1\">Mohsen Guizani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1\">Mounir Hamdi</a>",
          "description": "Unmanned Aerial Vehicles (UAVs) have attracted great interest in the last few\nyears owing to their ability to cover large areas and access difficult and\nhazardous target zones, which is not the case of traditional systems relying on\ndirect observations obtained from fixed cameras and sensors. Furthermore,\nthanks to the advancements in computer vision and machine learning, UAVs are\nbeing adopted for a broad range of solutions and applications. However, Deep\nNeural Networks (DNNs) are progressing toward deeper and complex models that\nprevent them from being executed on-board. In this paper, we propose a DNN\ndistribution methodology within UAVs to enable data classification in\nresource-constrained devices and avoid extra delays introduced by the\nserver-based solutions due to data communication over air-to-ground links. The\nproposed method is formulated as an optimization problem that aims to minimize\nthe latency between data collection and decision-making while considering the\nmobility model and the resource constraints of the UAVs as part of the\nair-to-air communication. We also introduce the mobility prediction to adapt\nour system to the dynamics of UAVs and the network variation. The simulation\nconducted to evaluate the performance and benchmark the proposed methods,\nnamely Optimal UAV-based Layer Distribution (OULD) and OULD with Mobility\nPrediction (OULD-MP), were run in an HPC cluster. The obtained results show\nthat our optimization solution outperforms the existing and heuristic-based\napproaches.",
          "link": "http://arxiv.org/abs/2105.11013",
          "publishedOn": "2021-05-25T01:56:09.257Z",
          "wordCount": 687,
          "title": "Distributed CNN Inference on Resource-Constrained UAVs for Surveillance Systems: Design and Optimization. (arXiv:2105.11013v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10867",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+An_S/0/1/0/all/0/1\">SeungHwan An</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jeon_J/0/1/0/all/0/1\">Jong-June Jeon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Choi_H/0/1/0/all/0/1\">Hosik Choi</a>",
          "description": "We propose a new semi-supervised learning method of Variational AutoEncoder\n(VAE) which yields explainable latent space by EXplainable encoder Network\n(EXoN). The EXoN provides two useful tools for implementing VAE. First, we can\nfreely assign a conceptual center of latent distribution for a specific label.\nWe separate the latent space of VAE with multi-modal property of the Gaussian\nmixture distribution according to labels of observations. Next, we can easily\ninvestigate the latent subspace by a simple statistics, known as\n$F$-statistics, obtained from the EXoN. We found that both negative\ncross-entropy and Kullback-Leibler divergence play a crucial role in\nconstructing explainable latent space and the variability of the generated\nsamples from our proposed model depends on a specific subspace, called\n`activated latent subspace'. With MNIST and CIFAR-10 dataset, we show that the\nEXoN can produce explainable latent space which effectively represents labels\nand characteristics of the images.",
          "link": "http://arxiv.org/abs/2105.10867",
          "publishedOn": "2021-05-25T01:56:09.247Z",
          "wordCount": 563,
          "title": "EXoN: EXplainable encoder Network. (arXiv:2105.10867v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1\">Dustin Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Scientific document understanding is challenging as the data is highly domain\nspecific and diverse. However, datasets for tasks with scientific text require\nexpensive manual annotation and tend to be small and limited to only one or a\nfew fields. At the same time, scientific documents contain many potential\ntraining signals, such as citations, which can be used to build large labelled\ndatasets. Given this, we present an in-depth study of cite-worthiness detection\nin English, where a sentence is labelled for whether or not it cites an\nexternal source. To accomplish this, we introduce CiteWorth, a large,\ncontextualized, rigorously cleaned labelled dataset for cite-worthiness\ndetection built from a massive corpus of extracted plain-text scientific\ndocuments. We show that CiteWorth is high-quality, challenging, and suitable\nfor studying problems such as domain adaptation. Our best performing\ncite-worthiness detection model is a paragraph-level contextualized sentence\nlabelling model based on Longformer, exhibiting a 5 F1 point improvement over\nSciBERT which considers only individual sentences. Finally, we demonstrate that\nlanguage model fine-tuning with cite-worthiness as a secondary task leads to\nimproved performance on downstream scientific document understanding tasks.",
          "link": "http://arxiv.org/abs/2105.10912",
          "publishedOn": "2021-05-25T01:56:09.230Z",
          "wordCount": 625,
          "title": "CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding. (arXiv:2105.10912v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tanveer Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalas_A/0/1/0/all/0/1\">Antonis Michalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhunzada_A/0/1/0/all/0/1\">Adnan Akhunzada</a>",
          "description": "Social Networks' omnipresence and ease of use has revolutionized the\ngeneration and distribution of information in today's world. However, easy\naccess to information does not equal an increased level of public knowledge.\nUnlike traditional media channels, social networks also facilitate faster and\nwider spread of disinformation and misinformation. Viral spread of false\ninformation has serious implications on the behaviors, attitudes and beliefs of\nthe public, and ultimately can seriously endanger the democratic processes.\nLimiting false information's negative impact through early detection and\ncontrol of extensive spread presents the main challenge facing researchers\ntoday. In this survey paper, we extensively analyze a wide range of different\nsolutions for the early detection of fake news in the existing literature. More\nprecisely, we examine Machine Learning (ML) models for the identification and\nclassification of fake news, online fake news detection competitions,\nstatistical outputs as well as the advantages and disadvantages of some of the\navailable data sets. Finally, we evaluate the online web browsing tools\navailable for detecting and mitigating fake news and present some open research\nchallenges.",
          "link": "http://arxiv.org/abs/2105.10671",
          "publishedOn": "2021-05-25T01:56:09.222Z",
          "wordCount": 615,
          "title": "SOK: Fake News Outbreak 2021: Can We Stop the Viral Spread?. (arXiv:2105.10671v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_P/0/1/0/all/0/1\">Pascal Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smajic_A/0/1/0/all/0/1\">Alen Smajic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehler_A/0/1/0/all/0/1\">Alexander Mehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrami_G/0/1/0/all/0/1\">Giuseppe Abrami</a>",
          "description": "As global trends are shifting towards data-driven industries, the demand for\nautomated algorithms that can convert digital images of scanned documents into\nmachine readable information is rapidly growing. Besides the opportunity of\ndata digitization for the application of data analytic tools, there is also a\nmassive improvement towards automation of processes, which previously would\nrequire manual inspection of the documents. Although the introduction of\noptical character recognition technologies mostly solved the task of converting\nhuman-readable characters from images into machine-readable characters, the\ntask of extracting table semantics has been less focused on over the years. The\nrecognition of tables consists of two main tasks, namely table detection and\ntable structure recognition. Most prior work on this problem focuses on either\ntask without offering an end-to-end solution or paying attention to real\napplication conditions like rotated images or noise artefacts inside the\ndocument image. Recent work shows a clear trend towards deep learning\napproaches coupled with the use of transfer learning for the task of table\nstructure recognition due to the lack of sufficiently large datasets. In this\npaper we present a multistage pipeline named Multi-Type-TD-TSR, which offers an\nend-to-end solution for the problem of table recognition. It utilizes\nstate-of-the-art deep learning models for table detection and differentiates\nbetween 3 different types of tables based on the tables' borders. For the table\nstructure recognition we use a deterministic non-data driven algorithm, which\nworks on all table types. We additionally present two algorithms. One for\nunbordered tables and one for bordered tables, which are the base of the used\ntable structure recognition algorithm. We evaluate Multi-Type-TD-TSR on the\nICDAR 2019 table structure recognition dataset and achieve a new\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2105.11021",
          "publishedOn": "2021-05-25T01:56:09.213Z",
          "wordCount": 760,
          "title": "Multi-Type-TD-TSR -- Extracting Tables from Document Images using a Multi-stage Pipeline for Table Detection and Table Structure Recognition: from OCR to Structured Table Representations. (arXiv:2105.11021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhanpeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper aims to formulate the problem of estimating the optimal baseline\nvalues for the Shapley value in game theory. The Shapley value measures the\nattribution of each input variable of a complex model, which is computed as the\nmarginal benefit from the presence of this variable w.r.t.its absence under\ndifferent contexts. To this end, people usually set the input variable to its\nbaseline value to represent the absence of this variable (i.e.the no-signal\nstate of this variable). Previous studies usually determine the baseline values\nin an empirical manner, which hurts the trustworthiness of the Shapley value.\nIn this paper, we revisit the feature representation of a deep model from the\nperspective of game theory, and define the multi-variate interaction patterns\nof input variables to define the no-signal state of an input variable. Based on\nthe multi-variate interaction, we learn the optimal baseline value of each\ninput variable. Experimental results have demonstrated the effectiveness of our\nmethod.",
          "link": "http://arxiv.org/abs/2105.10719",
          "publishedOn": "2021-05-25T01:56:09.205Z",
          "wordCount": 581,
          "title": "Learning Baseline Values for Shapley Values. (arXiv:2105.10719v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanding Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Mingyang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chao Wu</a>",
          "description": "Graph neural networks (GNN) have been successful in many fields, and derived\nvarious researches and applications in real industries. However, in some\nprivacy sensitive scenarios (like finance, healthcare), training a GNN model\ncentrally faces challenges due to the distributed data silos. Federated\nlearning (FL) is a an emerging technique that can collaboratively train a\nshared model while keeping the data decentralized, which is a rational solution\nfor distributed GNN training. We term it as federated graph learning (FGL).\nAlthough FGL has received increasing attention recently, the definition and\nchallenges of FGL is still up in the air. In this position paper, we present a\ncategorization to clarify it. Considering how graph data are distributed among\nclients, we propose four types of FGL: inter-graph FL, intra-graph FL and\ngraph-structured FL, where intra-graph is further divided into horizontal and\nvertical FGL. For each type of FGL, we make a detailed discussion about the\nformulation and applications, and propose some potential challenges.",
          "link": "http://arxiv.org/abs/2105.11099",
          "publishedOn": "2021-05-25T01:56:09.198Z",
          "wordCount": 598,
          "title": "Federated Graph Learning -- A Position Paper. (arXiv:2105.11099v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasirigerdeh_R/0/1/0/all/0/1\">Reza Nasirigerdeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torkzadehmahani_R/0/1/0/all/0/1\">Reihaneh Torkzadehmahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matschinske_J/0/1/0/all/0/1\">Julian Matschinske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumbach_J/0/1/0/all/0/1\">Jan Baumbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "Federated learning (FL) enables multiple clients to jointly train a global\nmodel under the coordination of a central server. Although FL is a\nprivacy-aware paradigm, where raw data sharing is not required, recent studies\nhave shown that FL might leak the private data of a client through the model\nparameters shared with the server or the other clients. In this paper, we\npresent the HyFed framework, which enhances the privacy of FL while preserving\nthe utility of the global model. HyFed provides developers with a generic API\nto develop federated, privacy-preserving algorithms. HyFed supports both\nsimulation and federated operation modes and its source code is publicly\navailable at https://github.com/tum-aimed/hyfed.",
          "link": "http://arxiv.org/abs/2105.10545",
          "publishedOn": "2021-05-25T01:56:09.192Z",
          "wordCount": 545,
          "title": "HyFed: A Hybrid Federated Framework for Privacy-preserving Machine Learning. (arXiv:2105.10545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1\">Marco Visca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1\">Sampo Kuutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1\">Roger Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>",
          "description": "Terrain traversability analysis plays a major role in ensuring safe robotic\nnavigation in unstructured environments. However, real-time constraints\nfrequently limit the accuracy of online tests, especially in scenarios where\nrealistic robot-terrain interactions are complex to model. In this context, we\npropose a deep learning framework, trained in an end-to-end fashion from\nelevation maps and trajectories, to estimate the occurrence of failure events.\nThe network is first trained and tested in simulation over synthetic maps\ngenerated by the OpenSimplex algorithm. The prediction performance of the Deep\nLearning framework is illustrated by being able to retain over 94% recall of\nthe original simulator at 30% of the computational time. Finally, the network\nis transferred and tested on real elevation maps collected by the SEEKER\nconsortium during the Martian rover test trial in the Atacama desert in Chile.\nWe show that transferring and fine-tuning of an application-independent\npre-trained model retains better performance than training uniquely on scarcely\navailable real data.",
          "link": "http://arxiv.org/abs/2105.10937",
          "publishedOn": "2021-05-25T01:56:09.185Z",
          "wordCount": 606,
          "title": "Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salazar_Gomez_G/0/1/0/all/0/1\">Gustavo A. Salazar-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saavedra_Ruiz_M/0/1/0/all/0/1\">Miguel A. Saavedra-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Cano_V/0/1/0/all/0/1\">Victor A. Romero-Cano</a>",
          "description": "This paper tackles the 3D object detection problem, which is of vital\nimportance for applications such as autonomous driving. Our framework uses a\nMachine Learning (ML) pipeline on a combination of monocular camera and LiDAR\ndata to detect vehicles in the surrounding 3D space of a moving platform. It\nuses frustum region proposals generated by State-Of-The-Art (SOTA) 2D object\ndetectors to segment LiDAR point clouds into point clusters which represent\npotentially individual objects. We evaluate the performance of classical ML\nalgorithms as part of an holistic pipeline for estimating the parameters of 3D\nbounding boxes which surround the vehicles around the moving platform. Our\nresults demonstrate an efficient and accurate inference on a validation set,\nachieving an overall accuracy of 87.1%.",
          "link": "http://arxiv.org/abs/2105.11060",
          "publishedOn": "2021-05-25T01:56:09.178Z",
          "wordCount": 571,
          "title": "High-level camera-LiDAR fusion for 3D object detection with machine learning. (arXiv:2105.11060v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carnerero_Cano_J/0/1/0/all/0/1\">Javier Carnerero-Cano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Gonzalez_L/0/1/0/all/0/1\">Luis Mu&#xf1;oz-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spencer_P/0/1/0/all/0/1\">Phillippa Spencer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1\">Emil C. Lupu</a>",
          "description": "Machine learning algorithms are vulnerable to poisoning attacks, where a\nfraction of the training data is manipulated to degrade the algorithms'\nperformance. We show that current approaches, which typically assume that\nregularization hyperparameters remain constant, lead to an overly pessimistic\nview of the algorithms' robustness and of the impact of regularization. We\npropose a novel optimal attack formulation that considers the effect of the\nattack on the hyperparameters, modelling the attack as a \\emph{minimax bilevel\noptimization problem}. This allows to formulate optimal attacks, select\nhyperparameters and evaluate robustness under worst case conditions. We apply\nthis formulation to logistic regression using $L_2$ regularization, empirically\nshow the limitations of previous strategies and evidence the benefits of using\n$L_2$ regularization to dampen the effect of poisoning attacks.",
          "link": "http://arxiv.org/abs/2105.10948",
          "publishedOn": "2021-05-25T01:56:09.171Z",
          "wordCount": 583,
          "title": "Regularization Can Help Mitigate Poisoning Attacks... with the Right Hyperparameters. (arXiv:2105.10948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1\">Won Joon Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1\">Byungju Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1\">Young-Chai Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jihong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>",
          "description": "In this article, we study the problem of air-to-ground ultra-reliable and\nlow-latency communication (URLLC) for a moving ground user. This is done by\ncontrolling multiple unmanned aerial vehicles (UAVs) in real time while\navoiding inter-UAV collisions. To this end, we propose a novel multi-agent deep\nreinforcement learning (MADRL) framework, coined a graph attention exchange\nnetwork (GAXNet). In GAXNet, each UAV constructs an attention graph locally\nmeasuring the level of attention to its neighboring UAVs, while exchanging the\nattention weights with other UAVs so as to reduce the attention mismatch\nbetween them. Simulation results corroborates that GAXNet achieves up to 4.5x\nhigher rewards during training. At execution, without incurring inter-UAV\ncollisions, GAXNet achieves 6.5x lower latency with the target 0.0000001 error\nrate, compared to a state-of-the-art baseline framework.",
          "link": "http://arxiv.org/abs/2105.10716",
          "publishedOn": "2021-05-25T01:56:09.138Z",
          "wordCount": 566,
          "title": "Attention-based Reinforcement Learning for Real-Time UAV Semantic Communication. (arXiv:2105.10716v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10832",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Furuya_T/0/1/0/all/0/1\">Takashi Furuya</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suetake_K/0/1/0/all/0/1\">Kazuma Suetake</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Taniguchi_K/0/1/0/all/0/1\">Koichi Taniguchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kusumoto_H/0/1/0/all/0/1\">Hiroyuki Kusumoto</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saiin_R/0/1/0/all/0/1\">Ryuji Saiin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Daimon_T/0/1/0/all/0/1\">Tomohiro Daimon</a>",
          "description": "Pruning techniques for neural networks with a recurrent architecture, such as\nthe recurrent neural network (RNN), are strongly desired for their application\nto edge-computing devices. However, the recurrent architecture is generally not\nrobust to pruning because even small pruning causes accumulation error and the\ntotal error increases significantly over time. In this paper, we propose an\nappropriate pruning algorithm for RNNs inspired by \"spectral pruning\", and\nprovide the generalization error bounds for compressed RNNs. We also provide\nnumerical experiments to demonstrate our theoretical results and show the\neffectiveness of our pruning method compared with existing methods.",
          "link": "http://arxiv.org/abs/2105.10832",
          "publishedOn": "2021-05-25T01:56:09.114Z",
          "wordCount": 529,
          "title": "Spectral Pruning for Recurrent Neural Networks. (arXiv:2105.10832v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">John Y. Shin</a>",
          "description": "Heavy-tailed distributions have been studied in statistics, random matrix\ntheory, physics, and econometrics as models of correlated systems, among other\ndomains. Further, heavy-tail distributed eigenvalues of the covariance matrix\nof the weight matrices in neural networks have been shown to empirically\ncorrelate with test set accuracy in several works (e.g. arXiv:1901.08276), but\na formal relationship between heavy-tail distributed parameters and\ngeneralization bounds was yet to be demonstrated. In this work, the compression\nframework of arXiv:1802.05296 is utilized to show that matrices with heavy-tail\ndistributed matrix elements can be compressed, resulting in networks with\nsparse weight matrices. Since the parameter count has been reduced to a sum of\nthe non-zero elements of sparse matrices, the compression framework allows us\nto bound the generalization gap of the resulting compressed network with a\nnon-vacuous generalization bound. Further, the action of these matrices on a\nvector is discussed, and how they may relate to compression and resilient\nclassification is analyzed.",
          "link": "http://arxiv.org/abs/2105.11025",
          "publishedOn": "2021-05-25T01:56:09.089Z",
          "wordCount": 580,
          "title": "Compressing Heavy-Tailed Weight Matrices for Non-Vacuous Generalization Bounds. (arXiv:2105.11025v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10766",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Manjunath_G/0/1/0/all/0/1\">G Manjunath</a>",
          "description": "The celebrated Takens' embedding theorem concerns embedding an attractor of a\ndynamical system in a Euclidean space of appropriate dimension through a\ngeneric delay-observation map. The embedding also establishes a topological\nconjugacy. In this paper, we show how an arbitrary sequence can be mapped into\nanother space as an attractive solution of a nonautonomous dynamical system.\nSuch mapping also entails a topological conjugacy and an embedding between the\nsequence and the attractive solution spaces. This result is not a\ngeneralization of Takens embedding theorem but helps us understand what exactly\nis required by discrete-time state space models widely used in applications to\nembed an external stimulus onto its solution space. Our results settle another\nbasic problem concerning the perturbation of an autonomous dynamical system. We\ndescribe what exactly happens to the dynamics when exogenous noise perturbs\ncontinuously a local irreducible attracting set (such as a stable fixed point)\nof a discrete-time autonomous dynamical system.",
          "link": "http://arxiv.org/abs/2105.10766",
          "publishedOn": "2021-05-25T01:56:09.081Z",
          "wordCount": 573,
          "title": "Embedding Information onto a Dynamical System. (arXiv:2105.10766v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuchen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liangyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yibo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuanxiong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canini_M/0/1/0/all/0/1\">Marco Canini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Arvind Krishnamurthy</a>",
          "description": "The learning rate (LR) schedule is one of the most important hyper-parameters\nneeding careful tuning in training DNNs. However, it is also one of the least\nautomated parts of machine learning systems and usually costs significant\nmanual effort and computing. Though there are pre-defined LR schedules and\noptimizers with adaptive LR, they introduce new hyperparameters that need to be\ntuned separately for different tasks/datasets. In this paper, we consider the\nquestion: Can we automatically tune the LR over the course of training without\nhuman involvement? We propose an efficient method, AutoLRS, which automatically\noptimizes the LR for each training stage by modeling training dynamics. AutoLRS\naims to find an LR applied to every $\\tau$ steps that minimizes the resulted\nvalidation loss. We solve this black-box optimization on the fly by Bayesian\noptimization (BO). However, collecting training instances for BO requires a\nsystem to evaluate each LR queried by BO's acquisition function for $\\tau$\nsteps, which is prohibitively expensive in practice. Instead, we apply each\ncandidate LR for only $\\tau'\\ll\\tau$ steps and train an exponential model to\npredict the validation loss after $\\tau$ steps. This mutual-training process\nbetween BO and the loss-prediction model allows us to limit the training steps\ninvested in the BO search. We demonstrate the advantages and the generality of\nAutoLRS through extensive experiments of training DNNs for tasks from diverse\ndomains using different optimizers. The LR schedules auto-generated by AutoLRS\nlead to a speedup of $1.22\\times$, $1.43\\times$, and $1.5\\times$ when training\nResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in\ntheir original papers, and an average speedup of $1.31\\times$ over\nstate-of-the-art heavily-tuned LR schedules.",
          "link": "http://arxiv.org/abs/2105.10762",
          "publishedOn": "2021-05-25T01:56:09.075Z",
          "wordCount": 715,
          "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly. (arXiv:2105.10762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Ashwin Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskar_A/0/1/0/all/0/1\">A Baskar</a>",
          "description": "We present a general technique for constructing Graph Neural Networks (GNNs)\ncapable of using multi-relational domain knowledge. The technique is based on\nmode-directed inverse entailment (MDIE) developed in Inductive Logic\nProgramming (ILP). Given a data instance $e$ and background knowledge $B$, MDIE\nidentifies a most-specific logical formula $\\bot_B(e)$ that contains all the\nrelational information in $B$ that is related to $e$. We transform $\\bot_B(e)$\ninto a corresponding \"bottom-graph\" that can be processed for use by standard\nGNN implementations. This transformation allows a principled way of\nincorporating generic background knowledge into GNNs: we use the term `BotGNN'\nfor this form of graph neural networks. For several GNN variants, using\nreal-world datasets with substantial background knowledge, we show that BotGNNs\nperform significantly better than both GNNs without background knowledge and a\nrecently proposed simplified technique for including domain knowledge into\nGNNs. We also provide experimental evidence comparing BotGNNs favourably to\nmulti-layer perceptrons (MLPs) that use features representing a\n\"propositionalised\" form of the background knowledge; and BotGNNs to a standard\nILP based on the use of most-specific clauses. Taken together, these results\npoint to BotGNNs as capable of combining the computational efficacy of GNNs\nwith the representational versatility of ILP.",
          "link": "http://arxiv.org/abs/2105.10709",
          "publishedOn": "2021-05-25T01:56:09.068Z",
          "wordCount": 647,
          "title": "Inclusion of Domain-Knowledge into GNNs using Mode-Directed Inverse Entailment. (arXiv:2105.10709v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PYKL_S/0/1/0/all/0/1\">Srinivas PYKL</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_P/0/1/0/all/0/1\">Prerana Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulabaigari_V/0/1/0/all/0/1\">Viswanath Pulabaigari</a>",
          "description": "Contending hate speech in social media is one of the most challenging social\nproblems of our time. There are various types of anti-social behavior in social\nmedia. Foremost of them is aggressive behavior, which is causing many social\nissues such as affecting the social lives and mental health of social media\nusers. In this paper, we propose an end-to-end ensemble-based architecture to\nautomatically identify and classify aggressive tweets. Tweets are classified\ninto three categories - Covertly Aggressive, Overtly Aggressive, and\nNon-Aggressive. The proposed architecture is an ensemble of smaller subnetworks\nthat are able to characterize the feature embeddings effectively. We\ndemonstrate qualitatively that each of the smaller subnetworks is able to learn\nunique features. Our best model is an ensemble of Capsule Networks and results\nin a 65.2% F1 score on the Facebook test set, which results in a performance\ngain of 0.95% over the TRAC-2018 winners. The code and the model weights are\npublicly available at\nhttps://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks.",
          "link": "http://arxiv.org/abs/2105.11219",
          "publishedOn": "2021-05-25T01:56:09.061Z",
          "wordCount": 600,
          "title": "Hater-O-Genius Aggression Classification using Capsule Networks. (arXiv:2105.11219v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11187",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kiourt_C/0/1/0/all/0/1\">Chairi Kiourt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feretzakis_G/0/1/0/all/0/1\">Georgios Feretzakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalamarinis_K/0/1/0/all/0/1\">Konstantinos Dalamarinis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalles_D/0/1/0/all/0/1\">Dimitris Kalles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pantos_G/0/1/0/all/0/1\">Georgios Pantos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papadopoulos_I/0/1/0/all/0/1\">Ioannis Papadopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kouris_S/0/1/0/all/0/1\">Spyros Kouris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ioannakis_G/0/1/0/all/0/1\">George Ioannakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loupelis_E/0/1/0/all/0/1\">Evangelos Loupelis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sakagianni_A/0/1/0/all/0/1\">Aikaterini Sakagianni</a>",
          "description": "The main objective of this work is to utilize state-of-the-art deep learning\napproaches for the identification of pulmonary embolism in CTPA-Scans for\nCOVID-19 patients, provide an initial assessment of their performance and,\nultimately, provide a fast-track prototype solution (system). We adopted and\nassessed some of the most popular convolutional neural network architectures\nthrough transfer learning approaches, to strive to combine good model accuracy\nwith fast training. Additionally, we exploited one of the most popular\none-stage object detection models for the localization (through object\ndetection) of the pulmonary embolism regions-of-interests. The models of both\napproaches are trained on an original CTPA-Scan dataset, where we annotated of\n673 CTPA-Scan images with 1,465 bounding boxes in total, highlighting pulmonary\nembolism regions-of-interests. We provide a brief assessment of some\nstate-of-the-art image classification models by achieving validation accuracies\nof 91% in pulmonary embolism classification. Additionally, we achieved a\nprecision of about 68% on average in the object detection model for the\npulmonary embolism localization under 50% IoU threshold. For both approaches,\nwe provide the entire training pipelines for future studies (step by step\nprocesses through source code). In this study, we present some of the most\naccurate and fast deep learning models for pulmonary embolism identification in\nCTPA-Scans images, through classification and localization (object detection)\napproaches for patients infected by COVID-19. We provide a fast-track solution\n(system) for the research community of the area, which combines both\nclassification and object detection models for improving the precision of\nidentifying pulmonary embolisms.",
          "link": "http://arxiv.org/abs/2105.11187",
          "publishedOn": "2021-05-25T01:56:09.038Z",
          "wordCount": 774,
          "title": "Pulmonary embolism identification in computerized tomography pulmonary angiography scans with deep learning technologies in COVID-19 patients. (arXiv:2105.11187v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1\">Sudipta Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manasi_S/0/1/0/all/0/1\">Susmita Dey Manasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1\">Kishor Kunal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapatnekar_S/0/1/0/all/0/1\">Sachin S. Sapatnekar</a>",
          "description": "Analysis engines based on Graph Neural Networks (GNNs) are vital for many\nreal-world problems that model relationships using large graphs. Challenges for\na GNN hardware platform include the ability to (a) host a variety of GNNs, (b)\nhandle high sparsity in input node feature vectors and the graph adjacency\nmatrix and the accompanying random memory access patterns, and (c) maintain\nload-balanced computation in the face of uneven workloads induced by high\nsparsity and power-law vertex degree distributions in real datasets. The\nproposes GNNIE, an accelerator designed to run a broad range of GNNs. It\ntackles workload imbalance by (i) splitting node feature operands into blocks,\n(ii) reordering and redistributing computations, and (iii) using a flexible MAC\narchitecture with low communication overheads among the processing elements. In\naddition, it adopts a graph partitioning scheme and a graph-specific caching\npolicy that efficiently uses off-chip memory bandwidth that is well suited to\nthe characteristics of real-world graphs. Random memory access effects are\nmitigated by partitioning and degree-aware caching to enable the reuse of\nhigh-degree vertices. GNNIE achieves average speedups of over 8890x over a CPU\nand 295x over a GPU over multiple datasets on graph attention networks (GATs),\ngraph convolutional networks (GCNs), GraphSAGE, GINConv, and DiffPool, Compared\nto prior approaches, GNNIE achieves an average speedup of 9.74x over HyGCN for\nGCN, GraphSAGE, and GINConv; HyGCN cannot implement GATs. GNNIE achieves an\naverage speedup of 2.28x over AWB-GCN (which runs only GCNs), despite using\n3.4x fewer processing units.",
          "link": "http://arxiv.org/abs/2105.10554",
          "publishedOn": "2021-05-25T01:56:09.030Z",
          "wordCount": 676,
          "title": "GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching. (arXiv:2105.10554v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imola_J/0/1/0/all/0/1\">Jacob Imola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1\">Kamalika Chaudhuri</a>",
          "description": "Balancing privacy and accuracy is a major challenge in designing\ndifferentially private machine learning algorithms. To improve this tradeoff,\nprior work has looked at privacy amplification methods which analyze how common\ntraining operations such as iteration and subsampling the data can lead to\nhigher privacy. In this paper, we analyze privacy amplification properties of a\nnew operation, sampling from the posterior, that is used in Bayesian inference.\nIn particular, we look at Bernoulli sampling from a posterior that is described\nby a differentially private parameter. We provide an algorithm to compute the\namplification factor in this setting, and establish upper and lower bounds on\nthis factor. Finally, we look at what happens when we draw k posterior samples\ninstead of one.",
          "link": "http://arxiv.org/abs/2105.10594",
          "publishedOn": "2021-05-25T01:56:09.023Z",
          "wordCount": 550,
          "title": "Privacy Amplification Via Bernoulli Sampling. (arXiv:2105.10594v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuo Shao</a>",
          "description": "This paper studies \\emph{differential privacy (DP)} and \\emph{local\ndifferential privacy (LDP)} in cascading bandits. Under DP, we propose an\nalgorithm which guarantees $\\epsilon$-indistinguishability and a regret of\n$\\mathcal{O}((\\frac{\\log T}{\\epsilon})^{1+\\xi})$ for an arbitrarily small\n$\\xi$. This is a significant improvement from the previous work of\n$\\mathcal{O}(\\frac{\\log^3 T}{\\epsilon})$ regret. Under\n($\\epsilon$,$\\delta$)-LDP, we relax the $K^2$ dependence through the tradeoff\nbetween privacy budget $\\epsilon$ and error probability $\\delta$, and obtain a\nregret of $\\mathcal{O}(\\frac{K\\log (1/\\delta) \\log T}{\\epsilon^2})$, where $K$\nis the size of the arm subset. This result holds for both Gaussian mechanism\nand Laplace mechanism by analyses on the composition. Our results extend to\ncombinatorial semi-bandit. We show respective lower bounds for DP and LDP\ncascading bandits. Extensive experiments corroborate our theoretic findings.",
          "link": "http://arxiv.org/abs/2105.11126",
          "publishedOn": "2021-05-25T01:56:09.016Z",
          "wordCount": 540,
          "title": "Cascading Bandit under Differential Privacy. (arXiv:2105.11126v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haitong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yang Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shegnbo Eben Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangteng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sifa Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianyu Chen</a>",
          "description": "The safety constraints commonly used by existing safe reinforcement learning\n(RL) methods are defined only on expectation of initial states, but allow each\ncertain state to be unsafe, which is unsatisfying for real-world\nsafety-critical tasks. In this paper, we introduce the feasible actor-critic\n(FAC) algorithm, which is the first model-free constrained RL method that\nconsiders statewise safety, e.g, safety for each initial state. We claim that\nsome states are inherently unsafe no matter what policy we choose, while for\nother states there exist policies ensuring safety, where we say such states and\npolicies are feasible. By constructing a statewise Lagrange function available\non RL sampling and adopting an additional neural network to approximate the\nstatewise Lagrange multiplier, we manage to obtain the optimal feasible policy\nwhich ensures safety for each feasible state and the safest possible policy for\ninfeasible states. Furthermore, the trained multiplier net can indicate whether\na given state is feasible or not through the statewise complementary slackness\ncondition. We provide theoretical guarantees that FAC outperforms previous\nexpectation-based constrained RL methods in terms of both constraint\nsatisfaction and reward optimization. Experimental results on both robot\nlocomotive tasks and safe exploration tasks verify the safety enhancement and\nfeasibility interpretation of the proposed method.",
          "link": "http://arxiv.org/abs/2105.10682",
          "publishedOn": "2021-05-25T01:56:09.003Z",
          "wordCount": 635,
          "title": "Feasible Actor-Critic: Constrained Reinforcement Learning for Ensuring Statewise Safety. (arXiv:2105.10682v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10915",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chae_Y/0/1/0/all/0/1\">Younghwan Chae</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wilke_D/0/1/0/all/0/1\">Daniel N. Wilke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kafka_D/0/1/0/all/0/1\">Dominic Kafka</a>",
          "description": "Mini-batch sub-sampling (MBSS) is favored in deep neural network training to\nreduce the computational cost. Still, it introduces an inherent sampling error,\nmaking the selection of appropriate learning rates challenging. The sampling\nerrors can manifest either as a bias or variances in a line search. Dynamic\nMBSS re-samples a mini-batch at every function evaluation. Hence, dynamic MBSS\nresults in point-wise discontinuous loss functions with smaller bias but larger\nvariance than static sampled loss functions. However, dynamic MBSS has the\nadvantage of having larger data throughput during training but requires the\ncomplexity regarding discontinuities to be resolved. This study extends the\ngradient-only surrogate (GOS), a line search method using quadratic\napproximation models built with only directional derivative information, for\ndynamic MBSS loss functions. We propose a gradient-only approximation line\nsearch (GOALS) with strong convergence characteristics with defined optimality\ncriterion. We investigate GOALS's performance by applying it on various\noptimizers that include SGD, RMSprop and Adam on ResNet-18 and EfficientNetB0.\nWe also compare GOALS's against the other existing learning rate methods. We\nquantify both the best performing and most robust algorithms. For the latter,\nwe introduce a relative robust criterion that allows us to quantify the\ndifference between an algorithm and the best performing algorithm for a given\nproblem. The results show that training a model with the recommended learning\nrate for a class of search directions helps to reduce the model errors in\nmultimodal cases.",
          "link": "http://arxiv.org/abs/2105.10915",
          "publishedOn": "2021-05-25T01:56:08.997Z",
          "wordCount": 695,
          "title": "GOALS: Gradient-Only Approximations for Line Searches Towards Robust and Consistent Training of Deep Neural Networks. (arXiv:2105.10915v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Needell_C/0/1/0/all/0/1\">Coen D. Needell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bainbridge_W/0/1/0/all/0/1\">Wilma A. Bainbridge</a>",
          "description": "Various work has suggested that the memorability of an image is consistent\nacross people, and thus can be treated as an intrinsic property of an image.\nUsing computer vision models, we can make specific predictions about what\npeople will remember or forget. While older work has used now-outdated deep\nlearning architectures to predict image memorability, innovations in the field\nhave given us new techniques to apply to this problem. Here, we propose and\nevaluate five alternative deep learning models which exploit developments in\nthe field from the last five years, largely the introduction of residual neural\nnetworks, which are intended to allow the model to use semantic information in\nthe memorability estimation process. These new models were tested against the\nprior state of the art with a combined dataset built to optimize both\nwithin-category and across-category predictions. Our findings suggest that the\nkey prior memorability network had overstated its generalizability and was\noverfit on its training set. Our new models outperform this prior model,\nleading us to conclude that Residual Networks outperform simpler convolutional\nneural networks in memorability regression. We make our new state-of-the-art\nmodel readily available to the research community, allowing memory researchers\nto make predictions about memorability on a wider range of images.",
          "link": "http://arxiv.org/abs/2105.10598",
          "publishedOn": "2021-05-25T01:56:08.978Z",
          "wordCount": 663,
          "title": "Embracing New Techniques in Deep Learning for Estimating Image Memorability. (arXiv:2105.10598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shomron_G/0/1/0/all/0/1\">Gil Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_F/0/1/0/all/0/1\">Freddy Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurzum_S/0/1/0/all/0/1\">Samer Kurzum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiser_U/0/1/0/all/0/1\">Uri Weiser</a>",
          "description": "Quantization is a technique used in deep neural networks (DNNs) to increase\nexecution performance and hardware efficiency. Uniform post-training\nquantization (PTQ) methods are common, since they can be implemented\nefficiently in hardware and do not require extensive hardware resources or a\ntraining set. Mapping FP32 models to INT8 using uniform PTQ yields models with\nnegligible accuracy degradation; however, reducing precision below 8 bits with\nPTQ is challenging, as accuracy degradation becomes noticeable, due to the\nincrease in quantization noise. In this paper, we propose a sparsity-aware\nquantization (SPARQ) method, in which the unstructured and dynamic activation\nsparsity is leveraged in different representation granularities. 4-bit\nquantization, for example, is employed by dynamically examining the bits of\n8-bit values and choosing a window of 4 bits, while first skipping zero-value\nbits. Moreover, instead of quantizing activation-by-activation to 4 bits, we\nfocus on pairs of 8-bit activations and examine whether one of the two is equal\nto zero. If one is equal to zero, the second can opportunistically use the\nother's 4-bit budget; if both do not equal zero, then each is dynamically\nquantized to 4 bits, as described. SPARQ achieves minor accuracy degradation,\n2x speedup over widely used hardware architectures, and a practical hardware\nimplementation. The code is available at https://github.com/gilshm/sparq.",
          "link": "http://arxiv.org/abs/2105.11010",
          "publishedOn": "2021-05-25T01:56:08.971Z",
          "wordCount": 636,
          "title": "Post-Training Sparsity-Aware Quantization. (arXiv:2105.11010v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atsidakou_A/0/1/0/all/0/1\">Alexia Atsidakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadigenopoulos_O/0/1/0/all/0/1\">Orestis Papadigenopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Soumya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caramanis_C/0/1/0/all/0/1\">Constantine Caramanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>",
          "description": "Recent work has considered natural variations of the multi-armed bandit\nproblem, where the reward distribution of each arm is a special function of the\ntime passed since its last pulling. In this direction, a simple (yet widely\napplicable) model is that of blocking bandits, where an arm becomes unavailable\nfor a deterministic number of rounds after each play. In this work, we extend\nthe above model in two directions: (i) We consider the general combinatorial\nsetting where more than one arms can be played at each round, subject to\nfeasibility constraints. (ii) We allow the blocking time of each arm to be\nstochastic. We first study the computational/unconditional hardness of the\nabove setting and identify the necessary conditions for the problem to become\ntractable (even in an approximate sense). Based on these conditions, we provide\na tight analysis of the approximation guarantee of a natural greedy heuristic\nthat always plays the maximum expected reward feasible subset among the\navailable (non-blocked) arms. When the arms' expected rewards are unknown, we\nadapt the above heuristic into a bandit algorithm, based on UCB, for which we\nprovide sublinear (approximate) regret guarantees, matching the theoretical\nlower bounds in the limiting case of absence of delays.",
          "link": "http://arxiv.org/abs/2105.10625",
          "publishedOn": "2021-05-25T01:56:08.964Z",
          "wordCount": 636,
          "title": "Combinatorial Blocking Bandits with Stochastic Delays. (arXiv:2105.10625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1\">Shadab Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barigidad_S/0/1/0/all/0/1\">Susmith Barigidad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_S/0/1/0/all/0/1\">Shadab Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suaib_M/0/1/0/all/0/1\">Md Suaib</a>",
          "description": "Healthcare is one of the most important aspects of human life. Heart disease\nis known to be one of the deadliest diseases which is hampering the lives of\nmany people around the world. Heart disease must be detected early so the loss\nof lives can be prevented. The availability of large-scale data for medical\ndiagnosis has helped developed complex machine learning and deep learning-based\nmodels for automated early diagnosis of heart diseases. The classical\napproaches have been limited in terms of not generalizing well to new data\nwhich have not been seen in the training set. This is indicated by a large gap\nin training and test accuracies. This paper proposes a novel deep learning\narchitecture using a 1D convolutional neural network for classification between\nhealthy and non-healthy persons to overcome the limitations of classical\napproaches. Various clinical parameters are used for assessing the risk profile\nin the patients which helps in early diagnosis. Various techniques are used to\navoid overfitting in the proposed network. The proposed network achieves over\n97% training accuracy and 96% test accuracy on the dataset. The accuracy of the\nmodel is compared in detail with other classification algorithms using various\nperformance parameters which proves the effectiveness of the proposed\narchitecture.",
          "link": "http://arxiv.org/abs/2105.10816",
          "publishedOn": "2021-05-25T01:56:08.957Z",
          "wordCount": 635,
          "title": "Novel Deep Learning Architecture for Heart Disease Prediction using Convolutional Neural Network. (arXiv:2105.10816v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gasimova_A/0/1/0/all/0/1\">Aydan Gasimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montana_G/0/1/0/all/0/1\">Giovanni Montana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "Gathering manually annotated images for the purpose of training a predictive\nmodel is far more challenging in the medical domain than for natural images as\nit requires the expertise of qualified radiologists. We therefore propose to\ntake advantage of past radiological exams (specifically, knee X-ray\nexaminations) and formulate a framework capable of learning the correspondence\nbetween the images and reports, and hence be capable of generating diagnostic\nreports for a given X-ray examination consisting of an arbitrary number of\nimage views. We demonstrate how aggregating the image features of individual\nexams and using them as conditional inputs when training a language generation\nmodel results in auto-generated exam reports that correlate well with\nradiologist-generated reports.",
          "link": "http://arxiv.org/abs/2105.10702",
          "publishedOn": "2021-05-25T01:56:08.951Z",
          "wordCount": 548,
          "title": "Automated Knee X-ray Report Generation. (arXiv:2105.10702v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamabattula_S/0/1/0/all/0/1\">Sree Ram Kamabattula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musini_K/0/1/0/all/0/1\">Kumudha Musini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazi_B/0/1/0/all/0/1\">Babak Namazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_G/0/1/0/all/0/1\">Ganesh Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devarajan_V/0/1/0/all/0/1\">Venkat Devarajan</a>",
          "description": "Training Deep neural networks (DNNs) on noisy labeled datasets is a\nchallenging problem, because learning on mislabeled examples deteriorates the\nperformance of the network. As the ground truth availability is limited with\nreal-world noisy datasets, previous papers created synthetic noisy datasets by\nrandomly modifying the labels of training examples of clean datasets. However,\nno final conclusions can be derived by just using this random noise, since it\nexcludes feature-dependent noise. Thus, it is imperative to generate\nfeature-dependent noisy datasets that additionally provide ground truth.\nTherefore, we propose an intuitive approach to creating feature-dependent noisy\ndatasets by utilizing the training predictions of DNNs on clean datasets that\nalso retain true label information. We refer to these datasets as \"Pseudo Noisy\ndatasets\". We conduct several experiments to establish that Pseudo noisy\ndatasets resemble feature-dependent noisy datasets across different conditions.\nWe further randomly generate synthetic noisy datasets with the same noise\ndistribution as that of Pseudo noise (referred as \"Randomized Noise\") to\nempirically show that i) learning is easier with feature-dependent label noise\ncompared to random noise, ii) irrespective of noise distribution, Pseudo noisy\ndatasets mimic feature-dependent label noise and iii) current training methods\nare not generalizable to feature-dependent label noise. Therefore, we believe\nthat Pseudo noisy datasets will be quite helpful to study and develop robust\ntraining methods.",
          "link": "http://arxiv.org/abs/2105.10796",
          "publishedOn": "2021-05-25T01:56:08.929Z",
          "wordCount": 652,
          "title": "Generation and Analysis of Feature-Dependent Pseudo Noise for Training Deep Neural Networks. (arXiv:2105.10796v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zogan_H/0/1/0/all/0/1\">Hamad Zogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_I/0/1/0/all/0/1\">Imran Razzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jameel_S/0/1/0/all/0/1\">Shoaib Jameel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guandong Xu</a>",
          "description": "Twitter is currently a popular online social media platform which allows\nusers to share their user-generated content. This publicly-generated user data\nis also crucial to healthcare technologies because the discovered patterns\nwould hugely benefit them in several ways. One of the applications is in\nautomatically discovering mental health problems, e.g., depression. Previous\nstudies to automatically detect a depressed user on online social media have\nlargely relied upon the user behaviour and their linguistic patterns including\nuser's social interactions. The downside is that these models are trained on\nseveral irrelevant content which might not be crucial towards detecting a\ndepressed user. Besides, these content have a negative impact on the overall\nefficiency and effectiveness of the model. To overcome the shortcomings in the\nexisting automatic depression detection methods, we propose a novel\ncomputational framework for automatic depression detection that initially\nselects relevant content through a hybrid extractive and abstractive\nsummarization strategy on the sequence of all user tweets leading to a more\nfine-grained and relevant content. The content then goes to our novel deep\nlearning framework comprising of a unified learning machinery comprising of\nConvolutional Neural Network (CNN) coupled with attention-enhanced Gated\nRecurrent Units (GRU) models leading to better empirical performance than\nexisting strong baselines.",
          "link": "http://arxiv.org/abs/2105.10878",
          "publishedOn": "2021-05-25T01:56:08.921Z",
          "wordCount": 651,
          "title": "DepressionNet: A Novel Summarization Boosted Deep Framework for Depression Detection on Social Media. (arXiv:2105.10878v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>",
          "description": "The advances in pre-trained models (e.g., BERT, XLNET and etc) have largely\nrevolutionized the predictive performance of various modern natural language\nprocessing tasks. This allows corporations to provide machine learning as a\nservice (MLaaS) by encapsulating fine-tuned BERT-based models as commercial\nAPIs. However, previous works have discovered a series of vulnerabilities in\nBERT- based APIs. For example, BERT-based APIs are vulnerable to both model\nextraction attack and adversarial example transferrability attack. However, due\nto the high capacity of BERT-based APIs, the fine-tuned model is easy to be\noverlearned, what kind of information can be leaked from the extracted model\nremains unknown and is lacking. To bridge this gap, in this work, we first\npresent an effective model extraction attack, where the adversary can\npractically steal a BERT-based API (the target/victim model) by only querying a\nlimited number of queries. We further develop an effective attribute inference\nattack to expose the sensitive attribute of the training data used by the\nBERT-based APIs. Our extensive experiments on benchmark datasets under various\nrealistic settings demonstrate the potential vulnerabilities of BERT-based\nAPIs.",
          "link": "http://arxiv.org/abs/2105.10909",
          "publishedOn": "2021-05-25T01:56:08.913Z",
          "wordCount": 629,
          "title": "Killing Two Birds with One Stone: Stealing Model and Inferring Attribute from BERT-based APIs. (arXiv:2105.10909v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yifan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1\">Christopher M. Poskitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Sudipta Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqi Chen</a>",
          "description": "The threats faced by cyber-physical systems (CPSs) in critical infrastructure\nhave motivated research into a multitude of attack detection mechanisms,\nincluding anomaly detectors based on neural network models. The effectiveness\nof anomaly detectors can be assessed by subjecting them to test suites of\nattacks, but less consideration has been given to adversarial attackers that\ncraft noise specifically designed to deceive them. While successfully applied\nin domains such as images and audio, adversarial attacks are much harder to\nimplement in CPSs due to the presence of other built-in defence mechanisms such\nas rule checkers(or invariant checkers). In this work, we present an\nadversarial attack that simultaneously evades the anomaly detectors and rule\ncheckers of a CPS. Inspired by existing gradient-based approaches, our\nadversarial attack crafts noise over the sensor and actuator values, then uses\na genetic algorithm to optimise the latter, ensuring that the neural network\nand the rule checking system are both deceived.We implemented our approach for\ntwo real-world critical infrastructure testbeds, successfully reducing the\nclassification accuracy of their detectors by over 50% on average, while\nsimultaneously avoiding detection by rule checkers. Finally, we explore whether\nthese attacks can be mitigated by training the detectors on adversarial\nsamples.",
          "link": "http://arxiv.org/abs/2105.10707",
          "publishedOn": "2021-05-25T01:56:08.906Z",
          "wordCount": 656,
          "title": "Adversarial Attacks and Mitigation for Anomaly Detectors of Cyber-Physical Systems. (arXiv:2105.10707v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10688",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_Y/0/1/0/all/0/1\">Yajie Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1\">Lingtao Wu</a>",
          "description": "In complex lane change (LC) scenarios, semantic interpretation and safety\nanalysis of dynamic interactive pattern are necessary for autonomous vehicles\nto make appropriate decisions. This study proposes an unsupervised learning\nframework that combines primitive-based interactive pattern recognition methods\nand risk analysis methods. The Hidden Markov Model with the Gaussian mixture\nmodel (GMM-HMM) approach is developed to decompose the LC scenarios into\nprimitives. Then the Dynamic Time Warping (DTW) distance based K-means\nclustering is applied to gather the primitives to 13 types of interactive\npatterns. Finally, this study considers two types of time-to-collision (TTC)\ninvolved in the LC process as indicators to analyze the risk of the interactive\npatterns and extract high-risk LC interactive patterns. The results obtained\nfrom The Highway Drone Dataset (highD) demonstrate that the identified LC\ninteractive patterns contain interpretable semantic information. This study\nexplores the spatiotemporal evolution law and risk formation mechanism of the\nLC interactive patterns and the findings are useful for comprehensively\nunderstanding the latent interactive patterns, improving the rationality and\nsafety of autonomous vehicle's decision-making.",
          "link": "http://arxiv.org/abs/2105.10688",
          "publishedOn": "2021-05-25T01:56:08.899Z",
          "wordCount": 617,
          "title": "V2V Spatiotemporal Interactive Pattern Recognition and Risk Analysis in Lane Changes. (arXiv:2105.10688v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1\">Hermawan Mulyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">Desmond Chan</a>",
          "description": "Climate change has largely impacted our daily lives. As one of its\nconsequences, we are experiencing more wildfires. In the year 2020, wildfires\nburned a record number of 8,888,297 acres in the US. To awaken people's\nattention to climate change, and to visualize the current risk of wildfires, We\ndeveloped RtFPS, \"Real-Time Fire Prediction System\". It provides a real-time\nprediction visualization of wildfire risk at specific locations base on a\nMachine Learning model. It also provides interactive map features that show the\nhistorical wildfire events with environmental info.",
          "link": "http://arxiv.org/abs/2105.10880",
          "publishedOn": "2021-05-25T01:56:08.875Z",
          "wordCount": 545,
          "title": "RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10590",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chan_J/0/1/0/all/0/1\">Jeffrey Chan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pacchiano_A/0/1/0/all/0/1\">Aldo Pacchiano</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tripuraneni_N/0/1/0/all/0/1\">Nilesh Tripuraneni</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1\">Yun S. Song</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter Bartlett</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Standard approaches to decision-making under uncertainty focus on sequential\nexploration of the space of decisions. However, \\textit{simultaneously}\nproposing a batch of decisions, which leverages available resources for\nparallel experimentation, has the potential to rapidly accelerate exploration.\nWe present a family of (parallel) contextual linear bandit algorithms, whose\nregret is nearly identical to their perfectly sequential counterparts -- given\naccess to the same total number of oracle queries -- up to a lower-order\n\"burn-in\" term that is dependent on the context-set geometry. We provide\nmatching information-theoretic lower bounds on parallel regret performance to\nestablish our algorithms are asymptotically optimal in the time horizon.\nFinally, we also present an empirical evaluation of these parallel algorithms\nin several domains, including materials discovery and biological sequence\ndesign problems, to demonstrate the utility of parallelized bandits in\npractical settings.",
          "link": "http://arxiv.org/abs/2105.10590",
          "publishedOn": "2021-05-25T01:56:08.867Z",
          "wordCount": 567,
          "title": "Parallelizing Contextual Linear Bandits. (arXiv:2105.10590v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalvit_A/0/1/0/all/0/1\">Anand Kalvit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeevi_A/0/1/0/all/0/1\">Assaf Zeevi</a>",
          "description": "We consider a stochastic bandit problem with countably many arms that belong\nto a finite set of types, each characterized by a unique mean reward. In\naddition, there is a fixed distribution over types which sets the proportion of\neach type in the population of arms. The decision maker is oblivious to the\ntype of any arm and to the aforementioned distribution over types, but\nperfectly knows the total number of types occurring in the population of arms.\nWe propose a fully adaptive online learning algorithm that achieves O(log n)\ndistribution-dependent expected cumulative regret after any number of plays n,\nand show that this order of regret is best possible. The analysis of our\nalgorithm relies on newly discovered concentration and convergence properties\nof optimism-based policies like UCB in finite-armed bandit problems with \"zero\ngap,\" which may be of independent interest.",
          "link": "http://arxiv.org/abs/2105.10721",
          "publishedOn": "2021-05-25T01:56:08.859Z",
          "wordCount": 560,
          "title": "From Finite to Countable-Armed Bandits. (arXiv:2105.10721v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tashman_M/0/1/0/all/0/1\">Michael Tashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">John Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiayi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fengdan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morsali_A/0/1/0/all/0/1\">Atefeh Morsali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winikor_L/0/1/0/all/0/1\">Lee Winikor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerami_R/0/1/0/all/0/1\">Rouzbeh Gerami</a>",
          "description": "Reinforcement learning (RL) is an effective technique for training\ndecision-making agents through interactions with their environment. The advent\nof deep learning has been associated with highly notable successes with\nsequential decision making problems - such as defeating some of the\nhighest-ranked human players at Go. In digital advertising, real-time bidding\n(RTB) is a common method of allocating advertising inventory through real-time\nauctions. Bidding strategies need to incorporate logic for dynamically\nadjusting parameters in order to deliver pre-assigned campaign goals. Here we\ndiscuss techniques toward using RL to train bidding agents. As a campaign\nmetric we particularly focused on viewability: the percentage of inventory\nwhich goes on to be viewed by an end user.\n\nThis paper is presented as a survey of techniques and experiments which we\ndeveloped through the course of this research. We discuss expanding our\ntraining data to include edge cases by training on simulated interactions. We\ndiscuss the experimental results comparing the performance of several promising\nRL algorithms, and an approach to hyperparameter optimization of an\nactor/critic training pipeline through Bayesian optimization. Finally, we\npresent live-traffic tests of some of our RL agents against a rule-based\nfeedback-control approach, demonstrating the potential for this method as well\nas areas for further improvement. This paper therefore presents an arrangement\nof our findings in this quickly developing field, and ways that it can be\napplied to an RTB use case.",
          "link": "http://arxiv.org/abs/2105.10587",
          "publishedOn": "2021-05-25T01:56:08.850Z",
          "wordCount": 673,
          "title": "Techniques Toward Optimizing Viewability in RTB Ad Campaigns Using Reinforcement Learning. (arXiv:2105.10587v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruichu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1\">Jie Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhifeng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Keli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>",
          "description": "Learning Granger causality among event types on multi-type event sequences is\nan important but challenging task. Existing methods, such as the Multivariate\nHawkes processes, mostly assumed that each sequence is independent and\nidentically distributed. However, in many real-world applications, it is\ncommonplace to encounter a topological network behind the event sequences such\nthat an event is excited or inhibited not only by its history but also by its\ntopological neighbors. Consequently, the failure in describing the topological\ndependency among the event sequences leads to the error detection of the causal\nstructure. By considering the Hawkes processes from the view of temporal\nconvolution, we propose a Topological Hawkes processes (THP) to draw a\nconnection between the graph convolution in topology domain and the temporal\nconvolution in time domains. We further propose a Granger causality learning\nmethod on THP in a likelihood framework. The proposed method is featured with\nthe graph convolution-based likelihood function of THP and a sparse\noptimization scheme with an Expectation-Maximization of the likelihood\nfunction. Theoretical analysis and experiments on both synthetic and real-world\ndata demonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2105.10884",
          "publishedOn": "2021-05-25T01:56:08.836Z",
          "wordCount": 616,
          "title": "THP: Topological Hawkes Processes for Learning Granger Causality on Event Sequences. (arXiv:2105.10884v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xingcheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Senzhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingyun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "Graph embedding is essential for graph mining tasks. With the prevalence of\ngraph data in real-world applications, many methods have been proposed in\nrecent years to learn high-quality graph embedding vectors various types of\ngraphs. However, most existing methods usually randomly select the negative\nsamples from the original graph to enhance the training data without\nconsidering the noise. In addition, most of these methods only focus on the\nexplicit graph structures and cannot fully capture complex semantics of edges\nsuch as various relationships or asymmetry. In order to address these issues,\nwe propose a robust and generalized framework for adversarial graph embedding\nbased on generative adversarial networks. Inspired by generative adversarial\nnetwork, we propose a robust and generalized framework for adversarial graph\nembedding, named AGE. AGE generates the fake neighbor nodes as the enhanced\nnegative samples from the implicit distribution, and enables the discriminator\nand generator to jointly learn each node's robust and generalized\nrepresentation. Based on this framework, we propose three models to handle\nthree types of graph data and derive the corresponding optimization algorithms,\ni.e., UG-AGE and DG-AGE for undirected and directed homogeneous graphs,\nrespectively, and HIN-AGE for heterogeneous information networks. Extensive\nexperiments show that our methods consistently and significantly outperform\nexisting state-of-the-art methods across multiple graph mining tasks, including\nlink prediction, node classification, and graph reconstruction.",
          "link": "http://arxiv.org/abs/2105.10651",
          "publishedOn": "2021-05-25T01:56:08.815Z",
          "wordCount": 660,
          "title": "A Robust and Generalized Framework for Adversarial Graph Embedding. (arXiv:2105.10651v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yulin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1\">Soung Chang Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>",
          "description": "Noisy neural networks (NoisyNNs) refer to the inference and training of NNs\nin the presence of noise. Noise is inherent in most communication and storage\nsystems; hence, NoisyNNs emerge in many new applications, including federated\nedge learning, where wireless devices collaboratively train a NN over a noisy\nwireless channel, or when NNs are implemented/stored in an analog storage\nmedium. This paper studies a fundamental problem of NoisyNNs: how to estimate\nthe uncontaminated NN weights from their noisy observations or manifestations.\nWhereas all prior works relied on the maximum likelihood (ML) estimation to\nmaximize the likelihood function of the estimated NN weights, this paper\ndemonstrates that the ML estimator is in general suboptimal. To overcome the\nsuboptimality of the conventional ML estimator, we put forth an\n$\\text{MMSE}_{pb}$ estimator to minimize a compensated mean squared error (MSE)\nwith a population compensator and a bias compensator. Our approach works well\nfor NoisyNNs arising in both 1) noisy inference, where noise is introduced only\nin the inference phase on the already-trained NN weights; and 2) noisy\ntraining, where noise is introduced over the course of training. Extensive\nexperiments on the CIFAR-10 and SST-2 datasets with different NN architectures\nverify the significant performance gains of the $\\text{MMSE}_{pb}$ estimator\nover the ML estimator when used to denoise the NoisyNN. For noisy inference,\nthe average gains are up to $156\\%$ for a noisy ResNet34 model and $14.7\\%$ for\na noisy BERT model; for noisy training, the average gains are up to $18.1$ dB\nfor a noisy ResNet18 model.",
          "link": "http://arxiv.org/abs/2105.10699",
          "publishedOn": "2021-05-25T01:56:08.808Z",
          "wordCount": 717,
          "title": "Denoising Noisy Neural Networks: A Bayesian Approach with Compensation. (arXiv:2105.10699v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11004",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sobczyk_A/0/1/0/all/0/1\">Aleksandros Sobczyk</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Gallopoulos_E/0/1/0/all/0/1\">Efstratios Gallopoulos</a> (2) ((1) IBM Research Europe, Zurich, Switzerland (2) Computer Engineering and Informatics Department, University of Patras, Greece)",
          "description": "We study algorithms for estimating the statistical leverage scores of\nrectangular dense or sparse matrices of arbitrary rank. Our approach is based\non combining rank revealing methods with compositions of dense and sparse\nrandomized dimensionality reduction transforms. We first develop a set of fast\nnovel algorithms for rank estimation, column subset selection and least squares\npreconditioning. We then describe the design and implementation of leverage\nscore estimators based on these primitives. These estimators are also effective\nfor rank deficient input, which is frequently the case in data analytics\napplications. We provide detailed complexity analyses for all algorithms as\nwell as meaningful approximation bounds and comparisons with the\nstate-of-the-art. We conduct extensive numerical experiments to evaluate our\nalgorithms and to illustrate their properties and performance using synthetic\nand real world data sets.",
          "link": "http://arxiv.org/abs/2105.11004",
          "publishedOn": "2021-05-25T01:56:08.798Z",
          "wordCount": 600,
          "title": "Estimating leverage scores via rank revealing methods and randomization. (arXiv:2105.11004v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10578",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1\">Cheng Ye</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1\">Rowan Swiers</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1\">Stephen Bonner</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1\">Ian Barrett</a>",
          "description": "The drug discovery and development process is a long and expensive one,\ncosting over 1 billion USD on average per drug and taking 10-15 years. To\nreduce the high levels of attrition throughout the process, there has been a\ngrowing interest in applying machine learning methodologies to various stages\nof drug discovery process in the recent decade, including at the earliest stage\n- identification of druggable disease genes. In this paper, we have developed a\nnew tensor factorisation model to predict potential drug targets (i.e.,genes or\nproteins) for diseases. We created a three dimensional tensor which consists of\n1,048 targets, 860 diseases and 230,011 evidence attributes and clinical\noutcomes connecting them, using data extracted from the Open Targets and\nPharmaProjects databases. We enriched the data with gene representations\nlearned from a drug discovery-oriented knowledge graph and applied our proposed\nmethod to predict the clinical outcomes for unseen target and dis-ease pairs.\nWe designed three evaluation strategies to measure the prediction performance\nand benchmarked several commonly used machine learning classifiers together\nwith matrix and tensor factorisation methods. The result shows that\nincorporating knowledge graph embeddings significantly improves the prediction\naccuracy and that training tensor factorisation alongside a dense neural\nnetwork outperforms other methods. In summary, our framework combines two\nactively studied machine learning approaches to disease target identification,\ntensor factorisation and knowledge graph representation learning, which could\nbe a promising avenue for further exploration in data-driven drug discovery.",
          "link": "http://arxiv.org/abs/2105.10578",
          "publishedOn": "2021-05-25T01:56:08.786Z",
          "wordCount": 676,
          "title": "Predicting Potential Drug Targets Using Tensor Factorisation and Knowledge Graph Embeddings. (arXiv:2105.10578v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_P/0/1/0/all/0/1\">Philip M. Long</a>",
          "description": "The Neural Tangent Kernel (NTK) is the wide-network limit of a kernel defined\nusing neural networks at initialization, whose embedding is the gradient of the\noutput of the network with respect to its parameters. We study the \"after\nkernel\", which is defined using the same embedding, except after training, for\nneural networks with standard architectures, on binary classification problems\nextracted from MNIST and CIFAR-10, trained using SGD in a standard way. Lyu and\nLi described a sense in which neural networks, under certain conditions, are\nequivalent to SVM with the after kernel. Our experiments are consistent with\nthis proposition under natural conditions. For networks with an architecure\nsimilar to VGG, the after kernel is more \"global\", in the sense that it is less\ninvariant to transformations of input images that disrupt the global structure\nof the image while leaving the local statistics largely intact. For fully\nconnected networks, the after kernel is less global in this sense. The after\nkernel tends to be more invariant to small shifts, rotations and zooms; data\naugmentation does not improve these invariances. The (finite approximation to\nthe) conjugate kernel, obtained using the last layer of hidden nodes,\nsometimes, but not always, provides a good approximation to the NTK and the\nafter kernel.",
          "link": "http://arxiv.org/abs/2105.10585",
          "publishedOn": "2021-05-25T01:56:08.767Z",
          "wordCount": 630,
          "title": "Properties of the After Kernel. (arXiv:2105.10585v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xingyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>",
          "description": "Robotic manipulation of cloth remains challenging for robotics due to the\ncomplex dynamics of the cloth, lack of a low-dimensional state representation,\nand self-occlusions. In contrast to previous model-based approaches that learn\na pixel-based dynamics model or a compressed latent vector dynamics, we propose\nto learn a particle-based dynamics model from a partial point cloud\nobservation. To overcome the challenges of partial observability, we infer\nwhich visible points are connected on the underlying cloth mesh. We then learn\na dynamics model over this visible connectivity graph. Compared to previous\nlearning-based approaches, our model poses strong inductive bias with its\nparticle based representation for learning the underlying cloth physics; it is\ninvariant to visual features; and the predictions can be more easily\nvisualized. We show that our method greatly outperforms previous\nstate-of-the-art model-based and model-free reinforcement learning methods in\nsimulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we\ndeploy the model trained in simulation on a Franka arm and show that the model\ncan successfully smooth different types of cloth from crumpled configurations.\nVideos can be found on our project website.",
          "link": "http://arxiv.org/abs/2105.10389",
          "publishedOn": "2021-05-24T05:08:42.352Z",
          "wordCount": 602,
          "title": "Learning Visible Connectivity Dynamics for Cloth Smoothing. (arXiv:2105.10389v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">David J. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1\">George Kesidis</a>",
          "description": "Deep Neural Networks (DNNs) have been shown vulnerable to adversarial\n(Test-Time Evasion (TTE)) attacks which, by making small changes to the input,\nalter the DNN's decision. We propose an attack detector based on\nclass-conditional Generative Adversarial Networks (GANs). We model the\ndistribution of clean data conditioned on the predicted class label by an\nAuxiliary Classifier GAN (ACGAN). Given a test sample and its predicted class,\nthree detection statistics are calculated using the ACGAN Generator and\nDiscriminator. Experiments on image classification datasets under different TTE\nattack methods show that our method outperforms state-of-the-art detection\nmethods. We also investigate the effectiveness of anomaly detection using\ndifferent DNN layers (input features or internal-layer features) and\ndemonstrate that anomalies are harder to detect using features closer to the\nDNN's output layer.",
          "link": "http://arxiv.org/abs/2105.10101",
          "publishedOn": "2021-05-24T05:08:42.346Z",
          "wordCount": 563,
          "title": "Anomaly Detection of Test-Time Evasion Attacks using Class-conditional Generative Adversarial Networks. (arXiv:2105.10101v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hufei Zhu</a>",
          "description": "The existing low-memory BLS implementation proposed recently avoids the need\nfor storing and inverting large matrices, to achieve efficient usage of\nmemories. However, the existing low-memory BLS implementation sacrifices the\ntesting accuracy as a price for efficient usage of memories, since it can no\nlonger obtain the generalized inverse or ridge solution for the output weights\nduring incremental learning, and it cannot work under the very small ridge\nparameter that is utilized in the original BLS. Accordingly, it is required to\ndevelop the low-memory BLS implementations, which can work under very small\nridge parameters and compute the generalized inverse or ridge solution for the\noutput weights in the process of incremental learning. In this paper, firstly\nwe propose the low-memory implementations for the recently proposed recursive\nand square-root BLS algorithms on added inputs and the recently proposed\nsquareroot BLS algorithm on added nodes, by simply processing a batch of inputs\nor nodes in each recursion. Since the recursive BLS implementation includes the\nrecursive updates of the inverse matrix that may introduce numerical\ninstabilities after a large number of iterations, and needs the extra\ncomputational load to decompose the inverse matrix into the Cholesky factor\nwhen cooperating with the proposed low-memory implementation of the square-root\nBLS algorithm on added nodes, we only improve the low-memory implementations of\nthe square-root BLS algorithms on added inputs and nodes, to propose the full\nlowmemory implementation of the square-root BLS algorithm. All the proposed\nlow-memory BLS implementations compute the ridge solution for the output\nweights in the process of incremental learning, and most of them can work under\nvery small ridge parameters.",
          "link": "http://arxiv.org/abs/2105.10424",
          "publishedOn": "2021-05-24T05:08:42.339Z",
          "wordCount": 694,
          "title": "Low-Memory Implementations of Ridge Solutions for Broad Learning System with Incremental Learning. (arXiv:2105.10424v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sigrist_F/0/1/0/all/0/1\">Fabio Sigrist</a>",
          "description": "Latent Gaussian models and boosting are widely used techniques in statistics\nand machine learning. Tree-boosting shows excellent predictive accuracy on many\ndata sets, but potential drawbacks are that it assumes conditional independence\nof samples, produces discontinuous predictions for, e.g., spatial data, and it\ncan have difficulty with high-cardinality categorical variables. Latent\nGaussian models, such as Gaussian process and grouped random effects models,\nare flexible prior models that allow for making probabilistic predictions.\nHowever, existing latent Gaussian models usually assume either a zero or a\nlinear prior mean function which can be an unrealistic assumption. This article\nintroduces a novel approach that combines boosting and latent Gaussian models\nin order to remedy the above-mentioned drawbacks and to leverage the advantages\nof both techniques. We obtain increased predictive accuracy compared to\nexisting approaches in both simulated and real-world data experiments.",
          "link": "http://arxiv.org/abs/2105.08966",
          "publishedOn": "2021-05-24T05:08:42.333Z",
          "wordCount": 580,
          "title": "Latent Gaussian Model Boosting. (arXiv:2105.08966v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1\">Emma Slade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1\">Francesco Farina</a>",
          "description": "In this draft paper, we introduce a novel architecture for graph networks\nwhich is equivariant to the Euclidean group in $n$-dimensions. The model is\ndesigned to work with graph networks in their general form and can be shown to\ninclude particular variants as special cases. Thanks to its equivariance\nproperties, we expect the proposed model to be more data efficient with respect\nto classical graph architectures and also intrinsically equipped with a better\ninductive bias. We defer investigating this matter to future work.",
          "link": "http://arxiv.org/abs/2103.14066",
          "publishedOn": "2021-05-24T05:08:42.327Z",
          "wordCount": 534,
          "title": "Beyond permutation equivariance in graph networks. (arXiv:2103.14066v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyusong Lee</a>",
          "description": "Text-to-image retrieval is an essential task in cross-modal information\nretrieval, i.e., retrieving relevant images from a large and unlabelled dataset\ngiven textual queries. In this paper, we propose VisualSparta, a novel\n(Visual-text Sparse Transformer Matching) model that shows significant\nimprovement in terms of both accuracy and efficiency. VisualSparta is capable\nof outperforming previous state-of-the-art scalable methods in MSCOCO and\nFlickr30K. We also show that it achieves substantial retrieving speed\nadvantages, i.e., for a 1 million image index, VisualSparta using CPU gets\n~391X speedup compared to CPU vector search and ~5.4X speedup compared to\nvector search with GPU acceleration. Experiments show that this speed advantage\neven gets bigger for larger datasets because VisualSparta can be efficiently\nimplemented as an inverted index. To the best of our knowledge, VisualSparta is\nthe first transformer-based text-to-image retrieval model that can achieve\nreal-time searching for large-scale datasets, with significant accuracy\nimprovement compared to previous state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2101.00265",
          "publishedOn": "2021-05-24T05:08:42.310Z",
          "wordCount": 629,
          "title": "VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words. (arXiv:2101.00265v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>",
          "description": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP",
          "link": "http://arxiv.org/abs/2105.03075",
          "publishedOn": "2021-05-24T05:08:42.075Z",
          "wordCount": 634,
          "title": "A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seyedi_S/0/1/0/all/0/1\">Salman Seyedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemati_S/0/1/0/all/0/1\">Shamim Nemati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1\">Gari D. Clifford</a>",
          "description": "The increasing complexity of algorithms for analyzing medical data, including\nde-identification tasks, raises the possibility that complex algorithms are\nlearning not just the general representation of the problem, but specifics of\ngiven individuals within the data. Modern legal frameworks specifically\nprohibit the intentional or accidental distribution of patient data, but have\nnot addressed this potential avenue for leakage of such protected health\ninformation. Modern deep learning algorithms have the highest potential of such\nleakage due to complexity of the models. Recent research in the field has\nhighlighted such issues in non-medical data, but all analysis is likely to be\ndata and algorithm specific. We, therefore, chose to analyze a state-of-the-art\nfree-text de-identification algorithm based on LSTM (Long Short-Term Memory)\nand its potential in encoding any individual in the training set. Using the\ni2b2 Challenge Data, we trained, then analyzed the model to assess whether the\noutput of the LSTM, before the compression layer of the classifier, could be\nused to estimate the membership of the training data. Furthermore, we used\ndifferent attacks including membership inference attack method to attack the\nmodel. Results indicate that the attacks could not identify whether members of\nthe training data were distinguishable from non-members based on the model\noutput. This indicates that the model does not provide any strong evidence into\nthe identification of the individuals in the training data set and there is not\nyet empirical evidence it is unsafe to distribute the model for general use.",
          "link": "http://arxiv.org/abs/2101.12099",
          "publishedOn": "2021-05-24T05:08:42.050Z",
          "wordCount": 716,
          "title": "An Analysis Of Protected Health Information Leakage In Deep-Learning Based De-Identification Algorithms. (arXiv:2101.12099v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>",
          "description": "Conventional federated learning directly averages model weights, which is\nonly possible for collaboration between models with homogeneous architectures.\nSharing prediction instead of weight removes this obstacle and eliminates the\nrisk of white-box inference attacks in conventional federated learning.\nHowever, the predictions from local models are sensitive and would leak\ntraining data privacy to the public. To address this issue, one naive approach\nis adding the differentially private random noise to the predictions, which\nhowever brings a substantial trade-off between privacy budget and model\nperformance. In this paper, we propose a novel framework called FEDMD-NFDP,\nwhich applies a Noise-Free Differential Privacy (NFDP) mechanism into a\nfederated model distillation framework. Our extensive experimental results on\nvarious datasets validate that FEDMD-NFDP can deliver not only comparable\nutility and communication efficiency but also provide a noise-free differential\nprivacy guarantee. We also demonstrate the feasibility of our FEDMD-NFDP by\nconsidering both IID and non-IID setting, heterogeneous model architectures,\nand unlabelled public datasets from a different distribution.",
          "link": "http://arxiv.org/abs/2009.05537",
          "publishedOn": "2021-05-24T05:08:42.031Z",
          "wordCount": 621,
          "title": "Federated Model Distillation with Noise-Free Differential Privacy. (arXiv:2009.05537v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03625",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_Z/0/1/0/all/0/1\">Zhishun Wang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhang_K/0/1/0/all/0/1\">Kaixin Zhang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixi Zhao</a>",
          "description": "It is a difficult task for both professional investors and individual traders\ncontinuously making profit in stock market. With the development of computer\nscience and deep reinforcement learning, Buy\\&Hold (B\\&H) has been oversteped\nby many artificial intelligence trading algorithms. However, the information\nand process are not enough, which limit the performance of reinforcement\nlearning algorithms. Thus, we propose a parallel-network continuous\nquantitative trading model with GARCH and PPO to enrich the basical deep\nreinforcement learning model, where the deep learning parallel network layers\ndeal with 3 different frequencies data (including GARCH information) and\nproximal policy optimization (PPO) algorithm interacts actions and rewards with\nstock trading environment. Experiments in 5 stocks from Chinese stock market\nshow our method achieves more extra profit comparing with basical reinforcement\nlearning methods and bench models.",
          "link": "http://arxiv.org/abs/2105.03625",
          "publishedOn": "2021-05-24T05:08:42.019Z",
          "wordCount": 586,
          "title": "A parallel-network continuous quantitative trading model with GARCH and PPO. (arXiv:2105.03625v2 [q-fin.TR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1\">Andrea Apicella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1\">Francesco Isgr&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollastro_A/0/1/0/all/0/1\">Andrea Pollastro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1\">Roberto Prevete</a>",
          "description": "Over the last few years, we have seen increasing data generated from\nnon-Euclidean domains, which are usually represented as graphs with complex\nrelationships, and Graph Neural Networks (GNN) have gained a high interest\nbecause of their potential in processing graph-structured data. In particular,\nthere is a strong interest in exploring the possibilities in performing\nconvolution on graphs using an extension of the GNN architecture, generally\nreferred to as Graph Convolutional Neural Networks (GCNN). Convolution on\ngraphs has been achieved mainly in two forms: spectral and spatial\nconvolutions. Due to the higher flexibility in exploring and exploiting the\ngraph structure of data, recently, there is an increasing interest in\ninvestigating the possibilities that the spatial approach can offer. The idea\nof finding a way to adapt the network behaviour to the inputs they process to\nmaximize the total performances has aroused much interest in the neural\nnetworks literature over the years. This paper presents a novel method to adapt\nthe behaviour of a GCNN to the input proposing two ways to perform spatial\nconvolution on graphs using input-based filters which are dynamically\ngenerated. Our model also investigates the problem of discovering and refining\nrelations among nodes. The experimental assessment confirms the capabilities of\nthe proposed approach, which achieves satisfying results using simple\narchitectures with a low number of filters.",
          "link": "http://arxiv.org/abs/2105.10377",
          "publishedOn": "2021-05-24T05:08:42.006Z",
          "wordCount": 671,
          "title": "Dynamic Filters in Graph Convolutional Neural Networks. (arXiv:2105.10377v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08059",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1\">Yilmaz Korkmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1\">Salman UH Dar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1\">Muzaffer &#xd6;zbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>",
          "description": "Supervised deep learning has swiftly become a workhorse for accelerated MRI\nin recent years, offering state-of-the-art performance in image reconstruction\nfrom undersampled acquisitions. Training deep supervised models requires large\ndatasets of undersampled and fully-sampled acquisitions typically from a\nmatching set of subjects. Given scarce access to large medical datasets, this\nlimitation has sparked interest in unsupervised methods that reduce reliance on\nfully-sampled ground-truth data. A common framework is based on the deep image\nprior, where network-driven regularization is enforced directly during\ninference on undersampled acquisitions. Yet, canonical convolutional\narchitectures are suboptimal in capturing long-range relationships, and\nrandomly initialized networks may hamper convergence. To address these\nlimitations, here we introduce a novel unsupervised MRI reconstruction method\nbased on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a\ndeep adversarial network with cross-attention transformer blocks to map noise\nand latent variables onto MR images. This unconditional network learns a\nhigh-quality MRI prior in a self-supervised encoding task. A zero-shot\nreconstruction is performed on undersampled test data, where inference is\nperformed by optimizing network parameters, latent and noise variables to\nensure maximal consistency to multi-coil MRI data. Comprehensive experiments on\nbrain MRI datasets clearly demonstrate the superior performance of SLATER\nagainst several state-of-the-art unsupervised methods.",
          "link": "http://arxiv.org/abs/2105.08059",
          "publishedOn": "2021-05-24T05:08:41.964Z",
          "wordCount": 666,
          "title": "Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers. (arXiv:2105.08059v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10100",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Muhan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jiajia Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_C/0/1/0/all/0/1\">Chao-Kai Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_S/0/1/0/all/0/1\">Shi Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Geoffrey Ye Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_A/0/1/0/all/0/1\">Ang Yang</a>",
          "description": "Massive multiple-input multiple-output can obtain more performance gain by\nexploiting the downlink channel state information (CSI) at the base station\n(BS). Therefore, studying CSI feedback with limited communication resources in\nfrequency-division duplexing systems is of great importance. Recently, deep\nlearning (DL)-based CSI feedback has shown considerable potential. However, the\nexisting DL-based explicit feedback schemes are difficult to deploy because\ncurrent fifth-generation mobile communication protocols and systems are\ndesigned based on an implicit feedback mechanism. In this paper, we propose a\nDL-based implicit feedback architecture to inherit the low-overhead\ncharacteristic, which uses neural networks (NNs) to replace the precoding\nmatrix indicator (PMI) encoding and decoding modules. By using environment\ninformation, the NNs can achieve a more refined mapping between the precoding\nmatrix and the PMI compared with codebooks. The correlation between subbands is\nalso used to further improve the feedback performance. Simulation results show\nthat, for a single resource block (RB), the proposed architecture can save\n25.0% and 40.0% of overhead compared with Type I codebook under two antenna\nconfigurations, respectively. For a wideband system with 52 RBs, overhead can\nbe saved by 30.7% and 48.0% compared with Type II codebook when ignoring and\nconsidering extracting subband correlation, respectively.",
          "link": "http://arxiv.org/abs/2105.10100",
          "publishedOn": "2021-05-24T05:08:41.945Z",
          "wordCount": 668,
          "title": "Deep Learning-based Implicit CSI Feedback in Massive MIMO. (arXiv:2105.10100v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Ming Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>",
          "description": "This work studies the statistical limits of uniform convergence for offline\npolicy evaluation (OPE) problems with model-based methods (for finite horizon\nMDP) and provides a unified view towards optimal learning for several\nwell-motivated offline tasks. Uniform OPE\n$\\sup_\\Pi|Q^\\pi-\\hat{Q}^\\pi|<\\epsilon$ (initiated by \\citet{yin2021near}) is a\nstronger measure than the point-wise (fixed policy) OPE and ensures offline\npolicy learning when $\\Pi$ contains all policies (global policy class). In this\npaper, we establish an $\\Omega(H^2 S/d_m\\epsilon^2)$ lower bound (over\nmodel-based family) for the global uniform OPE, where $d_m$ is the minimal\nstate-action probability induced by the behavior policy. Next, our main result\nestablishes an episode complexity of $\\tilde{O}(H^2/d_m\\epsilon^2)$ for\n\\emph{local} uniform convergence that applies to all \\emph{near-empirically\noptimal} policies for the MDPs with \\emph{stationary} transition. This result\nimplies the optimal sample complexity for offline learning and separates the\nlocal uniform OPE from the global case due to the extra $S$ factor.\nParamountly, the model-based method combining with our new analysis technique\n(singleton absorbing MDP) can be adapted to the new settings: offline\ntask-agnostic and the offline reward-free with optimal complexity\n$\\tilde{O}(H^2\\log(K)/d_m\\epsilon^2)$ ($K$ is the number of tasks) and\n$\\tilde{O}(H^2S/d_m\\epsilon^2)$ respectively, which provides a unified\nframework for simultaneously solving different offline RL problems.",
          "link": "http://arxiv.org/abs/2105.06029",
          "publishedOn": "2021-05-24T05:08:41.938Z",
          "wordCount": 657,
          "title": "Optimal Uniform OPE and Model-based Offline Reinforcement Learning in Time-Homogeneous, Reward-Free and Task-Agnostic Settings. (arXiv:2105.06029v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Rui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>",
          "description": "We present MedMNIST, a collection of 10 pre-processed medical open datasets.\nMedMNIST is standardized to perform classification tasks on lightweight 28x28\nimages, which requires no background knowledge. Covering the primary data\nmodalities in medical image analysis, it is diverse on data scale (from 100 to\n100,000) and tasks (binary/multi-class, ordinal regression and multi-label).\nMedMNIST could be used for educational purpose, rapid prototyping, multi-modal\nmachine learning or AutoML in medical image analysis. Moreover, MedMNIST\nClassification Decathlon is designed to benchmark AutoML algorithms on all 10\ndatasets; We have compared several baseline methods, including open-source or\ncommercial AutoML tools. The datasets, evaluation code and baseline methods for\nMedMNIST are publicly available at https://medmnist.github.io/.",
          "link": "http://arxiv.org/abs/2010.14925",
          "publishedOn": "2021-05-24T05:08:41.932Z",
          "wordCount": 612,
          "title": "MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. (arXiv:2010.14925v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10315",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>",
          "description": "Negative control is a strategy for learning the causal relationship between\ntreatment and outcome in the presence of unmeasured confounding. The treatment\neffect can nonetheless be identified if two auxiliary variables are available:\na negative control treatment (which has no effect on the actual outcome), and a\nnegative control outcome (which is not affected by the actual treatment). These\nauxiliary variables can also be viewed as proxies for a traditional set of\ncontrol variables, and they bear resemblance to instrumental variables. I\npropose a family of algorithms based on kernel ridge regression for learning\nnonparametric treatment effects with negative controls. Examples include dose\nresponse curves, dose response curves with distribution shift, and\nheterogeneous treatment effects. Data may be discrete or continuous, and low,\nhigh, or infinite dimensional. I prove uniform consistency and provide finite\nsample rates of convergence. I estimate the dose response curve of cigarette\nsmoking on infant birth weight adjusting for unobserved confounding due to\nhousehold income, using a data set of singleton births in the state of\nPennsylvania between 1989 and 1991.",
          "link": "http://arxiv.org/abs/2012.10315",
          "publishedOn": "2021-05-24T05:08:41.925Z",
          "wordCount": 632,
          "title": "Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments. (arXiv:2012.10315v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11128",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gong_S/0/1/0/all/0/1\">Shu Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_K/0/1/0/all/0/1\">Kaibo Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cichocki_A/0/1/0/all/0/1\">Andrzej Cichocki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Junhua Li</a>",
          "description": "Deep learning has achieved excellent performance in a wide range of domains,\nespecially in speech recognition and computer vision. Relatively less work has\nbeen done for EEG, but there is still significant progress attained in the last\ndecade. Due to the lack of a comprehensive and topic widely covered survey for\ndeep learning in EEG, we attempt to summarize recent progress to provide an\noverview, as well as perspectives for future developments. We first briefly\nmention the artifacts removal for EEG signal and then introduce deep learning\nmodels that have been utilized in EEG processing and classification.\nSubsequently, the applications of deep learning in EEG are reviewed by\ncategorizing them into groups such as brain-computer interface, disease\ndetection, and emotion recognition. They are followed by the discussion, in\nwhich the pros and cons of deep learning are presented and future directions\nand challenges for deep learning in EEG are proposed. We hope that this paper\ncould serve as a summary of past work for deep learning in EEG and the\nbeginning of further developments and achievements of EEG studies based on deep\nlearning.",
          "link": "http://arxiv.org/abs/2011.11128",
          "publishedOn": "2021-05-24T05:08:41.918Z",
          "wordCount": 677,
          "title": "Deep Learning in EEG: Advance of the Last Ten-Year Critical Period. (arXiv:2011.11128v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ennen_P/0/1/0/all/0/1\">Philipp Ennen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozbay_A/0/1/0/all/0/1\">Ali Girayhan Ozbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Insalata_F/0/1/0/all/0/1\">Ferdinando Insalata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalali_S/0/1/0/all/0/1\">Sepehr Jalali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1\">Da-shan Shiu</a>",
          "description": "In a dialogue system pipeline, a natural language generation (NLG) unit\nconverts the dialogue direction and content to a corresponding natural language\nrealization. A recent trend for dialogue systems is to first pre-train on large\ndatasets and then fine-tune in a supervised manner using datasets annotated\nwith application-specific features. Though novel behaviours can be learned from\ncustom annotation, the required effort severely bounds the quantity of the\ntraining set, and the application-specific nature limits the reuse. In light of\nthe recent success of data-driven approaches, we propose the novel future\nbridging NLG (FBNLG) concept for dialogue systems and simulators. The critical\nstep is for an FBNLG to accept a future user or system utterance to bridge the\npresent context towards. Future bridging enables self supervised training over\nannotation-free datasets, decoupled the training of NLG from the rest of the\nsystem. An FBNLG, pre-trained with massive datasets, is expected to apply in\nclassical or new dialogue scenarios with minimal adaptation effort. We evaluate\na prototype FBNLG to show that future bridging can be a viable approach to a\nuniversal few-shot NLG for task-oriented and chit-chat dialogues.",
          "link": "http://arxiv.org/abs/2105.10267",
          "publishedOn": "2021-05-24T05:08:41.900Z",
          "wordCount": 638,
          "title": "Towards a Universal NLG for Dialogue Systems and Simulators with Future Bridging. (arXiv:2105.10267v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2008.00358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1\">Benjamin Moseley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruhs_K/0/1/0/all/0/1\">Kirk Pruhs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samadian_A/0/1/0/all/0/1\">Alireza Samadian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyan Wang</a>",
          "description": "This paper gives a k-means approximation algorithm that is efficient in the\nrelational algorithms model. This is an algorithm that operates directly on a\nrelational database without performing a join to convert it to a matrix whose\nrows represent the data points. The running time is potentially exponentially\nsmaller than $N$, the number of data points to be clustered that the relational\ndatabase represents.\n\nFew relational algorithms are known and this paper offers techniques for\ndesigning relational algorithms as well as characterizing their limitations. We\nshow that given two data points as cluster centers, if we cluster points\naccording to their closest centers, it is NP-Hard to approximate the number of\npoints in the clusters on a general relational input. This is trivial for\nconventional data inputs and this result exemplifies that standard algorithmic\ntechniques may not be directly applied when designing an efficient relational\nalgorithm. This paper then introduces a new method that leverages rejection\nsampling and the $k$-means++ algorithm to construct an O(1)-approximate k-means\nsolution.",
          "link": "http://arxiv.org/abs/2008.00358",
          "publishedOn": "2021-05-24T05:08:41.893Z",
          "wordCount": 624,
          "title": "Relational Algorithms for k-means Clustering. (arXiv:2008.00358v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiansheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wentai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Ligang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Keqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zomaya_A/0/1/0/all/0/1\">Albert Y.Zomaya</a>",
          "description": "The issue of potential privacy leakage during centralized AI's model training\nhas drawn intensive concern from the public. A Parallel and Distributed\nComputing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new\nparadigm to cope with the privacy issue by allowing clients to perform model\ntraining locally, without the necessity to upload their personal sensitive\ndata. In FL, the number of clients could be sufficiently large, but the\nbandwidth available for model distribution and re-upload is quite limited,\nmaking it sensible to only involve part of the volunteers to participate in the\ntraining process. The client selection policy is critical to an FL process in\nterms of training efficiency, the final model's quality as well as fairness. In\nthis paper, we will model the fairness guaranteed client selection as a\nLyapunov optimization problem and then a C2MAB-based method is proposed for\nestimation of the model exchange time between each client and the server, based\non which we design a fairness guaranteed algorithm termed RBCS-F for\nproblem-solving. The regret of RBCS-F is strictly bounded by a finite constant,\njustifying its theoretical feasibility. Barring the theoretical results, more\nempirical data can be derived from our real training experiments on public\ndatasets.",
          "link": "http://arxiv.org/abs/2011.01783",
          "publishedOn": "2021-05-24T05:08:41.866Z",
          "wordCount": 699,
          "title": "An Efficiency-boosting Client Selection Scheme for Federated Learning with Fairness Guarantee. (arXiv:2011.01783v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuening Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengnan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Denghui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Time-series representation learning is a fundamental task for time-series\nanalysis. While significant progress has been made to achieve accurate\nrepresentations for downstream applications, the learned representations often\nlack interpretability and do not expose semantic meanings. Different from\nprevious efforts on the entangled feature space, we aim to extract the\nsemantic-rich temporal correlations in the latent interpretable factorized\nrepresentation of the data. Motivated by the success of disentangled\nrepresentation learning in computer vision, we study the possibility of\nlearning semantic-rich time-series representations, which remains unexplored\ndue to three main challenges: 1) sequential data structure introduces complex\ntemporal correlations and makes the latent representations hard to interpret,\n2) sequential models suffer from KL vanishing problem, and 3) interpretable\nsemantic concepts for time-series often rely on multiple factors instead of\nindividuals. To bridge the gap, we propose Disentangle Time Series (DTS), a\nnovel disentanglement enhancement framework for sequential data. Specifically,\nto generate hierarchical semantic concepts as the interpretable and\ndisentangled representation of time-series, DTS introduces multi-level\ndisentanglement strategies by covering both individual latent factors and group\nsemantic segments. We further theoretically show how to alleviate the KL\nvanishing problem: DTS introduces a mutual information maximization term, while\npreserving a heavier penalty on the total correlation and the dimension-wise KL\nto keep the disentanglement property. Experimental results on various\nreal-world benchmark datasets demonstrate that the representations learned by\nDTS achieve superior performance in downstream applications, with high\ninterpretability of semantic concepts.",
          "link": "http://arxiv.org/abs/2105.08179",
          "publishedOn": "2021-05-24T05:08:41.847Z",
          "wordCount": 683,
          "title": "Learning Disentangled Representations for Time Series. (arXiv:2105.08179v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1\">Kanchana Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "Vision transformers (ViT) have demonstrated impressive performance across\nvarious machine vision problems. These models are based on multi-head\nself-attention mechanisms that can flexibly attend to a sequence of image\npatches to encode contextual cues. An important question is how such\nflexibility in attending image-wide context conditioned on a given patch can\nfacilitate handling nuisances in natural images e.g., severe occlusions, domain\nshifts, spatial permutations, adversarial and natural perturbations. We\nsystematically study this question via an extensive set of experiments\nencompassing three ViT families and comparisons with a high-performing\nconvolutional neural network (CNN). We show and analyze the following\nintriguing properties of ViT: (a) Transformers are highly robust to severe\nocclusions, perturbations and domain shifts, e.g., retain as high as 60% top-1\naccuracy on ImageNet even after randomly occluding 80% of the image content.\n(b) The robust performance to occlusions is not due to a bias towards local\ntextures, and ViTs are significantly less biased towards textures compared to\nCNNs. When properly trained to encode shape-based features, ViTs demonstrate\nshape recognition capability comparable to that of human visual system,\npreviously unmatched in the literature. (c) Using ViTs to encode shape\nrepresentation leads to an interesting consequence of accurate semantic\nsegmentation without pixel-level supervision. (d) Off-the-shelf features from a\nsingle ViT model can be combined to create a feature ensemble, leading to high\naccuracy rates across a range of classification datasets in both traditional\nand few-shot learning paradigms. We show effective features of ViTs are due to\nflexible and dynamic receptive fields possible via the self-attention\nmechanism.",
          "link": "http://arxiv.org/abs/2105.10497",
          "publishedOn": "2021-05-24T05:08:41.834Z",
          "wordCount": 698,
          "title": "Intriguing Properties of Vision Transformers. (arXiv:2105.10497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mokhtarian_E/0/1/0/all/0/1\">Ehsan Mokhtarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_S/0/1/0/all/0/1\">Sina Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1\">AmirEmad Ghassami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1\">Negar Kiyavash</a>",
          "description": "Constraint-based methods are one of the main approaches for causal structure\nlearning that are particularly valued as they are asymptotically guaranteed to\nfind a structure that is Markov equivalent to the causal graph of the system.\nOn the other hand, they may require an exponentially large number of\nconditional independence (CI) tests in the number of variables of the system.\nIn this paper, we propose a novel recursive constraint-based method for causal\nstructure learning that significantly reduces the required number of CI tests\ncompared to the existing literature. The idea of the proposed approach is to\nuse Markov boundary information to identify a specific variable that can be\nremoved from the set of variables without affecting the statistical\ndependencies among the other variables. Having identified such a variable, we\ndiscover its neighborhood, remove that variable from the set of variables, and\nrecursively learn the causal structure over the remaining variables. We further\nprovide a lower bound on the number of CI tests required by any\nconstraint-based method. Comparing this lower bound to our achievable bound\ndemonstrates the efficiency of the proposed approach. Our experimental results\nshow that the proposed algorithm outperforms state-of-the-art both on synthetic\nand real-world structures.",
          "link": "http://arxiv.org/abs/2010.04992",
          "publishedOn": "2021-05-24T05:08:41.827Z",
          "wordCount": 676,
          "title": "A Recursive Markov Boundary-Based Approach to Causal Structure Learning. (arXiv:2010.04992v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.03659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1\">John Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitski_O/0/1/0/all/0/1\">Osvald Nitski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bader_G/0/1/0/all/0/1\">Gary Bader</a>",
          "description": "Sentence embeddings are an important component of many natural language\nprocessing (NLP) systems. Like word embeddings, sentence embeddings are\ntypically learned on large text corpora and then transferred to various\ndownstream tasks, such as clustering and retrieval. Unlike word embeddings, the\nhighest performing solutions for learning sentence embeddings require labelled\ndata, limiting their usefulness to languages and domains where labelled data is\nabundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for\nUnsupervised Textual Representations. Inspired by recent advances in deep\nmetric learning (DML), we carefully design a self-supervised objective for\nlearning universal sentence embeddings that does not require labelled training\ndata. When used to extend the pretraining of transformer-based language models,\nour approach closes the performance gap between unsupervised and supervised\npretraining for universal sentence encoders. Importantly, our experiments\nsuggest that the quality of the learned embeddings scale with both the number\nof trainable parameters and the amount of unlabelled training data, making\nfurther improvements straightforward. Our code and pretrained models are\npublicly available and can be easily adapted to new domains or used to embed\nunseen text.",
          "link": "http://arxiv.org/abs/2006.03659",
          "publishedOn": "2021-05-24T05:08:41.820Z",
          "wordCount": 649,
          "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. (arXiv:2006.03659v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jefferson_B/0/1/0/all/0/1\">Brett Jefferson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joslyn_C/0/1/0/all/0/1\">Cliff Joslyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purvine_E/0/1/0/all/0/1\">Emilie Purvine</a>",
          "description": "As data grows in size and complexity, finding frameworks which aid in\ninterpretation and analysis has become critical. This is particularly true when\ndata comes from complex systems where extensive structure is available, but\nmust be drawn from peripheral sources. In this paper we argue that in such\nsituations, sheaves can provide a natural framework to analyze how well a\nstatistical model fits at the local level (that is, on subsets of related\ndatapoints) vs the global level (on all the data). The sheaf-based approach\nthat we propose is suitably general enough to be useful in a range of\napplications, from analyzing sensor networks to understanding the feature space\nof a deep learning model.",
          "link": "http://arxiv.org/abs/2105.10414",
          "publishedOn": "2021-05-24T05:08:41.687Z",
          "wordCount": 561,
          "title": "Sheaves as a Framework for Understanding and Interpreting Model Fit. (arXiv:2105.10414v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10488",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1\">Stephen Bonner</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1\">Ian P Barrett</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1\">Cheng Ye</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1\">Rowan Swiers</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Engkvist_O/0/1/0/all/0/1\">Ola Engkvist</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hamilton_W/0/1/0/all/0/1\">William L Hamilton</a>",
          "description": "Knowledge Graphs (KG) and associated Knowledge Graph Embedding (KGE) models\nhave recently begun to be explored in the context of drug discovery and have\nthe potential to assist in key challenges such as target identification. In the\ndrug discovery domain, KGs can be employed as part of a process which can\nresult in lab-based experiments being performed, or impact on other decisions,\nincurring significant time and financial costs and most importantly, ultimately\ninfluencing patient healthcare. For KGE models to have impact in this domain, a\nbetter understanding of not only of performance, but also the various factors\nwhich determine it, is required.\n\nIn this study we investigate, over the course of many thousands of\nexperiments, the predictive performance of five KGE models on two public drug\ndiscovery-oriented KGs. Our goal is not to focus on the best overall model or\nconfiguration, instead we take a deeper look at how performance can be affected\nby changes in the training setup, choice of hyperparameters, model parameter\ninitialisation seed and different splits of the datasets. Our results highlight\nthat these factors have significant impact on performance and can even affect\nthe ranking of models. Indeed these factors should be reported along with model\narchitectures to ensure complete reproducibility and fair comparisons of future\nwork, and we argue this is critical for the acceptance of use, and impact of\nKGEs in a biomedical setting. To aid reproducibility of our own work, we\nrelease all experimentation code.",
          "link": "http://arxiv.org/abs/2105.10488",
          "publishedOn": "2021-05-24T05:08:41.669Z",
          "wordCount": 682,
          "title": "Understanding the Performance of Knowledge Graph Embeddings in Drug Discovery. (arXiv:2105.10488v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fraboni_Y/0/1/0/all/0/1\">Yann Fraboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Richard Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kameni_L/0/1/0/all/0/1\">Laetitia Kameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>",
          "description": "This work addresses the problem of optimizing communications between server\nand clients in federated learning (FL). Current sampling approaches in FL are\neither biased, or non optimal in terms of server-clients communications and\ntraining stability. To overcome this issue, we introduce \\textit{clustered\nsampling} for clients selection. We prove that clustered sampling leads to\nbetter clients representatitivity and to reduced variance of the clients\nstochastic aggregation weights in FL. Compatibly with our theory, we provide\ntwo different clustering approaches enabling clients aggregation based on 1)\nsample size, and 2) models similarity. Through a series of experiments in\nnon-iid and unbalanced scenarios, we demonstrate that model aggregation through\nclustered sampling consistently leads to better training convergence and\nvariability when compared to standard sampling approaches. Our approach does\nnot require any additional operation on the clients side, and can be seamlessly\nintegrated in standard FL implementations. Finally, clustered sampling is\ncompatible with existing methods and technologies for privacy enhancement, and\nfor communication reduction through model compression.",
          "link": "http://arxiv.org/abs/2105.05883",
          "publishedOn": "2021-05-24T05:08:41.595Z",
          "wordCount": 616,
          "title": "Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning. (arXiv:2105.05883v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1\">Huixuan Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1\">Qinfen Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Hong Xia</a>",
          "description": "Graph Convolutional Networks (GCNs) and subsequent variants have been\nproposed to solve tasks on graphs, especially node classification tasks. In the\nliterature, however, most tricks or techniques are either briefly mentioned as\nimplementation details or only visible in source code. In this paper, we first\nsummarize some existing effective tricks used in GCNs mini-batch training.\nBased on this, two novel tricks named GCN_res Framework and Embedding Usage are\nproposed by leveraging residual network and pre-trained embedding to improve\nbaseline's test accuracy in different datasets. Experiments on Open Graph\nBenchmark (OGB) show that, by combining these techniques, the test accuracy of\nvarious GCNs increases by 1.21%~2.84%. We open source our implementation at\nhttps://github.com/ytchx1999/PyG-OGB-Tricks.",
          "link": "http://arxiv.org/abs/2105.08330",
          "publishedOn": "2021-05-24T05:08:41.530Z",
          "wordCount": 579,
          "title": "Residual Network and Embedding Usage: New Tricks of Node Classification with Graph Convolutional Networks. (arXiv:2105.08330v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09261",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+dAndrimont_R/0/1/0/all/0/1\">Rapha&#xeb;l d&#x27;Andrimont</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Verhegghen_A/0/1/0/all/0/1\">Astrid Verhegghen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lemoine_G/0/1/0/all/0/1\">Guido Lemoine</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kempeneers_P/0/1/0/all/0/1\">Pieter Kempeneers</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meroni_M/0/1/0/all/0/1\">Michele Meroni</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Velde_M/0/1/0/all/0/1\">Marijn van der Velde</a>",
          "description": "Detailed parcel-level crop type mapping for the whole European Union (EU) is\nnecessary for the evaluation of agricultural policies. The Copernicus program,\nand Sentinel-1 (S1) in particular, offers the opportunity to monitor\nagricultural land at a continental scale and in a timely manner. However, so\nfar the potential of S1 has not been explored at such a scale. Capitalizing on\nthe unique LUCAS 2018 Copernicus in-situ survey, we present the first\ncontinental crop type map at 10-m spatial resolution for the EU based on S1A\nand S1B Synthetic Aperture Radar observations for the year 2018. Random forest\nclassification algorithms are tuned to detect 19 different crop types. We\nassess the accuracy of this EU crop map with three approaches. First, the\naccuracy is assessed with independent LUCAS core in-situ observations over the\ncontinent. Second, an accuracy assessment is done specifically for main crop\ntypes from farmers declarations from 6 EU member countries or regions totaling\n>3M parcels and 8.21 Mha. Finally, the crop areas derived by classification are\ncompared to the subnational (NUTS 2) area statistics reported by Eurostat. The\noverall accuracy for the map is reported as 80.3% when grouping main crop\nclasses and 76% when considering all 19 crop type classes separately. Highest\naccuracies are obtained for rape and turnip rape with user and produced\naccuracies higher than 96%. The correlation between the remotely sensed\nestimated and Eurostat reported crop area ranges from 0.93 (potatoes) to 0.99\n(rape and turnip rape). Finally, we discuss how the framework presented here\ncan underpin the operational delivery of in-season high-resolution based crop\nmapping.",
          "link": "http://arxiv.org/abs/2105.09261",
          "publishedOn": "2021-05-24T05:08:41.483Z",
          "wordCount": 748,
          "title": "From parcel to continental scale -- A first European crop type map based on Sentinel-1 and LUCAS Copernicus in-situ observations. (arXiv:2105.09261v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Context-aware machine translation models are designed to leverage contextual\ninformation, but often fail to do so. As a result, they inaccurately\ndisambiguate pronouns and polysemous words that require context for resolution.\nIn this paper, we ask several questions: What contexts do human translators use\nto resolve ambiguous words? Are models paying large amounts of attention to the\nsame context? What if we explicitly train them to do so? To answer these\nquestions, we introduce SCAT (Supporting Context for Ambiguous Translations), a\nnew English-French dataset comprising supporting context words for 14K\ntranslations that professional translators found useful for pronoun\ndisambiguation. Using SCAT, we perform an in-depth analysis of the context used\nto disambiguate, examining positional and lexical characteristics of the\nsupporting words. Furthermore, we measure the degree of alignment between the\nmodel's attention scores and the supporting context from SCAT, and apply a\nguided attention strategy to encourage agreement between the two.",
          "link": "http://arxiv.org/abs/2105.06977",
          "publishedOn": "2021-05-24T05:08:41.459Z",
          "wordCount": 613,
          "title": "Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1\">Patrik Joslin Kenfack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arapov_D/0/1/0/all/0/1\">Daniil Dmitrievich Arapov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1\">Rasheed Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1\">S.M. Ahsan Kazmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Adil Mehmood Khan</a>",
          "description": "Generative adversarial networks (GANs) are one of the greatest advances in AI\nin recent years. With their ability to directly learn the probability\ndistribution of data, and then sample synthetic realistic data. Many\napplications have emerged, using GANs to solve classical problems in machine\nlearning, such as data augmentation, class unbalance problems, and fair\nrepresentation learning. In this paper, we analyze and highlight fairness\nconcerns of GANs model. In this regard, we show empirically that GANs models\nmay inherently prefer certain groups during the training process and therefore\nthey're not able to homogeneously generate data from different groups during\nthe testing phase. Furthermore, we propose solutions to solve this issue by\nconditioning the GAN model towards samples' group or using ensemble method\n(boosting) to allow the GAN model to leverage distributed structure of data\nduring the training phase and generate groups at equal rate during the testing\nphase.",
          "link": "http://arxiv.org/abs/2103.00950",
          "publishedOn": "2021-05-24T05:08:41.413Z",
          "wordCount": 624,
          "title": "On the Fairness of Generative Adversarial Networks (GANs). (arXiv:2103.00950v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08798",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jian-Qing Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_X/0/1/0/all/0/1\">Xiu-Li Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eyre_J/0/1/0/all/0/1\">Janet Eyre</a>",
          "description": "Stroke is known as a major global health problem, and for stroke survivors it\nis key to monitor the recovery levels. However, traditional stroke\nrehabilitation assessment methods (such as the popular clinical assessment) can\nbe subjective and expensive, and it is also less convenient for patients to\nvisit clinics in a high frequency. To address this issue, in this work based on\nwearable sensing and machine learning techniques, we developed an automated\nsystem that can predict the assessment score in an objective manner. With\nwrist-worn sensors, accelerometer data was collected from 59 stroke survivors\nin free-living environments for a duration of 8 weeks, and we aim to map the\nweek-wise accelerometer data (3 days per week) to the assessment score by\ndeveloping signal processing and predictive model pipeline. To achieve this, we\nproposed two types of new features, which can encode the rehabilitation\ninformation from both paralysed/non-paralysed sides while suppressing the\nhigh-level noises such as irrelevant daily activities. Based on the proposed\nfeatures, we further developed the longitudinal mixed-effects model with\nGaussian process prior (LMGP), which can model the random effects caused by\ndifferent subjects and time slots (during the 8 weeks). Comprehensive\nexperiments were conducted to evaluate our system on both acute and chronic\npatients, and the results suggested its effectiveness.",
          "link": "http://arxiv.org/abs/2009.08798",
          "publishedOn": "2021-05-24T05:08:41.403Z",
          "wordCount": 679,
          "title": "Automated Stroke Rehabilitation Assessment using Wearable Accelerometers in Free-Living Environments. (arXiv:2009.08798v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wentai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Ligang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Rui Mao</a>",
          "description": "Federated Learning (FL) has shown great potential as a privacy-preserving\nsolution to learning from decentralized data which are only accessible locally\non end devices (i.e., clients). In many scenarios, however, a large proportion\nof the clients are probably in possession of low-quality data that are biased,\nnoisy or even irrelevant. As a result, they could significantly slow down the\nconvergence of the global model we aim to build and also compromise its\nquality. In light of this, we propose FedProf, a novel protocol for optimizing\nFL under such circumstances without breaching data privacy. The key of our\napproach is using the global model to dynamically profile the latent\nrepresentations of data (termed representation footprints) on the clients. By\nmatching local footprints on clients against a baseline footprint on the\nserver, we adaptively score each client and adjust its probability of being\nselected each round so as to mitigate the impact of the clients with\nlow-quality data on the training process. We have conducted extensive\nexperiments on public data sets using various FL settings. The results show\nthat FedProf effectively reduces the number of communication rounds and overall\ntime (providing up to 4.5x speedup) for the global model to converge while\nimproving the accuracy of the final global model.",
          "link": "http://arxiv.org/abs/2102.01733",
          "publishedOn": "2021-05-24T05:08:41.397Z",
          "wordCount": 694,
          "title": "FedProf: Efficient Federated Learning with Data Representation Profiling. (arXiv:2102.01733v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandna_P/0/1/0/all/0/1\">Pritish Chandna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramires_A/0/1/0/all/0/1\">Ant&#xf3;nio Ramires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1\">Xavier Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_E/0/1/0/all/0/1\">Emilia G&#xf3;mez</a>",
          "description": "Loops, seamlessly repeatable musical segments, are a cornerstone of modern\nmusic production. Contemporary artists often mix and match various sampled or\npre-recorded loops based on musical criteria such as rhythm, harmony and\ntimbral texture to create compositions. Taking such criteria into account, we\npresent LoopNet, a feed-forward generative model for creating loops conditioned\non intuitive parameters. We leverage Music Information Retrieval (MIR) models\nas well as a large collection of public loop samples in our study and use the\nWave-U-Net architecture to map control parameters to audio. We also evaluate\nthe quality of the generated audio and propose intuitive controls for composers\nto map the ideas in their minds to an audio loop.",
          "link": "http://arxiv.org/abs/2105.10371",
          "publishedOn": "2021-05-24T05:08:41.378Z",
          "wordCount": 547,
          "title": "LoopNet: Musical Loop Synthesis Conditioned On Intuitive Musical Parameters. (arXiv:2105.10371v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khoshsirat_A/0/1/0/all/0/1\">Aria Khoshsirat</a>",
          "description": "Quantifying uncertainty in a model's predictions is important as it enables\nthe safety of an AI system to be increased by acting on the model's output in\nan informed manner. This is crucial for applications where the cost of an error\nis high, such as in autonomous vehicle control, medical image analysis,\nfinancial estimations or legal fields. Deep Neural Networks are powerful\npredictors that have recently achieved state-of-the-art performance on a wide\nspectrum of tasks. Quantifying predictive uncertainty in DNNs is a challenging\nand yet on-going problem. In this paper we propose a complete framework to\ncapture and quantify all of these three types of uncertainties in DNNs for\nimage classification. This framework includes an ensemble of CNNs for model\nuncertainty, a supervised reconstruction auto-encoder to capture distributional\nuncertainty and using the output of activation functions in the last layer of\nthe network, to capture data uncertainty. Finally we demonstrate the efficiency\nof our method on popular image datasets for classification.",
          "link": "http://arxiv.org/abs/2011.08712",
          "publishedOn": "2021-05-24T05:08:41.367Z",
          "wordCount": 651,
          "title": "Quantifying Uncertainty from Different Sources in Deep Neural Networks for Image Classification. (arXiv:2011.08712v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diallo_A/0/1/0/all/0/1\">A&#xef;ssatou Diallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>",
          "description": "Ordinal embedding aims at finding a low dimensional representation of objects\nfrom a set of constraints of the form \"item $j$ is closer to item $i$ than item\n$k$\". Typically, each object is mapped onto a point vector in a low dimensional\nmetric space. We argue that mapping to a density instead of a point vector\nprovides some interesting advantages, including an inherent reflection of the\nuncertainty about the representation itself and its relative location in the\nspace. Indeed, in this paper, we propose to embed each object as a Gaussian\ndistribution. We investigate the ability of these embeddings to capture the\nunderlying structure of the data while satisfying the constraints, and explore\nproperties of the representation. Experiments on synthetic and real-world\ndatasets showcase the advantages of our approach. In addition, we illustrate\nthe merit of modelling uncertainty, which enriches the visual perception of the\nmapped objects in the space.",
          "link": "http://arxiv.org/abs/2105.10457",
          "publishedOn": "2021-05-24T05:08:41.329Z",
          "wordCount": 575,
          "title": "Elliptical Ordinal Embedding. (arXiv:2105.10457v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Sanchez_P/0/1/0/all/0/1\">Pedro A. Moreno-Sanchez</a>",
          "description": "Currently, Chronic Kidney Disease (CKD) is experiencing a globally increasing\nincidence and high cost to health systems. A delayed recognition leads to\npremature mortality due to progressive loss of kidney function. The employment\nof data mining to discover subtle patterns in CKD indicators would contribute\nto an early diagnosis. This work develops a classifier model that would support\nhealthcare professionals in the early diagnosis of CKD patients. Through a data\npipeline, an exhaustive search is performed to find the best data mining\nclassifier with different parameters of the data preparation's sub-stages like\ndata missing or feature selection. Therefore, Extra Trees is selected as the\nbest classifier with a 100% and 99% of accuracy with, respectively,\ncross-validation technique and with new unseen data. Moreover, the 8 features\nselected are employed to assess the explainability of the model's results\ndenoting which features are more relevant in the model's output.",
          "link": "http://arxiv.org/abs/2105.10368",
          "publishedOn": "2021-05-24T05:08:41.320Z",
          "wordCount": 573,
          "title": "An Explainable Classification Model for Chronic Kidney Disease Patients. (arXiv:2105.10368v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Izza_Y/0/1/0/all/0/1\">Yacine Izza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1\">Joao Marques-Silva</a>",
          "description": "Random Forest (RFs) are among the most widely used Machine Learning (ML)\nclassifiers. Even though RFs are not interpretable, there are no dedicated\nnon-heuristic approaches for computing explanations of RFs. Moreover, there is\nrecent work on polynomial algorithms for explaining ML models, including naive\nBayes classifiers. Hence, one question is whether finding explanations of RFs\ncan be solved in polynomial time. This paper answers this question negatively,\nby proving that computing one PI-explanation of an RF is D^P-complete.\nFurthermore, the paper proposes a propositional encoding for computing\nexplanations of RFs, thus enabling finding PI-explanations with a SAT solver.\nThis contrasts with earlier work on explaining boosted trees (BTs) and neural\nnetworks (NNs), which requires encodings based on SMT/MILP. Experimental\nresults, obtained on a wide range of publicly available datasets, demontrate\nthat the proposed SAT-based approach scales to RFs of sizes common in practical\napplications. Perhaps more importantly, the experimental results demonstrate\nthat, for the vast majority of examples considered, the SAT-based approach\nproposed in this paper significantly outperforms existing heuristic approaches.",
          "link": "http://arxiv.org/abs/2105.10278",
          "publishedOn": "2021-05-24T05:08:41.312Z",
          "wordCount": 599,
          "title": "On Explaining Random Forests with SAT. (arXiv:2105.10278v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10341",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bragilevsky_L/0/1/0/all/0/1\">Lior Bragilevsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1\">Ivan V. Baji&#x107;</a>",
          "description": "In the race to bring Artificial Intelligence (AI) to the edge, collaborative\nintelligence has emerged as a promising way to lighten the computation load on\nedge devices that run applications based on Deep Neural Networks (DNNs).\nTypically, a deep model is split at a certain layer into edge and cloud\nsub-models. The deep feature tensor produced by the edge sub-model is\ntransmitted to the cloud, where the remaining computationally intensive\nworkload is performed by the cloud sub-model. The communication channel between\nthe edge and cloud is imperfect, which will result in missing data in the deep\nfeature tensor received at the cloud side. In this study, we examine the\neffectiveness of four low-rank tensor completion methods in recovering missing\ndata in the deep feature tensor. We consider both sparse tensors, such as those\nproduced by the VGG16 model, as well as non-sparse tensors, such as those\nproduced by ResNet34 model. We study tensor completion effectiveness in both\nconplexity-constrained and unconstrained scenario.",
          "link": "http://arxiv.org/abs/2105.10341",
          "publishedOn": "2021-05-24T05:08:41.305Z",
          "wordCount": 620,
          "title": "Error Resilient Collaborative Intelligence via Low-Rank Tensor Completion. (arXiv:2105.10341v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prost_F/0/1/0/all/0/1\">Flavien Prost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjal Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumm_N/0/1/0/all/0/1\">Nick Blumm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumthekar_A/0/1/0/all/0/1\">Aditee Kumthekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potter_T/0/1/0/all/0/1\">Trevor Potter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Li Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1\">Alex Beutel</a>",
          "description": "In this work we study the problem of measuring the fairness of a machine\nlearning model under noisy information. Focusing on group fairness metrics, we\ninvestigate the particular but common situation when the evaluation requires\ncontrolling for the confounding effect of covariate variables. In a practical\nsetting, we might not be able to jointly observe the covariate and group\ninformation, and a standard workaround is to then use proxies for one or more\nof these variables. Prior works have demonstrated the challenges with using a\nproxy for sensitive attributes, and strong independence assumptions are needed\nto provide guarantees on the accuracy of the noisy estimates. In contrast, in\nthis work we study using a proxy for the covariate variable and present a\ntheoretical analysis that aims to characterize weaker conditions under which\naccurate fairness evaluation is possible.\n\nFurthermore, our theory identifies potential sources of errors and decouples\nthem into two interpretable parts $\\gamma$ and $\\epsilon$. The first part\n$\\gamma$ depends solely on the performance of the proxy such as precision and\nrecall, whereas the second part $\\epsilon$ captures correlations between all\nthe variables of interest. We show that in many scenarios the error in the\nestimates is dominated by $\\gamma$ via a linear dependence, whereas the\ndependence on the correlations $\\epsilon$ only constitutes a lower order term.\nAs a result we expand the understanding of scenarios where measuring model\nfairness via proxies can be an effective approach. Finally, we compare, via\nsimulations, the theoretical upper-bounds to the distribution of simulated\nestimation errors and show that assuming some structure on the data, even weak,\nis key to significantly improve both theoretical guarantees and empirical\nresults.",
          "link": "http://arxiv.org/abs/2105.09985",
          "publishedOn": "2021-05-24T05:08:41.287Z",
          "wordCount": 717,
          "title": "Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective. (arXiv:2105.09985v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ayazoglu_M/0/1/0/all/0/1\">Mustafa Ayazoglu</a>",
          "description": "Single-Image Super Resolution (SISR) is a classical computer vision problem\nand it has been studied for over decades. With the recent success of deep\nlearning methods, recent work on SISR focuses solutions with deep learning\nmethodologies and achieves state-of-the-art results. However most of the\nstate-of-the-art SISR methods contain millions of parameters and layers, which\nlimits their practical applications. In this paper, we propose a hardware\n(Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization\nrobust real-time super resolution network (XLSR). The proposed model's building\nblock is inspired from root modules for Image classification. We successfully\napplied root modules to SISR problem, further more to make the model uint8\nquantization robust we used Clipped ReLU at the last layer of the network and\nachieved great balance between reconstruction quality and runtime. Furthermore,\nalthough the proposed network contains 30x fewer parameters than VDSR its\nperformance surpasses it on Div2K validation set. The network proved itself by\nwinning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.",
          "link": "http://arxiv.org/abs/2105.10288",
          "publishedOn": "2021-05-24T05:08:41.280Z",
          "wordCount": 619,
          "title": "Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices. (arXiv:2105.10288v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10126",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Kim_D/0/1/0/all/0/1\">Doojin Kim</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Kong_K/0/1/0/all/0/1\">Kyoungchul Kong</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Matchev_K/0/1/0/all/0/1\">Konstantin T. Matchev</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Park_M/0/1/0/all/0/1\">Myeonghun Park</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Shyamsundar_P/0/1/0/all/0/1\">Prasanth Shyamsundar</a>",
          "description": "The choice of optimal event variables is crucial for achieving the maximal\nsensitivity of experimental analyses. Over time, physicists have derived\nsuitable kinematic variables for many typical event topologies in collider\nphysics. Here we introduce a deep learning technique to design good event\nvariables, which are sensitive over a wide range of values for the unknown\nmodel parameters. We demonstrate that the neural networks trained with our\ntechnique on some simple event topologies are able to reproduce standard event\nvariables like invariant mass, transverse mass, and stransverse mass. The\nmethod is automatable, completely general, and can be used to derive sensitive,\npreviously unknown, event variables for other, more complex event topologies.",
          "link": "http://arxiv.org/abs/2105.10126",
          "publishedOn": "2021-05-24T05:08:41.274Z",
          "wordCount": 566,
          "title": "Deep-Learned Event Variables for Collider Phenomenology. (arXiv:2105.10126v1 [hep-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Searcy_J/0/1/0/all/0/1\">Jacob A. Searcy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolowski_S/0/1/0/all/0/1\">Susan L. Sokolowski</a>",
          "description": "Sizing and fitting of Personal Protective Equipment (PPE) is a critical part\nof the product creation process; however, traditional methods to do this type\nof work can be labor intensive and based on limited or non-representative\nanthropomorphic data. In the case of PPE, a poor fit can jeopardize an\nindividual's health and safety. In this paper we present an unsupervised\nmachine learning algorithm that can identify a representative set of exemplars,\nindividuals that can be utilized by designers as idealized sizing models. The\nalgorithm is based around a Variational Autoencoder (VAE) with a Point-Net\ninspired encoder and decoder architecture trained on Human point-cloud data\nobtained from the CEASAR dataset. The learned latent space is then clustered to\nidentify a specified number of sizing groups. We demonstrate this technique on\nscans of human faces to provide designers of masks and facial coverings a\nreference set of individuals to test existing mask styles.",
          "link": "http://arxiv.org/abs/2105.10067",
          "publishedOn": "2021-05-24T05:08:41.258Z",
          "wordCount": 590,
          "title": "Towards Automatic Sizing for PPE with a Point Cloud Based Variational Autoencoder. (arXiv:2105.10067v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishtiaq_A/0/1/0/all/0/1\">Arhum Ishtiaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_S/0/1/0/all/0/1\">Sara Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anees_M/0/1/0/all/0/1\">Maheen Anees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mumtaz_N/0/1/0/all/0/1\">Neha Mumtaz</a>",
          "description": "With time, machine learning models have increased in their scope,\nfunctionality and size. Consequently, the increased functionality and size of\nsuch models requires high-end hardware to both train and provide inference\nafter the fact. This paper aims to explore the possibilities within the domain\nof model compression and discuss the efficiency of each of the possible\napproaches while comparing model size and performance with respect to pre- and\npost-compression.",
          "link": "http://arxiv.org/abs/2105.10059",
          "publishedOn": "2021-05-24T05:08:41.222Z",
          "wordCount": 480,
          "title": "Model Compression. (arXiv:2105.10059v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_B/0/1/0/all/0/1\">Boaz Shmueli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Soumya Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>",
          "description": "Datasets with induced emotion labels are scarce but of utmost importance for\nmany NLP tasks. We present a new, automated method for collecting texts along\nwith their induced reaction labels. The method exploits the online use of\nreaction GIFs, which capture complex affective states. We show how to augment\nthe data with induced emotion and induced sentiment labels. We use our method\nto create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K\ntweets. We provide baselines for three new tasks, including induced sentiment\nprediction and multilabel classification of induced emotions. Our method and\ndataset open new research opportunities in emotion detection and affective\ncomputing.",
          "link": "http://arxiv.org/abs/2105.09967",
          "publishedOn": "2021-05-24T05:08:41.216Z",
          "wordCount": 563,
          "title": "Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter. (arXiv:2105.09967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_P/0/1/0/all/0/1\">Pasha Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>",
          "description": "Understanding the behavior of learned classifiers is an important task, and\nvarious black-box explanations, logical reasoning approaches, and\nmodel-specific methods have been proposed. In this paper, we introduce\nprobabilistic sufficient explanations, which formulate explaining an instance\nof classification as choosing the \"simplest\" subset of features such that only\nobserving those features is \"sufficient\" to explain the classification. That\nis, sufficient to give us strong probabilistic guarantees that the model will\nbehave similarly when all features are observed under the data distribution. In\naddition, we leverage tractable probabilistic reasoning tools such as\nprobabilistic circuits and expected predictions to design a scalable algorithm\nfor finding the desired explanations while keeping the guarantees intact. Our\nexperiments demonstrate the effectiveness of our algorithm in finding\nsufficient explanations, and showcase its advantages compared to Anchors and\nlogical explanations.",
          "link": "http://arxiv.org/abs/2105.10118",
          "publishedOn": "2021-05-24T05:08:41.199Z",
          "wordCount": 551,
          "title": "Probabilistic Sufficient Explanations. (arXiv:2105.10118v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan. Z Li</a>",
          "description": "In this paper, we propose a novel framework for Deep Clustering and\nmulti-manifold Representation Learning (DCRL) that preserves the geometric\nstructure of data. In the proposed framework, manifold clustering is done in\nthe latent space guided by a clustering loss. To overcome the problem that\nclustering-oriented losses may deteriorate the geometric structure of\nembeddings in the latent space, an isometric loss is proposed for preserving\nintra-manifold structure locally and a ranking loss for inter-manifold\nstructure globally. Experimental results on various datasets show that DCRL\nleads to performances comparable to current state-of-the-art deep clustering\nalgorithms, yet exhibits superior performance for manifold representation. Our\nresults also demonstrate the importance and effectiveness of the proposed\nlosses in preserving geometric structure in terms of visualization and\nperformance metrics.",
          "link": "http://arxiv.org/abs/2009.09590",
          "publishedOn": "2021-05-24T05:08:41.162Z",
          "wordCount": 602,
          "title": "Deep Clustering and Representation Learning with Geometric Structure Preservation. (arXiv:2009.09590v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milan_G/0/1/0/all/0/1\">Giulia Milan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassio_L/0/1/0/all/0/1\">Luca Vassio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drago_I/0/1/0/all/0/1\">Idilio Drago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellia_M/0/1/0/all/0/1\">Marco Mellia</a>",
          "description": "Our life is getting filled by Internet of Things (IoT) devices. These devices\noften rely on closed or poorly documented protocols, with unknown formats and\nsemantics. Learning how to interact with such devices in an autonomous manner\nis the key for interoperability and automatic verification of their\ncapabilities. In this paper, we propose RL-IoT, a system that explores how to\nautomatically interact with possibly unknown IoT devices. We leverage\nreinforcement learning (RL) to recover the semantics of protocol messages and\nto take control of the device to reach a given goal, while minimizing the\nnumber of interactions. We assume to know only a database of possible IoT\nprotocol messages, whose semantics are however unknown. RL-IoT exchanges\nmessages with the target IoT device, learning those commands that are useful to\nreach the given goal. Our results show that RL-IoT is able to solve both simple\nand complex tasks. With properly tuned parameters, RL-IoT learns how to perform\nactions with the target device, a Yeelight smart bulb in our case study,\ncompleting non-trivial patterns with as few as 400 interactions. RL-IoT paves\nthe road for automatic interactions with poorly documented IoT protocols, thus\nenabling interoperable systems.",
          "link": "http://arxiv.org/abs/2105.00884",
          "publishedOn": "2021-05-24T05:08:41.150Z",
          "wordCount": 655,
          "title": "RL-IoT: Reinforcement Learning to Interact with IoT Devices. (arXiv:2105.00884v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08954",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1\">Yuling Yao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pirs_G/0/1/0/all/0/1\">Gregor Pir&#x161;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1\">Aki Vehtari</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1\">Andrew Gelman</a>",
          "description": "Stacking is a widely used model averaging technique that asymptotically\nyields optimal predictions among linear averages. We show that stacking is most\neffective when model predictive performance is heterogeneous in inputs, and we\ncan further improve the stacked mixture with a hierarchical model. We\ngeneralize stacking to Bayesian hierarchical stacking. The model weights are\nvarying as a function of data, partially-pooled, and inferred using Bayesian\ninference. We further incorporate discrete and continuous inputs, other\nstructured priors, and time series and longitudinal data. To verify the\nperformance gain of the proposed method, we derive theory bounds, and\ndemonstrate on several applied problems.",
          "link": "http://arxiv.org/abs/2101.08954",
          "publishedOn": "2021-05-24T05:08:41.136Z",
          "wordCount": 557,
          "title": "Bayesian hierarchical stacking: Some models are (somewhere) useful. (arXiv:2101.08954v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10347",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sekkat_I/0/1/0/all/0/1\">Inass Sekkat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Stoltz_G/0/1/0/all/0/1\">Gabriel Stoltz</a>",
          "description": "The computational cost of usual Monte Carlo methods for sampling a posteriori\nlaws in Bayesian inference scales linearly with the number of data points. One\noption to reduce it to a fraction of this cost is to resort to mini-batching in\nconjunction with unadjusted discretizations of Langevin dynamics, in which case\nonly a random fraction of the data is used to estimate the gradient. However,\nthis leads to an additional noise in the dynamics and hence a bias on the\ninvariant measure which is sampled by the Markov chain. We advocate using the\nso-called Adaptive Langevin dynamics, which is a modification of standard\ninertial Langevin dynamics with a dynamical friction which automatically\ncorrects for the increased noise arising from mini-batching. We investigate the\npractical relevance of the assumptions underpinning Adaptive Langevin (constant\ncovariance for the estimation of the gradient), which are not satisfied in\ntypical models of Bayesian inference; and show how to extend the approach to\nmore general situations.",
          "link": "http://arxiv.org/abs/2105.10347",
          "publishedOn": "2021-05-24T05:08:41.096Z",
          "wordCount": 591,
          "title": "Removing the mini-batching error in Bayesian inference using Adaptive Langevin dynamics. (arXiv:2105.10347v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2011.09999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anwar_U/0/1/0/all/0/1\">Usman Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_S/0/1/0/all/0/1\">Shehryar Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghasi_A/0/1/0/all/0/1\">Alireza Aghasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Ali Ahmed</a>",
          "description": "In real world settings, numerous constraints are present which are hard to\nspecify mathematically. However, for the real world deployment of reinforcement\nlearning (RL), it is critical that RL agents are aware of these constraints, so\nthat they can act safely. In this work, we consider the problem of learning\nconstraints from demonstrations of a constraint-abiding agent's behavior. We\nexperimentally validate our approach and show that our framework can\nsuccessfully learn the most likely constraints that the agent respects. We\nfurther show that these learned constraints are \\textit{transferable} to new\nagents that may have different morphologies and/or reward functions. Previous\nworks in this regard have either mainly been restricted to tabular (discrete)\nsettings, specific types of constraints or assume the environment's transition\ndynamics. In contrast, our framework is able to learn arbitrary\n\\textit{Markovian} constraints in high-dimensions in a completely model-free\nsetting. The code can be found it:\n\\url{https://github.com/shehryar-malik/icrl}.",
          "link": "http://arxiv.org/abs/2011.09999",
          "publishedOn": "2021-05-24T05:08:41.089Z",
          "wordCount": 618,
          "title": "Inverse Constrained Reinforcement Learning. (arXiv:2011.09999v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "The popularity of multimodal sensors and the accessibility of the Internet\nhave brought us a massive amount of unlabeled multimodal data. Since existing\ndatasets and well-trained models are primarily unimodal, the modality gap\nbetween a unimodal network and unlabeled multimodal data poses an interesting\nproblem: how to transfer a pre-trained unimodal network to perform the same\ntask on unlabeled multimodal data? In this work, we propose multimodal\nknowledge expansion (MKE), a knowledge distillation-based framework to\neffectively utilize multimodal data without requiring labels. Opposite to\ntraditional knowledge distillation, where the student is designed to be\nlightweight and inferior to the teacher, we observe that a multimodal student\nmodel consistently denoises pseudo labels and generalizes better than its\nteacher. Extensive experiments on four tasks and different modalities verify\nthis finding. Furthermore, we connect the mechanism of MKE to semi-supervised\nlearning and offer both empirical and theoretical explanations to understand\nthe denoising capability of a multimodal student.",
          "link": "http://arxiv.org/abs/2103.14431",
          "publishedOn": "2021-05-24T05:08:41.068Z",
          "wordCount": 607,
          "title": "Multimodal Knowledge Expansion. (arXiv:2103.14431v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14928",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Brochard_A/0/1/0/all/0/1\">Antoine Brochard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Blaszczyszyn_B/0/1/0/all/0/1\">Bart&#x142;omiej B&#x142;aszczyszyn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mallat_S/0/1/0/all/0/1\">St&#xe9;phane Mallat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1\">Sixin Zhang</a>",
          "description": "This paper introduces a generative model for planar point processes in a\nsquare window, built upon a single realization of a stationary, ergodic point\nprocess observed in this window. Inspired by recent advances in gradient\ndescent methods for maximum entropy models, we propose a method to generate\nsimilar point patterns by jointly moving particles of an initial Poisson\nconfiguration towards a target counting measure. The target measure is\ngenerated via a deterministic gradient descent algorithm, so as to match a set\nof statistics of the given, observed realization. Our statistics are estimators\nof the multi-scale wavelet phase harmonic covariance, recently proposed in\nimage modeling. They allow one to capture geometric structures through\nmulti-scale interactions between wavelet coefficients. Both our statistics and\nthe gradient descent algorithm scale better with the number of observed points\nthan the classical k-nearest neighbour distances previously used in generative\nmodels for point processes, based on the rejection sampling or\nsimulated-annealing. The overall quality of our model is evaluated on point\nprocesses with various geometric structures through spectral and topological\ndata analysis.",
          "link": "http://arxiv.org/abs/2010.14928",
          "publishedOn": "2021-05-24T05:08:41.039Z",
          "wordCount": 629,
          "title": "Particle gradient descent model for point process generation. (arXiv:2010.14928v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1\">Anastasiya Dykyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1\">Paulo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1\">Eric Jul</a>",
          "description": "Capillaries are the smallest vessels in the body responsible for the delivery\nof oxygen and nutrients to the surrounding cells. Various diseases have been\nshown to alter the density of nutritive capillaries and the flow velocity of\nerythrocytes. In previous studies, capillary density and flow velocity have\nbeen assessed manually by trained specialists. Manual analysis of a 20-second\nlong microvascular video takes on average 20 minutes and requires extensive\ntraining. Several studies have reported that manual analysis hinders the\napplication of microvascular microscopy in a clinical setting. In this paper,\nwe present a fully automated system, called CapillaryNet, that can automate\nmicrovascular microscopy analysis so it can be used as a clinical application.\nMoreover, CapillaryNet measures several microvascular parameters that\nresearchers were previously unable to quantify, i.e. capillary hematocrit and\nintra-capillary flow velocity heterogeneity.",
          "link": "http://arxiv.org/abs/2104.11574",
          "publishedOn": "2021-05-24T05:08:41.032Z",
          "wordCount": 607,
          "title": "CapillaryNet: An Automated System to Analyze Microcirculation Videos from Handheld Vital Microscopy. (arXiv:2104.11574v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ronca_A/0/1/0/all/0/1\">Alessandro Ronca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1\">Giuseppe De Giacomo</a>",
          "description": "Recently regular decision processes have been proposed as a well-behaved form\nof non-Markov decision process. Regular decision processes are characterised by\na transition function and a reward function that depend on the whole history,\nthough regularly (as in regular languages). In practice both the transition and\nthe reward functions can be seen as finite transducers. We study reinforcement\nlearning in regular decision processes. Our main contribution is to show that a\nnear-optimal policy can be PAC-learned in polynomial time in a set of\nparameters that describe the underlying decision process. We argue that the\nidentified set of parameters is minimal and it reasonably captures the\ndifficulty of a regular decision process.",
          "link": "http://arxiv.org/abs/2105.06784",
          "publishedOn": "2021-05-24T05:08:41.018Z",
          "wordCount": 563,
          "title": "Efficient PAC Reinforcement Learning in Regular Decision Processes. (arXiv:2105.06784v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10439",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1\">Alexander Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_A/0/1/0/all/0/1\">Andrew H. Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1\">Berkin Bilgic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ba_D/0/1/0/all/0/1\">Demba Ba</a>",
          "description": "Sparse Bayesian learning (SBL) is a powerful framework for tackling the\nsparse coding problem while also providing uncertainty quantification. However,\nthe most popular inference algorithms for SBL become too expensive for\nhigh-dimensional problems due to the need to maintain a large covariance\nmatrix. To resolve this issue, we introduce a new SBL inference algorithm that\navoids explicit computation of the covariance matrix, thereby saving\nsignificant time and space. Instead of performing costly matrix inversions, our\ncovariance-free method solves multiple linear systems to obtain provably\nunbiased estimates of the posterior statistics needed by SBL. These systems can\nbe solved in parallel, enabling further acceleration of the algorithm via\ngraphics processing units. In practice, our method can be up to thousands of\ntimes faster than existing baselines, reducing hours of computation time to\nseconds. We showcase how our new algorithm enables SBL to tractably tackle\nhigh-dimensional signal recovery problems, such as deconvolution of calcium\nimaging data and multi-contrast reconstruction of magnetic resonance images.\nFinally, we open-source a toolbox containing all of our implementations to\ndrive future research in SBL.",
          "link": "http://arxiv.org/abs/2105.10439",
          "publishedOn": "2021-05-24T05:08:41.001Z",
          "wordCount": 609,
          "title": "Covariance-Free Sparse Bayesian Learning. (arXiv:2105.10439v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2010.15195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1\">Wilka Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_A/0/1/0/all/0/1\">Anthony Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sungryull Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_R/0/1/0/all/0/1\">Richard L. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>",
          "description": "First-person object-interaction tasks in high-fidelity, 3D, simulated\nenvironments such as the AI2Thor virtual home-environment pose significant\nsample-efficiency challenges for reinforcement learning (RL) agents learning\nfrom sparse task rewards. To alleviate these challenges, prior work has\nprovided extensive supervision via a combination of reward-shaping,\nground-truth object-information, and expert demonstrations. In this work, we\nshow that one can learn object-interaction tasks from scratch without\nsupervision by learning an attentive object-model as an auxiliary task during\ntask learning with an object-centric relational RL agent. Our key insight is\nthat learning an object-model that incorporates object-attention into forward\nprediction provides a dense learning signal for unsupervised representation\nlearning of both objects and their relationships. This, in turn, enables faster\npolicy learning for an object-centric relational RL agent. We demonstrate our\nagent by introducing a set of challenging object-interaction tasks in the\nAI2Thor environment where learning with our attentive object-model is key to\nstrong performance. Specifically, we compare our agent and relational RL agents\nwith alternative auxiliary tasks to a relational RL agent equipped with\nground-truth object-information, and show that learning with our object-model\nbest closes the performance gap in terms of both learning speed and maximum\nsuccess rate. Additionally, we find that incorporating object-attention into an\nobject-model's forward predictions is key to learning representations which\ncapture object-category and object-state.",
          "link": "http://arxiv.org/abs/2010.15195",
          "publishedOn": "2021-05-24T05:08:40.993Z",
          "wordCount": 695,
          "title": "Reinforcement Learning for Sparse-Reward Object-Interaction Tasks in a First-person Simulated 3D Environment. (arXiv:2010.15195v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harlim_J/0/1/0/all/0/1\">John Harlim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiantao Li</a>",
          "description": "This paper studies the theoretical underpinnings of machine learning of\nergodic It\\^o diffusions. The objective is to understand the convergence\nproperties of the invariant statistics when the underlying system of stochastic\ndifferential equations (SDEs) is empirically estimated with a supervised\nregression framework. Using the perturbation theory of ergodic Markov chains\nand the linear response theory, we deduce a linear dependence of the errors of\none-point and two-point invariant statistics on the error in the learning of\nthe drift and diffusion coefficients. More importantly, our study shows that\nthe usual $L^2$-norm characterization of the learning generalization error is\ninsufficient for achieving this linear dependence result. We find that\nsufficient conditions for such a linear dependence result are through learning\nalgorithms that produce a uniformly Lipschitz and consistent estimator in the\nhypothesis space that retains certain characteristics of the drift\ncoefficients, such as the usual linear growth condition that guarantees the\nexistence of solutions of the underlying SDEs. We examine these conditions on\ntwo well-understood learning algorithms: the kernel-based spectral regression\nmethod and the shallow random neural networks with the ReLU activation\nfunction.",
          "link": "http://arxiv.org/abs/2105.10102",
          "publishedOn": "2021-05-24T05:08:40.984Z",
          "wordCount": 633,
          "title": "Error Bounds of the Invariant Statistics in Machine Learning of Ergodic It\\^o Diffusions. (arXiv:2105.10102v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.08614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1\">Kuldeep Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajhans Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan Turaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aswin Sankaranarayanan</a>",
          "description": "Recently, there has been substantial progress in image synthesis from\nsemantic labelmaps. However, methods used for this task assume the availability\nof complete and unambiguous labelmaps, with instance boundaries of objects, and\nclass labels for each pixel. This reliance on heavily annotated inputs\nrestricts the application of image synthesis techniques to real-world\napplications, especially under uncertainty due to weather, occlusion, or noise.\nOn the other hand, algorithms that can synthesize images from sparse labelmaps\nor sketches are highly desirable as tools that can guide content creators and\nartists to quickly generate scenes by simply specifying locations of a few\nobjects. In this paper, we address the problem of complex scene completion from\nsparse labelmaps. Under this setting, very few details about the scene (30\\% of\nobject instances) are available as input for image synthesis. We propose a\ntwo-stage deep network based method, called `Halluci-Net', that learns\nco-occurence relationships between objects in scenes, and then exploits these\nrelationships to produce a dense and complete labelmap. The generated dense\nlabelmap can then be used as input by state-of-the-art image synthesis\ntechniques like pix2pixHD to obtain the final image. The proposed method is\nevaluated on the Cityscapes dataset and it outperforms two baselines methods on\nperformance metrics like Fr\\'echet Inception Distance (FID), semantic\nsegmentation accuracy, and similarity in object co-occurrences. We also show\nqualitative results on a subset of ADE20K dataset that contains bedroom images.",
          "link": "http://arxiv.org/abs/2004.08614",
          "publishedOn": "2021-05-24T05:08:40.959Z",
          "wordCount": 709,
          "title": "Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships. (arXiv:2004.08614v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vipul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1\">Dhruv Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1\">Ping Tak Peter Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaohan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejariwal_A/0/1/0/all/0/1\">Arun Kejariwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "In this paper, we consider hybrid parallelism -- a paradigm that employs both\nData Parallelism (DP) and Model Parallelism (MP) -- to scale distributed\ntraining of large recommendation models. We propose a compression framework\ncalled Dynamic Communication Thresholding (DCT) for communication-efficient\nhybrid training. DCT filters the entities to be communicated across the network\nthrough a simple hard-thresholding function, allowing only the most relevant\ninformation to pass through. For communication efficient DP, DCT compresses the\nparameter gradients sent to the parameter server during model synchronization.\nThe threshold is updated only once every few thousand iterations to reduce the\ncomputational overhead of compression. For communication efficient MP, DCT\nincorporates a novel technique to compress the activations and gradients sent\nacross the network during the forward and backward propagation, respectively.\nThis is done by identifying and updating only the most relevant neurons of the\nneural network for each training sample in the data. We evaluate DCT on\npublicly available natural language processing and recommender models and\ndatasets, as well as recommendation systems used in production at Facebook. DCT\nreduces communication by at least $100\\times$ and $20\\times$ during DP and MP,\nrespectively. The algorithm has been deployed in production, and it improves\nend-to-end training time for a state-of-the-art industrial recommender model by\n37\\%, without any loss in performance.",
          "link": "http://arxiv.org/abs/2010.08899",
          "publishedOn": "2021-05-24T05:08:40.952Z",
          "wordCount": 713,
          "title": "Training Recommender Systems at Scale: Communication-Efficient Model and Data Parallelism. (arXiv:2010.08899v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.14605",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Borysenko_O/0/1/0/all/0/1\">Oleksandr Borysenko</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Byshkin_M/0/1/0/all/0/1\">Maksym Byshkin</a>",
          "description": "Deep learning applications require global optimization of non-convex\nobjective functions, which have multiple local minima. The same problem is\noften found in physical simulations and may be resolved by the methods of\nLangevin dynamics with Simulated Annealing, which is a well-established\napproach for minimization of many-particle potentials. This analogy provides\nuseful insights for non-convex stochastic optimization in machine learning.\nHere we find that integration of the discretized Langevin equation gives a\ncoordinate updating rule equivalent to the famous Momentum optimization\nalgorithm. As a main result, we show that a gradual decrease of the momentum\ncoefficient from the initial value close to unity until zero is equivalent to\napplication of Simulated Annealing or slow cooling, in physical terms. Making\nuse of this novel approach, we propose CoolMomentum -- a new stochastic\noptimization method. Applying Coolmomentum to optimization of Resnet-20 on\nCifar-10 dataset and Efficientnet-B0 on Imagenet, we demonstrate that it is\nable to achieve high accuracies.",
          "link": "http://arxiv.org/abs/2005.14605",
          "publishedOn": "2021-05-24T05:08:40.934Z",
          "wordCount": 637,
          "title": "CoolMomentum: A Method for Stochastic Optimization by Langevin Dynamics with Simulated Annealing. (arXiv:2005.14605v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>",
          "description": "We design multi-horizon forecasting models for limit order book (LOB) data by\nusing deep learning techniques. Unlike standard structures where a single\nprediction is made, we adopt encoder-decoder models with sequence-to-sequence\nand Attention mechanisms, to generate a forecasting path. Our methods achieve\ncomparable performance to state-of-art algorithms at short prediction horizons.\nImportantly, they outperform when generating predictions over long horizons by\nleveraging the multi-horizon setup. Given that encoder-decoder models rely on\nrecurrent neural layers, they generally suffer from a slow training process. To\nremedy this, we experiment with utilising novel hardware, so-called Intelligent\nProcessing Units (IPUs) produced by Graphcore. IPUs are specifically designed\nfor machine intelligence workload with the aim to speed up the computation\nprocess. We show that in our setup this leads to significantly faster training\ntimes when compared to training models with GPUs.",
          "link": "http://arxiv.org/abs/2105.10430",
          "publishedOn": "2021-05-24T05:08:40.927Z",
          "wordCount": 595,
          "title": "Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning Approaches and Hardware Acceleration using Intelligent Processing Units. (arXiv:2105.10430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.14708",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Raut_P/0/1/0/all/0/1\">Prasanna Sanjay Raut</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sadeghi_O/0/1/0/all/0/1\">Omid Sadeghi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fazel_M/0/1/0/all/0/1\">Maryam Fazel</a>",
          "description": "In this paper, we consider online continuous DR-submodular maximization with\nlinear stochastic long-term constraints. Compared to the prior work on online\nsubmodular maximization, our setting introduces the extra complication of\nstochastic linear constraint functions that are i.i.d. generated at each round.\nTo be precise, at step $t\\in\\{1,\\dots,T\\}$, a DR-submodular utility function\n$f_t(\\cdot)$ and a constraint vector $p_t$, i.i.d. generated from an unknown\ndistribution with mean $p$, are revealed after committing to an action $x_t$\nand we aim to maximize the overall utility while the expected cumulative\nresource consumption $\\sum_{t=1}^T \\langle p,x_t\\rangle$ is below a fixed\nbudget $B_T$. Stochastic long-term constraints arise naturally in applications\nwhere there is a limited budget or resource available and resource consumption\nat each step is governed by stochastically time-varying environments. We\npropose the Online Lagrangian Frank-Wolfe (OLFW) algorithm to solve this class\nof online problems. We analyze the performance of the OLFW algorithm and we\nobtain sub-linear regret bounds as well as sub-linear cumulative constraint\nviolation bounds, both in expectation and with high probability.",
          "link": "http://arxiv.org/abs/2005.14708",
          "publishedOn": "2021-05-24T05:08:40.916Z",
          "wordCount": 636,
          "title": "Online DR-Submodular Maximization with Stochastic Cumulative Constraints. (arXiv:2005.14708v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Debasmit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1\">Yash Bhalgat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>",
          "description": "In this work, we propose a data-driven scheme to initialize the parameters of\na deep neural network. This is in contrast to traditional approaches which\nrandomly initialize parameters by sampling from transformed standard\ndistributions. Such methods do not use the training data to produce a more\ninformed initialization. Our method uses a sequential layer-wise approach where\neach layer is initialized using its input activations. The initialization is\ncast as an optimization problem where we minimize a combination of encoding and\ndecoding losses of the input activations, which is further constrained by a\nuser-defined latent code. The optimization problem is then restructured into\nthe well-known Sylvester equation, which has fast and efficient gradient-free\nsolutions. Our data-driven method achieves a boost in performance compared to\nrandom initialization methods, both before start of training and after training\nis over. We show that our proposed method is especially effective in few-shot\nand fine-tuning settings. We conclude this paper with analyses on time\ncomplexity and the effect of different latent codes on the recognition\nperformance.",
          "link": "http://arxiv.org/abs/2105.10335",
          "publishedOn": "2021-05-24T05:08:40.901Z",
          "wordCount": 616,
          "title": "Data-driven Weight Initialization with Sylvester Solvers. (arXiv:2105.10335v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1\">Kaleel Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevin_D/0/1/0/all/0/1\">Deniz Gurevin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1\">Marten van Dijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Ha Nguyen</a>",
          "description": "Many defenses have recently been proposed at venues like NIPS, ICML, ICLR and\nCVPR. These defenses are mainly focused on mitigating white-box attacks. They\ndo not properly examine black-box attacks. In this paper, we expand upon the\nanalysis of these defenses to include adaptive black-box adversaries. Our\nevaluation is done on nine defenses including Barrage of Random Transforms,\nComDefend, Ensemble Diversity, Feature Distillation, The Odds are Odd, Error\nCorrecting Codes, Distribution Classifier Defense, K-Winner Take All and Buffer\nZones. Our investigation is done using two black-box adversarial models and six\nwidely studied adversarial attacks for CIFAR-10 and Fashion-MNIST datasets. Our\nanalyses show most recent defenses (7 out of 9) provide only marginal\nimprovements in security ($<25\\%$), as compared to undefended networks. For\nevery defense, we also show the relationship between the amount of data the\nadversary has at their disposal, and the effectiveness of adaptive black-box\nattacks. Overall, our results paint a clear picture: defenses need both\nthorough white-box and black-box analyses to be considered secure. We provide\nthis large scale study and analyses to motivate the field to move towards the\ndevelopment of more robust black-box defenses.",
          "link": "http://arxiv.org/abs/2006.10876",
          "publishedOn": "2021-05-24T05:08:40.883Z",
          "wordCount": 656,
          "title": "Beware the Black-Box: on the Robustness of Recent Defenses to Adversarial Examples. (arXiv:2006.10876v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Collier_M/0/1/0/all/0/1\">Mark Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokiopoulou_E/0/1/0/all/0/1\">Efi Kokiopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1\">Rodolphe Jenatton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berent_J/0/1/0/all/0/1\">Jesse Berent</a>",
          "description": "Large scale image classification datasets often contain noisy labels. We take\na principled probabilistic approach to modelling input-dependent, also known as\nheteroscedastic, label noise in these datasets. We place a multivariate Normal\ndistributed latent variable on the final hidden layer of a neural network\nclassifier. The covariance matrix of this latent variable, models the aleatoric\nuncertainty due to label noise. We demonstrate that the learned covariance\nstructure captures known sources of label noise between semantically similar\nand co-occurring classes. Compared to standard neural network training and\nother baselines, we show significantly improved accuracy on Imagenet ILSVRC\n2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a\nnew state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These\ndatasets range from over 1M to over 300M training examples and from 1k classes\nto more than 21k classes. Our method is simple to use, and we provide an\nimplementation that is a drop-in replacement for the final fully-connected\nlayer in a deep classifier.",
          "link": "http://arxiv.org/abs/2105.10305",
          "publishedOn": "2021-05-24T05:08:40.876Z",
          "wordCount": 609,
          "title": "Correlated Input-Dependent Label Noise in Large-Scale Image Classification. (arXiv:2105.10305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10489",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bilbrey_J/0/1/0/all/0/1\">Jenna Bilbrey</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ward_L/0/1/0/all/0/1\">Logan Ward</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Choudhury_S/0/1/0/all/0/1\">Sutanay Choudhury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kumar_N/0/1/0/all/0/1\">Neeraj Kumar</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sivaraman_G/0/1/0/all/0/1\">Ganesh Sivaraman</a>",
          "description": "We examine a pair of graph generative models for the therapeutic design of\nnovel drug candidates targeting SARS-CoV-2 viral proteins. Due to a sense of\nurgency, we chose well-validated models with unique strengths: an autoencoder\nthat generates molecules with similar structures to a dataset of drugs with\nanti-SARS activity and a reinforcement learning algorithm that generates highly\nnovel molecules. During generation, we explore optimization toward several\ndesign targets to balance druglikeness, synthetic accessability, and anti-SARS\nactivity based on \\icfifty. This generative\nframework\\footnote{https://github.com/exalearn/covid-drug-design} will\naccelerate drug discovery in future pandemics through the high-throughput\ngeneration of targeted therapeutic candidates.",
          "link": "http://arxiv.org/abs/2105.10489",
          "publishedOn": "2021-05-24T05:08:40.870Z",
          "wordCount": 608,
          "title": "Evening the Score: Targeting SARS-CoV-2 Protease Inhibition in Graph Generative Models for Therapeutic Candidates. (arXiv:2105.10489v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10358",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shoji_T/0/1/0/all/0/1\">Taku Shoji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshida_N/0/1/0/all/0/1\">Noboru Yoshida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_T/0/1/0/all/0/1\">Toshihisa Tanaka</a>",
          "description": "Electroencephalography (EEG) is essential for the diagnosis of epilepsy, but\nit requires expertise and experience to identify abnormalities. It is thus\ncrucial to develop automated models for the detection of abnormal EEGs related\nto epilepsy. This paper describes the development of a novel class of compact\nand efficient convolutional neural networks (CNNs) for detecting abnormal time\nintervals and electrodes in EEGs for epilepsy. The designed model is inspired\nby a CNN developed for brain-computer interfacing called multichannel EEGNet\n(mEEGNet). Unlike the EEGNet, the proposed model, mEEGNet, has the same number\nof electrode inputs and outputs to detect abnormalities. The mEEGNet was\nevaluated with a clinical dataset consisting of 29 cases of juvenile and\nchildhood absence epilepsy labeled by a clinical expert. The labels were given\nto paroxysmal discharges visually observed in both ictal (seizure) and\ninterictal (nonseizure) intervals. Results showed that the mEEGNet detected\nabnormal EEGs with the area under the curve, F1-values, and sensitivity\nequivalent to or higher than those of existing CNNs. Moreover, the number of\nparameters is much smaller than other CNN models. To our knowledge, the dataset\nof absence epilepsy validated with machine learning through this research is\nthe largest in the literature.",
          "link": "http://arxiv.org/abs/2105.10358",
          "publishedOn": "2021-05-24T05:08:40.863Z",
          "wordCount": 639,
          "title": "Automated Detection of Abnormal EEGs in Epilepsy With a Compact and Efficient CNN Model. (arXiv:2105.10358v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianwei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xun Chen</a>",
          "description": "Train machine learning models on sensitive user data has raised increasing\nprivacy concerns in many areas. Federated learning is a popular approach for\nprivacy protection that collects the local gradient information instead of real\ndata. One way to achieve a strict privacy guarantee is to apply local\ndifferential privacy into federated learning. However, previous works do not\ngive a practical solution due to three issues. First, the noisy data is close\nto its original value with high probability, increasing the risk of information\nexposure. Second, a large variance is introduced to the estimated average,\ncausing poor accuracy. Last, the privacy budget explodes due to the high\ndimensionality of weights in deep learning models. In this paper, we proposed a\nnovel design of local differential privacy mechanism for federated learning to\naddress the abovementioned issues. It is capable of making the data more\ndistinct from its original value and introducing lower variance. Moreover, the\nproposed mechanism bypasses the curse of dimensionality by splitting and\nshuffling model updates. A series of empirical evaluations on three commonly\nused datasets, MNIST, Fashion-MNIST and CIFAR-10, demonstrate that our solution\ncan not only achieve superior deep learning performance but also provide a\nstrong privacy guarantee at the same time.",
          "link": "http://arxiv.org/abs/2007.15789",
          "publishedOn": "2021-05-24T05:08:40.812Z",
          "wordCount": 670,
          "title": "LDP-FL: Practical Private Aggregation in Federated Learning with Local Differential Privacy. (arXiv:2007.15789v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1\">Wolfgang Fuhl</a>",
          "description": "In this work, we present an alternative to conventional residual connections,\nwhich is inspired by maxout nets. This means that instead of the addition in\nresidual connections, our approach only propagates the maximum value or, in the\nleaky formulation, propagates a percentage of both. In our evaluation, we show\non different public data sets that the presented approaches are comparable to\nthe residual connections and have other interesting properties, such as better\ngeneralization with a constant batch normalization, faster learning, and also\nthe possibility to generalize without additional activation functions. In\naddition, the proposed approaches work very well if ensembles together with\nresidual networks are formed.",
          "link": "http://arxiv.org/abs/2105.10277",
          "publishedOn": "2021-05-24T05:08:40.800Z",
          "wordCount": 518,
          "title": "Maximum and Leaky Maximum Propagation. (arXiv:2105.10277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.13300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thapa_C/0/1/0/all/0/1\">Chandra Thapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jun Wen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1\">Alsharif Abuadbba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yansong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camtepe_S/0/1/0/all/0/1\">Seyit Camtepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1\">Surya Nepal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almashor_M/0/1/0/all/0/1\">Mahathir Almashor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yifeng Zheng</a>",
          "description": "The use of Artificial Intelligence (AI) to detect phishing emails is\nprimarily dependent on large-scale centralized datasets, which opens it up to a\nmyriad of privacy, trust, and legal issues. Moreover, organizations are loathed\nto share emails, given the risk of leakage of commercially sensitive\ninformation. So, it is uncommon to obtain sufficient emails to train a global\nAI model efficiently. Accordingly, privacy-preserving distributed and\ncollaborative machine learning, particularly Federated Learning (FL), is a\ndesideratum. Already prevalent in the healthcare sector, questions remain\nregarding the effectiveness and efficacy of FL-based phishing detection within\nthe context of multi-organization collaborations. To the best of our knowledge,\nthe work herein is the first to investigate the use of FL in email\nanti-phishing. This paper builds upon a deep neural network model, particularly\nRNN and BERT for phishing email detection. It analyzes the FL-entangled\nlearning performance under various settings, including balanced and\nasymmetrical data distribution. Our results corroborate comparable performance\nstatistics of FL in phishing email detection to centralized learning for\nbalanced datasets, and low organization counts. Moreover, we observe a\nvariation in performance when increasing organizational counts. For a fixed\ntotal email dataset, the global RNN based model suffers by a 1.8% accuracy drop\nwhen increasing organizational counts from 2 to 10. In contrast, BERT accuracy\nrises by 0.6% when going from 2 to 5 organizations. However, if we allow\nincreasing the overall email dataset with the introduction of new organizations\nin the FL framework, the organizational level performance is improved by\nachieving a faster convergence speed. Besides, FL suffers in its overall global\nmodel performance due to highly unstable outputs if the email dataset\ndistribution is highly asymmetric.",
          "link": "http://arxiv.org/abs/2007.13300",
          "publishedOn": "2021-05-24T05:08:40.795Z",
          "wordCount": 756,
          "title": "Evaluation of Federated Learning in Phishing Email Detection. (arXiv:2007.13300v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franci_B/0/1/0/all/0/1\">Barbara Franci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grammatico_S/0/1/0/all/0/1\">Sergio Grammatico</a>",
          "description": "Generative adversarial networks (GANs) are a class of generative models with\ntwo antagonistic neural networks: a generator and a discriminator. These two\nneural networks compete against each other through an adversarial process that\ncan be modeled as a stochastic Nash equilibrium problem. Since the associated\ntraining process is challenging, it is fundamental to design reliable\nalgorithms to compute an equilibrium. In this paper, we propose a stochastic\nrelaxed forward-backward (SRFB) algorithm for GANs and we show convergence to\nan exact solution when an increasing number of data is available. We also show\nconvergence of an averaged variant of the SRFB algorithm to a neighborhood of\nthe solution when only few samples are available. In both cases, convergence is\nguaranteed when the pseudogradient mapping of the game is monotone. This\nassumption is among the weakest known in the literature. Moreover, we apply our\nalgorithm to the image generation problem.",
          "link": "http://arxiv.org/abs/2010.10013",
          "publishedOn": "2021-05-24T05:08:40.782Z",
          "wordCount": 626,
          "title": "Training Generative Adversarial Networks via stochastic Nash games. (arXiv:2010.10013v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seurin_M/0/1/0/all/0/1\">Mathieu Seurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1\">Florian Strub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1\">Philippe Preux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1\">Olivier Pietquin</a>",
          "description": "Sparse rewards are double-edged training signals in reinforcement learning:\neasy to design but hard to optimize. Intrinsic motivation guidances have thus\nbeen developed toward alleviating the resulting exploration problem. They\nusually incentivize agents to look for new states through novelty signals. Yet,\nsuch methods encourage exhaustive exploration of the state space rather than\nfocusing on the environment's salient interaction opportunities. We propose a\nnew exploration method, called Don't Do What Doesn't Matter (DoWhaM), shifting\nthe emphasis from state novelty to state with relevant actions. While most\nactions consistently change the state when used, \\textit{e.g.} moving the\nagent, some actions are only effective in specific states, \\textit{e.g.},\n\\emph{opening} a door, \\emph{grabbing} an object. DoWhaM detects and rewards\nactions that seldom affect the environment. We evaluate DoWhaM on the\nprocedurally-generated environment MiniGrid, against state-of-the-art methods\nand show that DoWhaM greatly reduces sample complexity.",
          "link": "http://arxiv.org/abs/2105.09992",
          "publishedOn": "2021-05-24T05:08:40.748Z",
          "wordCount": 573,
          "title": "Don't Do What Doesn't Matter: Intrinsic Motivation with Action Usefulness. (arXiv:2105.09992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">C.-H. Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1\">Mohit Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Y.-C. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Quan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1\">Tomoaki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakam_T/0/1/0/all/0/1\">Tomokazu Murakam</a>",
          "description": "Camera movement and unpredictable environmental conditions like dust and wind\ninduce noise into video feeds. We observe that popular unsupervised MOT methods\nare dependent on noise-free conditions. We show that the addition of a small\namount of artificial random noise causes a sharp degradation in model\nperformance on benchmark metrics. We resolve this problem by introducing a\nrobust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed\nsingle-head attention model helps limit the negative impact of noise by\nlearning visual representations at different segment scales. AttU-Net shows\nbetter unsupervised MOT tracking performance over variational inference-based\nstate-of-the-art baselines. We evaluate our method in the MNIST and the Atari\ngame video benchmark. We also provide two extended video datasets consisting of\ncomplex visual patterns that include Kuzushiji characters and fashion images to\nvalidate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2105.10005",
          "publishedOn": "2021-05-24T05:08:40.726Z",
          "wordCount": 592,
          "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kwan Ho Ryan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Haozhi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">John Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>",
          "description": "This work attempts to provide a plausible theoretical framework that aims to\ninterpret modern deep (convolutional) networks from the principles of data\ncompression and discriminative representation. We show that for\nhigh-dimensional multi-class data, the optimal linear discriminative\nrepresentation maximizes the coding rate difference between the whole dataset\nand the average of all the subsets. We show that the basic iterative gradient\nascent scheme for optimizing the rate reduction objective naturally leads to a\nmulti-layer deep network, named ReduNet, that shares common characteristics of\nmodern deep networks. The deep layered architectures, linear and nonlinear\noperators, and even parameters of the network are all explicitly constructed\nlayer-by-layer via forward propagation, instead of learned via back\npropagation. All components of so-obtained \"white-box\" network have precise\noptimization, statistical, and geometric interpretation. Moreover, all linear\noperators of the so-derived network naturally become multi-channel convolutions\nwhen we enforce classification to be rigorously shift-invariant. The derivation\nalso indicates that such a deep convolution network is significantly more\nefficient to construct and learn in the spectral domain. Our preliminary\nsimulations and experiments clearly verify the effectiveness of both the rate\nreduction objective and the associated ReduNet. All code and data are available\nat https://github.com/Ma-Lab-Berkeley.",
          "link": "http://arxiv.org/abs/2105.10446",
          "publishedOn": "2021-05-24T05:08:40.710Z",
          "wordCount": 672,
          "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hongbo Jiang</a>",
          "description": "In intelligent transportation system, the key problem of traffic forecasting\nis how to extract the periodic temporal dependencies and complex spatial\ncorrelation. Current state-of-the-art methods for traffic flow prediction are\nbased on graph architectures and sequence learning models, but they do not\nfully exploit spatial-temporal dynamic information in traffic system.\nSpecifically, the temporal dependence of short-range is diluted by recurrent\nneural networks, and existing sequence model ignores local spatial information\nbecause the convolution operation uses global average pooling. Besides, there\nwill be some traffic accidents during the transitions of objects causing\ncongestion in the real world that trigger increased prediction deviation. To\novercome these challenges, we propose the Spatial-Temporal Conv-sequence\nLearning (STCL), in which a focused temporal block uses unidirectional\nconvolution to effectively capture short-term periodic temporal dependence, and\na spatial-temporal fusion module is able to extract the dependencies of both\ninteractions and decrease the feature dimensions. Moreover, the accidents\nfeatures impact on local traffic congestion and position encoding is employed\nto detect anomalies in complex traffic situations. We conduct extensive\nexperiments on large-scale real-world tasks and verify the effectiveness of our\nproposed method.",
          "link": "http://arxiv.org/abs/2105.10478",
          "publishedOn": "2021-05-24T05:08:40.703Z",
          "wordCount": 617,
          "title": "Spatial-Temporal Conv-sequence Learning with Accident Encoding for Traffic Flow Prediction. (arXiv:2105.10478v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1804.07209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1\">Marco Ciccone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallieri_M/0/1/0/all/0/1\">Marco Gallieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masci_J/0/1/0/all/0/1\">Jonathan Masci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osendorfer_C/0/1/0/all/0/1\">Christian Osendorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_F/0/1/0/all/0/1\">Faustino Gomez</a>",
          "description": "This paper introduces Non-Autonomous Input-Output Stable Network(NAIS-Net), a\nvery deep architecture where each stacked processing block is derived from a\ntime-invariant non-autonomous dynamical system. Non-autonomy is implemented by\nskip connections from the block input to each of the unrolled processing stages\nand allows stability to be enforced so that blocks can be unrolled adaptively\nto a pattern-dependent processing depth. NAIS-Net induces non-trivial,\nLipschitz input-output maps, even for an infinite unroll length. We prove that\nthe network is globally asymptotically stable so that for every initial\ncondition there is exactly one input-dependent equilibrium assuming $tanh$\nunits, and incrementally stable for ReL units. An efficient implementation that\nenforces the stability under derived conditions for both fully-connected and\nconvolutional layers is also presented. Experimental results show how NAIS-Net\nexhibits stability in practice, yielding a significant reduction in\ngeneralization gap compared to ResNets.",
          "link": "http://arxiv.org/abs/1804.07209",
          "publishedOn": "2021-05-24T05:08:40.619Z",
          "wordCount": 642,
          "title": "NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations. (arXiv:1804.07209v4 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10315",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1\">Ruiqi Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yuan_M/0/1/0/all/0/1\">Mingao Yuan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shang_Z/0/1/0/all/0/1\">Zuofeng Shang</a>",
          "description": "Stochastic gradient descent (SGD) and projected stochastic gradient descent\n(PSGD) are scalable algorithms to compute model parameters in unconstrained and\nconstrained optimization problems. In comparison with stochastic gradient\ndescent (SGD), PSGD forces its iterative values into the constrained parameter\nspace via projection. The convergence rate of PSGD-type estimates has been\nexhaustedly studied, while statistical properties such as asymptotic\ndistribution remain less explored. From a purely statistical point of view,\nthis paper studies the limiting distribution of PSGD-based estimate when the\ntrue parameters satisfying some linear-equality constraints. Our theoretical\nfindings reveal the role of projection played in the uncertainty of the PSGD\nestimate. As a byproduct, we propose an online hypothesis testing procedure to\ntest the linear-equality constraints. Simulation studies on synthetic data and\nan application to a real-world dataset confirm our theory.",
          "link": "http://arxiv.org/abs/2105.10315",
          "publishedOn": "2021-05-24T05:08:40.607Z",
          "wordCount": 561,
          "title": "Online Statistical Inference for Parameters Estimation with Linear-Equality Constraints. (arXiv:2105.10315v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10172",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beckh_K/0/1/0/all/0/1\">Katharina Beckh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1\">Sebastian M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobs_M/0/1/0/all/0/1\">Matthias Jakobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toborek_V/0/1/0/all/0/1\">Vanessa Toborek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_R/0/1/0/all/0/1\">Raphael Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welke_P/0/1/0/all/0/1\">Pascal Welke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houben_S/0/1/0/all/0/1\">Sebastian Houben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueden_L/0/1/0/all/0/1\">Laura von Rueden</a>",
          "description": "This survey presents an overview of integrating prior knowledge into machine\nlearning systems in order to improve explainability. The complexity of machine\nlearning models has elicited research to make them more explainable. However,\nmost explainability methods cannot provide insight beyond the given data,\nrequiring additional information about the context. We propose to harness prior\nknowledge to improve upon the explanation capabilities of machine learning\nmodels. In this paper, we present a categorization of current research into\nthree main categories which either integrate knowledge into the machine\nlearning pipeline, into the explainability method or derive knowledge from\nexplanations. To classify the papers, we build upon the existing taxonomy of\ninformed machine learning and extend it from the perspective of explainability.\nWe conclude with open challenges and research directions.",
          "link": "http://arxiv.org/abs/2105.10172",
          "publishedOn": "2021-05-24T05:08:40.595Z",
          "wordCount": 562,
          "title": "Explainable Machine Learning with Prior Knowledge: An Overview. (arXiv:2105.10172v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10360",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1\">Doudou Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1\">Tianxi Cai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_J/0/1/0/all/0/1\">Junwei Lu</a>",
          "description": "Matrix completion has attracted a lot of attention in many fields including\nstatistics, applied mathematics and electrical engineering. Most of works focus\non the independent sampling models under which the individual observed entries\nare sampled independently. Motivated by applications in the integration of\nmultiple (point-wise mutual information) PMI matrices, we propose the model\n{\\bf B}lockwise missing {\\bf E}mbedding {\\bf L}earning {\\bf T}ransformer (BELT)\nto treat row-wise/column-wise missingness. Specifically, our proposed method\naims at efficient matrix recovery when every pair of matrices from multiple\nsources has an overlap. We provide theoretical justification for the proposed\nBELT method. Simulation studies show that the method performs well in finite\nsample under a variety of configurations. The method is applied to integrate\nseveral PMI matrices built by EHR data and Chinese medical text data, which\nenables us to construct a comprehensive embedding set for CUI and Chinese with\nhigh quality.",
          "link": "http://arxiv.org/abs/2105.10360",
          "publishedOn": "2021-05-24T05:08:40.586Z",
          "wordCount": 580,
          "title": "BELT: Blockwise Missing Embedding Learning Transfomer. (arXiv:2105.10360v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tufano_M/0/1/0/all/0/1\">Michele Tufano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shao Kun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>",
          "description": "Automated unit test case generation tools facilitate test-driven development\nand support developers by suggesting tests intended to identify flaws in their\ncode. Existing approaches are usually guided by the test coverage criteria,\ngenerating synthetic test cases that are often difficult for developers to read\nor understand. In this paper we propose AthenaTest, an approach that aims to\ngenerate unit test cases by learning from real-world focal methods and\ndeveloper-written testcases. We formulate unit test case generation as a\nsequence-to-sequence learning task, adopting a two-step training procedure\nconsisting of denoising pretraining on a large unsupervised Java corpus, and\nsupervised finetuning for a downstream translation task of generating unit\ntests. We investigate the impact of natural language and source code\npretraining, as well as the focal context information surrounding the focal\nmethod. Both techniques provide improvements in terms of validation loss, with\npretraining yielding 25% relative improvement and focal context providing\nadditional 11.1% improvement. We also introduce Methods2Test, the largest\npublicly available supervised parallel corpus of unit test case methods and\ncorresponding focal methods in Java, which comprises 780K test cases mined from\n91K open-source repositories from GitHub. We evaluate AthenaTest on five\ndefects4j projects, generating 25K passing test cases covering 43.7% of the\nfocal methods with only 30 attempts. We execute the test cases, collect test\ncoverage information, and compare them with test cases generated by EvoSuite\nand GPT-3, finding that our approach outperforms GPT-3 and has comparable\ncoverage w.r.t. EvoSuite. Finally, we survey professional developers on their\npreference in terms of readability, understandability, and testing\neffectiveness of the generated tests, showing overwhelmingly preference towards\nAthenaTest.",
          "link": "http://arxiv.org/abs/2009.05617",
          "publishedOn": "2021-05-24T05:08:40.578Z",
          "wordCount": 731,
          "title": "Unit Test Case Generation with Transformers and Focal Context. (arXiv:2009.05617v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assaad_K/0/1/0/all/0/1\">Karim Assaad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devijver_E/0/1/0/all/0/1\">Emilie Devijver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaussier_E/0/1/0/all/0/1\">Eric Gaussier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ait_Bachir_A/0/1/0/all/0/1\">Ali Ait-Bachir</a>",
          "description": "We address in this study the problem of learning a summary causal graph on\ntime series with potentially different sampling rates. To do so, we first\npropose a new temporal mutual information measure defined on a window-based\nrepresentation of time series. We then show how this measure relates to an\nentropy reduction principle that can be seen as a special case of the\nProbabilistic Raising Principle. We finally combine these two ingredients in a\nPC-like algorithm to construct the summary causal graph. This algorithm is\nevaluated on several datasets that shows both its efficacy and efficiency.",
          "link": "http://arxiv.org/abs/2105.10381",
          "publishedOn": "2021-05-24T05:08:40.560Z",
          "wordCount": 527,
          "title": "Entropy-based Discovery of Summary Causal Graphs in Time Series. (arXiv:2105.10381v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/1908.06077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_Kebrya_A/0/1/0/all/0/1\">Ali Ramezani-Kebrya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1\">Fartash Faghri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1\">Ilya Markov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksenov_V/0/1/0/all/0/1\">Vitalii Aksenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Daniel M. Roy</a>",
          "description": "As the size and complexity of models and datasets grow, so does the need for\ncommunication-efficient variants of stochastic gradient descent that can be\ndeployed to perform parallel model training. One popular\ncommunication-compression method for data-parallel SGD is QSGD (Alistarh et\nal., 2017), which quantizes and encodes gradients to reduce communication\ncosts. The baseline variant of QSGD provides strong theoretical guarantees,\nhowever, for practical purposes, the authors proposed a heuristic variant which\nwe call QSGDinf, which demonstrated impressive empirical gains for distributed\ntraining of large neural networks. In this paper, we build on this work to\npropose a new gradient quantization scheme, and show that it has both stronger\ntheoretical guarantees than QSGD, and matches and exceeds the empirical\nperformance of the QSGDinf heuristic and of other compression methods.",
          "link": "http://arxiv.org/abs/1908.06077",
          "publishedOn": "2021-05-24T05:08:40.555Z",
          "wordCount": 607,
          "title": "NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization. (arXiv:1908.06077v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sifaou_H/0/1/0/all/0/1\">Houssem Sifaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+kammoun_A/0/1/0/all/0/1\">Abla kammoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alouini_M/0/1/0/all/0/1\">Mohamed-Slim Alouini</a>",
          "description": "In this paper, we study the hard and soft support vector regression\ntechniques applied to a set of $n$ linear measurements of the form\n$y_i=\\boldsymbol{\\beta}_\\star^{T}{\\bf x}_i +n_i$ where\n$\\boldsymbol{\\beta}_\\star$ is an unknown vector, $\\left\\{{\\bf\nx}_i\\right\\}_{i=1}^n$ are the feature vectors and\n$\\left\\{{n}_i\\right\\}_{i=1}^n$ model the noise. Particularly, under some\nplausible assumptions on the statistical distribution of the data, we\ncharacterize the feasibility condition for the hard support vector regression\nin the regime of high dimensions and, when feasible, derive an asymptotic\napproximation for its risk. Similarly, we study the test risk for the soft\nsupport vector regression as a function of its parameters. Our results are then\nused to optimally tune the parameters intervening in the design of hard and\nsoft support vector regression algorithms. Based on our analysis, we illustrate\nthat adding more samples may be harmful to the test performance of support\nvector regression, while it is always beneficial when the parameters are\noptimally selected. Such a result reminds a similar phenomenon observed in\nmodern learning architectures according to which optimally tuned architectures\npresent a decreasing test performance curve with respect to the number of\nsamples.",
          "link": "http://arxiv.org/abs/2105.10373",
          "publishedOn": "2021-05-24T05:08:40.533Z",
          "wordCount": 615,
          "title": "A Precise Performance Analysis of Support Vector Regression. (arXiv:2105.10373v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_A/0/1/0/all/0/1\">Atsushi Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitanda_A/0/1/0/all/0/1\">Atsushi Nitanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linchuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavazza_M/0/1/0/all/0/1\">Marc Cavazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamanishi_K/0/1/0/all/0/1\">Kenji Yamanishi</a>",
          "description": "Hyperbolic ordinal embedding (HOE) represents entities as points in\nhyperbolic space so that they agree as well as possible with given constraints\nin the form of entity i is more similar to entity j than to entity k. It has\nbeen experimentally shown that HOE can obtain representations of hierarchical\ndata such as a knowledge base and a citation network effectively, owing to\nhyperbolic space's exponential growth property. However, its theoretical\nanalysis has been limited to ideal noiseless settings, and its generalization\nerror in compensation for hyperbolic space's exponential representation ability\nhas not been guaranteed. The difficulty is that existing generalization error\nbound derivations for ordinal embedding based on the Gramian matrix do not work\nin HOE, since hyperbolic space is not inner-product space. In this paper,\nthrough our novel characterization of HOE with decomposed Lorentz Gramian\nmatrices, we provide a generalization error bound of HOE for the first time,\nwhich is at most exponential with respect to the embedding space's radius. Our\ncomparison between the bounds of HOE and Euclidean ordinal embedding shows that\nHOE's generalization error is reasonable as a cost for its exponential\nrepresentation ability.",
          "link": "http://arxiv.org/abs/2105.10475",
          "publishedOn": "2021-05-24T05:08:40.494Z",
          "wordCount": 615,
          "title": "Generalization Error Bound for Hyperbolic Ordinal Embedding. (arXiv:2105.10475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karande_H/0/1/0/all/0/1\">Hema Karande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walambe_R/0/1/0/all/0/1\">Rahee Walambe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benjamin_V/0/1/0/all/0/1\">Victor Benjamin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1\">Ketan Kotecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghu_T/0/1/0/all/0/1\">T. S. Raghu</a>",
          "description": "The evolution of electronic media is a mixed blessing. Due to the easy\naccess, low cost, and faster reach of the information, people search out and\ndevour news from online social networks. In contrast, the increasing acceptance\nof social media reporting leads to the spread of fake news. This is a minacious\nproblem that causes disputes and endangers societal stability and harmony. Fake\nnews spread has gained attention from researchers due to its vicious nature.\nproliferation of misinformation in all media, from the internet to cable news,\npaid advertising and local news outlets, has made it essential for people to\nidentify the misinformation and sort through the facts. Researchers are trying\nto analyze the credibility of information and curtail false information on such\nplatforms. Credibility is the believability of the piece of information at\nhand. Analyzing the credibility of fake news is challenging due to the intent\nof its creation and the polychromatic nature of the news. In this work, we\npropose a model for detecting fake news. Our method investigates the content of\nthe news at the early stage i.e. when the news is published but is yet to be\ndisseminated through social media. Our work interprets the content with\nautomatic feature extraction and the relevance of the text pieces. In summary,\nwe introduce stance as one of the features along with the content of the\narticle and employ the pre-trained contextualized word embeddings BERT to\nobtain the state-of-art results for fake news detection. The experiment\nconducted on the real-world dataset indicates that our model outperforms the\nprevious work and enables fake news detection with an accuracy of 95.32%.",
          "link": "http://arxiv.org/abs/2105.10272",
          "publishedOn": "2021-05-24T05:08:40.476Z",
          "wordCount": 711,
          "title": "Stance Detection with BERT Embeddings for Credibility Analysis of Information on Social Media. (arXiv:2105.10272v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10162",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sen_P/0/1/0/all/0/1\">Pinaki Sen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bhatia_A/0/1/0/all/0/1\">Amandeep Singh Bhatia</a>",
          "description": "In quantum computing, the variational quantum algorithms (VQAs) are well\nsuited for finding optimal combinations of things in specific applications\nranging from chemistry all the way to finance. The training of VQAs with\ngradient descent optimization algorithm has shown a good convergence. At an\nearly stage, the simulation of variational quantum circuits on noisy\nintermediate-scale quantum (NISQ) devices suffers from noisy outputs. Just like\nclassical deep learning, it also suffers from vanishing gradient problems. It\nis a realistic goal to study the topology of loss landscape, to visualize the\ncurvature information and trainability of these circuits in the existence of\nvanishing gradients. In this paper, we calculated the Hessian and visualized\nthe loss landscape of variational quantum classifiers at different points in\nparameter space. The curvature information of variational quantum classifiers\n(VQC) is interpreted and the loss function's convergence is shown. It helps us\nbetter understand the behavior of variational quantum circuits to tackle\noptimization problems efficiently. We investigated the variational quantum\nclassifiers via Hessian on quantum computers, started with a simple 4-bit\nparity problem to gain insight into the practical behavior of Hessian, then\nthoroughly analyzed the behavior of Hessian's eigenvalues on training the\nvariational quantum classifier for the Diabetes dataset.",
          "link": "http://arxiv.org/abs/2105.10162",
          "publishedOn": "2021-05-24T05:08:40.469Z",
          "wordCount": 635,
          "title": "Variational Quantum Classifiers Through the Lens of the Hessian. (arXiv:2105.10162v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoel_C/0/1/0/all/0/1\">Carl-Johan Hoel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_K/0/1/0/all/0/1\">Krister Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_L/0/1/0/all/0/1\">Leo Laine</a>",
          "description": "Reinforcement learning (RL) can be used to create a decision-making agent for\nautonomous driving. However, previous approaches provide only black-box\nsolutions, which do not offer information on how confident the agent is about\nits decisions. An estimate of both the aleatoric and epistemic uncertainty of\nthe agent's decisions is fundamental for real-world applications of autonomous\ndriving. Therefore, this paper introduces the Ensemble Quantile Networks (EQN)\nmethod, which combines distributional RL with an ensemble approach, to obtain a\ncomplete uncertainty estimate. The distribution over returns is estimated by\nlearning its quantile function implicitly, which gives the aleatoric\nuncertainty, whereas an ensemble of agents is trained on bootstrapped data to\nprovide a Bayesian estimation of the epistemic uncertainty. A criterion for\nclassifying which decisions that have an unacceptable uncertainty is also\nintroduced. The results show that the EQN method can balance risk and time\nefficiency in different occluded intersection scenarios, by considering the\nestimated aleatoric uncertainty. Furthermore, it is shown that the trained\nagent can use the epistemic uncertainty information to identify situations that\nthe agent has not been trained for and thereby avoid making unfounded,\npotentially dangerous, decisions outside of the training distribution.",
          "link": "http://arxiv.org/abs/2105.10266",
          "publishedOn": "2021-05-24T05:08:40.462Z",
          "wordCount": 626,
          "title": "Ensemble Quantile Networks: Uncertainty-Aware Reinforcement Learning with Applications in Autonomous Driving. (arXiv:2105.10266v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1\">Jana Kierdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1\">Immanuel Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kicherer_A/0/1/0/all/0/1\">Anna Kicherer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabawa_L/0/1/0/all/0/1\">Laura Zabawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drees_L/0/1/0/all/0/1\">Lukas Drees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>",
          "description": "The need for accurate yield estimates for viticulture is becoming more\nimportant due to increasing competition in the wine market worldwide. One of\nthe most promising methods to estimate the harvest is berry counting, as it can\nbe approached non-destructively, and its process can be automated. In this\narticle, we present a method that addresses the challenge of occluded berries\nwith leaves to obtain a more accurate estimate of the number of berries that\nwill enable a better estimate of the harvest. We use generative adversarial\nnetworks, a deep learning-based approach that generates a likely scenario\nbehind the leaves exploiting learned patterns from images with non-occluded\nberries. Our experiments show that the estimate of the number of berries after\napplying our method is closer to the manually counted reference. In contrast to\napplying a factor to the berry count, our approach better adapts to local\nconditions by directly involving the appearance of the visible berries.\nFurthermore, we show that our approach can identify which areas in the image\nshould be changed by adding new berries without explicitly requiring\ninformation about hidden areas.",
          "link": "http://arxiv.org/abs/2105.10325",
          "publishedOn": "2021-05-24T05:08:40.438Z",
          "wordCount": 644,
          "title": "Behind the leaves -- Estimation of occluded grapevine berries with conditional generative adversarial networks. (arXiv:2105.10325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozyegen_O/0/1/0/all/0/1\">Ozan Ozyegen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabe_D/0/1/0/all/0/1\">Devika Kabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevik_M/0/1/0/all/0/1\">Mucahit Cevik</a>",
          "description": "The medical domain is often subject to information overload. The digitization\nof healthcare, constant updates to online medical repositories, and increasing\navailability of biomedical datasets make it challenging to effectively analyze\nthe data. This creates additional work for medical professionals who are\nheavily dependent on medical data to complete their research and consult their\npatients. This paper aims to show how different text highlighting techniques\ncan capture relevant medical context. This would reduce the doctors' cognitive\nload and response time to patients by facilitating them in making faster\ndecisions, thus improving the overall quality of online medical services. Three\ndifferent word-level text highlighting methodologies are implemented and\nevaluated. The first method uses TF-IDF scores directly to highlight important\nparts of the text. The second method is a combination of TF-IDF scores and the\napplication of Local Interpretable Model-Agnostic Explanations to\nclassification models. The third method uses neural networks directly to make\npredictions on whether or not a word should be highlighted. The results of our\nexperiments show that the neural network approach is successful in highlighting\nmedically-relevant terms and its performance is improved as the size of the\ninput segment increases.",
          "link": "http://arxiv.org/abs/2105.10400",
          "publishedOn": "2021-05-24T05:08:40.432Z",
          "wordCount": 620,
          "title": "Word-level Text Highlighting of Medical Texts forTelehealth Services. (arXiv:2105.10400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drton_M/0/1/0/all/0/1\">Mathias Drton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shojaie_A/0/1/0/all/0/1\">Ali Shojaie</a>",
          "description": "In causal graphical models based on directed acyclic graphs (DAGs), directed\npaths represent causal pathways between the corresponding variables. The\nvariable at the beginning of such a path is referred to as an ancestor of the\nvariable at the end of the path. Ancestral relations between variables play an\nimportant role in causal modeling. In existing literature on structure\nlearning, these relations are usually deduced from learned structures and used\nfor orienting edges or formulating constraints of the space of possible DAGs.\nHowever, they are usually not posed as immediate target of inference. In this\nwork we investigate the graphical characterization of ancestral relations via\nCPDAGs and d-separation relations. We propose a framework that can learn\ndefinite non-ancestral relations without first learning the skeleton. This\nframe-work yields structural information that can be used in both score- and\nconstraint-based algorithms to learn causal DAGs more efficiently.",
          "link": "http://arxiv.org/abs/2105.10350",
          "publishedOn": "2021-05-24T05:08:40.424Z",
          "wordCount": 567,
          "title": "Definite Non-Ancestral Relations and Structure Learning. (arXiv:2105.10350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Real_R/0/1/0/all/0/1\">Ric Real</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopsill_J/0/1/0/all/0/1\">James Gopsill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1\">David Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snider_C/0/1/0/all/0/1\">Chris Snider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hicks_B/0/1/0/all/0/1\">Ben Hicks</a>",
          "description": "Prior work has shown Convolutional Neural Networks (CNNs) trained on\nsurrogate Computer Aided Design (CAD) models are able to detect and classify\nreal-world artefacts from photographs. The applications of which support\ntwinning of digital and physical assets in design, including rapid extraction\nof part geometry from model repositories, information search \\& retrieval and\nidentifying components in the field for maintenance, repair, and recording. The\nperformance of CNNs in classification tasks have been shown dependent on\ntraining data set size and number of classes. Where prior works have used\nrelatively small surrogate model data sets ($<100$ models), the question\nremains as to the ability of a CNN to differentiate between models in\nincreasingly large model repositories. This paper presents a method for\ngenerating synthetic image data sets from online CAD model repositories, and\nfurther investigates the capacity of an off-the-shelf CNN architecture trained\non synthetic data to classify models as class size increases. 1,000 CAD models\nwere curated and processed to generate large scale surrogate data sets,\nfeaturing model coverage at steps of 10$^{\\circ}$, 30$^{\\circ}$, 60$^{\\circ}$,\nand 120$^{\\circ}$ degrees. The findings demonstrate the capability of computer\nvision algorithms to classify artefacts in model repositories of up to 200,\nbeyond this point the CNN's performance is observed to deteriorate\nsignificantly, limiting its present ability for automated twinning of physical\nto digital artefacts. Although, a match is more often found in the top-5\nresults showing potential for information search and retrieval on large\nrepositories of surrogate models.",
          "link": "http://arxiv.org/abs/2105.10448",
          "publishedOn": "2021-05-24T05:08:40.405Z",
          "wordCount": 704,
          "title": "Distinguishing artefacts: evaluating the saturation point of convolutional neural networks. (arXiv:2105.10448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1\">Caglar Gulcehre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paine_T/0/1/0/all/0/1\">Tom Le Paine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gretton_A/0/1/0/all/0/1\">Arthur Gretton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1\">Nando de Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "We show that the popular reinforcement learning (RL) strategy of estimating\nthe state-action value (Q-function) by minimizing the mean squared Bellman\nerror leads to a regression problem with confounding, the inputs and output\nnoise being correlated. Hence, direct minimization of the Bellman error can\nresult in significantly biased Q-function estimates. We explain why fixing the\ntarget Q-network in Deep Q-Networks and Fitted Q Evaluation provides a way of\novercoming this confounding, thus shedding new light on this popular but not\nwell understood trick in the deep RL literature. An alternative approach to\naddress confounding is to leverage techniques developed in the causality\nliterature, notably instrumental variables (IV). We bring together here the\nliterature on IV and RL by investigating whether IV approaches can lead to\nimproved Q-function estimates. This paper analyzes and compares a wide range of\nrecent IV methods in the context of offline policy evaluation (OPE), where the\ngoal is to estimate the value of a policy using logged data only. By applying\ndifferent IV techniques to OPE, we are not only able to recover previously\nproposed OPE methods such as model-based techniques but also to obtain\ncompetitive new techniques. We find empirically that state-of-the-art OPE\nmethods are closely matched in performance by some IV methods such as AGMM,\nwhich were not developed for OPE. We open-source all our code and datasets at\nhttps://github.com/liyuan9988/IVOPEwithACME.",
          "link": "http://arxiv.org/abs/2105.10148",
          "publishedOn": "2021-05-24T05:08:40.397Z",
          "wordCount": 666,
          "title": "On Instrumental Variable Regression for Deep Offline Policy Evaluation. (arXiv:2105.10148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10239",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ambati_A/0/1/0/all/0/1\">Anirudh Ambati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>",
          "description": "Covid-19 global pandemic continues to devastate health care systems across\nthe world. In many countries, the 2nd wave is very severe. Economical and rapid\ntesting, as well as diagnosis, is urgently needed to control the pandemic. At\npresent, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR)\ntesting can be the fastest, scalable, and non-invasive method. The existing\nmethods suffer due to the limited CXR samples available from Covid-19. Thus,\ninspired by the limitations of the open-source work in this field, we propose\nattention guided contrastive CNN architecture (AC-CovidNet) for Covid-19\ndetection in CXR images. The proposed method learns the robust and\ndiscriminative features with the help of contrastive loss. Moreover, the\nproposed method gives more importance to the infected regions as guided by the\nattention mechanism. We compute the sensitivity of the proposed method over the\npublicly available Covid-19 dataset. It is observed that the proposed\nAC-CovidNet exhibits very promising performance as compared to the existing\nmethods even with limited training data. It can tackle the bottleneck of CXR\nCovid-19 datasets being faced by the researchers. The code used in this paper\nis released publicly at \\url{https://github.com/shivram1987/AC-CovidNet/}.",
          "link": "http://arxiv.org/abs/2105.10239",
          "publishedOn": "2021-05-24T05:08:40.383Z",
          "wordCount": 687,
          "title": "AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images. (arXiv:2105.10239v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.14709",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1\">Sebastian Goldt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Loureiro_B/0/1/0/all/0/1\">Bruno Loureiro</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reeves_G/0/1/0/all/0/1\">Galen Reeves</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Krzakala_F/0/1/0/all/0/1\">Florent Krzakala</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mezard_M/0/1/0/all/0/1\">Marc M&#xe9;zard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zdeborova_L/0/1/0/all/0/1\">Lenka Zdeborov&#xe1;</a>",
          "description": "Understanding the impact of data structure on the computational tractability\nof learning is a key challenge for the theory of neural networks. Many\ntheoretical works do not explicitly model training data, or assume that inputs\nare drawn component-wise independently from some simple probability\ndistribution. Here, we go beyond this simple paradigm by studying the\nperformance of neural networks trained on data drawn from pre-trained\ngenerative models. This is possible due to a Gaussian equivalence stating that\nthe key metrics of interest, such as the training and test errors, can be fully\ncaptured by an appropriately chosen Gaussian model. We provide three strands of\nrigorous, analytical and numerical evidence corroborating this equivalence.\nFirst, we establish rigorous conditions for the Gaussian equivalence to hold in\nthe case of single-layer generative models, as well as deterministic rates for\nconvergence in distribution. Second, we leverage this equivalence to derive a\nclosed set of equations describing the generalisation performance of two widely\nstudied machine learning problems: two-layer neural networks trained using\none-pass stochastic gradient descent, and full-batch pre-learned features or\nkernel methods. Finally, we perform experiments demonstrating how our theory\napplies to deep, pre-trained generative models. These results open a viable\npath to the theoretical study of machine learning models with realistic data.",
          "link": "http://arxiv.org/abs/2006.14709",
          "publishedOn": "2021-05-24T05:08:40.355Z",
          "wordCount": 699,
          "title": "The Gaussian equivalence of generative models for learning with shallow neural networks. (arXiv:2006.14709v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jennifer C. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>",
          "description": "Probes are models devised to investigate the encoding of knowledge -- e.g.\nsyntactic structure -- in contextual representations. Probes are often designed\nfor simplicity, which has led to restrictions on probe design that may not\nallow for the full exploitation of the structure of encoded information; one\nsuch restriction is linearity. We examine the case of a structural probe\n(Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic\nstructure in contextual representations through learning only linear\ntransformations. By observing that the structural probe learns a metric, we are\nable to kernelize it and develop a novel non-linear variant with an identical\nnumber of parameters. We test on 6 languages and find that the radial-basis\nfunction (RBF) kernel, in conjunction with regularization, achieves a\nstatistically significant improvement over the baseline in all languages --\nimplying that at least part of the syntactic knowledge is encoded non-linearly.\nWe conclude by discussing how the RBF kernel resembles BERT's self-attention\nlayers and speculate that this resemblance leads to the RBF-based probe's\nstronger performance.",
          "link": "http://arxiv.org/abs/2105.10185",
          "publishedOn": "2021-05-24T05:08:40.348Z",
          "wordCount": 600,
          "title": "A Non-Linear Structural Probe. (arXiv:2105.10185v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1\">Leman Akoglu</a>",
          "description": "Anomaly mining is an important problem that finds numerous applications in\nvarious real world domains such as environmental monitoring, cybersecurity,\nfinance, healthcare and medicine, to name a few. In this article, I focus on\ntwo areas, (1) point-cloud and (2) graph-based anomaly mining. I aim to present\na broad view of each area, and discuss classes of main research problems,\nrecent trends and future directions. I conclude with key take-aways and\noverarching open problems.",
          "link": "http://arxiv.org/abs/2105.10077",
          "publishedOn": "2021-05-24T05:08:40.331Z",
          "wordCount": 499,
          "title": "Anomaly Mining -- Past, Present and Future. (arXiv:2105.10077v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">S.K. Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paoletti_M/0/1/0/all/0/1\">M.E. Paoletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haut_J/0/1/0/all/0/1\">J.M. Haut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">S.R. Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">P. Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1\">A. Plaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_B/0/1/0/all/0/1\">B.B. Chaudhuri</a>",
          "description": "Convolutional neural networks (CNNs) are trained using stochastic gradient\ndescent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam)\noptimizer has become very popular due to its adaptive momentum, which tackles\nthe dying gradient problem of SGD. Nevertheless, existing optimizers are still\nunable to exploit the optimization curvature information efficiently. This\npaper proposes a new AngularGrad optimizer that considers the behavior of the\ndirection/angle of consecutive gradients. This is the first attempt in the\nliterature to exploit the gradient angular information apart from its\nmagnitude. The proposed AngularGrad generates a score to control the step size\nbased on the gradient angular information of previous iterations. Thus, the\noptimization steps become smoother as a more accurate step size of immediate\npast gradients is captured through the angular information. Two variants of\nAngularGrad are developed based on the use of Tangent or Cosine functions for\ncomputing the gradient angular information. Theoretically, AngularGrad exhibits\nthe same regret bound as Adam for convergence purposes. Nevertheless, extensive\nexperiments conducted on benchmark data sets against state-of-the-art methods\nreveal a superior performance of AngularGrad. The source code will be made\npublicly available at: https://github.com/mhaut/AngularGrad.",
          "link": "http://arxiv.org/abs/2105.10190",
          "publishedOn": "2021-05-24T05:08:40.312Z",
          "wordCount": 641,
          "title": "AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks. (arXiv:2105.10190v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwinn_L/0/1/0/all/0/1\">Leo Schwinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raab_R/0/1/0/all/0/1\">Ren&#xe9; Raab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">An Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1\">Dario Zanca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1\">Bjoern Eskofier</a>",
          "description": "Progress in making neural networks more robust against adversarial attacks is\nmostly marginal, despite the great efforts of the research community. Moreover,\nthe robustness evaluation is often imprecise, making it difficult to identify\npromising approaches. We analyze the classification decisions of 19 different\nstate-of-the-art neural networks trained to be robust against adversarial\nattacks. Our findings suggest that current untargeted adversarial attacks\ninduce misclassification towards only a limited amount of different classes.\nAdditionally, we observe that both over- and under-confidence in model\npredictions result in an inaccurate assessment of model robustness. Based on\nthese observations, we propose a novel loss function for adversarial attacks\nthat consistently improves attack success rate compared to prior loss functions\nfor 19 out of 19 analyzed models.",
          "link": "http://arxiv.org/abs/2105.10304",
          "publishedOn": "2021-05-24T05:08:40.277Z",
          "wordCount": 551,
          "title": "Exploring Robust Misclassifications of Neural Networks to Enhance Adversarial Attacks. (arXiv:2105.10304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1\">Andrew Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1\">Nga Than</a>",
          "description": "Topic models are widely used in studying social phenomena. We conduct a\ncomparative study examining state-of-the-art neural versus non-neural topic\nmodels, performing a rigorous quantitative and qualitative assessment on a\ndataset of tweets about the COVID-19 pandemic. Our results show that not only\ndo neural topic models outperform their classical counterparts on standard\nevaluation metrics, but they also produce more coherent topics, which are of\ngreat benefit when studying complex social problems. We also propose a novel\nregularization term for neural topic models, which is designed to address the\nwell-documented problem of mode collapse, and demonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2105.10165",
          "publishedOn": "2021-05-24T05:08:40.269Z",
          "wordCount": 605,
          "title": "Have you tried Neural Topic Models? Comparative Analysis of Neural and Non-Neural Topic Models with Application to COVID-19 Twitter Data. (arXiv:2105.10165v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1\">Atul Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1\">Anshul Nasery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "Recently, unsupervised parsing of syntactic trees has gained considerable\nattention. A prototypical approach to such unsupervised parsing employs\nreinforcement learning and auto-encoders. However, no mechanism ensures that\nthe learnt model leverages the well-understood language grammar. We propose an\napproach that utilizes very generic linguistic knowledge of the language\npresent in the form of syntactic rules, thus inducing better syntactic\nstructures. We introduce a novel formulation that takes advantage of the\nsyntactic grammar rules and is independent of the base system. We achieve new\nstate-of-the-art results on two benchmarks datasets, MNLI and WSJ. The source\ncode of the paper is available at https://github.com/anshuln/Diora_with_rules.",
          "link": "http://arxiv.org/abs/2105.10193",
          "publishedOn": "2021-05-24T05:08:40.263Z",
          "wordCount": 543,
          "title": "Rule Augmented Unsupervised Constituency Parsing. (arXiv:2105.10193v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rohrer_T/0/1/0/all/0/1\">Tobias Rohrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolberg_J/0/1/0/all/0/1\">Jascha Kolberg</a>",
          "description": "The need for reliable systems to determine fingerprint presentation attacks\ngrows with the rising use of the fingerprint for authentication. This work\npresents a new approach to single-class classification for software-based\nfingerprint presentation attach detection. The described method utilizes a\nWasserstein GAN to apply transfer learning to a deep convolutional autoencoder.\nBy doing so, the autoencoder could be pretrained and finetuned on the\nLivDet2021 Dermalog sensor dataset with only 1122 bona fide training samples.\nWithout making use of any presentation attack samples, the model could archive\nan average classification error rate of 16.79%. The Wasserstein GAN implemented\nto pretrain the autoencoders weights can further be used to generate\nrealistic-looking artificial fingerprint patches. Extensive testing of\ndifferent autoencoder architectures and hyperparameters led to coarse\narchitectural guidelines as well as multiple implementations which can be\nutilized for future work.",
          "link": "http://arxiv.org/abs/2105.10213",
          "publishedOn": "2021-05-24T05:08:40.241Z",
          "wordCount": 566,
          "title": "GAN pretraining for deep convolutional autoencoders applied to Software-based Fingerprint Presentation Attack Detection. (arXiv:2105.10213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabanelli_E/0/1/0/all/0/1\">Enrico Tabanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1\">Davide Brunelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acquaviva_A/0/1/0/all/0/1\">Andrea Acquaviva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>",
          "description": "Non-Intrusive Load Monitoring (NILM) enables the disaggregation of the global\npower consumption of multiple loads, taken from a single smart electrical\nmeter, into appliance-level details. State-of-the-Art approaches are based on\nMachine Learning methods and exploit the fusion of time- and frequency-domain\nfeatures from current and voltage sensors. Unfortunately, these methods are\ncompute-demanding and memory-intensive. Therefore, running low-latency NILM on\nlow-cost, resource-constrained MCU-based meters is currently an open challenge.\nThis paper addresses the optimization of the feature spaces as well as the\ncomputational and storage cost reduction needed for executing State-of-the-Art\n(SoA) NILM algorithms on memory- and compute-limited MCUs. We compare four\nsupervised learning techniques on different classification scenarios and\ncharacterize the overall NILM pipeline's implementation on a MCU-based Smart\nMeasurement Node. Experimental results demonstrate that optimizing the feature\nspace enables edge MCU-based NILM with 95.15% accuracy, resulting in a small\ndrop compared to the most-accurate feature vector deployment (96.19%) while\nachieving up to 5.45x speed-up and 80.56% storage reduction. Furthermore, we\nshow that low-latency NILM relying only on current measurements reaches almost\n80% accuracy, allowing a major cost reduction by removing voltage sensors from\nthe hardware design.",
          "link": "http://arxiv.org/abs/2105.10302",
          "publishedOn": "2021-05-24T05:08:40.168Z",
          "wordCount": 628,
          "title": "Trimming Feature Extraction and Inference for MCU-based Edge NILM: a Systematic Approach. (arXiv:2105.10302v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10238",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zapaishchykova_A/0/1/0/all/0/1\">Anna Zapaishchykova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreizin_D/0/1/0/all/0/1\">David Dreizin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoshuo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jie Ying Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roohi_S/0/1/0/all/0/1\">Shahrooz Faghih Roohi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>",
          "description": "Pelvic ring disruptions result from blunt injury mechanisms and are often\nfound in patients with multi-system trauma. To grade pelvic fracture severity\nin trauma victims based on whole-body CT, the Tile AO/OTA classification is\nfrequently used. Due to the high volume of whole-body trauma CTs generated in\nbusy trauma centers, an automated approach to Tile classification would provide\nsubstantial value, e.,g., to prioritize the reading queue of the attending\ntrauma radiologist. In such scenario, an automated method should perform\ngrading based on a transparent process and based on interpretable features to\nenable interaction with human readers and lower their workload by offering\ninsights from a first automated read of the scan. This paper introduces an\nautomated yet interpretable pelvic trauma decision support system to assist\nradiologists in fracture detection and Tile grade classification. The method\noperates similarly to human interpretation of CT scans and first detects\ndistinct pelvic fractures on CT with high specificity using a Faster-RCNN model\nthat are then interpreted using a structural causal model based on clinical\nbest practices to infer an initial Tile grade. The Bayesian causal model and\nfinally, the object detector are then queried for likely co-occurring fractures\nthat may have been rejected initially due to the highly specific operating\npoint of the detector, resulting in an updated list of detected fractures and\ncorresponding final Tile grade. Our method is transparent in that it provides\nfinding location and type using the object detector, as well as information on\nimportant counterfactuals that would invalidate the system's recommendation and\nachieves an AUC of 83.3%/85.1% for translational/rotational instability.\nDespite being designed for human-machine teaming, our approach does not\ncompromise on performance compared to previous black-box approaches.",
          "link": "http://arxiv.org/abs/2105.10238",
          "publishedOn": "2021-05-24T05:08:40.152Z",
          "wordCount": 736,
          "title": "An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma. (arXiv:2105.10238v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morik_K/0/1/0/all/0/1\">Katharina Morik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heppe_L/0/1/0/all/0/1\">Lukas Heppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_D/0/1/0/all/0/1\">Danny Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_R/0/1/0/all/0/1\">Raphael Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mucke_S/0/1/0/all/0/1\">Sascha M&#xfc;cke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_A/0/1/0/all/0/1\">Andreas Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobs_M/0/1/0/all/0/1\">Matthias Jakobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piatkowski_N/0/1/0/all/0/1\">Nico Piatkowski</a>",
          "description": "Machine learning applications have become ubiquitous. Their applications from\nmachine embedded control in production over process optimization in diverse\nareas (e.g., traffic, finance, sciences) to direct user interactions like\nadvertising and recommendations. This has led to an increased effort of making\nmachine learning trustworthy. Explainable and fair AI have already matured.\nThey address knowledgeable users and application engineers. However, there are\nusers that want to deploy a learned model in a similar way as their washing\nmachine. These stakeholders do not want to spend time understanding the model.\nInstead, they want to rely on guaranteed properties. What are the relevant\nproperties? How can they be expressed to stakeholders without presupposing\nmachine learning knowledge? How can they be guaranteed for a certain\nimplementation of a model? These questions move far beyond the current\nstate-of-the-art and we want to address them here. We propose a unified\nframework that certifies learning methods via care labels. They are easy to\nunderstand and draw inspiration from well-known certificates like textile\nlabels or property cards of electronic devices. Our framework considers both,\nthe machine learning theory and a given implementation. We test the\nimplementation's compliance with theoretical properties and bounds. In this\npaper, we illustrate care labels by a prototype implementation of a\ncertification suite for a selection of probabilistic graphical models.",
          "link": "http://arxiv.org/abs/2105.10197",
          "publishedOn": "2021-05-24T05:08:40.117Z",
          "wordCount": 666,
          "title": "Yes We Care! -- Certification for Machine Learning Methods through the Care Label Framework. (arXiv:2105.10197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1\">Matthew Wicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurenti_L/0/1/0/all/0/1\">Luca Laurenti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patane_A/0/1/0/all/0/1\">Andrea Patane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paoletti_N/0/1/0/all/0/1\">Nicola Paoletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1\">Alessandro Abate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1\">Marta Kwiatkowska</a>",
          "description": "We consider the problem of computing reach-avoid probabilities for iterative\npredictions made with Bayesian neural network (BNN) models. Specifically, we\nleverage bound propagation techniques and backward recursion to compute lower\nbounds for the probability that trajectories of the BNN model reach a given set\nof states while avoiding a set of unsafe states. We use the lower bounds in the\ncontext of control and reinforcement learning to provide safety certification\nfor given control policies, as well as to synthesize control policies that\nimprove the certification bounds. On a set of benchmarks, we demonstrate that\nour framework can be employed to certify policies over BNNs predictions for\nproblems of more than $10$ dimensions, and to effectively synthesize policies\nthat significantly increase the lower bound on the satisfaction probability.",
          "link": "http://arxiv.org/abs/2105.10134",
          "publishedOn": "2021-05-24T05:08:40.074Z",
          "wordCount": 560,
          "title": "Certification of Iterative Predictions in Bayesian Neural Networks. (arXiv:2105.10134v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1\">Ioannis Karamouzas</a>",
          "description": "We present a simple and intuitive approach for interactive control of\nphysically simulated characters. Our work builds upon generative adversarial\nnetworks (GAN) and reinforcement learning, and introduces an imitation learning\nframework where an ensemble of classifiers and an imitation policy are trained\nin tandem given pre-processed reference clips. The classifiers are trained to\ndiscriminate the reference motion from the motion generated by the imitation\npolicy, while the policy is rewarded for fooling the discriminators. Using our\nGAN-based approach, multiple motor control policies can be trained separately\nto imitate different behaviors. In runtime, our system can respond to external\ncontrol signal provided by the user and interactively switch between different\npolicies. Compared to existing methods, our proposed approach has the following\nattractive properties: 1) achieves state-of-the-art imitation performance\nwithout manually designing and fine tuning a reward function; 2) directly\ncontrols the character without having to track any target reference pose\nexplicitly or implicitly through a phase state; and 3) supports interactive\npolicy switching without requiring any motion generation or motion matching\nmechanism. We highlight the applicability of our approach in a range of\nimitation and interactive control tasks, while also demonstrating its ability\nto withstand external perturbations as well as to recover balance. Overall, our\napproach generates high-fidelity motion, has low runtime cost, and can be\neasily integrated into interactive applications and games.",
          "link": "http://arxiv.org/abs/2105.10066",
          "publishedOn": "2021-05-24T05:08:40.063Z",
          "wordCount": 649,
          "title": "A GAN-Like Approach for Physics-Based Imitation Learning and Interactive Character Control. (arXiv:2105.10066v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chih-Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hsuan-Cheng Liao</a>",
          "description": "Within the context of autonomous driving, safety-related metrics for deep\nneural networks have been widely studied for image classification and object\ndetection. In this paper, we further consider safety-aware correctness and\nrobustness metrics specialized for semantic segmentation. The novelty of our\nproposal is to move beyond pixel-level metrics: Given two images with each\nhaving N pixels being class-flipped, the designed metrics should, depending on\nthe clustering of pixels being class-flipped or the location of occurrence,\nreflect a different level of safety criticality. The result evaluated on an\nautonomous driving dataset demonstrates the validity and practicality of our\nproposed methodology.",
          "link": "http://arxiv.org/abs/2105.10142",
          "publishedOn": "2021-05-24T05:08:40.037Z",
          "wordCount": 531,
          "title": "Safety Metrics for Semantic Segmentation in Autonomous Driving. (arXiv:2105.10142v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10019",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Poh_D/0/1/0/all/0/1\">Daniel Poh</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Lim_B/0/1/0/all/0/1\">Bryan Lim</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "The performance of a cross-sectional currency strategy depends crucially on\naccurately ranking instruments prior to portfolio construction. While this\nranking step is traditionally performed using heuristics, or by sorting outputs\nproduced by pointwise regression or classification models, Learning to Rank\nalgorithms have recently presented themselves as competitive and viable\nalternatives. Despite improving ranking accuracy on average however, these\ntechniques do not account for the possibility that assets positioned at the\nextreme ends of the ranked list -- which are ultimately used to construct the\nlong/short portfolios -- can assume different distributions in the input space,\nand thus lead to sub-optimal strategy performance. Drawing from research in\nInformation Retrieval that demonstrates the utility of contextual information\nembedded within top-ranked documents to learn the query's characteristics to\nimprove ranking, we propose an analogous approach: exploiting the features of\nboth out- and under-performing instruments to learn a model for refining the\noriginal ranked list. Under a re-ranking framework, we adapt the Transformer\narchitecture to encode the features of extreme assets for refining our\nselection of long/short instruments obtained with an initial retrieval.\nBacktesting on a set of 31 currencies, our proposed methodology significantly\nboosts Sharpe ratios -- by approximately 20% over the original LTR algorithms\nand double that of traditional baselines.",
          "link": "http://arxiv.org/abs/2105.10019",
          "publishedOn": "2021-05-24T05:08:40.020Z",
          "wordCount": 655,
          "title": "Enhancing Cross-Sectional Currency Strategies by Ranking Refinement with Transformer-based Architectures. (arXiv:2105.10019v1 [q-fin.PM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09994",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Korba_A/0/1/0/all/0/1\">Anna Korba</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aubin_Frankowski_P/0/1/0/all/0/1\">Pierre-Cyril Aubin-Frankowski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Majewski_S/0/1/0/all/0/1\">Szymon Majewski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1\">Pierre Ablin</a>",
          "description": "Among dissimilarities between probability distributions, the Kernel Stein\nDiscrepancy (KSD) has received much interest recently. We investigate the\nproperties of its Wasserstein gradient flow to approximate a target probability\ndistribution $\\pi$ on $\\mathbb{R}^d$, known up to a normalization constant.\nThis leads to a straightforwardly implementable, deterministic score-based\nmethod to sample from $\\pi$, named KSD Descent, which uses a set of particles\nto approximate $\\pi$. Remarkably, owing to a tractable loss function, KSD\nDescent can leverage robust parameter-free optimization schemes such as L-BFGS;\nthis contrasts with other popular particle-based schemes such as the Stein\nVariational Gradient Descent algorithm. We study the convergence properties of\nKSD Descent and demonstrate its practical relevance. However, we also highlight\nfailure cases by showing that the algorithm can get stuck in spurious local\nminima.",
          "link": "http://arxiv.org/abs/2105.09994",
          "publishedOn": "2021-05-24T05:08:40.003Z",
          "wordCount": 547,
          "title": "Kernel Stein Discrepancy Descent. (arXiv:2105.09994v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1\">Qiuxia Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yannan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>",
          "description": "Deep learning (DL) has achieved unprecedented success in a variety of tasks.\nHowever, DL systems are notoriously difficult to test and debug due to the lack\nof explainability of DL models and the huge test input space to cover.\nGenerally speaking, it is relatively easy to collect a massive amount of test\ndata, but the labeling cost can be quite high. Consequently, it is essential to\nconduct test selection and label only those selected \"high quality\"\nbug-revealing test inputs for test cost reduction.\n\nIn this paper, we propose a novel test prioritization technique that brings\norder into the unlabeled test instances according to their bug-revealing\ncapabilities, namely TestRank. Different from existing solutions, TestRank\nleverages both intrinsic attributes and contextual attributes of test instances\nwhen prioritizing them. To be specific, we first build a similarity graph on\ntest instances and training samples, and we conduct graph-based semi-supervised\nlearning to extract contextual features. Then, for a particular test instance,\nthe contextual features extracted from the graph neural network (GNN) and the\nintrinsic features obtained with the DL model itself are combined to predict\nits bug-revealing probability. Finally, TestRank prioritizes unlabeled test\ninstances in descending order of the above probability value. We evaluate the\nperformance of TestRank on a variety of image classification datasets.\nExperimental results show that the debugging efficiency of our method\nsignificantly outperforms existing test prioritization techniques.",
          "link": "http://arxiv.org/abs/2105.10113",
          "publishedOn": "2021-05-24T05:08:39.980Z",
          "wordCount": 659,
          "title": "TestRank: Bringing Order into Unlabeled Test Instances for Deep Learning Tasks. (arXiv:2105.10113v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berrada_L/0/1/0/all/0/1\">Leonard Berrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>",
          "description": "This is a short note on the performance of the ALI-G algorithm (Berrada et\nal., 2020) as reported in (Loizou et al., 2021). ALI-G (Berrada et al., 2020)\nand SPS (Loizou et al., 2021) are both adaptations of the Polyak step-size to\noptimize machine learning models that can interpolate the training data. The\nmain algorithmic differences are that (1) SPS employs a multiplicative constant\nin the denominator of the learning-rate while ALI-G uses an additive constant,\nand (2) SPS uses an iteration-dependent maximal learning-rate while ALI-G uses\na constant one. There are also differences in the analysis provided by the two\nworks, with less restrictive assumptions proposed in (Loizou et al., 2021). In\ntheir experiments, (Loizou et al., 2021) did not use momentum for ALI-G (which\nis a standard part of the algorithm) or standard hyper-parameter tuning (for\ne.g. learning-rate and regularization). Hence this note as a reference for the\nimproved performance that ALI-G can obtain with well-chosen hyper-parameters.\nIn particular, we show that when training a ResNet-34 on CIFAR-10 and\nCIFAR-100, the performance of ALI-G can reach respectively 93.5% (+6%) and 76%\n(+8%) with a very small amount of tuning. Thus ALI-G remains a very competitive\nmethod for training interpolating neural networks.",
          "link": "http://arxiv.org/abs/2105.10011",
          "publishedOn": "2021-05-24T05:08:39.973Z",
          "wordCount": 627,
          "title": "Comment on Stochastic Polyak Step-Size: Performance of ALI-G. (arXiv:2105.10011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avdiukhin_D/0/1/0/all/0/1\">Dmitrii Avdiukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaroslavtsev_G/0/1/0/all/0/1\">Grigory Yaroslavtsev</a>",
          "description": "Stochastic gradient descent (SGD) is a prevalent optimization technique for\nlarge-scale distributed machine learning. While SGD computation can be\nefficiently divided between multiple machines, communication typically becomes\na bottleneck in the distributed setting. Gradient compression methods can be\nused to alleviate this problem, and a recent line of work shows that SGD\naugmented with gradient compression converges to an $\\varepsilon$-first-order\nstationary point. In this paper we extend these results to convergence to an\n$\\varepsilon$-second-order stationary point ($\\varepsilon$-SOSP), which is to\nthe best of our knowledge the first result of this type. In addition, we show\nthat, when the stochastic gradient is not Lipschitz, compressed SGD with\nRandomK compressor converges to an $\\varepsilon$-SOSP with the same number of\niterations as uncompressed SGD [Jin et al.,2021] (JACM), while improving the\ntotal communication by a factor of $\\tilde \\Theta(\\sqrt{d}\n\\varepsilon^{-3/4})$, where $d$ is the dimension of the optimization problem.\nWe present additional results for the cases when the compressor is arbitrary\nand when the stochastic gradient is Lipschitz.",
          "link": "http://arxiv.org/abs/2105.10090",
          "publishedOn": "2021-05-24T05:08:39.954Z",
          "wordCount": 591,
          "title": "Escaping Saddle Points with Compressed SGD. (arXiv:2105.10090v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yiming Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dengzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_Y/0/1/0/all/0/1\">Yingan Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Z/0/1/0/all/0/1\">Zhengrong Ruan</a>",
          "description": "The energy consumption of the HVAC system accounts for a significant portion\nof the energy consumption of the public building system, and using an efficient\nenergy consumption prediction model can assist it in carrying out effective\nenergy-saving transformation. Unlike the traditional energy consumption\nprediction model, this paper extracts features from large data sets using\nXGBoost, trains them separately to obtain multiple models, then fuses them with\nLightGBM's independent prediction results using MAE, infers energy consumption\nrelated variables, and successfully applies this model to the self-developed\nInternet of Things platform.",
          "link": "http://arxiv.org/abs/2105.09945",
          "publishedOn": "2021-05-24T05:08:39.944Z",
          "wordCount": 522,
          "title": "XGBoost energy consumption prediction based on multi-system data HVAC. (arXiv:2105.09945v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhuangdi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Junyuan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiayu Zhou</a>",
          "description": "Federated Learning (FL) is a decentralized machine-learning paradigm, in\nwhich a global server iteratively averages the model parameters of local users\nwithout accessing their data. User heterogeneity has imposed significant\nchallenges to FL, which can incur drifted global models that are slow to\nconverge. Knowledge Distillation has recently emerged to tackle this issue, by\nrefining the server model using aggregated knowledge from heterogeneous users,\nother than directly averaging their model parameters. This approach, however,\ndepends on a proxy dataset, making it impractical unless such a prerequisite is\nsatisfied. Moreover, the ensemble knowledge is not fully utilized to guide\nlocal model learning, which may in turn affect the quality of the aggregated\nmodel. Inspired by the prior art, we propose a data-free knowledge\ndistillation} approach to address heterogeneous FL, where the server learns a\nlightweight generator to ensemble user information in a data-free manner, which\nis then broadcasted to users, regulating local training using the learned\nknowledge as an inductive bias. Empirical studies powered by theoretical\nimplications show that, our approach facilitates FL with better generalization\nperformance using fewer communication rounds, compared with the\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2105.10056",
          "publishedOn": "2021-05-24T05:08:39.924Z",
          "wordCount": 610,
          "title": "Data-Free Knowledge Distillation for Heterogeneous Federated Learning. (arXiv:2105.10056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1\">Diego Klabjan</a>",
          "description": "Neural network pruning techniques reduce the number of parameters without\ncompromising predicting ability of a network. Many algorithms have been\ndeveloped for pruning both over-parameterized fully-connected networks (FCNs)\nand convolutional neural networks (CNNs), but analytical studies of\ncapabilities and compression ratios of such pruned sub-networks are lacking. We\ntheoretically study the performance of two pruning techniques (random and\nmagnitude-based) on FCNs and CNNs. Given a target network {whose weights are\nindependently sampled from appropriate distributions}, we provide a universal\napproach to bound the gap between a pruned and the target network in a\nprobabilistic sense. The results establish that there exist pruned networks\nwith expressive power within any specified bound from the target network.",
          "link": "http://arxiv.org/abs/2105.10065",
          "publishedOn": "2021-05-24T05:08:39.917Z",
          "wordCount": 531,
          "title": "A Probabilistic Approach to Neural Network Pruning. (arXiv:2105.10065v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carton_F/0/1/0/all/0/1\">Florence Carton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1\">David Filliat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1\">Jaonary Rabarisoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quoc Cuong Pham</a>",
          "description": "In recent years, we have witnessed increasingly high performance in the field\nof autonomous end-to-end driving. In particular, more and more research is\nbeing done on driving in urban environments, where the car has to follow high\nlevel commands to navigate. However, few evaluations are made on the ability of\nthese agents to react in an unexpected situation. Specifically, no evaluations\nare conducted on the robustness of driving agents in the event of a bad\nhigh-level command. We propose here an evaluation method, namely a benchmark\nthat allows to assess the robustness of an agent, and to appreciate its\nunderstanding of the environment through its ability to keep a safe behavior,\nregardless of the instruction.",
          "link": "http://arxiv.org/abs/2105.10014",
          "publishedOn": "2021-05-24T05:08:39.892Z",
          "wordCount": 567,
          "title": "Evaluating Robustness over High Level Driving Instruction for Autonomous Driving. (arXiv:2105.10014v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitman_J/0/1/0/all/0/1\">Julian Whitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Travers_M/0/1/0/all/0/1\">Matthew Travers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1\">Howie Choset</a>",
          "description": "To make a modular robotic system both capable and scalable, the controller\nmust be equally as modular as the mechanism. Given the large number of designs\nthat can be generated from even a small set of modules, it becomes impractical\nto create a new system-wide controller for each design. Instead, we construct a\nmodular control policy that handles a broad class of designs. We take the view\nthat a module is both form and function, i.e. both mechanism and controller. As\nthe modules are physically re-configured, the policy automatically\nre-configures to match the kinematic structure. This novel policy is trained\nwith a new model-based reinforcement learning algorithm, which interleaves\nmodel learning and trajectory optimization to guide policy learning for\nmultiple designs simultaneously. Training the policy on a varied set of designs\nteaches it how to adapt its behavior to the design. We show that the policy can\nthen generalize to a larger set of designs not seen during training. We\ndemonstrate one policy controlling many designs with different combinations of\nlegs and wheels to locomote both in simulation and on real robots.",
          "link": "http://arxiv.org/abs/2105.10049",
          "publishedOn": "2021-05-24T05:08:39.886Z",
          "wordCount": 599,
          "title": "Learning Modular Robot Control Policies. (arXiv:2105.10049v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identifying\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-05-24T05:08:39.877Z",
          "wordCount": 583,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duran_Lopez_L/0/1/0/all/0/1\">Lourdes Duran-Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_Morales_J/0/1/0/all/0/1\">Juan P. Dominguez-Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Galan_D/0/1/0/all/0/1\">Daniel Gutierrez-Galan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_Navarro_A/0/1/0/all/0/1\">Antonio Rios-Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_Fernandez_A/0/1/0/all/0/1\">Angel Jimenez-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicente_Diaz_S/0/1/0/all/0/1\">Saturnino Vicente-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linares_Barranco_A/0/1/0/all/0/1\">Alejandro Linares-Barranco</a>",
          "description": "Prostate cancer (PCa) is one of the most commonly diagnosed cancer and one of\nthe leading causes of death among men, with almost 1.41 million new cases and\naround 375,000 deaths in 2020. Artificial Intelligence algorithms have had a\nhuge impact in medical image analysis, including digital histopathology, where\nConvolutional Neural Networks (CNNs) are used to provide a fast and accurate\ndiagnosis, supporting experts in this task. To perform an automatic diagnosis,\nprostate tissue samples are first digitized into gigapixel-resolution\nwhole-slide images. Due to the size of these images, neural networks cannot use\nthem as input and, therefore, small subimages called patches are extracted and\npredicted, obtaining a patch-level classification. In this work, a novel patch\naggregation method based on a custom Wide & Deep neural network model is\npresented, which performs a slide-level classification using the patch-level\nclasses obtained from a CNN. The malignant tissue ratio, a 10-bin malignant\nprobability histogram, the least squares regression line of the histogram, and\nthe number of malignant connected components are used by the proposed model to\nperform the classification. An accuracy of 94.24% and a sensitivity of 98.87%\nwere achieved, proving that the proposed system could aid pathologists by\nspeeding up the screening process and, thus, contribute to the fight against\nPCa.",
          "link": "http://arxiv.org/abs/2105.09974",
          "publishedOn": "2021-05-24T05:08:39.858Z",
          "wordCount": 660,
          "title": "Wide & Deep neural network model for patch aggregation in CNN-based prostate cancer detection systems. (arXiv:2105.09974v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1\">Dripta S. Raychaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sujoy Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1\">Jeroen van Baar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "Imitation learning seeks to circumvent the difficulty in designing proper\nreward functions for training agents by utilizing expert behavior. With\nenvironments modeled as Markov Decision Processes (MDP), most of the existing\nimitation algorithms are contingent on the availability of expert\ndemonstrations in the same MDP as the one in which a new imitation policy is to\nbe learned. In this paper, we study the problem of how to imitate tasks when\nthere exist discrepancies between the expert and agent MDP. These discrepancies\nacross domains could include differing dynamics, viewpoint, or morphology; we\npresent a novel framework to learn correspondences across such domains.\nImportantly, in contrast to prior works, we use unpaired and unaligned\ntrajectories containing only states in the expert domain, to learn this\ncorrespondence. We utilize a cycle-consistency constraint on both the state\nspace and a domain agnostic latent space to do this. In addition, we enforce\nconsistency on the temporal position of states via a normalized position\nestimator function, to align the trajectories across the two domains. Once this\ncorrespondence is found, we can directly transfer the demonstrations on one\ndomain to the other and use it for imitation. Experiments across a wide variety\nof challenging domains demonstrate the efficacy of our approach.",
          "link": "http://arxiv.org/abs/2105.10037",
          "publishedOn": "2021-05-24T05:08:39.851Z",
          "wordCount": 637,
          "title": "Cross-domain Imitation from Observations. (arXiv:2105.10037v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manjanna_S/0/1/0/all/0/1\">Sandeep Manjanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_A/0/1/0/all/0/1\">Ani Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Gregory Dudek</a>",
          "description": "This paper presents a distributed scalable multi-robot planning algorithm for\nnon-uniform sampling of quasi-static spatial fields. We address the problem of\nefficient data collection using multiple autonomous vehicles. In this paper, we\nare interested in analyzing the effect of communication between multiple\nrobots, acting independently, on the overall sampling performance of the team.\nOur focus is on distributed sampling problem where the robots are operating\nindependent of their teammates, but have the ability to communicate their\nstates to other neighbors with a constraint on the communication range. We\ndesign and apply an informed non-myopic path planning technique on multiple\nrobotic platforms to efficiently collect measurements from a spatial field. Our\nproposed approach is highly adaptive to challenging environments, growing team\nsize, and runs in real-time, which are the key features for any real-world\nscenario. The results show that our distributed sampling approach is able to\nachieve efficient sampling with minimal communication between the robots. We\nevaluate our approach in simulation over multiple distributions commonly\noccurring in nature and on the real-world data collected during a field trial.",
          "link": "http://arxiv.org/abs/2105.10018",
          "publishedOn": "2021-05-24T05:08:39.795Z",
          "wordCount": 613,
          "title": "Scalable Multi-Robot System for Non-myopic Spatial Sampling. (arXiv:2105.10018v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amelard_R/0/1/0/all/0/1\">Robert Amelard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedge_E/0/1/0/all/0/1\">Eric T Hedge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughson_R/0/1/0/all/0/1\">Richard L Hughson</a>",
          "description": "Oxygen consumption (VO$_2$) provides established clinical and physiological\nindicators of cardiorespiratory function and exercise capacity. However, VO$_2$\nmonitoring is largely limited to specialized laboratory settings, making its\nwidespread monitoring elusive. Here, we investigate temporal prediction of\nVO$_2$ from wearable sensors during cycle ergometer exercise using a temporal\nconvolutional network (TCN). Cardiorespiratory signals were acquired from a\nsmart shirt with integrated textile sensors alongside ground-truth VO$_2$ from\na metabolic system on twenty-two young healthy adults. Participants performed\none ramp-incremental and three pseudorandom binary sequence exercise protocols\nto assess a range of VO$_2$ dynamics. A TCN model was developed using causal\nconvolutions across an effective history length to model the time-dependent\nnature of VO$_2$. Optimal history length was determined through minimum\nvalidation loss across hyperparameter values. The best performing model encoded\n218 s history length (TCN-VO$_2$ A), with 187 s, 97 s, and 76 s yielding less\nthan 3% deviation from the optimal validation loss. TCN-VO$_2$ A showed strong\nprediction accuracy (mean, 95% CI) across all exercise intensities (-22\nml.min$^{-1}$, [-262, 218]), spanning transitions from low-moderate (-23\nml.min$^{-1}$, [-250, 204]), low-heavy (14 ml.min$^{-1}$, [-252, 280]),\nventilatory threshold-heavy (-49 ml.min$^{-1}$, [-274, 176]), and maximal (-32\nml.min$^{-1}$, [-261, 197]) exercise. Second-by-second classification of\nphysical activity across 16090 s of predicted VO$_2$ was able to discern\nbetween vigorous, moderate, and light activity with high accuracy (94.1%). This\nsystem enables quantitative aerobic activity monitoring in non-laboratory\nsettings across a range of exercise intensities using wearable sensors for\nmonitoring exercise prescription adherence and personal fitness.",
          "link": "http://arxiv.org/abs/2105.09987",
          "publishedOn": "2021-05-24T05:08:39.767Z",
          "wordCount": 685,
          "title": "Temporal prediction of oxygen uptake dynamics from wearable sensors during low-, moderate-, and heavy-intensity exercise. (arXiv:2105.09987v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rothblum_G/0/1/0/all/0/1\">Guy N Rothblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yona_G/0/1/0/all/0/1\">Gal Yona</a>",
          "description": "An agnostic PAC learning algorithm finds a predictor that is competitive with\nthe best predictor in a benchmark hypothesis class, where competitiveness is\nmeasured with respect to a given loss function. However, its predictions might\nbe quite sub-optimal for structured subgroups of individuals, such as protected\ndemographic groups. Motivated by such fairness concerns, we study \"multi-group\nagnostic PAC learnability\": fixing a measure of loss, a benchmark class $\\H$\nand a (potentially) rich collection of subgroups $\\G$, the objective is to\nlearn a single predictor such that the loss experienced by every group $g \\in\n\\G$ is not much larger than the best possible loss for this group within $\\H$.\nUnder natural conditions, we provide a characterization of the loss functions\nfor which such a predictor is guaranteed to exist. For any such loss function\nwe construct a learning algorithm whose sample complexity is logarithmic in the\nsize of the collection $\\G$. Our results unify and extend previous positive and\nnegative results from the multi-group fairness literature, which applied for\nspecific loss functions.",
          "link": "http://arxiv.org/abs/2105.09989",
          "publishedOn": "2021-05-24T05:08:39.739Z",
          "wordCount": 591,
          "title": "Multi-group Agnostic PAC Learnability. (arXiv:2105.09989v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korani_W/0/1/0/all/0/1\">Wael Korani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouhoub_M/0/1/0/all/0/1\">Malek Mouhoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadaoui_S/0/1/0/all/0/1\">Samira Sadaoui</a>",
          "description": "This study aims to optimize Deep Feedforward Neural Networks (DFNNs) training\nusing nature-inspired optimization algorithms, such as PSO, MTO, and its\nvariant called MTOCL. We show how these algorithms efficiently update the\nweights of DFNNs when learning from data. We evaluate the performance of DFNN\nfused with optimization algorithms using three Wisconsin breast cancer\ndatasets, Original, Diagnostic, and Prognosis, under different experimental\nscenarios. The empirical analysis demonstrates that MTOCL is the most\nperforming in most scenarios across the three datasets. Also, MTOCL is\ncomparable to past weight optimization algorithms for the original dataset, and\nsuperior for the other datasets, especially for the challenging Prognostic\ndataset.",
          "link": "http://arxiv.org/abs/2105.09983",
          "publishedOn": "2021-05-24T05:08:39.696Z",
          "wordCount": 535,
          "title": "Optimizing Neural Network Weights using Nature-Inspired Algorithms. (arXiv:2105.09983v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09966",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Sandford_E/0/1/0/all/0/1\">Emily Sandford</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kipping_D/0/1/0/all/0/1\">David Kipping</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>",
          "description": "A planetary system consists of a host star and one or more planets, arranged\ninto a particular configuration. Here, we consider what information belongs to\nthe configuration, or ordering, of 4286 Kepler planets in their 3277 planetary\nsystems. First, we train a neural network model to predict the radius and\nperiod of a planet based on the properties of its host star and the radii and\nperiod of its neighbors. The mean absolute error of the predictions of the\ntrained model is a factor of 2.1 better than the MAE of the predictions of a\nnaive model which draws randomly from dynamically allowable periods and radii.\nSecond, we adapt a model used for unsupervised part-of-speech tagging in\ncomputational linguistics to investigate whether planets or planetary systems\nfall into natural categories with physically interpretable \"grammatical rules.\"\nThe model identifies two robust groups of planetary systems: (1) compact\nmulti-planet systems and (2) systems around giant stars ($\\log{g} \\lesssim\n4.0$), although the latter group is strongly sculpted by the selection bias of\nthe transit method. These results reinforce the idea that planetary systems are\nnot random sequences -- instead, as a population, they contain predictable\npatterns that can provide insight into the formation and evolution of planetary\nsystems.",
          "link": "http://arxiv.org/abs/2105.09966",
          "publishedOn": "2021-05-24T05:08:39.628Z",
          "wordCount": 639,
          "title": "On planetary systems as ordered sequences. (arXiv:2105.09966v1 [astro-ph.EP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_B/0/1/0/all/0/1\">Bahador Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1\">Nikolaos N. Vlassis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">WaiChing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanxun Xu</a>",
          "description": "This paper presents a computational framework that generates ensemble\npredictive mechanics models with uncertainty quantification (UQ). We first\ndevelop a causal discovery algorithm to infer causal relations among\ntime-history data measured during each representative volume element (RVE)\nsimulation through a directed acyclic graph (DAG). With multiple plausible sets\nof causal relationships estimated from multiple RVE simulations, the\npredictions are propagated in the derived causal graph while using a deep\nneural network equipped with dropout layers as a Bayesian approximation for\nuncertainty quantification. We select two representative numerical examples\n(traction-separation laws for frictional interfaces, elastoplasticity models\nfor granular assembles) to examine the accuracy and robustness of the proposed\ncausal discovery method for the common material law predictions in civil\nengineering applications.",
          "link": "http://arxiv.org/abs/2105.09980",
          "publishedOn": "2021-05-24T05:08:39.522Z",
          "wordCount": 569,
          "title": "Data-driven discovery of interpretable causal relations for deep learning material laws with uncertainty propagation. (arXiv:2105.09980v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.13693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pares_F/0/1/0/all/0/1\">Ferran Par&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_Duart_A/0/1/0/all/0/1\">Anna Arias-Duart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1\">Dario Garcia-Gasulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campo_Frances_G/0/1/0/all/0/1\">Gema Campo-Franc&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viladrich_N/0/1/0/all/0/1\">Nina Viladrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1\">Eduard Ayguad&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1\">Jes&#xfa;s Labarta</a>",
          "description": "In the image classification task, the most common approach is to resize all\nimages in a dataset to a unique shape, while reducing their precision to a size\nwhich facilitates experimentation at scale. This practice has benefits from a\ncomputational perspective, but it entails negative side-effects on performance\ndue to loss of information and image deformation. In this work we introduce the\nMAMe dataset, an image classification dataset with remarkable high resolution\nand variable shape properties. The goal of MAMe is to provide a tool for\nstudying the impact of such properties in image classification, while\nmotivating research in the field. The MAMe dataset contains thousands of\nartworks from three different museums, and proposes a classification task\nconsisting on differentiating between 29 mediums (i.e. materials and\ntechniques) supervised by art experts. After reviewing the singularity of MAMe\nin the context of current image classification tasks, a thorough description of\nthe task is provided, together with dataset statistics. Experiments are\nconducted to evaluate the impact of using high resolution images, variable\nshape inputs and both properties at the same time. Results illustrate the\npositive impact in performance when using high resolution images, while\nhighlighting the lack of solutions to exploit variable shapes. An additional\nexperiment exposes the distinctiveness between the MAMe dataset and the\nprototypical ImageNet dataset. Finally, the baselines are inspected using\nexplainability methods and expert knowledge, to gain insights on the challenges\nthat remain ahead.",
          "link": "http://arxiv.org/abs/2007.13693",
          "publishedOn": "2021-05-23T06:10:40.919Z",
          "wordCount": 725,
          "title": "The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties. (arXiv:2007.13693v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09872",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yoon_J/0/1/0/all/0/1\">Jun Ho Yoon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kim_S/0/1/0/all/0/1\">Seyoung Kim</a>",
          "description": "In many real-world problems, complex dependencies are present both among\nsamples and among features. The Kronecker sum or the Cartesian product of two\ngraphs, each modeling dependencies across features and across samples, has been\nused as an inverse covariance matrix for a matrix-variate Gaussian\ndistribution, as an alternative to a Kronecker-product inverse covariance\nmatrix, due to its more intuitive sparse structure. However, the existing\nmethods for sparse Kronecker-sum inverse covariance estimation are limited in\nthat they do not scale to more than a few hundred features and samples and that\nthe unidentifiable parameters pose challenges in estimation. In this paper, we\nintroduce EiGLasso, a highly scalable method for sparse Kronecker-sum inverse\ncovariance estimation, based on Newton's method combined with\neigendecomposition of the two graphs for exploiting the structure of Kronecker\nsum. EiGLasso further reduces computation time by approximating the Hessian\nbased on the eigendecomposition of the sample and feature graphs. EiGLasso\nachieves quadratic convergence with the exact Hessian and linear convergence\nwith the approximate Hessian. We describe a simple new approach to estimating\nthe unidentifiable parameters that generalizes the existing methods. On\nsimulated and real-world data, we demonstrate that EiGLasso achieves two to\nthree orders-of-magnitude speed-up compared to the existing methods.",
          "link": "http://arxiv.org/abs/2105.09872",
          "publishedOn": "2021-05-23T06:10:40.912Z",
          "wordCount": 626,
          "title": "EiGLasso for Scalable Sparse Kronecker-Sum Inverse Covariance Estimation. (arXiv:2105.09872v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walton_N/0/1/0/all/0/1\">Neil Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kuang Xu</a>",
          "description": "We review the role of information and learning in the stability and\noptimization of queueing systems. In recent years, techniques from supervised\nlearning, bandit learning and reinforcement learning have been applied to\nqueueing systems supported by increasing role of information in decision\nmaking. We present observations and new results that help rationalize the\napplication of these areas to queueing systems.\n\nWe prove that the MaxWeight and BackPressure policies are an application of\nBlackwell's Approachability Theorem. This connects queueing theoretic results\nwith adversarial learning. We then discuss the requirements of statistical\nlearning for service parameter estimation. As an example, we show how queue\nsize regret can be bounded when applying a perceptron algorithm to classify\nservice. Next, we discuss the role of state information in improved decision\nmaking. Here we contrast the roles of epistemic information (information on\nuncertain parameters) and aleatoric information (information on an uncertain\nstate). Finally we review recent advances in the theory of reinforcement\nlearning and queueing, as well as, provide discussion on current research\nchallenges.",
          "link": "http://arxiv.org/abs/2105.08769",
          "publishedOn": "2021-05-23T06:10:40.895Z",
          "wordCount": 609,
          "title": "Learning and Information in Stochastic Networks and Queues. (arXiv:2105.08769v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kefato_Z/0/1/0/all/0/1\">Zekarias T. Kefato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdzijauskas_S/0/1/0/all/0/1\">Sarunas Girdzijauskas</a>",
          "description": "Real world data is mostly unlabeled or only few instances are labeled.\nManually labeling data is a very expensive and daunting task. This calls for\nunsupervised learning techniques that are powerful enough to achieve comparable\nresults as semi-supervised/supervised techniques. Contrastive self-supervised\nlearning has emerged as a powerful direction, in some cases outperforming\nsupervised techniques. In this study, we propose, SelfGNN, a novel contrastive\nself-supervised graph neural network (GNN) without relying on explicit\ncontrastive terms. We leverage Batch Normalization, which introduces implicit\ncontrastive terms, without sacrificing performance. Furthermore, as data\naugmentation is key in contrastive learning, we introduce four feature\naugmentation (FA) techniques for graphs. Though graph topological augmentation\n(TA) is commonly used, our empirical findings show that FA perform as good as\nTA. Moreover, FA incurs no computational overhead, unlike TA, which often has\nO(N^3) time complexity, N-number of nodes. Our empirical evaluation on seven\npublicly available real-world data shows that, SelfGNN is powerful and leads to\na performance comparable with SOTA supervised GNNs and always better than SOTA\nsemi-supervised and unsupervised GNNs. The source code is available at\nhttps://github.com/zekarias-tilahun/SelfGNN.",
          "link": "http://arxiv.org/abs/2103.14958",
          "publishedOn": "2021-05-23T06:10:40.871Z",
          "wordCount": 666,
          "title": "Self-supervised Graph Neural Networks without explicit negative sampling. (arXiv:2103.14958v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>",
          "description": "Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of available\nEnglish datasets by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in low-resource languages. We project\npredictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,\nSpanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in\nTRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in\nOffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513\nF1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our\napproach compares favourably to the best systems submitted to recent shared\ntasks on these three languages. Additionally, we report competitive performance\non Arabic, and Turkish using the training and development sets of OffensEval\n2020 shared task. The results for all languages confirm the robustness of\ncross-lingual contextual embeddings and transfer learning for this task.",
          "link": "http://arxiv.org/abs/2105.05996",
          "publishedOn": "2021-05-23T06:10:40.860Z",
          "wordCount": 695,
          "title": "Multilingual Offensive Language Identification for Low-resource Languages. (arXiv:2105.05996v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingsheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>",
          "description": "Federated learning enables multiple parties to collaboratively learn a model\nwithout exchanging their data. While most existing federated learning\nalgorithms need many rounds to converge, one-shot federated learning (i.e.,\nfederated learning with a single communication round) is a promising approach\nto make federated learning applicable in cross-silo setting in practice.\nHowever, existing one-shot algorithms only support specific models and do not\nprovide any privacy guarantees, which significantly limit the applications in\npractice. In this paper, we propose a practical one-shot federated learning\nalgorithm named FedKT. By utilizing the knowledge transfer technique, FedKT can\nbe applied to any classification models and can flexibly achieve differential\nprivacy guarantees. Our experiments on various tasks show that FedKT can\nsignificantly outperform the other state-of-the-art federated learning\nalgorithms with a single communication round.",
          "link": "http://arxiv.org/abs/2010.01017",
          "publishedOn": "2021-05-23T06:10:40.852Z",
          "wordCount": 581,
          "title": "Practical One-Shot Federated Learning for Cross-Silo Setting. (arXiv:2010.01017v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuqing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1\">Olivia Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>",
          "description": "Policies trained in simulation often fail when transferred to the real world\ndue to the `reality gap' where the simulator is unable to accurately capture\nthe dynamics and visual properties of the real world. Current approaches to\ntackle this problem, such as domain randomization, require prior knowledge and\nengineering to determine how much to randomize system parameters in order to\nlearn a policy that is robust to sim-to-real transfer while also not being too\nconservative. We propose a method for automatically tuning simulator system\nparameters to match the real world using only raw RGB images of the real world\nwithout the need to define rewards or estimate state. Our key insight is to\nreframe the auto-tuning of parameters as a search problem where we iteratively\nshift the simulation system parameters to approach the real-world system\nparameters. We propose a Search Param Model (SPM) that, given a sequence of\nobservations and actions and a set of system parameters, predicts whether the\ngiven parameters are higher or lower than the true parameters used to generate\nthe observations. We evaluate our method on multiple robotic control tasks in\nboth sim-to-sim and sim-to-real transfer, demonstrating significant improvement\nover naive domain randomization. Project videos and code at\nhttps://yuqingd.github.io/autotuned-sim2real/",
          "link": "http://arxiv.org/abs/2104.07662",
          "publishedOn": "2021-05-23T06:10:40.837Z",
          "wordCount": 681,
          "title": "Auto-Tuned Sim-to-Real Transfer. (arXiv:2104.07662v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yueyao Yu</a>",
          "description": "What makes an artificial neural network easier to train and more likely to\nproduce desirable solutions than other comparable networks? In this paper, we\nprovide a new angle to study such issues under the setting of a fixed number of\nmodel parameters which in general is the most dominant cost factor. We\nintroduce a notion of variability and show that it correlates positively to the\nactivation ratio and negatively to a phenomenon called {Collapse to Constants}\n(or C2C), which is closely related but not identical to the phenomenon commonly\nknown as vanishing gradient. Experiments on a styled model problem empirically\nverify that variability is indeed a key performance indicator for fully\nconnected neural networks. The insights gained from this variability study will\nhelp the design of new and effective neural network architectures.",
          "link": "http://arxiv.org/abs/2105.08911",
          "publishedOn": "2021-05-23T06:10:40.822Z",
          "wordCount": 564,
          "title": "Variability of Artificial Neural Networks. (arXiv:2105.08911v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_L/0/1/0/all/0/1\">Lekshmi Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_C/0/1/0/all/0/1\">Chandra R. Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1\">Himanshu Tyagi</a>",
          "description": "In the problem of multiple support recovery, we are given access to linear\nmeasurements of multiple sparse samples in $\\mathbb{R}^{d}$. These samples can\nbe partitioned into $\\ell$ groups, with samples having the same support\nbelonging to the same group. For a given budget of $m$ measurements per sample,\nthe goal is to recover the $\\ell$ underlying supports, in the absence of the\nknowledge of group labels. We study this problem with a focus on the\nmeasurement-constrained regime where $m$ is smaller than the support size $k$\nof each sample. We design a two-step procedure that estimates the union of the\nunderlying supports first, and then uses a spectral algorithm to estimate the\nindividual supports. Our proposed estimator can recover the supports with $m<k$\nmeasurements per sample, from $\\tilde{O}(k^{4}\\ell^{4}/m^{4})$ samples. Our\nguarantees hold for a general, generative model assumption on the samples and\nmeasurement matrices. We also provide results from experiments conducted on\nsynthetic data and on the MNIST dataset.",
          "link": "http://arxiv.org/abs/2105.09855",
          "publishedOn": "2021-05-23T06:10:40.816Z",
          "wordCount": 591,
          "title": "Multiple Support Recovery Using Very Few Measurements Per Sample. (arXiv:2105.09855v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08506",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1\">Sara Atito Ali Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1\">Mehmet Can Yavuz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sen_M/0/1/0/all/0/1\">Mehmet Umut Sen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulsen_F/0/1/0/all/0/1\">Fatih Gulsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tutar_O/0/1/0/all/0/1\">Onur Tutar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korkmazer_B/0/1/0/all/0/1\">Bora Korkmazer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samanci_C/0/1/0/all/0/1\">Cesur Samanci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirolu_S/0/1/0/all/0/1\">Sabri Sirolu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamid_R/0/1/0/all/0/1\">Rauf Hamid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eryurekli_A/0/1/0/all/0/1\">Ali Ergun Eryurekli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mammadov_T/0/1/0/all/0/1\">Toghrul Mammadov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yanikoglu_B/0/1/0/all/0/1\">Berrin Yanikoglu</a>",
          "description": "Detecting COVID-19 in computed tomography (CT) or radiography images has been\nproposed as a supplement to the definitive RT-PCR test. We present a deep\nlearning ensemble for detecting COVID-19 infection, combining slice-based (2D)\nand volume-based (3D) approaches. The 2D system detects the infection on each\nCT slice independently, combining them to obtain the patient-level decision via\ndifferent methods (averaging and long-short term memory networks). The 3D\nsystem takes the whole CT volume to arrive to the patient-level decision in one\nstep. A new high resolution chest CT scan dataset, called the IST-C dataset, is\nalso collected in this work. The proposed ensemble, called IST-CovNet, obtains\n90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting\nCOVID-19 among normal controls and other types of lung pathologies; and 93.69%\naccuracy and 0.99 AUC score on the publicly available MosMed dataset that\nconsists of COVID-19 scans and normal controls only. The system is deployed at\nIstanbul University Cerrahpasa School of Medicine.",
          "link": "http://arxiv.org/abs/2105.08506",
          "publishedOn": "2021-05-23T06:10:40.796Z",
          "wordCount": 693,
          "title": "COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches. (arXiv:2105.08506v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retrain. Our key innovation is to redefine\nthe gradient to a new synaptic parameter, allowing better exploration of\nnetwork structures by taking full advantage of the competition between pruning\nand regrowth of connections. The experimental results show that the proposed\nmethod achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset\nso far. Moreover, it reaches a $\\sim$3.5% accuracy loss under unprecedented\n0.73% connectivity, which reveals remarkable structure refining capability in\nSNNs. Our work suggests that there exists extremely high redundancy in deep\nSNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring .",
          "link": "http://arxiv.org/abs/2105.04916",
          "publishedOn": "2021-05-23T06:10:40.788Z",
          "wordCount": 696,
          "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhiwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dingyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bingchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yongxin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>",
          "description": "Large ride-hailing platforms, such as DiDi, Uber and Lyft, connect tens of\nthousands of vehicles in a city to millions of ride demands throughout the day,\nproviding great promises for improving transportation efficiency through the\ntasks of order dispatching and vehicle repositioning. Existing studies,\nhowever, usually consider the two tasks in simplified settings that hardly\naddress the complex interactions between the two, the real-time fluctuations\nbetween supply and demand, and the necessary coordinations due to the\nlarge-scale nature of the problem. In this paper we propose a unified\nvalue-based dynamic learning framework (V1D3) for tackling both tasks. At the\ncenter of the framework is a globally shared value function that is updated\ncontinuously using online experiences generated from real-time platform\ntransactions. To improve the sample-efficiency and the robustness, we further\npropose a novel periodic ensemble method combining the fast online learning\nwith a large-scale offline training scheme that leverages the abundant\nhistorical driver trajectory data. This allows the proposed framework to adapt\nquickly to the highly dynamic environment, to generalize robustly to recurrent\npatterns and to drive implicit coordinations among the population of managed\nvehicles. Extensive experiments based on real-world datasets show considerably\nimprovements over other recently proposed methods on both tasks. Particularly,\nV1D3 outperforms the first prize winners of both dispatching and repositioning\ntracks in the KDD Cup 2020 RL competition, achieving state-of-the-art results\non improving both total driver income and user experience related metrics.",
          "link": "http://arxiv.org/abs/2105.08791",
          "publishedOn": "2021-05-23T06:10:40.781Z",
          "wordCount": 705,
          "title": "Value Function is All You Need: A Unified Learning Framework for Ride Hailing Platforms. (arXiv:2105.08791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08147",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rister_B/0/1/0/all/0/1\">Blaine Rister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L. Rubin</a>",
          "description": "Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently\nobtained to determine the extent of lung disease and are a valuable source of\ndata for creating artificial intelligence models. Most work to date assessing\ndisease severity on chest imaging has focused on segmenting computed tomography\n(CT) images; however, given that CTs are performed much less frequently than\nchest X-rays for COVID-19 patients, automated lung lesion segmentation on chest\nX-rays could be clinically valuable. There currently exists a universal\nshortage of chest X-rays with ground truth COVID-19 lung lesion annotations,\nand manually contouring lung opacities is a tedious, labor-intensive task. To\naccelerate severity detection and augment the amount of publicly available\nchest X-ray training data for supervised deep learning (DL) models, we leverage\nexisting annotated CT images to generate frontal projection \"chest X-ray\"\nimages for training COVID-19 chest X-ray models. In this paper, we propose an\nautomated pipeline for segmentation of COVID-19 lung lesions on chest X-rays\ncomprised of a Mask R-CNN trained on a mixed dataset of open-source chest\nX-rays and coronal X-ray projections computed from annotated volumetric CTs. On\na test set containing 40 chest X-rays of COVID-19 positive patients, our model\nachieved IoU scores of 0.81 $\\pm$ 0.03 and 0.79 $\\pm$ 0.03 when trained on a\ndataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50\nprojections from CTs, respectively. Our model far outperforms current baselines\nwith limited supervised training and may assist in automated COVID-19 severity\nquantification on chest X-rays.",
          "link": "http://arxiv.org/abs/2105.08147",
          "publishedOn": "2021-05-23T06:10:40.774Z",
          "wordCount": 779,
          "title": "COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs. (arXiv:2105.08147v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Akul Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1\">Ethan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puranik_S/0/1/0/all/0/1\">Samir Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Horace He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "While programming is one of the most broadly applicable skills in modern\nsociety, modern machine learning models still cannot code solutions to basic\nproblems. It can be difficult to accurately assess code generation performance,\nand there has been surprisingly little work on evaluating code generation in a\nway that is both flexible and rigorous. To meet this challenge, we introduce\nAPPS, a benchmark for code generation. Unlike prior work in more restricted\nsettings, our benchmark measures the ability of models to take an arbitrary\nnatural language specification and generate Python code fulfilling this\nspecification. Similar to how companies assess candidate software developers,\nwe then evaluate models by checking their generated code on test cases. Our\nbenchmark includes 10,000 problems, which range from having simple one-line\nsolutions to being substantial algorithmic challenges. We fine-tune large\nlanguage models on both GitHub and our training set, and we find that the\nprevalence of syntax errors is decreasing exponentially. Recent models such as\nGPT-Neo can pass approximately 15% of the test cases of introductory problems,\nso we find that machine learning models are beginning to learn how to code. As\nthe social significance of automatic code generation increases over the coming\nyears, our benchmark can provide an important measure for tracking\nadvancements.",
          "link": "http://arxiv.org/abs/2105.09938",
          "publishedOn": "2021-05-23T06:10:40.767Z",
          "wordCount": 662,
          "title": "Measuring Coding Challenge Competence With APPS. (arXiv:2105.09938v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guandong Xu</a>",
          "description": "In recommendation systems, the existence of the missing-not-at-random (MNAR)\nproblem results in the selection bias issue, degrading the recommendation\nperformance ultimately. A common practice to address MNAR is to treat missing\nentries from the so-called \"exposure\" perspective, i.e., modeling how an item\nis exposed (provided) to a user. Most of the existing approaches use heuristic\nmodels or re-weighting strategy on observed ratings to mimic the\nmissing-at-random setting. However, little research has been done to reveal how\nthe ratings are missing from a causal perspective. To bridge the gap, we\npropose an unbiased and robust method called DENC (De-bias Network Confounding\nin Recommendation) inspired by confounder analysis in causal inference. In\ngeneral, DENC provides a causal analysis on MNAR from both the inherent factors\n(e.g., latent user or item factors) and auxiliary network's perspective.\nParticularly, the proposed exposure model in DENC can control the social\nnetwork confounder meanwhile preserves the observed exposure information. We\nalso develop a deconfounding model through the balanced representation learning\nto retain the primary user and item features, which enables DENC generalize\nwell on the rating prediction. Extensive experiments on three datasets validate\nthat our proposed model outperforms the state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2105.07775",
          "publishedOn": "2021-05-23T06:10:40.748Z",
          "wordCount": 632,
          "title": "Be Causal: De-biasing Social Network Confounding in Recommendation. (arXiv:2105.07775v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1\">Lan V. Truong</a>",
          "description": "We establish exact asymptotic expressions for the normalized mutual\ninformation and minimum mean-square-error (MMSE) of sparse linear regression in\nthe sub-linear sparsity regime. Our result is achieved by a generalization of\nthe adaptive interpolation method in Bayesian inference for linear regimes to\nsub-linear ones. A modification of the well-known approximate message passing\nalgorithm to approach the MMSE fundamental limit is also proposed, and its\nstate evolution is rigorously analysed. Our results show that the traditional\nlinear assumption between the signal dimension and number of observations in\nthe replica and adaptive interpolation methods is not necessary for sparse\nsignals. They also show how to modify the existing well-known AMP algorithms\nfor linear regimes to sub-linear ones.",
          "link": "http://arxiv.org/abs/2101.11156",
          "publishedOn": "2021-05-23T06:10:40.742Z",
          "wordCount": 596,
          "title": "Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1\">Sukhdeep S. Sodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1\">Ellie Ka-In Chio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1\">Ambarish Jash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apte_A/0/1/0/all/0/1\">Ajit Apte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeje_A/0/1/0/all/0/1\">Ayooluwakunmi Jeje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1\">Dima Kuzmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_H/0/1/0/all/0/1\">Harry Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Effrat_J/0/1/0/all/0/1\">Jon Effrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_T/0/1/0/all/0/1\">Tarush Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1\">Nitin Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sarvjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Senqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tameen Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wankhede_A/0/1/0/all/0/1\">Amol Wankhede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1\">Moustafa Alzantot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Allen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1\">Tushar Chandra</a>",
          "description": "As more and more online search queries come from voice, automatic speech\nrecognition becomes a key component to deliver relevant search results. Errors\nintroduced by automatic speech recognition (ASR) lead to irrelevant search\nresults returned to the user, thus causing user dissatisfaction. In this paper,\nwe introduce an approach, Mondegreen, to correct voice queries in text space\nwithout depending on audio signals, which may not always be available due to\nsystem constraints or privacy or bandwidth (for example, some ASR systems run\non-device) considerations. We focus on voice queries transcribed via several\nproprietary commercial ASR systems. These queries come from users making\ninternet, or online service search queries. We first present an analysis\nshowing how different the language distribution coming from user voice queries\nis from that in traditional text corpora used to train off-the-shelf ASR\nsystems. We then demonstrate that Mondegreen can achieve significant\nimprovements in increased user interaction by correcting user voice queries in\none of the largest search systems in Google. Finally, we see Mondegreen as\ncomplementing existing highly-optimized production ASR systems, which may not\nbe frequently retrained and thus lag behind due to vocabulary drifts.",
          "link": "http://arxiv.org/abs/2105.09930",
          "publishedOn": "2021-05-23T06:10:40.735Z",
          "wordCount": 678,
          "title": "Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries. (arXiv:2105.09930v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenwen Yu</a>",
          "description": "Intrusion detection has been a key topic in the field of cyber security, and\nthe common network threats nowadays have the characteristics of varieties and\nvariation. Considering the serious imbalance of intrusion detection datasets\nwill result in low classification performance on attack behaviors of small\nsample size and difficulty to detect network attacks accurately and\nefficiently, using Adaptive Synthetic Sampling (ADASYN) method to balance\ndatasets was proposed in this paper. In addition, Random Forest algorithm was\nused to train intrusion detection classifiers. Through the comparative\nexperiment of Intrusion detection on CICIDS 2017 dataset, it is found that\nADASYN with Random Forest performs better. Based on the experimental results,\nthe improvement of precision, recall, F1 scores and AUC values after ADASYN is\nthen analyzed. Experiments show that the proposed method can be applied to\nintrusion detection with large data, and can effectively improve the\nclassification accuracy of network attack behaviors. Compared with traditional\nmachine learning models, it has better performance, generalization ability and\nrobustness.",
          "link": "http://arxiv.org/abs/2105.04301",
          "publishedOn": "2021-05-23T06:10:40.728Z",
          "wordCount": 615,
          "title": "ADASYN-Random Forest Based Intrusion Detection Model. (arXiv:2105.04301v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianchen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chaosheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingyuan Deng</a>",
          "description": "In this paper, we investigate a new multi-armed bandit (MAB) online learning\nmodel that considers real-world phenomena in many recommender systems: (i) the\nlearning agent cannot pull the arms by itself and thus has to offer rewards to\nusers to incentivize arm-pulling indirectly; and (ii) if users with specific\narm preferences are well rewarded, they induce a \"self-reinforcing\" effect in\nthe sense that they will attract more users of similar arm preferences. Besides\naddressing the tradeoff of exploration and exploitation, another key feature of\nthis new MAB model is to balance reward and incentivizing payment. The goal of\nthe agent is to maximize the total reward over a fixed time horizon $T$ with a\nlow total payment. Our contributions in this paper are two-fold: (i) We propose\na new MAB model with random arm selection that considers the relationship of\nusers' self-reinforcing preferences and incentives; and (ii) We leverage the\nproperties of a multi-color Polya urn with nonlinear feedback model to propose\ntwo MAB policies termed \"At-Least-$n$ Explore-Then-Commit\" and \"UCB-List\". We\nprove that both policies achieve $O(log T)$ expected regret with $O(log T)$\nexpected payment over a time horizon $T$. We conduct numerical simulations to\ndemonstrate and verify the performances of these two policies and study their\nrobustness under various settings.",
          "link": "http://arxiv.org/abs/2105.08869",
          "publishedOn": "2021-05-23T06:10:40.722Z",
          "wordCount": 672,
          "title": "Incentivized Bandit Learning with Self-Reinforcing User Preferences. (arXiv:2105.08869v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majdabadi_M/0/1/0/all/0/1\">Mahdiyar Molahasani Majdabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Younhee Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deivalakshmi_S/0/1/0/all/0/1\">S. Deivalakshmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Seokbum Ko</a>",
          "description": "Prostate cancer is a very common disease among adult men. One in seven\nCanadian men is diagnosed with this cancer in their lifetime. Super-Resolution\n(SR) can facilitate early diagnosis and potentially save many lives. In this\npaper, a robust and accurate model is proposed for prostate MRI SR. The model\nis trained on the Prostate-Diagnosis and PROSTATEx datasets. The proposed model\noutperformed the state-of-the-art prostate SR model in all similarity metrics\nwith notable margins. A new task-specific similarity assessment is introduced\nas well. A classifier is trained for severe cancer detection and the drop in\nthe accuracy of this model when dealing with super-resolved images is used for\nevaluating the ability of medical detail reconstruction of the SR models. The\nproposed SR model is a step towards an efficient and accurate general medical\nSR platform.",
          "link": "http://arxiv.org/abs/2105.07495",
          "publishedOn": "2021-05-23T06:10:40.703Z",
          "wordCount": 573,
          "title": "Capsule GAN for Prostate MRI Super-Resolution. (arXiv:2105.07495v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutellier_T/0/1/0/all/0/1\">Thibaud Lutellier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lin Tan</a>",
          "description": "Automatic program repair (APR) is crucial to improve software reliability.\nRecently, neural machine translation (NMT) techniques have been used to fix\nsoftware bugs automatically. While promising, these approaches have two major\nlimitations. Their search space often does not contain the correct fix, and\ntheir search strategy ignores software knowledge such as strict code syntax.\nDue to these limitations, existing NMT-based techniques underperform the best\ntemplate-based approaches.\n\nWe propose CURE, a new NMT-based APR technique with three major novelties.\nFirst, CURE pre-trains a programming language (PL) model on a large software\ncodebase to learn developer-like source code before the APR task. Second, CURE\ndesigns a new code-aware search strategy that finds more correct fixes by\nfocusing on compilable patches and patches that are close in length to the\nbuggy code. Finally, CURE uses a subword tokenization technique to generate a\nsmaller search space that contains more correct fixes.\n\nOur evaluation on two widely-used benchmarks shows that CURE correctly fixes\n57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR\ntechniques on both benchmarks.",
          "link": "http://arxiv.org/abs/2103.00073",
          "publishedOn": "2021-05-23T06:10:40.696Z",
          "wordCount": 659,
          "title": "CURE: Code-Aware Neural Machine Translation for Automatic Program Repair. (arXiv:2103.00073v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Greenberg_I/0/1/0/all/0/1\">Ido Greenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannay_N/0/1/0/all/0/1\">Netanel Yannay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>",
          "description": "Determining the noise parameters of a Kalman Filter (KF) has been studied for\ndecades. A huge body of research focuses on the task of estimation of the noise\nunder various conditions, since precise noise estimation is considered\nequivalent to minimization of the filtering errors. However, we show that even\na small violation of the KF assumptions can significantly modify the effective\nnoise, breaking the equivalence between the tasks and making noise estimation\nan inferior strategy. We show that such violations are very common, and are\noften not trivial to handle or even notice. Consequentially, we argue that a\nrobust solution is needed - rather than choosing a dedicated model per problem.\nTo that end, we apply gradient-based optimization to the filtering errors\ndirectly, with relation to a simple and efficient parameterization of the\nsymmetric and positive-definite parameters of KF. In radar tracking and video\ntracking, we show that the optimization improves both the accuracy of KF and\nits robustness to design decisions. In addition, we demonstrate how an\noptimized neural network model can seem to reduce the errors significantly\ncompared to a KF - and how this reduction vanishes once the KF is optimized\nsimilarly. This indicates how complicated models can be wrongly identified as\nsuperior to KF, while in fact they were merely more optimized.",
          "link": "http://arxiv.org/abs/2104.02372",
          "publishedOn": "2021-05-23T06:10:40.667Z",
          "wordCount": 689,
          "title": "Noise Estimation Is Not Optimal: How to Use Kalman Filter the Right Way. (arXiv:2104.02372v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00685",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Marvin_D/0/1/0/all/0/1\">Dario Marvin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nespoli_L/0/1/0/all/0/1\">Lorenzo Nespoli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Strepparava_D/0/1/0/all/0/1\">Davide Strepparava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Medici_V/0/1/0/all/0/1\">Vasco Medici</a>",
          "description": "The ability to forecast the concentration of air pollutants in an urban\nregion is crucial for decision-makers wishing to reduce the impact of pollution\non public health through active measures (e.g. temporary traffic closures). In\nthis study, we present a machine learning approach applied to the forecast of\nthe day-ahead maximum value of the ozone concentration for several geographical\nlocations in southern Switzerland. Due to the low density of measurement\nstations and to the complex orography of the use case terrain, we adopted\nfeature selection methods instead of explicitly restricting relevant features\nto a neighbourhood of the prediction sites, as common in spatio-temporal\nforecasting methods. We then used Shapley values to assess the explainability\nof the learned models in terms of feature importance and feature interactions\nin relation to ozone predictions; our analysis suggests that the trained models\neffectively learned explanatory cross-dependencies among atmospheric variables.\nFinally, we show how weighting observations helps in increasing the accuracy of\nthe forecasts for specific ranges of ozone's daily peak values.",
          "link": "http://arxiv.org/abs/2012.00685",
          "publishedOn": "2021-05-23T06:10:40.661Z",
          "wordCount": 629,
          "title": "A data-driven approach to the forecasting of ground-level ozone concentration. (arXiv:2012.00685v3 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hingane_S/0/1/0/all/0/1\">Shreeshail Hingane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Artistic style transfer aims to transfer the style characteristics of one\nimage onto another image while retaining its content. Existing approaches\ncommonly leverage various normalization techniques, although these face\nlimitations in adequately transferring diverse textures to different spatial\nlocations. Self-Attention-based approaches have tackled this issue with partial\nsuccess but suffer from unwanted artifacts. Motivated by these observations,\nthis paper aims to combine the best of both worlds: self-attention and\nnormalization. That yields a new plug-and-play module that we name\nSelf-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially\na spatially adaptive normalization module whose parameters are inferred through\nattention on the content and style image. We demonstrate that plugging SAFIN\ninto the base network of another state-of-the-art method results in enhanced\nstylization. We also develop a novel base network composed of Wavelet Transform\nfor multi-scale style transfer, which when combined with SAFIN, produces\nvisually appealing results with lesser unwanted textures.",
          "link": "http://arxiv.org/abs/2105.06129",
          "publishedOn": "2021-05-23T06:10:40.654Z",
          "wordCount": 616,
          "title": "SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization. (arXiv:2105.06129v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guofeng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1\">Yanguang Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>",
          "description": "The study of multi-type Protein-Protein Interaction (PPI) is fundamental for\nunderstanding biological processes from a systematic perspective and revealing\ndisease mechanisms. Existing methods suffer from significant performance\ndegradation when tested in unseen dataset. In this paper, we investigate the\nproblem and find that it is mainly attributed to the poor performance for\ninter-novel-protein interaction prediction. However, current evaluations\noverlook the inter-novel-protein interactions, and thus fail to give an\ninstructive assessment. As a result, we propose to address the problem from\nboth the evaluation and the methodology. Firstly, we design a new evaluation\nframework that fully respects the inter-novel-protein interactions and gives\nconsistent assessment across datasets. Secondly, we argue that correlations\nbetween proteins must provide useful information for analysis of novel\nproteins, and based on this, we propose a graph neural network based method\n(GNN-PPI) for better inter-novel-protein interaction prediction. Experimental\nresults on real-world datasets of different scales demonstrate that GNN-PPI\nsignificantly outperforms state-of-the-art PPI prediction methods, especially\nfor the inter-novel-protein interaction prediction.",
          "link": "http://arxiv.org/abs/2105.06709",
          "publishedOn": "2021-05-23T06:10:40.635Z",
          "wordCount": 631,
          "title": "Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction. (arXiv:2105.06709v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>",
          "description": "This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge\nTransfer), a novel method for the automatic transfer of explanatory knowledge\nthrough neural encoding mechanisms. We demonstrate that N-XKT is able to\nimprove accuracy and generalization on science Question Answering (QA).\nSpecifically, by leveraging facts from background explanatory knowledge\ncorpora, the N-XKT model shows a clear improvement on zero-shot QA.\nFurthermore, we show that N-XKT can be fine-tuned on a target QA dataset,\nenabling faster convergence and more accurate results. A systematic analysis is\nconducted to quantitatively analyze the performance of the N-XKT model and the\nimpact of different categories of knowledge on the zero-shot generalization\ntask.",
          "link": "http://arxiv.org/abs/2105.05737",
          "publishedOn": "2021-05-23T06:10:40.486Z",
          "wordCount": 554,
          "title": "Encoding Explanatory Knowledge for Zero-shot Science Question Answering. (arXiv:2105.05737v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leshchinskiy_B/0/1/0/all/0/1\">Brandon Leshchinskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Requena_Mesa_C/0/1/0/all/0/1\">Christian Requena-Mesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chishtie_F/0/1/0/all/0/1\">Farrukh Chishtie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1\">Natalia D&#xed;az-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1\">Oc&#xe9;ane Boulais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aruna Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pina_A/0/1/0/all/0/1\">Aaron Pi&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raissi_C/0/1/0/all/0/1\">Chedy Ra&#xef;ssi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1\">Alexander Lavin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1\">Dava Newman</a>",
          "description": "As climate change increases the intensity of natural disasters, society needs\nbetter tools for adaptation. Floods, for example, are the most frequent natural\ndisaster, and better tools for flood risk communication could increase the\nsupport for flood-resilient infrastructure development. Our work aims to enable\nmore visual communication of large-scale climate impacts via visualizing the\noutput of coastal flood models as satellite imagery. We propose the first deep\nlearning pipeline to ensure physical-consistency in synthetic visual satellite\nimagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it\nproduces imagery that is physically-consistent with the output of an\nexpert-validated storm surge model (NOAA SLOSH). By evaluating the imagery\nrelative to physics-based flood maps, we find that our proposed framework\noutperforms baseline models in both physical-consistency and photorealism. We\nenvision our work to be the first step towards a global visualization of how\nclimate change shapes our landscape. Continuing on this path, we show that the\nproposed pipeline generalizes to visualize arctic sea ice melt. We also publish\na dataset of over 25k labelled image-pairs to study image-to-image translation\nin Earth observation.",
          "link": "http://arxiv.org/abs/2104.04785",
          "publishedOn": "2021-05-23T06:10:40.479Z",
          "wordCount": 685,
          "title": "Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization. (arXiv:2104.04785v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01187",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Qi_Z/0/1/0/all/0/1\">Zhengling Qi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Miao_R/0/1/0/all/0/1\">Rui Miao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoke Zhang</a>",
          "description": "Data-driven individualized decision making has recently received increasing\nresearch interests. Most existing methods rely on the assumption of no\nunmeasured confounding, which unfortunately cannot be ensured in practice\nespecially in observational studies. Motivated by the recent proposed proximal\ncausal inference, we develop several proximal learning approaches to estimating\noptimal individualized treatment regimes (ITRs) in the presence of unmeasured\nconfounding. In particular, we establish several identification results for\ndifferent classes of ITRs, exhibiting the trade-off between the risk of making\nuntestable assumptions and the value function improvement in decision making.\nBased on these results, we propose several classification-based approaches to\nfinding a variety of restricted in-class optimal ITRs and develop their\ntheoretical properties. The appealing numerical performance of our proposed\nmethods is demonstrated via an extensive simulation study and one real data\napplication.",
          "link": "http://arxiv.org/abs/2105.01187",
          "publishedOn": "2021-05-23T06:10:40.465Z",
          "wordCount": 580,
          "title": "Proximal Learning for Individualized Treatment Regimes Under Unmeasured Confounding. (arXiv:2105.01187v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Ed Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the accuracy of popular NAR models adopted in neural machine\ntranslation by a large margin.",
          "link": "http://arxiv.org/abs/2105.03842",
          "publishedOn": "2021-05-23T06:10:40.453Z",
          "wordCount": 751,
          "title": "FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1\">Arnd Koeppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1\">Franz Bamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1\">Michael Selzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1\">Britta Nestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1\">Bernd Markert</a>",
          "description": "(Artificial) neural networks have become increasingly popular in mechanics as\nmeans to accelerate computations with model order reduction techniques and as\nuniversal models for a wide variety of materials. However, the major\ndisadvantage of neural networks remains: their numerous parameters are\nchallenging to interpret and explain. Thus, neural networks are often labeled\nas black boxes, and their results often elude human interpretation. In\nmechanics, the new and active field of physics-informed neural networks\nattempts to mitigate this disadvantage by designing deep neural networks on the\nbasis of mechanical knowledge. By using this a priori knowledge, deeper and\nmore complex neural networks became feasible, since the mechanical assumptions\ncould be explained. However, the internal reasoning and explanation of neural\nnetwork parameters remain mysterious.\n\nComplementary to the physics-informed approach, we propose a first step\ntowards a physics-informing approach, which explains neural networks trained on\nmechanical data a posteriori. This novel explainable artificial intelligence\napproach aims at elucidating the black box of neural networks and their\nhigh-dimensional representations. Therein, the principal component analysis\ndecorrelates the distributed representations in cell states of RNNs and allows\nthe comparison to known and fundamental functions. The novel approach is\nsupported by a systematic hyperparameter search strategy that identifies the\nbest neural network architectures and training parameters. The findings of\nthree case studies on fundamental constitutive models (hyperelasticity,\nelastoplasticity, and viscoelasticity) imply that the proposed strategy can\nhelp identify numerical and analytical closed-form solutions to characterize\nnew materials.",
          "link": "http://arxiv.org/abs/2104.10683",
          "publishedOn": "2021-05-23T06:10:40.434Z",
          "wordCount": 713,
          "title": "Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1\">Quentin Paletta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>",
          "description": "Improving irradiance forecasting is critical to further increase the share of\nsolar in the energy mix. On a short time scale, fish-eye cameras on the ground\nare used to capture cloud displacements causing the local variability of the\nelectricity production. As most of the solar radiation comes directly from the\nSun, current forecasting approaches use its position in the image as a\nreference to interpret the cloud cover dynamics. However, existing Sun tracking\nmethods rely on external data and a calibration of the camera, which requires\naccess to the device. To address these limitations, this study introduces an\nimage-based Sun tracking algorithm to localise the Sun in the image when it is\nvisible and interpolate its daily trajectory from past observations. We\nvalidate the method on a set of sky images collected over a year at SIRTA's\nlab. Experimental results show that the proposed method provides robust smooth\nSun trajectories with a mean absolute error below 1% of the image size.",
          "link": "http://arxiv.org/abs/2012.01059",
          "publishedOn": "2021-05-23T06:10:40.406Z",
          "wordCount": 634,
          "title": "A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wang-Zhou Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muggleton_S/0/1/0/all/0/1\">Stephen H. Muggleton</a>",
          "description": "For many reasoning-heavy tasks involving raw inputs, it is challenging to\ndesign an appropriate end-to-end learning pipeline. Neuro-Symbolic Learning,\ndivide the process into sub-symbolic perception and symbolic reasoning, trying\nto utilise data-driven machine learning and knowledge-driven reasoning\nsimultaneously. However, they suffer from the exponential computational\ncomplexity within the interface between these two components, where the\nsub-symbolic learning model lacks direct supervision, and the symbolic model\nlacks accurate input facts. Hence, most of them assume the existence of a\nstrong symbolic knowledge base and only learn the perception model while\navoiding a crucial problem: where does the knowledge come from? In this paper,\nwe present Abductive Meta-Interpretive Learning ($Meta_{Abd}$) that unites\nabduction and induction to learn neural networks and induce logic theories\njointly from raw data. Experimental results demonstrate that $Meta_{Abd}$ not\nonly outperforms the compared systems in predictive accuracy and data\nefficiency but also induces logic programs that can be re-used as background\nknowledge in subsequent learning tasks. To the best of our knowledge,\n$Meta_{Abd}$ is the first system that can jointly learn neural networks from\nscratch and induce recursive first-order logic theories with predicate\ninvention.",
          "link": "http://arxiv.org/abs/2010.03514",
          "publishedOn": "2021-05-23T06:10:40.399Z",
          "wordCount": 639,
          "title": "Abductive Knowledge Induction From Raw Data. (arXiv:2010.03514v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>",
          "description": "Machine learning on graphs has been extensively studied in both academic and\nindustry. However, as the literature on graph learning booms with a vast number\nof emerging methods and techniques, it becomes increasingly difficult to\nmanually design the optimal machine learning algorithm for different\ngraph-related tasks. To solve this critical challenge, automated machine\nlearning (AutoML) on graphs which combines the strength of graph machine\nlearning and AutoML together, is gaining attention from the research community.\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\n(NAS) for graph machine learning. We further overview libraries related to\nautomated graph machine learning and in-depth discuss AutoGL, the first\ndedicated open-source library for AutoML on graphs. In the end, we share our\ninsights on future research directions for automated graph machine learning.\nThis paper is the first systematic and comprehensive review of automated\nmachine learning on graphs to the best of our knowledge.",
          "link": "http://arxiv.org/abs/2103.00742",
          "publishedOn": "2021-05-23T06:10:40.390Z",
          "wordCount": 625,
          "title": "Automated Machine Learning on Graphs: A Survey. (arXiv:2103.00742v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaohang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1\">Md Zakirul Alam Bhuiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lianzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "Depression is one of the most common mental illness problems, and the\nsymptoms shown by patients are not consistent, making it difficult to diagnose\nin the process of clinical practice and pathological research. Although\nresearchers hope that artificial intelligence can contribute to the diagnosis\nand treatment of depression, the traditional centralized machine learning needs\nto aggregate patient data, and the data privacy of patients with mental illness\nneeds to be strictly confidential, which hinders machine learning algorithms\nclinical application. To solve the problem of privacy of the medical history of\npatients with depression, we implement federated learning to analyze and\ndiagnose depression. First, we propose a general multi-view federated learning\nframework using multi-source data, which can extend any traditional machine\nlearning model to support federated learning across different institutions or\nparties. Secondly, we adopt late fusion methods to solve the problem of\ninconsistent time series of multi-view data. Finally, we compare the federated\nframework with other cooperative learning frameworks in performance and discuss\nthe related results.",
          "link": "http://arxiv.org/abs/2102.09342",
          "publishedOn": "2021-05-23T06:10:40.383Z",
          "wordCount": 672,
          "title": "FedMood: Federated Learning on Mobile Health Data for Mood Detection. (arXiv:2102.09342v6 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dapeng Zhao</a>",
          "description": "Thesis document of the degree of Master of Science in Robotics of Carnegie\nMellon University School of Computer Science.",
          "link": "http://arxiv.org/abs/2104.10241",
          "publishedOn": "2021-05-23T06:10:40.362Z",
          "wordCount": 465,
          "title": "Predicting Human Trajectories by Learning and Matching Patterns. (arXiv:2104.10241v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santu_S/0/1/0/all/0/1\">Shubhra Kanti Karmaker Santu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1\">Md. Mahadi Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1\">Micah J. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1\">Kalyan Veeramachaneni</a>",
          "description": "As big data becomes ubiquitous across domains, and more and more stakeholders\naspire to make the most of their data, demand for machine learning tools has\nspurred researchers to explore the possibilities of automated machine learning\n(AutoML). AutoML tools aim to make machine learning accessible for non-machine\nlearning experts (domain experts), to improve the efficiency of machine\nlearning, and to accelerate machine learning research. But although automation\nand efficiency are among AutoML's main selling points, the process still\nrequires human involvement at a number of vital steps, including understanding\nthe attributes of domain-specific data, defining prediction problems, creating\na suitable training data set, and selecting a promising machine learning\ntechnique. These steps often require a prolonged back-and-forth that makes this\nprocess inefficient for domain experts and data scientists alike, and keeps\nso-called AutoML systems from being truly automatic. In this review article, we\nintroduce a new classification system for AutoML systems, using a seven-tiered\nschematic to distinguish these systems based on their level of autonomy. We\nbegin by describing what an end-to-end machine learning pipeline actually looks\nlike, and which subtasks of the machine learning pipeline have been automated\nso far. We highlight those subtasks which are still done manually - generally\nby a data scientist - and explain how this limits domain experts' access to\nmachine learning. Next, we introduce our novel level-based taxonomy for AutoML\nsystems and define each level according to the scope of automation support\nprovided. Finally, we lay out a roadmap for the future, pinpointing the\nresearch required to further automate the end-to-end machine learning pipeline\nand discussing important challenges that stand in the way of this ambitious\ngoal.",
          "link": "http://arxiv.org/abs/2010.10777",
          "publishedOn": "2021-05-23T06:10:40.354Z",
          "wordCount": 775,
          "title": "AutoML to Date and Beyond: Challenges and Opportunities. (arXiv:2010.10777v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1\">Joel Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1\">Alex Gu</a>",
          "description": "The Continual Learning (CL) problem involves performing well on a sequence of\ntasks under limited compute. Current algorithms in the domain are either slow,\noffline or sensitive to hyper-parameters. La-MAML, an optimization-based\nmeta-learning algorithm claims to be better than other replay-based,\nprior-based and meta-learning based approaches. According to the MER paper [1],\nmetrics to measure performance in the continual learning arena are Retained\nAccuracy (RA) and Backward Transfer-Interference (BTI). La-MAML claims to\nperform better in these values when compared to the SOTA in the domain. This is\nthe main claim of the paper, which we shall be verifying in this report.",
          "link": "http://arxiv.org/abs/2102.05824",
          "publishedOn": "2021-05-23T06:10:40.345Z",
          "wordCount": 550,
          "title": "Reproducibility Report: La-MAML: Look-ahead Meta Learning for Continual Learning. (arXiv:2102.05824v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12365",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "This article addresses several fundamental issues associated with the\napproximation theory of neural networks, including the characterization of\napproximation spaces, the determination of the metric entropy of these spaces,\nand approximation rates of neural networks. For any activation function\n$\\sigma$, we show that the largest Banach space of functions which can be\nefficiently approximated by the corresponding shallow neural networks is the\nspace whose norm is given by the gauge of the closed convex hull of the set\n$\\{\\pm\\sigma(\\omega\\cdot x + b)\\}$. We characterize this space for the ReLU$^k$\nand cosine activation functions and, in particular, show that the resulting\ngauge space is equivalent to the spectral Barron space if $\\sigma=\\cos$ and is\nequivalent to the Barron space when $\\sigma={\\rm ReLU}$. Our main result\nestablishes the precise asymptotics of the $L^2$-metric entropy of the unit\nball of these guage spaces and, as a consequence, the optimal approximation\nrates for shallow ReLU$^k$ networks. The sharpest previous results hold only in\nthe special case that $k=0$ and $d=2$, where the metric entropy has been\ndetermined up to logarithmic factors. When $k > 0$ or $d > 2$, there is a\nsignificant gap between the previous best upper and lower bounds. We close all\nof these gaps and determine the precise asymptotics of the metric entropy for\nall $k \\geq 0$ and $d\\geq 2$, including removing the logarithmic factors\npreviously mentioned. Finally, we use these results to quantify how much is\nlost by Barron's spectral condition relative to the convex hull of\n$\\{\\pm\\sigma(\\omega\\cdot x + b)\\}$ when $\\sigma={\\rm ReLU}^k$. Finally, we also\nshow that the orthogonal greedy algorithm can algorithmically realize the\nimproved approximation rates which have been derived.",
          "link": "http://arxiv.org/abs/2101.12365",
          "publishedOn": "2021-05-23T06:10:40.334Z",
          "wordCount": 762,
          "title": "Optimal Approximation Rates and Metric Entropy of ReLU$^k$ and Cosine Networks. (arXiv:2101.12365v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04350",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zheng_L/0/1/0/all/0/1\">Liangzhen Zheng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lan_H/0/1/0/all/0/1\">Haidong Lan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>",
          "description": "Proteins structure prediction has long been a grand challenge over the past\n50 years, owing to its board scientific and application interests. There are\ntwo major types of modelling algorithm, template-free modelling and\ntemplate-based modelling, which is suitable for easy prediction tasks, and is\nwidely adopted in computer aided drug discoveries for drug design and\nscreening. Although it has been several decades since its first edition, the\ncurrent template-based modeling approach suffers from two important problems:\n1) there are many missing regions in the template-query sequence alignment, and\n2) the accuracy of the distance pairs from different regions of the template\nvaries, and this information is not well introduced into the modeling. To solve\nthe two problems, we propose a structural optimization process based on\ntemplate modelling, introducing two neural network models predict the distance\ninformation of the missing regions and the accuracy of the distance pairs of\ndifferent regions in the template modeling structure. The predicted distances\nand residue pairwise specific accuracy information are incorporated into the\npotential energy function for structural optimization, which significantly\nimproves the qualities of the original template modelling decoys.",
          "link": "http://arxiv.org/abs/2105.04350",
          "publishedOn": "2021-05-23T06:10:40.318Z",
          "wordCount": 649,
          "title": "tFold-TR: Combining Deep Learning Enhanced Hybrid Potential Energy for Template-Based Modelling Structure Refinement. (arXiv:2105.04350v2 [physics.bio-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08796",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaojun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdollahi_A/0/1/0/all/0/1\">Ali Abdollahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_T/0/1/0/all/0/1\">Trevor Jones</a>",
          "description": "For electric vehicles (EV) and energy storage (ES) batteries, thermal runaway\nis a critical issue as it can lead to uncontrollable fires or even explosions.\nThermal anomaly detection can identify problematic battery packs that may\neventually undergo thermal runaway. However, there are common challenges like\ndata unavailability, environment and configuration variations, and battery\naging. We propose a data-driven method to detect battery thermal anomaly based\non comparing shape-similarity between thermal measurements. Based on their\nshapes, the measurements are continuously being grouped into different\nclusters. Anomaly is detected by monitoring deviations within the clusters.\nUnlike model-based or other data-driven methods, the proposed method is robust\nto data loss and requires minimal reference data for different pack\nconfigurations. As the initial experimental results show, the method not only\ncan be more accurate than the onboard BMS and but also can detect unforeseen\nanomalies at the early stage.",
          "link": "http://arxiv.org/abs/2103.08796",
          "publishedOn": "2021-05-23T06:10:40.311Z",
          "wordCount": 607,
          "title": "Data-driven Thermal Anomaly Detection for Batteries using Unsupervised Shape Clustering. (arXiv:2103.08796v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cachay_S/0/1/0/all/0/1\">Salva R&#xfc;hling Cachay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erickson_E/0/1/0/all/0/1\">Emma Erickson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucker_A/0/1/0/all/0/1\">Arthur Fender C. Bucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokropek_E/0/1/0/all/0/1\">Ernest Pokropek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potosnak_W/0/1/0/all/0/1\">Willa Potosnak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bire_S/0/1/0/all/0/1\">Suyash Bire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>",
          "description": "Deep learning-based models have recently outperformed state-of-the-art\nseasonal forecasting models, such as for predicting El Ni\\~no-Southern\nOscillation (ENSO). However, current deep learning models are based on\nconvolutional neural networks which are difficult to interpret and can fail to\nmodel large-scale atmospheric patterns. In comparison, graph neural networks\n(GNNs) are capable of modeling large-scale spatial dependencies and are more\ninterpretable due to the explicit modeling of information flow through edge\nconnections. We propose the first application of graph neural networks to\nseasonal forecasting. We design a novel graph connectivity learning module that\nenables our GNN model to learn large-scale spatial interactions jointly with\nthe actual ENSO forecasting task. Our model, \\graphino, outperforms\nstate-of-the-art deep learning-based models for forecasts up to six months\nahead. Additionally, we show that our model is more interpretable as it learns\nsensible connectivity structures that correlate with the ENSO anomaly pattern.",
          "link": "http://arxiv.org/abs/2104.05089",
          "publishedOn": "2021-05-23T06:10:40.304Z",
          "wordCount": 632,
          "title": "The World as a Graph: Improving El Ni\\~no Forecasts with Graph Neural Networks. (arXiv:2104.05089v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1\">Dylan R. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1\">Sina Ghiassian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "Catastrophic forgetting remains a severe hindrance to the broad application\nof artificial neural networks (ANNs), however, it continues to be a poorly\nunderstood phenomenon. Despite the extensive amount of work on catastrophic\nforgetting, we argue that it is still unclear how exactly the phenomenon should\nbe quantified, and, moreover, to what degree all of the choices we make when\ndesigning learning systems affect the amount of catastrophic forgetting. We use\nvarious testbeds from the reinforcement learning and supervised learning\nliterature to (1) provide evidence that the choice of which modern\ngradient-based optimization algorithm is used to train an ANN has a significant\nimpact on the amount of catastrophic forgetting and show that-surprisingly-in\nmany instances classical algorithms such as vanilla SGD experience less\ncatastrophic forgetting than the more modern algorithms such as Adam. We\nempirically compare four different existing metrics for quantifying\ncatastrophic forgetting and (2) show that the degree to which the learning\nsystems experience catastrophic forgetting is sufficiently sensitive to the\nmetric used that a change from one principled metric to another is enough to\nchange the conclusions of a study dramatically. Our results suggest that a much\nmore rigorous experimental methodology is required when looking at catastrophic\nforgetting. Based on our results, we recommend inter-task forgetting in\nsupervised learning must be measured with both retention and relearning metrics\nconcurrently, and intra-task forgetting in reinforcement learning must-at the\nvery least-be measured with pairwise interference.",
          "link": "http://arxiv.org/abs/2102.07686",
          "publishedOn": "2021-05-23T06:10:40.297Z",
          "wordCount": 752,
          "title": "Does Standard Backpropagation Forget Less Catastrophically Than Adam?. (arXiv:2102.07686v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02694",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Perugachi_Diaz_Y/0/1/0/all/0/1\">Yura Perugachi-Diaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bhulai_S/0/1/0/all/0/1\">Sandjai Bhulai</a>",
          "description": "We introduce Invertible Dense Networks (i-DenseNets), a more parameter\nefficient extension of Residual Flows. The method relies on an analysis of the\nLipschitz continuity of the concatenation in DenseNets, where we enforce\ninvertibility of the network by satisfying the Lipschitz constant. Furthermore,\nwe propose a learnable weighted concatenation, which not only improves the\nmodel performance but also indicates the importance of the concatenated\nweighted representation. Additionally, we introduce the Concatenated LipSwish\nas activation function, for which we show how to enforce the Lipschitz\ncondition and which boosts performance. The new architecture, i-DenseNet,\nout-performs Residual Flow and other flow-based models on density estimation\nevaluated in bits per dimension, where we utilize an equal parameter budget.\nMoreover, we show that the proposed model out-performs Residual Flows when\ntrained as a hybrid model where the model is both a generative and a\ndiscriminative model.",
          "link": "http://arxiv.org/abs/2102.02694",
          "publishedOn": "2021-05-23T06:10:40.286Z",
          "wordCount": 588,
          "title": "Invertible DenseNets with Concatenated LipSwish. (arXiv:2102.02694v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frerix_T/0/1/0/all/0/1\">Thomas Frerix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkov_D/0/1/0/all/0/1\">Dmitrii Kochkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jamie A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyer_S/0/1/0/all/0/1\">Stephan Hoyer</a>",
          "description": "Variational data assimilation optimizes for an initial state of a dynamical\nsystem such that its evolution fits observational data. The physical model can\nsubsequently be evolved into the future to make predictions. This principle is\na cornerstone of large scale forecasting applications such as numerical weather\nprediction. As such, it is implemented in current operational systems of\nweather forecasting agencies across the globe. However, finding a good initial\nstate poses a difficult optimization problem in part due to the non-invertible\nrelationship between physical states and their corresponding observations. We\nlearn a mapping from observational data to physical states and show how it can\nbe used to improve optimizability. We employ this mapping in two ways: to\nbetter initialize the non-convex optimization problem, and to reformulate the\nobjective function in better behaved physics space instead of observation\nspace. Our experimental results for the Lorenz96 model and a two-dimensional\nturbulent fluid flow demonstrate that this procedure significantly improves\nforecast quality for chaotic systems.",
          "link": "http://arxiv.org/abs/2102.11192",
          "publishedOn": "2021-05-23T06:10:40.278Z",
          "wordCount": 643,
          "title": "Variational Data Assimilation with a Learned Inverse Observation Operator. (arXiv:2102.11192v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chenjian Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongjin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Liqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwei Xu</a>",
          "description": "Tensor completion refers to the task of estimating the missing data from an\nincomplete measurement or observation, which is a core problem frequently\narising from the areas of big data analysis, computer vision, and network\nengineering. Due to the multidimensional nature of high-order tensors, the\nmatrix approaches, e.g., matrix factorization and direct matricization of\ntensors, are often not ideal for tensor completion and recovery. In this paper,\nwe introduce a unified low-rank and sparse enhanced Tucker decomposition model\nfor tensor completion. Our model possesses a sparse regularization term to\npromote a sparse core tensor of the Tucker decomposition, which is beneficial\nfor tensor data compression. Moreover, we enforce low-rank regularization terms\non factor matrices of the Tucker decomposition for inducing the low-rankness of\nthe tensor with a cheap computational cost. Numerically, we propose a\ncustomized ADMM with enough easy subproblems to solve the underlying model. It\nis remarkable that our model is able to deal with different types of real-world\ndata sets, since it exploits the potential periodicity and inherent correlation\nproperties appeared in tensors. A series of computational experiments on\nreal-world data sets, including internet traffic data sets, color images, and\nface recognition, demonstrate that our model performs better than many existing\nstate-of-the-art matricization and tensorization approaches in terms of\nachieving higher recovery accuracy.",
          "link": "http://arxiv.org/abs/2010.00359",
          "publishedOn": "2021-05-23T06:10:40.269Z",
          "wordCount": 699,
          "title": "Low-Rank and Sparse Enhanced Tucker Decomposition for Tensor Completion. (arXiv:2010.00359v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1\">Max Schwarzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Ankesh Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rishab Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1\">R Devon Hjelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1\">Philip Bachman</a>",
          "description": "While deep reinforcement learning excels at solving tasks where large amounts\nof data can be collected through virtually unlimited interaction with the\nenvironment, learning from limited interaction remains a key challenge. We\nposit that an agent can learn more efficiently if we augment reward\nmaximization with self-supervised objectives based on structure in its visual\ninput and sequential interaction with the environment. Our method,\nSelf-Predictive Representations(SPR), trains an agent to predict its own latent\nstate representations multiple steps into the future. We compute target\nrepresentations for future states using an encoder which is an exponential\nmoving average of the agent's parameters and we make predictions using a\nlearned transition model. On its own, this future prediction objective\noutperforms prior methods for sample-efficient deep RL from pixels. We further\nimprove performance by adding data augmentation to the future prediction loss,\nwhich forces the agent's representations to be consistent across multiple views\nof an observation. Our full self-supervised objective, which combines future\nprediction and data augmentation, achieves a median human-normalized score of\n0.415 on Atari in a setting limited to 100k steps of environment interaction,\nwhich represents a 55% relative improvement over the previous state-of-the-art.\nNotably, even in this limited data regime, SPR exceeds expert human scores on 7\nout of 26 games. The code associated with this work is available at\nhttps://github.com/mila-iqia/spr",
          "link": "http://arxiv.org/abs/2007.05929",
          "publishedOn": "2021-05-23T06:10:40.251Z",
          "wordCount": 723,
          "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations. (arXiv:2007.05929v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02995",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhong_W/0/1/0/all/0/1\">Wenxuan Zhong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_P/0/1/0/all/0/1\">Ping Ma</a>",
          "description": "Optimal transport has been one of the most exciting subjects in mathematics,\nstarting from the 18th century. As a powerful tool to transport between two\nprobability measures, optimal transport methods have been reinvigorated\nnowadays in a remarkable proliferation of modern data science applications. To\nmeet the big data challenges, various computational tools have been developed\nin the recent decade to accelerate the computation for optimal transport\nmethods. In this review, we present some cutting-edge computational optimal\ntransport methods with a focus on the regularization-based methods and the\nprojection-based methods. We discuss their real-world applications in\nbiomedical research.",
          "link": "http://arxiv.org/abs/2008.02995",
          "publishedOn": "2021-05-23T06:10:40.243Z",
          "wordCount": 566,
          "title": "A Review on Modern Computational Optimal Transport Methods with Applications in Biomedical Research. (arXiv:2008.02995v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10718",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rangi_A/0/1/0/all/0/1\">Anshuka Rangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khojasteh_M/0/1/0/all/0/1\">Mohammad Javad Khojasteh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Franceschetti_M/0/1/0/all/0/1\">Massimo Franceschetti</a>",
          "description": "We study the problem of learning-based attacks in linear systems, where the\ncommunication channel between the controller and the plant can be hijacked by a\nmalicious attacker. We assume the attacker learns the dynamics of the system\nfrom observations, then overrides the controller's actuation signal, while\nmimicking legitimate operation by providing fictitious sensor readings to the\ncontroller. On the other hand, the controller is on a lookout to detect the\npresence of the attacker and tries to enhance the detection performance by\ncarefully crafting its control signals. We study the trade-offs between the\ninformation acquired by the attacker from observations, the detection\ncapabilities of the controller, and the control cost. Specifically, we provide\ntight upper and lower bounds on the expected $\\epsilon$-deception time, namely\nthe time required by the controller to make a decision regarding the presence\nof an attacker with confidence at least $(1-\\epsilon\\log(1/\\epsilon))$. We then\nshow a probabilistic lower bound on the time that must be spent by the attacker\nlearning the system, in order for the controller to have a given expected\n$\\epsilon$-deception time. We show that this bound is also order optimal, in\nthe sense that if the attacker satisfies it, then there exists a learning\nalgorithm with the given order expected deception time. Finally, we show a\nlower bound on the expected energy expenditure required to guarantee detection\nwith confidence at least $1-\\epsilon \\log(1/\\epsilon)$.",
          "link": "http://arxiv.org/abs/2011.10718",
          "publishedOn": "2021-05-23T06:10:40.226Z",
          "wordCount": 720,
          "title": "Learning-based attacks in Cyber-Physical Systems: Exploration, Detection, and Control Cost trade-offs. (arXiv:2011.10718v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1\">George Michalopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaka_H/0/1/0/all/0/1\">Hussam Kaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Helen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have\nachieved state-of-the-art results in biomedical natural language processing\ntasks by focusing their pre-training process on domain-specific corpora.\nHowever, such models do not take into consideration expert domain knowledge.\n\nIn this work, we introduced UmlsBERT, a contextual embedding model that\nintegrates domain knowledge during the pre-training process via a novel\nknowledge augmentation strategy. More specifically, the augmentation on\nUmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was\nperformed in two ways: i) connecting words that have the same underlying\n`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to\ncreate clinically meaningful input embeddings. By applying these two\nstrategies, UmlsBERT can encode clinical domain knowledge into word embeddings\nand outperform existing domain-specific models on common named-entity\nrecognition (NER) and clinical natural language inference clinical NLP tasks.",
          "link": "http://arxiv.org/abs/2010.10391",
          "publishedOn": "2021-05-23T06:10:40.219Z",
          "wordCount": 642,
          "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus. (arXiv:2010.10391v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03814",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1\">Phairot Autthasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1\">Rattanaphon Chaisaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1\">Thapanun Sudhawiyangkul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1\">Phurin Rangpong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1\">Suktipol Kiatthaveephong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1\">Nat Dilokthanakul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1\">Gun Bhakdisongkhram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>",
          "description": "Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)\nallow control of several applications by decoding neurophysiological phenomena,\nwhich are usually recorded by electroencephalography (EEG) using a non-invasive\ntechnique. Despite great advances in MI-based BCI, EEG rhythms are specific to\na subject and various changes over time. These issues point to significant\nchallenges to enhance the classification performance, especially in a\nsubject-independent manner. To overcome these challenges, we propose MIN2Net, a\nnovel end-to-end multi-task learning to tackle this task. We integrate deep\nmetric learning into a multi-task autoencoder to learn a compact and\ndiscriminative latent representation from EEG and perform classification\nsimultaneously. This approach reduces the complexity in pre-processing, results\nin significant performance improvement on EEG classification. Experimental\nresults in a subject-independent manner show that MIN2Net outperforms the\nstate-of-the-art techniques, achieving an F1-score improvement of 6.72%, and\n2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that\nMIN2Net improves discriminative information in the latent representation. This\nstudy indicates the possibility and practicality of using this model to develop\nMI-based BCI applications for new users without the need for calibration.",
          "link": "http://arxiv.org/abs/2102.03814",
          "publishedOn": "2021-05-23T06:10:40.212Z",
          "wordCount": 664,
          "title": "MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruimin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiayi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baofeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunlei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jie Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongjiang Wei</a>",
          "description": "Quantitative susceptibility mapping (QSM) has demonstrated great potential in\nquantifying tissue susceptibility in various brain diseases. However, the\nintrinsic ill-posed inverse problem relating the tissue phase to the underlying\nsusceptibility distribution affects the accuracy for quantifying tissue\nsusceptibility. Recently, deep learning has shown promising results to improve\naccuracy by reducing the streaking artifacts. However, there exists a mismatch\nbetween the observed phase and the theoretical forward phase estimated by the\nsusceptibility label. In this study, we proposed a model-based deep learning\narchitecture that followed the STI (susceptibility tensor imaging) physical\nmodel, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the\nrelationship between STI-derived phase contrast induced by the susceptibility\ntensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The\nconvolution neural networks are embedded into the physical model to learn a\nregularization term containing prior information. ki33 and phase induced by\nki13 and ki23 terms were used as the labels for network training. Quantitative\nevaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed\ndeep learning QSM methods. The results showed that MoDL-QSM achieved superior\nperformance, demonstrating its potential for future applications.",
          "link": "http://arxiv.org/abs/2101.08413",
          "publishedOn": "2021-05-23T06:10:40.206Z",
          "wordCount": 660,
          "title": "MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility Mapping. (arXiv:2101.08413v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Basat_R/0/1/0/all/0/1\">Ran Ben-Basat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitzenmacher_M/0/1/0/all/0/1\">Michael Mitzenmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1\">Shay Vargaftik</a>",
          "description": "We consider the fundamental problem of communicating an estimate of a real\nnumber $x\\in[0,1]$ using a single bit. A sender that knows $x$ chooses a value\n$X\\in\\set{0,1}$ to transmit. In turn, a receiver estimates $x$ based on the\nvalue of $X$. We consider both the biased and unbiased estimation problems and\naim to minimize the cost. For the biased case, the cost is the worst-case (over\nthe choice of $x$) expected squared error, which coincides with the variance if\nthe algorithm is required to be unbiased.\n\nWe first overview common biased and unbiased estimation approaches and prove\ntheir optimality when no shared randomness is allowed. We then show how a small\namount of shared randomness, which can be as low as a single bit, reduces the\ncost in both cases. Specifically, we derive lower bounds on the cost attainable\nby any algorithm with unrestricted use of shared randomness and propose\nnear-optimal solutions that use a small number of shared random bits. Finally,\nwe discuss open problems and future directions.",
          "link": "http://arxiv.org/abs/2010.02331",
          "publishedOn": "2021-05-23T06:10:40.199Z",
          "wordCount": 664,
          "title": "How to send a real number using a single bit (and some shared randomness). (arXiv:2010.02331v4 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06284",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>",
          "description": "This paper addresses the issue of long-scale correlations that is\ncharacteristic for symbolic music and is a challenge for modern generative\nalgorithms. It suggests a very simple workaround for this challenge, namely,\ngeneration of a drum pattern that could be further used as a foundation for\nmelody generation. The paper presents a large dataset of drum patterns\nalongside with corresponding melodies. It explores two possible methods for\ndrum pattern generation. Exploring a latent space of drum patterns one could\ngenerate new drum patterns with a given music style. Finally, the paper\ndemonstrates that a simple artificial neural network could be trained to\ngenerate melodies corresponding with these drum patters used as inputs.\nResulting system could be used for end-to-end generation of symbolic music with\nsong-like structure and higher long-scale correlations between the notes.",
          "link": "http://arxiv.org/abs/2007.06284",
          "publishedOn": "2021-05-23T06:10:40.190Z",
          "wordCount": 604,
          "title": "Artificial Neural Networks Jamming on the Beat. (arXiv:2007.06284v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishad_S/0/1/0/all/0/1\">Sunil Nishad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shubhangi Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranu_S/0/1/0/all/0/1\">Sayan Ranu</a>",
          "description": "Majority of the existing graph neural networks (GNN) learn node embeddings\nthat encode their local neighborhoods but not their positions. Consequently,\ntwo nodes that are vastly distant but located in similar local neighborhoods\nmap to similar embeddings in those networks. This limitation prevents accurate\nperformance in predictive tasks that rely on position information. In this\npaper,we develop GraphReach, a position-aware inductive GNN that captures the\nglobal positions of nodes through reachability estimations with respect to a\nset of anchor nodes. The anchors are strategically selected so that\nreachability estimations across all the nodes are maximized. We show that this\ncombinatorial anchor selection problem is NP-hard and, consequently, develop a\ngreedy (1-1/e) approximation heuristic. Empirical evaluation against\nstate-of-the-art GNN architectures reveal that GraphReach provides up to 40%\nrelative improvement in accuracy. In addition, it is more robust to adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2008.09657",
          "publishedOn": "2021-05-23T06:10:40.181Z",
          "wordCount": 616,
          "title": "GraphReach: Position-Aware Graph Neural Network using Reachability Estimations. (arXiv:2008.09657v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bowen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "With the explosive growth of e-commerce, online transaction fraud has become\none of the biggest challenges for e-commerce platforms. The historical\nbehaviors of users provide rich information for digging into the users' fraud\nrisk. While considerable efforts have been made in this direction, a\nlong-standing challenge is how to effectively exploit internal user information\nand provide explainable prediction results. In fact, the value variations of\nsame field from different events and the interactions of different fields\ninside one event have proven to be strong indicators for fraudulent behaviors.\nIn this paper, we propose the Dual Importance-aware Factorization Machines\n(DIFM), which exploits the internal field information among users' behavior\nsequence from dual perspectives, i.e., field value variations and field\ninteractions simultaneously for fraud detection. The proposed model is deployed\nin the risk management system of one of the world's largest e-commerce\nplatforms, which utilize it to provide real-time transaction fraud detection.\nExperimental results on real industrial data from different regions in the\nplatform clearly demonstrate that our model achieves significant improvements\ncompared with various state-of-the-art baseline models. Moreover, the DIFM\ncould also give an insight into the explanation of the prediction results from\ndual perspectives.",
          "link": "http://arxiv.org/abs/2008.05600",
          "publishedOn": "2021-05-23T06:10:40.174Z",
          "wordCount": 673,
          "title": "Modeling the Field Value Variations and Field Interactions Simultaneously for Fraud Detection. (arXiv:2008.05600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cordonnier_J/0/1/0/all/0/1\">Jean-Baptiste Cordonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1\">Andreas Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Attention layers are widely used in natural language processing (NLP) and are\nbeginning to influence computer vision architectures. Training very large\ntransformer models allowed significant improvement in both fields, but once\ntrained, these networks show symptoms of over-parameterization. For instance,\nit is known that many attention heads can be pruned without impacting accuracy.\nThis work aims to enhance current understanding on how multiple heads interact.\nMotivated by the observation that attention heads learn redundant key/query\nprojections, we propose a collaborative multi-head attention layer that enables\nheads to learn shared projections. Our scheme decreases the number of\nparameters in an attention layer and can be used as a drop-in replacement in\nany transformer architecture. Our experiments confirm that sharing key/query\ndimensions can be exploited in language understanding, machine translation and\nvision. We also show that it is possible to re-parametrize a pre-trained\nmulti-head attention layer into our collaborative attention layer.\nCollaborative multi-head attention reduces the size of the key and query\nprojections by 4 for same accuracy and speed. Our code is public.",
          "link": "http://arxiv.org/abs/2006.16362",
          "publishedOn": "2021-05-23T06:10:40.021Z",
          "wordCount": 628,
          "title": "Multi-Head Attention: Collaborate Instead of Concatenate. (arXiv:2006.16362v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gou_J/0/1/0/all/0/1\">Jianping Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1\">Stephen John Maybank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "In recent years, deep neural networks have been successful in both industry\nand academia, especially for computer vision tasks. The great success of deep\nlearning is mainly due to its scalability to encode large-scale data and to\nmaneuver billions of model parameters. However, it is a challenge to deploy\nthese cumbersome deep models on devices with limited resources, e.g., mobile\nphones and embedded devices, not only because of the high computational\ncomplexity but also the large storage requirements. To this end, a variety of\nmodel compression and acceleration techniques have been developed. As a\nrepresentative type of model compression and acceleration, knowledge\ndistillation effectively learns a small student model from a large teacher\nmodel. It has received rapid increasing attention from the community. This\npaper provides a comprehensive survey of knowledge distillation from the\nperspectives of knowledge categories, training schemes, teacher-student\narchitecture, distillation algorithms, performance comparison and applications.\nFurthermore, challenges in knowledge distillation are briefly reviewed and\ncomments on future research are discussed and forwarded.",
          "link": "http://arxiv.org/abs/2006.05525",
          "publishedOn": "2021-05-23T06:10:39.966Z",
          "wordCount": 677,
          "title": "Knowledge Distillation: A Survey. (arXiv:2006.05525v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cramer_B/0/1/0/all/0/1\">Benjamin Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Billaudelle_S/0/1/0/all/0/1\">Sebastian Billaudelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanya_S/0/1/0/all/0/1\">Simeon Kanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibfried_A/0/1/0/all/0/1\">Aron Leibfried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubl_A/0/1/0/all/0/1\">Andreas Gr&#xfc;bl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karasenko_V/0/1/0/all/0/1\">Vitali Karasenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pehle_C/0/1/0/all/0/1\">Christian Pehle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreiber_K/0/1/0/all/0/1\">Korbinian Schreiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stradmann_Y/0/1/0/all/0/1\">Yannik Stradmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weis_J/0/1/0/all/0/1\">Johannes Weis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schemmel_J/0/1/0/all/0/1\">Johannes Schemmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenke_F/0/1/0/all/0/1\">Friedemann Zenke</a>",
          "description": "To rapidly process temporal information at a low metabolic cost, biological\nneurons integrate inputs as an analog sum but communicate with spikes, binary\nevents in time. Analog neuromorphic hardware uses the same principles to\nemulate spiking neural networks with exceptional energy-efficiency. However,\ninstantiating high-performing spiking networks on such hardware remains a\nsignificant challenge due to device mismatch and the lack of efficient training\nalgorithms. Here, we introduce a general in-the-loop learning framework based\non surrogate gradients that resolves these issues. Using the BrainScaleS-2\nneuromorphic system, we show that learning self-corrects for device mismatch\nresulting in competitive spiking network performance on both vision and speech\nbenchmarks. Our networks display sparse spiking activity with, on average, far\nless than one spike per hidden neuron and input, perform inference at rates of\nup to 85 k frames/second, and consume less than 200 mW. In summary, our work\nsets several new benchmarks for low-energy spiking network processing on analog\nneuromorphic hardware and paves the way for future on-chip learning algorithms.",
          "link": "http://arxiv.org/abs/2006.07239",
          "publishedOn": "2021-05-23T06:10:39.959Z",
          "wordCount": 664,
          "title": "Surrogate gradients for analog neuromorphic computing. (arXiv:2006.07239v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09917",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1\">Aleksandr Beknazaryan</a>",
          "description": "An example of an activation function $\\sigma$ is given such that networks\nwith activations $\\{\\sigma, \\lfloor\\cdot\\rfloor\\}$, integer weights and a fixed\narchitecture depending on $d$ approximate continuous functions on $[0,1]^d$.\nThe range of integer weights required for $\\varepsilon$-approximation of\nH\\\"older continuous functions is derived, which leads to a convergence rate of\norder $n^{\\frac{-2\\beta}{2\\beta+d}}\\log_2n$ for neural network regression\nestimation of unknown $\\beta$-H\\\"older continuous function with given $n$\nsamples.",
          "link": "http://arxiv.org/abs/2105.09917",
          "publishedOn": "2021-05-23T06:10:39.952Z",
          "wordCount": 489,
          "title": "Neural networks with superexpressive activations and integer weights. (arXiv:2105.09917v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1912.07913",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Grelier_E/0/1/0/all/0/1\">Erwan Grelier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nouy_A/0/1/0/all/0/1\">Anthony Nouy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lebrun_R/0/1/0/all/0/1\">R&#xe9;gis Lebrun</a>",
          "description": "We consider the problem of the estimation of a high-dimensional probability\ndistribution from i.i.d. samples of the distribution using model classes of\nfunctions in tree-based tensor formats, a particular case of tensor networks\nassociated with a dimension partition tree. The distribution is assumed to\nadmit a density with respect to a product measure, possibly discrete for\nhandling the case of discrete random variables.\n\nAfter discussing the representation of classical model classes in tree-based\ntensor formats, we present learning algorithms based on empirical risk\nminimization using a $L^2$ contrast.\n\nThese algorithms exploit the multilinear parametrization of the formats to\nrecast the nonlinear minimization problem into a sequence of empirical risk\nminimization problems with linear models. A suitable parametrization of the\ntensor in tree-based tensor format allows to obtain a linear model with\northogonal bases, so that each problem admits an explicit expression of the\nsolution and cross-validation risk estimates. These estimations of the risk\nenable the model selection, for instance when exploiting sparsity in the\ncoefficients of the representation.\n\nA strategy for the adaptation of the tensor format (dimension tree and\ntree-based ranks) is provided, which allows to discover and exploit some\nspecific structures of high-dimensional probability distributions such as\nindependence or conditional independence.\n\nWe illustrate the performances of the proposed algorithms for the\napproximation of classical probabilistic models (such as Gaussian distribution,\ngraphical models, Markov chain).",
          "link": "http://arxiv.org/abs/1912.07913",
          "publishedOn": "2021-05-23T06:10:39.944Z",
          "wordCount": 685,
          "title": "Learning high-dimensional probability distributions using tree tensor networks. (arXiv:1912.07913v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.01383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Jan Paul Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangrong Xu</a>",
          "description": "This paper proposes a novel automatically generating image masks method for\nthe state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method\nachieves the best results in object detection until now, however, it is very\ntime-consuming and laborious to get the object Masks for training, the proposed\nmethod is composed by a two-stage design, to automatically generating image\nmasks, the first stage implements a fully convolutional networks (FCN) based\nsegmentation network, the second stage network, a Mask R-CNN based object\ndetection network, which is trained on the object image masks from FCN output,\nthe original input image, and additional label information. Through\nexperimentation, our proposed method can obtain the image masks automatically\nto train Mask R-CNN, and it can achieve very high classification accuracy with\nan over 90% mean of average precision (mAP) for segmentation",
          "link": "http://arxiv.org/abs/2003.01383",
          "publishedOn": "2021-05-23T06:10:39.936Z",
          "wordCount": 603,
          "title": "Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN. (arXiv:2003.01383v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.08797",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1\">Soufiane Hayou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1\">Jean-Francois Ton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Overparameterized Neural Networks (NN) display state-of-the-art performance.\nHowever, there is a growing need for smaller, energy-efficient, neural networks\ntobe able to use machine learning applications on devices with limited\ncomputational resources. A popular approach consists of using pruning\ntechniques. While these techniques have traditionally focused on pruning\npre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et\nal. (2018) has shown promising results when pruning at initialization. However,\nfor Deep NNs, such procedures remain unsatisfactory as the resulting pruned\nnetworks can be difficult to train and, for instance, they do not prevent one\nlayer from being fully pruned. In this paper, we provide a comprehensive\ntheoretical analysis of Magnitude and Gradient based pruning at initialization\nand training of sparse architectures. This allows us to propose novel\nprincipled approaches which we validate experimentally on a variety of NN\narchitectures.",
          "link": "http://arxiv.org/abs/2002.08797",
          "publishedOn": "2021-05-23T06:10:39.916Z",
          "wordCount": 619,
          "title": "Robust Pruning at Initialization. (arXiv:2002.08797v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1\">Luiz Giovanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceschin_F/0/1/0/all/0/1\">Fabr&#xed;cio Ceschin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Mirela Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aokun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1\">Ramchandra Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_S/0/1/0/all/0/1\">Sanjay Banda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lysaght_M/0/1/0/all/0/1\">Madison Lysaght</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1\">Heng Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapountzis_N/0/1/0/all/0/1\">Nikolaos Sapountzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruimin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthews_B/0/1/0/all/0/1\">Brandon Matthews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dapeng Oliver Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregio_A/0/1/0/all/0/1\">Andr&#xe9; Gr&#xe9;gio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniela Oliveira</a>",
          "description": "This paper investigates whether computer usage profiles comprised of\nprocess-, network-, mouse- and keystroke-related events are unique and\ntemporally consistent in a naturalistic setting, discussing challenges and\nopportunities of using such profiles in applications of continuous\nauthentication. We collected ecologically-valid computer usage profiles from 28\nMS Windows 10 computer users over 8 weeks and submitted this data to\ncomprehensive machine learning analysis involving a diverse set of online and\noffline classifiers. We found that (i) computer usage profiles have the\npotential to uniquely characterize computer users (with a maximum F-score of\n99.94%); (ii) network-related events were the most useful features to properly\nrecognize profiles (95.14% of the top features distinguishing users being\nnetwork-related); (iii) user profiles were mostly inconsistent over the 8-week\ndata collection period, with 92.86% of users exhibiting drifts in terms of time\nand usage habits; and (iv) online models are better suited to handle computer\nusage profiles compared to offline models (maximum F-score for each approach\nwas 95.99% and 99.94%, respectively).",
          "link": "http://arxiv.org/abs/2105.09900",
          "publishedOn": "2021-05-23T06:10:39.909Z",
          "wordCount": 613,
          "title": "Computer Users Have Unique Yet Temporally Inconsistent Computer Usage Profiles. (arXiv:2105.09900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1812.00002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1\">Sen Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Congzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>",
          "description": "Interactive news recommendation has been launched and attracted much\nattention recently. In this scenario, user's behavior evolves from single click\nbehavior to multiple behaviors including like, comment, share etc. However,\nmost of the existing methods still use single click behavior as the unique\ncriterion of judging user's preferences. Further, although heterogeneous graphs\nhave been applied in different areas, a proper way to construct a heterogeneous\ngraph for interactive news data with an appropriate learning mechanism on it is\nstill desired. To address the above concerns, we propose a graph-based\nbehavior-aware network, which simultaneously considers six different types of\nbehaviors as well as user's demand on the news diversity. We have three main\nsteps. First, we build an interaction behavior graph for multi-level and\nmulti-category data. Second, we apply DeepWalk on the behavior graph to obtain\nentity semantics, then build a graph-based convolutional neural network called\nG-CNN to learn news representations, and an attention-based LSTM to learn\nbehavior sequence representations. Third, we introduce core and coritivity\nfeatures for the behavior graph, which measure the concentration degree of\nuser's interests. These features affect the trade-off between accuracy and\ndiversity of our personalized recommendation system. Taking these features into\naccount, our system finally achieves recommending news to different users at\ntheir different levels of concentration degrees.",
          "link": "http://arxiv.org/abs/1812.00002",
          "publishedOn": "2021-05-23T06:10:39.898Z",
          "wordCount": 678,
          "title": "The Graph-Based Behavior-Aware Recommendation for Interactive News. (arXiv:1812.00002v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wangyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from human's visual and intuitive perspective. We take\nthe first step to bridge the gap by proposing a deep learning-based technique\nto automatically classify road networks into four classes on a visual basis.\nThe method is implemented by generating an image of the street network (Colored\nRoad Hierarchy Diagram), which we introduce in this paper, and classifying it\nusing a deep convolutional neural network (ResNet-34). The model achieves an\noverall classification accuracy of 0.875. Nine cities around the world are\nselected as the study areas and their road networks are acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through a\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: the\neffectiveness of our human perception augmentation is examined by a case study\nof urban vitality prediction. An advanced tree-based regression model is for\nthe first time designated to establish the relationship between morphological\nindices and vitality indicators. A positive effect of human perception\naugmentation is detected in the comparative experiment of baseline model and\naugmented model. This work expands the toolkit of quantitative urban morphology\nstudy with new techniques, supporting further studies in the future.",
          "link": "http://arxiv.org/abs/2105.09908",
          "publishedOn": "2021-05-23T06:10:39.891Z",
          "wordCount": 703,
          "title": "Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1\">Manav Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1\">Peter Jakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1\">Tobias Schmid-Schirling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Multi-view classification is inspired by the behavior of humans, especially\nwhen fine-grained features or in our case rarely occurring anomalies are to be\ndetected. Current contributions point to the problem of how high-dimensional\ndata can be fused. In this work, we build upon the deep support vector data\ndescription algorithm and address multi-perspective anomaly detection using\nthree different fusion techniques i.e. early fusion, late fusion, and late\nfusion with multiple decoders. We employ different augmentation techniques with\na denoising process to deal with scarce one-class data, which further improves\nthe performance (ROC AUC = 80\\%). Furthermore, we introduce the dices dataset\nthat consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g. drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed approach\nexceeds the state-of-the-art on both the MNIST and dices datasets. To the best\nof our knowledge, this is the first work that focuses on addressing\nmulti-perspective anomaly detection in images by jointly using different\nperspectives together with one single objective function for anomaly detection.",
          "link": "http://arxiv.org/abs/2105.09903",
          "publishedOn": "2021-05-23T06:10:39.885Z",
          "wordCount": 633,
          "title": "Multi-Perspective Anomaly Detection. (arXiv:2105.09903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.13934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>",
          "description": "Methods to quantify the complexity of trajectory datasets are still a missing\npiece in benchmarking human trajectory prediction models. In order to gain a\nbetter understanding of the complexity of trajectory prediction tasks and\nfollowing the intuition, that more complex datasets contain more information,\nan approach for quantifying the amount of information contained in a dataset\nfrom a prototype-based dataset representation is proposed. The dataset\nrepresentation is obtained by first employing a non-trivial spatial sequence\nalignment, which enables a subsequent learning vector quantization (LVQ) stage.\nA large-scale complexity analysis is conducted on several human trajectory\nprediction benchmarking datasets, followed by a brief discussion on indications\nfor human trajectory prediction and benchmarking.",
          "link": "http://arxiv.org/abs/2005.13934",
          "publishedOn": "2021-05-23T06:10:39.867Z",
          "wordCount": 600,
          "title": "Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction. (arXiv:2005.13934v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanli Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden M. Lake</a>",
          "description": "Humans are highly efficient learners, with the ability to grasp the meaning\nof a new concept from just a few examples. Unlike popular computer vision\nsystems, humans can flexibly leverage the compositional structure of the visual\nworld, understanding new concepts as combinations of existing concepts. In the\ncurrent paper, we study how people learn different types of visual\ncompositions, using abstract visual forms with rich relational structure. We\nfind that people can make meaningful compositional generalizations from just a\nfew examples in a variety of scenarios, and we develop a Bayesian program\ninduction model that provides a close fit to the behavioral data. Unlike past\nwork examining special cases of compositionality, our work shows how a single\ncomputational approach can account for many distinct types of compositional\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09848",
          "publishedOn": "2021-05-23T06:10:39.860Z",
          "wordCount": 587,
          "title": "Flexible Compositional Learning of Structured Visual Concepts. (arXiv:2105.09848v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhuangwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanbu Guo</a>",
          "description": "Chinese word segmentation (CWS) is the basic of Chinese natural language\nprocessing (NLP). The quality of word segmentation will directly affect the\nrest of NLP tasks. Recently, with the artificial intelligence tide rising\nagain, Long Short-Term Memory (LSTM) neural network, as one of easily modeling\nin sequence, has been widely utilized in various kinds of NLP tasks, and\nfunctions well. Attention mechanism is an ingenious method to solve the memory\ncompression problem on LSTM. Furthermore, inspired by the powerful abilities of\nbidirectional LSTM models for modeling sequence and CRF model for decoding, we\npropose a Bidirectional LSTM-CRF Attention-based Model in this paper.\nExperiments on PKU and MSRA benchmark datasets show that our model performs\nbetter than the baseline methods modeling by other neural networks.",
          "link": "http://arxiv.org/abs/2105.09681",
          "publishedOn": "2021-05-23T06:10:39.853Z",
          "wordCount": 546,
          "title": "Bidirectional LSTM-CRF Attention-based Model for Chinese Word Segmentation. (arXiv:2105.09681v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$~GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.",
          "link": "http://arxiv.org/abs/2105.09858",
          "publishedOn": "2021-05-23T06:10:39.847Z",
          "wordCount": 646,
          "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>",
          "description": "Recommender systems are gaining increasing and critical impacts on human and\nsociety since a growing number of users use them for information seeking and\ndecision making. Therefore, it is crucial to address the potential unfairness\nproblems in recommendations. Just like users have personalized preferences on\nitems, users' demands for fairness are also personalized in many scenarios.\nTherefore, it is important to provide personalized fair recommendations for\nusers to satisfy their personalized fairness demands. Besides, previous works\non fair recommendation mainly focus on association-based fairness. However, it\nis important to advance from associative fairness notions to causal fairness\nnotions for assessing fairness more properly in recommender systems. Based on\nthe above considerations, this paper focuses on achieving personalized\ncounterfactual fairness for users in recommender systems. To this end, we\nintroduce a framework for achieving counterfactually fair recommendations\nthrough adversary learning by generating feature-independent user embeddings\nfor recommendation. The framework allows recommender systems to achieve\npersonalized fairness for users while also covering non-personalized\nsituations. Experiments on two real-world datasets with shallow and deep\nrecommendation algorithms show that our method can generate fairer\nrecommendations for users with a desirable recommendation performance.",
          "link": "http://arxiv.org/abs/2105.09829",
          "publishedOn": "2021-05-23T06:10:39.840Z",
          "wordCount": 630,
          "title": "Towards Personalized Fairness based on Causal Notion. (arXiv:2105.09829v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09866",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Guet_C/0/1/0/all/0/1\">Claude Guet</a>",
          "description": "Solving physics problems for which we know the equations, boundary conditions\nand symmetries can be done by deep learning. The constraints can be either\nimposed as terms in a loss function or used to formulate a neural ansatz. In\nthe present case study, we calculate the induced field inside and outside a\ndielectric cube placed in a uniform electric field, wherein the dielectric\nmismatch at edges and corners of the cube makes accurate calculations\nnumerically challenging. The electric potential is expressed as an ansatz\nincorporating neural networks with known leading order behaviors and symmetries\nand the Laplace's equation is then solved with boundary conditions at the\ndielectric interface by minimizing a loss function. The loss function ensures\nthat both Laplace's equation and boundary conditions are satisfied everywhere\ninside a large solution domain. We study how the electric potential inside and\noutside a quasi-cubic particle evolves through a sequence of shapes from a\nsphere to a cube. The neural network being differentiable, it is\nstraightforward to calculate the electric field over the whole domain, the\ninduced surface charge distribution and the polarizability. The neural network\nbeing retentive, one can efficiently follow how the field changes upon\nparticle's shape or dielectric constant by iterating from any previously\nconverged solution. The present work's objective is two-fold, first to show how\nan a priori knowledge can be incorporated into neural networks to achieve\nefficient learning and second to apply the method and study how the induced\nfield and polarizability change when a dielectric particle progressively\nchanges its shape from a sphere to a cube.",
          "link": "http://arxiv.org/abs/2105.09866",
          "publishedOn": "2021-05-23T06:10:39.834Z",
          "wordCount": 707,
          "title": "Deep learning in physics: a study of dielectric quasi-cubic particles in a uniform electric field. (arXiv:2105.09866v1 [physics.class-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7~GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.",
          "link": "http://arxiv.org/abs/2105.09856",
          "publishedOn": "2021-05-23T06:10:39.814Z",
          "wordCount": 635,
          "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awad_N/0/1/0/all/0/1\">Noor Awad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1\">Neeratyoy Mallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "Modern machine learning algorithms crucially rely on several design decisions\nto achieve strong performance, making the problem of Hyperparameter\nOptimization (HPO) more important than ever. Here, we combine the advantages of\nthe popular bandit-based HPO method Hyperband (HB) and the evolutionary search\napproach of Differential Evolution (DE) to yield a new HPO method which we call\nDEHB. Comprehensive results on a very broad range of HPO problems, as well as a\nwide range of tabular benchmarks from neural architecture search, demonstrate\nthat DEHB achieves strong performance far more robustly than all previous HPO\nmethods we are aware of, especially for high-dimensional problems with discrete\ninput dimensions. For example, DEHB is up to 1000x faster than random search.\nIt is also efficient in computational time, conceptually simple and easy to\nimplement, positioning it well to become a new default HPO method.",
          "link": "http://arxiv.org/abs/2105.09821",
          "publishedOn": "2021-05-23T06:10:39.793Z",
          "wordCount": 571,
          "title": "DEHB: Evolutionary Hyberband for Scalable, Robust and Efficient Hyperparameter Optimization. (arXiv:2105.09821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuangshuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karayiannidis_Y/0/1/0/all/0/1\">Yiannis Karayiannidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bjorkman_M/0/1/0/all/0/1\">M&#xe5;rten Bj&#xf6;rkman</a>",
          "description": "Learning generative models and inferring latent trajectories have shown to be\nchallenging for time series due to the intractable marginal likelihoods of\nflexible generative models. It can be addressed by surrogate objectives for\noptimization. We propose Monte Carlo filtering objectives (MCFOs), a family of\nvariational objectives for jointly learning parametric generative models and\namortized adaptive importance proposals of time series. MCFOs extend the\nchoices of likelihood estimators beyond Sequential Monte Carlo in\nstate-of-the-art objectives, possess important properties revealing the factors\nfor the tightness of objectives, and allow for less biased and variant gradient\nestimates. We demonstrate that the proposed MCFOs and gradient estimations lead\nto efficient and stable model learning, and learned generative models well\nexplain data and importance proposals are more sample efficient on various\nkinds of time series data.",
          "link": "http://arxiv.org/abs/2105.09801",
          "publishedOn": "2021-05-23T06:10:39.786Z",
          "wordCount": 595,
          "title": "Monte Carlo Filtering Objectives: A New Family of Variational Objectives to Learn Generative Model and Neural Adaptive Proposal for Time Series. (arXiv:2105.09801v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ran Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rujun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhuoling Xiao</a>",
          "description": "The technology for Visual Odometry (VO) that estimates the position and\norientation of the moving object through analyzing the image sequences captured\nby on-board cameras, has been well investigated with the rising interest in\nautonomous driving. This paper studies monocular VO from the perspective of\nDeep Learning (DL). Unlike most current learning-based methods, our approach,\ncalled DeepAVO, is established on the intuition that features contribute\ndiscriminately to different motion patterns. Specifically, we present a novel\nfour-branch network to learn the rotation and translation by leveraging\nConvolutional Neural Networks (CNNs) to focus on different quadrants of optical\nflow input. To enhance the ability of feature selection, we further introduce\nan effective channel-spatial attention mechanism to force each branch to\nexplicitly distill related information for specific Frame to Frame (F2F) motion\nestimation. Experiments on various datasets involving outdoor driving and\nindoor walking scenarios show that the proposed DeepAVO outperforms the\nstate-of-the-art monocular methods by a large margin, demonstrating competitive\nperformance to the stereo VO algorithm and verifying promising potential for\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09899",
          "publishedOn": "2021-05-23T06:10:39.777Z",
          "wordCount": 619,
          "title": "DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry. (arXiv:2105.09899v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Cong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jingwen Leng</a>",
          "description": "Leveraging sparsity in deep neural network (DNN) models is promising for\naccelerating model inference. Yet existing GPUs can only leverage the sparsity\nfrom weights but not activations, which are dynamic, unpredictable, and hence\nchallenging to exploit. In this work, we propose a novel architecture to\nefficiently harness the dual-side sparsity (i.e., weight and activation\nsparsity). We take a systematic approach to understand the (dis)advantages of\nprevious sparsity-related architectures and propose a novel, unexplored\nparadigm that combines outer-product computation primitive and bitmap-based\nencoding format. We demonstrate the feasibility of our design with minimal\nchanges to the existing production-scale inner-product-based Tensor Core. We\npropose a set of novel ISA extensions and co-design the matrix-matrix\nmultiplication and convolution algorithms, which are the two dominant\ncomputation patterns in today's DNN models, to exploit our new dual-side sparse\nTensor Core. Our evaluation shows that our design can fully unleash the\ndual-side DNN sparsity and improve the performance by up to one order of\nmagnitude with \\hl{small} hardware overhead.",
          "link": "http://arxiv.org/abs/2105.09564",
          "publishedOn": "2021-05-23T06:10:39.770Z",
          "wordCount": 586,
          "title": "Dual-side Sparse Tensor Core. (arXiv:2105.09564v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09720",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mudiyanselage_T/0/1/0/all/0/1\">Thosini Bamunu Mudiyanselage</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Senanayake_N/0/1/0/all/0/1\">Nipuna Senanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1\">Chunyan Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yi Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanqing Zhang</a>",
          "description": "The novel corona virus (Covid-19) has introduced significant challenges due\nto its rapid spreading nature through respiratory transmission. As a result,\nthere is a huge demand for Artificial Intelligence (AI) based quick disease\ndiagnosis methods as an alternative to high demand tests such as Polymerase\nChain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective\nradiography technique due to resource availability and quick screening. But, a\nsufficient and systematic data collection that is required by complex deep\nleaning (DL) models is more difficult and hence there are recent efforts that\nutilize transfer learning to address this issue. Still these transfer learnt\nmodels suffer from lack of generalization and increased bias to the training\ndataset resulting poor performance for unseen data. Limited correlation of the\ntransferred features from the pre-trained model to a specific medical imaging\ndomain like X-ray and overfitting on fewer data can be reasons for this\ncircumstance. In this work, we propose a novel Graph Convolution Neural Network\n(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR\nimages and meta information about patients. The proposed method exploits\nimportant relational knowledge between data instances and their features using\ngraph representation and applies convolution to learn the graph data which is\nnot possible with conventional convolution on Euclidean domain. The results of\nextensive experiments of proposed model on binary (Covid vs normal) and three\nclass (Covid, normal, other pneumonia) classification problems outperform\ndifferent benchmark transfer learnt models, hence overcoming the aforementioned\ndrawbacks.",
          "link": "http://arxiv.org/abs/2105.09720",
          "publishedOn": "2021-05-23T06:10:39.750Z",
          "wordCount": 747,
          "title": "Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks. (arXiv:2105.09720v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1\">Kasra Arnavaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1\">Jelena M. Krivokapic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1\">Silja Heilmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1\">Jakob Andreas B&#xe6;rentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1\">Pia Nyeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>",
          "description": "Motivated by a challenging tubular network segmentation task, this paper\ntackles two commonly encountered problems in biomedical imaging: Topological\nconsistency of the segmentation, and limited annotations. We propose a\ntopological score which measures both topological and geometric consistency\nbetween the predicted and ground truth segmentations, applied for model\nselection and validation. We apply our topological score in three scenarios: i.\na U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised\nU-net architecture, which offers a straightforward approach to jointly training\nthe network both as an autoencoder and a segmentation algorithm. This allows us\nto utilize un-annotated data for training a representation that generalizes\nacross test data variability, in spite of our annotated training data having\nvery limited variation. Our contributions are validated on a challenging\nsegmentation task, locating tubular structures in the fetal pancreas from noisy\nlive imaging confocal microscopy.",
          "link": "http://arxiv.org/abs/2105.09737",
          "publishedOn": "2021-05-23T06:10:39.743Z",
          "wordCount": 590,
          "title": "Semi-supervised, Topology-Aware Segmentation of Tubular Structures from Live Imaging 3D Microscopy. (arXiv:2105.09737v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yue Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>",
          "description": "The Graph Augmented Multi-layer Perceptron (GA-MLP) model is an attractive\nalternative to Graph Neural Networks (GNNs). This is because it is resistant to\nthe over-smoothing problem, and deeper GA-MLP models yield better performance.\nGA-MLP models are traditionally optimized by the Stochastic Gradient Descent\n(SGD). However, SGD suffers from the layer dependency problem, which prevents\nthe gradients of different layers of GA-MLP models from being calculated in\nparallel. In this paper, we propose a parallel deep learning Alternating\nDirection Method of Multipliers (pdADMM) framework to achieve model\nparallelism: parameters in each layer of GA-MLP models can be updated in\nparallel. The extended pdADMM-Q algorithm reduces communication cost by\nutilizing the quantization technique. Theoretical convergence to a critical\npoint of the pdADMM algorithm and the pdADMM-Q algorithm is provided with a\nsublinear convergence rate $o(1/k)$. Extensive experiments in six benchmark\ndatasets demonstrate that the pdADMM can lead to high speedup, and outperforms\nall the existing state-of-the-art comparison methods.",
          "link": "http://arxiv.org/abs/2105.09837",
          "publishedOn": "2021-05-23T06:10:39.688Z",
          "wordCount": 638,
          "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on Gradient-Free ADMM framework. (arXiv:2105.09837v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Thai_B/0/1/0/all/0/1\">Binh Nguyen-Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1\">Catherine Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawi_N/0/1/0/all/0/1\">Nadia Badawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "The absence or abnormality of fidgety movements of joints or limbs is\nstrongly indicative of cerebral palsy in infants. Developing computer-based\nmethods for assessing infant movements in videos is pivotal for improved\ncerebral palsy screening. Most existing methods use appearance-based features\nand are thus sensitive to strong but irrelevant signals caused by background\nclutter or a moving camera. Moreover, these features are computed over the\nwhole frame, thus they measure gross whole body movements rather than specific\njoint/limb motion.\n\nAddressing these challenges, we develop and validate a new method for fidgety\nmovement assessment from consumer-grade videos using human poses extracted from\nshort clips. Human poses capture only relevant motion profiles of joints and\nlimbs and are thus free from irrelevant appearance artifacts. The dynamics and\ncoordination between joints are modeled using spatio-temporal graph\nconvolutional networks. Frames and body parts that contain discriminative\ninformation about fidgety movements are selected through a spatio-temporal\nattention mechanism. We validate the proposed model on the cerebral palsy\nscreening task using a real-life consumer-grade video dataset collected at an\nAustralian hospital through the Cerebral Palsy Alliance, Australia. Our\nexperiments show that the proposed method achieves the ROC-AUC score of 81.87%,\nsignificantly outperforming existing competing methods with better\ninterpretability.",
          "link": "http://arxiv.org/abs/2105.09783",
          "publishedOn": "2021-05-23T06:10:39.663Z",
          "wordCount": 657,
          "title": "A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos. (arXiv:2105.09783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Devleena Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_Y/0/1/0/all/0/1\">Yasutaka Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivek_R/0/1/0/all/0/1\">Rajan P. Vivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_N/0/1/0/all/0/1\">Naoto Takeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fish_S/0/1/0/all/0/1\">Sean T. Fish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploetz_T/0/1/0/all/0/1\">Thomas Ploetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1\">Sonia Chernova</a>",
          "description": "Smart home environments are designed to provide services that help improve\nthe quality of life for the occupant via a variety of sensors and actuators\ninstalled throughout the space. Many automated actions taken by a smart home\nare governed by the output of an underlying activity recognition system.\nHowever, activity recognition systems may not be perfectly accurate and\ntherefore inconsistencies in smart home operations can lead a user to wonder\n\"why did the smart home do that?\" In this work, we build on insights from\nExplainable Artificial Intelligence (XAI) techniques to contribute\ncomputational methods for explainable activity recognition. Specifically, we\ngenerate explanations for smart home activity recognition systems that explain\nwhat about an activity led to the given classification. To do so, we introduce\nfour computational techniques for generating natural language explanations of\nsmart home data and compare their effectiveness at generating meaningful\nexplanations. Through a study with everyday users, we evaluate user preferences\ntowards the four explanation types. Our results show that the leading approach,\nSHAP, has a 92% success rate in generating accurate explanations. Moreover, 84%\nof sampled scenarios users preferred natural language explanations over a\nsimple activity label, underscoring the need for explainable activity\nrecognition systems. Finally, we show that explanations generated by some XAI\nmethods can lead users to lose confidence in the accuracy of the underlying\nactivity recognition model, while others lead users to gain confidence. Taking\nall studied factors into consideration, we make a recommendation regarding\nwhich existing XAI method leads to the best performance in the domain of smart\nhome automation, and discuss a range of topics for future work in this area.",
          "link": "http://arxiv.org/abs/2105.09787",
          "publishedOn": "2021-05-23T06:10:39.642Z",
          "wordCount": 704,
          "title": "Explainable Activity Recognition for Smart Home Systems. (arXiv:2105.09787v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09788",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1\">Ruiqi Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_G/0/1/0/all/0/1\">Ganggang Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shang_Z/0/1/0/all/0/1\">Zuofeng Shang</a>",
          "description": "When data is of an extraordinarily large size or physically stored in\ndifferent locations, the distributed nearest neighbor (NN) classifier is an\nattractive tool for classification. We propose a novel distributed adaptive NN\nclassifier for which the number of nearest neighbors is a tuning parameter\nstochastically chosen by a data-driven criterion. An early stopping rule is\nproposed when searching for the optimal tuning parameter, which not only speeds\nup the computation but also improves the finite sample performance of the\nproposed Algorithm. Convergence rate of excess risk of the distributed adaptive\nNN classifier is investigated under various sub-sample size compositions. In\nparticular, we show that when the sub-sample sizes are sufficiently large, the\nproposed classifier achieves the nearly optimal convergence rate. Effectiveness\nof the proposed approach is demonstrated through simulation studies as well as\nan empirical application to a real-world dataset.",
          "link": "http://arxiv.org/abs/2105.09788",
          "publishedOn": "2021-05-23T06:10:39.622Z",
          "wordCount": 571,
          "title": "Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory. (arXiv:2105.09788v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borkar_J/0/1/0/all/0/1\">Jaydeep Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "There has been a rise in the use of Machine Learning as a Service (MLaaS)\nVision APIs as they offer multiple services including pre-built models and\nalgorithms, which otherwise take a huge amount of resources if built from\nscratch. As these APIs get deployed for high-stakes applications, it's very\nimportant that they are robust to different manipulations. Recent works have\nonly focused on typical adversarial attacks when evaluating the robustness of\nvision APIs. We propose two new aspects of adversarial image generation methods\nand evaluate them on the robustness of Google Cloud Vision API's optical\ncharacter recognition service and object detection APIs deployed in real-world\nsettings such as sightengine.com, picpurify.com, Google Cloud Vision API, and\nMicrosoft Azure's Computer Vision API. Specifically, we go beyond the\nconventional small-noise adversarial attacks and introduce secret embedding and\ntransparent adversarial examples as a simpler way to evaluate robustness. These\nmethods are so straightforward that even non-specialists can craft such\nattacks. As a result, they pose a serious threat where APIs are used for\nhigh-stakes applications. Our transparent adversarial examples successfully\nevade state-of-the art object detections APIs such as Azure Cloud Vision\n(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).\n90% of the images have a secret embedded text that successfully fools the\nvision of time-limited humans but is detected by Google Cloud Vision API's\noptical character recognition. Complementing to current research, our results\nprovide simple but unconventional methods on robustness evaluation.",
          "link": "http://arxiv.org/abs/2105.09685",
          "publishedOn": "2021-05-23T06:10:39.616Z",
          "wordCount": 694,
          "title": "Simple Transparent Adversarial Examples. (arXiv:2105.09685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09716",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">Yongfeng Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhao_M/0/1/0/all/0/1\">Mingming Zhao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wen_Z/0/1/0/all/0/1\">Zaiwen Wen</a>",
          "description": "In this paper, we consider the linear programming (LP) formulation for deep\nreinforcement learning. The number of the constraints depends on the size of\nstate and action spaces, which makes the problem intractable in large or\ncontinuous environments. The general augmented Lagrangian method suffers the\ndouble-sampling obstacle in solving the LP. Namely, the conditional\nexpectations originated from the constraint functions and the quadratic\npenalties in the augmented Lagrangian function impose difficulties in sampling\nand evaluation. Motivated from the updates of the multipliers, we overcome the\nobstacles in minimizing the augmented Lagrangian function by replacing the\nintractable conditional expectations with the multipliers. Therefore, a deep\nparameterized augment Lagrangian method is proposed. Furthermore, the\nreplacement provides a promising breakthrough to integrate the two steps in the\naugmented Lagrangian method into a single constrained problem. A general\ntheoretical analysis shows that the solutions generated from a sequence of the\nconstrained optimizations converge to the optimal solution of the LP if the\nerror is controlled properly. A theoretical analysis on the quadratic penalty\nalgorithm under neural tangent kernel setting shows the residual can be\narbitrarily small if the parameter in network and optimization algorithm is\nchosen suitably. Preliminary experiments illustrate that our method is\ncompetitive to other state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2105.09716",
          "publishedOn": "2021-05-23T06:10:39.608Z",
          "wordCount": 642,
          "title": "A Stochastic Composite Augmented Lagrangian Method For Reinforcement Learning. (arXiv:2105.09716v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pryor_W/0/1/0/all/0/1\">Will Pryor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnoy_Y/0/1/0/all/0/1\">Yotam Barnoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raval_S/0/1/0/all/0/1\">Suraj Raval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mair_L/0/1/0/all/0/1\">Lamar Mair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerner_D/0/1/0/all/0/1\">Daniel Lerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erin_O/0/1/0/all/0/1\">Onder Erin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory D. Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Mercado_Y/0/1/0/all/0/1\">Yancy Diaz-Mercado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krieger_A/0/1/0/all/0/1\">Axel Krieger</a>",
          "description": "Real-time visual localization of needles is necessary for various surgical\napplications, including surgical automation and visual feedback. In this study\nwe investigate localization and autonomous robotic control of needles in the\ncontext of our magneto-suturing system. Our system holds the potential for\nsurgical manipulation with the benefit of minimal invasiveness and reduced\npatient side effects. However, the non-linear magnetic fields produce\nunintuitive forces and demand delicate position-based control that exceeds the\ncapabilities of direct human manipulation. This makes automatic needle\nlocalization a necessity. Our localization method combines neural network-based\nsegmentation and classical techniques, and we are able to consistently locate\nour needle with 0.73 mm RMS error in clean environments and 2.72 mm RMS error\nin challenging environments with blood and occlusion. The average localization\nRMS error is 2.16 mm for all environments we used in the experiments. We\ncombine this localization method with our closed-loop feedback control system\nto demonstrate the further applicability of localization to autonomous control.\nOur needle is able to follow a running suture path in (1) no blood, no tissue;\n(2) heavy blood, no tissue; (3) no blood, with tissue; and (4) heavy blood,\nwith tissue environments. The tip position tracking error ranges from 2.6 mm to\n3.7 mm RMS, opening the door towards autonomous suturing tasks.",
          "link": "http://arxiv.org/abs/2105.09481",
          "publishedOn": "2021-05-23T06:10:39.583Z",
          "wordCount": 664,
          "title": "Localization and Control of Magnetic Suture Needles in Cluttered Surgical Site with Blood and Tissue. (arXiv:2105.09481v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1\">Keyue Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuzhuo Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Distantly supervised (DS) relation extraction (RE) has attracted much\nattention in the past few years as it can utilize large-scale auto-labeled\ndata. However, its evaluation has long been a problem: previous works either\ntook costly and inconsistent methods to manually examine a small sample of\nmodel predictions, or directly test models on auto-labeled data -- which, by\nour check, produce as much as 53% wrong labels at the entity pair level in the\npopular NYT10 dataset. This problem has not only led to inaccurate evaluation,\nbut also made it hard to understand where we are and what's left to improve in\nthe research of DS-RE. To evaluate DS-RE models in a more credible way, we\nbuild manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,\nand thoroughly evaluate several competitive models, especially the latest\npre-trained ones. The experimental results show that the manual evaluation can\nindicate very different conclusions from automatic ones, especially some\nunexpected observations, e.g., pre-trained models can achieve dominating\nperformance while being more susceptible to false-positives compared to\nprevious methods. We hope that both our manual test sets and novel observations\ncan help advance future DS-RE research.",
          "link": "http://arxiv.org/abs/2105.09543",
          "publishedOn": "2021-05-23T06:10:39.576Z",
          "wordCount": 643,
          "title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction. (arXiv:2105.09543v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09679",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Kimura_S/0/1/0/all/0/1\">Shun Kimura</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ota_K/0/1/0/all/0/1\">Keisuke Ota</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Takeda_K/0/1/0/all/0/1\">Koujin Takeda</a>",
          "description": "Neuronal ensemble inference is a significant problem in the study of\nbiological neural networks. Various methods have been proposed for ensemble\ninference from experimental data of neuronal activity. Among them, Bayesian\ninference approach with generative model was proposed recently. However, this\nmethod requires large computational cost for appropriate inference. In this\nwork, we give an improved Bayesian inference algorithm by modifying update rule\nin Markov chain Monte Carlo method and introducing the idea of simulated\nannealing for hyperparameter control. We compare the performance of ensemble\ninference between our algorithm and the original one, and discuss the advantage\nof our method.",
          "link": "http://arxiv.org/abs/2105.09679",
          "publishedOn": "2021-05-23T06:10:39.539Z",
          "wordCount": 556,
          "title": "Improved Neuronal Ensemble Inference with Generative Model and MCMC. (arXiv:2105.09679v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gruttemeier_N/0/1/0/all/0/1\">Niels Gr&#xfc;ttemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1\">Christian Komusiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morawietz_N/0/1/0/all/0/1\">Nils Morawietz</a>",
          "description": "A Bayesian network is a directed acyclic graph that represents statistical\ndependencies between variables of a joint probability distribution. A\nfundamental task in data science is to learn a Bayesian network from observed\ndata. \\textsc{Polytree Learning} is the problem of learning an optimal Bayesian\nnetwork that fulfills the additional property that its underlying undirected\ngraph is a forest. In this work, we revisit the complexity of \\textsc{Polytree\nLearning}. We show that \\textsc{Polytree Learning} can be solved in $3^n \\cdot\n|I|^{\\mathcal{O}(1)}$ time where $n$ is the number of variables and $|I|$ is\nthe total instance size. Moreover, we consider the influence of the number of\nvariables $d$ that might receive a nonempty parent set in the final DAG on the\ncomplexity of \\textsc{Polytree Learning}. We show that \\textsc{Polytree\nLearning} has no $f(d)\\cdot |I|^{\\mathcal{O}(1)}$-time algorithm, unlike\nBayesian network learning which can be solved in $2^d \\cdot\n|I|^{\\mathcal{O}(1)}$ time. We show that, in contrast, if $d$ and the maximum\nparent set size are bounded, then we can obtain efficient algorithms.",
          "link": "http://arxiv.org/abs/2105.09675",
          "publishedOn": "2021-05-23T06:10:39.519Z",
          "wordCount": 602,
          "title": "On the Parameterized Complexity of Polytree Learning. (arXiv:2105.09675v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09670",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_H/0/1/0/all/0/1\">Huolan Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yongkai Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_C/0/1/0/all/0/1\">Chenguang Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cheng_H/0/1/0/all/0/1\">Huimin Cheng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhong_W/0/1/0/all/0/1\">Wenxuan Zhong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>",
          "description": "Background: Extensive clinical evidence suggests that a preventive screening\nof coronary heart disease (CHD) at an earlier stage can greatly reduce the\nmortality rate. We use 64 two-dimensional speckle tracking echocardiography\n(2D-STE) features and seven clinical features to predict whether one has CHD.\nMethods: We develop a machine learning approach that integrates a number of\npopular classification methods together by model stacking, and generalize the\ntraditional stacking method to a two-step stacking method to improve the\ndiagnostic performance. Results: By borrowing strengths from multiple\nclassification models through the proposed method, we improve the CHD\nclassification accuracy from around 70% to 87.7% on the testing set. The\nsensitivity of the proposed method is 0.903 and the specificity is 0.843, with\nan AUC of 0.904, which is significantly higher than those of the individual\nclassification models. Conclusions: Our work lays a foundation for the\ndeployment of speckle tracking echocardiography-based screening tools for\ncoronary heart disease.",
          "link": "http://arxiv.org/abs/2105.09670",
          "publishedOn": "2021-05-23T06:10:39.511Z",
          "wordCount": 612,
          "title": "Ensemble machine learning approach for screening of coronary heart disease based on echocardiography and risk factors. (arXiv:2105.09670v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1\">Danilo Dessi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1\">Rim Helaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1\">Daniele Riboni</a>",
          "description": "Today, we are seeing an ever-increasing number of clinical notes that contain\nclinical results, images, and textual descriptions of patient's health state.\nAll these data can be analyzed and employed to cater novel services that can\nhelp people and domain experts with their common healthcare tasks. However,\nmany technologies such as Deep Learning and tools like Word Embeddings have\nstarted to be investigated only recently, and many challenges remain open when\nit comes to healthcare domain applications. To address these challenges, we\npropose the use of Deep Learning and Word Embeddings for identifying sixteen\nmorbidity types within textual descriptions of clinical records. For this\npurpose, we have used a Deep Learning model based on Bidirectional Long-Short\nTerm Memory (LSTM) layers which can exploit state-of-the-art vector\nrepresentations of data such as Word Embeddings. We have employed pre-trained\nWord Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained\non the target domain. Furthermore, we have compared the performances of the\ndeep learning approaches against the traditional tf-idf using Support Vector\nMachine and Multilayer perceptron (our baselines). From the obtained results it\nseems that the latter outperforms the combination of Deep Learning approaches\nusing any word embeddings. Our preliminary results indicate that there are\nspecific features that make the dataset biased in favour of traditional machine\nlearning approaches.",
          "link": "http://arxiv.org/abs/2105.09632",
          "publishedOn": "2021-05-23T06:10:39.502Z",
          "wordCount": 680,
          "title": "TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1\">Amit Daniely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granot_E/0/1/0/all/0/1\">Elad Granot</a>",
          "description": "As machine learning increasingly becomes more prevalent in our everyday life,\nmany organizations offer neural-networks based services as a black-box. The\nreasons for hiding a learning model may vary: e.g., preventing copying of its\nbehavior or keeping back an adversarial from reverse-engineering its mechanism\nand revealing sensitive information about its training data.\n\nHowever, even as a black-box, some information can still be discovered by\nspecific queries. In this work, we show a polynomial-time algorithm that uses a\npolynomial number of queries to mimic precisely the behavior of a three-layer\nneural network that uses ReLU activation.",
          "link": "http://arxiv.org/abs/2105.09673",
          "publishedOn": "2021-05-23T06:10:39.448Z",
          "wordCount": 521,
          "title": "An Exact Poly-Time Membership-Queries Algorithm for Extraction a three-Layer ReLU Network. (arXiv:2105.09673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09618",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Malem_Shinitski_N/0/1/0/all/0/1\">Noa Malem-Shinitski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ojeda_C/0/1/0/all/0/1\">Cesar Ojeda</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Opper_M/0/1/0/all/0/1\">Manfred Opper</a>",
          "description": "Traditionally, Hawkes processes are used to model time--continuous point\nprocesses with history dependence. Here we propose an extended model where the\nself--effects are of both excitatory and inhibitory type and follow a Gaussian\nProcess. Whereas previous work either relies on a less flexible\nparameterization of the model, or requires a large amount of data, our\nformulation allows for both a flexible model and learning when data are scarce.\nWe continue the line of work of Bayesian inference for Hawkes processes, and\nour approach dispenses with the necessity of estimating a branching structure\nfor the posterior, as we perform inference on an aggregated sum of Gaussian\nProcesses. Efficient approximate Bayesian inference is achieved via data\naugmentation, and we describe a mean--field variational inference approach to\nlearn the model parameters. To demonstrate the flexibility of the model we\napply our methodology on data from three different domains and compare it to\npreviously reported results.",
          "link": "http://arxiv.org/abs/2105.09618",
          "publishedOn": "2021-05-23T06:10:39.438Z",
          "wordCount": 584,
          "title": "Nonlinear Hawkes Process with Gaussian Process Self Effects. (arXiv:2105.09618v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1\">Raluca Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepecki_J/0/1/0/all/0/1\">Jaroslaw Rzepecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuniga_E/0/1/0/all/0/1\">Evelyn Zuniga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costello_G/0/1/0/all/0/1\">Gavin Costello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1\">Guy Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1\">Ali Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>",
          "description": "A key challenge on the path to developing agents that learn complex\nhuman-like behavior is the need to quickly and accurately quantify\nhuman-likeness. While human assessments of such behavior can be highly\naccurate, speed and scalability are limited. We address these limitations\nthrough a novel automated Navigation Turing Test (ANTT) that learns to predict\nhuman judgments of human-likeness. We demonstrate the effectiveness of our\nautomated NTT on a navigation task in a complex 3D environment. We investigate\nsix classification models to shed light on the types of architectures best\nsuited to this task, and validate them against data collected through a human\nNTT. Our best models achieve high accuracy when distinguishing true human and\nagent behavior. At the same time, we show that predicting finer-grained human\nassessment of agents' progress towards human-like behavior remains unsolved.\nOur work takes an important step towards agents that more effectively learn\ncomplex human-like behavior.",
          "link": "http://arxiv.org/abs/2105.09637",
          "publishedOn": "2021-05-23T06:10:39.418Z",
          "wordCount": 589,
          "title": "Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation. (arXiv:2105.09637v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose {\\name}, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. {\\name} utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that {\\name} achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.",
          "link": "http://arxiv.org/abs/2105.09601",
          "publishedOn": "2021-05-23T06:10:39.393Z",
          "wordCount": 667,
          "title": "See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bountrogiannis_K/0/1/0/all/0/1\">Konstantinos Bountrogiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzagkarakis_G/0/1/0/all/0/1\">George Tzagkarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalides_P/0/1/0/all/0/1\">Panagiotis Tsakalides</a>",
          "description": "Due to the importance of the lower bounding distances and the attractiveness\nof symbolic representations, the family of symbolic aggregate approximations\n(SAX) has been used extensively for encoding time series data. However, typical\nSAX-based methods rely on two restrictive assumptions; the Gaussian\ndistribution and equiprobable symbols. This paper proposes two novel\ndata-driven SAX-based symbolic representations, distinguished by their\ndiscretization steps. The first representation, oriented for general data\ncompaction and indexing scenarios, is based on the combination of kernel\ndensity estimation and Lloyd-Max quantization to minimize the information loss\nand mean squared error in the discretization step. The second method, oriented\nfor high-level mining tasks, employs the Mean-Shift clustering method and is\nshown to enhance anomaly detection in the lower-dimensional space. Besides, we\nverify on a theoretical basis a previously observed phenomenon of the intrinsic\nprocess that results in a lower than the expected variance of the intermediate\npiecewise aggregate approximation. This phenomenon causes an additional\ninformation loss but can be avoided with a simple modification. The proposed\nrepresentations possess all the attractive properties of the conventional SAX\nmethod. Furthermore, experimental evaluation on real-world datasets\ndemonstrates their superiority compared to the traditional SAX and an\nalternative data-driven SAX variant.",
          "link": "http://arxiv.org/abs/2105.09592",
          "publishedOn": "2021-05-23T06:10:39.382Z",
          "wordCount": 634,
          "title": "Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zejin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>",
          "description": "The increasing concerns about data privacy and security drives the emergence\nof a new field of studying privacy-preserving machine learning from isolated\ndata sources, i.e., \\textit{federated learning}. Vertical federated learning,\nwhere different parties hold different features for common users, has a great\npotential of driving a more variety of business cooperation among enterprises\nin different fields. Decision tree models especially decision tree ensembles\nare a class of widely applied powerful machine learning models with high\ninterpretability and modeling efficiency. However, the interpretability are\ncompromised in these works such as SecureBoost since the feature names are not\nexposed to avoid possible data breaches due to the unprotected decision path.\nIn this paper, we shall propose Fed-EINI, an efficient and interpretable\ninference framework for federated decision tree models with only one round of\nmulti-party communication. We shall compute the candidate sets of leaf nodes\nbased on the local data at each party in parallel, followed by securely\ncomputing the weight of the only leaf node in the intersection of the candidate\nsets. We propose to protect the decision path by the efficient additively\nhomomorphic encryption method, which allows the disclosure of feature names and\nthus makes the federated decision trees interpretable. The advantages of\nFed-EINI will be demonstrated through theoretical analysis and extensive\nnumerical results. Experiments show that the inference efficiency is improved\nby over $50\\%$ in average.",
          "link": "http://arxiv.org/abs/2105.09540",
          "publishedOn": "2021-05-23T06:10:39.363Z",
          "wordCount": 684,
          "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voiculescu_I/0/1/0/all/0/1\">Irina Voiculescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>",
          "description": "Entanglement is a physical phenomenon, which has fueled recent successes of\nquantum algorithms. Although quantum neural networks (QNNs) have shown\npromising results in solving simple machine learning tasks recently, for the\ntime being, the effect of entanglement in QNNs and the behavior of QNNs in\nbinary pattern classification are still underexplored. In this work, we provide\nsome theoretical insight into the properties of QNNs by presenting and\nanalyzing a new form of invariance embedded in QNNs for both quantum binary\nclassification and quantum representation learning, which we term negational\nsymmetry. Given a quantum binary signal and its negational counterpart where a\nbitwise NOT operation is applied to each quantum bit of the binary signal, a\nQNN outputs the same logits. That is to say, QNNs cannot differentiate a\nquantum binary signal and its negational counterpart in a binary classification\ntask. We further empirically evaluate the negational symmetry of QNNs in binary\npattern classification tasks using Google's quantum computing framework. The\ntheoretical and experimental results suggest that negational symmetry is a\nfundamental property of QNNs, which is not shared by classical models. Our\nfindings also imply that negational symmetry is a double-edged sword in\npractical quantum applications.",
          "link": "http://arxiv.org/abs/2105.09580",
          "publishedOn": "2021-05-23T06:10:39.355Z",
          "wordCount": 634,
          "title": "Negational Symmetry of Quantum Neural Networks for Binary Pattern Classification. (arXiv:2105.09580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1\">Takashi Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1\">Liu Ziyin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masahito Ueda</a>",
          "description": "Stochastic gradient descent (SGD) undergoes complicated multiplicative noise\nfor the mean-square loss. We use this property of the SGD noise to derive a\nstochastic differential equation (SDE) with simpler additive noise by\nperforming a non-uniform transformation of the time variable. In the SDE, the\ngradient of the loss is replaced by that of the logarithmized loss.\nConsequently, we show that, near a local or global minimum, the stationary\ndistribution $P_\\mathrm{ss}(\\theta)$ of the network parameters $\\theta$ follows\na power-law with respect to the loss function $L(\\theta)$, i.e.\n$P_\\mathrm{ss}(\\theta)\\propto L(\\theta)^{-\\phi}$ with the exponent $\\phi$\nspecified by the mini-batch size, the learning rate, and the Hessian at the\nminimum. We obtain the escape rate formula from a local minimum, which is\ndetermined not by the loss barrier height $\\Delta L=L(\\theta^s)-L(\\theta^*)$\nbetween a minimum $\\theta^*$ and a saddle $\\theta^s$ but by the logarithmized\nloss barrier height $\\Delta\\log L=\\log[L(\\theta^s)/L(\\theta^*)]$. Our\nescape-rate formula explains an empirical fact that SGD prefers flat minima\nwith low effective dimensions.",
          "link": "http://arxiv.org/abs/2105.09557",
          "publishedOn": "2021-05-23T06:10:39.348Z",
          "wordCount": 604,
          "title": "Logarithmic landscape and power-law escape rate of SGD. (arXiv:2105.09557v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagtap_A/0/1/0/all/0/1\">Ameya D. Jagtap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yeonjong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "We propose a new type of neural networks, Kronecker neural networks (KNNs),\nthat form a general framework for neural networks with adaptive activation\nfunctions. KNNs employ the Kronecker product, which provides an efficient way\nof constructing a very wide network while keeping the number of parameters low.\nOur theoretical analysis reveals that under suitable conditions, KNNs induce a\nfaster decay of the loss than that by the feed-forward networks. This is also\nempirically verified through a set of computational examples. Furthermore,\nunder certain technical assumptions, we establish global convergence of\ngradient descent for KNNs. As a specific case, we propose the Rowdy activation\nfunction that is designed to get rid of any saturation region by injecting\nsinusoidal fluctuations, which include trainable parameters. The proposed Rowdy\nactivation function can be employed in any neural network architecture like\nfeed-forward neural networks, Recurrent neural networks, Convolutional neural\nnetworks etc. The effectiveness of KNNs with Rowdy activation is demonstrated\nthrough various computational experiments including function approximation\nusing feed-forward neural networks, solution inference of partial differential\nequations using the physics-informed neural networks, and standard deep\nlearning benchmark problems using convolutional and fully-connected neural\nnetworks.",
          "link": "http://arxiv.org/abs/2105.09513",
          "publishedOn": "2021-05-23T06:10:39.334Z",
          "wordCount": 632,
          "title": "Deep Kronecker neural networks: A general framework for neural networks with adaptive activation functions. (arXiv:2105.09513v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09536",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Fried_S/0/1/0/all/0/1\">Sela Fried</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wolfer_G/0/1/0/all/0/1\">Geoffrey Wolfer</a>",
          "description": "We formulate extendibility of the minimax one-trajectory length of several\nstatistical Markov chains inference problems and give sufficient conditions for\nboth the possibility and impossibility of such extensions. We follow up and\napply this framework to recently published results on learning and identity\ntesting of ergodic Markov chains. In particular, we show that for some of the\naforementioned results, we can omit the aperiodicity requirement by simulating\nan $\\alpha$-lazy version of the original process, and quantify the incurred\ncost of removing this assumption.",
          "link": "http://arxiv.org/abs/2105.09536",
          "publishedOn": "2021-05-23T06:10:39.325Z",
          "wordCount": 516,
          "title": "On the $\\alpha$-lazy version of Markov chains in estimation and testing problems. (arXiv:2105.09536v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Takamichi Toda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriwaki_D/0/1/0/all/0/1\">Daisuke Moriwaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1\">Kazuhiro Ota</a>",
          "description": "Large and acute economic shocks such as the 2007-2009 financial crisis and\nthe current COVID-19 infections rapidly change the economic environment. In\nsuch a situation, the importance of real-time economic analysis using\nalternative datais emerging. Alternative data such as search query and location\ndata are closer to real-time and richer than official statistics that are\ntypically released once a month in an aggregated form. We take advantage of\nspatio-temporal granularity of alternative data and propose a\nmixed-FrequencyAggregate Learning (MF-AGL)model that predicts economic\nindicators for the smaller areas in real-time. We apply the model for the\nreal-world problem; prediction of the number of job applicants which is closely\nrelated to the unemployment rates. We find that the proposed model predicts (i)\nthe regional heterogeneity of the labor market condition and (ii) the rapidly\nchanging economic status. The model can be applied to various tasks, especially\neconomic analysis",
          "link": "http://arxiv.org/abs/2105.09579",
          "publishedOn": "2021-05-23T06:10:39.318Z",
          "wordCount": 613,
          "title": "Aggregate Learning for Mixed Frequency Data. (arXiv:2105.09579v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09506",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Cai_S/0/1/0/all/0/1\">Shengze Cai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiping Mao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yin_M/0/1/0/all/0/1\">Minglang Yin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "Despite the significant progress over the last 50 years in simulating flow\nproblems using numerical discretization of the Navier-Stokes equations (NSE),\nwe still cannot incorporate seamlessly noisy data into existing algorithms,\nmesh-generation is complex, and we cannot tackle high-dimensional problems\ngoverned by parametrized NSE. Moreover, solving inverse flow problems is often\nprohibitively expensive and requires complex and expensive formulations and new\ncomputer codes. Here, we review flow physics-informed learning, integrating\nseamlessly data and mathematical models, and implementing them using\nphysics-informed neural networks (PINNs). We demonstrate the effectiveness of\nPINNs for inverse problems related to three-dimensional wake flows, supersonic\nflows, and biomedical flows.",
          "link": "http://arxiv.org/abs/2105.09506",
          "publishedOn": "2021-05-23T06:10:39.309Z",
          "wordCount": 535,
          "title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review. (arXiv:2105.09506v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berahmand_K/0/1/0/all/0/1\">Kamal Berahmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasiri_E/0/1/0/all/0/1\">Elahe Nasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forouzandeh_S/0/1/0/all/0/1\">Saman Forouzandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuefeng Li</a>",
          "description": "Predicting links in complex networks has been one of the essential topics\nwithin the realm of data mining and science discovery over the past few years.\nThis problem remains an attempt to identify future, deleted, and redundant\nlinks using the existing links in a graph. Local random walk is considered to\nbe one of the most well-known algorithms in the category of quasi-local\nmethods. It traverses the network using the traditional random walk with a\nlimited number of steps, randomly selecting one adjacent node in each step\namong the nodes which have equal importance. Then this method uses the\ntransition probability between node pairs to calculate the similarity between\nthem. However, in most datasets, this method is not able to perform accurately\nin scoring remarkably similar nodes. In the present article, an efficient\nmethod is proposed for improving local random walk by encouraging random walk\nto move, in every step, towards the node which has a stronger influence.\nTherefore, the next node is selected according to the influence of the source\nnode. To do so, using mutual information, the concept of the asymmetric mutual\ninfluence of nodes is presented. A comparison between the proposed method and\nother similarity-based methods (local, quasi-local, and global) has been\nperformed, and results have been reported for 11 real-world networks. It had a\nhigher prediction accuracy compared with other link prediction approaches.",
          "link": "http://arxiv.org/abs/2105.09494",
          "publishedOn": "2021-05-23T06:10:39.302Z",
          "wordCount": 671,
          "title": "A Preference Random Walk Algorithm for Link Prediction through Mutual Influence Nodes in Complex Networks. (arXiv:2105.09494v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose \\method, a\ntraining method to obtain a single unified multilingual translation model.\nmCOLT is empowered by two techniques: (i) a contrastive learning scheme to\nclose the gap among representations of different languages, and (ii) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mCOLT achieves\ncompetitive or even better performance than a strong pre-trained model mBART on\ntens of WMT benchmarks. For non-English directions, mCOLT achieves an\nimprovement of average 10+ BLEU compared with the multilingual baseline.",
          "link": "http://arxiv.org/abs/2105.09501",
          "publishedOn": "2021-05-23T06:10:39.262Z",
          "wordCount": 576,
          "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frostig_R/0/1/0/all/0/1\">Roy Frostig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Matthew J. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maclaurin_D/0/1/0/all/0/1\">Dougal Maclaurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paszke_A/0/1/0/all/0/1\">Adam Paszke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radul_A/0/1/0/all/0/1\">Alexey Radul</a>",
          "description": "We decompose reverse-mode automatic differentiation into (forward-mode)\nlinearization followed by transposition. Doing so isolates the essential\ndifference between forward- and reverse-mode AD, and simplifies their joint\nimplementation. In particular, once forward-mode AD rules are defined for every\nprimitive operation in a source language, only linear primitives require an\nadditional transposition rule in order to arrive at a complete reverse-mode AD\nimplementation. This is how reverse-mode AD is written in JAX and Dex.",
          "link": "http://arxiv.org/abs/2105.09469",
          "publishedOn": "2021-05-23T06:10:39.240Z",
          "wordCount": 507,
          "title": "Decomposing reverse-mode automatic differentiation. (arXiv:2105.09469v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haghighat_E/0/1/0/all/0/1\">Ehsan Haghighat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekar_A/0/1/0/all/0/1\">Ali Can Bekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madenci_E/0/1/0/all/0/1\">Erdogan Madenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juanes_R/0/1/0/all/0/1\">Ruben Juanes</a>",
          "description": "Deep learning has been the most popular machine learning method in the last\nfew years. In this chapter, we present the application of deep learning and\nphysics-informed neural networks concerning structural mechanics and vibration\nproblems. Demonstration problems involve de-noising data, solution to\ntime-dependent ordinary and partial differential equations, and characterizing\nthe system's response for a given data.",
          "link": "http://arxiv.org/abs/2105.09477",
          "publishedOn": "2021-05-23T06:10:39.180Z",
          "wordCount": 491,
          "title": "Deep learning for solution and inversion of structural mechanics and vibrations. (arXiv:2105.09477v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leblebici_A/0/1/0/all/0/1\">Asim Leblebici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gesoglu_O/0/1/0/all/0/1\">Omer Gesoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basbinar_Y/0/1/0/all/0/1\">Yasemin Basbinar</a>",
          "description": "Until the beginning of 2021, lung cancer is known to be the most common\ncancer in the world. The disease is common due to factors such as occupational\nexposure, smoking and environmental pollution. The early diagnosis and\ntreatment of the disease is of great importance as well as the prevention of\nthe causes that cause the disease. The study was planned to create a web\ninterface that works with machine learning algorithms to predict prognosis\nusing lung cancer clinical and gene expression in the GDC data portal.",
          "link": "http://arxiv.org/abs/2105.09471",
          "publishedOn": "2021-05-23T06:10:39.153Z",
          "wordCount": 515,
          "title": "AI-Decision Support System Interface Using Cancer Related Data for Lung Cancer Prognosis. (arXiv:2105.09471v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lazic_S/0/1/0/all/0/1\">Stanley E. Lazic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_D/0/1/0/all/0/1\">Dominic P. Williams</a>",
          "description": "Knowing the uncertainty in a prediction is critical when making expensive\ninvestment decisions and when patient safety is paramount, but machine learning\n(ML) models in drug discovery typically provide only a single best estimate and\nignore all sources of uncertainty. Predictions from these models may therefore\nbe over-confident, which can put patients at risk and waste resources when\ncompounds that are destined to fail are further developed. Probabilistic\npredictive models (PPMs) can incorporate uncertainty in both the data and\nmodel, and return a distribution of predicted values that represents the\nuncertainty in the prediction. PPMs not only let users know when predictions\nare uncertain, but the intuitive output from these models makes communicating\nrisk easier and decision making better. Many popular machine learning methods\nhave a PPM or Bayesian analogue, making PPMs easy to fit into current\nworkflows. We use toxicity prediction as a running example, but the same\nprinciples apply for all prediction models used in drug discovery. The\nconsequences of ignoring uncertainty and how PPMs account for uncertainty are\nalso described. We aim to make the discussion accessible to a broad\nnon-mathematical audience. Equations are provided to make ideas concrete for\nmathematical readers (but can be skipped without loss of understanding) and\ncode is available for computational researchers\n(https://github.com/stanlazic/ML_uncertainty_quantification).",
          "link": "http://arxiv.org/abs/2105.09474",
          "publishedOn": "2021-05-23T06:10:39.144Z",
          "wordCount": 649,
          "title": "Quantifying sources of uncertainty in drug discovery predictions with probabilistic models. (arXiv:2105.09474v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09467",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Yan_B/0/1/0/all/0/1\">Bicheng Yan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Harp_D/0/1/0/all/0/1\">Dylan Robert Harp</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_B/0/1/0/all/0/1\">Bailian Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pawar_R/0/1/0/all/0/1\">Rajesh Pawar</a>",
          "description": "In this work, an efficient physics-constrained deep learning model is\ndeveloped for solving multiphase flow in 3D heterogeneous porous media. The\nmodel fully leverages the spatial topology predictive capability of\nconvolutional neural networks, and is coupled with an efficient\ncontinuity-based smoother to predict flow responses that need spatial\ncontinuity. Furthermore, the transient regions are penalized to steer the\ntraining process such that the model can accurately capture flow in these\nregions. The model takes inputs including properties of porous media, fluid\nproperties and well controls, and predicts the temporal-spatial evolution of\nthe state variables (pressure and saturation). While maintaining the continuity\nof fluid flow, the 3D spatial domain is decomposed into 2D images for reducing\ntraining cost, and the decomposition results in an increased number of training\ndata samples and better training efficiency. Additionally, a surrogate model is\nseparately constructed as a postprocessor to calculate well flow rate based on\nthe predictions of state variables from the deep learning model. We use the\nexample of CO2 injection into saline aquifers, and apply the\nphysics-constrained deep learning model that is trained from physics-based\nsimulation data and emulates the physics process. The model performs prediction\nwith a speedup of ~1400 times compared to physics-based simulations, and the\naverage temporal errors of predicted pressure and saturation plumes are 0.27%\nand 0.099% respectively. Furthermore, water production rate is efficiently\npredicted by a surrogate model for well flow rate, with a mean error less than\n5%. Therefore, with its unique scheme to cope with the fidelity in fluid flow\nin porous media, the physics-constrained deep learning model can become an\nefficient predictive model for computationally demanding inverse problems or\nother coupled processes.",
          "link": "http://arxiv.org/abs/2105.09467",
          "publishedOn": "2021-05-23T06:10:39.136Z",
          "wordCount": 727,
          "title": "A Physics-Constrained Deep Learning Model for Simulating Multiphase Flow in 3D Heterogeneous Porous Media. (arXiv:2105.09467v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09468",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Tang_H/0/1/0/all/0/1\">Hewei Tang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fu_P/0/1/0/all/0/1\">Pengcheng Fu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sherman_C/0/1/0/all/0/1\">Christopher S. Sherman</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_J/0/1/0/all/0/1\">Jize Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ju_X/0/1/0/all/0/1\">Xin Ju</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hamon_F/0/1/0/all/0/1\">Fran&#xe7;ois Hamon</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Azzolina_N/0/1/0/all/0/1\">Nicholas A. Azzolina</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Burton_Kelly_M/0/1/0/all/0/1\">Matthew Burton-Kelly</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Morris_J/0/1/0/all/0/1\">Joseph P. Morris</a>",
          "description": "Fast assimilation of monitoring data to update forecasts of pressure buildup\nand carbon dioxide (CO2) plume migration under geologic uncertainties is a\nchallenging problem in geologic carbon storage. The high computational cost of\ndata assimilation with a high-dimensional parameter space impedes fast\ndecision-making for commercial-scale reservoir management. We propose to\nleverage physical understandings of porous medium flow behavior with deep\nlearning techniques to develop a fast history matching-reservoir response\nforecasting workflow. Applying an Ensemble Smoother Multiple Data Assimilation\nframework, the workflow updates geologic properties and predicts reservoir\nperformance with quantified uncertainty from pressure history and CO2 plumes\ninterpreted through seismic inversion. As the most computationally expensive\ncomponent in such a workflow is reservoir simulation, we developed surrogate\nmodels to predict dynamic pressure and CO2 plume extents under multi-well\ninjection. The surrogate models employ deep convolutional neural networks,\nspecifically, a wide residual network and a residual U-Net. The workflow is\nvalidated against a flat three-dimensional reservoir model representative of a\nclastic shelf depositional environment. Intelligent treatments are applied to\nbridge between quantities in a true-3D reservoir model and those in a\nsingle-layer reservoir model. The workflow can complete history matching and\nreservoir forecasting with uncertainty quantification in less than one hour on\na mainstream personal workstation.",
          "link": "http://arxiv.org/abs/2105.09468",
          "publishedOn": "2021-05-23T06:10:39.125Z",
          "wordCount": 658,
          "title": "A Deep Learning-Accelerated Data Assimilation and Forecasting Workflow for Commercial-Scale Geologic Carbon Storage. (arXiv:2105.09468v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Kababji_A/0/1/0/all/0/1\">Ayman Al-Kababji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amira_A/0/1/0/all/0/1\">Abbes Amira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensaali_F/0/1/0/all/0/1\">Faycal Bensaali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarouf_A/0/1/0/all/0/1\">Abdulah Jarouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shidqi_L/0/1/0/all/0/1\">Lisan Shidqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djelouat_H/0/1/0/all/0/1\">Hamza Djelouat</a>",
          "description": "Fall detection is a serious healthcare issue that needs to be solved. Falling\nwithout quick medical intervention would lower the chances of survival for the\nelderly, especially if living alone. Hence, the need is there for developing\nfall detection algorithms with high accuracy. This paper presents a novel\nIoT-based system for fall detection that includes a sensing device transmitting\ndata to a mobile application through a cloud-connected gateway device. Then,\nthe focus is shifted to the algorithmic aspect where multiple features are\nextracted from 3-axis accelerometer data taken from existing datasets. The\nresults emphasize on the significance of Continuous Wavelet Transform (CWT) as\nan influential feature for determining falls. CWT, Signal Energy (SE), Signal\nMagnitude Area (SMA), and Signal Vector Magnitude (SVM) features have shown\npromising classification results using K-Nearest Neighbors (KNN) and E-Nearest\nNeighbors (ENN). For all performance metrics (accuracy, recall, precision,\nspecificity, and F1 Score), the achieved results are higher than 95% for a\ndataset of small size, while more than 98.47% score is achieved in the\naforementioned criteria over the UniMiB-SHAR dataset by the same algorithms,\nwhere the classification time for a single test record is extremely efficient\nand is real-time",
          "link": "http://arxiv.org/abs/2105.09461",
          "publishedOn": "2021-05-23T06:10:39.102Z",
          "wordCount": 658,
          "title": "An IoT-Based Framework for Remote Fall Monitoring. (arXiv:2105.09461v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rundi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changxi Zheng</a>",
          "description": "Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation -- describing a shape as\na sequence of computer-aided design (CAD) operations. Unlike meshes and point\nclouds, CAD models encode the user creation process of 3D shapes, widely used\nin numerous industrial and engineering design tasks. However, the sequential\nand irregular structure of CAD operations poses significant challenges for\nexisting 3D generative models. Drawing an analogy between CAD operations and\nnatural language, we propose a CAD generative network based on the Transformer.\nWe demonstrate the performance of our model for both shape autoencoding and\nrandom shape generation. To train our network, we create a new CAD dataset\nconsisting of 179,133 models and their CAD construction sequences. We have made\nthis dataset publicly available to promote future research on this topic.",
          "link": "http://arxiv.org/abs/2105.09492",
          "publishedOn": "2021-05-23T06:10:39.091Z",
          "wordCount": 608,
          "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "Superpixels are higher-order perceptual groups of pixels in an image, often\ncarrying much more information than raw pixels. There is an inherent relational\nstructure to the relationship among different superpixels of an image. This\nrelational information can convey some form of domain information about the\nimage, e.g. relationship between superpixels representing two eyes in a cat\nimage. Our interest in this paper is to construct computer vision models,\nspecifically those based on Deep Neural Networks (DNNs) to incorporate these\nsuperpixels information. We propose a methodology to construct a hybrid model\nthat leverages (a) Convolutional Neural Network (CNN) to deal with spatial\ninformation in an image, and (b) Graph Neural Network (GNN) to deal with\nrelational superpixel information in the image. The proposed deep model is\nlearned using a generic hybrid loss function that we call a `hybrid' loss. We\nevaluate the predictive performance of our proposed hybrid vision model on four\npopular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.\nMoreover, we evaluate our method on three real-world classification tasks:\nCOVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint\nIdentification. The results demonstrate that the relational superpixel\ninformation provided via a GNN could improve the performance of standard\nCNN-based vision systems.",
          "link": "http://arxiv.org/abs/2105.09448",
          "publishedOn": "2021-05-23T06:10:39.076Z",
          "wordCount": 693,
          "title": "Superpixel-based Domain-Knowledge Infusion in Computer Vision. (arXiv:2105.09448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alegre_L/0/1/0/all/0/1\">Lucas N. Alegre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzan_A/0/1/0/all/0/1\">Ana L. C. Bazzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1\">Bruno C. da Silva</a>",
          "description": "Non-stationary environments are challenging for reinforcement learning\nalgorithms. If the state transition and/or reward functions change based on\nlatent factors, the agent is effectively tasked with optimizing a behavior that\nmaximizes performance over a possibly infinite random sequence of Markov\nDecision Processes (MDPs), each of which drawn from some unknown distribution.\nWe call each such MDP a context. Most related works make strong assumptions\nsuch as knowledge about the distribution over contexts, the existence of\npre-training phases, or a priori knowledge about the number, sequence, or\nboundaries between contexts. We introduce an algorithm that efficiently learns\npolicies in non-stationary environments. It analyzes a possibly infinite stream\nof data and computes, in real-time, high-confidence change-point detection\nstatistics that reflect whether novel, specialized policies need to be created\nand deployed to tackle novel contexts, or whether previously-optimized ones\nmight be reused. We show that (i) this algorithm minimizes the delay until\nunforeseen changes to a context are detected, thereby allowing for rapid\nresponses; and (ii) it bounds the rate of false alarm, which is important in\norder to minimize regret. Our method constructs a mixture model composed of a\n(possibly infinite) ensemble of probabilistic dynamics predictors that model\nthe different modes of the distribution over underlying latent MDPs. We\nevaluate our algorithm on high-dimensional continuous reinforcement learning\nproblems and show that it outperforms state-of-the-art (model-free and\nmodel-based) RL algorithms, as well as state-of-the-art meta-learning methods\nspecially designed to deal with non-stationarity.",
          "link": "http://arxiv.org/abs/2105.09452",
          "publishedOn": "2021-05-23T06:10:39.068Z",
          "wordCount": 713,
          "title": "Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection. (arXiv:2105.09452v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "The past decades have witnessed the prosperity of graph mining, with a\nmultitude of sophisticated models and algorithms designed for various mining\ntasks, such as ranking, classification, clustering and anomaly detection.\nGenerally speaking, the vast majority of the existing works aim to answer the\nfollowing question, that is, given a graph, what is the best way to mine it? In\nthis paper, we introduce the graph sanitation problem, to answer an orthogonal\nquestion. That is, given a mining task and an initial graph, what is the best\nway to improve the initially provided graph? By learning a better graph as part\nof the input of the mining model, it is expected to benefit graph mining in a\nvariety of settings, ranging from denoising, imputation to defense. We\nformulate the graph sanitation problem as a bilevel optimization problem, and\nfurther instantiate it by semi-supervised node classification, together with an\neffective solver named GaSoliNe. Extensive experimental results demonstrate\nthat the proposed method is (1) broadly applicable with respect to different\ngraph neural network models and flexible graph modification strategies, (2)\neffective in improving the node classification accuracy on both the original\nand contaminated graphs in various perturbation scenarios. In particular, it\nbrings up to 25% performance improvement over the existing robust graph neural\nnetwork methods.",
          "link": "http://arxiv.org/abs/2105.09384",
          "publishedOn": "2021-05-23T06:10:38.908Z",
          "wordCount": 631,
          "title": "Graph Sanitation with Application to Node Classification. (arXiv:2105.09384v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09365",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Uysal_E/0/1/0/all/0/1\">Enes Sadi Uysal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilici_M/0/1/0/all/0/1\">M.&#x15e;afak Bilici</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaza_B/0/1/0/all/0/1\">B. Selin Zaza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozgenc_M/0/1/0/all/0/1\">M. Yi&#x11f;it &#xd6;zgen&#xe7;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyar_O/0/1/0/all/0/1\">Onur Boyar</a>",
          "description": "Retinal Vessel Segmentation is important for diagnosis of various diseases.\nThe research on retinal vessel segmentation focuses mainly on improvement of\nthe segmentation model which is usually based on U-Net architecture. In our\nstudy we use the U-Net architecture and we rely on heavy data augmentation in\norder to achieve better performance. The success of the data augmentation\nrelies on successfully addressing the problem of input images. By analyzing\ninput images and performing the augmentation accordingly we show that the\nperformance of the U-Net model can be increased dramatically. Results are\nreported using the most widely used retina dataset, DRIVE.",
          "link": "http://arxiv.org/abs/2105.09365",
          "publishedOn": "2021-05-23T06:10:38.879Z",
          "wordCount": 564,
          "title": "Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation. (arXiv:2105.09365v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_E/0/1/0/all/0/1\">Evan Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guvenc_L/0/1/0/all/0/1\">Levent Guven&#xe7;</a>",
          "description": "As passenger vehicle technologies have advanced, so have their capabilities\nto avoid obstacles, especially with developments in tires, suspensions,\nsteering, as well as safety technologies like ABS, ESC, and more recently, ADAS\nsystems. However, environments around passenger vehicles have also become more\ncomplex, and dangerous. There have previously been studies that outline driver\ntendencies and performance capabilities when attempting to avoid obstacles\nwhile driving passenger vehicles. Now that autonomous vehicles are being\ndeveloped with obstacle avoidance capabilities, it is important to target\nperformance that meets or exceeds that of human drivers. This manuscript\nhighlights systems that are crucial for an emergency obstacle avoidance\nmaneuver (EOAM) and identifies the state-of-the-art for each of the related\nsystems, while considering the nuances of traveling at highway speeds. Some of\nthe primary EOAM-related systems/areas that are discussed in this review are:\ngeneral path planning methods, system hierarchies, decision-making, trajectory\ngeneration, and trajectory-tracking control methods. After concluding remarks,\nsuggestions for future work which could lead to an ideal EOAM development, are\ndiscussed.",
          "link": "http://arxiv.org/abs/2105.09446",
          "publishedOn": "2021-05-23T06:10:38.865Z",
          "wordCount": 619,
          "title": "A Review of Autonomous Road Vehicle Integrated Approaches to an Emergency Obstacle Avoidance Maneuver. (arXiv:2105.09446v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Ming-Chang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jia-Chun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gran_E/0/1/0/all/0/1\">Ernst Gunnar Gran</a>",
          "description": "Over the past decade, many approaches have been introduced for traffic speed\nprediction. However, providing fine-grained, accurate, time-efficient, and\nadaptive traffic speed prediction for a growing transportation network where\nthe size of the network keeps increasing and new traffic detectors are\nconstantly deployed has not been well studied. To address this issue, this\npaper presents DistTune based on Long Short-Term Memory (LSTM) and the\nNelder-Mead method. Whenever encountering an unprocessed detector, DistTune\ndecides if it should customize an LSTM model for this detector by comparing the\ndetector with other processed detectors in terms of the normalized traffic\nspeed patterns they have observed. If similarity is found, DistTune directly\nshares an existing LSTM model with this detector to achieve time-efficient\nprocessing. Otherwise, DistTune customizes an LSTM model for the detector to\nachieve fine-grained prediction. To make DistTune even more time-efficient,\nDistTune performs on a cluster of computing nodes in parallel. To achieve\nadaptive traffic speed prediction, DistTune also provides LSTM re-customization\nfor detectors that suffer from unsatisfactory prediction accuracy due to for\ninstance traffic speed pattern change. Extensive experiments based on traffic\ndata collected from freeway I5-N in California are conducted to evaluate the\nperformance of DistTune. The results demonstrate that DistTune provides\nfine-grained, accurate, time-efficient, and adaptive traffic speed prediction\nfor a growing transportation network.",
          "link": "http://arxiv.org/abs/2105.09421",
          "publishedOn": "2021-05-23T06:10:38.850Z",
          "wordCount": 658,
          "title": "DistTune: Distributed Fine-Grained Adaptive Traffic Speed Prediction for Growing Transportation Networks. (arXiv:2105.09421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pau-Chen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eykholt_K/0/1/0/all/0/1\">Kevin Eykholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhongshu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamjoom_H/0/1/0/all/0/1\">Hani Jamjoom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaram_K/0/1/0/all/0/1\">K. R. Jayaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdez_E/0/1/0/all/0/1\">Enriquillo Valdez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Ashish Verma</a>",
          "description": "Federated Learning (FL) enables collaborative training among mutually\ndistrusting parties. Model updates, rather than training data, are concentrated\nand fused in a central aggregation server. A key security challenge in FL is\nthat an untrustworthy or compromised aggregation process might lead to\nunforeseeable information leakage. This challenge is especially acute due to\nrecently demonstrated attacks that have reconstructed large fractions of\ntraining data from ostensibly \"sanitized\" model updates.\n\nIn this paper, we introduce TRUDA, a new cross-silo FL system, employing a\ntrustworthy and decentralized aggregation architecture to break down\ninformation concentration with regard to a single aggregator. Based on the\nunique computational properties of model-fusion algorithms, all exchanged model\nupdates in TRUDA are disassembled at the parameter-granularity and re-stitched\nto random partitions designated for multiple TEE-protected aggregators. Thus,\neach aggregator only has a fragmentary and shuffled view of model updates and\nis oblivious to the model architecture. Our new security mechanisms can\nfundamentally mitigate training reconstruction attacks, while still preserving\nthe final accuracy of trained models and keeping performance overheads low.",
          "link": "http://arxiv.org/abs/2105.09400",
          "publishedOn": "2021-05-23T06:10:38.835Z",
          "wordCount": 604,
          "title": "Separation of Powers in Federated Learning. (arXiv:2105.09400v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Javaheri_B/0/1/0/all/0/1\">Behzad Javaheri</a>",
          "description": "Herein, we have compared the performance of SVM and MLP in emotion\nrecognition using speech and song channels of the RAVDESS dataset. We have\nundertaken a journey to extract various audio features, identify optimal\nscaling strategy and hyperparameter for our models. To increase sample size, we\nhave performed audio data augmentation and addressed data imbalance using\nSMOTE. Our data indicate that optimised SVM outperforms MLP with an accuracy of\n82 compared to 75%. Following data augmentation, the performance of both\nalgorithms was identical at ~79%, however, overfitting was evident for the SVM.\nOur final exploration indicated that the performance of both SVM and MLP were\nsimilar in which both resulted in lower accuracy for the speech channel\ncompared to the song channel. Our findings suggest that both SVM and MLP are\npowerful classifiers for emotion recognition in a vocal-dependent manner.",
          "link": "http://arxiv.org/abs/2105.09406",
          "publishedOn": "2021-05-23T06:10:38.820Z",
          "wordCount": 576,
          "title": "Speech & Song Emotion Recognition Using Multilayer Perceptron and Standard Vector Machine. (arXiv:2105.09406v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahlou_C/0/1/0/all/0/1\">Chuhong Lahlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crayton_A/0/1/0/all/0/1\">Ancil Crayton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1\">Caroline Trier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willett_E/0/1/0/all/0/1\">Evan Willett</a>",
          "description": "In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an\nArtificial Intelligence (AI) Health Outcomes Challenge seeking solutions to\npredict risk in value-based care for incorporation into CMS Innovation Center\npayment and service delivery models. Recently, modern language models have\nplayed key roles in a number of health related tasks. This paper presents, to\nthe best of our knowledge, the first application of these models to patient\nreadmission prediction. To facilitate this, we create a dataset of 1.2 million\nmedical history samples derived from the Limited Dataset (LDS) issued by CMS.\nMoreover, we propose a comprehensive modeling solution centered on a deep\nlearning framework for this data. To demonstrate the framework, we train an\nattention-based Transformer to learn Medicare semantics in support of\nperforming downstream prediction tasks thereby achieving 0.91 AUC and 0.91\nrecall on readmission classification. We also introduce a novel data\npre-processing pipeline and discuss pertinent deployment considerations\nsurrounding model explainability and bias.",
          "link": "http://arxiv.org/abs/2105.09428",
          "publishedOn": "2021-05-23T06:10:38.807Z",
          "wordCount": 592,
          "title": "Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder. (arXiv:2105.09428v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1\">Aditya Parulekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1\">Advait Parulekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "We consider the problem of finding an approximate solution to $\\ell_1$\nregression while only observing a small number of labels. Given an $n \\times d$\nunlabeled data matrix $X$, we must choose a small set of $m \\ll n$ rows to\nobserve the labels of, then output an estimate $\\widehat{\\beta}$ whose error on\nthe original problem is within a $1 + \\varepsilon$ factor of optimal. We show\nthat sampling from $X$ according to its Lewis weights and outputting the\nempirical minimizer succeeds with probability $1-\\delta$ for $m >\nO(\\frac{1}{\\varepsilon^2} d \\log \\frac{d}{\\varepsilon \\delta})$. This is\nanalogous to the performance of sampling according to leverage scores for\n$\\ell_2$ regression, but with exponentially better dependence on $\\delta$. We\nalso give a corresponding lower bound of $\\Omega(\\frac{d}{\\varepsilon^2} + (d +\n\\frac{1}{\\varepsilon^2}) \\log\\frac{1}{\\delta})$.",
          "link": "http://arxiv.org/abs/2105.09433",
          "publishedOn": "2021-05-23T06:10:38.786Z",
          "wordCount": 552,
          "title": "L1 Regression with Lewis Weights Subsampling. (arXiv:2105.09433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yada Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>",
          "description": "With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and characterized with multiple labels,\nthus exhibiting the co-existence of multiple types of heterogeneity. Although\nstate-of-the-art techniques are good at modeling the complex heterogeneity with\nsufficient label information, such label information can be quite expensive to\nobtain in real applications, leading to sub-optimal performance using these\ntechniques. Inspired by the capability of contrastive learning to utilize rich\nunlabeled data for improving performance, in this paper, we propose a unified\nheterogeneous learning framework, which combines both weighted unsupervised\ncontrastive loss and weighted supervised contrastive loss to model multiple\ntypes of heterogeneity. We also provide theoretical analyses showing that the\nproposed weighted supervised contrastive loss is the lower bound of the mutual\ninformation of two samples from the same class and the weighted unsupervised\ncontrastive loss is the lower bound of the mutual information between the\nhidden representation of two views of the same sample. Experimental results on\nreal-world data sets demonstrate the effectiveness and the efficiency of the\nproposed method modeling multiple types of heterogeneity.",
          "link": "http://arxiv.org/abs/2105.09401",
          "publishedOn": "2021-05-23T06:10:38.751Z",
          "wordCount": 613,
          "title": "Heterogeneous Contrastive Learning. (arXiv:2105.09401v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1\">Daniel Glasner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1\">Srikumar Ramalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papineni_K/0/1/0/all/0/1\">Kishore Papineni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>",
          "description": "It is generally believed that robust training of extremely large networks is\ncritical to their success in real-world applications. However, when taken to\nthe extreme, methods that promote robustness can hurt the model's sensitivity\nto rare or underrepresented patterns. In this paper, we discuss this trade-off\nbetween sensitivity and robustness to natural (non-adversarial) perturbations\nby introducing two notions: contextual feature utility and contextual feature\nsensitivity. We propose Feature Contrastive Learning (FCL) that encourages a\nmodel to be more sensitive to the features that have higher contextual utility.\nEmpirical results demonstrate that models trained with FCL achieve a better\nbalance of robustness and sensitivity, leading to improved generalization in\nthe presence of noise on both vision and NLP datasets.",
          "link": "http://arxiv.org/abs/2105.09394",
          "publishedOn": "2021-05-23T06:10:38.598Z",
          "wordCount": 552,
          "title": "Balancing Robustness and Sensitivity using Feature Contrastive Learning. (arXiv:2105.09394v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Avraham Adler</a>",
          "description": "This paper reviews a wide selection of machine learning models built to\npredict both the presence of diabetes and the presence of undiagnosed diabetes\nusing eight years of National Health and Nutrition Examination Survey (NHANES)\ndata. Models are tuned and compared via their Brier Scores. The most relevant\nvariables of the best performing models are then compared. A Support Vector\nMachine with a linear kernel performed best for predicting diabetes, returning\na Brier score of 0.0654 and an AUROC of 0.9235 on the test set. An elastic net\nregression performed best for predicting undiagnosed diabetes with a Brier\nscore of 0.0294 and an AUROC of 0.9439 on the test set. Similar features appear\nprominently in the models for both sets of models. Blood osmolality, family\nhistory, the prevalance of various compounds, and hypertension are key\nindicators for all diabetes risk. For undiagnosed diabetes in particular, there\nare ethnicity or genetic components which arise as strong correlates as well.",
          "link": "http://arxiv.org/abs/2105.09379",
          "publishedOn": "2021-05-23T06:10:38.585Z",
          "wordCount": 588,
          "title": "Using Machine Learning Techniques to Identify Key Risk Factors for Diabetes and Undiagnosed Diabetes. (arXiv:2105.09379v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuesu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>",
          "description": "While imitation learning for vision based autonomous mobile robot navigation\nhas recently received a great deal of attention in the research community,\nexisting approaches typically require state action demonstrations that were\ngathered using the deployment platform. However, what if one cannot easily\noutfit their platform to record these demonstration signals or worse yet the\ndemonstrator does not have access to the platform at all? Is imitation learning\nfor vision based autonomous navigation even possible in such scenarios? In this\nwork, we hypothesize that the answer is yes and that recent ideas from the\nImitation from Observation (IfO) literature can be brought to bear such that a\nrobot can learn to navigate using only ego centric video collected by a\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\nintroduce a new algorithm, Visual Observation only Imitation Learning for\nAutonomous navigation (VOILA), that can successfully learn navigation policies\nfrom a single video demonstration collected from a physically different agent.\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\nnot only successfully imitates the expert, but that it also learns navigation\npolicies that can generalize to novel environments. Further, we demonstrate the\neffectiveness of VOILA in a real world setting by showing that it allows a\nwheeled Jackal robot to successfully imitate a human walking in an environment\nusing a video recorded using a mobile phone camera.",
          "link": "http://arxiv.org/abs/2105.09371",
          "publishedOn": "2021-05-23T06:10:38.567Z",
          "wordCount": 671,
          "title": "VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.",
          "link": "http://arxiv.org/abs/2105.09356",
          "publishedOn": "2021-05-23T06:10:38.549Z",
          "wordCount": 621,
          "title": "Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wainakh_A/0/1/0/all/0/1\">Aidmar Wainakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1\">Fabrizio Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mussig_T/0/1/0/all/0/1\">Till M&#xfc;&#xdf;ig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keim_J/0/1/0/all/0/1\">Jens Keim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordero_C/0/1/0/all/0/1\">Carlos Garcia Cordero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_E/0/1/0/all/0/1\">Ephraim Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grube_T/0/1/0/all/0/1\">Tim Grube</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhlhauser_M/0/1/0/all/0/1\">Max M&#xfc;hlh&#xe4;user</a>",
          "description": "Federated learning enables multiple users to build a joint model by sharing\ntheir model updates (gradients), while their raw data remains local on their\ndevices. In contrast to the common belief that this provides privacy benefits,\nwe here add to the very recent results on privacy risks when sharing gradients.\nSpecifically, we propose Label Leakage from Gradients (LLG), a novel attack to\nextract the labels of the users' training data from their shared gradients. The\nattack exploits the direction and magnitude of gradients to determine the\npresence or absence of any label. LLG is simple yet effective, capable of\nleaking potential sensitive information represented by labels, and scales well\nto arbitrary batch sizes and multiple classes. We empirically and\nmathematically demonstrate the validity of our attack under different settings.\nMoreover, empirical results show that LLG successfully extracts labels with\nhigh accuracy at the early stages of model training. We also discuss different\ndefense mechanisms against such leakage. Our findings suggest that gradient\ncompression is a practical technique to prevent our attack.",
          "link": "http://arxiv.org/abs/2105.09369",
          "publishedOn": "2021-05-23T06:10:38.516Z",
          "wordCount": 611,
          "title": "User Label Leakage from Gradients in Federated Learning. (arXiv:2105.09369v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin B. Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrato_G/0/1/0/all/0/1\">Guillermo Serrato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>",
          "description": "The joint task of bug localization and program repair is an integral part of\nthe software development process. In this work we present DeepDebug, an\napproach to automated debugging using large, pretrained transformers. We begin\nby training a bug-creation model on reversed commit data for the purpose of\ngenerating synthetic bugs. We apply these synthetic bugs toward two ends.\nFirst, we directly train a backtranslation model on all functions from 200K\nrepositories. Next, we focus on 10K repositories for which we can execute\ntests, and create buggy versions of all functions in those repositories that\nare covered by passing tests. This provides us with rich debugging information\nsuch as stack traces and print statements, which we use to finetune our model\nwhich was pretrained on raw source code. Finally, we strengthen all our models\nby expanding the context window beyond the buggy function itself, and adding a\nskeleton consisting of that function's parent class, imports, signatures,\ndocstrings, and method bodies, in order of priority. On the QuixBugs benchmark,\nwe increase the total number of fixes found by over 50%, while also decreasing\nthe false positive rate from 35% to 5% and decreasing the timeout from six\nhours to one minute. On our own benchmark of executable tests, our model fixes\n68% of all bugs on its first attempt without using traces, and after adding\ntraces it fixes 75% on first attempt. We will open-source our framework and\nvalidation set for evaluating on executable tests.",
          "link": "http://arxiv.org/abs/2105.09352",
          "publishedOn": "2021-05-23T06:10:38.347Z",
          "wordCount": 678,
          "title": "DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and Code Skeletons. (arXiv:2105.09352v1 [cs.SE])"
        }
      ]
    }
  ],
  "cliVersion": "1.9.0"
}