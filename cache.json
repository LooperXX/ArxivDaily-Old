{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2012.14094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Montero_I/0/1/0/all/0/1\">Ivan Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Andrew J. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Christopher DuBois</a>",
          "description": "Existing methods for open-retrieval question answering in lower resource\nlanguages (LRLs) lag significantly behind English. They not only suffer from\nthe shortcomings of non-English document retrieval, but are reliant on\nlanguage-specific supervision for either the task or translation. We formulate\na task setup more realistic to available resources, that circumvents document\nretrieval to reliably transfer knowledge from English to lower resource\nlanguages. Assuming a strong English question answering model or database, we\ncompare and analyze methods that pivot through English: to map foreign queries\nto English and then English answers back to target language answers. Within\nthis task setup we propose Reranked Multilingual Maximal Inner Product Search\n(RM-MIPS), akin to semantic similarity retrieval over the English training set\nwith reranking, which outperforms the strongest baselines by 2.7% on XQuAD and\n6.2% on MKQA. Analysis demonstrates the particular efficacy of this strategy\nover state-of-the-art alternatives in challenging settings: low-resource\nlanguages, with extensive distractor data and query distribution misalignment.\nCircumventing retrieval, our analysis shows this approach offers rapid answer\ngeneration to almost any language off-the-shelf, without the need for any\nadditional training data in the target language.",
          "link": "http://arxiv.org/abs/2012.14094",
          "publishedOn": "2021-07-19T00:49:05.908Z",
          "wordCount": 651,
          "title": "Pivot Through English: Reliably Answering Multilingual Questions without Document Retrieval. (arXiv:2012.14094v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yiran Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakemeyer_G/0/1/0/all/0/1\">Gerhard Lakemeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>",
          "description": "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a\nTransformer-based sequence-to-sequence model capable of reasoning about\ncommonsense knowledge from multimodal inputs of images and texts. We adapt the\ngenerative BART architecture to a multimodal model with visual and textual\ninputs. We further develop novel pretraining tasks to improve the model\nperformance on the Visual Commonsense Generation (VCG) task. In particular, our\npretraining task of Knowledge-based Commonsense Generation (KCG) boosts model\nperformance on the VCG task by leveraging commonsense knowledge from a large\nlanguage model pretrained on external commonsense knowledge graphs. To the best\nof our knowledge, we are the first to propose a dedicated task for improving\nmodel performance on the VCG task. Experimental results show that our model\nreaches state-of-the-art performance on the VCG task by applying these novel\npretraining tasks.",
          "link": "http://arxiv.org/abs/2101.00419",
          "publishedOn": "2021-07-19T00:49:05.844Z",
          "wordCount": 612,
          "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation. (arXiv:2101.00419v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>",
          "description": "Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.",
          "link": "http://arxiv.org/abs/2010.11524",
          "publishedOn": "2021-07-19T00:49:05.837Z",
          "wordCount": 654,
          "title": "SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1\">Su Lin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1\">Zeerak Waseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.",
          "link": "http://arxiv.org/abs/2106.11410",
          "publishedOn": "2021-07-19T00:49:05.830Z",
          "wordCount": 620,
          "title": "A Survey of Race, Racism, and Anti-Racism in NLP. (arXiv:2106.11410v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Learning effective language representations from crowdsourced labels is\ncrucial for many real-world machine learning tasks. A challenging aspect of\nthis problem is that the quality of crowdsourced labels suffer high intra- and\ninter-observer variability. Since the high-capacity deep neural networks can\neasily memorize all disagreements among crowdsourced labels, directly applying\nexisting supervised language representation learning algorithms may yield\nsuboptimal solutions. In this paper, we propose \\emph{TACMA}, a\n\\underline{t}emporal-\\underline{a}ware language representation learning\nheuristic for \\underline{c}rowdsourced labels with \\underline{m}ultiple\n\\underline{a}nnotators. The proposed approach (1) explicitly models the\nintra-observer variability with attention mechanism; (2) computes and\naggregates per-sample confidence scores from multiple workers to address the\ninter-observer disagreements. The proposed heuristic is extremely easy to\nimplement in around 5 lines of code. The proposed heuristic is evaluated on\nfour synthetic and four real-world data sets. The results show that our\napproach outperforms a wide range of state-of-the-art baselines in terms of\nprediction accuracy and AUC. To encourage the reproducible results, we make our\ncode publicly available at \\url{https://github.com/CrowdsourcingMining/TACMA}.",
          "link": "http://arxiv.org/abs/2107.07958",
          "publishedOn": "2021-07-19T00:49:05.801Z",
          "wordCount": 621,
          "title": "Temporal-aware Language Representation Learning From Crowdsourced Labels. (arXiv:2107.07958v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1\">Wilbert Heeringa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_M/0/1/0/all/0/1\">Martha Hofman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drenth_E/0/1/0/all/0/1\">Eduard Drenth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijffels_J/0/1/0/all/0/1\">Jan Wijffels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velde_H/0/1/0/all/0/1\">Hans Van de Velde</a>",
          "description": "We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a\ncorpus of 44,714 words in 3,126 sentences that were annotated according to the\nguidelines of Universal Dependency version 2. POS tags were assigned to words\nby using a Dutch POS tagger that was applied to a literal word-by-word\ntranslation, or to sentences of a Dutch parallel text. Best results were\nobtained when using literal translations that were created by using the Frisian\ntranslation program Oersetter. Morphologic and syntactic annotations were\ngenerated on the basis of a literal Dutch translation as well. The performance\nof the lemmatizer/tagger/annotator when it was trained using default parameters\nwas compared to the performance that was obtained when using the parameter\nvalues that were used for training the LassySmall UD 2.5 corpus. A significant\nimprovement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency\nparser is released as a web app and as a web service.",
          "link": "http://arxiv.org/abs/2107.07974",
          "publishedOn": "2021-07-19T00:49:05.794Z",
          "wordCount": 606,
          "title": "POS tagging, lemmatization and dependency parsing of West Frisian. (arXiv:2107.07974v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiangpeng Wei</a>",
          "description": "End-to-End intelligent neural dialogue systems suffer from the problems of\ngenerating inconsistent and repetitive responses. Existing dialogue models pay\nattention to unilaterally incorporating personal knowledge into the dialog\nwhile ignoring the fact that incorporating the personality-related conversation\ninformation into personal knowledge taken as the bilateral information flow\nboosts the quality of the subsequent conversation. Besides, it is indispensable\nto control personal knowledge utilization over the conversation level. In this\npaper, we propose a conversation-adaption multi-view persona aware response\ngeneration model that aims at enhancing conversation consistency and\nalleviating the repetition from two folds. First, we consider conversation\nconsistency from multiple views. From the view of the persona profile, we\ndesign a novel interaction module that not only iteratively incorporates\npersonalized knowledge into each turn conversation but also captures the\npersonality-related information from conversation to enhance personalized\nknowledge semantic representation. From the view of speaking style, we\nintroduce the speaking style vector and feed it into the decoder to keep the\nspeaking style consistency. To avoid conversation repetition, we devise a\ncoverage mechanism to keep track of the activation of personal knowledge\nutilization. Experiments on both automatic and human evaluation verify the\nsuperiority of our model over previous models.",
          "link": "http://arxiv.org/abs/2107.07771",
          "publishedOn": "2021-07-19T00:49:05.774Z",
          "wordCount": 639,
          "title": "Know Deeper: Knowledge-Conversation Cyclic Utilization Mechanism for Open-domain Dialogue Generation. (arXiv:2107.07771v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koenders_C/0/1/0/all/0/1\">Camille Koenders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filla_J/0/1/0/all/0/1\">Johannes Filla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nicolai Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woloszyn_V/0/1/0/all/0/1\">Vinicius Woloszyn</a>",
          "description": "As the spread of false information on the internet has increased dramatically\nin recent years, more and more attention is being paid to automated fake news\ndetection. Some fake news detection methods are already quite successful.\nNevertheless, there are still many vulnerabilities in the detection algorithms.\nThe reason for this is that fake news publishers can structure and formulate\ntheir texts in such a way that a detection algorithm does not expose this text\nas fake news. This paper shows that it is possible to automatically attack\nstate-of-the-art models that have been trained to detect Fake News, making\nthese vulnerable. For this purpose, corresponding models were first trained\nbased on a dataset. Then, using Text-Attack, an attempt was made to manipulate\nthe trained models in such a way that previously correctly identified fake news\nwas classified as true news. The results show that it is possible to\nautomatically bypass Fake News detection mechanisms, leading to implications\nconcerning existing policy initiatives.",
          "link": "http://arxiv.org/abs/2107.07970",
          "publishedOn": "2021-07-19T00:49:05.715Z",
          "wordCount": 604,
          "title": "How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks?. (arXiv:2107.07970v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengju Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yonghui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Muhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>",
          "description": "Recent studies on Knowledge Base Question Answering (KBQA) have shown great\nprogress on this task via better question understanding. Previous works for\nencoding questions mainly focus on the word sequences, but seldom consider the\ninformation from syntactic trees.In this paper, we propose an approach to learn\nsyntax-based representations for KBQA. First, we encode path-based syntax by\nconsidering the shortest dependency paths between keywords. Then, we propose\ntwo encoding strategies to mode the information of whole syntactic trees to\nobtain tree-based syntax. Finally, we combine both path-based and tree-based\nsyntax representations for KBQA. We conduct extensive experiments on a widely\nused benchmark dataset and the experimental results show that our syntax-aware\nsystems can make full use of syntax information in different settings and\nachieve state-of-the-art performance of KBQA.",
          "link": "http://arxiv.org/abs/2107.07940",
          "publishedOn": "2021-07-19T00:49:05.695Z",
          "wordCount": 565,
          "title": "Exploiting Rich Syntax for Better Knowledge Base Question Answering. (arXiv:2107.07940v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qin Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
          "link": "http://arxiv.org/abs/2107.07705",
          "publishedOn": "2021-07-19T00:49:05.680Z",
          "wordCount": 491,
          "title": "Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-19T00:49:05.656Z",
          "wordCount": 620,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonet_O/0/1/0/all/0/1\">Ona de Gibert Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melero_M/0/1/0/all/0/1\">Maite Melero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>",
          "description": "Multilingual language models have been a crucial breakthrough as they\nconsiderably reduce the need of data for under-resourced languages.\nNevertheless, the superiority of language-specific models has already been\nproven for languages having access to large amounts of data. In this work, we\nfocus on Catalan with the aim to explore to what extent a medium-sized\nmonolingual language model is competitive with state-of-the-art large\nmultilingual models. For this, we: (1) build a clean, high-quality textual\nCatalan corpus (CaText), the largest to date (but only a fraction of the usual\nsize of the previous work in monolingual language models), (2) train a\nTransformer-based language model for Catalan (BERTa), and (3) devise a thorough\nevaluation in a diversity of settings, comprising a complete array of\ndownstream tasks, namely, Part of Speech Tagging, Named Entity Recognition and\nClassification, Text Classification, Question Answering, and Semantic Textual\nSimilarity, with most of the corresponding datasets being created ex novo. The\nresult is a new benchmark, the Catalan Language Understanding Benchmark (CLUB),\nwhich we publish as an open resource, together with the clean textual corpus,\nthe language model, and the cleaning pipeline. Using state-of-the-art\nmultilingual models and a monolingual model trained only on Wikipedia as\nbaselines, we consistently observe the superiority of our model across tasks\nand settings.",
          "link": "http://arxiv.org/abs/2107.07903",
          "publishedOn": "2021-07-19T00:49:05.605Z",
          "wordCount": 674,
          "title": "Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? A Comprehensive Assessment for Catalan. (arXiv:2107.07903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.02721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gelderloos_L/0/1/0/all/0/1\">Lieke Gelderloos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alishahi_A/0/1/0/all/0/1\">Afra Alishahi</a>",
          "description": "Speech directed to children differs from adult-directed speech in linguistic\naspects such as repetition, word choice, and sentence length, as well as in\naspects of the speech signal itself, such as prosodic and phonemic variation.\nHuman language acquisition research indicates that child-directed speech helps\nlanguage learners. This study explores the effect of child-directed speech when\nlearning to extract semantic information from speech directly. We compare the\ntask performance of models trained on adult-directed speech (ADS) and\nchild-directed speech (CDS). We find indications that CDS helps in the initial\nstages of learning, but eventually, models trained on ADS reach comparable task\nperformance, and generalize better. The results suggest that this is at least\npartially due to linguistic rather than acoustic properties of the two\nregisters, as we see the same pattern when looking at models trained on\nacoustically comparable synthetic speech.",
          "link": "http://arxiv.org/abs/2005.02721",
          "publishedOn": "2021-07-19T00:49:05.581Z",
          "wordCount": 671,
          "title": "Learning to Understand Child-directed and Adult-directed Speech. (arXiv:2005.02721v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "The quality of vocal delivery is one of the key indicators for evaluating\nteacher enthusiasm, which has been widely accepted to be connected to the\noverall course qualities. However, existing evaluation for vocal delivery is\nmainly conducted with manual ratings, which faces two core challenges:\nsubjectivity and time-consuming. In this paper, we present a novel machine\nlearning approach that utilizes pairwise comparisons and a multimodal\northogonal fusing algorithm to generate large-scale objective evaluation\nresults of the teacher vocal delivery in terms of fluency and passion. We\ncollect two datasets from real-world education scenarios and the experiment\nresults demonstrate the effectiveness of our algorithm. To encourage\nreproducible results, we make our code public available at\n\\url{https://github.com/tal-ai/ML4VocalDelivery.git}.",
          "link": "http://arxiv.org/abs/2107.07956",
          "publishedOn": "2021-07-19T00:49:05.566Z",
          "wordCount": 577,
          "title": "A Multimodal Machine Learning Framework for Teacher Vocal Delivery Evaluation. (arXiv:2107.07956v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shiting Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1\">Peilei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Task requirements (TRs) writing is an important question type in Key English\nTest and Preliminary English Test. A TR writing question may include multiple\nrequirements and a high-quality essay must respond to each requirement\nthoroughly and accurately. However, the limited teacher resources prevent\nstudents from getting detailed grading instantly. The majority of existing\nautomatic essay scoring systems focus on giving a holistic score but rarely\nprovide reasons to support it. In this paper, we proposed an end-to-end\nframework based on machine reading comprehension (MRC) to address this problem\nto some extent. The framework not only detects whether an essay responds to a\nrequirement question, but clearly marks where the essay answers the question.\nOur framework consists of three modules: question normalization module, ELECTRA\nbased MRC module and response locating module. We extensively explore\nstate-of-the-art MRC methods. Our approach achieves 0.93 accuracy score and\n0.85 F1 score on a real-world educational dataset. To encourage reproducible\nresults, we make our code publicly available at\n\\url{https://github.com/aied2021TRMRC/AIED_2021_TRMRC_code}.",
          "link": "http://arxiv.org/abs/2107.07957",
          "publishedOn": "2021-07-19T00:49:05.545Z",
          "wordCount": 619,
          "title": "Automatic Task Requirements Writing Evaluation via Machine Reading Comprehension. (arXiv:2107.07957v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yukun Jiang</a>",
          "description": "Active learning is a state-of-art machine learning approach to deal with an\nabundance of unlabeled data. In the field of Natural Language Processing,\ntypically it is costly and time-consuming to have all the data annotated. This\ninefficiency inspires out our application of active learning in text\nclassification. Traditional unsupervised k-means clustering is first modified\ninto a semi-supervised version in this research. Then, a novel attempt is\napplied to further extend the algorithm into active learning scenario with\nPenalized Min-Max-selection, so as to make limited queries that yield more\nstable initial centroids. This method utilizes both the interactive query\nresults from users and the underlying distance representation. After tested on\na Chinese news dataset, it shows a consistent increase in accuracy while\nlowering the cost in training.",
          "link": "http://arxiv.org/abs/2107.07682",
          "publishedOn": "2021-07-19T00:49:05.525Z",
          "wordCount": 600,
          "title": "The Application of Active Query K-Means in Text Classification. (arXiv:2107.07682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magee_L/0/1/0/all/0/1\">Liam Magee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahremanlou_L/0/1/0/all/0/1\">Lida Ghahremanlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldatic_K/0/1/0/all/0/1\">Karen Soldatic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1\">Shanthi Robertson</a>",
          "description": "To examine whether intersectional bias can be observed in language\ngeneration, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size\nfrom 124 million to ~2.7 billion parameters. We conduct an experiment combining\nup to three social categories - gender, religion and disability - into\nunconditional or zero-shot prompts used to generate sentences that are then\nanalysed for sentiment. Our results confirm earlier tests conducted with\nauto-regressive causal models, including the \\emph{GPT} family of models. We\nalso illustrate why bias may be resistant to techniques that target single\ncategories (e.g. gender, religion and race), as it can also manifest, in often\nsubtle ways, in texts prompted by concatenated social categories. To address\nthese difficulties, we suggest technical and community-based approaches need to\ncombine to acknowledge and address complex and intersectional language model\nbias.",
          "link": "http://arxiv.org/abs/2107.07691",
          "publishedOn": "2021-07-19T00:49:05.500Z",
          "wordCount": 568,
          "title": "Intersectional Bias in Causal Language Models. (arXiv:2107.07691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07634",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1\">Takuya Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Anmol Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1\">Chandra Dhir</a>",
          "description": "Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\noutperformed the conventional multi-task learning with split branches and a\nbi-directional long short-team memory decoder by 12% on average.",
          "link": "http://arxiv.org/abs/2107.07634",
          "publishedOn": "2021-07-19T00:49:05.441Z",
          "wordCount": 652,
          "title": "Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "Despite recent improvements in open-domain dialogue models, state of the art\nmodels are trained and evaluated on short conversations with little context. In\ncontrast, the long-term conversation setting has hardly been studied. In this\nwork we collect and release a human-human dataset consisting of multiple chat\nsessions whereby the speaking partners learn about each other's interests and\ndiscuss the things they have learnt from past sessions. We show how existing\nmodels trained on existing datasets perform poorly in this long-term\nconversation setting in both automatic and human evaluations, and we study\nlong-context models that can perform much better. In particular, we find\nretrieval-augmented methods and methods with an ability to summarize and recall\nprevious conversations outperform the standard encoder-decoder architectures\ncurrently considered state of the art.",
          "link": "http://arxiv.org/abs/2107.07567",
          "publishedOn": "2021-07-19T00:49:05.430Z",
          "wordCount": 556,
          "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation. (arXiv:2107.07567v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yihan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>",
          "description": "This paper improves the robustness of the pretrained language model BERT\nagainst word substitution-based adversarial attacks by leveraging\nself-supervised contrastive learning with adversarial perturbations. One\nadvantage of our method compared to previous works is that it is capable of\nimproving model robustness without using any labels. Additionally, we also\ncreate an adversarial attack for word-level adversarial training on BERT. The\nattack is efficient, allowing adversarial training for BERT on adversarial\nexamples generated on the fly during training. Experimental results on four\ndatasets show that our method improves the robustness of BERT against four\ndifferent word substitution-based adversarial attacks. Furthermore, to\nunderstand why our method can improve the model robustness against adversarial\nattacks, we study vector representations of clean examples and their\ncorresponding adversarial examples before and after applying our method. As our\nmethod improves model robustness with unlabeled raw data, it opens up the\npossibility of using large text datasets to train robust language models.",
          "link": "http://arxiv.org/abs/2107.07610",
          "publishedOn": "2021-07-19T00:49:05.402Z",
          "wordCount": 596,
          "title": "Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models. (arXiv:2107.07610v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-guang Lou</a>",
          "description": "Recent years pre-trained language models hit a success on modeling natural\nlanguage sentences and (semi-)structured tables. However, existing table\npre-training techniques always suffer from low data quality and low\npre-training efficiency. In this paper, we show that table pre-training can be\nrealized by learning a neural SQL executor over a synthetic corpus, which is\nobtained by automatically synthesizing executable SQL queries. By pre-training\non the synthetic corpus, our approach TAPEX dramatically improves the\nperformance on downstream tasks, boosting existing language models by at most\n19.5%. Meanwhile, TAPEX has remarkably high pre-training efficiency and yields\nstrong results when using a small pre-trained corpus. Experimental results\ndemonstrate that TAPEX outperforms previous table pre-training approaches by a\nlarge margin, and our model achieves new state-of-the-art results on four\nwell-known datasets, including improving the WikiSQL denotation accuracy to\n89.6% (+4.9%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the\nSQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.6%\n(+3.6%). Our work opens the way to reason over structured data by pre-training\non synthetic executable programs.",
          "link": "http://arxiv.org/abs/2107.07653",
          "publishedOn": "2021-07-19T00:49:05.328Z",
          "wordCount": 624,
          "title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor. (arXiv:2107.07653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "The largest store of continually updating knowledge on our planet can be\naccessed via internet search. In this work we study giving access to this\ninformation to conversational agents. Large language models, even though they\nstore an impressive amount of knowledge within their weights, are known to\nhallucinate facts when generating dialogue (Shuster et al., 2021); moreover,\nthose facts are frozen in time at the point of model training. In contrast, we\npropose an approach that learns to generate an internet search query based on\nthe context, and then conditions on the search results to finally generate a\nresponse, a method that can employ up-to-the-minute relevant information. We\ntrain and evaluate such models on a newly collected dataset of human-human\nconversations whereby one of the speakers is given access to internet search\nduring knowledgedriven discussions in order to ground their responses. We find\nthat search-query based access of the internet in conversation provides\nsuperior performance compared to existing approaches that either use no\naugmentation or FAISS-based retrieval (Lewis et al., 2020).",
          "link": "http://arxiv.org/abs/2107.07566",
          "publishedOn": "2021-07-19T00:49:05.279Z",
          "wordCount": 594,
          "title": "Internet-Augmented Dialogue Generation. (arXiv:2107.07566v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07509",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>",
          "description": "In this work, we propose novel decoding algorithms to enable streaming\nautomatic speech recognition (ASR) on unsegmented long-form recordings without\nvoice activity detection (VAD), based on monotonic chunkwise attention (MoChA)\nwith an auxiliary connectionist temporal classification (CTC) objective. We\npropose a block-synchronous beam search decoding to take advantage of efficient\nbatched output-synchronous and low-latency input-synchronous searches. We also\npropose a VAD-free inference algorithm that leverages CTC probabilities to\ndetermine a suitable timing to reset the model states to tackle the\nvulnerability to long-form data. Experimental evaluations demonstrate that the\nblock-synchronous decoding achieves comparable accuracy to the\nlabel-synchronous one. Moreover, the VAD-free inference can recognize long-form\nspeech robustly for up to a few hours.",
          "link": "http://arxiv.org/abs/2107.07509",
          "publishedOn": "2021-07-16T00:48:23.310Z",
          "wordCount": 560,
          "title": "VAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording. (arXiv:2107.07509v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00635",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>",
          "description": "While attention-based encoder-decoder (AED) models have been successfully\nextended to the online variants for streaming automatic speech recognition\n(ASR), such as monotonic chunkwise attention (MoChA), the models still have a\nlarge label emission latency because of the unconstrained end-to-end training\nobjective. Previous works tackled this problem by leveraging alignment\ninformation to control the timing to emit tokens during training. In this work,\nwe propose a simple alignment-free regularization method, StableEmit, to\nencourage MoChA to emit tokens earlier. StableEmit discounts the selection\nprobabilities in hard monotonic attention for token boundary detection by a\nconstant factor and regularizes them to recover the total attention mass during\ntraining. As a result, the scale of the selection probabilities is increased,\nand the values can reach a threshold for token emission earlier, leading to a\nreduction of emission latency and deletion errors. Moreover, StableEmit can be\ncombined with methods that constraint alignments to further improve the\naccuracy and latency. Experimental evaluations with LSTM and Conformer encoders\ndemonstrate that StableEmit significantly reduces the recognition errors and\nthe emission latency simultaneously. We also show that the use of alignment\ninformation is complementary in both metrics.",
          "link": "http://arxiv.org/abs/2107.00635",
          "publishedOn": "2021-07-16T00:48:23.271Z",
          "wordCount": 662,
          "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR. (arXiv:2107.00635v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1\">Jonah Casebeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.",
          "link": "http://arxiv.org/abs/2105.04727",
          "publishedOn": "2021-07-16T00:48:23.256Z",
          "wordCount": 627,
          "title": "Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:23.230Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1\">Razieh Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>",
          "description": "Machine Reading Comprehension (MRC) is an active field in natural language\nprocessing with many successful developed models in recent years. Despite their\nhigh in-distribution accuracy, these models suffer from two issues: high\ntraining cost and low out-of-distribution accuracy. Even though some approaches\nhave been presented to tackle the generalization problem, they have high,\nintolerable training costs. In this paper, we investigate the effect of\nensemble learning approach to improve generalization of MRC systems without\nretraining a big model. After separately training the base models with\ndifferent structures on different datasets, they are ensembled using weighting\nand stacking approaches in probabilistic and non-probabilistic settings. Three\nconfigurations are investigated including heterogeneous, homogeneous, and\nhybrid on eight datasets and six state-of-the-art models. We identify the\nimportant factors in the effectiveness of ensemble methods. Also, we compare\nthe robustness of ensemble and fine-tuned models against data distribution\nshifts. The experimental results show the effectiveness and robustness of the\nensemble approach in improving the out-of-distribution accuracy of MRC systems,\nespecially when the base models are similar in accuracies.",
          "link": "http://arxiv.org/abs/2107.00368",
          "publishedOn": "2021-07-16T00:48:23.222Z",
          "wordCount": 625,
          "title": "Ensemble Learning-Based Approach for Improving Generalization Capability of Machine Reading Comprehension Systems. (arXiv:2107.00368v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaojing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenyang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hai Hu</a>",
          "description": "Pretrained Language Models (PLMs) have achieved tremendous success in natural\nlanguage understanding tasks. While different learning schemes -- fine-tuning,\nzero-shot and few-shot learning -- have been widely explored and compared for\nlanguages such as English, there is comparatively little work in Chinese to\nfairly and comprehensively evaluate and compare these methods. This work first\nintroduces Chinese Few-shot Learning Evaluation Benchmark (FewCLUE), the first\ncomprehensive small sample evaluation benchmark in Chinese. It includes nine\ntasks, ranging from single-sentence and sentence-pair classification tasks to\nmachine reading comprehension tasks. Given the high variance of the few-shot\nlearning performance, we provide multiple training/validation sets to\nfacilitate a more accurate and stable evaluation of few-shot modeling. An\nunlabeled training set with up to 20,000 additional samples per task is\nprovided, allowing researchers to explore better ways of using unlabeled\nsamples. Next, we implement a set of state-of-the-art (SOTA) few-shot learning\nmethods (including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their\nperformance with fine-tuning and zero-shot learning schemes on the newly\nconstructed FewCLUE benchmark.Our results show that: 1) all five few-shot\nlearning methods exhibit better performance than fine-tuning or zero-shot\nlearning; 2) among the five methods, PET is the best performing few-shot\nmethod; 3) few-shot learning performance is highly dependent on the specific\ntask. Our benchmark and code are available at\nhttps://github.com/CLUEbenchmark/FewCLUE",
          "link": "http://arxiv.org/abs/2107.07498",
          "publishedOn": "2021-07-16T00:48:23.183Z",
          "wordCount": 671,
          "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark. (arXiv:2107.07498v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Abul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levene_M/0/1/0/all/0/1\">Mark Levene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_D/0/1/0/all/0/1\">David Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fromson_R/0/1/0/all/0/1\">Renate Fromson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koslover_N/0/1/0/all/0/1\">Nicolas Koslover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levene_T/0/1/0/all/0/1\">Tamara Levene</a>",
          "description": "Objective: This study aims to develop an end-to-end natural language\nprocessing pipeline for triage and diagnosis of COVID-19 from patient-authored\nsocial media posts, in order to provide researchers and other interested\nparties with additional information on the symptoms, severity and prevalence of\nthe disease. Materials and Methods: The text processing pipeline first extracts\nCOVID-19 symptoms and related concepts such as severity, duration, negations,\nand body parts from patients posts using conditional random fields. An\nunsupervised rule-based algorithm is then applied to establish relations\nbetween concepts in the next step of the pipeline. The extracted concepts and\nrelations are subsequently used to construct two different vector\nrepresentations of each post. These vectors are applied separately to build\nsupport vector machine learning models to triage patients into three categories\nand diagnose them for COVID-19. Results: We report that macro- and\nmicro-averaged F1 scores in the range of 71-96% and 61-87%, respectively, for\nthe triage and diagnosis of COVID-19, when the models are trained on human\nlabelled data. Our experimental results indicate that similar performance can\nbe achieved when the models are trained using predicted labels from concept\nextraction and rule-based classifiers, thus yielding end-to-end machine\nlearning. Also, we highlight important features uncovered by our diagnostic\nmachine learning models and compare them with the most frequent symptoms\nrevealed in another COVID-19 dataset. In particular, we found that the most\nimportant features are not always the most frequent ones.",
          "link": "http://arxiv.org/abs/2103.11850",
          "publishedOn": "2021-07-16T00:48:23.138Z",
          "wordCount": 757,
          "title": "Triage and diagnosis of COVID-19 from medical social media. (arXiv:2103.11850v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xilin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "Recent progress in audio source separation lead by deep learning has enabled\nmany neural network models to provide robust solutions to this fundamental\nestimation problem. In this study, we provide a family of efficient neural\nnetwork architectures for general purpose audio source separation while\nfocusing on multiple computational aspects that hinder the application of\nneural networks in real-world scenarios. The backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRM-RF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. This mechanism enables\nour models to obtain high fidelity signal separation in a wide variety of\nsettings where variable number of sources are present and with limited\ncomputational resources (e.g. floating point operations, memory footprint,\nnumber of parameters and latency). Our experiments show that SuDoRM-RF models\nperform comparably and even surpass several state-of-the-art benchmarks with\nsignificantly higher computational resource requirements. The causal variation\nof SuDoRM-RF is able to obtain competitive performance in real-time speech\nseparation of around 10dB scale-invariant signal-to-distortion ratio\nimprovement (SI-SDRi) while remaining up to 20 times faster than real-time on a\nlaptop device.",
          "link": "http://arxiv.org/abs/2103.02644",
          "publishedOn": "2021-07-16T00:48:23.079Z",
          "wordCount": 674,
          "title": "Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilijevic_M/0/1/0/all/0/1\">Momcilo Vasilijevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Video transcript summarization is a fundamental task for video understanding.\nConventional approaches for transcript summarization are usually built upon the\nsummarization data for written language such as news articles, while the domain\ndiscrepancy may degrade the model performance on spoken text. In this paper, we\npresent VT-SSum, a benchmark dataset with spoken language for video transcript\nsegmentation and summarization, which includes 125K transcript-summary pairs\nfrom 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET\nby leveraging the slides content as the weak supervision to generate the\nextractive summary for video transcripts. Experiments with a state-of-the-art\ndeep learning approach show that the model trained with VT-SSum brings a\nsignificant improvement on the AMI spoken text summarization benchmark. VT-SSum\nis publicly available at https://github.com/Dod-o/VT-SSum to support the future\nresearch of video transcript segmentation and summarization tasks.",
          "link": "http://arxiv.org/abs/2106.05606",
          "publishedOn": "2021-07-16T00:48:23.072Z",
          "wordCount": 605,
          "title": "VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and Summarization. (arXiv:2106.05606v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1\">Alon Talmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "Models pre-trained with a language modeling objective possess ample world\nknowledge and language skills, but are known to struggle in tasks that require\nreasoning. In this work, we propose to leverage semi-structured tables, and\nautomatically generate at scale question-paragraph pairs, where answering the\nquestion requires reasoning over multiple facts in the paragraph. We add a\npre-training step over this synthetic data, which includes examples that\nrequire 16 different reasoning skills such as number comparison, conjunction,\nand fact composition. To improve data efficiency, we propose sampling\nstrategies that focus training on reasoning skills the model is currently\nlacking. We evaluate our approach on three reading comprehension datasets that\nare focused on reasoning, and show that our model, PReasM, substantially\noutperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling\nexamples based on current model errors leads to faster training and higher\noverall performance.",
          "link": "http://arxiv.org/abs/2107.07261",
          "publishedOn": "2021-07-16T00:48:22.991Z",
          "wordCount": 587,
          "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>",
          "description": "Making controlled perturbations is essential for various tasks (e.g., data\naugmentation), but building task-specific generators can be expensive. We\nintroduce Tailor, a task-agnostic generation system that perturbs text in a\nsemantically-controlled way. With unlikelihood training, we design Tailor's\ngenerator to follow a series of control codes derived from semantic roles.\nThrough modifications of these control codes, Tailor can produce fine-grained\nperturbations. We implement a set of operations on control codes that can be\ncomposed into complex perturbation strategies, and demonstrate their\neffectiveness in three distinct applications: First, Tailor facilitates the\nconstruction of high-quality contrast sets that are lexically diverse, and less\nbiased than original task test data. Second, paired with automated labeling\nheuristics, Tailor helps improve model generalization through data\naugmentation: We obtain an average gain of 1.73 on an NLI challenge set by\nperturbing just 5% of training data. Third, without any finetuning overhead,\nTailor's perturbations effectively improve compositionality in fine-grained\nstyle transfer, outperforming fine-tuned baselines on 6 transfers.",
          "link": "http://arxiv.org/abs/2107.07150",
          "publishedOn": "2021-07-16T00:48:22.906Z",
          "wordCount": 594,
          "title": "Tailor: Generating and Perturbing Text with Semantic Controls. (arXiv:2107.07150v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>",
          "description": "Few-shot NLP research is highly active, yet conducted in disjoint research\nthreads with evaluation suites that lack challenging-yet-realistic testing\nsetups and fail to employ careful experimental design. Consequently, the\ncommunity does not know which techniques perform best or even if they\noutperform simple baselines. We formulate desiderata for an ideal few-shot NLP\nbenchmark and present FLEX, the first benchmark, public leaderboard, and\nframework that provides unified, comprehensive measurement for few-shot NLP\ntechniques. FLEX incorporates and introduces new best practices for few-shot\nevaluation, including measurement of four transfer settings, textual labels for\nzero-shot evaluation, and a principled approach to benchmark design that\noptimizes statistical accuracy while keeping evaluation costs accessible to\nresearchers without large compute resources. In addition, we present UniFew, a\nsimple yet strong prompt-based model for few-shot learning which unifies the\npretraining and finetuning prompt formats, eschewing complex machinery of\nrecent prompt-based approaches in adapting downstream task formats to language\nmodel pretraining objectives. We demonstrate that despite simplicity UniFew\nachieves results competitive with both popular meta-learning and prompt-based\napproaches.",
          "link": "http://arxiv.org/abs/2107.07170",
          "publishedOn": "2021-07-16T00:48:22.899Z",
          "wordCount": 619,
          "title": "FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1\">Weiping Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Many real-world applications involve the use of Optical Character Recognition\n(OCR) engines to transform handwritten images into transcripts on which\ndownstream Natural Language Processing (NLP) models are applied. In this\nprocess, OCR engines may introduce errors and inputs to downstream NLP models\nbecome noisy. Despite that pre-trained models achieve state-of-the-art\nperformance in many NLP benchmarks, we prove that they are not robust to noisy\ntexts generated by real OCR engines. This greatly limits the application of NLP\nmodels in real-world scenarios. In order to improve model performance on noisy\nOCR transcripts, it is natural to train the NLP model on labelled noisy texts.\nHowever, in most cases there are only labelled clean texts. Since there is no\nhandwritten pictures corresponding to the text, it is impossible to directly\nuse the recognition model to obtain noisy labelled data. Human resources can be\nemployed to copy texts and take pictures, but it is extremely expensive\nconsidering the size of data for model training. Consequently, we are\ninterested in making NLP models intrinsically robust to OCR errors in a low\nresource manner. We propose a novel robust training framework which 1) employs\nsimple but effective methods to directly simulate natural OCR noises from clean\ntexts and 2) iteratively mines the hard examples from a large number of\nsimulated samples for optimal performance. 3) To make our model learn\nnoise-invariant representations, a stability loss is employed. Experiments on\nthree real-world datasets show that the proposed framework boosts the\nrobustness of pre-trained models by a large margin. We believe that this work\ncan greatly promote the application of NLP models in actual scenarios, although\nthe algorithm we use is simple and straightforward. We make our codes and three\ndatasets publicly\navailable\\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.",
          "link": "http://arxiv.org/abs/2107.07113",
          "publishedOn": "2021-07-16T00:48:22.816Z",
          "wordCount": 754,
          "title": "Robust Learning for Text Classification with Multi-source Noise Simulation and Hard Example Mining. (arXiv:2107.07113v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1\">Hannah Rashkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1\">Gaurav Singh Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>",
          "description": "Knowledge-grounded dialogue systems are intended to convey information that\nis based on evidence provided in a given source text. We discuss the challenges\nof training a generative neural dialogue model for such systems that is\ncontrolled to stay faithful to the evidence. Existing datasets contain a mix of\nconversational responses that are faithful to selected evidence as well as more\nsubjective or chit-chat style responses. We propose different evaluation\nmeasures to disentangle these different styles of responses by quantifying the\ninformativeness and objectivity. At training time, additional inputs based on\nthese evaluation measures are given to the dialogue model. At generation time,\nthese additional inputs act as stylistic controls that encourage the model to\ngenerate responses that are faithful to the provided evidence. We also\ninvestigate the usage of additional controls at decoding time using resampling\ntechniques. In addition to automatic metrics, we perform a human evaluation\nstudy where raters judge the output of these controlled generation models to be\ngenerally more objective and faithful to the evidence compared to baseline\ndialogue systems.",
          "link": "http://arxiv.org/abs/2107.06963",
          "publishedOn": "2021-07-16T00:48:22.800Z",
          "wordCount": 609,
          "title": "Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features. (arXiv:2107.06963v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1\">Han shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip L.H. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Transformer-based pre-trained language models like BERT and its variants have\nrecently achieved promising performance in various natural language processing\n(NLP) tasks. However, the conventional paradigm constructs the backbone by\npurely stacking the manually designed global self-attention layers, introducing\ninductive bias and thus leading to sub-optimal. In this work, we propose an\nOperation-Priority Neural Architecture Search (OP-NAS) algorithm to\nautomatically search for promising hybrid backbone architectures. Our\nwell-designed search space (i) contains primitive math operations in the\nintra-layer level to explore novel attention structures, and (ii) leverages\nconvolution blocks to be the supplementary for attention structure in the\ninter-layer level to better learn local dependency. We optimize both the search\nalgorithm and evaluation of candidate models to boost the efficiency of our\nproposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution\nstrategy to facilitate model search via balancing exploration and exploitation.\nFurthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for\nfast model evaluation. Extensive experiments show that the searched\narchitecture (named AutoBERT-Zero) significantly outperforms BERT and its\nvariants of different model capacities in various downstream tasks, proving the\narchitecture's transfer and generalization abilities. Remarkably,\nAutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and\nBERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE\ntest set. Code and pre-trained models will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.07445",
          "publishedOn": "2021-07-16T00:48:22.784Z",
          "wordCount": 660,
          "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1\">Neeraj Chimmwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>",
          "description": "We present a CLSRIL-23, a self supervised learning based audio pre-trained\nmodel which learns cross lingual speech representations from raw audio across\n23 Indic languages. It is built on top of wav2vec 2.0 which is solved by\ntraining a contrastive task over masked latent speech representations and\njointly learns the quantization of latents shared across all languages. We\ncompare the language wise loss during pretraining to compare effects of\nmonolingual and multilingual pretraining. Performance on some downstream\nfine-tuning tasks for speech recognition is also compared and our experiments\nshow that multilingual pretraining outperforms monolingual training, in terms\nof learning speech representations which encodes phonetic similarity of\nlanguages and also in terms of performance on down stream tasks. A decrease of\n5% is observed in WER and 9.5% in CER when a multilingual pretrained model is\nused for finetuning in Hindi. All the code models are also open sourced.\nCLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio\ndata to facilitate research in speech recognition for Indic languages. We hope\nthat new state of the art systems will be created using the self supervised\napproach, especially for low resources Indic languages.",
          "link": "http://arxiv.org/abs/2107.07402",
          "publishedOn": "2021-07-16T00:48:22.760Z",
          "wordCount": 652,
          "title": "CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1\">Joaqu&#xed;n Silveira-Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>",
          "description": "This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as\nwell as the corresponding performance evaluations. Both models were pre-trained\nusing the largest Spanish corpus known to date, with a total of 570GB of clean\nand deduplicated text processed for this work, compiled from the web crawlings\nperformed by the National Library of Spain from 2009 to 2019.",
          "link": "http://arxiv.org/abs/2107.07253",
          "publishedOn": "2021-07-16T00:48:22.751Z",
          "wordCount": 497,
          "title": "Spanish Language Models. (arXiv:2107.07253v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1\">Tazin Afrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Elaine Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1\">Lindsay C. Matsumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1\">Richard Correnti</a>",
          "description": "Automated writing evaluation systems can improve students' writing insofar as\nstudents attend to the feedback provided and revise their essay drafts in ways\naligned with such feedback. Existing research on revision of argumentative\nwriting in such systems, however, has focused on the types of revisions\nstudents make (e.g., surface vs. content) rather than the extent to which\nrevisions actually respond to the feedback provided and improve the essay. We\nintroduce an annotation scheme to capture the nature of sentence-level\nrevisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-\nand 6th-grade students' argumentative essays. We show that reliable manual\nannotation can be achieved and that revision annotations correlate with a\nholistic assessment of essay improvement in line with the feedback provided.\nFurthermore, we explore the feasibility of automatically classifying revisions\naccording to our scheme.",
          "link": "http://arxiv.org/abs/2107.06990",
          "publishedOn": "2021-07-16T00:48:22.730Z",
          "wordCount": 605,
          "title": "Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luckin_R/0/1/0/all/0/1\">Rose Luckin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "In this work, we study computational approaches to detect online dialogic\ninstructions, which are widely used to help students understand learning\nmaterials, and build effective study habits. This task is rather challenging\ndue to the widely-varying quality and pedagogical styles of dialogic\ninstructions. To address these challenges, we utilize pre-trained language\nmodels, and propose a multi-task paradigm which enhances the ability to\ndistinguish instances of different classes by enlarging the margin between\ncategories via contrastive loss. Furthermore, we design a strategy to fully\nexploit the misclassified examples during the training stage. Extensive\nexperiments on a real-world online educational data set demonstrate that our\napproach achieves superior performance compared to representative baselines. To\nencourage reproducible results, we make our implementation online available at\n\\url{https://github.com/AIED2021/multitask-dialogic-instruction}.",
          "link": "http://arxiv.org/abs/2107.07119",
          "publishedOn": "2021-07-16T00:48:22.722Z",
          "wordCount": 586,
          "title": "Multi-Task Learning based Online Dialogic Instruction Detection with Pre-trained Language Models. (arXiv:2107.07119v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>",
          "description": "In this paper, we describe our end-to-end multilingual speech translation\nsystem submitted to the IWSLT 2021 evaluation campaign on the Multilingual\nSpeech Translation shared task. Our system is built by leveraging transfer\nlearning across modalities, tasks and languages. First, we leverage\ngeneral-purpose multilingual modules pretrained with large amounts of\nunlabelled and labelled data. We further enable knowledge transfer from the\ntext task to the speech task by training two tasks jointly. Finally, our\nmultilingual model is finetuned on speech translation task-specific data to\nachieve the best translation results. Experimental results show our system\noutperforms the reported systems, including both end-to-end and cascaded based\napproaches, by a large margin.\n\nIn some translation directions, our speech translation results evaluated on\nthe public Multilingual TEDx test set are even comparable with the ones from a\nstrong text-to-text translation system, which uses the oracle speech\ntranscripts as input.",
          "link": "http://arxiv.org/abs/2107.06959",
          "publishedOn": "2021-07-16T00:48:22.698Z",
          "wordCount": 610,
          "title": "FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task. (arXiv:2107.06959v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Luke Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>",
          "description": "As neural language models grow in effectiveness, they are increasingly being\napplied in real-world settings. However these applications tend to be limited\nin the modes of interaction they support. In this extended abstract, we propose\nWordcraft, an AI-assisted editor for story writing in which a writer and a\ndialog system collaborate to write a story. Our novel interface uses few-shot\nlearning and the natural affordances of conversation to support a variety of\ninteractions. Our editor provides a sandbox for writers to probe the boundaries\nof transformer-based language models and paves the way for future\nhuman-in-the-loop training pipelines and novel evaluation methods.",
          "link": "http://arxiv.org/abs/2107.07430",
          "publishedOn": "2021-07-16T00:48:22.690Z",
          "wordCount": 550,
          "title": "Wordcraft: a Human-AI Collaborative Editor for Story Writing. (arXiv:2107.07430v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mawalim_C/0/1/0/all/0/1\">Candy Olivia Mawalim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unoki_M/0/1/0/all/0/1\">Masashi Unoki</a>",
          "description": "Speaker anonymization aims to suppress speaker individuality to protect\nprivacy in speech while preserving the other aspects, such as speech content.\nOne effective solution for anonymization is to modify the McAdams coefficient.\nIn this work, we propose a method to improve the security for speaker\nanonymization based on the McAdams coefficient by using a speech watermarking\napproach. The proposed method consists of two main processes: one for embedding\nand one for detection. In embedding process, two different McAdams coefficients\nrepresent binary bits ``0\" and ``1\". The watermarked speech is then obtained by\nframe-by-frame bit inverse switching. Subsequently, the detection process is\ncarried out by a power spectrum comparison. We conducted objective evaluations\nwith reference to the VoicePrivacy 2020 Challenge (VP2020) and of the speech\nwatermarking with reference to the Information Hiding Challenge (IHC) and found\nthat our method could satisfy the blind detection, inaudibility, and robustness\nrequirements in watermarking. It also significantly improved the anonymization\nperformance in comparison to the secondary baseline system in VP2020.",
          "link": "http://arxiv.org/abs/2107.07223",
          "publishedOn": "2021-07-16T00:48:22.664Z",
          "wordCount": 603,
          "title": "Improving Security in McAdams Coefficient-Based Speaker Anonymization by Watermarking Method. (arXiv:2107.07223v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of data efficiency, than plain text prompts do for\nexisting LMs, and that HTLM is highly effective at auto-prompting itself, by\nsimply generating the most likely hyper-text formatting for any available\ntraining data. We will release all code and models to support future HTLM\nresearch.",
          "link": "http://arxiv.org/abs/2107.06955",
          "publishedOn": "2021-07-16T00:48:22.621Z",
          "wordCount": 633,
          "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiongqiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiafu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Feng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Sentence completion (SC) questions present a sentence with one or more blanks\nthat need to be filled in, three to five possible words or phrases as options.\nSC questions are widely used for students learning English as a Second Language\n(ESL) and building computational approaches to automatically solve such\nquestions is beneficial to language learners. In this work, we propose a neural\nframework to solve SC questions in English examinations by utilizing\npre-trained language models. We conduct extensive experiments on a real-world\nK-12 ESL SC question dataset and the results demonstrate the superiority of our\nmodel in terms of prediction accuracy. Furthermore, we run precision-recall\ntrade-off analysis to discuss the practical issues when deploying it in\nreal-life scenarios. To encourage reproducible results, we make our code\npublicly available at \\url{https://github.com/AIED2021/ESL-SentenceCompletion}.",
          "link": "http://arxiv.org/abs/2107.07122",
          "publishedOn": "2021-07-16T00:48:22.592Z",
          "wordCount": 595,
          "title": "Solving ESL Sentence Completion Questions via Pre-trained Neural Language Models. (arXiv:2107.07122v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, in the last few years, a large research effort\nhas been devoted to image captioning, i.e. the task of describing images with\nsyntactically and semantically meaningful sentences. Starting from 2015 the\ntask has generally been addressed with pipelines composed of a visual encoding\nstep and a language model for text generation. During these years, both\ncomponents have evolved considerably through the exploitation of object\nregions, attributes, and relationships and the introduction of multi-modal\nconnections, fully-attentive approaches, and BERT-like early-fusion strategies.\nHowever, regardless of the impressive results obtained, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview and categorization of image captioning approaches,\nfrom visual encoding and text generation to training strategies, used datasets,\nand evaluation metrics. In this respect, we quantitatively compare many\nrelevant state-of-the-art approaches to identify the most impactful technical\ninnovations in image captioning architectures and training strategies.\nMoreover, many variants of the problem and its open challenges are analyzed and\ndiscussed. The final goal of this work is to serve as a tool for understanding\nthe existing state-of-the-art and highlighting the future directions for an\narea of research where Computer Vision and Natural Language Processing can find\nan optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-07-16T00:48:22.510Z",
          "wordCount": 664,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lillian Lee</a>",
          "description": "We propose a transition-based bubble parser to perform coordination structure\nidentification and dependency-based syntactic analysis simultaneously. Bubble\nrepresentations were proposed in the formal linguistics literature decades ago;\nthey enhance dependency trees by encoding coordination boundaries and internal\nrelationships within coordination structures explicitly. In this paper, we\nintroduce a transition system and neural models for parsing these\nbubble-enhanced structures. Experimental results on the English Penn Treebank\nand the English GENIA corpus show that our parsers beat previous\nstate-of-the-art approaches on the task of coordination structure prediction,\nespecially for the subset of sentences with complex coordination structures.",
          "link": "http://arxiv.org/abs/2107.06905",
          "publishedOn": "2021-07-16T00:48:22.501Z",
          "wordCount": 536,
          "title": "Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction. (arXiv:2107.06905v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lillian Lee</a>",
          "description": "We present our contribution to the IWPT 2021 shared task on parsing into\nenhanced Universal Dependencies. Our main system component is a hybrid\ntree-graph parser that integrates (a) predictions of spanning trees for the\nenhanced graphs with (b) additional graph edges not present in the spanning\ntrees. We also adopt a finetuning strategy where we first train a\nlanguage-generic parser on the concatenation of data from all available\nlanguages, and then, in a second step, finetune on each individual language\nseparately. Additionally, we develop our own complete set of pre-processing\nmodules relevant to the shared task, including tokenization, sentence\nsegmentation, and multiword token expansion, based on pre-trained XLM-R models\nand our own pre-training of character-level language models. Our submission\nreaches a macro-average ELAS of 89.24 on the test set. It ranks top among all\nteams, with a margin of more than 2 absolute ELAS over the next best-performing\nsubmission, and best score on 16 out of 17 languages.",
          "link": "http://arxiv.org/abs/2107.06907",
          "publishedOn": "2021-07-16T00:48:22.340Z",
          "wordCount": 610,
          "title": "TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage Generic- to Individual-Language Finetuning. (arXiv:2107.06907v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengnan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Neural sequence models exhibit limited compositional generalization ability\nin semantic parsing tasks. Compositional generalization requires algebraic\nrecombination, i.e., dynamically recombining structured expressions in a\nrecursive manner. However, most previous studies mainly concentrate on\nrecombining lexical units, which is an important but not sufficient part of\nalgebraic recombination. In this paper, we propose LeAR, an end-to-end neural\nmodel to learn algebraic recombination for compositional generalization. The\nkey insight is to model the semantic parsing task as a homomorphism between a\nlatent syntactic algebra and a semantic algebra, thus encouraging algebraic\nrecombination. Specifically, we learn two modules jointly: a Composer for\nproducing latent syntax, and an Interpreter for assigning semantic operations.\nExperiments on two realistic and comprehensive compositional generalization\nbenchmarks demonstrate the effectiveness of our model. The source code is\npublicly available at https://github.com/microsoft/ContextualSP.",
          "link": "http://arxiv.org/abs/2107.06516",
          "publishedOn": "2021-07-15T01:59:02.545Z",
          "wordCount": 580,
          "title": "Learning Algebraic Recombination for Compositional Generalization. (arXiv:2107.06516v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Multi-choice Machine Reading Comprehension (MRC) as a challenge requires\nmodel to select the most appropriate answer from a set of candidates given\npassage and question. Most of the existing researches focus on the modeling of\nthe task datasets without explicitly referring to external fine-grained\nknowledge sources, which is supposed to greatly make up the deficiency of the\ngiven passage. Thus we propose a novel reference-based knowledge enhancement\nmodel called Reference Knowledgeable Network (RekNet), which refines critical\ninformation from the passage and quote explicit knowledge in necessity. In\ndetail, RekNet refines fine-grained critical information and defines it as\nReference Span, then quotes explicit knowledge quadruples by the co-occurrence\ninformation of Reference Span and candidates. The proposed RekNet is evaluated\non three multi-choice MRC benchmarks: RACE, DREAM and Cosmos QA, which shows\nconsistent and remarkable performance improvement with observable statistical\nsignificance level over strong baselines.",
          "link": "http://arxiv.org/abs/2012.03709",
          "publishedOn": "2021-07-15T01:59:02.527Z",
          "wordCount": 602,
          "title": "Reference Knowledgeable Network for Machine Reading Comprehension. (arXiv:2012.03709v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>",
          "description": "Emotion recognition in conversation (ERC) is a crucial component in affective\ndialogue systems, which helps the system understand users' emotions and\ngenerate empathetic responses. However, most works focus on modeling speaker\nand contextual information primarily on the textual modality or simply\nleveraging multimodal information through feature concatenation. In order to\nexplore a more effective way of utilizing both multimodal and long-distance\ncontextual information, we propose a new model based on multimodal fused graph\nconvolutional network, MMGCN, in this work. MMGCN can not only make use of\nmultimodal dependencies effectively, but also leverage speaker information to\nmodel inter-speaker and intra-speaker dependency. We evaluate our proposed\nmodel on two public benchmark datasets, IEMOCAP and MELD, and the results prove\nthe effectiveness of MMGCN, which outperforms other SOTA methods by a\nsignificant margin under the multimodal conversation setting.",
          "link": "http://arxiv.org/abs/2107.06779",
          "publishedOn": "2021-07-15T01:59:02.521Z",
          "wordCount": 587,
          "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation. (arXiv:2107.06779v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awalina_A/0/1/0/all/0/1\">Aisyah Awalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaid_J/0/1/0/all/0/1\">Jibran Fawaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krisnabayu_R/0/1/0/all/0/1\">Rifky Yunus Krisnabayu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "Fake news is a problem faced by society in this era. It is not rare for fake\nnews to cause provocation and problem for the people. Indonesia, as a country\nwith the 4th largest population, has a problem in dealing with fake news. More\nthan 30% of rural and urban population are deceived by this fake news problem.\nAs we have been studying, there is only few literatures on preventing the\nspread of fake news in Bahasa Indonesia. So, this research is conducted to\nprevent these problems. The dataset used in this research was obtained from a\nnews portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on\nthis page, we got 1116 data consisting of valid news and fake news. The dataset\ncan be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This\ndataset will be combined with other available datasets. The methods used are\nCNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This\nresearch shows that the BERT method with Transformer Network has the best\nresults with an accuracy of up to 90%.",
          "link": "http://arxiv.org/abs/2107.06796",
          "publishedOn": "2021-07-15T01:59:02.515Z",
          "wordCount": 608,
          "title": "Indonesia's Fake News Detection using Transformer Network. (arXiv:2107.06796v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cysouw_M/0/1/0/all/0/1\">Michael Cysouw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "With more than 7000 languages worldwide, multilingual natural language\nprocessing (NLP) is essential both from an academic and commercial perspective.\nResearching typological properties of languages is fundamental for progress in\nmultilingual NLP. Examples include assessing language similarity for effective\ntransfer learning, injecting inductive biases into machine learning models or\ncreating resources such as dictionaries and inflection tables. We provide\nParCourE, an online tool that allows to browse a word-aligned parallel corpus,\ncovering 1334 languages. We give evidence that this is useful for typological\nresearch. ParCourE can be set up for any parallel corpus and can thus be used\nfor typological research on other corpora as well as for exploring their\nquality and properties.",
          "link": "http://arxiv.org/abs/2107.06632",
          "publishedOn": "2021-07-15T01:59:02.507Z",
          "wordCount": 576,
          "title": "ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus. (arXiv:2107.06632v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nystrom_A/0/1/0/all/0/1\">Andrew Nystrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1\">Douglas Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
          "link": "http://arxiv.org/abs/2107.06499",
          "publishedOn": "2021-07-15T01:59:02.453Z",
          "wordCount": 571,
          "title": "Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tarunesh_I/0/1/0/all/0/1\">Ishan Tarunesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Syamantak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>",
          "description": "Generating code-switched text is a problem of growing interest, especially\ngiven the scarcity of corpora containing large volumes of real code-switched\ntext. In this work, we adapt a state-of-the-art neural machine translation\nmodel to generate Hindi-English code-switched sentences starting from\nmonolingual Hindi sentences. We outline a carefully designed curriculum of\npretraining steps, including the use of synthetic code-switched text, that\nenable the model to generate high-quality code-switched text. Using text\ngenerated from our model as data augmentation, we show significant reductions\nin perplexity on a language modeling task, compared to using text from other\ngenerative models of CS text. We also show improvements using our text for a\ndownstream code-switched natural language inference task. Our generated text is\nfurther subjected to a rigorous evaluation using a human evaluation study and a\nrange of objective metrics, where we show performance comparable (and sometimes\neven superior) to code-switched text obtained via crowd workers who are native\nHindi speakers.",
          "link": "http://arxiv.org/abs/2107.06483",
          "publishedOn": "2021-07-15T01:59:02.435Z",
          "wordCount": 617,
          "title": "From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text. (arXiv:2107.06483v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanying Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>",
          "description": "Multilingual neural machine translation with a single model has drawn much\nattention due to its capability to deal with multiple languages. However, the\ncurrent multilingual translation paradigm often makes the model tend to\npreserve the general knowledge, but ignore the language-specific knowledge.\nSome previous works try to solve this problem by adding various kinds of\nlanguage-specific modules to the model, but they suffer from the parameter\nexplosion problem and require specialized manual design. To solve these\nproblems, we propose to divide the model neurons into general and\nlanguage-specific parts based on their importance across languages. The general\npart is responsible for preserving the general knowledge and participating in\nthe translation of all the languages, while the language-specific part is\nresponsible for preserving the language-specific knowledge and participating in\nthe translation of some specific languages. Experimental results on several\nlanguage pairs, covering IWSLT and Europarl corpus datasets, demonstrate the\neffectiveness and universality of the proposed method.",
          "link": "http://arxiv.org/abs/2107.06569",
          "publishedOn": "2021-07-15T01:59:02.429Z",
          "wordCount": 589,
          "title": "Importance-based Neuron Allocation for Multilingual Neural Machine Translation. (arXiv:2107.06569v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukmadewa_A/0/1/0/all/0/1\">Anantha Yullian Sukmadewa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DW_H/0/1/0/all/0/1\">Haftittah Wuswilahaken DW</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachtiar_F/0/1/0/all/0/1\">Fitra Abdurrachman Bachtiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "User reviews have an essential role in the success of the developed mobile\napps. User reviews in the textual form are unstructured data, creating a very\nhigh complexity when processed for sentiment analysis. Previous approaches that\nhave been used often ignore the context of reviews. In addition, the relatively\nsmall data makes the model overfitting. A new approach, BERT, has been\nintroduced as a transfer learning model with a pre-trained model that has\npreviously been trained to have a better context representation. This study\nexamines the effectiveness of fine-tuning BERT for sentiment analysis using two\ndifferent pre-trained models. Besides the multilingual pre-trained model, we\nuse the pre-trained model that only has been trained in Indonesian. The dataset\nused is Indonesian user reviews of the ten best apps in 2020 in Google Play\nsites. We also perform hyper-parameter tuning to find the optimum trained\nmodel. Two training data labeling approaches were also tested to determine the\neffectiveness of the model, which is score-based and lexicon-based. The\nexperimental results show that pre-trained models trained in Indonesian have\nbetter average accuracy on lexicon-based data. The pre-trained Indonesian model\nhighest accuracy is 84%, with 25 epochs and a training time of 24 minutes.\nThese results are better than all of the machine learning and multilingual\npre-trained models.",
          "link": "http://arxiv.org/abs/2107.06802",
          "publishedOn": "2021-07-15T01:59:02.403Z",
          "wordCount": 657,
          "title": "BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps Reviews. (arXiv:2107.06802v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_R/0/1/0/all/0/1\">Razin A. Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_L/0/1/0/all/0/1\">Lia Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodatz_B/0/1/0/all/0/1\">Benjamin Rodatz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>",
          "description": "Negation in natural language does not follow Boolean logic and is therefore\ninherently difficult to model. In particular, it takes into account the broader\nunderstanding of what is being negated. In previous work, we proposed a\nframework for negation of words that accounts for `worldly context'. In this\npaper, we extend that proposal now accounting for the compositional structure\ninherent in language, within the DisCoCirc framework. We compose the negations\nof single words to capture the negation of sentences. We also describe how to\nmodel the negation of words whose meanings evolve in the text.",
          "link": "http://arxiv.org/abs/2107.06820",
          "publishedOn": "2021-07-15T01:59:02.397Z",
          "wordCount": 532,
          "title": "Composing Conversational Negation. (arXiv:2107.06820v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "The rise of big data analytics on top of NLP increases the computational\nburden for text processing at scale. The problems faced in NLP are very high\ndimensional text, so it takes a high computation resource. The MapReduce allows\nparallelization of large computations and can improve the efficiency of text\nprocessing. This research aims to study the effect of big data processing on\nNLP tasks based on a deep learning approach. We classify a big text of news\ntopics with fine-tuning BERT used pre-trained models. Five pre-trained models\nwith a different number of parameters were used in this study. To measure the\nefficiency of this method, we compared the performance of the BERT with the\npipelines from Spark NLP. The result shows that BERT without Spark NLP gives\nhigher accuracy compared to BERT with Spark NLP. The accuracy average and\ntraining time of all models using BERT is 0.9187 and 35 minutes while using\nBERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will\ntake more computation resources and need a longer time to complete the tasks.\nHowever, the accuracy of BERT with Spark NLP only decreased by an average of\n5.7%, while the training time was reduced significantly by 62.9% compared to\nBERT without Spark NLP.",
          "link": "http://arxiv.org/abs/2107.06785",
          "publishedOn": "2021-07-15T01:59:02.381Z",
          "wordCount": 653,
          "title": "Large-Scale News Classification using BERT Language Model: Spark NLP Approach. (arXiv:2107.06785v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06776",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Meichanetzidis_K/0/1/0/all/0/1\">Konstantinos Meichanetzidis</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a>",
          "description": "This is a story about making quantum computers speak, and doing so in a\nquantum-native, compositional and meaning-aware manner. Recently we did\nquestion-answering with an actual quantum computer. We explain what we did,\nstress that this was all done in terms of pictures, and provide many pointers\nto the related literature. In fact, besides natural language, many other things\ncan be implemented in a quantum-native, compositional and meaning-aware manner,\nand we provide the reader with some indications of that broader pictorial\nlandscape, including our account on the notion of compositionality. We also\nprovide some guidance for the actual execution, so that the reader can give it\na go as well.",
          "link": "http://arxiv.org/abs/2107.06776",
          "publishedOn": "2021-07-15T01:59:02.375Z",
          "wordCount": 550,
          "title": "How to make qubits speak. (arXiv:2107.06776v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabanac_G/0/1/0/all/0/1\">Guillaume Cabanac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labbe_C/0/1/0/all/0/1\">Cyril Labb&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magazinov_A/0/1/0/all/0/1\">Alexander Magazinov</a>",
          "description": "Probabilistic text generators have been used to produce fake scientific\npapers for more than a decade. Such nonsensical papers are easily detected by\nboth human and machine. Now more complex AI-powered generation techniques\nproduce texts indistinguishable from that of humans and the generation of\nscientific texts from a few keywords has been documented. Our study introduces\nthe concept of tortured phrases: unexpected weird phrases in lieu of\nestablished ones, such as 'counterfeit consciousness' instead of 'artificial\nintelligence.' We combed the literature for tortured phrases and study one\nreputable journal where these concentrated en masse. Hypothesising the use of\nadvanced language models we ran a detector on the abstracts of recent articles\nof this journal and on several control sets. The pairwise comparisons reveal a\nconcentration of abstracts flagged as 'synthetic' in the journal. We also\nhighlight irregularities in its operation, such as abrupt changes in editorial\ntimelines. We substantiate our call for investigation by analysing several\nindividual dubious articles, stressing questionable features: tortured writing\nstyle, citation of non-existent literature, and unacknowledged image reuse.\nSurprisingly, some websites offer to rewrite texts for free, generating\ngobbledegook full of tortured phrases. We believe some authors used rewritten\ntexts to pad their manuscripts. We wish to raise the awareness on publications\ncontaining such questionable AI-generated or rewritten texts that passed (poor)\npeer review. Deception with synthetic texts threatens the integrity of the\nscientific literature.",
          "link": "http://arxiv.org/abs/2107.06751",
          "publishedOn": "2021-07-15T01:59:02.369Z",
          "wordCount": 688,
          "title": "Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals. (arXiv:2107.06751v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alishahia_A/0/1/0/all/0/1\">Afra Alishahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1\">Alejandrina Cristia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1\">Bertrand Higy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1\">Marvin Lavechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chen Yu</a>",
          "description": "We present the visually-grounded language modelling track that was introduced\nin the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the\nnew track and discuss participation rules in detail. We also present the two\nbaseline systems that were developed for this track.",
          "link": "http://arxiv.org/abs/2107.06546",
          "publishedOn": "2021-07-15T01:59:02.362Z",
          "wordCount": 501,
          "title": "ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition. (arXiv:2107.06546v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagaraja_V/0/1/0/all/0/1\">Varun Nagaraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>",
          "description": "On-device speech recognition requires training models of different sizes for\ndeploying on devices with various computational budgets. When building such\ndifferent models, we can benefit from training them jointly to take advantage\nof the knowledge shared between them. Joint training is also efficient since it\nreduces the redundancy in the training procedure's data handling operations. We\npropose a method for collaboratively training acoustic encoders of different\nsizes for speech recognition. We use a sequence transducer setup where\ndifferent acoustic encoders share a common predictor and joiner modules. The\nacoustic encoders are also trained using co-distillation through an auxiliary\ntask for frame level chenone prediction, along with the transducer loss. We\nperform experiments using the LibriSpeech corpus and demonstrate that the\ncollaboratively trained acoustic encoders can provide up to a 11% relative\nimprovement in the word error rate on both the test partitions.",
          "link": "http://arxiv.org/abs/2106.08960",
          "publishedOn": "2021-07-15T01:59:02.356Z",
          "wordCount": 622,
          "title": "Collaborative Training of Acoustic Encoders for Speech Recognition. (arXiv:2106.08960v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>",
          "description": "Transformer is important for text modeling. However, it has difficulty in\nhandling long documents due to the quadratic complexity with input text length.\nIn order to handle this problem, we propose a hierarchical interactive\nTransformer (Hi-Transformer) for efficient and effective long document\nmodeling. Hi-Transformer models documents in a hierarchical way, i.e., first\nlearns sentence representations and then learns document representations. It\ncan effectively reduce the complexity and meanwhile capture global document\ncontext in the modeling of each sentence. More specifically, we first use a\nsentence Transformer to learn the representations of each sentence. Then we use\na document Transformer to model the global document context from these sentence\nrepresentations. Next, we use another sentence Transformer to enhance sentence\nmodeling using the global document context. Finally, we use hierarchical\npooling method to obtain document embedding. Extensive experiments on three\nbenchmark datasets validate the efficiency and effectiveness of Hi-Transformer\nin long document modeling.",
          "link": "http://arxiv.org/abs/2106.01040",
          "publishedOn": "2021-07-15T01:59:02.349Z",
          "wordCount": 619,
          "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling. (arXiv:2106.01040v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Kaufhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>",
          "description": "Data augmentation, the artificial creation of training data for machine\nlearning by transformations, is a widely studied research field across machine\nlearning disciplines. While it is useful for increasing the generalization\ncapabilities of a model, it can also address many other challenges and\nproblems, from overcoming a limited amount of training data over regularizing\nthe objective to limiting the amount data used to protect privacy. Based on a\nprecise description of the goals and applications of data augmentation (C1) and\na taxonomy for existing works (C2), this survey is concerned with data\naugmentation methods for textual classification and aims to achieve a concise\nand comprehensive overview for researchers and practitioners (C3). Derived from\nthe taxonomy, we divided more than 100 methods into 12 different groupings and\nprovide state-of-the-art references expounding which methods are highly\npromising (C4). Finally, research perspectives that may constitute a building\nblock for future work are given (C5).",
          "link": "http://arxiv.org/abs/2107.03158",
          "publishedOn": "2021-07-15T01:59:02.329Z",
          "wordCount": 608,
          "title": "A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Ankita De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>",
          "description": "MultiWOZ is one of the most popular multi-domain task-oriented dialog\ndatasets, containing 10K+ annotated dialogs covering eight domains. It has been\nwidely accepted as a benchmark for various dialog tasks, e.g., dialog state\ntracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog\nmodeling. In this work, we identify an overlooked issue with dialog state\nannotation inconsistencies in the dataset, where a slot type is tagged\ninconsistently across similar dialogs leading to confusion for DST modeling. We\npropose an automated correction for this issue, which is present in a whopping\n70% of the dialogs. Additionally, we notice that there is significant entity\nbias in the dataset (e.g., \"cambridge\" appears in 50% of the destination cities\nin the train domain). The entity bias can potentially lead to named entity\nmemorization in generative models, which may go unnoticed as the test set\nsuffers from a similar entity bias as well. We release a new test set with all\nentities replaced with unseen entities. Finally, we benchmark joint goal\naccuracy (JGA) of the state-of-the-art DST baselines on these modified versions\nof the data. Our experiments show that the annotation inconsistency corrections\nlead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in\nJGA when models are evaluated on the new test set with unseen entities.",
          "link": "http://arxiv.org/abs/2105.14150",
          "publishedOn": "2021-07-15T01:59:02.318Z",
          "wordCount": 689,
          "title": "Annotation Inconsistency and Entity Bias in MultiWOZ. (arXiv:2105.14150v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00120",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1\">Timo Lohrenz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>",
          "description": "Stream fusion, also known as system combination, is a common technique in\nautomatic speech recognition for traditional hybrid hidden Markov model\napproaches, yet mostly unexplored for modern deep neural network end-to-end\nmodel architectures. Here, we investigate various fusion techniques for the\nall-attention-based encoder-decoder architecture known as the transformer,\nstriving to achieve optimal fusion by investigating different fusion levels in\nan example single-microphone setting with fusion of standard magnitude and\nphase features. We introduce a novel multi-encoder learning method that\nperforms a weighted combination of two encoder-decoder multi-head attention\noutputs only during training. Employing then only the magnitude feature encoder\nin inference, we are able to show consistent improvement on Wall Street Journal\n(WSJ) with language model and on Librispeech, without increase in runtime or\nparameters. Combining two such multi-encoder trained models by a simple late\nfusion in inference, we achieve state-of-the-art performance for\ntransformer-based models on WSJ with a significant WER reduction of 19%\nrelative compared to the current benchmark approach.",
          "link": "http://arxiv.org/abs/2104.00120",
          "publishedOn": "2021-07-15T01:59:02.309Z",
          "wordCount": 638,
          "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition. (arXiv:2104.00120v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Camargo_A/0/1/0/all/0/1\">Augusto Camargo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1\">Wesley Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peressim_F/0/1/0/all/0/1\">Felipe Peressim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_A/0/1/0/all/0/1\">Alan Barzilay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1\">Marcelo Finger</a>",
          "description": "In this paper, we studied whether models based on BiLSTM and BERT can predict\nhashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a\nsizable financial impact on Ecommerce. We processed a corpus of Ecommerce\nreviews as inputs, and predicted hashtags as outputs. We evaluated the results\nusing four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A\nword cloud was used as a qualitative metric. While all computer-generated\nmetrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results\nproduced amazing scores. We concluded that the texts predicted by the neural\nnetworks are very promising for use as hashtags for products on Ecommerce\nwebsites. The code for this work is available at\nhttps://github.com/augustocamargo/text-to-hashtag.",
          "link": "http://arxiv.org/abs/2102.00904",
          "publishedOn": "2021-07-15T01:59:02.301Z",
          "wordCount": 573,
          "title": "Text-to-hashtag Generation using Seq2seq Learning. (arXiv:2102.00904v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klicpera_J/0/1/0/all/0/1\">Johannes Klicpera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1\">Marten Lienen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "The current best practice for computing optimal transport (OT) is via entropy\nregularization and Sinkhorn iterations. This algorithm runs in quadratic time\nas it requires the full pairwise cost matrix, which is prohibitively expensive\nfor large sets of objects. In this work we propose two effective log-linear\ntime approximations of the cost matrix: First, a sparse approximation based on\nlocality-sensitive hashing (LSH) and, second, a Nystr\\\"om approximation with\nLSH-based sparse corrections, which we call locally corrected Nystr\\\"om (LCN).\nThese approximations enable general log-linear time algorithms for\nentropy-regularized OT that perform well even for the complex, high-dimensional\nspaces common in deep learning. We analyse these approximations theoretically\nand evaluate them experimentally both directly and end-to-end as a component\nfor real-world applications. Using our approximations for unsupervised word\nembedding alignment enables us to speed up a state-of-the-art method by a\nfactor of 3 while also improving the accuracy by 3.1 percentage points without\nany additional model changes. For graph distance regression we propose the\ngraph transport network (GTN), which combines graph neural networks (GNNs) with\nenhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales\nlog-linearly in the number of nodes.",
          "link": "http://arxiv.org/abs/2107.06876",
          "publishedOn": "2021-07-15T01:59:02.294Z",
          "wordCount": 658,
          "title": "Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sujeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamrakar_A/0/1/0/all/0/1\">Amir Tamrakar</a>",
          "description": "We propose a method to generate optimal natural language for block placement\ndirectives generated by a machine's planner during human-agent interactions in\nthe blocks world. A non user-friendly machine directive, e.g., move(ObjId,\ntoPos), is transformed into visually and contextually grounded referring\nexpressions that are much easier for the user to comprehend. We describe an\nalgorithm that progressively and generatively transforms the machine's\ndirective in ECI (Elementary Composable Ideas)-space, generating many\nalternative versions of the directive. We then define a cost function to\nevaluate the ease of comprehension of these alternatives and select the best\noption. The parameters for this cost function were derived empirically from a\nuser study that measured utterance-to-action timings.",
          "link": "http://arxiv.org/abs/2107.06886",
          "publishedOn": "2021-07-15T01:59:02.276Z",
          "wordCount": 564,
          "title": "\"How to best say it?\" : Translating Directives in Machine Language into Natural Language in the Blocks World. (arXiv:2107.06886v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>",
          "description": "Accurately linking news articles to scientific research works is a critical\ncomponent in a number of applications, such as measuring the social impact of a\nresearch work and detecting inaccuracies or distortions in science news.\nAlthough the lack of links between news and literature has been a challenge in\nthese applications, it is a relatively unexplored research problem. In this\npaper we designed and evaluated a new approach that consists of (1) augmenting\nlatest named-entity recognition techniques to extract various metadata, and (2)\ndesigning a new elastic search engine that can facilitate the use of enriched\nmetadata queries. To evaluate our approach, we constructed two datasets of\npaired news articles and research papers: one is used for training models to\nextract metadata, and the other for evaluation. Our experiments showed that the\nnew approach performed significantly better than a baseline approach used by\naltmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate\nthe effectiveness of the approach, we also conducted a study on 37,600\nhealth-related press releases published on EurekAlert!, which showed that our\napproach was able to identify the corresponding research papers with a top-1\naccuracy of at least 0.97.",
          "link": "http://arxiv.org/abs/2107.06472",
          "publishedOn": "2021-07-15T01:59:02.269Z",
          "wordCount": 632,
          "title": "Linking Health News to Research Literature. (arXiv:2107.06472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual\nencoders, using a relatively small set of manually-annotated data (as compared\nto web-crawled data), to perceive the visual world. However, it has been\nobserved that large-scale pretraining usually can result in better\ngeneralization performance, e.g., CLIP (Contrastive Language-Image\nPre-training), trained on a massive amount of image-caption pairs, has shown a\nstrong zero-shot capability on various vision tasks. To further study the\nadvantage brought by CLIP, we propose to use CLIP as the visual encoder in\nvarious V&L models in two typical scenarios: 1) plugging CLIP into\ntask-specific fine-tuning; 2) combining CLIP with V&L pre-training and\ntransferring to downstream tasks. We show that CLIP significantly outperforms\nwidely-used visual encoders trained with in-domain annotated data, such as\nBottomUp-TopDown. We achieve competitive or better results on diverse V&L\ntasks, while establishing new state-of-the-art results on Visual Question\nAnswering, Visual Entailment, and V&L Navigation tasks. We release our code at\nhttps://github.com/clip-vil/CLIP-ViL.",
          "link": "http://arxiv.org/abs/2107.06383",
          "publishedOn": "2021-07-15T01:59:02.234Z",
          "wordCount": 618,
          "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1\">Sergio Rojas-Galeano</a>",
          "description": "One of the stratagems used to deceive spam filters is to substitute vocables\nwith synonyms or similar words that turn the message unrecognisable by the\ndetection algorithms. In this paper we investigate whether the recent\ndevelopment of language models sensitive to the semantics and context of words,\nsuch as Google's BERT, may be useful to overcome this adversarial attack\n(called \"Mad-lib\" as per the word substitution game). Using a dataset of 5572\nSMS spam messages, we first established a baseline of detection performance\nusing widely known document representation models (BoW and TFIDF) and the novel\nBERT model, coupled with a variety of classification algorithms (Decision Tree,\nkNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we\nbuilt a thesaurus of the vocabulary contained in these messages, and set up a\nMad-lib attack experiment in which we modified each message of a held out\nsubset of data (not used in the baseline experiment) with different rates of\nsubstitution of original words with synonyms from the thesaurus. Lastly, we\nevaluated the detection performance of the three representation models (BoW,\nTFIDF and BERT) coupled with the best classifier from the baseline experiment\n(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)\nin the original dataset, whereas the BERT model obtained 96%. On the other\nhand, the Mad-lib attack experiment showed that BERT encodings manage to\nmaintain a similar BA performance of 96% with an average substitution rate of\n1.82 words per message, and 95% with 3.34 words substituted per message. In\ncontrast, the BA performance of the BoW and TFIDF encoders dropped to chance.\nThese results hint at the potential advantage of BERT models to combat these\ntype of ingenious attacks, offsetting to some extent for the inappropriate use\nof semantic relationships in language.",
          "link": "http://arxiv.org/abs/2107.06400",
          "publishedOn": "2021-07-15T01:59:02.123Z",
          "wordCount": 732,
          "title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nath_A/0/1/0/all/0/1\">Apurba Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubba_A/0/1/0/all/0/1\">Aayush Kubba</a>",
          "description": "Can we discover dialog structure by dividing utterances into labelled\nclusters. Can these labels be generated from the data. Typically for dialogs we\nneed an ontology and use that to discover structure, however by using\nunsupervised classification and self-labelling we are able to intuit this\nstructure without any labels or ontology. In this paper we apply SCAN (Semantic\nClustering using Nearest Neighbors) to dialog data. We used BERT for pretext\ntask and an adaptation of SCAN for clustering and self labeling. These clusters\nare used to identify transition probabilities and create the dialog structure.\nThe self-labelling method used for SCAN makes these structures interpretable as\nevery cluster has a label. As the approach is unsupervised, evaluation metrics\nis a challenge, we use statistical measures as proxies for structure quality",
          "link": "http://arxiv.org/abs/2107.06426",
          "publishedOn": "2021-07-15T01:59:02.099Z",
          "wordCount": 561,
          "title": "TSCAN : Dialog Structure discovery using SCAN. (arXiv:2107.06426v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zining Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>",
          "description": "As the numbers of submissions to conferences grow quickly, the task of\nassessing the quality of academic papers automatically, convincingly, and with\nhigh accuracy attracts increasing attention. We argue that studying\ninterpretable dimensions of these submissions could lead to scalable solutions.\nWe extract a collection of writing features, and construct a suite of\nprediction tasks to assess the usefulness of these features in predicting\ncitation counts and the publication of AI-related papers. Depending on the\nvenues, the writing features can predict the conference vs. workshop appearance\nwith F1 scores up to 60-90, sometimes even outperforming the content-based\ntf-idf features and RoBERTa. We show that the features describe writing style\nmore than content. To further understand the results, we estimate the causal\nimpact of the most indicative features. Our analysis on writing features\nprovides a perspective to assessing and refining the writing of academic\narticles at scale.",
          "link": "http://arxiv.org/abs/2107.06310",
          "publishedOn": "2021-07-15T01:59:02.084Z",
          "wordCount": 585,
          "title": "What do writing features tell us about AI papers?. (arXiv:2107.06310v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fareri_S/0/1/0/all/0/1\">Silvia Fareri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melluso_N/0/1/0/all/0/1\">Nicola Melluso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiarello_F/0/1/0/all/0/1\">Filippo Chiarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fantoni_G/0/1/0/all/0/1\">Gualtiero Fantoni</a>",
          "description": "In today's digital world, there is an increasing focus on soft skills. On the\none hand, they facilitate innovation at companies, but on the other, they are\nunlikely to be automated soon. Researchers struggle with accurately approaching\nquantitatively the study of soft skills due to the lack of data-driven methods\nto retrieve them. This limits the possibility for psychologists and HR managers\nto understand the relation between humans and digitalisation. This paper\npresents SkillNER, a novel data-driven method for automatically extracting soft\nskills from text. It is a named entity recognition (NER) system trained with a\nsupport vector machine (SVM) on a corpus of more than 5000 scientific papers.\nWe developed this system by measuring the performance of our approach against\ndifferent training models and validating the results together with a team of\npsychologists. Finally, SkillNER was tested in a real-world case study using\nthe job descriptions of ESCO (European Skill/Competence Qualification and\nOccupation) as textual source. The system enabled the detection of communities\nof job profiles based on their shared soft skills and communities of soft\nskills based on their shared job profiles. This case study demonstrates that\nthe tool can automatically retrieve soft skills from a large corpus in an\nefficient way, proving useful for firms, institutions, and workers. The tool is\nopen and available online to foster quantitative methods for the study of soft\nskills.",
          "link": "http://arxiv.org/abs/2101.11431",
          "publishedOn": "2021-07-14T01:41:49.489Z",
          "wordCount": 699,
          "title": "SkillNER: Mining and Mapping Soft Skills from any Text. (arXiv:2101.11431v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiusheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1\">Nikhil Bhendawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Ting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1\">Desheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1\">Bingyu Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>",
          "description": "Transformer-based models have made tremendous impacts in natural language\ngeneration. However the inference speed is a bottleneck due to large model size\nand intensive computing involved in auto-regressive decoding process. We\ndevelop FastSeq framework to accelerate sequence generation without accuracy\nloss. The proposed optimization techniques include an attention cache\noptimization, an efficient algorithm for detecting repeated n-grams, and an\nasynchronous generation pipeline with parallel I/O. These optimizations are\ngeneral enough to be applicable to Transformer-based models (e.g., T5, GPT2,\nand UniLM). Our benchmark results on a set of widely used and diverse models\ndemonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use\nwith a simple one-line code change. The source code is available at\nhttps://github.com/microsoft/fastseq.",
          "link": "http://arxiv.org/abs/2106.04718",
          "publishedOn": "2021-07-14T01:41:49.481Z",
          "wordCount": 595,
          "title": "FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.00773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>",
          "description": "This paper analyzes challenges in cloze-style reading comprehension on\nmultiparty dialogue and suggests two new tasks for more comprehensive\npredictions of personal entities in daily conversations. We first demonstrate\nthat there are substantial limitations to the evaluation methods of previous\nwork, namely that randomized assignment of samples to training and test data\nsubstantially decreases the complexity of cloze-style reading comprehension.\nAccording to our analysis, replacing the random data split with a chronological\ndata split reduces test accuracy on previous single-variable passage completion\ntask from 72\\% to 34\\%, that leaves much more room to improve. Our proposed\ntasks extend the previous single-variable passage completion task by replacing\nmore character mentions with variables. Several deep learning models are\ndeveloped to validate these three tasks. A thorough error analysis is provided\nto understand the challenges and guide the future direction of this research.",
          "link": "http://arxiv.org/abs/1911.00773",
          "publishedOn": "2021-07-14T01:41:49.461Z",
          "wordCount": 611,
          "title": "Design and Challenges of Cloze-Style Reading Comprehension Tasks on Multiparty Dialogue. (arXiv:1911.00773v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doo Soon Kim</a>",
          "description": "Since the first end-to-end neural coreference resolution model was\nintroduced, many extensions to the model have been proposed, ranging from using\nhigher-order inference to directly optimizing evaluation metrics using\nreinforcement learning. Despite improving the coreference resolution\nperformance by a large margin, these extensions add a lot of extra complexity\nto the original model. Motivated by this observation and the recent advances in\npre-trained Transformer language models, we propose a simple yet effective\nbaseline for coreference resolution. Our model is a simplified version of the\noriginal neural coreference resolution model, however, it achieves impressive\nperformance, outperforming all recent extended works on the public English\nOntoNotes benchmark. Our work provides evidence for the necessity of carefully\njustifying the complexity of existing or newly proposed models, as introducing\na conceptual or practical simplification to an existing model can still yield\ncompetitive results.",
          "link": "http://arxiv.org/abs/2107.01700",
          "publishedOn": "2021-07-14T01:41:49.447Z",
          "wordCount": 593,
          "title": "End-to-end Neural Coreference Resolution Revisited: A Simple yet Effective Baseline. (arXiv:2107.01700v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David I. Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adebonojo_D/0/1/0/all/0/1\">Damilola Adebonojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayeni_A/0/1/0/all/0/1\">Adesina Ayeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofe Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1\">Ayodele Awokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>",
          "description": "Massively multilingual machine translation (MT) has shown impressive\ncapabilities, including zero and few-shot translation between low-resource\nlanguage pairs. However, these models are often evaluated on high-resource\nlanguages with the assumption that they generalize to low-resource ones. The\ndifficulty of evaluating MT models on low-resource pairs is often due to lack\nof standardized evaluation datasets. In this paper, we present MENYO-20k, the\nfirst multi-domain parallel corpus with a special focus on clean orthography\nfor Yor\\`ub\\'a--English with standardized train-test splits for benchmarking.\nWe provide several neural MT benchmarks and compare them to the performance of\npopular pre-trained (massively multilingual) MT models both for the\nheterogeneous test set and its subdomains. Since these pre-trained models use\nhuge amounts of data with uncertain quality, we also analyze the effect of\ndiacritics, a major characteristic of Yor\\`ub\\'a, in the training data. We\ninvestigate how and when this training condition affects the final quality and\nintelligibility of a translation. Our models outperform massively multilingual\nmodels such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$ BLEU) when\ntranslating to Yor\\`ub\\'a, setting a high quality benchmark for future\nresearch.",
          "link": "http://arxiv.org/abs/2103.08647",
          "publishedOn": "2021-07-14T01:41:49.440Z",
          "wordCount": 662,
          "title": "The Effect of Domain and Diacritics in Yor\\`ub\\'a-English Neural Machine Translation. (arXiv:2103.08647v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1\">Pedram Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1\">Pouya Pezeshkpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminnaseri_M/0/1/0/all/0/1\">Moin Aminnaseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitaab_M/0/1/0/all/0/1\">Marzieh Bitaab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheini_M/0/1/0/all/0/1\">Mozhdeh Gheini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabiri_A/0/1/0/all/0/1\">Arman Kabiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1\">Rabeeh Karimi Mahabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memarrast_O/0/1/0/all/0/1\">Omid Memarrast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosallanezhad_A/0/1/0/all/0/1\">Ahmadreza Mosallanezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noury_E/0/1/0/all/0/1\">Erfan Noury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raji_S/0/1/0/all/0/1\">Shahab Raji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_S/0/1/0/all/0/1\">Sepideh Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azer_E/0/1/0/all/0/1\">Erfan Sadeqi Azer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samghabadi_N/0/1/0/all/0/1\">Niloofar Safi Samghabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_M/0/1/0/all/0/1\">Mahsa Shafaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheybani_S/0/1/0/all/0/1\">Saber Sheybani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1\">Ali Tazarv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>",
          "description": "Despite the progress made in recent years in addressing natural language\nunderstanding (NLU) challenges, the majority of this progress remains to be\nconcentrated on resource-rich languages like English. This work focuses on\nPersian language, one of the widely spoken languages in the world, and yet\nthere are few NLU datasets available for this rich language. The availability\nof high-quality evaluation datasets is a necessity for reliable assessment of\nthe progress on different NLU tasks and domains. We introduce ParsiNLU, the\nfirst benchmark in Persian language that includes a range of high-level tasks\n-- Reading Comprehension, Textual Entailment, etc. These datasets are collected\nin a multitude of ways, often involving manual annotations by native speakers.\nThis results in over 14.5$k$ new instances across 6 distinct NLU tasks.\nBesides, we present the first results on state-of-the-art monolingual and\nmulti-lingual pre-trained language-models on this benchmark and compare them\nwith human performance, which provides valuable insights into our ability to\ntackle natural language understanding challenges in Persian. We hope ParsiNLU\nfosters further research and advances in Persian language understanding.",
          "link": "http://arxiv.org/abs/2012.06154",
          "publishedOn": "2021-07-14T01:41:49.433Z",
          "wordCount": 703,
          "title": "ParsiNLU: A Suite of Language Understanding Challenges for Persian. (arXiv:2012.06154v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Jui Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>",
          "description": "In recent times, BERT based transformer models have become an inseparable\npart of the 'tech stack' of text processing models. Similar progress is being\nobserved in the speech domain with a multitude of models observing\nstate-of-the-art results by using audio transformer models to encode speech.\nThis begs the question of what are these audio transformer models learning.\nMoreover, although the standard methodology is to choose the last layer\nembedding for any downstream task, but is it the optimal choice? We try to\nanswer these questions for the two recent audio transformer models, Mockingjay\nand wave2vec2.0. We compare them on a comprehensive set of language delivery\nand structure features including audio, fluency and pronunciation features.\nAdditionally, we probe the audio models' understanding of textual surface,\nsyntax, and semantic features and compare them to BERT. We do this over\nexhaustive settings for native, non-native, synthetic, read and spontaneous\nspeech datasets",
          "link": "http://arxiv.org/abs/2101.00387",
          "publishedOn": "2021-07-14T01:41:49.397Z",
          "wordCount": 641,
          "title": "What all do audio transformer models hear? Probing Acoustic Representations for Language Delivery and its Structure. (arXiv:2101.00387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahim Shahriar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1\">Mueeze Al Mushabbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>",
          "description": "Chatbots are intelligent software built to be used as a replacement for human\ninteraction. However, existing studies typically do not provide enough support\nfor low-resource languages like Bangla. Moreover, due to the increasing\npopularity of social media, we can also see the rise of interactions in Bangla\ntransliteration (mostly in English) among the native Bangla speakers. In this\npaper, we propose a novel approach to build a Bangla chatbot aimed to be used\nas a business assistant which can communicate in Bangla and Bangla\nTransliteration in English with high confidence consistently. Since annotated\ndata was not available for this purpose, we had to work on the whole machine\nlearning life cycle (data preparation, machine learning modeling, and model\ndeployment) using Rasa Open Source Framework, fastText embeddings, Polyglot\nembeddings, Flask, and other systems as building blocks. While working with the\nskewed annotated dataset, we try out different setups and pipelines to evaluate\nwhich works best and provide possible reasoning behind the observed results.\nFinally, we present a pipeline for intent classification and entity extraction\nwhich achieves reasonable performance (accuracy: 83.02\\%, precision: 80.82\\%,\nrecall: 83.02\\%, F1-score: 80\\%).",
          "link": "http://arxiv.org/abs/2107.05541",
          "publishedOn": "2021-07-14T01:41:49.390Z",
          "wordCount": 652,
          "title": "End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengdi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Minghui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shaosheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>",
          "description": "In the Chinese medical insurance industry, the assessor's role is essential\nand requires significant efforts to converse with the claimant. This is a\nhighly professional job that involves many parts, such as identifying personal\ninformation, collecting related evidence, and making a final insurance report.\nDue to the coronavirus (COVID-19) pandemic, the previous offline insurance\nassessment has to be conducted online. However, for the junior assessor often\nlacking practical experience, it is not easy to quickly handle such a complex\nonline procedure, yet this is important as the insurance company needs to\ndecide how much compensation the claimant should receive based on the\nassessor's feedback. In order to promote assessors' work efficiency and speed\nup the overall procedure, in this paper, we propose a dialogue-based\ninformation extraction system that integrates advanced NLP technologies for\nmedical insurance assessment. With the assistance of our system, the average\ntime cost of the procedure is reduced from 55 minutes to 35 minutes, and the\ntotal human resources cost is saved 30% compared with the previous offline\nprocedure. Until now, the system has already served thousands of online claim\ncases.",
          "link": "http://arxiv.org/abs/2107.05866",
          "publishedOn": "2021-07-14T01:41:49.383Z",
          "wordCount": 687,
          "title": "A Dialogue-based Information Extraction System for Medical Insurance Assessment. (arXiv:2107.05866v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalamkar_P/0/1/0/all/0/1\">Prathamesh Kalamkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D%2E_J/0/1/0/all/0/1\">Janani Venugopalan Ph.D.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D_V/0/1/0/all/0/1\">Vivek Raghavan Ph.D</a>",
          "description": "Availability of challenging benchmarks is the key to advancement of AI in a\nspecific field.Since Legal Text is significantly different than normal English\ntext, there is a need to create separate Natural Language Processing benchmarks\nfor Indian Legal Text which are challenging and focus on tasks specific to\nLegal Systems. This will spur innovation in applications of Natural language\nProcessing for Indian Legal Text and will benefit AI community and Legal\nfraternity. We review the existing work in this area and propose ideas to\ncreate new benchmarks for Indian Legal Natural Language Processing.",
          "link": "http://arxiv.org/abs/2107.06056",
          "publishedOn": "2021-07-14T01:41:49.355Z",
          "wordCount": 526,
          "title": "Indian Legal NLP Benchmarks : A Survey. (arXiv:2107.06056v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.12006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>",
          "description": "We present a method to represent input texts by contextualizing them jointly\nwith dynamically retrieved textual encyclopedic background knowledge from\nmultiple documents. We apply our method to reading comprehension tasks by\nencoding questions and passages together with background sentences about the\nentities they mention. We show that integrating background knowledge from text\nis effective for tasks focusing on factual reasoning and allows direct reuse of\npowerful pretrained BERT-style encoders. Moreover, knowledge integration can be\nfurther improved with suitable pretraining via a self-supervised masked\nlanguage model objective over words in background-augmented input text. On\nTriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable\nRoBERTa models which do not integrate background knowledge dynamically. On\nMRQA, a large collection of diverse QA datasets, we see consistent gains\nin-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2\nF1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1).",
          "link": "http://arxiv.org/abs/2004.12006",
          "publishedOn": "2021-07-14T01:41:49.338Z",
          "wordCount": 612,
          "title": "Contextualized Representations Using Textual Encyclopedic Knowledge. (arXiv:2004.12006v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Boyuan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Augmenting pre-trained language models with knowledge graphs (KGs) has\nachieved success on various commonsense reasoning tasks. However, for a given\ntask instance, the KG, or certain parts of the KG, may not be useful. Although\nKG-augmented models often use attention to focus on specific KG components, the\nKG is still always used, and the attention mechanism is never explicitly taught\nwhich KG components should be used. Meanwhile, saliency methods can measure how\nmuch a KG feature (e.g., graph, node, path) influences the model to make the\ncorrect prediction, thus explaining which KG features are useful. This paper\nexplores how saliency explanations can be used to improve KG-augmented models'\nperformance. First, we propose to create coarse (Is the KG useful?) and fine\n(Which nodes/paths in the KG are useful?) saliency explanations. Second, we\npropose SalKG, a framework for KG-augmented models to learn from coarse and/or\nfine saliency explanations. Given saliency explanations created from a task's\ntraining set, SalKG jointly trains the model to predict the explanations, then\nsolve the task by attending to KG features highlighted by the predicted\nexplanations. On two popular commonsense QA benchmarks (CSQA, OBQA), we show\nthat \\textsc{SalKG} models can yield large performance gains -- up to 3.27% on\nCSQA.",
          "link": "http://arxiv.org/abs/2104.08793",
          "publishedOn": "2021-07-14T01:41:49.331Z",
          "wordCount": 675,
          "title": "SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning. (arXiv:2104.08793v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1\">Alina Karakanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>",
          "description": "Speech translation (ST) has lately received growing interest for the\ngeneration of subtitles without the need for an intermediate source language\ntranscription and timing (i.e. captions). However, the joint generation of\nsource captions and target subtitles does not only bring potential output\nquality advantages when the two decoding processes inform each other, but it is\nalso often required in multilingual scenarios. In this work, we focus on ST\nmodels which generate consistent captions-subtitles in terms of structure and\nlexical content. We further introduce new metrics for evaluating subtitling\nconsistency. Our findings show that joint decoding leads to increased\nperformance and consistency between the generated captions and subtitles while\nstill allowing for sufficient flexibility to produce subtitles conforming to\nlanguage-specific needs and norms.",
          "link": "http://arxiv.org/abs/2107.06246",
          "publishedOn": "2021-07-14T01:41:49.303Z",
          "wordCount": 563,
          "title": "Between Flexibility and Consistency: Joint Generation of Captions and Subtitles. (arXiv:2107.06246v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rossenbach_N/0/1/0/all/0/1\">Nick Rossenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilmes_B/0/1/0/all/0/1\">Benedikt Hilmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "Recent publications on automatic-speech-recognition (ASR) have a strong focus\non attention encoder-decoder (AED) architectures which tend to suffer from\nover-fitting in low resource scenarios. One solution to tackle this issue is to\ngenerate synthetic data with a trained text-to-speech system (TTS) if\nadditional text is available. This was successfully applied in many\npublications with AED systems, but only very limited in the context of other\nASR architectures. We investigate the effect of varying pre-processing, the\nspeaker embedding and input encoding of the TTS system w.r.t. the effectiveness\nof the synthesized data for AED-ASR training. Additionally, we also consider\ninternal language model subtraction for the first time, resulting in up to 38%\nrelative improvement. We compare the AED results to a state-of-the-art hybrid\nASR system, a monophone based system using\nconnectionist-temporal-classification (CTC) and a monotonic transducer based\nsystem. We show that for the later systems the addition of synthetic data has\nno relevant effect, but they still outperform the AED systems on\nLibriSpeech-100h. We achieve a final word-error-rate of 3.3%/10.0% with a\nhybrid system on the clean/noisy test-sets, surpassing any previous\nstate-of-the-art systems on Librispeech-100h that do not include unlabeled\naudio data.",
          "link": "http://arxiv.org/abs/2104.05379",
          "publishedOn": "2021-07-14T01:41:49.292Z",
          "wordCount": 681,
          "title": "Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures. (arXiv:2104.05379v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guojun Wu</a>",
          "description": "The rise of manipulating fake news as a political weapon has become a global\nconcern and highlighted the incapability of manually fact checking against\nrapidly produced fake news. Thus, statistical approaches are required if we are\nto address this problem efficiently. The shortage of publicly available\ndatasets is one major bottleneck of automated fact checking. To remedy this, we\ncollected 24K manually rated statements from PolitiFact. The class values\nexhibit a natural order with respect to truthfulness as shown in Table 1. Thus,\nour task represents a twist from standard classification, due to the various\ndegrees of similarity between classes. To investigate this, we defined\ncoarse-to-fine classification regimes, which presents new challenge for\nclassification. To address this, we propose BERT-based models. After training,\nclass similarity is sensible over the multi-class datasets, especially in the\nfine-grained one. Under all the regimes, BERT achieves state of the art, while\nthe additional layers provide insignificant improvement.",
          "link": "http://arxiv.org/abs/2107.06051",
          "publishedOn": "2021-07-14T01:41:49.285Z",
          "wordCount": 573,
          "title": "Rating Facts under Coarse-to-fine Regimes. (arXiv:2107.06051v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.01463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">Didier Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>",
          "description": "Adapter modules were recently introduced as an efficient alternative to\nfine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters\nof a model and injecting lightweight modules between layers, resulting in the\naddition of only a small number of task-specific trainable parameters. While\nadapter tuning was investigated for multilingual neural machine translation,\nthis paper proposes a comprehensive analysis of adapters for multilingual\nspeech translation (ST). Starting from different pre-trained models (a\nmultilingual ST trained on parallel data or a multilingual BART (mBART) trained\non non-parallel multilingual data), we show that adapters can be used to: (a)\nefficiently specialize ST to specific language pairs with a low extra cost in\nterms of parameters, and (b) transfer from an automatic speech recognition\n(ASR) task and an mBART pre-trained model to a multilingual ST task.\nExperiments show that adapter tuning offer competitive results to full\nfine-tuning, while being much more parameter-efficient.",
          "link": "http://arxiv.org/abs/2106.01463",
          "publishedOn": "2021-07-14T01:41:49.258Z",
          "wordCount": 616,
          "title": "Lightweight Adapter Tuning for Multilingual Speech Translation. (arXiv:2106.01463v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vydana_H/0/1/0/all/0/1\">Hari Krishna Vydana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karafiat_M/0/1/0/all/0/1\">Martin Karafi&#x27;at</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burget_L/0/1/0/all/0/1\">Luk&#x27;as Burget</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cernocky_%22/0/1/0/all/0/1\">&quot;Honza&quot; Cernock&#x27;y</a>",
          "description": "The paper describes BUT's English to German offline speech translation(ST)\nsystems developed for IWSLT2021. They are based on jointly trained Automatic\nSpeech Recognition-Machine Translation models. Their performances is evaluated\non MustC-Common test set. In this work, we study their efficiency from the\nperspective of having a large amount of separate ASR training data and MT\ntraining data, and a smaller amount of speech-translation training data. Large\namounts of ASR and MT training data are utilized for pre-training the ASR and\nMT models. Speech-translation data is used to jointly optimize ASR-MT models by\ndefining an end-to-end differentiable path from speech to translations. For\nthis purpose, we use the internal continuous representations from the\nASR-decoder as the input to MT module. We show that speech translation can be\nfurther improved by training the ASR-decoder jointly with the MT-module using\nlarge amount of text-only MT training data. We also show significant\nimprovements by training an ASR module capable of generating punctuated text,\nrather than leaving the punctuation task to the MT module.",
          "link": "http://arxiv.org/abs/2107.06155",
          "publishedOn": "2021-07-14T01:41:49.250Z",
          "wordCount": 611,
          "title": "The IWSLT 2021 BUT Speech Translation Systems. (arXiv:2107.06155v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renggli_C/0/1/0/all/0/1\">Cedric Renggli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaus_B/0/1/0/all/0/1\">Benjamin Glaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_M/0/1/0/all/0/1\">Maria Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troendle_M/0/1/0/all/0/1\">Marius Troendle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_N/0/1/0/all/0/1\">Nicolas Langer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Until recently, human behavioral data from reading has mainly been of\ninterest to researchers to understand human cognition. However, these human\nlanguage processing signals can also be beneficial in machine learning-based\nnatural language processing tasks. Using EEG brain activity to this purpose is\nlargely unexplored as of yet. In this paper, we present the first large-scale\nstudy of systematically analyzing the potential of EEG brain activity data for\nimproving natural language processing tasks, with a special focus on which\nfeatures of the signal are most beneficial. We present a multi-modal machine\nlearning architecture that learns jointly from textual input as well as from\nEEG features. We find that filtering the EEG signals into frequency bands is\nmore beneficial than using the broadband signal. Moreover, for a range of word\nembedding types, EEG data improves binary and ternary sentiment classification\nand outperforms multiple baselines. For more complex tasks such as relation\ndetection, further research is needed. Finally, EEG data shows to be\nparticularly promising when limited training data is available.",
          "link": "http://arxiv.org/abs/2102.08655",
          "publishedOn": "2021-07-14T01:41:49.242Z",
          "wordCount": 639,
          "title": "Decoding EEG Brain Activity for Multi-Modal Natural Language Processing. (arXiv:2102.08655v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+We_S/0/1/0/all/0/1\">Shi-jie We</a>",
          "description": "This paper presents the participation of the MiniTrue team in the FinSim-3\nshared task on learning semantic similarities for the financial domain in\nEnglish language. Our approach combines contextual embeddings learned by\ntransformer-based language models with network structures embeddings extracted\non external knowledge sources, to create more meaningful representations of\nfinancial domain entities and terms. For this, two BERT based language models\nand a knowledge graph embedding model are used. Besides, we propose a voting\nfunction to joint three basic models for the final inference. Experimental\nresults show that the model with the knowledge graph embeddings has achieved a\nsuperior result than these models with only contextual embeddings.\nNevertheless, we also observe that our voting function brings an extra benefit\nto the final system.",
          "link": "http://arxiv.org/abs/2107.05885",
          "publishedOn": "2021-07-14T01:41:49.233Z",
          "wordCount": 566,
          "title": "Exploiting Network Structures to Improve Semantic Representation for the Financial Domain. (arXiv:2107.05885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jiajie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>",
          "description": "Attention is a key mechanism for information selection in both biological\nbrains and many state-of-the-art deep neural networks (DNNs). Here, we\ninvestigate whether humans and DNNs allocate attention in comparable ways when\nreading a text passage to subsequently answer a specific question. We analyze 3\ntransformer-based DNNs that reach human-level performance when trained to\nperform the reading comprehension task. We find that the DNN attention\ndistribution quantitatively resembles human attention distribution measured by\nfixation times. Human readers fixate longer on words that are more relevant to\nthe question-answering task, demonstrating that attention is modulated by\ntop-down reading goals, on top of lower-level visual and text features of the\nstimulus. Further analyses reveal that the attention weights in DNNs are also\ninfluenced by both top-down reading goals and lower-level stimulus features,\nwith the shallow layers more strongly influenced by lower-level text features\nand the deep layers attending more to task-relevant words. Additionally, deep\nlayers' attention to task-relevant words gradually emerges when pre-trained DNN\nmodels are fine-tuned to perform the reading comprehension task, which\ncoincides with the improvement in task performance. These results demonstrate\nthat DNNs can evolve human-like attention distribution through task\noptimization, which suggests that human attention during goal-directed reading\ncomprehension is a consequence of task optimization.",
          "link": "http://arxiv.org/abs/2107.05799",
          "publishedOn": "2021-07-14T01:41:49.225Z",
          "wordCount": 641,
          "title": "Deep Neural Networks Evolve Human-like Attention Distribution during Reading Comprehension. (arXiv:2107.05799v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>",
          "description": "Transformer-based end-to-end speech recognition models have received\nconsiderable attention in recent years due to their high training speed and\nability to model a long-range global context. Position embedding in the\ntransformer architecture is indispensable because it provides supervision for\ndependency modeling between elements at different positions in the input\nsequence. To make use of the time order of the input sequence, many works\ninject some information about the relative or absolute position of the element\ninto the input sequence. In this work, we investigate various position\nembedding methods in the convolution-augmented transformer (conformer) and\nadopt a novel implementation named rotary position embedding (RoPE). RoPE\nencodes absolute positional information into the input sequence by a rotation\nmatrix, and then naturally incorporates explicit relative position information\ninto a self-attention module. To evaluate the effectiveness of the RoPE method,\nwe conducted experiments on AISHELL-1 and LibriSpeech corpora. Results show\nthat the conformer enhanced with RoPE achieves superior performance in the\nspeech recognition task. Specifically, our model achieves a relative word error\nrate reduction of 8.70% and 7.27% over the conformer on test-clean and\ntest-other sets of the LibriSpeech corpus respectively.",
          "link": "http://arxiv.org/abs/2107.05907",
          "publishedOn": "2021-07-14T01:41:49.217Z",
          "wordCount": 630,
          "title": "Conformer-based End-to-end Speech Recognition With Rotary Position Embedding. (arXiv:2107.05907v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nishtha Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovic_M/0/1/0/all/0/1\">Maja Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groves_D/0/1/0/all/0/1\">Declan Groves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>",
          "description": "Gender bias is a frequent occurrence in NLP-based applications, especially\npronounced in gender-inflected languages. Bias can appear through associations\nof certain adjectives and animate nouns with the natural gender of referents,\nbut also due to unbalanced grammatical gender frequencies of inflected words.\nThis type of bias becomes more evident in generating conversational utterances\nwhere gender is not specified within the sentence, because most current NLP\napplications still work on a sentence-level context. As a step towards more\ninclusive NLP, this paper proposes an automatic and generalisable rewriting\napproach for short conversational sentences. The rewriting method can be\napplied to sentences that, without extra-sentential context, have multiple\nequivalent alternatives in terms of gender. The method can be applied both for\ncreating gender balanced outputs as well as for creating gender balanced\ntraining data. The proposed approach is based on a neural machine translation\n(NMT) system trained to 'translate' from one gender alternative to another.\nBoth the automatic and manual analysis of the approach show promising results\nfor automatic generation of gender alternatives for conversational sentences in\nSpanish.",
          "link": "http://arxiv.org/abs/2107.05987",
          "publishedOn": "2021-07-14T01:41:49.168Z",
          "wordCount": 608,
          "title": "Generating Gender Augmented Data for NLP. (arXiv:2107.05987v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1\">Ahmet &#xdc;st&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sportel_S/0/1/0/all/0/1\">Stephan Sportel</a>",
          "description": "Identifying factors that make certain languages harder to model than others\nis essential to reach language equality in future Natural Language Processing\ntechnologies. Free-order case-marking languages, such as Russian, Latin or\nTamil, have proved more challenging than fixed-order languages for the tasks of\nsyntactic parsing and subject-verb agreement prediction. In this work, we\ninvestigate whether this class of languages is also more difficult to translate\nby state-of-the-art Neural Machine Translation models (NMT). Using a variety of\nsynthetic languages and a newly introduced translation challenge set, we find\nthat word order flexibility in the source language only leads to a very small\nloss of NMT quality, even though the core verb arguments become impossible to\ndisambiguate in sentences without semantic cues. The latter issue is indeed\nsolved by the addition of case marking. However, in medium- and low-resource\nsettings, the overall NMT quality of fixed-order languages remains unmatched.",
          "link": "http://arxiv.org/abs/2107.06055",
          "publishedOn": "2021-07-14T01:41:49.071Z",
          "wordCount": 585,
          "title": "On the Difficulty of Translating Free-Order Case-Marking Languages. (arXiv:2107.06055v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinyin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>",
          "description": "Named entity recognition (NER) is a well-studied task in natural language\nprocessing. Traditional NER research only deals with flat entities and ignores\nnested entities. The span-based methods treat entity recognition as a span\nclassification task. Although these methods have the innate ability to handle\nnested NER, they suffer from high computational cost, ignorance of boundary\ninformation, under-utilization of the spans that partially match with entities,\nand difficulties in long entity recognition. To tackle these issues, we propose\na two-stage entity identifier. First we generate span proposals by filtering\nand boundary regression on the seed spans to locate the entities, and then\nlabel the boundary-adjusted span proposals with the corresponding categories.\nOur method effectively utilizes the boundary information of entities and\npartially matched spans during training. Through boundary regression, entities\nof any length can be covered theoretically, which improves the ability to\nrecognize long entities. In addition, many low-quality seed spans are filtered\nout in the first stage, which reduces the time complexity of inference.\nExperiments on nested NER datasets demonstrate that our proposed method\noutperforms previous state-of-the-art models.",
          "link": "http://arxiv.org/abs/2105.06804",
          "publishedOn": "2021-07-14T01:41:49.059Z",
          "wordCount": 655,
          "title": "Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition. (arXiv:2105.06804v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tu Anh Dinh</a>",
          "description": "Speech Translation (ST) is the task of translating speech in one language\ninto text in another language. Traditional cascaded approaches for ST, using\nAutomatic Speech Recognition (ASR) and Machine Translation (MT) systems, are\nprone to error propagation. End-to-end approaches use only one system to avoid\npropagating error, yet are difficult to employ due to data scarcity. We explore\nzero-shot translation, which enables translating a pair of languages that is\nunseen during training, thus avoid the use of end-to-end ST data. Zero-shot\ntranslation has been shown to work for multilingual machine translation, yet\nhas not been studied for speech translation. We attempt to build zero-shot ST\nmodels that are trained only on ASR and MT tasks but can do ST task during\ninference. The challenge is that the representation of text and audio is\nsignificantly different, thus the models learn ASR and MT tasks in different\nways, making it non-trivial to perform zero-shot. These models tend to output\nthe wrong language when performing zero-shot ST. We tackle the issues by\nincluding additional training data and an auxiliary loss function that\nminimizes the text-audio difference. Our experiment results and analysis show\nthat the methods are promising for zero-shot ST. Moreover, our methods are\nparticularly useful in the few-shot settings where a limited amount of ST data\nis available, with improvements of up to +11.8 BLEU points compared to direct\nend-to-end ST models and +3.9 BLEU points compared to ST models fine-tuned from\npre-trained ASR model.",
          "link": "http://arxiv.org/abs/2107.06010",
          "publishedOn": "2021-07-14T01:41:49.035Z",
          "wordCount": 663,
          "title": "Zero-shot Speech Translation. (arXiv:2107.06010v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>",
          "description": "The predominant challenge in weakly supervised semantic parsing is that of\nspurious programs that evaluate to correct answers for the wrong reasons. Prior\nwork uses elaborate search strategies to mitigate the prevalence of spurious\nprograms; however, they typically consider only one input at a time. In this\nwork we explore the use of consistency between the output programs for related\ninputs to reduce the impact of spurious programs. We bias the program search\n(and thus the model's training signal) towards programs that map the same\nphrase in related inputs to the same sub-parts in their respective programs.\nAdditionally, we study the importance of designing logical formalisms that\nfacilitate this kind of consAistency-based training. We find that a more\nconsistent formalism leads to improved model performance even without\nconsistency-based training. When combined together, these two insights lead to\na 10% absolute improvement over the best prior result on the Natural Language\nVisual Reasoning dataset.",
          "link": "http://arxiv.org/abs/2107.05833",
          "publishedOn": "2021-07-14T01:41:48.992Z",
          "wordCount": 585,
          "title": "Enforcing Consistency in Weakly Supervised Semantic Parsing. (arXiv:2107.05833v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Williams_E/0/1/0/all/0/1\">Evan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_P/0/1/0/all/0/1\">Paul Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Sieu Tran</a>",
          "description": "This paper discusses the approach used by the Accenture Team for CLEF2021\nCheckThat! Lab, Task 1, to identify whether a claim made in social media would\nbe interesting to a wide audience and should be fact-checked. Twitter training\nand test data were provided in English, Arabic, Spanish, Turkish, and\nBulgarian. Claims were to be classified (check-worthy/not check-worthy) and\nranked in priority order for the fact-checker. Our method used deep neural\nnetwork transformer models with contextually sensitive lexical augmentation\napplied on the supplied training datasets to create additional training\nsamples. This augmentation approach improved the performance for all languages.\nOverall, our architecture and data augmentation pipeline produced the best\nsubmitted system for Arabic, and performance scales according to the quantity\nof provided training data for English, Spanish, Turkish, and Bulgarian. This\npaper investigates the deep neural network architectures for each language as\nwell as the provided data to examine why the approach worked so effectively for\nArabic, and discusses additional data augmentation measures that should could\nbe useful to this problem.",
          "link": "http://arxiv.org/abs/2107.05684",
          "publishedOn": "2021-07-14T01:41:48.984Z",
          "wordCount": 665,
          "title": "Accenture at CheckThat! 2021: Interesting claim identification and ranking with contextually sensitive lexical training data augmentation. (arXiv:2107.05684v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naylor_M/0/1/0/all/0/1\">Mitchell Naylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+French_C/0/1/0/all/0/1\">Christi French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terker_S/0/1/0/all/0/1\">Samantha Terker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_U/0/1/0/all/0/1\">Uday Kamath</a>",
          "description": "The healthcare domain is one of the most exciting application areas for\nmachine learning, but a lack of model transparency contributes to a lag in\nadoption within the industry. In this work, we explore the current art of\nexplainability and interpretability within a case study in clinical text\nclassification, using a task of mortality prediction within MIMIC-III clinical\nnotes. We demonstrate various visualization techniques for fully interpretable\nmethods as well as model-agnostic post hoc attributions, and we provide a\ngeneralized method for evaluating the quality of explanations using infidelity\nand local Lipschitz across model types from logistic regression to BERT\nvariants. With these metrics, we introduce a framework through which\npractitioners and researchers can assess the frontier between a model's\npredictive performance and the quality of its available explanations. We make\nour code available to encourage continued refinement of these methods.",
          "link": "http://arxiv.org/abs/2107.05693",
          "publishedOn": "2021-07-14T01:41:48.976Z",
          "wordCount": 598,
          "title": "Quantifying Explainability in NLP and Analyzing Algorithms for Performance-Explainability Tradeoff. (arXiv:2107.05693v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>",
          "description": "$\\textit{No man is an island.}$ Humans communicate with a large community by\ncoordinating with different interlocutors within short conversations. This\nability has been understudied by the research on building neural communicative\nagents. We study the task of few-shot $\\textit{language coordination}$: agents\nquickly adapting to their conversational partners' language abilities.\nDifferent from current communicative agents trained with self-play, we require\nthe lead agent to coordinate with a $\\textit{population}$ of agents with\ndifferent linguistic abilities, quickly adapting to communicate with unseen\nagents in the population. This requires the ability to model the partner's\nbeliefs, a vital component of human communication. Drawing inspiration from\ntheory-of-mind (ToM; Premack& Woodruff (1978)), we study the effect of the\nspeaker explicitly modeling the listeners' mental states. The speakers, as\nshown in our experiments, acquire the ability to predict the reactions of their\npartner, which helps it generate instructions that concisely express its\ncommunicative goal. We examine our hypothesis that the instructions generated\nwith ToM modeling yield better communication performance in both a referential\ngame and a language navigation task. Positive results from our experiments hint\nat the importance of explicitly modeling communication as a socio-pragmatic\nprogress.",
          "link": "http://arxiv.org/abs/2107.05697",
          "publishedOn": "2021-07-14T01:41:48.952Z",
          "wordCount": 627,
          "title": "Few-shot Language Coordination by Modeling Theory of Mind. (arXiv:2107.05697v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>",
          "description": "Active learning is the iterative construction of a classification model\nthrough targeted labeling, enabling significant labeling cost savings. As most\nresearch on active learning has been carried out before transformer-based\nlanguage models (\"transformers\") became popular, despite its practical\nimportance, comparably few papers have investigated how transformers can be\ncombined with active learning to date. This can be attributed to the fact that\nusing state-of-the-art query strategies for transformers induces a prohibitive\nruntime overhead, which effectively cancels out, or even outweighs\naforementioned cost savings. In this paper, we revisit uncertainty-based query\nstrategies, which had been largely outperformed before, but are particularly\nsuited in the context of fine-tuning transformers. In an extensive evaluation\non five widely used text classification benchmarks, we show that considerable\nimprovements of up to 14.4 percentage points in area under the learning curve\nare achieved, as well as a final accuracy close to the state of the art for all\nbut one benchmark, using only between 0.4% and 15% of the training data.",
          "link": "http://arxiv.org/abs/2107.05687",
          "publishedOn": "2021-07-14T01:41:48.940Z",
          "wordCount": 599,
          "title": "Uncertainty-based Query Strategies for Active Learning with Transformers. (arXiv:2107.05687v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05876",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>",
          "description": "Multilingual automatic speech recognition (ASR) models have shown great\npromise in recent years because of the simplified model training and deployment\nprocess. Conventional methods either train a universal multilingual model\nwithout taking any language information or with a 1-hot language ID (LID)\nvector to guide the recognition of the target language. In practice, the user\ncan be prompted to pre-select several languages he/she can speak. The\nmultilingual model without LID cannot well utilize the language information set\nby the user while the multilingual model with LID can only handle one\npre-selected language. In this paper, we propose a novel configurable\nmultilingual model (CMM) which is trained only once but can be configured as\ndifferent models based on users' choices by extracting language-specific\nmodules together with a universal model from the trained CMM. Particularly, a\nsingle CMM can be deployed to any user scenario where the users can pre-select\nany combination of languages. Trained with 75K hours of transcribed anonymized\nMicrosoft multilingual data and evaluated with 10-language test sets, the\nproposed CMM improves from the universal multilingual model by 26.0%, 16.9%,\nand 10.4% relative word error reduction when the user selects 1, 2, or 3\nlanguages, respectively. CMM also performs significantly better on\ncode-switching test sets.",
          "link": "http://arxiv.org/abs/2107.05876",
          "publishedOn": "2021-07-14T01:41:48.929Z",
          "wordCount": 659,
          "title": "A Configurable Multilingual Model is All You Need to Recognize All Languages. (arXiv:2107.05876v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>",
          "description": "Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.",
          "link": "http://arxiv.org/abs/2107.05768",
          "publishedOn": "2021-07-14T01:41:48.921Z",
          "wordCount": 664,
          "title": "Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keymanesh_M/0/1/0/all/0/1\">Moniba Keymanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1\">Tanya Berger-Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsner_M/0/1/0/all/0/1\">Micha Elsner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>",
          "description": "In many applications such as recidivism prediction, facility inspection, and\nbenefit assignment, it's important for individuals to know the\ndecision-relevant information for the model's prediction. In addition, the\nmodel's predictions should be fairly justified. Essentially, decision-relevant\nfeatures should provide sufficient information for the predicted outcome and\nshould be independent of the membership of individuals in protected groups such\nas race and gender. In this work, we focus on the problem of (un)fairness in\nthe justification of the text-based neural models. We tie the explanatory power\nof the model to fairness in the outcome and propose a fairness-aware\nsummarization mechanism to detect and counteract the bias in such models. Given\na potentially biased natural language explanation for a decision, we use a\nmulti-task neural model and an attribution mechanism based on integrated\ngradients to extract the high-utility and discrimination-free justifications in\nthe form of a summary. The extracted summary is then used for training a model\nto make decisions for individuals. Results on several real-world datasets\nsuggests that our method: (i) assists users to understand what information is\nused for the model's decision and (ii) enhances the fairness in outcomes while\nsignificantly reducing the demographic leakage.",
          "link": "http://arxiv.org/abs/2107.06243",
          "publishedOn": "2021-07-14T01:41:48.911Z",
          "wordCount": 633,
          "title": "Fairness-aware Summarization for Justified Decision-Making. (arXiv:2107.06243v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genzel_D/0/1/0/all/0/1\">Dmitriy Genzel</a>",
          "description": "Pretraining and multitask learning are widely used to improve the speech to\ntext translation performance. In this study, we are interested in training a\nspeech to text translation model along with an auxiliary text to text\ntranslation task. We conduct a detailed analysis to understand the impact of\nthe auxiliary task on the primary task within the multitask learning framework.\nOur analysis confirms that multitask learning tends to generate similar decoder\nrepresentations from different modalities and preserve more information from\nthe pretrained text translation modules. We observe minimal negative transfer\neffect between the two tasks and sharing more parameters is helpful to transfer\nknowledge from the text task to the speech task. The analysis also reveals that\nthe modality representation difference at the top decoder layers is still not\nnegligible, and those layers are critical for the translation quality. Inspired\nby these findings, we propose three methods to improve translation quality.\nFirst, a parameter sharing and initialization strategy is proposed to enhance\ninformation sharing between the tasks. Second, a novel attention-based\nregularization is proposed for the encoders and pulls the representations from\ndifferent modalities closer. Third, an online knowledge distillation is\nproposed to enhance the knowledge transfer from the text to the speech task.\nOur experiments show that the proposed approach improves translation\nperformance by more than 2 BLEU over a strong baseline and achieves\nstate-of-the-art results on the \\textsc{MuST-C} English-German, English-French\nand English-Spanish language pairs.",
          "link": "http://arxiv.org/abs/2107.05782",
          "publishedOn": "2021-07-14T01:41:48.902Z",
          "wordCount": 695,
          "title": "Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task. (arXiv:2107.05782v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinhong Zhang</a>",
          "description": "This paper describes our system at NLPTEA-2020 Task: Chinese Grammatical\nError Diagnosis (CGED). The goal of CGED is to diagnose four types of\ngrammatical errors: word selection (S), redundant words (R), missing words (M),\nand disordered words (W). The automatic CGED system contains two parts\nincluding error detection and error correction and our system is designed to\nsolve the error detection problem. Our system is built on three models: 1) a\nBERT-based model leveraging syntactic information; 2) a BERT-based model\nleveraging contextual embeddings; 3) a lexicon-based graph neural network\nleveraging lexical information. We also design an ensemble mechanism to improve\nthe single model's performance. Finally, our system achieves the highest F1\nscores at detection level and identification level among all teams\nparticipating in the CGED 2020 task.",
          "link": "http://arxiv.org/abs/2105.09085",
          "publishedOn": "2021-07-13T01:59:33.737Z",
          "wordCount": 589,
          "title": "Combining GCN and Transformer for Chinese Grammatical Error Detection. (arXiv:2105.09085v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Celli_F/0/1/0/all/0/1\">Fabio Celli</a>",
          "description": "In this technical report, we propose an algorithm, called Lex2vec that\nexploits lexical resources to inject information into word embeddings and name\nthe embedding dimensions by means of knowledge bases. We evaluate the optimal\nparameters to extract a number of informative labels that is readable and has a\ngood coverage for the embedding dimensions.",
          "link": "http://arxiv.org/abs/2103.02269",
          "publishedOn": "2021-07-13T01:59:33.699Z",
          "wordCount": 512,
          "title": "Lex2vec: making Explainable Word Embeddings via Lexical Resources. (arXiv:2103.02269v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Thi-Vinh Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong-Thai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1\">Thanh-Le Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_K/0/1/0/all/0/1\">Khac-Quy Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>",
          "description": "Prior works have demonstrated that a low-resource language pair can benefit\nfrom multilingual machine translation (MT) systems, which rely on many language\npairs' joint training. This paper proposes two simple strategies to address the\nrare word issue in multilingual MT systems for two low-resource language pairs:\nFrench-Vietnamese and English-Vietnamese. The first strategy is about dynamical\nlearning word similarity of tokens in the shared space among source languages\nwhile another one attempts to augment the translation ability of rare words\nthrough updating their embeddings during the training. Besides, we leverage\nmonolingual data for multilingual MT systems to increase the amount of\nsynthetic parallel corpora while dealing with the data sparsity problem. We\nhave shown significant improvements of up to +1.62 and +2.54 BLEU points over\nthe bilingual baseline systems for both language pairs and released our\ndatasets for the research community.",
          "link": "http://arxiv.org/abs/2012.08743",
          "publishedOn": "2021-07-13T01:59:33.693Z",
          "wordCount": 627,
          "title": "Improving Multilingual Neural Machine Translation For Low-Resource Languages: French,English - Vietnamese. (arXiv:2012.08743v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gamzu_I/0/1/0/all/0/1\">Iftah Gamzu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutiel_G/0/1/0/all/0/1\">Gilad Kutiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Ran Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1\">Eugene Agichtein</a>",
          "description": "In recent years online shopping has gained momentum and became an important\nvenue for customers wishing to save time and simplify their shopping process. A\nkey advantage of shopping online is the ability to read what other customers\nare saying about products of interest. In this work, we aim to maintain this\nadvantage in situations where extreme brevity is needed, for example, when\nshopping by voice. We suggest a novel task of extracting a single\nrepresentative helpful sentence from a set of reviews for a given product. The\nselected sentence should meet two conditions: first, it should be helpful for a\npurchase decision and second, the opinion it expresses should be supported by\nmultiple reviewers. This task is closely related to the task of Multi Document\nSummarization in the product reviews domain but differs in its objective and\nits level of conciseness. We collect a dataset in English of sentence\nhelpfulness scores via crowd-sourcing and demonstrate its reliability despite\nthe inherent subjectivity involved. Next, we describe a complete model that\nextracts representative helpful sentences with positive and negative sentiment\ntowards the product and demonstrate that it outperforms several baselines.",
          "link": "http://arxiv.org/abs/2104.09792",
          "publishedOn": "2021-07-13T01:59:33.642Z",
          "wordCount": 660,
          "title": "Identifying Helpful Sentences in Product Reviews. (arXiv:2104.09792v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>",
          "description": "We introduce a novel embedding model, named NoGE, which aims to integrate\nco-occurrence among entities and relations into graph neural networks to\nimprove knowledge graph completion (i.e., link prediction). Given a knowledge\ngraph, NoGE constructs a single graph considering entities and relations as\nindividual nodes. NoGE then computes weights for edges among nodes based on the\nco-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion\nGraph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector\nrepresentations for entity and relation nodes. NoGE then adopts a score\nfunction to produce the triple scores. Comprehensive experimental results show\nthat NoGE obtains state-of-the-art results on three new and difficult benchmark\ndatasets CoDEx for knowledge graph completion.",
          "link": "http://arxiv.org/abs/2104.07396",
          "publishedOn": "2021-07-13T01:59:33.622Z",
          "wordCount": 593,
          "title": "Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction. (arXiv:2104.07396v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1\">Rafa&#x142; Powalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1\">&#x141;ukasz Borchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1\">Dawid Jurkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1\">Tomasz Dwojak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1\">Micha&#x142; Pietruszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1\">Gabriela Pa&#x142;ka</a>",
          "description": "We address the challenging problem of Natural Language Comprehension beyond\nplain-text documents by introducing the TILT neural network architecture which\nsimultaneously learns layout information, visual features, and textual\nsemantics. Contrary to previous approaches, we rely on a decoder capable of\nunifying a variety of problems involving natural language. The layout is\nrepresented as an attention bias and complemented with contextualized visual\ninformation, while the core of our model is a pretrained encoder-decoder\nTransformer. Our novel approach achieves state-of-the-art results in extracting\ninformation from documents and answering questions which demand layout\nunderstanding (DocVQA, CORD, SROIE). At the same time, we simplify the process\nby employing an end-to-end model.",
          "link": "http://arxiv.org/abs/2102.09550",
          "publishedOn": "2021-07-13T01:59:33.584Z",
          "wordCount": 589,
          "title": "Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. (arXiv:2102.09550v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1\">Nihal Potdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1\">Anderson R. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yiran Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>",
          "description": "End-to-end spoken language understanding (SLU) has recently attracted\nincreasing interest. Compared to the conventional tandem-based approach that\ncombines speech recognition and language understanding as separate modules, the\nnew approach extracts users' intentions directly from the speech signals,\nresulting in joint optimization and low latency. Such an approach, however, is\ntypically designed to process one intention at a time, which leads users to\ntake multiple rounds to fulfill their requirements while interacting with a\ndialogue system. In this paper, we propose a streaming end-to-end framework\nthat can process multiple intentions in an online and incremental way. The\nbackbone of our framework is a unidirectional RNN trained with the\nconnectionist temporal classification (CTC) criterion. By this design, an\nintention can be identified when sufficient evidence has been accumulated, and\nmultiple intentions can be identified sequentially. We evaluate our solution on\nthe Fluent Speech Commands (FSC) dataset and the intent detection accuracy is\nabout 97 % on all multi-intent settings. This result is comparable to the\nperformance of the state-of-the-art non-streaming models, but is achieved in an\nonline and incremental way. We also employ our model to a keyword spotting task\nusing the Google Speech Commands dataset and the results are also highly\npromising.",
          "link": "http://arxiv.org/abs/2105.10042",
          "publishedOn": "2021-07-13T01:59:33.511Z",
          "wordCount": 692,
          "title": "A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_D/0/1/0/all/0/1\">Dhivya Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1\">Vijay Mago</a>",
          "description": "Semantic textual similarity is one of the open research challenges in the\nfield of Natural Language Processing. Extensive research has been carried out\nin this field and near-perfect results are achieved by recent transformer-based\nmodels in existing benchmark datasets like the STS dataset and the SICK\ndataset. In this paper, we study the sentences in these datasets and analyze\nthe sensitivity of various word embeddings with respect to the complexity of\nthe sentences. We build a complex sentences dataset comprising of 50 sentence\npairs with associated semantic similarity values provided by 15 human\nannotators. Readability analysis is performed to highlight the increase in\ncomplexity of the sentences in the existing benchmark datasets and those in the\nproposed dataset. Further, we perform a comparative analysis of the performance\nof various word embeddings and language models on the existing benchmark\ndatasets and the proposed dataset. The results show the increase in complexity\nof the sentences has a significant impact on the performance of the embedding\nmodels resulting in a 10-20% decrease in Pearson's and Spearman's correlation.",
          "link": "http://arxiv.org/abs/2010.12637",
          "publishedOn": "2021-07-13T01:59:33.486Z",
          "wordCount": 661,
          "title": "Comparative analysis of word embeddings in assessing semantic similarity of complex sentences. (arXiv:2010.12637v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaotao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "Due to the excessive cost of large-scale language model pre-training,\nconsiderable efforts have been made to train BERT progressively -- start from\nan inferior but low-cost model and gradually grow the model to increase the\ncomputational complexity. Our objective is to advance the understanding of\nTransformer growth and discover principles that guide progressive training.\nFirst, we find that similar to network architecture search, Transformer growth\nalso favors compound scaling. Specifically, while existing methods only conduct\nnetwork growth in a single dimension, we observe that it is beneficial to use\ncompound growth operators and balance multiple dimensions (e.g., depth, width,\nand input length of the model). Moreover, we explore alternative growth\noperators in each dimension via controlled comparison to give operator\nselection practical guidance. In light of our analyses, the proposed method\nspeeds up BERT pre-training by 73.6% and 82.2% for the base and large models\nrespectively, while achieving comparable performances",
          "link": "http://arxiv.org/abs/2010.12562",
          "publishedOn": "2021-07-13T01:59:33.466Z",
          "wordCount": 632,
          "title": "On the Transformer Growth for Progressive BERT Training. (arXiv:2010.12562v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wood_Doughty_Z/0/1/0/all/0/1\">Zach Wood-Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Paiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>",
          "description": "Computational social science studies often contextualize content analysis\nwithin standard demographics. Since demographics are unavailable on many social\nmedia platforms (e.g. Twitter) numerous studies have inferred demographics\nautomatically. Despite many studies presenting proof of concept inference of\nrace and ethnicity, training of practical systems remains elusive since there\nare few annotated datasets. Existing datasets are small, inaccurate, or fail to\ncover the four most common racial and ethnic groups in the United States. We\npresent a method to identify self-reports of race and ethnicity from Twitter\nprofile descriptions. Despite errors inherent in automated supervision, we\nproduce models with good performance when measured on gold standard self-report\nsurvey data. The result is a reproducible method for creating large-scale\ntraining resources for race and ethnicity.",
          "link": "http://arxiv.org/abs/2005.00635",
          "publishedOn": "2021-07-13T01:59:33.459Z",
          "wordCount": 600,
          "title": "Using Noisy Self-Reports to Predict Twitter User Demographics. (arXiv:2005.00635v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>",
          "description": "We propose a semi-supervised learning method for building end-to-end rich\ntranscription-style automatic speech recognition (RT-ASR) systems from\nsmall-scale rich transcription-style and large-scale common transcription-style\ndatasets. In spontaneous speech tasks, various speech phenomena such as\nfillers, word fragments, laughter and coughs, etc. are often included. While\ncommon transcriptions do not give special awareness to these phenomena, rich\ntranscriptions explicitly convert them into special phenomenon tokens as well\nas textual tokens. In previous studies, the textual and phenomenon tokens were\nsimultaneously estimated in an end-to-end manner. However, it is difficult to\nbuild accurate RT-ASR systems because large-scale rich transcription-style\ndatasets are often unavailable. To solve this problem, our training method uses\na limited rich transcription-style dataset and common transcription-style\ndataset simultaneously. The Key process in our semi-supervised learning is to\nconvert the common transcription-style dataset into a pseudo-rich\ntranscription-style dataset. To this end, we introduce style tokens which\ncontrol phenomenon tokens are generated or not into transformer-based\nautoregressive modeling. We use this modeling for generating the pseudo-rich\ntranscription-style datasets and for building RT-ASR system from the pseudo and\noriginal datasets. Our experiments on spontaneous ASR tasks showed the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.05382",
          "publishedOn": "2021-07-13T01:59:33.442Z",
          "wordCount": 648,
          "title": "End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning. (arXiv:2107.05382v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hromada_D/0/1/0/all/0/1\">Daniel Devatman Hromada</a>",
          "description": "Departing from the postulate that Voynich Manuscript is not a hoax but rather\nencodes authentic contents, our article presents an evolutionary algorithm\nwhich aims to find the most optimal mapping between voynichian glyphs and\ncandidate phonemic values. Core component of the decoding algorithm is a\nprocess of maximization of a fitness function which aims to find most optimal\nset of substitution rules allowing to transcribe the part of the manuscript --\nwhich we call the Calendar -- into lists of feminine names. This leads to sets\nof character subsitution rules which allow us to consistently transcribe dozens\namong three hundred calendar tokens into feminine names: a result far\nsurpassing both ``popular'' as well as \"state of the art\" tentatives to crack\nthe manuscript. What's more, by using name lists stemming from different\nlanguages as potential cribs, our ``adaptive'' method can also be useful in\nidentification of the language in which the manuscript is written.\n\nAs far as we can currently tell, results of our experiments indicate that the\nCalendar part of the manuscript contains names from baltoslavic, balkanic or\nhebrew language strata. Two further indications are also given: primo, highest\nfitness values were obtained when the crib list contains names with specific\ninfixes at token's penultimate position as is the case, for example, for slavic\n\\textbf{feminine diminutives} (i.e. names ending with -ka and not -a). In the\nmost successful scenario, 240 characters contained in 35 distinct Voynichese\ntokens were successfully transcribed. Secundo, in case of crib stemming from\nHebrew language, whole adaptation process converges to significantly better\nfitness values when transcribing voynichian tokens whose order of individual\ncharacters have been reversed, and when lists feminine and not masculine names\nare used as the crib.",
          "link": "http://arxiv.org/abs/2107.05381",
          "publishedOn": "2021-07-13T01:59:33.435Z",
          "wordCount": 730,
          "title": "Can Evolutionary Computation Help us to Crib the Voynich Manuscript ?. (arXiv:2107.05381v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Anish Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rudrajit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1\">Sujay Sanghavi</a>",
          "description": "In this paper we study test time decoding; an ubiquitous step in almost all\nsequential text generation task spanning across a wide array of natural\nlanguage processing (NLP) problems. Our main contribution is to develop a\ncontinuous relaxation framework for the combinatorial NP-hard decoding problem\nand propose Disco - an efficient algorithm based on standard first order\ngradient based. We provide tight analysis and show that our proposed algorithm\nlinearly converges to within $\\epsilon$ neighborhood of the optima. Finally, we\nperform preliminary experiments on the task of adversarial text generation and\nshow superior performance of Disco over several popular decoding approaches.",
          "link": "http://arxiv.org/abs/2107.05380",
          "publishedOn": "2021-07-13T01:59:33.426Z",
          "wordCount": 552,
          "title": "DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation. (arXiv:2107.05380v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1\">Sebastian Monka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1\">Lavdim Halilaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1\">Stefan Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>",
          "description": "Traditional computer vision approaches, based on neural networks (NN), are\ntypically trained on a large amount of image data. By minimizing the\ncross-entropy loss between a prediction and a given class label, the NN and its\nvisual embedding space are learned to fulfill a given task. However, due to the\nsole dependence on the image data distribution of the training domain, these\nmodels tend to fail when applied to a target domain that differs from their\nsource domain. To learn a more robust NN to domain shifts, we propose the\nknowledge graph neural network (KG-NN), a neuro-symbolic approach that\nsupervises the training using image-data-invariant auxiliary knowledge. The\nauxiliary knowledge is first encoded in a knowledge graph with respective\nconcepts and their relationships, which is then transformed into a dense vector\nrepresentation via an embedding method. Using a contrastive loss function,\nKG-NN learns to adapt its visual embedding space and thus its weights according\nto the image-data invariant knowledge graph embedding space. We evaluate KG-NN\non visual transfer learning tasks for classification using the mini-ImageNet\ndataset and its derivatives, as well as road sign recognition datasets from\nGermany and China. The results show that a visual model trained with a\nknowledge graph as a trainer outperforms a model trained with cross-entropy in\nall experiments, in particular when the domain gap increases. Besides better\nperformance and stronger robustness to domain shifts, these KG-NN adapts to\nmultiple datasets and classes without suffering heavily from catastrophic\nforgetting.",
          "link": "http://arxiv.org/abs/2102.08747",
          "publishedOn": "2021-07-13T01:59:33.414Z",
          "wordCount": 728,
          "title": "Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.07019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhijie Sasha Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christenson_L/0/1/0/all/0/1\">Lauren Christenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulton_L/0/1/0/all/0/1\">Lawrence Fulton</a>",
          "description": "Social media has become an essential channel for posting disaster-related\ninformation, which provide governments and relief agencies real-time data for\nbetter disaster management. However, research in this field has not received\nsufficient attention and extracting useful information is still challenging.\nThis paper aims to improve disaster relief efficiency via mining and analyzing\nsocial media data like public attitudes towards disaster response and public\ndemands for targeted relief supplies during different types of disasters. We\nfocus on different natural disasters based on properties such as types,\ndurations, and damages, which contains a total of 41,993 tweets. In this paper,\npublic perception is assessed qualitatively by manually classified tweets,\nwhich contain information like the demand for targeted relief supplies,\nsatisfactions of disaster response, and public fear. Public attitudes to\nnatural disasters are studied via a quantitative analysis using eight machine\nlearning models. To better provide decision-makers with the appropriate model,\nthe comparison of machine learning models based on computational time and\nprediction accuracy is conducted. The change of public opinion during different\nnatural disasters and the evolution of people's behavior of using social media\nfor disaster relief in the face of the identical type of natural disasters as\nTwitter continues to evolve are studied. The results in this paper demonstrate\nthe feasibility and validation of the proposed research approach and provide\nrelief agencies with insights into better disaster management.",
          "link": "http://arxiv.org/abs/2005.07019",
          "publishedOn": "2021-07-13T01:59:33.407Z",
          "wordCount": 737,
          "title": "Social Media Information Sharing for Natural Disaster Response. (arXiv:2005.07019v5 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1\">Yiming Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qingyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>",
          "description": "The fifth Oriental Language Recognition (OLR) Challenge focuses on language\nrecognition in a variety of complex environments to promote its development.\nThe OLR 2020 Challenge includes three tasks: (1) cross-channel language\nidentification, (2) dialect identification, and (3) noisy language\nidentification. We choose Cavg as the principle evaluation metric, and the\nEqual Error Rate (EER) as the secondary metric. There were 58 teams\nparticipating in this challenge and one third of the teams submitted valid\nresults. Compared with the best baseline, the Cavg values of Top 1 system for\nthe three tasks were relatively reduced by 82%, 62% and 48%, respectively. This\npaper describes the three tasks, the database profile, and the final results.\nWe also outline the novel approaches that improve the performance of language\nrecognition systems most significantly, such as the utilization of auxiliary\ninformation.",
          "link": "http://arxiv.org/abs/2107.05365",
          "publishedOn": "2021-07-13T01:59:33.383Z",
          "wordCount": 575,
          "title": "Oriental Language Recognition (OLR) 2020: Summary and Analysis. (arXiv:2107.05365v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Che Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>",
          "description": "There are two major classes of natural language grammar -- the dependency\ngrammar that models one-to-one correspondences between words and the\nconstituency grammar that models the assembly of one or several corresponded\nwords. While previous unsupervised parsing methods mostly focus on only\ninducing one class of grammars, we introduce a novel model, StructFormer, that\ncan simultaneously induce dependency and constituency structure. To achieve\nthis, we propose a new parsing framework that can jointly generate a\nconstituency tree and dependency graph. Then we integrate the induced\ndependency relations into the transformer, in a differentiable manner, through\na novel dependency-constrained self-attention mechanism. Experimental results\nshow that our model can achieve strong results on unsupervised constituency\nparsing, unsupervised dependency parsing, and masked language modeling at the\nsame time.",
          "link": "http://arxiv.org/abs/2012.00857",
          "publishedOn": "2021-07-13T01:59:33.377Z",
          "wordCount": 626,
          "title": "StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling. (arXiv:2012.00857v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1\">Kenneth Enevoldsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielbo_K/0/1/0/all/0/1\">Kristoffer Nielbo</a>",
          "description": "Danish natural language processing (NLP) has in recent years obtained\nconsiderable improvements with the addition of multiple new datasets and\nmodels. However, at present, there is no coherent framework for applying\nstate-of-the-art models for Danish. We present DaCy: a unified framework for\nDanish NLP built on SpaCy. DaCy uses efficient multitask models which obtain\nstate-of-the-art performance on named entity recognition, part-of-speech\ntagging, and dependency parsing. DaCy contains tools for easy integration of\nexisting models such as for polarity, emotion, or subjectivity detection. In\naddition, we conduct a series of tests for biases and robustness of Danish NLP\npipelines through augmentation of the test set of DaNE. DaCy large compares\nfavorably and is especially robust to long input lengths and spelling\nvariations and errors. All models except DaCy large display significant biases\nrelated to ethnicity while only Polyglot shows a significant gender bias. We\nargue that for languages with limited benchmark sets, data augmentation can be\nparticularly useful for obtaining more realistic and fine-grained performance\nestimates. We provide a series of augmenters as a first step towards a more\nthorough evaluation of language models for low and medium resource languages\nand encourage further development.",
          "link": "http://arxiv.org/abs/2107.05295",
          "publishedOn": "2021-07-13T01:59:33.354Z",
          "wordCount": 632,
          "title": "DaCy: A Unified Framework for Danish NLP. (arXiv:2107.05295v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianwen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianwei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shenghuan He</a>",
          "description": "In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.",
          "link": "http://arxiv.org/abs/2107.05377",
          "publishedOn": "2021-07-13T01:59:33.347Z",
          "wordCount": 580,
          "title": "A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengsen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jinqiao Dai</a>",
          "description": "Controlling the model to generate texts of different categories is a\nchallenging task that is getting more and more attention. Recently, generative\nadversarial net (GAN) has shown promising results in category text generation.\nHowever, the texts generated by GANs usually suffer from the problems of mode\ncollapse and training instability. To avoid the above problems, we propose a\nnovel model named category-aware variational recurrent neural network\n(CatVRNN), which is inspired by multi-task learning. In our model, generation\nand classification are trained simultaneously, aiming at generating texts of\ndifferent categories. Moreover, the use of multi-task learning can improve the\nquality of generated texts, when the classification task is appropriate. And we\npropose a function to initialize the hidden state of CatVRNN to force model to\ngenerate texts of a specific category. Experimental results on three datasets\ndemonstrate that our model can do better than several state-of-the-art text\ngeneration methods based GAN in the category accuracy and quality of generated\ntexts.",
          "link": "http://arxiv.org/abs/2107.05219",
          "publishedOn": "2021-07-13T01:59:33.333Z",
          "wordCount": 586,
          "title": "CatVRNN: Generating Category Texts via Multi-task Learning. (arXiv:2107.05219v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duzha_A/0/1/0/all/0/1\">Armend Duzha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casadei_C/0/1/0/all/0/1\">Cristiano Casadei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosi_M/0/1/0/all/0/1\">Michael Tosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celli_F/0/1/0/all/0/1\">Fabio Celli</a>",
          "description": "Accurate detection of hate speech against politicians, policy making and\npolitical ideas is crucial to maintain democracy and free speech.\nUnfortunately, the amount of labelled data necessary for training models to\ndetect hate speech are limited and domain-dependent. In this paper, we address\nthe issue of classification of hate speech against policy makers from Twitter\nin Italian, producing the first resource of this type in this language. We\ncollected and annotated 1264 tweets, examined the cases of disagreements\nbetween annotators, and performed in-domain and cross-domain hate speech\nclassifications with different features and algorithms. We achieved a\nperformance of ROC AUC 0.83 and analyzed the most predictive attributes, also\nfinding the different language features in the anti-policymakers and\nanti-immigration domains. Finally, we visualized networks of hashtags to\ncapture the topics used in hateful and normal tweets.",
          "link": "http://arxiv.org/abs/2107.05357",
          "publishedOn": "2021-07-13T01:59:33.314Z",
          "wordCount": 595,
          "title": "Hate versus Politics: Detection of Hate against Policy makers in Italian tweets. (arXiv:2107.05357v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuqing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin I. P. Rubinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>",
          "description": "Neural machine translation systems are known to be vulnerable to adversarial\ntest inputs, however, as we show in this paper, these systems are also\nvulnerable to training attacks. Specifically, we propose a poisoning attack in\nwhich a malicious adversary inserts a small poisoned sample of monolingual text\ninto the training set of a system trained using back-translation. This sample\nis designed to induce a specific, targeted translation behaviour, such as\npeddling misinformation. We present two methods for crafting poisoned examples,\nand show that only a tiny handful of instances, amounting to only 0.02% of the\ntraining set, is sufficient to enact a successful attack. We outline a defence\nmethod against said attacks, which partly ameliorates the problem. However, we\nstress that this is a blind-spot in modern NMT, demanding immediate attention.",
          "link": "http://arxiv.org/abs/2107.05243",
          "publishedOn": "2021-07-13T01:59:33.307Z",
          "wordCount": 600,
          "title": "Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning. (arXiv:2107.05243v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">E. Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">C. M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mieder_W/0/1/0/all/0/1\">W. Mieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">P. S. Dodds</a>",
          "description": "Proverbs are an essential component of language and culture, and though much\nattention has been paid to their history and currency, there has been\ncomparatively little quantitative work on changes in the frequency with which\nthey are used over time. With wider availability of large corpora reflecting\nmany diverse genres of documents, it is now possible to take a broad and\ndynamic view of the importance of the proverb. Here, we measure temporal\nchanges in the relevance of proverbs within three corpora, differing in kind,\nscale, and time frame: Millions of books over centuries; hundreds of millions\nof news articles over twenty years; and billions of tweets over a decade. We\nfind that proverbs present heavy-tailed frequency-of-usage rank distributions\nin each venue; exhibit trends reflecting the cultural dynamics of the eras\ncovered; and have evolved into contemporary forms on social media.",
          "link": "http://arxiv.org/abs/2107.04929",
          "publishedOn": "2021-07-13T01:59:33.301Z",
          "wordCount": 608,
          "title": "Computational Paremiology: Charting the temporal, ecological dynamics of proverb use in books, news articles, and tweets. (arXiv:2107.04929v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Luyao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yating Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>",
          "description": "Legal judgment prediction(LJP) is an essential task for legal AI. While prior\nmethods studied on this topic in a pseudo setting by employing the\njudge-summarized case narrative as the input to predict the judgment,\nneglecting critical case life-cycle information in real court setting could\nthreaten the case logic representation quality and prediction correctness. In\nthis paper, we introduce a novel challenging dataset from real courtrooms to\npredict the legal judgment in a reasonably encyclopedic manner by leveraging\nthe genuine input of the case -- plaintiff's claims and court debate data, from\nwhich the case's facts are automatically recognized by comprehensively\nunderstanding the multi-role dialogues of the court debate, and then learnt to\ndiscriminate the claims so as to reach the final judgment through multi-task\nlearning. An extensive set of experiments with a large civil trial data set\nshows that the proposed model can more accurately characterize the interactions\namong claims, fact and debate for legal judgment prediction, achieving\nsignificant improvements over strong state-of-the-art baselines. Moreover, the\nuser study conducted with real judges and law school students shows the neural\npredictions can also be interpretable and easily observed, and thus enhancing\nthe trial efficiency and judgment quality.",
          "link": "http://arxiv.org/abs/2107.05192",
          "publishedOn": "2021-07-13T01:59:33.292Z",
          "wordCount": 648,
          "title": "Legal Judgment Prediction with Multi-Stage CaseRepresentation Learning in the Real Court Setting. (arXiv:2107.05192v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangyue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Y. Chai</a>",
          "description": "In this paper, we study the problem of recognizing compositional\nattribute-object concepts within the zero-shot learning (ZSL) framework. We\npropose an episode-based cross-attention (EpiCA) network which combines merits\nof cross-attention mechanism and episode-based training strategy to recognize\nnovel compositional concepts. Firstly, EpiCA bases on cross-attention to\ncorrelate concept-visual information and utilizes the gated pooling layer to\nbuild contextualized representations for both images and concepts. The updated\nrepresentations are used for a more in-depth multi-modal relevance calculation\nfor concept recognition. Secondly, a two-phase episode training strategy,\nespecially the transductive phase, is adopted to utilize unlabeled test\nexamples to alleviate the low-resource learning problem. Experiments on two\nwidely-used zero-shot compositional learning (ZSCL) benchmarks have\ndemonstrated the effectiveness of the model compared with recent approaches on\nboth conventional and generalized ZSCL settings.",
          "link": "http://arxiv.org/abs/2107.05176",
          "publishedOn": "2021-07-13T01:59:33.279Z",
          "wordCount": 560,
          "title": "Zero-Shot Compositional Concept Learning. (arXiv:2107.05176v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chengrui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1\">Keyu An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huahuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>",
          "description": "The use of phonological features (PFs) potentially allows language-specific\nphones to remain linked in training, which is highly desirable for information\nsharing for multilingual and crosslingual speech recognition methods for\nlow-resourced languages. A drawback suffered by previous methods in using\nphonological features is that the acoustic-to-PF extraction in a bottom-up way\nis itself difficult. In this paper, we propose to join phonology driven phone\nembedding (top-down) and deep neural network (DNN) based acoustic feature\nextraction (bottom-up) to calculate phone probabilities. The new method is\ncalled JoinAP (Joining of Acoustics and Phonology). Remarkably, no inversion\nfrom acoustics to phonological features is required for speech recognition. For\neach phone in the IPA (International Phonetic Alphabet) table, we encode its\nphonological features to a phonological-vector, and then apply linear or\nnonlinear transformation of the phonological-vector to obtain the phone\nembedding. A series of multilingual and crosslingual (both zero-shot and\nfew-shot) speech recognition experiments are conducted on the CommonVoice\ndataset (German, French, Spanish and Italian) and the AISHLL-1 dataset\n(Mandarin), and demonstrate the superiority of JoinAP with nonlinear phone\nembeddings over both JoinAP with linear phone embeddings and the traditional\nmethod with flat phone embeddings.",
          "link": "http://arxiv.org/abs/2107.05038",
          "publishedOn": "2021-07-13T01:59:33.262Z",
          "wordCount": 636,
          "title": "Multilingual and crosslingual speech recognition using phonological-vector based phone embeddings. (arXiv:2107.05038v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1\">Seethalakshmi Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Victor Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_Powell_G/0/1/0/all/0/1\">Gus Hahn-Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirunagar_B/0/1/0/all/0/1\">Bharadwaj Tirunagar</a>",
          "description": "The number of research articles in business and management has dramatically\nincreased along with terminology, constructs, and measures. Proper\nclassification of organizational performance constructs from research articles\nplays an important role in categorizing the literature and understanding to\nwhom its research implications may be relevant. In this work, we classify\nconstructs (i.e., concepts and terminology used to capture different aspects of\norganizational performance) in research articles into a three-level\ncategorization: (a) performance and non-performance categories (Level 0); (b)\nfor performance constructs, stakeholder group-level of performance concerning\ninvestors, customers, employees, and the society (community and natural\nenvironment) (Level 1); and (c) for each stakeholder group-level, subcategories\nof different ways of measurement (Level 2). We observed that increasing\ncontextual information with features extracted from surrounding sentences and\nexternal references improves classification of disaggregate-level labels, given\nlimited training data. Our research has implications for computer-assisted\nconstruct identification and classification - an essential step for research\nsynthesis.",
          "link": "http://arxiv.org/abs/2107.05133",
          "publishedOn": "2021-07-13T01:59:33.252Z",
          "wordCount": 589,
          "title": "Computer-assisted construct classification of organizational performance concerning different stakeholder groups. (arXiv:2107.05133v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haipang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>",
          "description": "Most recently proposed approaches in dialogue state tracking (DST) leverage\nthe context and the last dialogue states to track current dialogue states,\nwhich are often slot-value pairs. Although the context contains the complete\ndialogue information, the information is usually indirect and even requires\nreasoning to obtain. The information in the lastly predicted dialogue states is\ndirect, but when there is a prediction error, the dialogue information from\nthis source will be incomplete or erroneous. In this paper, we propose the\nDialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States\nand Conversations network (FPDSC). This model extracts information of each\ndialogue turn by modeling interactions among each turn utterance, the\ncorresponding last dialogue states, and dialogue slots. Then the representation\nof each dialogue turn is aggregated by a hierarchical structure to form the\npassage information, which is utilized in the current turn of DST. Experimental\nresults validate the effectiveness of the fusion network with 55.03% and 59.07%\njoint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets, which reaches the\nstate-of-the-art performance. Furthermore, we conduct the deleted-value and\nrelated-slot experiments on MultiWOZ 2.1 to evaluate our model.",
          "link": "http://arxiv.org/abs/2107.05168",
          "publishedOn": "2021-07-13T01:59:33.246Z",
          "wordCount": 633,
          "title": "Dialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States and Conversations. (arXiv:2107.05168v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gaochen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu1_B/0/1/0/all/0/1\">Bin Xu1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bangchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongwen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dejie Chang</a>",
          "description": "Extractive Reading Comprehension (ERC) has made tremendous advances enabled\nby the availability of large-scale high-quality ERC training data. Despite of\nsuch rapid progress and widespread application, the datasets in languages other\nthan high-resource languages such as English remain scarce. To address this\nissue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by\nmodelling existing high-quality extractive reading comprehension datasets in a\nmultilingual environment. To be specific, we present multilingual adaptive\nattention (MAA) to combine intra-attention and inter-attention to learn more\ngeneral generalizable semantic and lexical knowledge from each pair of language\nfamilies. Furthermore, to make full use of existing datasets, we adopt a new\ntraining framework to train our model by calculating task-level similarities\nbetween each existing dataset and target dataset. The experimental results show\nthat our XLTT model surpasses six baselines on two multilingual ERC benchmarks,\nespecially more effective for low-resource languages with 3.9 and 4.1 average\nimprovement in F1 and EM, respectively.",
          "link": "http://arxiv.org/abs/2107.05002",
          "publishedOn": "2021-07-13T01:59:33.238Z",
          "wordCount": 594,
          "title": "Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking. (arXiv:2107.05002v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1\">Bryar A. Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1\">Tarik A. Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirjalili_S/0/1/0/all/0/1\">Seyedali Mirjalili</a>",
          "description": "It is beneficial to automate the process of deriving concept hierarchies from\ncorpora since a manual construction of concept hierarchies is typically a\ntime-consuming and resource-intensive process. As such, the overall process of\nlearning concept hierarchies from corpora encompasses a set of steps: parsing\nthe text into sentences, splitting the sentences and then tokenising it. After\nthe lemmatisation step, the pairs are extracted using FCA. However, there might\nbe some uninteresting and erroneous pairs in the formal context. Generating\nformal context may lead to a time-consuming process, so formal context size\nreduction is required to remove uninterested and erroneous pairs, taking less\ntime to extract the concept lattice and concept hierarchies accordingly. In\nthis premise, this study aims to propose two frameworks: (1) A framework to\nreview the current process of deriving concept hierarchies from corpus\nutilising FCA; (2) A framework to decrease the formal contexts ambiguity of the\nfirst framework using an adaptive version of ECA*. Experiments are conducted by\napplying 385 sample corpora from Wikipedia on the two frameworks to examine the\nreducing size of formal context, which leads to yield concept lattice and\nconcept hierarchy. The resulting lattice of formal context is evaluated to the\nstandard one using concept lattice-invariants. Accordingly, the homomorphic\nbetween the two lattices preserves the quality of resulting concept hierarchies\nby 89% in contrast to the basic ones, and the reduced concept lattice inherits\nthe structural relation of the standard one. The adaptive ECA* is examined\nagainst its four counterpart baseline algorithms to measure the execution time\non random datasets with different densities (fill ratios). The results show\nthat adaptive ECA* performs concept lattice faster than other mentioned\ncompetitive techniques in different fill ratios.",
          "link": "http://arxiv.org/abs/2107.04781",
          "publishedOn": "2021-07-13T01:59:33.228Z",
          "wordCount": 736,
          "title": "Formal context reduction in deriving concept hierarchies from corpora using adaptive evolutionary clustering algorithm star. (arXiv:2107.04781v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Ju-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>",
          "description": "Recently proposed self-supervised learning approaches have been successful\nfor pre-training speech representation models. The utility of these learned\nrepresentations has been observed empirically, but not much has been studied\nabout the type or extent of information encoded in the pre-trained\nrepresentations themselves. Developing such insights can help understand the\ncapabilities and limits of these models and enable the research community to\nmore efficiently develop their usage for downstream applications. In this work,\nwe begin to fill this gap by examining one recent and successful pre-trained\nmodel (wav2vec 2.0), via its intermediate representation vectors, using a suite\nof analysis tools. We use the metrics of canonical correlation, mutual\ninformation, and performance on simple downstream tasks with non-parametric\nprobes, in order to (i) query for acoustic and linguistic information content,\n(ii) characterize the evolution of information across model layers, and (iii)\nunderstand how fine-tuning the model for automatic speech recognition (ASR)\naffects these observations. Our findings motivate modifying the fine-tuning\nprotocol for ASR, which produces improved word error rates in a low-resource\nsetting.",
          "link": "http://arxiv.org/abs/2107.04734",
          "publishedOn": "2021-07-13T01:59:33.186Z",
          "wordCount": 612,
          "title": "Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gaochen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bangchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongwen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dejie Chang</a>",
          "description": "Although there are a small number of work to conduct patent research by\nbuilding knowledge graph, but without constructing patent knowledge graph using\npatent documents and combining latest natural language processing methods to\nmine hidden rich semantic relationships in existing patents and predict new\npossible patents. In this paper, we propose a new patent vacancy prediction\napproach named PatentMiner to mine rich semantic knowledge and predict new\npotential patents based on knowledge graph (KG) and graph attention mechanism.\nFirstly, patent knowledge graph over time (e.g. year) is constructed by\ncarrying out named entity recognition and relation extrac-tion from patent\ndocuments. Secondly, Common Neighbor Method (CNM), Graph Attention Networks\n(GAT) and Context-enhanced Graph Attention Networks (CGAT) are proposed to\nperform link prediction in the constructed knowledge graph to dig out the\npotential triples. Finally, patents are defined on the knowledge graph by means\nof co-occurrence relationship, that is, each patent is represented as a fully\nconnected subgraph containing all its entities and co-occurrence relationships\nof the patent in the knowledge graph; Furthermore, we propose a new patent\nprediction task which predicts a fully connected subgraph with newly added\nprediction links as a new pa-tent. The experimental results demonstrate that\nour proposed patent predic-tion approach can correctly predict new patents and\nContext-enhanced Graph Attention Networks is much better than the baseline.\nMeanwhile, our proposed patent vacancy prediction task still has significant\nroom to im-prove.",
          "link": "http://arxiv.org/abs/2107.04880",
          "publishedOn": "2021-07-13T01:59:33.177Z",
          "wordCount": 672,
          "title": "PatentMiner: Patent Vacancy Mining via Context-enhanced and Knowledge-guided Graph Attention. (arXiv:2107.04880v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J.F. Gales</a>",
          "description": "Text-based machine comprehension (MC) systems have a wide-range of\napplications, and standard corpora exist for developing and evaluating\napproaches. There has been far less research on spoken question answering (SQA)\nsystems. The SQA task considered in this paper is to extract the answer from a\ncandidate$\\text{'}$s spoken response to a question in a prompt-response style\nlanguage assessment test. Applying these MC approaches to this SQA task rather\nthan, for example, off-topic response detection provides far more detailed\ninformation that can be used for further downstream processing. One significant\nchallenge is the lack of appropriately annotated speech corpora to train\nsystems for this task. Hence, a transfer-learning style approach is adopted\nwhere a system trained on text-based MC is evaluated on an SQA task with\nnon-native speakers. Mismatches must be considered between text documents and\nspoken responses; non-native spoken grammar and written grammar. In practical\nSQA, ASR systems are used, necessitating an investigation of the impact of ASR\nerrors. We show that a simple text-based ELECTRA MC model trained on SQuAD2.0\ntransfers well for SQA. It is found that there is an approximately linear\nrelationship between ASR errors and the SQA assessment scores but grammar\nmismatches have minimal impact.",
          "link": "http://arxiv.org/abs/2107.04691",
          "publishedOn": "2021-07-13T01:59:33.140Z",
          "wordCount": 629,
          "title": "An Initial Investigation of Non-Native Spoken Question-Answering. (arXiv:2107.04691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1\">Pierce Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiatong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>",
          "description": "Automatic speech recognition (ASR) has become increasingly ubiquitous on\nmodern edge devices. Past work developed streaming End-to-End (E2E) all-neural\nspeech recognizers that can run compactly on edge devices. However, E2E ASR\nmodels are prone to overfitting and have difficulties in generalizing to unseen\ntesting data. Various techniques have been proposed to regularize the training\nof ASR models, including layer normalization, dropout, spectrum data\naugmentation and speed distortions in the inputs. In this work, we present a\nsimple yet effective noisy training strategy to further improve the E2E ASR\nmodel training. By introducing random noise to the parameter space during\ntraining, our method can produce smoother models at convergence that generalize\nbetter. We apply noisy training to improve both dense and sparse\nstate-of-the-art Emformer models and observe consistent WER reduction.\nSpecifically, when training Emformers with 90% sparsity, we achieve 12% and 14%\nWER improvements on the LibriSpeech Test-other and Test-clean data set,\nrespectively.",
          "link": "http://arxiv.org/abs/2107.04677",
          "publishedOn": "2021-07-13T01:59:33.110Z",
          "wordCount": 593,
          "title": "Noisy Training Improves E2E ASR for the Edge. (arXiv:2107.04677v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shrey Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rill_J/0/1/0/all/0/1\">Justin Rill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_B/0/1/0/all/0/1\">Brian Moran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_S/0/1/0/all/0/1\">Safiyyah Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zotov_A/0/1/0/all/0/1\">Alexander Zotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1\">Ahmed Aly</a>",
          "description": "Data efficiency, despite being an attractive characteristic, is often\nchallenging to measure and optimize for in task-oriented semantic parsing;\nunlike exact match, it can require both model- and domain-specific setups,\nwhich have, historically, varied widely across experiments. In our work, as a\nstep towards providing a unified solution to data-efficiency-related questions,\nwe introduce a four-stage protocol which gives an approximate measure of how\nmuch in-domain, \"target\" data a parser requires to achieve a certain quality\nbar. Specifically, our protocol consists of (1) sampling target subsets of\ndifferent cardinalities, (2) fine-tuning parsers on each subset, (3) obtaining\na smooth curve relating target subset (%) vs. exact match (%), and (4)\nreferencing the curve to mine ad-hoc (target subset, exact match) points. We\napply our protocol in two real-world case studies -- model generalizability and\nintent complexity -- illustrating its flexibility and applicability to\npractitioners in task-oriented semantic parsing.",
          "link": "http://arxiv.org/abs/2107.04736",
          "publishedOn": "2021-07-13T01:59:33.084Z",
          "wordCount": 583,
          "title": "Assessing Data Efficiency in Task-Oriented Semantic Parsing. (arXiv:2107.04736v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1\">Hang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Fine-tuning pre-trained language models such as BERT has become a common\npractice dominating leaderboards across various NLP tasks. Despite its recent\nsuccess and wide adoption, this process is unstable when there are only a small\nnumber of training samples available. The brittleness of this process is often\nreflected by the sensitivity to random seeds. In this paper, we propose to\ntackle this problem based on the noise stability property of deep nets, which\nis investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020).\nSpecifically, we introduce a novel and effective regularization method to\nimprove fine-tuning on NLP tasks, referred to as Layer-wise Noise Stability\nRegularization (LNSR). We extend the theories about adding noise to the input\nand prove that our method gives a stabler regularization effect. We provide\nsupportive evidence by experimentally confirming that well-performing models\nshow a low sensitivity to noise and fine-tuning with LNSR exhibits clearly\nhigher generalizability and stability. Furthermore, our method also\ndemonstrates advantages over other state-of-the-art algorithms including L2-SP\n(Li et al., 2018), Mixout (Lee et al., 2020) and SMART (Jiang et al., 2020).",
          "link": "http://arxiv.org/abs/2107.04835",
          "publishedOn": "2021-07-13T01:59:33.076Z",
          "wordCount": 633,
          "title": "Noise Stability Regularization for Improving BERT Fine-tuning. (arXiv:2107.04835v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-19T00:49:05.316Z",
          "wordCount": 620,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1\">Ciro Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_J/0/1/0/all/0/1\">Jean-Francis Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bingqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1\">Patrick John Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassani_G/0/1/0/all/0/1\">Giovanni Cassani</a>",
          "description": "The 2021 SIGIR workshop on eCommerce is hosting the Coveo Data Challenge for\n\"In-session prediction for purchase intent and recommendations\". The challenge\naddresses the growing need for reliable predictions within the boundaries of a\nshopping session, as customer intentions can be different depending on the\noccasion. The need for efficient procedures for personalization is even clearer\nif we consider the e-commerce landscape more broadly: outside of giant digital\nretailers, the constraints of the problem are stricter, due to smaller user\nbases and the realization that most users are not frequently returning\ncustomers. We release a new session-based dataset including more than 30M\nfine-grained browsing events (product detail, add, purchase), enriched by\nlinguistic behavior (queries made by shoppers, with items clicked and items not\nclicked after the query) and catalog meta-data (images, text, pricing\ninformation). On this dataset, we ask participants to showcase innovative\nsolutions for two open problems: a recommendation task (where a model is shown\nsome events at the start of a session, and it is asked to predict future\nproduct interactions); an intent prediction task, where a model is shown a\nsession containing an add-to-cart event, and it is asked to predict whether the\nitem will be bought before the end of the session.",
          "link": "http://arxiv.org/abs/2104.09423",
          "publishedOn": "2021-07-19T00:49:05.195Z",
          "wordCount": 695,
          "title": "SIGIR 2021 E-Commerce Workshop Data Challenge. (arXiv:2104.09423v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arpita Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_D/0/1/0/all/0/1\">Debasis Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarma_M/0/1/0/all/0/1\">Monalisa Sarma</a>",
          "description": "User intention which often changes dynamically is considered to be an\nimportant factor for modeling users in the design of recommendation systems.\nRecent studies are starting to focus on predicting user intention (what users\nwant) beyond user preference (what users like). In this work, a user intention\nmodel is proposed based on deep sequential topic analysis. The model predicts a\nuser's intention in terms of the topic of interest. The Hybrid Topic Model\n(HTM) comprising Latent Dirichlet Allocation (LDA) and Word2Vec is proposed to\nderive the topic of interest of users and the history of preferences. HTM finds\nthe true topics of papers estimating word-topic distribution which includes\nsyntactic and semantic correlations among words. Next, to model user intention,\na Long Short Term Memory (LSTM) based sequential deep learning model is\nproposed. This model takes into account temporal context, namely the time\ndifference between clicks of two consecutive papers seen by a user. Extensive\nexperiments with the real-world research paper dataset indicate that the\nproposed approach significantly outperforms the state-of-the-art methods.\nFurther, the proposed approach introduces a new road map to model a user\nactivity suitable for the design of a research paper recommendation system.",
          "link": "http://arxiv.org/abs/2107.07831",
          "publishedOn": "2021-07-19T00:49:05.156Z",
          "wordCount": 630,
          "title": "Modeling User Behaviour in Research Paper Recommendation System. (arXiv:2107.07831v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Shivani Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luthra_T/0/1/0/all/0/1\">Tarun Luthra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ashima Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajat Singh</a>",
          "description": "Knowledge Graph embedding provides a versatile technique for representing\nknowledge. These techniques can be used in a variety of applications such as\ncompletion of knowledge graph to predict missing information, recommender\nsystems, question answering, query expansion, etc. The information embedded in\nKnowledge graph though being structured is challenging to consume in a\nreal-world application. Knowledge graph embedding enables the real-world\napplication to consume information to improve performance. Knowledge graph\nembedding is an active research area. Most of the embedding methods focus on\nstructure-based information. Recent research has extended the boundary to\ninclude text-based information and image-based information in entity embedding.\nEfforts have been made to enhance the representation with context information.\nThis paper introduces growth in the field of KG embedding from simple\ntranslation-based models to enrichment-based models. This paper includes the\nutility of the Knowledge graph in real-world applications.",
          "link": "http://arxiv.org/abs/2107.07842",
          "publishedOn": "2021-07-19T00:49:05.141Z",
          "wordCount": 580,
          "title": "A Survey of Knowledge Graph Embedding and Their Applications. (arXiv:2107.07842v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1\">Elona Shatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.",
          "link": "http://arxiv.org/abs/2107.07786",
          "publishedOn": "2021-07-19T00:49:05.131Z",
          "wordCount": 651,
          "title": "DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qin Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
          "link": "http://arxiv.org/abs/2107.07705",
          "publishedOn": "2021-07-19T00:49:04.905Z",
          "wordCount": 491,
          "title": "Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>",
          "description": "Dense retrieval conducts text retrieval in the embedding space and has shown\nmany advantages compared to sparse retrieval. Existing dense retrievers\noptimize representations of queries and documents with contrastive training and\nmap them to the embedding space. The embedding space is optimized by aligning\nthe matched query-document pairs and pushing the negative documents away from\nthe query. However, in such training paradigm, the queries are only optimized\nto align to the documents and are coarsely positioned, leading to an\nanisotropic query embedding space. In this paper, we analyze the embedding\nspace distributions and propose an effective training paradigm, Contrastive\nDual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained\nquery representations for dense retrieval. DANCE incorporates an additional\ndual training object of query retrieval, inspired by the classic information\nretrieval training axiom, query likelihood. With contrastive learning, the dual\ntraining object of DANCE learns more tailored representations for queries and\ndocuments to keep the embedding space smooth and uniform, thriving on the\nranking performance of DANCE on the MS MARCO document retrieval task. Different\nfrom ANCE that only optimized with the document retrieval task, DANCE\nconcentrates the query embeddings closer to document representations while\nmaking the document distribution more discriminative. Such concentrated query\nembedding distribution assigns more uniform negative sampling probabilities to\nqueries and helps to sufficiently optimize query representations in the query\nretrieval task. Our codes are released at https://github.com/thunlp/DANCE.",
          "link": "http://arxiv.org/abs/2107.07773",
          "publishedOn": "2021-07-19T00:49:04.885Z",
          "wordCount": 669,
          "title": "More Robust Dense Retrieval with Contrastive Dual Learning. (arXiv:2107.07773v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiayi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for\ncontent-based micro-video background music recommendation. CMVAE is a\nhierarchical Bayesian generative model that matches relevant background music\nto a micro-video by projecting these two multimodal inputs into a shared\nlow-dimensional latent space, where the alignment of two corresponding\nembeddings of a matched video-music pair is achieved by cross-generation.\nMoreover, the multimodal information is fused by the product-of-experts (PoE)\nprinciple, where the semantic information in visual and textual modalities of\nthe micro-video are weighted according to their variance estimations such that\nthe modality with a lower noise level is given more weights. Therefore, the\nmicro-video latent variables contain less irrelevant information that results\nin a more robust model generalization. Furthermore, we establish a large-scale\ncontent-based micro-video background music recommendation dataset, TT-150k,\ncomposed of approximately 3,000 different background music clips associated to\n150,000 micro-videos from different users. Extensive experiments on the\nestablished TT-150k dataset demonstrate the effectiveness of the proposed\nmethod. A qualitative assessment of CMVAE by visualizing some recommendation\nresults is also included.",
          "link": "http://arxiv.org/abs/2107.07268",
          "publishedOn": "2021-07-16T00:48:22.212Z",
          "wordCount": 606,
          "title": "Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1\">Sudhanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With the advancement in the technology sector spanning over every field, a\nhuge influx of information is inevitable. Among all the opportunities that the\nadvancements in the technology have brought, one of them is to propose\nefficient solutions for data retrieval. This means that from an enormous pile\nof data, the retrieval methods should allow the users to fetch the relevant and\nrecent data over time. In the field of entertainment and e-commerce,\nrecommender systems have been functioning to provide the aforementioned.\nEmploying the same systems in the medical domain could definitely prove to be\nuseful in variety of ways. Following this context, the goal of this paper is to\npropose collaborative filtering based recommender system in the healthcare\nsector to recommend remedies based on the symptoms experienced by the patients.\nFurthermore, a new dataset is developed consisting of remedies concerning\nvarious diseases to address the limited availability of the data. The proposed\nrecommender system accepts the prognostic markers of a patient as the input and\ngenerates the best remedy course. With several experimental trials, the\nproposed model achieved promising results in recommending the possible remedy\nfor given prognostic markers.",
          "link": "http://arxiv.org/abs/2107.07500",
          "publishedOn": "2021-07-16T00:48:22.196Z",
          "wordCount": 640,
          "title": "Recommending best course of treatment based on similarities of prognostic markers\\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_C/0/1/0/all/0/1\">Chintoo Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdary_C/0/1/0/all/0/1\">C. Ravindranath Chowdary</a>",
          "description": "In general, recommender systems are designed to provide personalized items to\na user. But in few cases, items are recommended for a group, and the challenge\nis to aggregate the individual user preferences to infer the recommendation to\na group. It is also important to consider the similarity of characteristics\namong the members of a group to generate a better recommendation. Members of an\nautomatically identified group will have similar characteristics, and reaching\na consensus with a decision-making process is preferable in this case. It\nrequires users-items and their rating interactions over a utility matrix to\nauto-detect the groups in group recommendations. We may not overlook other\nintrinsic information to form a group. The textual information also plays a\npivotal role in user clustering. In this paper, we auto-detect the groups based\non the textual similarity of the metadata (review texts). We consider the order\nin user preferences in our models. We have conducted extensive experiments over\ntwo real-world datasets to check the efficacy of the proposed models. We have\nalso conducted a competitive comparison with a baseline model to show the\nimprovements in the quality of recommendations.",
          "link": "http://arxiv.org/abs/2107.07284",
          "publishedOn": "2021-07-16T00:48:22.160Z",
          "wordCount": 618,
          "title": "Auto-detecting groups based on textual similarity for group recommendations. (arXiv:2107.07284v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenzhuo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shoujin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengsheng Wang</a>",
          "description": "The changing preferences of users towards items trigger the emergence of\nsession-based recommender systems (SBRSs), which aim to model the dynamic\npreferences of users for next-item recommendations. However, most of the\nexisting studies on SBRSs are based on long sessions only for recommendations,\nignoring short sessions, though short sessions, in fact, account for a large\nproportion in most of the real-world datasets. As a result, the applicability\nof existing SBRSs solutions is greatly reduced. In a short session, quite\nlimited contextual information is available, making the next-item\nrecommendation very challenging. To this end, in this paper, inspired by the\nsuccess of few-shot learning (FSL) in effectively learning a model with limited\ninstances, we formulate the next-item recommendation as an FSL problem.\nAccordingly, following the basic idea of a representative approach for FSL,\ni.e., meta-learning, we devise an effective SBRS called INter-SEssion\ncollaborative Recommender netTwork (INSERT) for next-item recommendations in\nshort sessions. With the carefully devised local module and global module,\nINSERT is able to learn an optimal preference representation of the current\nuser in a given short session. In particular, in the global module, a similar\nsession retrieval network (SSRN) is designed to find out the sessions similar\nto the current short session from the historical sessions of both the current\nuser and other users, respectively. The obtained similar sessions are then\nutilized to complement and optimize the preference representation learned from\nthe current short session by the local module for more accurate next-item\nrecommendations in this short session. Extensive experiments conducted on two\nreal-world datasets demonstrate the superiority of our proposed INSERT over the\nstate-of-the-art SBRSs when making next-item recommendations in short sessions.",
          "link": "http://arxiv.org/abs/2107.07453",
          "publishedOn": "2021-07-16T00:48:22.148Z",
          "wordCount": 707,
          "title": "Next-item Recommendations in Short Sessions. (arXiv:2107.07453v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fajie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengming Li</a>",
          "description": "Sequential recommender systems (SRS) have become a research hotspot due to\nits power in modeling user dynamic interests and sequential behavioral\npatterns. To maximize model expressive ability, a default choice is to apply a\nlarger and deeper network architecture, which, however, often brings high\nnetwork latency when generating online recommendations. Naturally, we argue\nthat compressing the heavy recommendation models into middle- or light- weight\nneural networks is of great importance for practical production systems. To\nrealize such a goal, we propose AdaRec, a knowledge distillation (KD) framework\nwhich compresses knowledge of a teacher model into a student model adaptively\naccording to its recommendation scene by using differentiable Neural\nArchitecture Search (NAS). Specifically, we introduce a target-oriented\ndistillation loss to guide the structure search process for finding the student\nnetwork architecture, and a cost-sensitive loss as constraints for model size,\nwhich achieves a superior trade-off between recommendation effectiveness and\nefficiency. In addition, we leverage Earth Mover's Distance (EMD) to realize\nmany-to-many layer mapping during knowledge distillation, which enables each\nintermediate student layer to learn from other intermediate teacher layers\nadaptively. Extensive experiments on real-world recommendation datasets\ndemonstrate that our model achieves competitive or better accuracy with notable\ninference speedup comparing to strong counterparts, while discovering diverse\nneural architectures for sequential recommender models under different\nrecommendation scenes.",
          "link": "http://arxiv.org/abs/2107.07173",
          "publishedOn": "2021-07-16T00:48:22.123Z",
          "wordCount": 654,
          "title": "Scene-adaptive Knowledge Distillation for Sequential Recommendation via Differentiable Architecture Search. (arXiv:2107.07173v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data. Towards this research\ngap, we first analyze the origin of biases from the perspective of \\textit{risk\ndiscrepancy} that represents the difference between the expectation empirical\nrisk and the true risk. Remarkably, we derive a general learning framework that\nwell summarizes most existing debiasing strategies by specifying some\nparameters of the general framework. This provides a valuable opportunity to\ndevelop a universal solution for debiasing, e.g., by learning the debiasing\nparameters from data. However, the training data lacks important signal of how\nthe data is biased and what the unbiased data looks like. To move this idea\nforward, we propose \\textit{AotoDebias} that leverages another (small) set of\nuniform data to optimize the debiasing parameters by solving the bi-level\noptimization problem with meta-learning. Through theoretical analyses, we\nderive the generalization bound for AutoDebias and prove its ability to acquire\nthe appropriate debiasing strategy. Extensive experiments on two real datasets\nand a simulated dataset demonstrated effectiveness of AutoDebias. The code is\navailable at \\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-07-16T00:48:22.054Z",
          "wordCount": 722,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1\">Alex Egg</a>",
          "description": "We propose a method to easily modify existing offline Recommender Systems to\nrun online using Transfer Learning. Online Learning for Recommender Systems has\ntwo main advantages: quality and scale. Like many Machine Learning algorithms\nin production if not regularly retrained will suffer from Concept Drift. A\npolicy that is updated frequently online can adapt to drift faster than a batch\nsystem. This is especially true for user-interaction systems like recommenders\nwhere the underlying distribution can shift drastically to follow user\nbehaviour. As a platform grows rapidly like Grubhub, the cost of running batch\ntraining jobs becomes material. A shift from stateless batch learning offline\nto stateful incremental learning online can recover, for example, at Grubhub,\nup to a 45x cost savings and a +20% metrics increase. There are a few\nchallenges to overcome with the transition to online stateful learning, namely\nconvergence, non-stationary embeddings and off-policy evaluation, which we\nexplore from our experiences running this system in production.",
          "link": "http://arxiv.org/abs/2107.07106",
          "publishedOn": "2021-07-16T00:48:21.994Z",
          "wordCount": 583,
          "title": "Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luptak_D/0/1/0/all/0/1\">D&#xe1;vid Lupt&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>",
          "description": "Math informational retrieval (MIR) search engines are absent in the\nwide-spread production use, even though documents in the STEM fields contain\nmany mathematical formulae, which are sometimes more important than text for\nunderstanding. We have developed and open-sourced the WebMIaS MIR search engine\nthat has been successfully deployed in the European Digital Mathematics Library\n(EuDML). However, its deployment is difficult to automate due to the complexity\nof this task. Moreover, the solutions developed so far to tackle this challenge\nare imperfect in terms of speed, maintenance, and robustness. In this paper, we\nwill describe the virtualization of WebMIaS using Docker that solves all three\nproblems and allows anyone to deploy containerized WebMIaS in a single line of\ncode. The publicly available Docker image will also help the community push the\ndevelopment of math-aware search engines in the ARQMath workshop series.",
          "link": "http://arxiv.org/abs/2106.00411",
          "publishedOn": "2021-07-15T01:59:02.249Z",
          "wordCount": 655,
          "title": "WebMIaS on Docker: Deploying Math-Aware Search in a Single Line of Code. (arXiv:2106.00411v2 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Sabuzima Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patgiri_R/0/1/0/all/0/1\">Ripon Patgiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waikhom_L/0/1/0/all/0/1\">Lilapati Waikhom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Arif Ahmed</a>",
          "description": "Edge technology aims to bring Cloud resources (specifically, the compute,\nstorage, and network) to the closed proximity of the Edge devices, i.e., smart\ndevices where the data are produced and consumed. Embedding computing and\napplication in Edge devices lead to emerging of two new concepts in Edge\ntechnology, namely, Edge computing and Edge analytics. Edge analytics uses some\ntechniques or algorithms to analyze the data generated by the Edge devices.\nWith the emerging of Edge analytics, the Edge devices have become a complete\nset. Currently, Edge analytics is unable to provide full support for the\nexecution of the analytic techniques. The Edge devices cannot execute advanced\nand sophisticated analytic algorithms following various constraints such as\nlimited power supply, small memory size, limited resources, etc. This article\naims to provide a detailed discussion on Edge analytics. A clear explanation to\ndistinguish between the three concepts of Edge technology, namely, Edge\ndevices, Edge computing, and Edge analytics, along with their issues.\nFurthermore, the article discusses the implementation of Edge analytics to\nsolve many problems in various areas such as retail, agriculture, industry, and\nhealthcare. In addition, the research papers of the state-of-the-art edge\nanalytics are rigorously reviewed in this article to explore the existing\nissues, emerging challenges, research opportunities and their directions, and\napplications.",
          "link": "http://arxiv.org/abs/2107.06835",
          "publishedOn": "2021-07-15T01:59:02.242Z",
          "wordCount": 685,
          "title": "A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises, Future Directions, and Applications. (arXiv:2107.06835v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1\">Felix Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1\">Christian Borgelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1\">Oliver Deussen</a>",
          "description": "Sorting and ranking supervision is a method for training neural networks\nend-to-end based on ordering constraints. That is, the ground truth order of\nsets of samples is known, while their absolute values remain unsupervised. For\nthat, we propose differentiable sorting networks by relaxing their pairwise\nconditional swap operations. To address the problems of vanishing gradients and\nextensive blurring that arise with larger numbers of layers, we propose mapping\nactivations to regions with moderate gradients. We consider odd-even as well as\nbitonic sorting networks, which outperform existing relaxations of the sorting\noperation. We show that bitonic sorting networks can achieve stable training on\nlarge input sets of up to 1024 elements.",
          "link": "http://arxiv.org/abs/2105.04019",
          "publishedOn": "2021-07-15T01:59:02.195Z",
          "wordCount": 595,
          "title": "Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision. (arXiv:2105.04019v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>",
          "description": "Accurately linking news articles to scientific research works is a critical\ncomponent in a number of applications, such as measuring the social impact of a\nresearch work and detecting inaccuracies or distortions in science news.\nAlthough the lack of links between news and literature has been a challenge in\nthese applications, it is a relatively unexplored research problem. In this\npaper we designed and evaluated a new approach that consists of (1) augmenting\nlatest named-entity recognition techniques to extract various metadata, and (2)\ndesigning a new elastic search engine that can facilitate the use of enriched\nmetadata queries. To evaluate our approach, we constructed two datasets of\npaired news articles and research papers: one is used for training models to\nextract metadata, and the other for evaluation. Our experiments showed that the\nnew approach performed significantly better than a baseline approach used by\naltmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate\nthe effectiveness of the approach, we also conducted a study on 37,600\nhealth-related press releases published on EurekAlert!, which showed that our\napproach was able to identify the corresponding research papers with a top-1\naccuracy of at least 0.97.",
          "link": "http://arxiv.org/abs/2107.06472",
          "publishedOn": "2021-07-15T01:59:02.187Z",
          "wordCount": 632,
          "title": "Linking Health News to Research Literature. (arXiv:2107.06472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kaize Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1\">James Caverlee</a>",
          "description": "A fundamental challenge for sequential recommenders is to capture the\nsequential patterns of users toward modeling how users transit among items. In\nmany practical scenarios, however, there are a great number of cold-start users\nwith only minimal logged interactions. As a result, existing sequential\nrecommendation models will lose their predictive power due to the difficulties\nin learning sequential patterns over users with only limited interactions. In\nthis work, we aim to improve sequential recommendation for cold-start users\nwith a novel framework named MetaTL, which learns to model the transition\npatterns of users through meta-learning. Specifically, the proposed MetaTL: (i)\nformulates sequential recommendation for cold-start users as a few-shot\nlearning problem; (ii) extracts the dynamic transition patterns among users\nwith a translation-based architecture; and (iii) adopts meta transitional\nlearning to enable fast learning for cold-start users with only limited\ninteractions, leading to accurate inference of sequential interactions.",
          "link": "http://arxiv.org/abs/2107.06427",
          "publishedOn": "2021-07-15T01:59:02.178Z",
          "wordCount": 579,
          "title": "Sequential Recommendation for Cold-start Users with Meta Transitional Learning. (arXiv:2107.06427v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1\">Sergio Rojas-Galeano</a>",
          "description": "One of the stratagems used to deceive spam filters is to substitute vocables\nwith synonyms or similar words that turn the message unrecognisable by the\ndetection algorithms. In this paper we investigate whether the recent\ndevelopment of language models sensitive to the semantics and context of words,\nsuch as Google's BERT, may be useful to overcome this adversarial attack\n(called \"Mad-lib\" as per the word substitution game). Using a dataset of 5572\nSMS spam messages, we first established a baseline of detection performance\nusing widely known document representation models (BoW and TFIDF) and the novel\nBERT model, coupled with a variety of classification algorithms (Decision Tree,\nkNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we\nbuilt a thesaurus of the vocabulary contained in these messages, and set up a\nMad-lib attack experiment in which we modified each message of a held out\nsubset of data (not used in the baseline experiment) with different rates of\nsubstitution of original words with synonyms from the thesaurus. Lastly, we\nevaluated the detection performance of the three representation models (BoW,\nTFIDF and BERT) coupled with the best classifier from the baseline experiment\n(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)\nin the original dataset, whereas the BERT model obtained 96%. On the other\nhand, the Mad-lib attack experiment showed that BERT encodings manage to\nmaintain a similar BA performance of 96% with an average substitution rate of\n1.82 words per message, and 95% with 3.34 words substituted per message. In\ncontrast, the BA performance of the BoW and TFIDF encoders dropped to chance.\nThese results hint at the potential advantage of BERT models to combat these\ntype of ingenious attacks, offsetting to some extent for the inappropriate use\nof semantic relationships in language.",
          "link": "http://arxiv.org/abs/2107.06400",
          "publishedOn": "2021-07-15T01:59:02.170Z",
          "wordCount": 732,
          "title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ashudeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1\">David Kempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1\">Thorsten Joachims</a>",
          "description": "Fairness has emerged as an important consideration in algorithmic\ndecision-making. Unfairness occurs when an agent with higher merit obtains a\nworse outcome than an agent with lower merit. Our central point is that a\nprimary cause of unfairness is uncertainty. A principal or algorithm making\ndecisions never has access to the agents' true merit, and instead uses proxy\nfeatures that only imperfectly predict merit (e.g., GPA, star ratings,\nrecommendation letters). None of these ever fully capture an agent's merit; yet\nexisting approaches have mostly been defining fairness notions directly based\non observed features and outcomes.\n\nOur primary point is that it is more principled to acknowledge and model the\nuncertainty explicitly. The role of observed features is to give rise to a\nposterior distribution of the agents' merits. We use this viewpoint to define a\nnotion of approximate fairness in ranking. We call an algorithm $\\phi$-fair\n(for $\\phi \\in [0,1]$) if it has the following property for all agents $x$ and\nall $k$: if agent $x$ is among the top $k$ agents with respect to merit with\nprobability at least $\\rho$ (according to the posterior merit distribution),\nthen the algorithm places the agent among the top $k$ agents in its ranking\nwith probability at least $\\phi \\rho$.\n\nWe show how to compute rankings that optimally trade off approximate fairness\nagainst utility to the principal. In addition to the theoretical\ncharacterization, we present an empirical analysis of the potential impact of\nthe approach in simulation studies. For real-world validation, we applied the\napproach in the context of a paper recommendation system that we built and\nfielded at a large conference.",
          "link": "http://arxiv.org/abs/2107.06720",
          "publishedOn": "2021-07-15T01:59:02.151Z",
          "wordCount": 708,
          "title": "Fairness in Ranking under Uncertainty. (arXiv:2107.06720v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petrescu_D/0/1/0/all/0/1\">Diana Petrescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Recommendations with personalized explanations have been shown to increase\nuser trust and perceived quality and help users make better decisions.\nMoreover, such explanations allow users to provide feedback by critiquing them.\nSeveral algorithms for recommendation systems with multi-step critiquing have\ntherefore been developed. However, providing a user-friendly interface based on\npersonalized explanations and critiquing has not been addressed in the last\ndecade. In this paper, we introduce four different web interfaces (available\nunder https://lia.epfl.ch/critiquing/) helping users making decisions and\nfinding their ideal item. We have chosen the hotel recommendation domain as a\nuse case even though our approach is trivially adaptable for other domains.\nMoreover, our system is model-agnostic (for both recommender systems and\ncritiquing models) allowing a great flexibility and further extensions. Our\ninterfaces are above all a useful tool to help research in recommendation with\ncritiquing. They allow to test such systems on a real use case and also to\nhighlight some limitations of these approaches to find solutions to overcome\nthem.",
          "link": "http://arxiv.org/abs/2107.06416",
          "publishedOn": "2021-07-15T01:59:02.063Z",
          "wordCount": 592,
          "title": "Multi-Step Critiquing User Interface for Recommender Systems. (arXiv:2107.06416v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+AlGhamdi_K/0/1/0/all/0/1\">Kholoud AlGhamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1\">Elena Simperl</a>",
          "description": "Wikidata is an open knowledge graph built by a global community of\nvolunteers. As it advances in scale, it faces substantial challenges around\neditor engagement. These challenges are in terms of both attracting new editors\nto keep up with the sheer amount of work and retaining existing editors.\nExperience from other online communities and peer-production systems, including\nWikipedia, suggests that personalised recommendations could help, especially\nnewcomers, who are sometimes unsure about how to contribute best to an ongoing\neffort. For this reason, we propose a recommender system WikidataRec for\nWikidata items. The system uses a hybrid of content-based and collaborative\nfiltering techniques to rank items for editors relying on both item features\nand item-editor previous interaction. A neural network, named a neural mixture\nof representations, is designed to learn fine weights for the combination of\nitem-based representations and optimize them with editor-based representation\nby item-editor interaction. To facilitate further research in this space, we\nalso create two benchmark datasets, a general-purpose one with 220,000 editors\nresponsible for 14 million interactions with 4 million items and a second one\nfocusing on the contributions of more than 8,000 more active editors. We\nperform an offline evaluation of the system on both datasets with promising\nresults. Our code and datasets are available at\nhttps://github.com/WikidataRec-developer/Wikidata_Recommender.",
          "link": "http://arxiv.org/abs/2107.06423",
          "publishedOn": "2021-07-15T01:59:02.033Z",
          "wordCount": 639,
          "title": "Learning to Recommend Items to Wikidata Editors. (arXiv:2107.06423v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabanac_G/0/1/0/all/0/1\">Guillaume Cabanac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labbe_C/0/1/0/all/0/1\">Cyril Labb&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magazinov_A/0/1/0/all/0/1\">Alexander Magazinov</a>",
          "description": "Probabilistic text generators have been used to produce fake scientific\npapers for more than a decade. Such nonsensical papers are easily detected by\nboth human and machine. Now more complex AI-powered generation techniques\nproduce texts indistinguishable from that of humans and the generation of\nscientific texts from a few keywords has been documented. Our study introduces\nthe concept of tortured phrases: unexpected weird phrases in lieu of\nestablished ones, such as 'counterfeit consciousness' instead of 'artificial\nintelligence.' We combed the literature for tortured phrases and study one\nreputable journal where these concentrated en masse. Hypothesising the use of\nadvanced language models we ran a detector on the abstracts of recent articles\nof this journal and on several control sets. The pairwise comparisons reveal a\nconcentration of abstracts flagged as 'synthetic' in the journal. We also\nhighlight irregularities in its operation, such as abrupt changes in editorial\ntimelines. We substantiate our call for investigation by analysing several\nindividual dubious articles, stressing questionable features: tortured writing\nstyle, citation of non-existent literature, and unacknowledged image reuse.\nSurprisingly, some websites offer to rewrite texts for free, generating\ngobbledegook full of tortured phrases. We believe some authors used rewritten\ntexts to pad their manuscripts. We wish to raise the awareness on publications\ncontaining such questionable AI-generated or rewritten texts that passed (poor)\npeer review. Deception with synthetic texts threatens the integrity of the\nscientific literature.",
          "link": "http://arxiv.org/abs/2107.06751",
          "publishedOn": "2021-07-15T01:59:02.007Z",
          "wordCount": 688,
          "title": "Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals. (arXiv:2107.06751v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Visually-aware recommendation on E-commerce platforms aims to leverage visual\ninformation of items to predict a user's preference. It is commonly observed\nthat user's attention to visual features does not always reflect the real\npreference. Although a user may click and view an item in light of a visual\nsatisfaction of their expectations, a real purchase does not always occur due\nto the unsatisfaction of other essential features (e.g., brand, material,\nprice). We refer to the reason for such a visually related interaction\ndeviating from the real preference as a visual bias. Existing visually-aware\nmodels make use of the visual features as a separate collaborative signal\nsimilarly to other features to directly predict the user's preference without\nconsidering a potential bias, which gives rise to a visually biased\nrecommendation. In this paper, we derive a causal graph to identify and analyze\nthe visual bias of these existing methods. In this causal graph, the visual\nfeature of an item acts as a mediator, which could introduce a spurious\nrelationship between the user and the item. To eliminate this spurious\nrelationship that misleads the prediction of the user's real preference, an\nintervention and a counterfactual inference are developed over the mediator.\nParticularly, the Total Indirect Effect is applied for a debiased prediction\nduring the testing phase of the model. This causal inference framework is model\nagnostic such that it can be integrated into the existing methods. Furthermore,\nwe propose a debiased visually-aware recommender system, denoted as CausalRec\nto effectively retain the supportive significance of the visual information and\nremove the visual bias. Extensive experiments are conducted on eight benchmark\ndatasets, which shows the state-of-the-art performance of CausalRec and the\nefficacy of debiasing.",
          "link": "http://arxiv.org/abs/2107.02390",
          "publishedOn": "2021-07-14T01:41:48.864Z",
          "wordCount": 741,
          "title": "CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation. (arXiv:2107.02390v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zamanifar_K/0/1/0/all/0/1\">Kamran Zamanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_G/0/1/0/all/0/1\">Golsa Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematbakhsh_N/0/1/0/all/0/1\">Naser Nematbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardookhi_F/0/1/0/all/0/1\">Farhad Mardookhi</a>",
          "description": "In this work we propose a new approach for semantic web matching to improve\nthe performance of Web Service replacement. Because in automatic systems we\nshould ensure the self-healing, self-configuration, self-optimization and\nself-management, all services should be always available and if one of them\ncrashes, it should be replaced with the most similar one. Candidate services\nare advertised in Universal Description, Discovery and Integration (UDDI) all\nin Web Ontology Language (OWL). By the help of bipartite graph, we did the\nmatching between the crashed service and a Candidate one. Then we chose the\nbest service, which had the maximum rate of matching. In fact we compare two\nservices` functionalities and capabilities to see how much they match. We found\nthat the best way for matching two web services, is comparing the\nfunctionalities of them.",
          "link": "http://arxiv.org/abs/2107.06083",
          "publishedOn": "2021-07-14T01:41:48.852Z",
          "wordCount": 573,
          "title": "A New Approach for Semantic Web Matching. (arXiv:2107.06083v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Formal_T/0/1/0/all/0/1\">Thibault Formal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>",
          "description": "In neural Information Retrieval, ongoing research is directed towards\nimproving the first retriever in ranking pipelines. Learning dense embeddings\nto conduct retrieval using efficient approximate nearest neighbors methods has\nproven to work well. Meanwhile, there has been a growing interest in learning\nsparse representations for documents and queries, that could inherit from the\ndesirable properties of bag-of-words models such as the exact matching of terms\nand the efficiency of inverted indexes. In this work, we present a new\nfirst-stage ranker based on explicit sparsity regularization and a\nlog-saturation effect on term weights, leading to highly sparse representations\nand competitive results with respect to state-of-the-art dense and sparse\nmethods. Our approach is simple, trained end-to-end in a single stage. We also\nexplore the trade-off between effectiveness and efficiency, by controlling the\ncontribution of the sparsity regularization.",
          "link": "http://arxiv.org/abs/2107.05720",
          "publishedOn": "2021-07-14T01:41:48.840Z",
          "wordCount": 572,
          "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. (arXiv:2107.05720v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fareri_S/0/1/0/all/0/1\">Silvia Fareri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melluso_N/0/1/0/all/0/1\">Nicola Melluso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiarello_F/0/1/0/all/0/1\">Filippo Chiarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fantoni_G/0/1/0/all/0/1\">Gualtiero Fantoni</a>",
          "description": "In today's digital world, there is an increasing focus on soft skills. On the\none hand, they facilitate innovation at companies, but on the other, they are\nunlikely to be automated soon. Researchers struggle with accurately approaching\nquantitatively the study of soft skills due to the lack of data-driven methods\nto retrieve them. This limits the possibility for psychologists and HR managers\nto understand the relation between humans and digitalisation. This paper\npresents SkillNER, a novel data-driven method for automatically extracting soft\nskills from text. It is a named entity recognition (NER) system trained with a\nsupport vector machine (SVM) on a corpus of more than 5000 scientific papers.\nWe developed this system by measuring the performance of our approach against\ndifferent training models and validating the results together with a team of\npsychologists. Finally, SkillNER was tested in a real-world case study using\nthe job descriptions of ESCO (European Skill/Competence Qualification and\nOccupation) as textual source. The system enabled the detection of communities\nof job profiles based on their shared soft skills and communities of soft\nskills based on their shared job profiles. This case study demonstrates that\nthe tool can automatically retrieve soft skills from a large corpus in an\nefficient way, proving useful for firms, institutions, and workers. The tool is\nopen and available online to foster quantitative methods for the study of soft\nskills.",
          "link": "http://arxiv.org/abs/2101.11431",
          "publishedOn": "2021-07-14T01:41:48.823Z",
          "wordCount": 699,
          "title": "SkillNER: Mining and Mapping Soft Skills from any Text. (arXiv:2101.11431v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1\">Keping Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croft_W/0/1/0/all/0/1\">W. Bruce Croft</a>",
          "description": "Users often need to look through multiple search result pages or reformulate\nqueries when they have complex information-seeking needs. Conversational search\nsystems make it possible to improve user satisfaction by asking questions to\nclarify users' search intents. This, however, can take significant effort to\nanswer a series of questions starting with \"what/why/how\". To quickly identify\nuser intent and reduce effort during interactions, we propose an intent\nclarification task based on yes/no questions where the system needs to ask the\ncorrect question about intents within the fewest conversation turns. In this\ntask, it is essential to use negative feedback about the previous questions in\nthe conversation history. To this end, we propose a Maximum-Marginal-Relevance\n(MMR) based BERT model (MMR-BERT) to leverage negative feedback based on the\nMMR principle for the next clarifying question selection. Experiments on the\nQulac dataset show that MMR-BERT outperforms state-of-the-art baselines\nsignificantly on the intent identification task and the selected questions also\nachieve significantly better performance in the associated document retrieval\ntasks.",
          "link": "http://arxiv.org/abs/2107.05760",
          "publishedOn": "2021-07-14T01:41:48.791Z",
          "wordCount": 606,
          "title": "Asking Clarifying Questions Based on Negative Feedback in Conversational Search. (arXiv:2107.05760v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirshafiee_M/0/1/0/all/0/1\">Mitra Sadat Mirshafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allahyari_M/0/1/0/all/0/1\">Mehdi Allahyari</a>",
          "description": "With the surge of pretrained language models, a new pathway has been opened\nto incorporate Persian text contextual information. Meanwhile, as many other\ncountries, including Iran, are fighting against COVID-19, a plethora of\nCOVID-19 related articles has been published in Iranian Healthcare magazines to\nbetter inform the public of the situation. However, finding answers in this\nsheer volume of information is an extremely difficult task. In this paper, we\ncollected a large dataset of these articles, leveraged different BERT\nvariations as well as other keyword models such as BM25 and TF-IDF, and created\na search engine to sift through these documents and rank them, given a user's\nquery. Our final search engine consists of a ranker and a re-ranker, which\nadapts itself to the query. We fine-tune our models using Semantic Textual\nSimilarity and evaluate them with standard task metrics. Our final method\noutperforms the rest by a considerable margin.",
          "link": "http://arxiv.org/abs/2107.05722",
          "publishedOn": "2021-07-14T01:41:48.775Z",
          "wordCount": 631,
          "title": "COPER a query-adaptable Semantics-based Search Engine for Persian COVID-19 Articles. (arXiv:2107.05722v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1\">Rodrigo Castellon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We demonstrate that language models pre-trained on codified\n(discretely-encoded) music audio learn representations that are useful for\ndownstream MIR tasks. Specifically, we explore representations from Jukebox\n(Dhariwal et al. 2020): a music generation system containing a language model\ntrained on codified audio from 1M songs. To determine if Jukebox's\nrepresentations contain useful information for MIR, we use them as input\nfeatures to train shallow models on several MIR tasks. Relative to\nrepresentations from conventional MIR models which are pre-trained on tagging,\nwe find that using representations from Jukebox as input features yields 30%\nstronger performance on average across four MIR tasks: tagging, genre\nclassification, emotion recognition, and key detection. For key detection, we\nobserve that representations from Jukebox are considerably stronger than those\nfrom models pre-trained on tagging, suggesting that pre-training via codified\naudio language modeling may address blind spots in conventional approaches. We\ninterpret the strength of Jukebox's representations as evidence that modeling\naudio instead of tags provides richer representations for MIR.",
          "link": "http://arxiv.org/abs/2107.05677",
          "publishedOn": "2021-07-14T01:41:48.737Z",
          "wordCount": 621,
          "title": "Codified audio language modeling learns useful representations for music information retrieval. (arXiv:2107.05677v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Williams_E/0/1/0/all/0/1\">Evan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_P/0/1/0/all/0/1\">Paul Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Sieu Tran</a>",
          "description": "This paper discusses the approach used by the Accenture Team for CLEF2021\nCheckThat! Lab, Task 1, to identify whether a claim made in social media would\nbe interesting to a wide audience and should be fact-checked. Twitter training\nand test data were provided in English, Arabic, Spanish, Turkish, and\nBulgarian. Claims were to be classified (check-worthy/not check-worthy) and\nranked in priority order for the fact-checker. Our method used deep neural\nnetwork transformer models with contextually sensitive lexical augmentation\napplied on the supplied training datasets to create additional training\nsamples. This augmentation approach improved the performance for all languages.\nOverall, our architecture and data augmentation pipeline produced the best\nsubmitted system for Arabic, and performance scales according to the quantity\nof provided training data for English, Spanish, Turkish, and Bulgarian. This\npaper investigates the deep neural network architectures for each language as\nwell as the provided data to examine why the approach worked so effectively for\nArabic, and discusses additional data augmentation measures that should could\nbe useful to this problem.",
          "link": "http://arxiv.org/abs/2107.05684",
          "publishedOn": "2021-07-14T01:41:48.465Z",
          "wordCount": 665,
          "title": "Accenture at CheckThat! 2021: Interesting claim identification and ranking with contextually sensitive lexical training data augmentation. (arXiv:2107.05684v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Volske_M/0/1/0/all/0/1\">Michael V&#xf6;lske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondarenko_A/0/1/0/all/0/1\">Alexander Bondarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1\">Maik Fr&#xf6;be</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1\">Matthias Hagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Recently, neural networks have been successfully employed to improve upon\nstate-of-the-art performance in ad-hoc retrieval tasks via machine-learned\nranking functions. While neural retrieval models grow in complexity and impact,\nlittle is understood about their correspondence with well-studied IR\nprinciples. Recent work on interpretability in machine learning has provided\ntools and techniques to understand neural models in general, yet there has been\nlittle progress towards explaining ranking models.\n\nWe investigate whether one can explain the behavior of neural ranking models\nin terms of their congruence with well understood principles of document\nranking by using established theories from axiomatic IR. Axiomatic analysis of\ninformation retrieval models has formalized a set of constraints on ranking\ndecisions that reasonable retrieval models should fulfill. We operationalize\nthis axiomatic thinking to reproduce rankings based on combinations of\nelementary constraints. This allows us to investigate to what extent the\nranking decisions of neural rankers can be explained in terms of retrieval\naxioms, and which axioms apply in which situations. Our experimental study\nconsiders a comprehensive set of axioms over several representative neural\nrankers. While the existing axioms can already explain the particularly\nconfident ranking decisions rather well, future work should extend the axiom\nset to also cover the other still \"unexplainable\" neural IR rank decisions.",
          "link": "http://arxiv.org/abs/2106.08019",
          "publishedOn": "2021-07-13T01:59:33.207Z",
          "wordCount": 686,
          "title": "Towards Axiomatic Explanations for Neural Ranking Models. (arXiv:2106.08019v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hromada_D/0/1/0/all/0/1\">Daniel Devatman Hromada</a>",
          "description": "Departing from the postulate that Voynich Manuscript is not a hoax but rather\nencodes authentic contents, our article presents an evolutionary algorithm\nwhich aims to find the most optimal mapping between voynichian glyphs and\ncandidate phonemic values. Core component of the decoding algorithm is a\nprocess of maximization of a fitness function which aims to find most optimal\nset of substitution rules allowing to transcribe the part of the manuscript --\nwhich we call the Calendar -- into lists of feminine names. This leads to sets\nof character subsitution rules which allow us to consistently transcribe dozens\namong three hundred calendar tokens into feminine names: a result far\nsurpassing both ``popular'' as well as \"state of the art\" tentatives to crack\nthe manuscript. What's more, by using name lists stemming from different\nlanguages as potential cribs, our ``adaptive'' method can also be useful in\nidentification of the language in which the manuscript is written.\n\nAs far as we can currently tell, results of our experiments indicate that the\nCalendar part of the manuscript contains names from baltoslavic, balkanic or\nhebrew language strata. Two further indications are also given: primo, highest\nfitness values were obtained when the crib list contains names with specific\ninfixes at token's penultimate position as is the case, for example, for slavic\n\\textbf{feminine diminutives} (i.e. names ending with -ka and not -a). In the\nmost successful scenario, 240 characters contained in 35 distinct Voynichese\ntokens were successfully transcribed. Secundo, in case of crib stemming from\nHebrew language, whole adaptation process converges to significantly better\nfitness values when transcribing voynichian tokens whose order of individual\ncharacters have been reversed, and when lists feminine and not masculine names\nare used as the crib.",
          "link": "http://arxiv.org/abs/2107.05381",
          "publishedOn": "2021-07-13T01:59:33.170Z",
          "wordCount": 730,
          "title": "Can Evolutionary Computation Help us to Crib the Voynich Manuscript ?. (arXiv:2107.05381v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1\">Naicheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaoshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qiongxu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kaixin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaobo Guo</a>",
          "description": "Session-based recommendation (SBR) learns users' preferences by capturing the\nshort-term and sequential patterns from the evolution of user behaviors. Among\nthe studies in the SBR field, graph-based approaches are a relatively powerful\nkind of way, which generally extract item information by message aggregation\nunder Euclidean space. However, such methods can't effectively extract the\nhierarchical information contained among consecutive items in a session, which\nis critical to represent users' preferences. In this paper, we present a\nhyperbolic contrastive graph recommender (HCGR), a principled session-based\nrecommendation framework involving Lorentz hyperbolic space to adequately\ncapture the coherence and hierarchical representations of the items. Within\nthis framework, we design a novel adaptive hyperbolic attention computation to\naggregate the graph message of each user's preference in a session-based\nbehavior sequence. In addition, contrastive learning is leveraged to optimize\nthe item representation by considering the geodesic distance between positive\nand negative samples in hyperbolic space. Extensive experiments on four\nreal-world datasets demonstrate that HCGR consistently outperforms\nstate-of-the-art baselines by 0.43$\\%$-28.84$\\%$ in terms of $HitRate$, $NDCG$\nand $MRR$.",
          "link": "http://arxiv.org/abs/2107.05366",
          "publishedOn": "2021-07-13T01:59:33.155Z",
          "wordCount": 616,
          "title": "HCGR: Hyperbolic Contrastive Graph Representation Learning for Session-based Recommendation. (arXiv:2107.05366v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Victor Zitian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montano_Campos_F/0/1/0/all/0/1\">Felipe Montano-Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_W/0/1/0/all/0/1\">Wlodek Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canfield_E/0/1/0/all/0/1\">Evan Canfield</a>",
          "description": "The volume of scientific publications in organizational research becomes\nexceedingly overwhelming for human researchers who seek to timely extract and\nreview knowledge. This paper introduces natural language processing (NLP)\nmodels to accelerate the discovery, extraction, and organization of theoretical\ndevelopments (i.e., hypotheses) from social science publications. We illustrate\nand evaluate NLP models in the context of a systematic review of stakeholder\nvalue constructs and hypotheses. Specifically, we develop NLP models to\nautomatically 1) detect sentences in scholarly documents as hypotheses or not\n(Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs)\nand links (causal/associative relationships) (Relationship Deconstruction ),\nand 3) classify the features of links in terms causality (versus association)\nand direction (positive, negative, versus nonlinear) (Feature Classification).\nOur models have reported high performance metrics for all three tasks. While\nour models are built in Python, we have made the pre-trained models fully\naccessible for non-programmers. We have provided instructions on installing and\nusing our pre-trained models via an R Shiny app graphic user interface (GUI).\nFinally, we suggest the next paths to extend our methodology for\ncomputer-assisted knowledge synthesis.",
          "link": "http://arxiv.org/abs/2106.16102",
          "publishedOn": "2021-07-13T01:59:33.132Z",
          "wordCount": 649,
          "title": "Machine Reading of Hypotheses for Organizational Research Reviews and Pre-trained Models via R Shiny App for Non-Programmers. (arXiv:2106.16102v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_D/0/1/0/all/0/1\">Dhivya Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1\">Vijay Mago</a>",
          "description": "Semantic textual similarity is one of the open research challenges in the\nfield of Natural Language Processing. Extensive research has been carried out\nin this field and near-perfect results are achieved by recent transformer-based\nmodels in existing benchmark datasets like the STS dataset and the SICK\ndataset. In this paper, we study the sentences in these datasets and analyze\nthe sensitivity of various word embeddings with respect to the complexity of\nthe sentences. We build a complex sentences dataset comprising of 50 sentence\npairs with associated semantic similarity values provided by 15 human\nannotators. Readability analysis is performed to highlight the increase in\ncomplexity of the sentences in the existing benchmark datasets and those in the\nproposed dataset. Further, we perform a comparative analysis of the performance\nof various word embeddings and language models on the existing benchmark\ndatasets and the proposed dataset. The results show the increase in complexity\nof the sentences has a significant impact on the performance of the embedding\nmodels resulting in a 10-20% decrease in Pearson's and Spearman's correlation.",
          "link": "http://arxiv.org/abs/2010.12637",
          "publishedOn": "2021-07-13T01:59:33.120Z",
          "wordCount": 661,
          "title": "Comparative analysis of word embeddings in assessing semantic similarity of complex sentences. (arXiv:2010.12637v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heidari_G/0/1/0/all/0/1\">Golsa Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamanifar_K/0/1/0/all/0/1\">Kamran Zamanifar</a>",
          "description": "Since using environments that are made according to the service oriented\narchitecture, we have more effective and dynamic applications. Semantic\nmatchmaking process is finding valuable service candidates for substitution. It\nis a very important aspect of using semantic Web Services. Our proposed\nmatchmaker algorithm performs semantic matching of Web Services on the basis of\ninput and output descriptions of semantic Web Services matching. This technique\ntakes advantages from a graph structure and flow networks. Our novel approach\nis assigning matchmaking scores to semantics of the inputs and outputs\nparameters and their types. It makes a flow network in which the weights of the\nedges are these scores, using FordFulkerson algorithm, we find matching rate of\ntwo web services. So, all services should be described in the same Ontology Web\nLanguage. Among these candidates, best one is chosen for substitution in the\ncase of an execution failure. Our approach uses the algorithm that has the\nleast running time among all others that can be used for bipartite matching.\nThe importance of problem is that in real systems, many fundamental problems\nwill occur by late answering. So system`s service should always be on and if\none of them crashes, it would be replaced fast. Semantic web matchmaker eases\nthis process.",
          "link": "http://arxiv.org/abs/2107.05368",
          "publishedOn": "2021-07-13T01:59:32.950Z",
          "wordCount": 660,
          "title": "A Three Phase Semantic Web Matchmaker. (arXiv:2107.05368v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanhua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiwen Xu</a>",
          "description": "Content feed, a type of product that recommends a sequence of items for users\nto browse and engage with, has gained tremendous popularity among social media\nplatforms. In this paper, we propose to study the diversity problem in such a\nscenario from an item sequence perspective using time series analysis\ntechniques. We derive a method called sliding spectrum decomposition (SSD) that\ncaptures users' perception of diversity in browsing a long item sequence. We\nalso share our experiences in designing and implementing a suitable item\nembedding method for accurate similarity measurement under long tail effect.\nCombined together, they are now fully implemented and deployed in Xiaohongshu\nApp's production recommender system that serves the main Explore Feed product\nfor tens of millions of users every day. We demonstrate the effectiveness and\nefficiency of the method through theoretical analysis, offline experiments and\nonline A/B tests.",
          "link": "http://arxiv.org/abs/2107.05204",
          "publishedOn": "2021-07-13T01:59:32.926Z",
          "wordCount": 596,
          "title": "Sliding Spectrum Decomposition for Diversified Recommendation. (arXiv:2107.05204v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1\">Jonathan Stray</a>",
          "description": "Polarization is implicated in the erosion of democracy and the progression to\nviolence, which makes the polarization properties of large algorithmic content\nselection systems (recommender systems) a matter of concern for peace and\nsecurity. While algorithm-driven social media does not seem to be a primary\ndriver of polarization at the country level, it could be a useful intervention\npoint in polarized societies. This paper examines algorithmic depolarization\ninterventions with the goal of conflict transformation: not suppressing or\neliminating conflict but moving towards more constructive conflict. Algorithmic\nintervention is considered at three stages: which content is available\n(moderation), how content is selected and personalized (ranking), and content\npresentation and controls (user interface). Empirical studies of online\nconflict suggest that the exposure diversity intervention proposed as an\nantidote to \"filter bubbles\" can be improved and can even worsen polarization\nunder some conditions. Using civility metrics in conjunction with diversity in\ncontent selection may be more effective. However, diversity-based interventions\nhave not been tested at scale and may not work in the diverse and dynamic\ncontexts of real platforms. Instead, intervening in platform polarization\ndynamics will likely require continuous monitoring of polarization metrics,\nsuch as the widely used \"feeling thermometer.\" These metrics can be used to\nevaluate product features, and potentially engineered as algorithmic\nobjectives. It may further prove necessary to include polarization measures in\nthe objective functions of recommender algorithms to prevent optimization\nprocesses from creating conflict as a side effect.",
          "link": "http://arxiv.org/abs/2107.04953",
          "publishedOn": "2021-07-13T01:59:32.859Z",
          "wordCount": 678,
          "title": "Designing Recommender Systems to Depolarize. (arXiv:2107.04953v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yutao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deyi Li</a>",
          "description": "Most of the existing deep learning-based sequential recommendation approaches\nutilize the recurrent neural network architecture or self-attention to model\nthe sequential patterns and temporal influence among a user's historical\nbehavior and learn the user's preference at a specific time. However, these\nmethods have two main drawbacks. First, they focus on modeling users' dynamic\nstates from a user-centric perspective and always neglect the dynamics of items\nover time. Second, most of them deal with only the first-order user-item\ninteractions and do not consider the high-order connectivity between users and\nitems, which has recently been proved helpful for the sequential\nrecommendation. To address the above problems, in this article, we attempt to\nmodel user-item interactions by a bipartite graph structure and propose a new\nrecommendation approach based on a Position-enhanced and Time-aware Graph\nConvolutional Network (PTGCN) for the sequential recommendation. PTGCN models\nthe sequential patterns and temporal dynamics between user-item interactions by\ndefining a position-enhanced and time-aware graph convolution operation and\nlearning the dynamic representations of users and items simultaneously on the\nbipartite graph with a self-attention aggregator. Also, it realizes the\nhigh-order connectivity between users and items by stacking multi-layer graph\nconvolutions. To demonstrate the effectiveness of PTGCN, we carried out a\ncomprehensive evaluation of PTGCN on three real-world datasets of different\nsizes compared with a few competitive baselines. Experimental results indicate\nthat PTGCN outperforms several state-of-the-art models in terms of two\ncommonly-used evaluation metrics for ranking.",
          "link": "http://arxiv.org/abs/2107.05235",
          "publishedOn": "2021-07-13T01:59:32.850Z",
          "wordCount": 686,
          "title": "Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations. (arXiv:2107.05235v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shuchang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "In recent years, graph neural networks (GNNs) have shown powerful ability in\ncollaborative filtering, which is a widely adopted recommendation scenario.\nWhile without any side information, existing graph neural network based methods\ngenerally learn a one-hot embedding for each user or item as the initial input\nrepresentation of GNNs. However, such one-hot embedding is intrinsically\ntransductive, making these methods with no inductive ability, i.e., failing to\ndeal with new users or new items that are unseen during training. Besides, the\nnumber of model parameters depends on the number of users and items, which is\nexpensive and not scalable. In this paper, we give a formal definition of\ninductive recommendation and solve the above problems by proposing Inductive\nrepresentation based Graph Convolutional Network (IGCN) for collaborative\nfiltering. Specifically, we design an inductive representation layer, which\nutilizes the interaction behavior with core users or items as the initial\nrepresentation, improving the general recommendation performance while bringing\ninductive ability. Note that, the number of parameters of IGCN only depends on\nthe number of core users or items, which is adjustable and scalable. Extensive\nexperiments on three public benchmarks demonstrate the state-of-the-art\nperformance of IGCN in both transductive and inductive recommendation\nscenarios, while with remarkably fewer model parameters. Our implementations\nare available here in PyTorch.",
          "link": "http://arxiv.org/abs/2107.05247",
          "publishedOn": "2021-07-13T01:59:32.788Z",
          "wordCount": 645,
          "title": "Inductive Representation Based Graph Convolution Network for Collaborative Filtering. (arXiv:2107.05247v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi-Geng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hui-Chu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>",
          "description": "Visual object localization is the key step in a series of object detection\ntasks. In the literature, high localization accuracy is achieved with the\nmainstream strongly supervised frameworks. However, such methods require\nobject-level annotations and are unable to detect objects of unknown\ncategories. Weakly supervised methods face similar difficulties. In this paper,\na self-paced learning framework is proposed to achieve accurate object\nlocalization on the rank list returned by instance search. The proposed\nframework mines the target instance gradually from the queries and their\ncorresponding top-ranked search results. Since a common instance is shared\nbetween the query and the images in the rank list, the target visual instance\ncan be accurately localized even without knowing what the object category is.\nIn addition to performing localization on instance search, the issue of\nfew-shot object detection is also addressed under the same framework. Superior\nperformance over state-of-the-art methods is observed on both tasks.",
          "link": "http://arxiv.org/abs/2107.05005",
          "publishedOn": "2021-07-13T01:59:32.764Z",
          "wordCount": 590,
          "title": "Towards Accurate Localization by Instance Search. (arXiv:2107.05005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young Kyun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>",
          "description": "Face image retrieval, which searches for images of the same identity from the\nquery input face image, is drawing more attention as the size of the image\ndatabase increases rapidly. In order to conduct fast and accurate retrieval, a\ncompact hash code-based methods have been proposed, and recently, deep face\nimage hashing methods with supervised classification training have shown\noutstanding performance. However, classification-based scheme has a\ndisadvantage in that it cannot reveal complex similarities between face images\ninto the hash code learning. In this paper, we attempt to improve the face\nimage retrieval quality by proposing a Similarity Guided Hashing (SGH) method,\nwhich gently considers self and pairwise-similarity simultaneously. SGH employs\nvarious data augmentations designed to explore elaborate similarities between\nface images, solving both intra and inter identity-wise difficulties. Extensive\nexperimental results on the protocols with existing benchmarks and an\nadditionally proposed large scale higher resolution face image dataset\ndemonstrate that our SGH delivers state-of-the-art retrieval performance.",
          "link": "http://arxiv.org/abs/2107.05025",
          "publishedOn": "2021-07-13T01:59:32.754Z",
          "wordCount": 596,
          "title": "Similarity Guided Deep Face Image Retrieval. (arXiv:2107.05025v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1\">Gabriel de Souza P. Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1\">Sara Rabhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ak_R/0/1/0/all/0/1\">Ronay Ak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md Yasin Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1\">Even Oldridge</a>",
          "description": "Session-based recommendation is an important task for e-commerce services,\nwhere a large number of users browse anonymously or may have very distinct\ninterests for different sessions. In this paper we present one of the winning\nsolutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce\nData Challenge. Our solution was inspired by NLP techniques and consists of an\nensemble of two Transformer architectures - Transformer-XL and XLNet - trained\nwith autoregressive and autoencoding approaches. To leverage most of the rich\ndataset made available for the competition, we describe how we prepared\nmulti-model features by combining tabular events with textual and image\nvectors. We also present a model prediction analysis to better understand the\neffectiveness of our architectures for the session-based recommendation.",
          "link": "http://arxiv.org/abs/2107.05124",
          "publishedOn": "2021-07-13T01:59:32.739Z",
          "wordCount": 593,
          "title": "Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. (arXiv:2107.05124v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1\">Noveen Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "We study the practical consequences of dataset sampling strategies on the\nperformance of recommendation algorithms. Recommender systems are generally\ntrained and evaluated on samples of larger datasets. Samples are often taken in\na naive or ad-hoc fashion: e.g. by sampling a dataset randomly or by selecting\nusers or items with many interactions. As we demonstrate, commonly-used data\nsampling schemes can have significant consequences on algorithm performance --\nmasking performance deficiencies in algorithms or altering the relative\nperformance of algorithms, as compared to models trained on the complete\ndataset. Following this observation, this paper makes the following main\ncontributions: (1) characterizing the effect of sampling on algorithm\nperformance, in terms of algorithm and dataset characteristics (e.g. sparsity\ncharacteristics, sequential dynamics, etc.); and (2) designing SVP-CF, which is\na data-specific sampling strategy, that aims to preserve the relative\nperformance of models after sampling, and is especially suited to long-tail\ninteraction data. Detailed experiments show that SVP-CF is more accurate than\ncommonly used sampling schemes in retaining the relative ranking of different\nrecommendation algorithms.",
          "link": "http://arxiv.org/abs/2107.04984",
          "publishedOn": "2021-07-13T01:59:32.720Z",
          "wordCount": 615,
          "title": "SVP-CF: Selection via Proxy for Collaborative Filtering Data. (arXiv:2107.04984v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haodong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yabo Chu</a>",
          "description": "Social-aware recommendation approaches have been recognized as an effective\nway to solve the data sparsity issue of traditional recommender systems. The\nassumption behind is that the knowledge in social user-user connections can be\nshared and transferred to the domain of user-item interactions, whereby to help\nlearn user preferences. However, most existing approaches merely adopt the\nfirst-order connections among users during transfer learning, ignoring those\nconnections in higher orders. We argue that better recommendation performance\ncan also benefit from high-order social relations. In this paper, we propose a\nnovel Propagation-aware Transfer Learning Network (PTLN) based on the\npropagation of social relations. We aim to better mine the sharing knowledge\nhidden in social networks and thus further improve recommendation performance.\nSpecifically, we explore social influence in two aspects: (a) higher-order\nfriends have been taken into consideration by order bias; (b) different friends\nin the same order will have distinct importance for recommendation by an\nattention mechanism. Besides, we design a novel regularization to bridge the\ngap between social relations and user-item interactions. We conduct extensive\nexperiments on two real-world datasets and beat other counterparts in terms of\nranking accuracy, especially for the cold-start users with few historical\ninteractions.",
          "link": "http://arxiv.org/abs/2107.04846",
          "publishedOn": "2021-07-13T01:59:32.708Z",
          "wordCount": 625,
          "title": "Propagation-aware Social Recommendation by Transfer Learning. (arXiv:2107.04846v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Recommending cold-start items is a long-standing and fundamental challenge in\nrecommender systems. Without any historical interaction on cold-start items, CF\nscheme fails to use collaborative signals to infer user preference on these\nitems. To solve this problem, extensive studies have been conducted to\nincorporate side information into the CF scheme. Specifically, they employ\nmodern neural network techniques (e.g., dropout, consistency constraint) to\ndiscover and exploit the coalition effect of content features and collaborative\nrepresentations. However, we argue that these works less explore the mutual\ndependencies between content features and collaborative representations and\nlack sufficient theoretical supports, thus resulting in unsatisfactory\nperformance. In this work, we reformulate the cold-start item representation\nlearning from an information-theoretic standpoint. It aims to maximize the\nmutual dependencies between item content and collaborative signals.\nSpecifically, the representation learning is theoretically lower-bounded by the\nintegration of two terms: mutual information between collaborative embeddings\nof users and items, and mutual information between collaborative embeddings and\nfeature representations of items. To model such a learning process, we devise a\nnew objective function founded upon contrastive learning and develop a simple\nyet effective Contrastive Learning-based Cold-start Recommendation\nframework(CLCRec). In particular, CLCRec consists of three components:\ncontrastive pair organization, contrastive embedding, and contrastive\noptimization modules. It allows us to preserve collaborative signals in the\ncontent representations for both warm and cold-start items. Through extensive\nexperiments on four publicly accessible datasets, we observe that CLCRec\nachieves significant improvements over state-of-the-art approaches in both\nwarm- and cold-start scenarios.",
          "link": "http://arxiv.org/abs/2107.05315",
          "publishedOn": "2021-07-13T01:59:32.618Z",
          "wordCount": 684,
          "title": "Contrastive Learning for Cold-Start Recommendation. (arXiv:2107.05315v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.07907",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kanglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_G/0/1/0/all/0/1\">Gaofeng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1\">Jiang Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "Single-image HDR reconstruction or inverse tone mapping (iTM) is a\nchallenging task. In particular, recovering information in over-exposed regions\nis extremely difficult because details in such regions are almost completely\nlost. In this paper, we present a deep learning based iTM method that takes\nadvantage of the feature extraction and mapping power of deep convolutional\nneural networks (CNNs) and uses a lightness prior to modulate the CNN to better\nexploit observations in the surrounding areas of the over-exposed regions to\nenhance the quality of HDR image reconstruction. Specifically, we introduce a\nHierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR\ninput and a Lightness Adpative Modulation Network (LAMN) to incorporate the the\nlightness prior knowledge in the inferring process. The HiSN hierarchically\nsynthesizes the high-brightness component and the low-brightness component of\nthe HDR image whilst the LAMN uses a lightness adaptive mask that separates\ndetail-less saturated bright pixels from well-exposed lower light pixels to\nenable HiSN to better infer the missing information, particularly in the\ndifficult over-exposed detail-less areas. We present experimental results to\ndemonstrate the effectiveness of the new technique based on quantitative\nmeasures and visual comparisons. In addition, we present ablation studies of\nHiSN and visualization of the activation maps inside LAMN to help gain a deeper\nunderstanding of the internal working of the new iTM algorithm and explain why\nit can achieve much improved performance over state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2107.07907",
          "publishedOn": "2021-07-19T00:49:04.940Z",
          "wordCount": 683,
          "title": "Lightness Modulated Deep Inverse Tone Mapping. (arXiv:2107.07907v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1\">Elona Shatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.",
          "link": "http://arxiv.org/abs/2107.07786",
          "publishedOn": "2021-07-19T00:49:04.774Z",
          "wordCount": 651,
          "title": "DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:22.536Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+HajiAkhondi_Meybodi_Z/0/1/0/all/0/1\">Zohreh HajiAkhondi-Meybodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abouei_J/0/1/0/all/0/1\">Jamshid Abouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1\">Ming Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "Recently, as a consequence of the COVID-19 pandemic, dependence on\ntelecommunication for remote working and telemedicine has significantly\nincreased. In cellular networks, incorporation of Unmanned Aerial Vehicles\n(UAVs) can result in enhanced connectivity for outdoor users due to the high\nprobability of establishing Line of Sight (LoS) links. The UAV's limited\nbattery life and its signal attenuation in indoor areas, however, make it\ninefficient to manage users' requests in indoor environments. Referred to as\nthe Cluster centric and Coded UAV-aided Femtocaching (CCUF) framework, the\nnetwork's coverage in both indoor and outdoor environments increases via a\ntwo-phase clustering for FAPs' formation and UAVs' deployment. First objective\nis to increase the content diversity. In this context, we propose a coded\ncontent placement in a cluster-centric cellular network, which is integrated\nwith the Coordinated Multi-Point (CoMP) to mitigate the inter-cell interference\nin edge areas. Then, we compute, experimentally, the number of coded contents\nto be stored in each caching node to increase the cache-hit ratio,\nSignal-to-Interference-plus-Noise Ratio (SINR), and cache diversity and\ndecrease the users' access delay and cache redundancy for different content\npopularity profiles. Capitalizing on clustering, our second objective is to\nassign the best caching node to indoor/outdoor users for managing their\nrequests. In this regard, we define the movement speed of ground users as the\ndecision metric of the transmission scheme for serving outdoor users' requests\nto avoid frequent handovers between FAPs and increase the battery life of UAVs.\nSimulation results illustrate that the proposed CCUF implementation increases\nthe cache hit-ratio, SINR, and cache diversity and decrease the users' access\ndelay, cache redundancy and UAVs' energy consumption.",
          "link": "http://arxiv.org/abs/2101.11787",
          "publishedOn": "2021-07-16T00:48:22.525Z",
          "wordCount": 790,
          "title": "Joint Transmission Scheme and Coded Content Placement in Cluster-centric UAV-aided Cellular Networks. (arXiv:2101.11787v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiayi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for\ncontent-based micro-video background music recommendation. CMVAE is a\nhierarchical Bayesian generative model that matches relevant background music\nto a micro-video by projecting these two multimodal inputs into a shared\nlow-dimensional latent space, where the alignment of two corresponding\nembeddings of a matched video-music pair is achieved by cross-generation.\nMoreover, the multimodal information is fused by the product-of-experts (PoE)\nprinciple, where the semantic information in visual and textual modalities of\nthe micro-video are weighted according to their variance estimations such that\nthe modality with a lower noise level is given more weights. Therefore, the\nmicro-video latent variables contain less irrelevant information that results\nin a more robust model generalization. Furthermore, we establish a large-scale\ncontent-based micro-video background music recommendation dataset, TT-150k,\ncomposed of approximately 3,000 different background music clips associated to\n150,000 micro-videos from different users. Extensive experiments on the\nestablished TT-150k dataset demonstrate the effectiveness of the proposed\nmethod. A qualitative assessment of CMVAE by visualizing some recommendation\nresults is also included.",
          "link": "http://arxiv.org/abs/2107.07268",
          "publishedOn": "2021-07-16T00:48:22.242Z",
          "wordCount": 606,
          "title": "Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyoungjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myungchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Laihyuk Park</a>",
          "description": "Video streaming services strive to support high-quality videos at higher\nresolutions and frame rates to improve the quality of experience (QoE).\nHowever, high-quality videos consume considerable amounts of energy on mobile\ndevices. This paper proposes NeuSaver, which reduces the power consumption of\nmobile devices when streaming videos by applying an adaptive frame rate to each\nvideo chunk without compromising user experience. NeuSaver generates an optimal\npolicy that determines the appropriate frame rate for each video chunk using\nreinforcement learning (RL). The RL model automatically learns the policy that\nmaximizes the QoE goals based on previous observations. NeuSaver also uses an\nasynchronous advantage actor-critic algorithm to reinforce the RL model quickly\nand robustly. Streaming servers that support NeuSaver preprocesses videos into\nsegments with various frame rates, which is similar to the process of creating\nvideos with multiple bit rates in dynamic adaptive streaming over HTTP.\nNeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in\nvarious experiments and a user study through four video categories along with\nthe state-of-the-art model. Our experiments showed that NeuSaver effectively\nreduces the power consumption of mobile devices when streaming video by an\naverage of 16.14% and up to 23.12% while achieving high QoE.",
          "link": "http://arxiv.org/abs/2107.07127",
          "publishedOn": "2021-07-16T00:48:22.137Z",
          "wordCount": 672,
          "title": "NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lobbers_S/0/1/0/all/0/1\">Sebastian L&#xf6;bbers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1\">Mathieu Barthet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "Sound synthesiser controls typically correspond to technical parameters of\nsignal processing algorithms rather than intuitive sound descriptors that\nrelate to human perception of sound. This makes it difficult to realise sound\nideas in a straightforward way. Cross-modal mappings, for example between\ngestures and sound, have been suggested as a more intuitive control mechanism.\nA large body of research shows consistency in human associations between sounds\nand shapes. However, the use of drawings to drive sound synthesis has not been\nexplored to its full extent. This paper presents an exploratory study that\nasked participants to sketch visual imagery of sounds with a monochromatic\ndigital drawing interface, with the aim to identify different representational\napproaches and determine whether timbral sound characteristics can be\ncommunicated reliably through visual sketches. Results imply that the\ndevelopment of a synthesiser exploiting sound-shape associations is feasible,\nbut a larger and more focused dataset is needed in followup studies.",
          "link": "http://arxiv.org/abs/2107.07360",
          "publishedOn": "2021-07-16T00:48:22.090Z",
          "wordCount": 599,
          "title": "Sketching sounds: an exploratory study on sound-shape associations. (arXiv:2107.07360v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>",
          "description": "As a key component of talking face generation, lip movements generation\ndetermines the naturalness and coherence of the generated talking face video.\nPrior literature mainly focuses on speech-to-lip generation while there is a\npaucity in text-to-lip (T2L) generation. T2L is a challenging task and existing\nend-to-end works depend on the attention mechanism and autoregressive (AR)\ndecoding manner. However, the AR decoding manner generates current lip frame\nconditioned on frames generated previously, which inherently hinders the\ninference speed, and also has a detrimental effect on the quality of generated\nlip frames due to error propagation. This encourages the research of parallel\nT2L generation. In this work, we propose a novel parallel decoding model for\nhigh-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we\npredict the duration of the encoded linguistic features and model the target\nlip frames conditioned on the encoded linguistic features with their duration\nin a non-autoregressive manner. Furthermore, we incorporate the structural\nsimilarity index loss and adversarial learning to improve perceptual quality of\ngenerated lip frames and alleviate the blurry prediction problem. Extensive\nexperiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L\ngenerates lip movements with competitive quality compared with the\nstate-of-the-art AR T2L model DualLip and exceeds the baseline AR model\nTransformerT2L by a notable margin benefiting from the mitigation of the error\npropagation problem; and 2) exhibits distinct superiority in inference speed\n(an average speedup of 19$\\times$ than DualLip on TCD-TIMIT).",
          "link": "http://arxiv.org/abs/2107.06831",
          "publishedOn": "2021-07-15T01:59:02.205Z",
          "wordCount": 668,
          "title": "High-Speed and High-Quality Text-to-Lip Generation. (arXiv:2107.06831v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Trinh Man Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjia Zhou</a>",
          "description": "COVID-19 leads to the high demand for remote interactive systems ever seen.\nOne of the key elements of these systems is video streaming, which requires a\nvery high network bandwidth due to its specific real-time demand, especially\nwith high-resolution video. Existing video compression methods are struggling\nin the trade-off between video quality and the speed requirement. Addressed\nthat the background information rarely changes in most remote meeting cases, we\nintroduce a Region-Of-Interests (ROI) based video compression framework (named\nRCLC) that leverages the cutting-edge learning-based and conventional\ntechnologies. In RCLC, each coming frame is marked as a background-updating\n(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the\nBU frame is compressed with low-quality and high-compression, while the ROI\nfrom RU-frame is compressed with high-quality and low-compression. The\nlearning-based methods are applied to detect the ROI, blend background-ROI, and\nenhance video quality. The experimental results show that our RCLC can reduce\nup to 32.55\\% BD-rate for the ROI region compared to H.265 video codec under a\nsimilar compression time with 1080p resolution.",
          "link": "http://arxiv.org/abs/2107.06492",
          "publishedOn": "2021-07-15T01:59:02.136Z",
          "wordCount": 663,
          "title": "RCLC: ROI-based joint conventional and learning video compression. (arXiv:2107.06492v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinda Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>",
          "description": "Fine-grained image recognition is challenging because discriminative clues\nare usually fragmented, whether from a single image or multiple images. Despite\ntheir significant improvements, most existing methods still focus on the most\ndiscriminative parts from a single image, ignoring informative details in other\nregions and lacking consideration of clues from other associated images. In\nthis paper, we analyze the difficulties of fine-grained image recognition from\na new perspective and propose a transformer architecture with the peak\nsuppression module and knowledge guidance module, which respects the\ndiversification of discriminative features in a single image and the\naggregation of discriminative clues among multiple images. Specifically, the\npeak suppression module first utilizes a linear projection to convert the input\nimage into sequential tokens. It then blocks the token based on the attention\nresponse generated by the transformer encoder. This module penalizes the\nattention to the most discriminative parts in the feature learning process,\ntherefore, enhancing the information exploitation of the neglected regions. The\nknowledge guidance module compares the image-based representation generated\nfrom the peak suppression module with the learnable knowledge embedding set to\nobtain the knowledge response coefficients. Afterwards, it formalizes the\nknowledge learning as a classification problem using response coefficients as\nthe classification scores. Knowledge embeddings and image-based representations\nare updated during training so that the knowledge embedding includes\ndiscriminative clues for different images. Finally, we incorporate the acquired\nknowledge embeddings into the image-based representations as comprehensive\nrepresentations, leading to significantly higher performance. Extensive\nevaluations on the six popular datasets demonstrate the advantage of the\nproposed method.",
          "link": "http://arxiv.org/abs/2107.06538",
          "publishedOn": "2021-07-15T01:59:02.048Z",
          "wordCount": 693,
          "title": "Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition. (arXiv:2107.06538v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qingyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargteil_A/0/1/0/all/0/1\">Adam Bargteil</a>",
          "description": "We explore computational approaches for visual guidance to aid in creating\naesthetically pleasing art and graphic design. Our work complements and builds\non previous work that developed models for how humans look at images. Our\napproach comprises three steps. First, we collected a dataset of art\nmasterpieces and labeled the visual fixations with state-of-art vision models.\nSecond, we clustered the visual guidance templates of the art masterpieces with\nunsupervised learning. Third, we developed a pipeline using generative\nadversarial networks to learn the principles of visual guidance and that can\nproduce aesthetically pleasing layouts. We show that the aesthetic visual\nguidance principles can be learned and integrated into a high-dimensional model\nand can be queried by the features of graphic elements. We evaluate our\napproach by generating layouts on various drawings and graphic designs.\nMoreover, our model considers the color and structure of graphic elements when\ngenerating layouts. Consequently, we believe our tool, which generates multiple\naesthetic layout options in seconds, can help artists create beautiful art and\ngraphic designs.",
          "link": "http://arxiv.org/abs/2107.06262",
          "publishedOn": "2021-07-14T01:41:48.550Z",
          "wordCount": 607,
          "title": "Learning Aesthetic Layouts via Visual Guidance. (arXiv:2107.06262v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoxin Liu</a>",
          "description": "Domain generalization (DG) aims to help models trained on a set of source\ndomains generalize better on unseen target domains. The performances of current\nDG methods largely rely on sufficient labeled data, which however are usually\ncostly or unavailable. While unlabeled data are far more accessible, we seek to\nexplore how unsupervised learning can help deep models generalizes across\ndomains. Specifically, we study a novel generalization problem called\nunsupervised domain generalization, which aims to learn generalizable models\nwith unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised\nLearning (DIUL) method to cope with the significant and misleading\nheterogeneity within unlabeled data and severe distribution shifts between\nsource and target data. Surprisingly we observe that DIUL can not only\ncounterbalance the scarcity of labeled data but also further strengthen the\ngeneralization ability of models when the labeled data are sufficient. As a\npretraining approach, DIUL shows superior to ImageNet pretraining protocol even\nwhen the available data are unlabeled and of a greatly smaller amount compared\nto ImageNet. Extensive experiments clearly demonstrate the effectiveness of our\nmethod compared with state-of-the-art unsupervised learning counterparts.",
          "link": "http://arxiv.org/abs/2107.06219",
          "publishedOn": "2021-07-14T01:41:48.534Z",
          "wordCount": 625,
          "title": "Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization. (arXiv:2107.06219v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gunjan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "Dance and music typically go hand in hand. The complexities in dance, music,\nand their synchronisation make them fascinating to study from a computational\ncreativity perspective. While several works have looked at generating dance for\na given music, automatically generating music for a given dance remains\nunder-explored. This capability could have several creative expression and\nentertainment applications. We present some early explorations in this\ndirection. We present a search-based offline approach that generates music\nafter processing the entire dance video and an online approach that uses a deep\nneural network to generate music on-the-fly as the video proceeds. We compare\nthese approaches to a strong heuristic baseline via human studies and present\nour findings. We have integrated our online approach in a live demo! A video of\nthe demo can be found here:\nhttps://sites.google.com/view/dance2music/live-demo.",
          "link": "http://arxiv.org/abs/2107.06252",
          "publishedOn": "2021-07-14T01:41:48.495Z",
          "wordCount": 564,
          "title": "Dance2Music: Automatic Dance-driven Music Generation. (arXiv:2107.06252v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1\">Rodrigo Castellon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We demonstrate that language models pre-trained on codified\n(discretely-encoded) music audio learn representations that are useful for\ndownstream MIR tasks. Specifically, we explore representations from Jukebox\n(Dhariwal et al. 2020): a music generation system containing a language model\ntrained on codified audio from 1M songs. To determine if Jukebox's\nrepresentations contain useful information for MIR, we use them as input\nfeatures to train shallow models on several MIR tasks. Relative to\nrepresentations from conventional MIR models which are pre-trained on tagging,\nwe find that using representations from Jukebox as input features yields 30%\nstronger performance on average across four MIR tasks: tagging, genre\nclassification, emotion recognition, and key detection. For key detection, we\nobserve that representations from Jukebox are considerably stronger than those\nfrom models pre-trained on tagging, suggesting that pre-training via codified\naudio language modeling may address blind spots in conventional approaches. We\ninterpret the strength of Jukebox's representations as evidence that modeling\naudio instead of tags provides richer representations for MIR.",
          "link": "http://arxiv.org/abs/2107.05677",
          "publishedOn": "2021-07-14T01:41:48.398Z",
          "wordCount": 621,
          "title": "Codified audio language modeling learns useful representations for music information retrieval. (arXiv:2107.05677v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Recommending cold-start items is a long-standing and fundamental challenge in\nrecommender systems. Without any historical interaction on cold-start items, CF\nscheme fails to use collaborative signals to infer user preference on these\nitems. To solve this problem, extensive studies have been conducted to\nincorporate side information into the CF scheme. Specifically, they employ\nmodern neural network techniques (e.g., dropout, consistency constraint) to\ndiscover and exploit the coalition effect of content features and collaborative\nrepresentations. However, we argue that these works less explore the mutual\ndependencies between content features and collaborative representations and\nlack sufficient theoretical supports, thus resulting in unsatisfactory\nperformance. In this work, we reformulate the cold-start item representation\nlearning from an information-theoretic standpoint. It aims to maximize the\nmutual dependencies between item content and collaborative signals.\nSpecifically, the representation learning is theoretically lower-bounded by the\nintegration of two terms: mutual information between collaborative embeddings\nof users and items, and mutual information between collaborative embeddings and\nfeature representations of items. To model such a learning process, we devise a\nnew objective function founded upon contrastive learning and develop a simple\nyet effective Contrastive Learning-based Cold-start Recommendation\nframework(CLCRec). In particular, CLCRec consists of three components:\ncontrastive pair organization, contrastive embedding, and contrastive\noptimization modules. It allows us to preserve collaborative signals in the\ncontent representations for both warm and cold-start items. Through extensive\nexperiments on four publicly accessible datasets, we observe that CLCRec\nachieves significant improvements over state-of-the-art approaches in both\nwarm- and cold-start scenarios.",
          "link": "http://arxiv.org/abs/2107.05315",
          "publishedOn": "2021-07-13T01:59:32.896Z",
          "wordCount": 684,
          "title": "Contrastive Learning for Cold-Start Recommendation. (arXiv:2107.05315v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Ahmed Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moy_F/0/1/0/all/0/1\">Foong Ming Moy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_N/0/1/0/all/0/1\">Nadine Kong</a>",
          "description": "Obesity is known to lower the quality of life substantially. It is often\nassociated with increased chances of non-communicable diseases such as\ndiabetes, cardiovascular problems, various cancers, etc. Evidence suggests that\ndiet-related mobile applications play a vital role in assisting individuals in\nmaking healthier choices and keeping track of food intake. However, due to an\nabundance of similar applications, it becomes pertinent to evaluate each of\nthem in terms of functionality, usability, and possible design issues to truly\ndetermine state-of-the-art solutions for the future. Since these applications\ninvolve implementing multiple user requirements and recommendations from\ndifferent dietitians, the evaluation becomes quite complex. Therefore, this\nstudy aims to review existing dietary applications at length to highlight key\nfeatures and problems that enhance or undermine an application's usability. For\nthis purpose, we have examined the published literature from various scientific\ndatabases of the PUBMED, CINAHL (January 2010-December 2019) and Science Direct\n(2010-2019). We followed PRISMA guidelines, and out of our findings, fifty-six\nprimary studies met our inclusion criteria after identification, screening,\neligibility and full-text evaluation. We analyzed 35 apps from the selected\nstudies and extracted the data of each of the identified apps.Following our\ndetailed analysis on the comprehensiveness of freely available mHealth\napplications, we specified potential future research challenges and stated\nrecommendations to help grow clinically accurate diet-related applications.",
          "link": "http://arxiv.org/abs/2008.09883",
          "publishedOn": "2021-07-13T01:59:32.823Z",
          "wordCount": 720,
          "title": "A Review of Critical Features and General Issues of Freely Available mHealth Apps For Dietary Assessment. (arXiv:2008.09883v4 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yi-Hui Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1\">I-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chin-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1\">Joann Ching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "This paper presents an attempt to employ the mask language modeling approach\nof BERT to pre-train a 12-layer Transformer model over 4,166 pieces of\npolyphonic piano MIDI files for tackling a number of symbolic-domain\ndiscriminative music understanding tasks. These include two note-level\nclassification tasks, i.e., melody extraction and velocity prediction, as well\nas two sequence-level classification tasks, i.e., composer classification and\nemotion classification. We find that, given a pre-trained Transformer, our\nmodels outperform recurrent neural network based baselines with less than 10\nepochs of fine-tuning. Ablation studies show that the pre-training remains\neffective even if none of the MIDI data of the downstream tasks are seen at the\npre-training stage, and that freezing the self-attention layers of the\nTransformer at the fine-tuning stage slightly degrades performance. All the\nfive datasets employed in this work are publicly available, as well as\ncheckpoints of our pre-trained and fine-tuned models. As such, our research can\nbe taken as a benchmark for symbolic-domain music understanding.",
          "link": "http://arxiv.org/abs/2107.05223",
          "publishedOn": "2021-07-13T01:59:32.810Z",
          "wordCount": 603,
          "title": "MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding. (arXiv:2107.05223v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1\">Shivangi Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Midoglu_C/0/1/0/all/0/1\">Cise Midoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Nguyen_D/0/1/0/all/0/1\">Duc-Tien Dang-Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael Alexander Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">Paal Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Niessner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bregler_C/0/1/0/all/0/1\">Chris Bregler</a>",
          "description": "Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\nmanipulations of multimedia content. Cheapfakes are known to be more prevalent\nthan deepfakes. Cheapfake media can be created using editing software for\nimage/video manipulations, or even without using any software, by simply\naltering the context of an image/video by sharing the media alongside\nmisleading claims. This alteration of context is referred to as out-of-context\n(OOC) misuse} of media. OOC media is much harder to detect than fake media,\nsince the images and videos are not tampered. In this challenge, we focus on\ndetecting OOC images, and more specifically the misuse of real photographs with\nconflicting image captions in news items. The aim of this challenge is to\ndevelop and benchmark models that can be used to detect whether given samples\n(news image and associated captions) are OOC, based on the recently compiled\nCOSMOS dataset.",
          "link": "http://arxiv.org/abs/2107.05297",
          "publishedOn": "2021-07-13T01:59:32.800Z",
          "wordCount": 577,
          "title": "MMSys'21 Grand Challenge on Detecting Cheapfakes. (arXiv:2107.05297v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-13T01:59:32.430Z",
          "wordCount": 612,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sunami_T/0/1/0/all/0/1\">Tomoya Sunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itahara_S/0/1/0/all/0/1\">Sohei Itahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koda_Y/0/1/0/all/0/1\">Yusuke Koda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1\">Takayuki Nishio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1\">Koji Yamamoto</a>",
          "description": "This paper demonstrates the feasibility of received power strength indicator\n(RSSI)-based single-antenna localization (R-SAL) with decimeter-level\nlocalization accuracy. To achieve decimeter-level accuracy, either fine-grained\nradio frequency (RF) information (e.g., channel state information) or\ncoarse-grained RF information (e.g., RSSI) from more than multiple antennas is\nrequired. Meanwhile, owing to deficiency of single-antenna RSSI which only\nindicates a distance between a receiver and a transmitter, realizing\nfine-grained localization accuracy with single coarse-grained RF information is\nchallenging. Our key idea to address this challenge is to leverage computer\nvision (CV) and to estimate the most likely Fresnel zone between the receiver\nand transmitter, where the role of RSSI is to detect blockage timings.\nSpecifically, historical positions of an obstacle that dynamically blocks the\nFresnel zone are detected by the CV technique, and we estimate positions at\nwhich a blockage starts and ends via a time series of RSSI. These estimated\nobstacle positions, in principle, coincide with points on the Fresnel zone\nboundaries, enabling the estimation of the Fresnel zone and localization of the\ntransmitter. The experimental evaluation revealed that the proposed R-SAL\nachieved decimeter-level localization in an indoor environment, which is\ncomparable to that of a simple previous RSSI-based localization with three\nreceivers.",
          "link": "http://arxiv.org/abs/2107.04770",
          "publishedOn": "2021-07-13T01:59:32.408Z",
          "wordCount": 642,
          "title": "Computer Vision-assisted Decimeter-level Single-antenna RSSI Localization Harnessing Dynamic Blockage Events. (arXiv:2107.04770v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1\">Bing-Kun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>",
          "description": "Video question answering is a challenging task, which requires agents to be\nable to understand rich video contents and perform spatial-temporal reasoning.\nHowever, existing graph-based methods fail to perform multi-step reasoning\nwell, neglecting two properties of VideoQA: (1) Even for the same video,\ndifferent questions may require different amount of video clips or objects to\ninfer the answer with relational reasoning; (2) During reasoning, appearance\nand motion features have complicated interdependence which are correlated and\ncomplementary to each other. Based on these observations, we propose a\nDual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an\nend-to-end fashion. The first contribution of our DualVGR is the design of an\nexplainable Query Punishment Module, which can filter out irrelevant visual\nfeatures through multiple cycles of reasoning. The second contribution is the\nproposed Video-based Multi-view Graph Attention Network, which captures the\nrelations between appearance and motion features. Our DualVGR network achieves\nstate-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and\ndemonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is\navailable at https://github.com/MMIR/DualVGR-VideoQA.",
          "link": "http://arxiv.org/abs/2107.04768",
          "publishedOn": "2021-07-13T01:59:32.384Z",
          "wordCount": 628,
          "title": "DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering. (arXiv:2107.04768v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shubham_K/0/1/0/all/0/1\">Kumar Shubham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnihotri_P/0/1/0/all/0/1\">Prateek Agnihotri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Movva_N/0/1/0/all/0/1\">Nitin D. Movva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessenyei_S/0/1/0/all/0/1\">Szilard Bessenyei</a>",
          "description": "It is easier to hear birds than see them, however, they still play an\nessential role in nature and they are excellent indicators of deteriorating\nenvironmental quality and pollution. Recent advances in Machine Learning and\nConvolutional Neural Networks allow us to detect and classify bird sounds, by\ndoing this, we can assist researchers in monitoring the status and trends of\nbird populations and biodiversity in ecosystems. We propose a sound detection\nand classification pipeline for analyzing complex soundscape recordings and\nidentify birdcalls in the background. Our pipeline learns from weak labels,\nclassifies fine-grained bird vocalizations in the wild, and is robust against\nbackground sounds (e.g., airplanes, rain, etc). Our solution achieved 10th\nplace of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle.",
          "link": "http://arxiv.org/abs/2107.04878",
          "publishedOn": "2021-07-13T01:59:32.340Z",
          "wordCount": 589,
          "title": "Weakly-Supervised Classification and Detection of Bird Sounds in the Wild. A BirdCLEF 2021 Solution. (arXiv:2107.04878v1 [cs.SD])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.07797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1\">Tien-Tsin Wong</a>",
          "description": "Graph convolutional networks have significantly improved 3D human pose\nestimation by representing the human skeleton as an undirected graph. However,\nthis representation fails to reflect the articulated characteristic of human\nskeletons as the hierarchical orders among the joints are not explicitly\npresented. In this paper, we propose to represent the human skeleton as a\ndirected graph with the joints as nodes and bones as edges that are directed\nfrom parent joints to child joints. By so doing, the directions of edges can\nexplicitly reflect the hierarchical relationships among the nodes. Based on\nthis representation, we adopt the spatial-temporal directed graph convolution\n(ST-DGConv) to extract features from 2D poses represented in a temporal\nsequence of directed graphs. We further propose a spatial-temporal conditional\ndirected graph convolution (ST-CondDGConv) to leverage varying non-local\ndependence for different poses by conditioning the graph topology on input\nposes. Altogether, we form a U-shaped network with ST-DGConv and ST-CondDGConv\nlayers, named U-shaped Conditional Directed Graph Convolutional Network\n(U-CondDGCN), for 3D human pose estimation from monocular videos. To evaluate\nthe effectiveness of our U-CondDGCN, we conducted extensive experiments on two\nchallenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both\nquantitative and qualitative results show that our method achieves top\nperformance. Also, ablation studies show that directed graphs can better\nexploit the hierarchy of articulated human skeletons than undirected graphs,\nand the conditional connections can yield adaptive graph topologies for\ndifferent kinds of poses.",
          "link": "http://arxiv.org/abs/2107.07797",
          "publishedOn": "2021-07-19T00:49:08.141Z",
          "wordCount": 683,
          "title": "Conditional Directed Graph Convolution for 3D Human Pose Estimation. (arXiv:2107.07797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1\">Vivien Sainte Fare Garnot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>",
          "description": "Unprecedented access to multi-temporal satellite imagery has opened new\nperspectives for a variety of Earth observation tasks. Among them,\npixel-precise panoptic segmentation of agricultural parcels has major economic\nand environmental implications. While researchers have explored this problem\nfor single images, we argue that the complex temporal patterns of crop\nphenology are better addressed with temporal sequences of images. In this\npaper, we present the first end-to-end, single-stage method for panoptic\nsegmentation of Satellite Image Time Series (SITS). This module can be combined\nwith our novel image sequence encoding network which relies on temporal\nself-attention to extract rich and adaptive multi-scale spatio-temporal\nfeatures. We also introduce PASTIS, the first open-access SITS dataset with\npanoptic annotations. We demonstrate the superiority of our encoder for\nsemantic segmentation against multiple competing architectures, and set up the\nfirst state-of-the-art of panoptic segmentation of SITS. Our implementation and\nPASTIS are publicly available.",
          "link": "http://arxiv.org/abs/2107.07933",
          "publishedOn": "2021-07-19T00:49:08.120Z",
          "wordCount": 605,
          "title": "Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyeon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyung-Kwon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaemin Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngtaek Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jinwook Seo</a>",
          "description": "We propose Steadiness and Cohesiveness, two novel metrics to measure the\ninter-cluster reliability of multidimensional projection (MDP), specifically\nhow well the inter-cluster structures are preserved between the original\nhigh-dimensional space and the low-dimensional projection space. Measuring\ninter-cluster reliability is crucial as it directly affects how well\ninter-cluster tasks (e.g., identifying cluster relationships in the original\nspace from a projected view) can be conducted; however, despite the importance\nof inter-cluster tasks, we found that previous metrics, such as Trustworthiness\nand Continuity, fail to measure inter-cluster reliability. Our metrics consider\ntwo aspects of the inter-cluster reliability: Steadiness measures the extent to\nwhich clusters in the projected space form clusters in the original space, and\nCohesiveness measures the opposite. They extract random clusters with arbitrary\nshapes and positions in one space and evaluate how much the clusters are\nstretched or dispersed in the other space. Furthermore, our metrics can\nquantify pointwise distortions, allowing for the visualization of inter-cluster\nreliability in a projection, which we call a reliability map. Through\nquantitative experiments, we verify that our metrics precisely capture the\ndistortions that harm inter-cluster reliability while previous metrics have\ndifficulty capturing the distortions. A case study also demonstrates that our\nmetrics and the reliability map 1) support users in selecting the proper\nprojection techniques or hyperparameters and 2) prevent misinterpretation while\nperforming inter-cluster tasks, thus allow an adequate identification of\ninter-cluster structure.",
          "link": "http://arxiv.org/abs/2107.07859",
          "publishedOn": "2021-07-19T00:49:08.097Z",
          "wordCount": 686,
          "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections. (arXiv:2107.07859v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_Y/0/1/0/all/0/1\">Yugo Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_D/0/1/0/all/0/1\">Delong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_Y/0/1/0/all/0/1\">Yukinobu Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinami_R/0/1/0/all/0/1\">Ryota Hinami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishiwatari_S/0/1/0/all/0/1\">Shonosuke Ishiwatari</a>",
          "description": "Japanese comics (called manga) are traditionally created in monochrome\nformat. In recent years, in addition to monochrome comics, full color comics, a\nmore attractive medium, have appeared. Unfortunately, color comics require\nmanual colorization, which incurs high labor costs. Although automatic\ncolorization methods have been recently proposed, most of them are designed for\nillustrations, not for comics. Unlike illustrations, since comics are composed\nof many consecutive images, the painting style must be consistent. To realize\nconsistent colorization, we propose here a semi-automatic colorization method\nbased on generative adversarial networks (GAN); the method learns the painting\nstyle of a specific comic from small amount of training data. The proposed\nmethod takes a pair of a screen tone image and a flat colored image as input,\nand outputs a colorized image. Experiments show that the proposed method\nachieves better performance than the existing alternatives.",
          "link": "http://arxiv.org/abs/2107.07943",
          "publishedOn": "2021-07-19T00:49:08.092Z",
          "wordCount": 595,
          "title": "Painting Style-Aware Manga Colorization Based on Generative Adversarial Networks. (arXiv:2107.07943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "This paper presents a novel unsupervised approach to reconstruct human shape\nand pose from noisy point cloud. Traditional approaches search for\ncorrespondences and conduct model fitting iteratively where a good\ninitialization is critical. Relying on large amount of dataset with\nground-truth annotations, recent learning-based approaches predict\ncorrespondences for every vertice on the point cloud; Chamfer distance is\nusually used to minimize the distance between a deformed template model and the\ninput point cloud. However, Chamfer distance is quite sensitive to noise and\noutliers, thus could be unreliable to assign correspondences. To address these\nissues, we model the probability distribution of the input point cloud as\ngenerated from a parametric human model under a Gaussian Mixture Model. Instead\nof explicitly aligning correspondences, we treat the process of correspondence\nsearch as an implicit probabilistic association by updating the posterior\nprobability of the template model given the input. A novel unsupervised loss is\nfurther derived that penalizes the discrepancy between the deformed template\nand the input point cloud conditioned on the posterior probability. Our\napproach is very flexible, which works with both complete point cloud and\nincomplete ones including even a single depth image as input. Our network is\ntrained from scratch with no need to warm-up the network with supervised data.\nCompared to previous unsupervised methods, our method shows the capability to\ndeal with substantial noise and outliers. Extensive experiments conducted on\nvarious public synthetic datasets as well as a very noisy real dataset (i.e.\nCMU Panoptic) demonstrate the superior performance of our approach over the\nstate-of-the-art methods. Code can be found\n\\url{https://github.com/wangsen1312/unsupervised3dhuman.git}",
          "link": "http://arxiv.org/abs/2107.07539",
          "publishedOn": "2021-07-19T00:49:07.772Z",
          "wordCount": 701,
          "title": "Unsupervised 3D Human Mesh Recovery from Noisy Point Clouds. (arXiv:2107.07539v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvaraju_R/0/1/0/all/0/1\">Ramprasaath R. Selvaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1\">Akhilesh Deepak Gotmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>",
          "description": "Large-scale vision and language representation learning has shown promising\nimprovements on various vision-language tasks. Most existing methods employ a\ntransformer-based multimodal encoder to jointly model visual tokens\n(region-based image features) and word tokens. Because the visual tokens and\nword tokens are unaligned, it is challenging for the multimodal encoder to\nlearn image-text interactions. In this paper, we introduce a contrastive loss\nto ALign the image and text representations BEfore Fusing (ALBEF) them through\ncross-modal attention, which enables more grounded vision and language\nrepresentation learning. Unlike most existing methods, our method does not\nrequire bounding box annotations nor high-resolution images. In order to\nimprove learning from noisy web data, we propose momentum distillation, a\nself-training method which learns from pseudo-targets produced by a momentum\nmodel. We provide a theoretical analysis of ALBEF from a mutual information\nmaximization perspective, showing that different training tasks can be\ninterpreted as different ways to generate views for an image-text pair. ALBEF\nachieves state-of-the-art performance on multiple downstream vision-language\ntasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained\non orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves\nabsolute improvements of 2.37% and 3.84% compared to the state-of-the-art,\nwhile enjoying faster inference speed. Code and pre-trained models are\navailable at https://github.com/salesforce/ALBEF/.",
          "link": "http://arxiv.org/abs/2107.07651",
          "publishedOn": "2021-07-19T00:49:07.747Z",
          "wordCount": 662,
          "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (arXiv:2107.07651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "In recent years, deep learning has made brilliant achievements in image\nclassification. However, image classification of small datasets is still not\nobtained good research results. This article first briefly explains the\napplication and characteristics of convolutional neural networks and visual\ntransformers. Meanwhile, the influence of small data set on classification and\nthe solution are introduced. Then a series of experiments are carried out on\nthe small datasets by using various models, and the problems of some models in\nthe experiments are discussed. Through the comparison of experimental results,\nthe recommended deep learning model is given according to the model application\nenvironment. Finally, we give directions for future work.",
          "link": "http://arxiv.org/abs/2107.07699",
          "publishedOn": "2021-07-19T00:49:07.742Z",
          "wordCount": 571,
          "title": "A Comparison of Deep Learning Classification Methods on Small-scale Image Data set: from Converlutional Neural Networks to Visual Transformers. (arXiv:2107.07699v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10785",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1\">Reza Godaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1\">Faezeh Toutounian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>",
          "description": "In this paper we tackle two important challenges related to the accurate\npartial singular value decomposition (SVD) and numerical rank estimation of a\nhuge matrix to use in low-rank learning problems in a fast way. We use the\nconcepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process\nas well as Ritz vectors to achieve these goals. Our experiments identify\nvarious advantages of the proposed methods compared to traditional and\nrandomized SVD (R-SVD) methods with respect to the accuracy of the singular\nvalues and corresponding singular vectors computed in a similar execution time.\nThe proposed methods are appropriate for applications involving huge matrices\nwhere accuracy in all spectrum of the desired singular values, and also all of\ncorresponding singular vectors is essential. We evaluate our method in the real\napplication of Riemannian similarity learning (RSL) between two various image\ndatasets of MNIST and USPS.",
          "link": "http://arxiv.org/abs/2104.10785",
          "publishedOn": "2021-07-19T00:49:07.530Z",
          "wordCount": 611,
          "title": "Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_D/0/1/0/all/0/1\">Dong Yue</a>",
          "description": "In recent years, how to strike a good trade-off between accuracy and\ninference speed has become the core issue for real-time semantic segmentation\napplications, which plays a vital role in real-world scenarios such as\nautonomous driving systems and drones. In this study, we devise a novel\nlightweight network using a multi-scale context fusion (MSCFNet) scheme, which\nexplores an asymmetric encoder-decoder architecture to dispose this problem.\nMore specifically, the encoder adopts some developed efficient asymmetric\nresidual (EAR) modules, which are composed of factorization depth-wise\nconvolution and dilation convolution. Meanwhile, instead of complicated\ncomputation, simple deconvolution is applied in the decoder to further reduce\nthe amount of parameters while still maintaining high segmentation accuracy.\nAlso, MSCFNet has branches with efficient attention modules from different\nstages of the network to well capture multi-scale contextual information. Then\nwe combine them before the final classification to enhance the expression of\nthe features and improve the segmentation efficiency. Comprehensive experiments\non challenging datasets have demonstrated that the proposed MSCFNet, which\ncontains only 1.15M parameters, achieves 71.9\\% Mean IoU on the Cityscapes\ntesting dataset and can run at over 50 FPS on a single Titan XP GPU\nconfiguration.",
          "link": "http://arxiv.org/abs/2103.13044",
          "publishedOn": "2021-07-19T00:49:07.523Z",
          "wordCount": 679,
          "title": "MSCFNet: A Lightweight Network With Multi-Scale Context Fusion for Real-Time Semantic Segmentation. (arXiv:2103.13044v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_W/0/1/0/all/0/1\">William Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_G/0/1/0/all/0/1\">Glen Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1\">Robert Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricardo_F/0/1/0/all/0/1\">Frederick Ricardo</a>",
          "description": "Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains. Adversarial image synthesis has drawn increasing\nattention and made tremendous progress in recent years because of its wide\nrange of applications in many computer vision and image processing problems.\nAmong the many applications of GAN, image synthesis is the most well-studied\none, and research in this area has already demonstrated the great potential of\nusing GAN in image synthesis. In this paper, we provide a taxonomy of methods\nused in image synthesis, review different models for text-to-image synthesis\nand image-to-image translation, and discuss some evaluation metrics as well as\npossible future research directions in image synthesis with GAN.",
          "link": "http://arxiv.org/abs/2106.16056",
          "publishedOn": "2021-07-19T00:49:07.510Z",
          "wordCount": 581,
          "title": "A Survey on Adversarial Image Synthesis. (arXiv:2106.16056v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "Humans are incredibly good at transferring knowledge from one domain to\nanother, enabling rapid learning of new tasks. Likewise, transfer learning has\nenabled enormous success in many computer vision problems using pretraining.\nHowever, the benefits of transfer in multi-domain learning, where a network\nlearns multiple tasks defined by different datasets, has not been adequately\nstudied. Learning multiple domains could be beneficial or these domains could\ninterfere with each other given limited network capacity. In this work, we\ndecipher the conditions where interference and knowledge transfer occur in\nmulti-domain learning. We propose new metrics disentangling interference and\ntransfer and set up experimental protocols. We further examine the roles of\nnetwork capacity, task grouping, and dynamic loss weighting in reducing\ninterference and facilitating transfer. We demonstrate our findings on the\nCIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.05445",
          "publishedOn": "2021-07-19T00:49:07.433Z",
          "wordCount": 591,
          "title": "Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davis_B/0/1/0/all/0/1\">Brian Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morse%7F_B/0/1/0/all/0/1\">Bryan Morse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price%7F_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tensmeyer%7F_C/0/1/0/all/0/1\">Chris Tensmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiginton_C/0/1/0/all/0/1\">Curtis Wiginton</a>",
          "description": "We address the problem of form understanding: finding text entities and the\nrelationships/links between them in form images. The proposed FUDGE model\nformulates this problem on a graph of text elements (the vertices) and uses a\nGraph Convolutional Network to predict changes to the graph. The initial\nvertices are detected text lines and do not necessarily correspond to the final\ntext entities, which can span multiple lines. Also, initial edges contain many\nfalse-positive relationships. FUDGE edits the graph structure by combining text\nsegments (graph vertices) and pruning edges in an iterative fashion to obtain\nthe final text entities and relationships. While recent work in this area has\nfocused on leveraging large-scale pre-trained Language Models (LM), FUDGE\nachieves almost the same level of entity linking performance on the FUNSD\ndataset by learning only visual features from the (small) provided training\nset. FUDGE can be applied on forms where text recognition is difficult (e.g.\ndegraded or historical forms) and on forms in resource-poor languages where\npre-training such LMs is challenging. FUDGE is state-of-the-art on the\nhistorical NAF dataset.",
          "link": "http://arxiv.org/abs/2105.08194",
          "publishedOn": "2021-07-19T00:49:06.730Z",
          "wordCount": 652,
          "title": "Visual FUDGE: Form Understanding via Dynamic Graph Editing. (arXiv:2105.08194v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ask_K/0/1/0/all/0/1\">Katrina Ask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Maheen Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1\">Pia Haubro Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>",
          "description": "Orthopedic disorders are a common cause for euthanasia among horses, which\noften could have been avoided with earlier detection. These conditions often\ncreate varying degrees of subtle but long-term pain. It is challenging to train\na visual pain recognition method with video data depicting such pain, since the\nresulting pain behavior also is subtle, sparsely appearing, and varying, making\nit challenging for even an expert human labeler to provide accurate\nground-truth for the data. We show that transferring features from a dataset of\nhorses with acute nociceptive pain (where labeling is less ambiguous) can aid\nthe learning to recognize more complex orthopedic pain. Moreover, we present a\nhuman expert baseline for the problem, as well as an extensive empirical study\nof various domain transfer methods and of what is detected by the pain\nrecognition method trained on acute pain in the orthopedic dataset. Finally,\nthis is accompanied with a discussion around the challenges posed by real-world\nanimal behavior datasets and how best practices can be established for similar\nfine-grained action recognition tasks. Our code is available at\nhttps://github.com/sofiabroome/painface-recognition.",
          "link": "http://arxiv.org/abs/2105.10313",
          "publishedOn": "2021-07-19T00:49:06.704Z",
          "wordCount": 665,
          "title": "Sharing Pain: Using Domain Transfer Between Pain Types for Recognition of Sparse Pain Expressions in Horses. (arXiv:2105.10313v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-19T00:49:06.626Z",
          "wordCount": 620,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03686",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1\">Udaya S.K.P. Miriya Thanthrige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1\">Aydin Sezgin</a>",
          "description": "We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-input and\nmultiple-output (MIMO) wireless radar. Here, the strong clutter due to the\nreflection of the layered structure's surface often makes the detection of the\ndefects challenging. Thus, sophisticated signal separation methods are required\nfor improved defect detection. In many scenarios, the number of defects that we\nare interested in is limited and the signaling response of the layered\nstructure can be modeled as a low-rank structure. Therefore, we propose joint\nrank and sparsity minimization for defect detection. In particular, we propose\na non-convex approach based on the iteratively reweighted nuclear and\n$\\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy\ncompared to the conventional nuclear norm and $\\ell_1-$norm minimization. To\nthis end, an iterative algorithm is designed to estimate the low-rank and\nsparse contributions. Further, we propose deep learning to learn the parameters\nof the algorithm (i.e., algorithm unfolding) to improve the accuracy and the\nspeed of convergence of the algorithm. Our numerical results show that the\nproposed approach outperforms the conventional approaches in terms of mean\nsquare errors of the recovered low-rank and sparse components and the speed of\nconvergence.",
          "link": "http://arxiv.org/abs/2106.03686",
          "publishedOn": "2021-07-19T00:49:06.597Z",
          "wordCount": 681,
          "title": "Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Despite remarkable progress achieved, most neural architecture search (NAS)\nmethods focus on searching for one single accurate and robust architecture. To\nfurther build models with better generalization capability and performance,\nmodel ensemble is usually adopted and performs better than stand-alone models.\nInspired by the merits of model ensemble, we propose to search for multiple\ndiverse models simultaneously as an alternative way to find powerful models.\nSearching for ensembles is non-trivial and has two key challenges: enlarged\nsearch space and potentially more complexity for the searched model. In this\npaper, we propose a one-shot neural ensemble architecture search (NEAS)\nsolution that addresses the two challenges. For the first challenge, we\nintroduce a novel diversity-based metric to guide search space shrinking,\nconsidering both the potentiality and diversity of candidate operators. For the\nsecond challenge, we enable a new search dimension to learn layer sharing among\ndifferent models for efficiency purposes. The experiments on ImageNet clearly\ndemonstrate that our solution can improve the supernet's capacity of ranking\nensemble architectures, and further lead to better search results. The\ndiscovered architectures achieve superior performance compared with\nstate-of-the-arts such as MobileNetV3 and EfficientNet families under aligned\nsettings. Moreover, we evaluate the generalization ability and robustness of\nour searched architecture on the COCO detection benchmark and achieve a 3.1%\nimprovement on AP compared with MobileNetV3. Codes and models are available at\nhttps://github.com/researchmm/NEAS.",
          "link": "http://arxiv.org/abs/2104.00597",
          "publishedOn": "2021-07-19T00:49:06.565Z",
          "wordCount": 700,
          "title": "One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search Space Shrinking. (arXiv:2104.00597v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1\">Yasunori Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1\">Takayoshi Yamashita</a>",
          "description": "It is difficult to collect data on a large scale in a monocular depth\nestimation because the task requires the simultaneous acquisition of RGB images\nand depths. Data augmentation is thus important to this task. However, there\nhas been little research on data augmentation for tasks such as monocular depth\nestimation, where the transformation is performed pixel by pixel. In this\npaper, we propose a data augmentation method, called CutDepth. In CutDepth,\npart of the depth is pasted onto an input image during training. The method\nextends variations data without destroying edge features. Experiments\nobjectively and subjectively show that the proposed method outperforms\nconventional methods of data augmentation. The estimation accuracy is improved\nwith CutDepth even though there are few training data at long distances.",
          "link": "http://arxiv.org/abs/2107.07684",
          "publishedOn": "2021-07-19T00:49:06.547Z",
          "wordCount": 561,
          "title": "CutDepth:Edge-aware Data Augmentation in Depth Estimation. (arXiv:2107.07684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junwei Liang</a>",
          "description": "With the advancement in computer vision deep learning, systems now are able\nto analyze an unprecedented amount of rich visual information from videos to\nenable applications such as autonomous driving, socially-aware robot assistant\nand public safety monitoring. Deciphering human behaviors to predict their\nfuture paths/trajectories and what they would do from videos is important in\nthese applications. However, human trajectory prediction still remains a\nchallenging task, as scene semantics and human intent are difficult to model.\nMany systems do not provide high-level semantic attributes to reason about\npedestrian future. This design hinders prediction performance in video data\nfrom diverse domains and unseen scenarios. To enable optimal future human\nbehavioral forecasting, it is crucial for the system to be able to detect and\nanalyze human activities as well as scene semantics, passing informative\nfeatures to the subsequent prediction module for context understanding.",
          "link": "http://arxiv.org/abs/2011.10670",
          "publishedOn": "2021-07-19T00:49:06.497Z",
          "wordCount": 626,
          "title": "From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video. (arXiv:2011.10670v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Neural architecture search (NAS) with an accuracy predictor that predicts the\naccuracy of candidate architectures has drawn increasing attention due to its\nsimplicity and effectiveness. Previous works usually employ neural\nnetwork-based predictors which require more delicate design and are easy to\noverfit. Considering that most architectures are represented as sequences of\ndiscrete symbols which are more like tabular data and preferred by non-neural\npredictors, in this paper, we study an alternative approach which uses\nnon-neural model for accuracy prediction. Specifically, as decision tree based\nmodels can better handle tabular data, we leverage gradient boosting decision\ntree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor\ncan achieve comparable (if not better) prediction accuracy than neural network\nbased predictors. Moreover, considering that a compact search space can ease\nthe search process, we propose to prune the search space gradually according to\nimportant features derived from GBDT. In this way, NAS can be performed by\nfirst pruning the search space and then searching a neural architecture, which\nis more efficient and effective. Experiments on NASBench-101 and ImageNet\ndemonstrate the effectiveness of using GBDT as predictor for NAS: (1) On\nNASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,\nregularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global\noptimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further\nachieves 23.4% top-1 error rate on ImageNet when enhanced with search space\npruning. Code is provided in the supplementary materials.",
          "link": "http://arxiv.org/abs/2007.04785",
          "publishedOn": "2021-07-19T00:49:06.477Z",
          "wordCount": 738,
          "title": "Accuracy Prediction with Non-neural Model for Neural Architecture Search. (arXiv:2007.04785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.11150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jihun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Siwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "In this work, we attempt to explain the prediction of any black-box\nclassifier from an information-theoretic perspective. For each input feature,\nwe compare the classifier outputs with and without that feature using two\ninformation-theoretic metrics. Accordingly, we obtain two attribution maps--an\ninformation gain (IG) map and a point-wise mutual information (PMI) map. IG map\nprovides a class-independent answer to \"How informative is each pixel?\", and\nPMI map offers a class-specific explanation of \"How much does each pixel\nsupport a specific class?\" Compared to existing methods, our method improves\nthe correctness of the attribution maps in terms of a quantitative metric. We\nalso provide a detailed analysis of an ImageNet classifier using the proposed\nmethod, and the code is available online.",
          "link": "http://arxiv.org/abs/2009.11150",
          "publishedOn": "2021-07-19T00:49:06.470Z",
          "wordCount": 580,
          "title": "Information-Theoretic Visual Explanation for Black-Box Classifiers. (arXiv:2009.11150v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cardenas_B/0/1/0/all/0/1\">Bryan G. Cardenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arya_D/0/1/0/all/0/1\">Devanshu Arya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak K. Gupta</a>",
          "description": "Recent developments related to generative models have made it possible to\ngenerate diverse high-fidelity images. In particular, layout-to-image\ngeneration models have gained significant attention due to their capability to\ngenerate realistic complex images containing distinct objects. These models are\ngenerally conditioned on either semantic layouts or textual descriptions.\nHowever, unlike natural images, providing auxiliary information can be\nextremely hard in domains such as biomedical imaging and remote sensing. In\nthis work, we propose a multi-object generation framework that can synthesize\nimages with multiple objects without explicitly requiring their contextual\ninformation during the generation process. Based on a vector-quantized\nvariational autoencoder (VQ-VAE) backbone, our model learns to preserve spatial\ncoherency within an image as well as semantic coherency between the objects and\nthe background through two powerful autoregressive priors: PixelSNAIL and\nLayoutPixelSNAIL. While the PixelSNAIL learns the distribution of the latent\nencodings of the VQ-VAE, the LayoutPixelSNAIL is used to specifically learn the\nsemantic distribution of the objects. An implicit advantage of our approach is\nthat the generated samples are accompanied by object-level annotations. We\ndemonstrate how coherency and fidelity are preserved with our method through\nexperiments on the Multi-MNIST and CLEVR datasets; thereby outperforming\nstate-of-the-art multi-object generative methods. The efficacy of our approach\nis demonstrated through application on medical imaging datasets, where we show\nthat augmenting the training set with generated samples using our approach\nimproves the performance of existing models.",
          "link": "http://arxiv.org/abs/2006.12150",
          "publishedOn": "2021-07-19T00:49:06.464Z",
          "wordCount": 711,
          "title": "Generating Annotated High-Fidelity Images Containing Multiple Coherent Objects. (arXiv:2006.12150v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Traditionally, distillation has been used to train a student model to emulate\nthe input/output functionality of a teacher. A more useful goal than emulation,\nyet under-explored, is for the student to learn feature representations that\ntransfer well to future tasks. However, we observe that standard distillation\nof task-specific teachers actually *reduces* the transferability of student\nrepresentations to downstream tasks. We show that a multi-head, multi-task\ndistillation method using an unlabeled proxy dataset and a generalist teacher\nis sufficient to consolidate representations from task-specific teacher(s) and\nimprove downstream performance, outperforming the teacher(s) and the strong\nbaseline of ImageNet pretrained features. Our method can also combine the\nrepresentational knowledge of multiple teachers trained on one or multiple\ndomains into a single model, whose representation is improved on all teachers'\ndomain(s).",
          "link": "http://arxiv.org/abs/2107.08039",
          "publishedOn": "2021-07-19T00:49:06.449Z",
          "wordCount": 568,
          "title": "Representation Consolidation for Training Expert Students. (arXiv:2107.08039v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joseph P. Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Ming Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>",
          "description": "Kinship is a soft biometric detectable in media with an abundance of\npractical applications. Despite the difficulty of detecting kinship, annual\ndata challenges using still-images have consistently improved performances and\nattracted new researchers. Now, systems reach performance levels unforeseeable\na decade ago, closing in on performances acceptable to deploy in practice.\nSimilar to other biometric tasks, we expect systems can benefit from additional\nmodalities. We hypothesize that adding modalities to FIW, which contains only\nstill-images, will improve performance. Thus, to narrow the gap between\nresearch and reality and enhance the power of kinship recognition systems, we\nextend FIW with multimedia (MM) data (i.e., video, audio, and text captions).\nSpecifically, we introduce the first publicly available multi-task MM kinship\ndataset. To build FIW MM, we developed machinery to automatically collect,\nannotate, and prepare the data, requiring minimal human input and no financial\ncost. The proposed MM corpus allows the problem statements to be more realistic\ntemplate-based protocols. We show significant improvements in all benchmarks\nwith the added modalities. The results highlight edge cases to inspire future\nresearch with different areas of improvement. FIW MM provides the data required\nto increase the potential of automated systems to detect kinship in MM. It also\nallows experts from diverse fields to collaborate in novel ways.",
          "link": "http://arxiv.org/abs/2007.14509",
          "publishedOn": "2021-07-19T00:49:06.444Z",
          "wordCount": 706,
          "title": "Families In Wild Multimedia (FIW MM): A Multi-Modal Database for Recognizing Kinship. (arXiv:2007.14509v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xudong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Generative Adversarial Networks (GANs) have been widely-used in image\ntranslation, but their high computational and storage costs impede the\ndeployment on mobile devices. Prevalent methods for CNN compression cannot be\ndirectly applied to GANs due to the complicated generator architecture and the\nunstable adversarial training. To solve these, in this paper, we introduce a\nnovel GAN compression method, termed DMAD, by proposing a Differentiable Mask\nand a co-Attention Distillation. The former searches for a light-weight\ngenerator architecture in a training-adaptive manner. To overcome channel\ninconsistency when pruning the residual connections, an adaptive cross-block\ngroup sparsity is further incorporated. The latter simultaneously distills\ninformative attention maps from both the generator and discriminator of a\npre-trained model to the searched generator, effectively stabilizing the\nadversarial training of our light-weight model. Experiments show that DMAD can\nreduce the Multiply Accumulate Operations (MACs) of CycleGAN by 13$\\times$ and\nthat of Pix2Pix by 4$\\times$ while retaining a comparable performance against\nthe full model. Our code can be available at https://github.com/SJLeo/DMAD.",
          "link": "http://arxiv.org/abs/2011.08382",
          "publishedOn": "2021-07-19T00:49:06.438Z",
          "wordCount": 663,
          "title": "Learning Efficient GANs for Image Translation via Differentiable Masks and co-Attention Distillation. (arXiv:2011.08382v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1\">Lina Achaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1\">Thibault Fouqueray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1\">Francois Aioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1\">Francois Charpillet</a>",
          "description": "The human driver is no longer the only one concerned with the complexity of\nthe driving scenarios. Autonomous vehicles (AV) are similarly becoming involved\nin the process. Nowadays, the development of AV in urban places underpins\nessential safety concerns for vulnerable road users (VRUs) such as pedestrians.\nTherefore, to make the roads safer, it is critical to classify and predict\ntheir future behavior. In this paper, we present a framework based on multiple\nvariations of the Transformer models to reason attentively about the dynamic\nevolution of the pedestrians' past trajectory and predict its future actions of\ncrossing or not crossing the street. We proved that using only bounding boxes\nas input to our model can outperform the previous state-of-the-art models and\nreach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset\nup to two seconds ahead in the future. In addition, we introduced a large-size\nsimulated dataset (CP2A) using CARLA for action prediction. Our model has\nsimilarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.\nInterestingly, we showed that pre-training our Transformer model on the\nsimulated dataset and then fine-tuning it on the real dataset can be very\neffective for the action prediction task.",
          "link": "http://arxiv.org/abs/2107.08031",
          "publishedOn": "2021-07-19T00:49:06.431Z",
          "wordCount": 656,
          "title": "Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lulan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guikang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "Multiple studies in the past have shown that there is a strong correlation\nbetween human vocal characteristics and facial features. However, existing\napproaches generate faces simply from voice, without exploring the set of\nfeatures that contribute to these observed correlations. A computational\nmethodology to explore this can be devised by rephrasing the question to: \"how\nmuch would a target face have to change in order to be perceived as the\noriginator of a source voice?\" With this in perspective, we propose a framework\nto morph a target face in response to a given voice in a way that facial\nfeatures are implicitly guided by learned voice-face correlation in this paper.\nOur framework includes a guided autoencoder that converts one face to another,\ncontrolled by a unique model-conditioning component called a gating controller\nwhich modifies the reconstructed face based on input voice recordings. We\nevaluate the framework on VoxCelab and VGGFace datasets through human subjects\nand face retrieval. Various experiments demonstrate the effectiveness of our\nproposed model.",
          "link": "http://arxiv.org/abs/2107.07988",
          "publishedOn": "2021-07-19T00:49:06.425Z",
          "wordCount": 621,
          "title": "Controlled AutoEncoders to Generate Faces from Voices. (arXiv:2107.07988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moing_G/0/1/0/all/0/1\">Guillaume Le Moing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "This presentation introduces a self-supervised learning approach to the\nsynthesis of new video clips from old ones, with several new key elements for\nimproved spatial resolution and realism: It conditions the synthesis process on\ncontextual information for temporal continuity and ancillary information for\nfine control. The prediction model is doubly autoregressive, in the latent\nspace of an autoencoder for forecasting, and in image space for updating\ncontextual information, which is also used to enforce spatio-temporal\nconsistency through a learnable optical flow module. Adversarial training of\nthe autoencoder in the appearance and temporal domains is used to further\nimprove the realism of its output. A quantizer inserted between the encoder and\nthe transformer in charge of forecasting future frames in latent space (and its\ninverse inserted between the transformer and the decoder) adds even more\nflexibility by affording simple mechanisms for handling multimodal ancillary\ninformation for controlling the synthesis process (eg, a few sample frames, an\naudio track, a trajectory in image space) and taking into account the\nintrinsically uncertain nature of the future by allowing multiple predictions.\nExperiments with an implementation of the proposed approach give very good\nqualitative and quantitative results on multiple tasks and standard benchmarks.",
          "link": "http://arxiv.org/abs/2107.08037",
          "publishedOn": "2021-07-19T00:49:06.418Z",
          "wordCount": 628,
          "title": "CCVS: Context-aware Controllable Video Synthesis. (arXiv:2107.08037v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1905.10170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>",
          "description": "Flow-based generative models have recently become one of the most efficient\napproaches to model data generation. Indeed, they are constructed with a\nsequence of invertible and tractable transformations. Glow first introduced a\nsimple type of generative flow using an invertible $1 \\times 1$ convolution.\nHowever, the $1 \\times 1$ convolution suffers from limited flexibility compared\nto the standard convolutions. In this paper, we propose a novel invertible $n\n\\times n$ convolution approach that overcomes the limitations of the invertible\n$1 \\times 1$ convolution. In addition, our proposed network is not only\ntractable and invertible but also uses fewer parameters than standard\nconvolutions. The experiments on CIFAR-10, ImageNet and Celeb-HQ datasets, have\nshown that our invertible $n \\times n$ convolution helps to improve the\nperformance of generative models significantly.",
          "link": "http://arxiv.org/abs/1905.10170",
          "publishedOn": "2021-07-19T00:49:06.400Z",
          "wordCount": 594,
          "title": "Generative Flow via Invertible nxn Convolution. (arXiv:1905.10170v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chull Hwan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hye Joo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>",
          "description": "We address representation learning for large-scale instance-level image\nretrieval. Apart from backbone, training pipelines and loss functions, popular\napproaches have focused on different spatial pooling and attention mechanisms,\nwhich are at the core of learning a powerful global image representation. There\nare different forms of attention according to the interaction of elements of\nthe feature tensor (local and global) and the dimensions where it is applied\n(spatial and channel). Unfortunately, each study addresses only one or two\nforms of attention and applies it to different problems like classification,\ndetection or retrieval.\n\nWe present global-local attention module (GLAM), which is attached at the end\nof a backbone network and incorporates all four forms of attention: local and\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\npooling, we learn a powerful embedding for image retrieval. Focusing on global\ndescriptors, we provide empirical evidence of the interaction of all forms of\nattention and improve the state of the art on standard benchmarks.",
          "link": "http://arxiv.org/abs/2107.08000",
          "publishedOn": "2021-07-19T00:49:06.393Z",
          "wordCount": 606,
          "title": "All the attention you need: Global-local, spatial-channel attention for image retrieval. (arXiv:2107.08000v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.03787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1\">Brandon Wagstaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>",
          "description": "The self-supervised loss formulation for jointly training depth and egomotion\nneural networks with monocular images is well studied and has demonstrated\nstate-of-the-art accuracy. One of the main limitations of this approach,\nhowever, is that the depth and egomotion estimates are only determined up to an\nunknown scale. In this paper, we present a novel scale recovery loss that\nenforces consistency between a known camera height and the estimated camera\nheight, generating metric (scaled) depth and egomotion predictions. We show\nthat our proposed method is competitive with other scale recovery techniques\nthat require more information. Further, we demonstrate that our method\nfacilitates network retraining within new environments, whereas other\nscale-resolving approaches are incapable of doing so. Notably, our egomotion\nnetwork is able to produce more accurate estimates than a similar method which\nrecovers scale at test time only.",
          "link": "http://arxiv.org/abs/2009.03787",
          "publishedOn": "2021-07-19T00:49:06.379Z",
          "wordCount": 630,
          "title": "Self-Supervised Scale Recovery for Monocular Depth and Egomotion Estimation. (arXiv:2009.03787v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07907",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kanglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_G/0/1/0/all/0/1\">Gaofeng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1\">Jiang Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "Single-image HDR reconstruction or inverse tone mapping (iTM) is a\nchallenging task. In particular, recovering information in over-exposed regions\nis extremely difficult because details in such regions are almost completely\nlost. In this paper, we present a deep learning based iTM method that takes\nadvantage of the feature extraction and mapping power of deep convolutional\nneural networks (CNNs) and uses a lightness prior to modulate the CNN to better\nexploit observations in the surrounding areas of the over-exposed regions to\nenhance the quality of HDR image reconstruction. Specifically, we introduce a\nHierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR\ninput and a Lightness Adpative Modulation Network (LAMN) to incorporate the the\nlightness prior knowledge in the inferring process. The HiSN hierarchically\nsynthesizes the high-brightness component and the low-brightness component of\nthe HDR image whilst the LAMN uses a lightness adaptive mask that separates\ndetail-less saturated bright pixels from well-exposed lower light pixels to\nenable HiSN to better infer the missing information, particularly in the\ndifficult over-exposed detail-less areas. We present experimental results to\ndemonstrate the effectiveness of the new technique based on quantitative\nmeasures and visual comparisons. In addition, we present ablation studies of\nHiSN and visualization of the activation maps inside LAMN to help gain a deeper\nunderstanding of the internal working of the new iTM algorithm and explain why\nit can achieve much improved performance over state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2107.07907",
          "publishedOn": "2021-07-19T00:49:06.350Z",
          "wordCount": 683,
          "title": "Lightness Modulated Deep Inverse Tone Mapping. (arXiv:2107.07907v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Josse_E/0/1/0/all/0/1\">Elias Josse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nerborg_A/0/1/0/all/0/1\">Amanda Nerborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Diaz_K/0/1/0/all/0/1\">Kevin Hernandez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>",
          "description": "The world is expecting an aging population and shortage of healthcare\nprofessionals. This poses the problem of providing a safe and dignified life\nfor the elderly. Technological solutions involving cameras can contribute to\nsafety, comfort and efficient emergency responses, but they are invasive of\nprivacy. We use 'Griddy', a prototype with a Panasonic Grid-EYE, a\nlow-resolution infrared thermopile array sensor, which offers more privacy.\nMounted over a bed, it can determine if the user is on the bed or not without\nhuman interaction. For this purpose, two datasets were captured, one (480\nimages) under constant conditions, and a second one (200 images) under\ndifferent variations such as use of a duvet, sleeping with a pet, or increased\nroom temperature. We test three machine learning algorithms: Support Vector\nMachines (SVM), k-Nearest Neighbors (k-NN) and Neural Network (NN). With\n10-fold cross validation, the highest accuracy in the main dataset is for both\nSVM and k-NN (99%). The results with variable data show a lower reliability\nunder certain circumstances, highlighting the need of extra work to meet the\nchallenge of variations in the environment.",
          "link": "http://arxiv.org/abs/2107.07986",
          "publishedOn": "2021-07-19T00:49:06.343Z",
          "wordCount": 628,
          "title": "In-Bed Person Monitoring Using Thermal Infrared Sensors. (arXiv:2107.07986v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weizhi Lu</a>",
          "description": "Recently, it has been observed that {0,1,-1}-ternary codes which are simply\ngenerated from deep features by hard thresholding, tend to outperform\n{-1,1}-binary codes in image retrieval. To obtain better ternary codes, we for\nthe first time propose to jointly learn the features with the codes by\nappending a smoothed function to the networks. During training, the function\ncould evolve into a non-smoothed ternary function by a continuation method. The\nmethod circumvents the difficulty of directly training discrete functions and\nreduces the quantization errors of ternary codes. Experiments show that the\ngenerated codes indeed could achieve higher retrieval accuracy.",
          "link": "http://arxiv.org/abs/2107.07987",
          "publishedOn": "2021-07-19T00:49:06.308Z",
          "wordCount": 532,
          "title": "Deep Learning to Ternary Hash Codes by Continuation. (arXiv:2107.07987v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fabbrizzi_S/0/1/0/all/0/1\">Simone Fabbrizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>",
          "description": "Computer Vision (CV) has achieved remarkable results, outperforming humans in\nseveral tasks. Nonetheless, it may result in major discrimination if not dealt\nwith proper care. CV systems highly depend on the data they are fed with and\ncan learn and amplify biases within such data. Thus, both the problems of\nunderstanding and discovering biases are of utmost importance. Yet, to date\nthere is no comprehensive survey on bias in visual datasets. To this end, this\nwork aims to: i) describe the biases that can affect visual datasets; ii)\nreview the literature on methods for bias discovery and quantification in\nvisual datasets; iii) discuss existing attempts to collect bias-aware visual\ndatasets. A key conclusion of our study is that the problem of bias discovery\nand quantification in visual datasets is still open and there is room for\nimprovement in terms of both methods and the range of biases that can be\naddressed; moreover, there is no such thing as a bias-free dataset, so\nscientists and practitioners must become aware of the biases in their datasets\nand make them explicit. To this end, we propose a checklist that can be used to\nspot different types of bias during visual dataset collection.",
          "link": "http://arxiv.org/abs/2107.07919",
          "publishedOn": "2021-07-19T00:49:06.301Z",
          "wordCount": 632,
          "title": "A Survey on Bias in Visual Datasets. (arXiv:2107.07919v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>",
          "description": "We study the problem of inferring an object-centric scene representation from\na single image, aiming to derive a representation that explains the image\nformation process, captures the scene's 3D nature, and is learned without\nsupervision. Most existing methods on scene decomposition lack one or more of\nthese characteristics, due to the fundamental challenge in integrating the\ncomplex 3D-to-2D image formation process into powerful inference schemes like\ndeep networks. In this paper, we propose unsupervised discovery of Object\nRadiance Fields (uORF), integrating recent progresses in neural 3D scene\nrepresentations and rendering with deep inference networks for unsupervised 3D\nscene decomposition. Trained on multi-view RGB images without annotations, uORF\nlearns to decompose complex scenes with diverse, textured background from a\nsingle image. We show that uORF performs well on unsupervised 3D scene\nsegmentation, novel view synthesis, and scene editing on three datasets.",
          "link": "http://arxiv.org/abs/2107.07905",
          "publishedOn": "2021-07-19T00:49:06.288Z",
          "wordCount": 580,
          "title": "Unsupervised Discovery of Object Radiance Fields. (arXiv:2107.07905v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Engel_N/0/1/0/all/0/1\">Nico Engel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>",
          "description": "We present a vehicle self-localization method using point-based deep neural\nnetworks. Our approach processes measurements and point features, i.e.\nlandmarks, from a high-definition digital map to infer the vehicle's pose. To\nlearn the best association and incorporate local information between the point\nsets, we propose an attention mechanism that matches the measurements to the\ncorresponding landmarks. Finally, we use this representation for the\npoint-cloud registration and the subsequent pose regression task. Furthermore,\nwe introduce a training simulation framework that artificially generates\nmeasurements and landmarks to facilitate the deployment process and reduce the\ncost of creating extensive datasets from real-world data. We evaluate our\nmethod on our dataset, as well as an adapted version of the Kitti odometry\ndataset, where we achieve superior performance compared to related approaches;\nand additionally show dominant generalization capabilities.",
          "link": "http://arxiv.org/abs/2107.07787",
          "publishedOn": "2021-07-19T00:49:06.279Z",
          "wordCount": 583,
          "title": "Attention-based Vehicle Self-Localization with HD Feature Maps. (arXiv:2107.07787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pont_M/0/1/0/all/0/1\">Mathieu Pont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1\">Jules Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delon_J/0/1/0/all/0/1\">Julie Delon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1\">Julien Tierny</a>",
          "description": "This paper presents a unified computational framework for the estimation of\ndistances, geodesics and barycenters of merge trees. We extend recent work on\nthe edit distance [106] and introduce a new metric, called the Wasserstein\ndistance between merge trees, which is purposely designed to enable efficient\ncomputations of geodesics and barycenters. Specifically, our new distance is\nstrictly equivalent to the L2-Wasserstein distance between extremum persistence\ndiagrams, but it is restricted to a smaller solution space, namely, the space\nof rooted partial isomorphisms between branch decomposition trees. This enables\na simple extension of existing optimization frameworks [112] for geodesics and\nbarycenters from persistence diagrams to merge trees. We introduce a task-based\nalgorithm which can be generically applied to distance, geodesic, barycenter or\ncluster computation. The task-based nature of our approach enables further\naccelerations with shared-memory parallelism. Extensive experiments on public\nensembles and SciVis contest benchmarks demonstrate the efficiency of our\napproach -- with barycenter computations in the orders of minutes for the\nlargest examples -- as well as its qualitative ability to generate\nrepresentative barycenter merge trees, visually summarizing the features of\ninterest found in the ensemble. We show the utility of our contributions with\ndedicated visualization applications: feature tracking, temporal reduction and\nensemble clustering. We provide a lightweight C++ implementation that can be\nused to reproduce our results.",
          "link": "http://arxiv.org/abs/2107.07789",
          "publishedOn": "2021-07-19T00:49:06.272Z",
          "wordCount": 664,
          "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees. (arXiv:2107.07789v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blekos_K/0/1/0/all/0/1\">Kostas Blekos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1\">Aris S Lalos</a>",
          "description": "Delineation approaches provide significant benefits to various domains,\nincluding agriculture, environmental and natural disasters monitoring. Most of\nthe work in the literature utilize traditional segmentation methods that\nrequire a large amount of computational and storage resources. Deep learning\nhas transformed computer vision and dramatically improved machine translation,\nthough it requires massive dataset for training and significant resources for\ninference. More importantly, energy-efficient embedded vision hardware\ndelivering real-time and robust performance is crucial in the aforementioned\napplication. In this work, we propose a U-Net based tree delineation method,\nwhich is effectively trained using multi-spectral imagery but can then\ndelineate single-spectrum images. The deep architecture that also performs\nlocalization, i.e., a class label corresponds to each pixel, has been\nsuccessfully used to allow training with a small set of segmented images. The\nground truth data were generated using traditional image denoising and\nsegmentation approaches. To be able to execute the proposed DNN efficiently in\nembedded platforms designed for deep learning approaches, we employ traditional\nmodel compression and acceleration methods. Extensive evaluation studies using\ndata collected from UAVs equipped with multi-spectral cameras demonstrate the\neffectiveness of the proposed methods in terms of delineation accuracy and\nexecution efficiency.",
          "link": "http://arxiv.org/abs/2107.07826",
          "publishedOn": "2021-07-19T00:49:06.255Z",
          "wordCount": 657,
          "title": "Efficient automated U-Net based tree crown delineation using UAV multi-spectral imagery on embedded devices. (arXiv:2107.07826v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runde Li</a>",
          "description": "To solve the issue of video dehazing, there are two main tasks to attain: how\nto align adjacent frames to the reference frame; how to restore the reference\nframe. Some papers adopt explicit approaches (e.g., the Markov random field,\noptical flow, deformable convolution, 3D convolution) to align neighboring\nframes with the reference frame in feature space or image space, they then use\nvarious restoration methods to achieve the final dehazing results. In this\npaper, we propose a progressive alignment and restoration method for video\ndehazing. The alignment process aligns consecutive neighboring frames stage by\nstage without using the optical flow estimation. The restoration process is not\nonly implemented under the alignment process but also uses a refinement network\nto improve the dehazing performance of the whole network. The proposed networks\ninclude four fusion networks and one refinement network. To decrease the\nparameters of networks, three fusion networks in the first fusion stage share\nthe same parameters. Extensive experiments demonstrate that the proposed video\ndehazing method achieves outstanding performance against the-state-of-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.07837",
          "publishedOn": "2021-07-19T00:49:06.241Z",
          "wordCount": 600,
          "title": "Progressive Deep Video Dehazing without Explicit Alignment Estimation. (arXiv:2107.07837v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07975",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Savioli_N/0/1/0/all/0/1\">Nicolo Savioli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marvao_A/0/1/0/all/0/1\">Antonio de Marvao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cook_S/0/1/0/all/0/1\">Stuart A. Cook</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chin_C/0/1/0/all/0/1\">Calvin W.L. Chin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ORegan_D/0/1/0/all/0/1\">Declan P. O&#x27;Regan</a>",
          "description": "Optimising the analysis of cardiac structure and function requires accurate\n3D representations of shape and motion. However, techniques such as cardiac\nmagnetic resonance imaging are conventionally limited to acquiring contiguous\ncross-sectional slices with low through-plane resolution and potential\ninter-slice spatial misalignment. Super-resolution in medical imaging aims to\nincrease the resolution of images but is conventionally trained on features\nfrom low resolution datasets and does not super-resolve corresponding\nsegmentations. Here we propose a semi-supervised multi-task generative\nadversarial network (Gemini-GAN) that performs joint super-resolution of the\nimages and their labels using a ground truth of high resolution 3D cines and\nsegmentations, while an unsupervised variational adversarial mixture\nautoencoder (V-AMA) is used for continuous domain adaptation. Our proposed\napproach is extensively evaluated on two transnational multi-ethnic populations\nof 1,331 and 205 adults respectively, delivering an improvement on state of the\nart methods in terms of Dice index, peak signal to noise ratio, and structural\nsimilarity index measure. This framework also exceeds the performance of state\nof the art generative domain adaptation models on external validation (Dice\nindex 0.81 vs 0.74 for the left ventricle). This demonstrates how joint\nsuper-resolution and segmentation, trained on 3D ground-truth data with\ncross-domain generalization, enables robust precision phenotyping in diverse\npopulations.",
          "link": "http://arxiv.org/abs/2107.07975",
          "publishedOn": "2021-07-19T00:49:06.224Z",
          "wordCount": 667,
          "title": "Joint Semi-supervised 3D Super-Resolution and Segmentation with Mixed Adversarial Gaussian Domain Adaptation. (arXiv:2107.07975v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kannan_N/0/1/0/all/0/1\">Nagajothi Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1\">Sravan Danda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1\">Aditya Challa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_D/0/1/0/all/0/1\">Daya Sagar B S</a>",
          "description": "The study of water bodies such as rivers is an important problem in the\nremote sensing community. A meaningful set of quantitative features reflecting\nthe geophysical properties help us better understand the formation and\nevolution of rivers. Typically, river sub-basins are analysed using Cartosat\nDigital Elevation Models (DEMs), obtained at regular time epochs. One of the\nuseful geophysical features of a river sub-basin is that of a roughness measure\non DEMs. However, to the best of our knowledge, there is not much literature\navailable on theoretical analysis of roughness measures. In this article, we\nrevisit the roughness measure on DEM data adapted from multiscale\ngranulometries in mathematical morphology, namely multiscale directional\ngranulometric index (MDGI). This measure was classically used to obtain\nshape-size analysis in greyscale images. In earlier works, MDGIs were\nintroduced to capture the characteristic surficial roughness of a river\nsub-basin along specific directions. Also, MDGIs can be efficiently computed\nand are known to be useful features for classification of river sub-basins. In\nthis article, we provide a theoretical analysis of a MDGI. In particular, we\ncharacterize non-trivial sufficient conditions on the structure of DEMs under\nwhich MDGIs are invariant. These properties are illustrated with some\nfictitious DEMs. We also provide connections to a discrete derivative of volume\nof a DEM. Based on these connections, we provide intuition as to why a MDGI is\nconsidered a roughness measure. Further, we experimentally illustrate on\nLower-Indus, Wardha, and Barmer river sub-basins that the proposed features\ncapture the characteristics of the river sub-basin.",
          "link": "http://arxiv.org/abs/2107.07827",
          "publishedOn": "2021-07-19T00:49:06.208Z",
          "wordCount": 708,
          "title": "A Theoretical Analysis of Granulometry-based Roughness Measures on Cartosat DEMs. (arXiv:2107.07827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07985",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rimner_A/0/1/0/all/0/1\">Andreas Rimner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deasy_J/0/1/0/all/0/1\">Joseph O. Deasy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1\">Harini Veeraraghavan</a>",
          "description": "Accurate and robust segmentation of lung cancers from CTs is needed to more\naccurately plan and deliver radiotherapy and to measure treatment response.\nThis is particularly difficult for tumors located close to mediastium, due to\nlow soft-tissue contrast. Therefore, we developed a new cross-modality educed\ndistillation (CMEDL) approach, using unpaired CT and MRI scans, whereby a\nteacher MRI network guides a student CT network to extract features that signal\nthe difference between foreground and background. Our contribution eliminates\ntwo requirements of distillation methods: (i) paired image sets by using an\nimage to image (I2I) translation and (ii) pre-training of the teacher network\nwith a large training set by using concurrent training of all networks. Our\nframework uses an end-to-end trained unpaired I2I translation, teacher, and\nstudent segmentation networks. Our framework can be combined with any I2I and\nsegmentation network. We demonstrate our framework's feasibility using 3\nsegmentation and 2 I2I methods. All networks were trained with 377 CT and 82\nT2w MRI from different sets of patients. Ablation tests and different\nstrategies for incorporating MRI information into CT were performed. Accuracy\nwas measured using Dice similarity (DSC), surface Dice (sDSC), and Hausdorff\ndistance at the 95$^{th}$ percentile (HD95). The CMEDL approach was\nsignificantly (p $<$ 0.001) more accurate than non-CMEDL methods,\nquantitatively and visually. It produced the highest segmentation accuracy\n(sDSC of 0.83 $\\pm$ 0.16 and HD95 of 5.20 $\\pm$ 6.86mm). CMEDL was also more\naccurate than using either pMRI's or the combination of CT's with pMRI's for\nsegmentation.",
          "link": "http://arxiv.org/abs/2107.07985",
          "publishedOn": "2021-07-19T00:49:06.202Z",
          "wordCount": 715,
          "title": "Unpaired cross-modality educed distillation (CMEDL) applied to CT lung tumor segmentation. (arXiv:2107.07985v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1\">Elona Shatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.",
          "link": "http://arxiv.org/abs/2107.07786",
          "publishedOn": "2021-07-19T00:49:06.177Z",
          "wordCount": 651,
          "title": "DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muzammul_M/0/1/0/all/0/1\">Muhammed Muzammul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "This survey paper specially analyzed computer vision-based object detection\nchallenges and solutions by different techniques. We mainly highlighted object\ndetection by three different trending strategies, i.e., 1) domain adaptive deep\nlearning-based approaches (discrepancy-based, Adversarial-based,\nReconstruction-based, Hybrid). We examined general as well as tiny object\ndetection-related challenges and offered solutions by historical and\ncomparative analysis. In part 2) we mainly focused on tiny object detection\ntechniques (multi-scale feature learning, Data augmentation, Training strategy\n(TS), Context-based detection, GAN-based detection). In part 3), To obtain\nknowledge-able findings, we discussed different object detection methods, i.e.,\nconvolutions and convolutional neural networks (CNN), pooling operations with\ntrending types. Furthermore, we explained results with the help of some object\ndetection algorithms, i.e., R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD,\nwhich are generally considered the base bone of CV, CNN, and OD. We performed\ncomparative analysis on different datasets such as MS-COCO, PASCAL VOC07,12,\nand ImageNet to analyze results and present findings. At the end, we showed\nfuture directions with existing challenges of the field. In the future, OD\nmethods and models can be analyzed for real-time object detection, tracking\nstrategies.",
          "link": "http://arxiv.org/abs/2107.07927",
          "publishedOn": "2021-07-19T00:49:06.171Z",
          "wordCount": 627,
          "title": "A Survey on Deep Domain Adaptation and Tiny Object Detection Challenges, Techniques and Datasets. (arXiv:2107.07927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Few-Shot image classification aims to utilize pretrained knowledge learned\nfrom a large-scale dataset to tackle a series of downstream classification\ntasks. Typically, each task involves only few training examples from brand-new\ncategories. This requires the pretraining models to focus on well-generalizable\nknowledge, but ignore domain-specific information. In this paper, we observe\nthat image background serves as a source of domain-specific knowledge, which is\na shortcut for models to learn in the source dataset, but is harmful when\nadapting to brand-new classes. To prevent the model from learning this shortcut\nknowledge, we propose COSOC, a novel Few-Shot Learning framework, to\nautomatically figure out foreground objects at both pretraining and evaluation\nstage. COSOC is a two-stage algorithm motivated by the observation that\nforeground objects from different images within the same class share more\nsimilar patterns than backgrounds. At the pretraining stage, for each class, we\ncluster contrastive-pretrained features of randomly cropped image patches, such\nthat crops containing only foreground objects can be identified by a single\ncluster. We then force the pretraining model to focus on found foreground\nobjects by a fusion sampling strategy; at the evaluation stage, among images in\neach training class of any few-shot task, we seek for shared contents and\nfilter out background. The recognized foreground objects of each class are used\nto match foreground of testing images. Extensive experiments tailored to\ninductive FSL tasks on two benchmarks demonstrate the state-of-the-art\nperformance of our method.",
          "link": "http://arxiv.org/abs/2107.07746",
          "publishedOn": "2021-07-19T00:49:06.158Z",
          "wordCount": 694,
          "title": "Rectifying the Shortcut Learning of Background: Shared Object Concentration for Few-Shot Image Recognition. (arXiv:2107.07746v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hubner_P/0/1/0/all/0/1\">Patrick H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Martin Weinmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wursthorn_S/0/1/0/all/0/1\">Sven Wursthorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1\">Stefan Hinz</a>",
          "description": "In this paper, we present a novel pose normalization method for indoor\nmapping point clouds and triangle meshes that is robust against large fractions\nof the indoor mapping geometries deviating from an ideal Manhattan World\nstructure. In the case of building structures that contain multiple Manhattan\nWorld systems, the dominant Manhattan World structure supported by the largest\nfraction of geometries is determined and used for alignment. In a first step, a\nvertical alignment orienting a chosen axis to be orthogonal to horizontal floor\nand ceiling surfaces is conducted. Subsequently, a rotation around the\nresulting vertical axis is determined that aligns the dataset horizontally with\nthe coordinate axes. The proposed method is evaluated quantitatively against\nseveral publicly available indoor mapping datasets. Our implementation of the\nproposed procedure along with code for reproducing the evaluation will be made\navailable to the public upon acceptance for publication.",
          "link": "http://arxiv.org/abs/2107.07778",
          "publishedOn": "2021-07-19T00:49:06.151Z",
          "wordCount": 590,
          "title": "Pose Normalization of Indoor Mapping Datasets Partially Compliant to the Manhattan World Assumption. (arXiv:2107.07778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Puck de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Sindy L&#xf6;we</a>",
          "description": "Reliable detection of anomalies is crucial when deploying machine learning\nmodels in practice, but remains challenging due to the lack of labeled data. To\ntackle this challenge, contrastive learning approaches are becoming\nincreasingly popular, given the impressive results they have achieved in\nself-supervised representation learning settings. However, while most existing\ncontrastive anomaly detection and segmentation approaches have been applied to\nimages, none of them can use the contrastive losses directly for both anomaly\ndetection and segmentation. In this paper, we close this gap by making use of\nthe Contrastive Predictive Coding model (arXiv:1807.03748). We show that its\npatch-wise contrastive loss can directly be interpreted as an anomaly score,\nand how this allows for the creation of anomaly segmentation masks. The\nresulting model achieves promising results for both anomaly detection and\nsegmentation on the challenging MVTec-AD dataset.",
          "link": "http://arxiv.org/abs/2107.07820",
          "publishedOn": "2021-07-19T00:49:06.129Z",
          "wordCount": 583,
          "title": "Contrastive Predictive Coding for Anomaly Detection. (arXiv:2107.07820v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qaiser_T/0/1/0/all/0/1\">Talha Qaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1\">Stefan Winzeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barfoot_T/0/1/0/all/0/1\">Theodore Barfoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barwick_T/0/1/0/all/0/1\">Tara Barwick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doran_S/0/1/0/all/0/1\">Simon J. Doran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1\">Martin F. Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedlake_L/0/1/0/all/0/1\">Linda Wedlake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunariu_N/0/1/0/all/0/1\">Nina Tunariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_D/0/1/0/all/0/1\">Dow-Mu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messiou_C/0/1/0/all/0/1\">Christina Messiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockall_A/0/1/0/all/0/1\">Andrea Rockall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Whole body magnetic resonance imaging (WB-MRI) is the recommended modality\nfor diagnosis of multiple myeloma (MM). WB-MRI is used to detect sites of\ndisease across the entire skeletal system, but it requires significant\nexpertise and is time-consuming to report due to the great number of images. To\naid radiological reading, we propose an auxiliary task-based multiple instance\nlearning approach (ATMIL) for MM classification with the ability to localize\nsites of disease. This approach is appealing as it only requires patient-level\nannotations where an attention mechanism is used to identify local regions with\nactive disease. We borrow ideas from multi-task learning and define an\nauxiliary task with adaptive reweighting to support and improve learning\nefficiency in the presence of data scarcity. We validate our approach on both\nsynthetic and real multi-center clinical data. We show that the MIL attention\nmodule provides a mechanism to localize bone regions while the adaptive\nreweighting of the auxiliary task considerably improves the performance.",
          "link": "http://arxiv.org/abs/2107.07805",
          "publishedOn": "2021-07-19T00:49:06.122Z",
          "wordCount": 623,
          "title": "Multiple Instance Learning with Auxiliary Task Weighting for Multiple Myeloma Classification. (arXiv:2107.07805v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07752",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cognolato_F/0/1/0/all/0/1\">Francesco Cognolato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OBrien_K/0/1/0/all/0/1\">Kieran O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jin Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_S/0/1/0/all/0/1\">Simon Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laun_F/0/1/0/all/0/1\">Frederik B. Laun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barth_M/0/1/0/all/0/1\">Markus Barth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollmann_S/0/1/0/all/0/1\">Steffen Bollmann</a>",
          "description": "Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great\npotential in recent years, outperforming traditional non-learning approaches in\nspeed and accuracy. However, many of the current deep learning approaches are\nnot data consistent, require in vivo training data or do not solve all steps of\nthe QSM processing pipeline. Here we aim to overcome these limitations and\ndeveloped a framework to solve the QSM processing steps jointly. We developed a\nnew hybrid training data generation method that enables the end-to-end training\nfor solving background field correction and dipole inversion in a\ndata-consistent fashion using a variational network that combines the QSM model\nterm and a learned regularizer. We demonstrate that NeXtQSM overcomes the\nlimitations of previous model-agnostic deep learning methods and show that\nNeXtQSM offers a complete deep learning based pipeline for computing robust,\nfast and accurate quantitative susceptibility maps.",
          "link": "http://arxiv.org/abs/2107.07752",
          "publishedOn": "2021-07-19T00:49:06.107Z",
          "wordCount": 613,
          "title": "NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07761",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mascolini_A/0/1/0/all/0/1\">Alessio Mascolini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardamone_D/0/1/0/all/0/1\">Dario Cardamone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponzio_F/0/1/0/all/0/1\">Francesco Ponzio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cataldo_S/0/1/0/all/0/1\">Santa Di Cataldo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ficarra_E/0/1/0/all/0/1\">Elisa Ficarra</a>",
          "description": "Computer-aided analysis of biological images typically requires extensive\ntraining on large-scale annotated datasets, which is not viable in many\nsituations. In this paper we present GAN-DL, a Discriminator Learner based on\nthe StyleGAN2 architecture, which we employ for self-supervised image\nrepresentation learning in the case of fluorescent biological images. We show\nthat Wasserstein Generative Adversarial Networks combined with linear Support\nVector Machines enable high-throughput compound screening based on raw images.\nWe demonstrate this by classifying active and inactive compounds tested for the\ninhibition of SARS-CoV-2 infection in VERO and HRCE cell lines. In contrast to\nprevious methods, our deep learning based approach does not require any\nannotation besides the one that is normally collected during the sample\npreparation process. We test our technique on the RxRx19a Sars-CoV-2 image\ncollection. The dataset consists of fluorescent images that were generated to\nassess the ability of regulatory-approved or in late-stage clinical trials\ncompound to modulate the in vitro infection from SARS-CoV-2 in both VERO and\nHRCE cell lines. We show that our technique can be exploited not only for\nclassification tasks, but also to effectively derive a dose response curve for\nthe tested treatments, in a self-supervised manner. Lastly, we demonstrate its\ngeneralization capabilities by successfully addressing a zero-shot learning\ntask, consisting in the categorization of four different cell types of the\nRxRx1 fluorescent images collection.",
          "link": "http://arxiv.org/abs/2107.07761",
          "publishedOn": "2021-07-19T00:49:06.096Z",
          "wordCount": 733,
          "title": "Exploiting generative self-supervised learning for the assessment of biological images with lack of annotations: a COVID-19 case-study. (arXiv:2107.07761v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07714",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Lavrik_E/0/1/0/all/0/1\">E. Lavrik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shiroya_M/0/1/0/all/0/1\">M. Shiroya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schmidt_H/0/1/0/all/0/1\">H.R. Schmidt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Toia_A/0/1/0/all/0/1\">A. Toia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heuser_J/0/1/0/all/0/1\">J.M. Heuser</a>",
          "description": "Optical inspection of 1191 silicon micro-strip sensors was performed using a\ncustom made optical inspection setup, employing a machine-learning based\napproach for the defect analysis and subsequent quality assurance. Furthermore,\nmetrological control of the sensor's surface was performed. In this manuscript,\nwe present the analysis of various sensor surface defects. Among these are\nimplant breaks, p-stop breaks, aluminium strip opens, aluminium strip shorts,\nsurface scratches, double metallization layer defects, passivation layer\ndefects, bias resistor defects as well as dust particle identification. The\ndefect detection was done using the application of Convolutional Deep Neural\nNetworks (CDNNs). From this, defective strips and defect clusters were\nidentified, as well as a 2D map of the defects using their geometrical\npositions on the sensor was performed. Based on the total number of defects\nfound on the sensor's surface, a method for the estimation of sensor's overall\nquality grade and quality score was proposed.",
          "link": "http://arxiv.org/abs/2107.07714",
          "publishedOn": "2021-07-19T00:49:06.076Z",
          "wordCount": 610,
          "title": "Optical Inspection of the Silicon Micro-strip Sensors for the CBM Experiment employing Artificial Intelligence. (arXiv:2107.07714v1 [physics.ins-det])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-07-19T00:49:06.068Z",
          "wordCount": 587,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1\">Euijoon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>",
          "description": "Recent supervised deep learning methods have shown that heart rate can be\nmeasured remotely using facial videos. However, the performance of these\nsupervised method are dependent on the availability of large-scale labelled\ndata and they have been limited to 2D deep learning architectures that do not\nfully exploit the 3D spatiotemporal information. To solve this problem, we\npresent a novel 3D self-supervised spatiotemporal learning framework for remote\nHR estimation on facial videos. Concretely, we propose a landmark-based spatial\naugmentation which splits the face into several informative parts based on the\nShafer's dichromatic reflection model and a novel sparsity-based temporal\naugmentation exploiting Nyquist-Shannon sampling theorem to enhance the signal\nmodelling ability. We evaluated our method on 3 public datasets and\noutperformed other self-supervised methods and achieved competitive accuracy\nwith the state-of-the-art supervised methods.",
          "link": "http://arxiv.org/abs/2107.07695",
          "publishedOn": "2021-07-19T00:49:06.044Z",
          "wordCount": 572,
          "title": "Self-Supervised Learning Framework for Remote Heart Rate Estimation Using Spatiotemporal Augmentation. (arXiv:2107.07695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1\">Niko S&#xfc;nderhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>",
          "description": "Probabilistic state-estimation approaches offer a principled foundation for\ndesigning localization systems, because they naturally integrate sequences of\nimperfect motion and exteroceptive sensor data. Recently, probabilistic\nlocalization systems utilizing appearance-invariant visual place recognition\n(VPR) methods as the primary exteroceptive sensor have demonstrated\nstate-of-the-art performance in the presence of substantial appearance change.\nHowever, existing systems 1) do not fully utilize odometry data within the\nmotion models, and 2) are unable to handle route deviations, due to the\nassumption that query traverses exactly repeat the mapping traverse. To address\nthese shortcomings, we present a new probabilistic topometric localization\nsystem which incorporates full 3-dof odometry into the motion model and\nfurthermore, adds an \"off-map\" state within the state-estimation framework,\nallowing query traverses which feature significant route detours from the\nreference map to be successfully localized. We perform extensive evaluation on\nmultiple query traverses from the Oxford RobotCar dataset exhibiting both\nsignificant appearance change and deviations from routes previously traversed.\nIn particular, we evaluate performance on two practically relevant localization\ntasks: loop closure detection and global localization. Our approach achieves\nmajor performance improvements over both existing and improved state-of-the-art\nsystems.",
          "link": "http://arxiv.org/abs/2107.07707",
          "publishedOn": "2021-07-19T00:49:06.038Z",
          "wordCount": 634,
          "title": "Probabilistic Appearance-Invariant Topometric Localization with New Place Awareness. (arXiv:2107.07707v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaojian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuchen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>",
          "description": "Semantic segmentation for scene understanding is nowadays widely demanded,\nraising significant challenges for the algorithm efficiency, especially its\napplications on resource-limited platforms. Current segmentation models are\ntrained and evaluated on massive high-resolution scene images (\"data level\")\nand suffer from the expensive computation arising from the required multi-scale\naggregation(\"network level\"). In both folds, the computational and energy costs\nin training and inference are notable due to the often desired large input\nresolutions and heavy computational burden of segmentation models. To this end,\nwe propose DANCE, general automated DAta-Network Co-optimization for Efficient\nsegmentation model training and inference. Distinct from existing efficient\nsegmentation approaches that focus merely on light-weight network design, DANCE\ndistinguishes itself as an automated simultaneous data-network co-optimization\nvia both input data manipulation and network architecture slimming.\nSpecifically, DANCE integrates automated data slimming which adaptively\ndownsamples/drops input images and controls their corresponding contribution to\nthe training loss guided by the images' spatial complexity. Such a downsampling\noperation, in addition to slimming down the cost associated with the input size\ndirectly, also shrinks the dynamic range of input object and context scales,\ntherefore motivating us to also adaptively slim the network to match the\ndownsampled data. Extensive experiments and ablating studies (on four SOTA\nsegmentation models with three popular segmentation datasets under two training\nsettings) demonstrate that DANCE can achieve \"all-win\" towards efficient\nsegmentation(reduced training cost, less expensive inference, and better mean\nIntersection-over-Union (mIoU)).",
          "link": "http://arxiv.org/abs/2107.07706",
          "publishedOn": "2021-07-19T00:49:06.031Z",
          "wordCount": 687,
          "title": "DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference. (arXiv:2107.07706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Mann Patel</a>",
          "description": "Violence rates however have been brought down about 57% during the span of\nthe past 4 decades yet it doesn't change the way that the demonstration of\nviolence actually happens, unseen by the law. Violence can be mass controlled\nsometimes by higher authorities, however, to hold everything in line one must\n\"Microgovern\" over each movement occurring in every road of each square. To\naddress the butterfly effects impact in our setting, I made a unique model and\na theorized system to handle the issue utilizing deep learning. The model takes\nthe input of the CCTV video feeds and after drawing inference, recognizes if a\nviolent movement is going on. And hypothesized architecture aims towards\nprobability-driven computation of video feeds and reduces overhead from naively\ncomputing for every CCTV video feeds.",
          "link": "http://arxiv.org/abs/2107.07578",
          "publishedOn": "2021-07-19T00:49:06.004Z",
          "wordCount": 561,
          "title": "Real-Time Violence Detection Using CNN-LSTM. (arXiv:2107.07578v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07596",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lo_C/0/1/0/all/0/1\">Chen-Chou Lo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandewalle_P/0/1/0/all/0/1\">Patrick Vandewalle</a>",
          "description": "We integrate sparse radar data into a monocular depth estimation model and\nintroduce a novel preprocessing method for reducing the sparseness and limited\nfield of view provided by radar. We explore the intrinsic error of different\nradar modalities and show our proposed method results in more data points with\nreduced error. We further propose a novel method for estimating dense depth\nmaps from monocular 2D images and sparse radar measurements using deep learning\nbased on the deep ordinal regression network by Fu et al. Radar data are\nintegrated by first converting the sparse 2D points to a height-extended 3D\nmeasurement and then including it into the network using a late fusion\napproach. Experiments are conducted on the nuScenes dataset. Our experiments\ndemonstrate state-of-the-art performance in both day and night scenes.",
          "link": "http://arxiv.org/abs/2107.07596",
          "publishedOn": "2021-07-19T00:49:05.988Z",
          "wordCount": 593,
          "title": "Depth Estimation from Monocular Images and Sparse radar using Deep Ordinal Regression Network. (arXiv:2107.07596v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Saravanabalagi Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John McDonald</a>",
          "description": "OdoViz is a reactive web-based tool for 3D visualization and processing of\nautonomous vehicle datasets designed to support common tasks in visual place\nrecognition research. The system includes functionality for loading,\ninspecting, visualizing, and processing GPS/INS poses, point clouds and camera\nimages. It supports a number of commonly used driving datasets and can be\nadapted to load custom datasets with minimal effort. OdoViz's design consists\nof a slim server to serve the datasets coupled with a rich client frontend.\nThis design supports multiple deployment configurations including single user\nstand-alone installations, research group installations serving datasets\ninternally across a lab, or publicly accessible web-frontends for providing\nonline interfaces for exploring and interacting with datasets. The tool allows\nviewing complete vehicle trajectories traversed at multiple different time\nperiods simultaneously, facilitating tasks such as sub-sampling, comparing and\nfinding pose correspondences both across and within sequences. This\nsignificantly reduces the effort required in creating subsets of data from\nexisting datasets for machine learning tasks. Further to the above, the system\nalso supports adding custom extensions and plugins to extend the capabilities\nof the software for other potential data management, visualization and\nprocessing tasks. The platform has been open-sourced to promote its use and\nencourage further contributions from the research community.",
          "link": "http://arxiv.org/abs/2107.07557",
          "publishedOn": "2021-07-19T00:49:05.971Z",
          "wordCount": 644,
          "title": "OdoViz: A 3D Odometry Visualization and Processing Tool. (arXiv:2107.07557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zida Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>",
          "description": "3D hand-object pose estimation is an important issue to understand the\ninteraction between human and environment. Current hand-object pose estimation\nmethods require detailed 3D labels, which are expensive and labor-intensive. To\ntackle the problem of data collection, we propose a semi-supervised 3D\nhand-object pose estimation method with two key techniques: pose dictionary\nlearning and an object-oriented coordinate system. The proposed pose dictionary\nlearning module can distinguish infeasible poses by reconstruction error,\nenabling unlabeled data to provide supervision signals. The proposed\nobject-oriented coordinate system can make 3D estimations equivariant to the\ncamera perspective. Experiments are conducted on FPHA and HO-3D datasets. Our\nmethod reduces estimation error by 19.5% / 24.9% for hands/objects compared to\nstraightforward use of labeled data on FPHA and outperforms several baseline\nmethods. Extensive experiments also validate the robustness of the proposed\nmethod.",
          "link": "http://arxiv.org/abs/2107.07676",
          "publishedOn": "2021-07-19T00:49:05.963Z",
          "wordCount": 571,
          "title": "Semi-supervised 3D Hand-Object Pose Estimation via Pose Dictionary Learning. (arXiv:2107.07676v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-19T00:49:05.921Z",
          "wordCount": 664,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdous_R/0/1/0/all/0/1\">Refat E Ferdous</a>",
          "description": "During the COVID-19 pandemic, most of the human-to-human interactions have\nbeen stopped. To mitigate the spread of deadly coronavirus, many offices took\nthe initiative so that the employees can work from home. But, tracking the\nemployees and finding out if they are really performing what they were supposed\nto turn out to be a serious challenge for all the companies and organizations\nwho are facilitating \"Work From Home\". To deal with the challenge effectively,\nwe came up with a solution to track the employees with face recognition. We\nhave been testing this system experimentally for our office. To train the face\nrecognition module, we used FaceNet with KNN using the Labeled Faces in the\nWild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model\ninto our central system, where the employees log their time. In this paper, we\ndiscuss in brief the system we have been experimenting with and the pros and\ncons of the system.",
          "link": "http://arxiv.org/abs/2107.07576",
          "publishedOn": "2021-07-19T00:49:05.915Z",
          "wordCount": 666,
          "title": "Real-Time Face Recognition System for Remote Employee Tracking. (arXiv:2107.07576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>",
          "description": "Contrastive learning is a discriminative approach that aims at grouping\nsimilar samples closer and diverse samples far from each other. It it an\nefficient technique to train an encoder generating distinguishable and\ninformative representations, and it may even increase the encoder's\ntransferability. Most current applications of contrastive learning benefit only\na single representation from the last layer of an encoder.In this paper, we\npropose a multi-level contrasitive learning approach which applies contrastive\nlosses at different layers of an encoder to learn multiple representations from\nthe encoder. Afterward, an ensemble can be constructed to take advantage of the\nmultiple representations for the downstream tasks. We evaluated the proposed\nmethod on few-shot learning problems and conducted experiments using the\nmini-ImageNet and the tiered-ImageNet datasets. Our model achieved the new\nstate-of-the-art results for both datasets, comparing to previous regular,\nensemble, and contrastive learing (single-level) based approaches.",
          "link": "http://arxiv.org/abs/2107.07608",
          "publishedOn": "2021-07-19T00:49:05.614Z",
          "wordCount": 572,
          "title": "Multi-Level Contrastive Learning for Few-Shot Problems. (arXiv:2107.07608v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.08177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1\">Juan S. Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio A. Gonz&#xe1;lez</a>",
          "description": "The dissimilarity mixture autoencoder (DMAE) is a neural network model for\nfeature-based clustering that incorporates a flexible dissimilarity function\nand can be integrated into any kind of deep learning architecture. It\ninternally represents a dissimilarity mixture model (DMM) that extends\nclassical methods like K-Means, Gaussian mixture models, or Bregman clustering\nto any convex and differentiable dissimilarity function through the\nreinterpretation of probabilities as neural network representations. DMAE can\nbe integrated with deep learning architectures into end-to-end models, allowing\nthe simultaneous estimation of the clustering and neural network's parameters.\nExperimental evaluation was performed on image and text clustering benchmark\ndatasets showing that DMAE is competitive in terms of unsupervised\nclassification accuracy and normalized mutual information. The source code with\nthe implementation of DMAE is publicly available at:\nhttps://github.com/juselara1/dmae",
          "link": "http://arxiv.org/abs/2006.08177",
          "publishedOn": "2021-07-16T00:48:26.226Z",
          "wordCount": 622,
          "title": "Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yinong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haonan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jingwen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1\">Lixiang Ru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In order to mitigate the spread of COVID-19, Wuhan was the first city to\nimplement strict lockdown policy in 2020. Even though numerous researches have\ndiscussed the travel restriction between cities and provinces, few studies\nfocus on the effect of transportation control inside the city due to the lack\nof the measurement and available data in Wuhan. Since the public transports\nhave been shut down in the beginning of city lockdown, the change of traffic\ndensity is a good indicator to reflect the intracity population flow.\nTherefore, in this paper, we collected time-series high-resolution remote\nsensing images with the resolution of 1m acquired before, during and after\nWuhan lockdown by GF-2 satellite. Vehicles on the road were extracted and\ncounted for the statistics of traffic density to reflect the changes of human\ntransmissions in the whole period of Wuhan lockdown. Open Street Map was used\nto obtain observation road surfaces, and a vehicle detection method combing\nmorphology filter and deep learning was utilized to extract vehicles with the\naccuracy of 62.56%. According to the experimental results, the traffic density\nof Wuhan dropped with the percentage higher than 80%, and even higher than 90%\non main roads during city lockdown; after lockdown lift, the traffic density\nrecovered to the normal rate. Traffic density distributions also show the\nobvious reduction and increase throughout the whole study area. The significant\nreduction and recovery of traffic density indicates that the lockdown policy in\nWuhan show effectiveness in controlling human transmission inside the city, and\nthe city returned to normal after lockdown lift.",
          "link": "http://arxiv.org/abs/2006.16098",
          "publishedOn": "2021-07-16T00:48:25.974Z",
          "wordCount": 813,
          "title": "An Investigation of Traffic Density Changes inside Wuhan during the COVID-19 Epidemic with GF-2 Time-Series Images. (arXiv:2006.16098v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.02068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rakesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1\">Peter Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meghanath_G/0/1/0/all/0/1\">Ganga Meghanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaterji_S/0/1/0/all/0/1\">Somali Chaterji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1\">Subrata Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>",
          "description": "Videos take a lot of time to transport over the network, hence running\nanalytics on the live video on embedded or mobile devices has become an\nimportant system driver. Considering that such devices, e.g., surveillance\ncameras or AR/VR gadgets, are resource constrained, creating lightweight deep\nneural networks (DNNs) for embedded devices is crucial. None of the current\napproximation techniques for object classification DNNs can adapt to changing\nruntime conditions, e.g., changes in resource availability on the device, the\ncontent characteristics, or requirements from the user. In this paper, we\nintroduce ApproxNet, a video object classification system for embedded or\nmobile clients. It enables novel dynamic approximation techniques to achieve\ndesired inference latency and accuracy trade-off under changing runtime\nconditions. It achieves this by enabling two approximation knobs within a\nsingle DNN model, rather than creating and maintaining an ensemble of models\n(e.g., MCDNN [MobiSys-16]. We show that ApproxNet can adapt seamlessly at\nruntime to these changes, provides low and stable latency for the image and\nvideo frame classification problems, and show the improvement in accuracy and\nlatency over ResNet [CVPR-16], MCDNN [MobiSys-16], MobileNets [Google-17],\nNestDNN [MobiCom-18], and MSDNet [ICLR-18].",
          "link": "http://arxiv.org/abs/1909.02068",
          "publishedOn": "2021-07-16T00:48:25.719Z",
          "wordCount": 717,
          "title": "ApproxNet: Content and Contention-Aware Video Analytics System for Embedded Clients. (arXiv:1909.02068v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aouayeb_M/0/1/0/all/0/1\">Mouath Aouayeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soladie_C/0/1/0/all/0/1\">Catherine Soladie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1\">Kidiyo Kpalma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seguier_R/0/1/0/all/0/1\">Renaud Seguier</a>",
          "description": "As various databases of facial expressions have been made accessible over the\nlast few decades, the Facial Expression Recognition (FER) task has gotten a lot\nof interest. The multiple sources of the available databases raised several\nchallenges for facial recognition task. These challenges are usually addressed\nby Convolution Neural Network (CNN) architectures. Different from CNN models, a\nTransformer model based on attention mechanism has been presented recently to\naddress vision tasks. One of the major issue with Transformers is the need of a\nlarge data for training, while most FER databases are limited compared to other\nvision applications. Therefore, we propose in this paper to learn a vision\nTransformer jointly with a Squeeze and Excitation (SE) block for FER task. The\nproposed method is evaluated on different publicly available FER databases\nincluding CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model\noutperforms state-of-the-art methods on CK+ and SFEW and achieves competitive\nresults on JAFFE and RAF-DB.",
          "link": "http://arxiv.org/abs/2107.03107",
          "publishedOn": "2021-07-16T00:48:25.308Z",
          "wordCount": 625,
          "title": "Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition. (arXiv:2107.03107v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mier_J/0/1/0/all/0/1\">Juan Carlos Mier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1\">Eddie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>",
          "description": "Lossy Image compression is necessary for efficient storage and transfer of\ndata. Typically the trade-off between bit-rate and quality determines the\noptimal compression level. This makes the image quality metric an integral part\nof any imaging system. While the existing full-reference metrics such as PSNR\nand SSIM may be less sensitive to perceptual quality, the recently introduced\nlearning methods may fail to generalize to unseen data. In this paper we\npropose the largest image compression quality dataset to date with human\nperceptual preferences, enabling the use of deep learning, and we develop a\nfull reference perceptual quality assessment metric for lossy image compression\nthat outperforms the existing state-of-the-art methods. We show that the\nproposed model can effectively learn from thousands of examples available in\nthe new dataset, and consequently it generalizes better to other unseen\ndatasets of human perceptual preference.",
          "link": "http://arxiv.org/abs/2103.01114",
          "publishedOn": "2021-07-16T00:48:24.289Z",
          "wordCount": 611,
          "title": "Deep Perceptual Image Quality Assessment for Compression. (arXiv:2103.01114v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "Markerless motion capture and understanding of professional non-daily human\nmovements is an important yet unsolved task, which suffers from complex motion\npatterns and severe self-occlusion, especially for the monocular setting. In\nthis paper, we propose SportsCap -- the first approach for simultaneously\ncapturing 3D human motions and understanding fine-grained actions from\nmonocular challenging sports video input. Our approach utilizes the semantic\nand temporally structured sub-motion prior in the embedding space for motion\ncapture and understanding in a data-driven multi-task manner. To enable robust\ncapture under complex motion patterns, we propose an effective motion embedding\nmodule to recover both the implicit motion embedding and explicit 3D motion\ndetails via a corresponding mapping function as well as a sub-motion\nclassifier. Based on such hybrid motion information, we introduce a\nmulti-stream spatial-temporal Graph Convolutional Network(ST-GCN) to predict\nthe fine-grained semantic action attributes, and adopt a semantic attribute\nmapping block to assemble various correlated action attributes into a\nhigh-level action label for the overall detailed understanding of the whole\nsequence, so as to enable various applications like action assessment or motion\nscoring. Comprehensive experiments on both public and our proposed datasets\nshow that with a challenging monocular sports video input, our novel approach\nnot only significantly improves the accuracy of 3D human motion capture, but\nalso recovers accurate fine-grained semantic action attributes.",
          "link": "http://arxiv.org/abs/2104.11452",
          "publishedOn": "2021-07-16T00:48:24.247Z",
          "wordCount": 713,
          "title": "SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos. (arXiv:2104.11452v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_J/0/1/0/all/0/1\">Jayender Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William Wells III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisken_S/0/1/0/all/0/1\">Sarah Frisken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1\">Raymond Kai-yu Tong</a>",
          "description": "In order to tackle the difficulty associated with the ill-posed nature of the\nimage registration problem, researchers use regularization to constrain the\nsolution space. For most learning-based registration approaches, the\nregularization usually has a fixed weight and only constrains the spatial\ntransformation. Such convention has two limitations: (1) The regularization\nstrength of a specific image pair should be associated with the content of the\nimages, thus the ``one value fits all'' scheme is not ideal; (2) Only spatially\nregularizing the transformation (but overlooking the temporal consistency of\ndifferent estimations) may not be the best strategy to cope with the\nill-posedness. In this study, we propose a mean-teacher based registration\nframework. This framework incorporates an additional \\textit{temporal\nregularization} term by encouraging the teacher model's temporal ensemble\nprediction to be consistent with that of the student model. At each training\nstep, it also automatically adjusts the weights of the \\textit{spatial\nregularization} and the \\textit{temporal regularization} by taking account of\nthe transformation uncertainty and appearance uncertainty derived from the\nperturbed teacher model. We perform experiments on multi- and uni-modal\nregistration tasks, and the results show that our strategy outperforms the\ntraditional and learning-based benchmark methods.",
          "link": "http://arxiv.org/abs/2107.02433",
          "publishedOn": "2021-07-16T00:48:24.188Z",
          "wordCount": 675,
          "title": "Double-Uncertainty Assisted Spatial and Temporal Regularization Weighting for Learning-based Registration. (arXiv:2107.02433v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-07-16T00:48:24.167Z",
          "wordCount": 614,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-16T00:48:24.161Z",
          "wordCount": 744,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08757",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenjian Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1\">Stephen J. Price</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>",
          "description": "We present an Expectation-Maximization (EM) Regularized Deep Learning\n(EMReDL) model for weakly supervised tumor segmentation. The proposed framework\nis tailored to glioblastoma, a type of malignant tumor characterized by its\ndiffuse infiltration into the surrounding brain tissue, which poses significant\nchallenge to treatment target and tumor burden estimation using conventional\nstructural MRI. Although physiological MRI provides more specific information\nregarding tumor infiltration, the relatively low resolution hinders a precise\nfull annotation. This has motivated us to develop a weakly supervised deep\nlearning solution that exploits the partial labelled tumor regions.\n\nEMReDL contains two components: a physiological prior prediction model and\nEM-regularized segmentation model. The physiological prior prediction model\nexploits the physiological MRI by training a classifier to generate a\nphysiological prior map. This map is passed to the segmentation model for\nregularization using the EM algorithm. We evaluated the model on a glioblastoma\ndataset with the pre-operative multiparametric and recurrence MRI available.\nEMReDL showed to effectively segment the infiltrated tumor from the partially\nlabelled region of potential infiltration. The segmented core tumor and\ninfiltrated tumor demonstrated high consistency with the tumor burden labelled\nby experts. The performance comparisons showed that EMReDL achieved higher\naccuracy than published state-of-the-art models. On MR spectroscopy, the\nsegmented region displayed more aggressive features than other partial labelled\nregion. The proposed model can be generalized to other segmentation tasks that\nrely on partial labels, with the CNN architecture flexible in the framework.",
          "link": "http://arxiv.org/abs/2101.08757",
          "publishedOn": "2021-07-16T00:48:24.154Z",
          "wordCount": 731,
          "title": "Expectation-Maximization Regularized Deep Learning for Weakly Supervised Tumor Segmentation for Glioblastoma. (arXiv:2101.08757v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parihar_U/0/1/0/all/0/1\">Udit Singh Parihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujarathi_A/0/1/0/all/0/1\">Aniket Gujarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1\">Kinal Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1\">Satyajit Tourani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">K. Madhava Krishna</a>",
          "description": "The use of local detectors and descriptors in typical computer vision\npipelines work well until variations in viewpoint and appearance change become\nextreme. Past research in this area has typically focused on one of two\napproaches to this challenge: the use of projections into spaces more suitable\nfor feature matching under extreme viewpoint changes, and attempting to learn\nfeatures that are inherently more robust to viewpoint change. In this paper, we\npresent a novel framework that combines learning of invariant descriptors\nthrough data augmentation and orthographic viewpoint projection. We propose\nrotation-robust local descriptors, learnt through training data augmentation\nbased on rotation homographies, and a correspondence ensemble technique that\ncombines vanilla feature correspondences with those obtained through\nrotation-robust features. Using a range of benchmark datasets as well as\ncontributing a new bespoke dataset for this research domain, we evaluate the\neffectiveness of the proposed approach on key tasks including pose estimation\nand visual place recognition. Our system outperforms a range of baseline and\nstate-of-the-art techniques, including enabling higher levels of place\nrecognition precision across opposing place viewpoints and achieves\npractically-useful performance levels even under extreme viewpoint changes.",
          "link": "http://arxiv.org/abs/2103.08573",
          "publishedOn": "2021-07-16T00:48:24.116Z",
          "wordCount": 677,
          "title": "RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. (arXiv:2103.08573v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Anqi Joyce Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barsan_I/0/1/0/all/0/1\">Ioan Andrei B&#xe2;rsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shenlong Wang</a>",
          "description": "Existing multi-camera SLAM systems assume synchronized shutters for all\ncameras, which is often not the case in practice. In this work, we propose a\ngeneralized multi-camera SLAM formulation which accounts for asynchronous\nsensor observations. Our framework integrates a continuous-time motion model to\nrelate information across asynchronous multi-frames during tracking, local\nmapping, and loop closing. For evaluation, we collected AMV-Bench, a\nchallenging new SLAM dataset covering 482 km of driving recorded using our\nasynchronous multi-camera robotic platform. AMV-Bench is over an order of\nmagnitude larger than previous multi-view HD outdoor SLAM datasets, and covers\ndiverse and challenging motions and environments. Our experiments emphasize the\nnecessity of asynchronous sensor modeling, and show that the use of multiple\ncameras is critical towards robust and accurate SLAM in challenging outdoor\nscenes. For additional information, please see the project website at:\nhttps://www.cs.toronto.edu/~ajyang/amv-slam",
          "link": "http://arxiv.org/abs/2101.06562",
          "publishedOn": "2021-07-16T00:48:24.104Z",
          "wordCount": 620,
          "title": "Asynchronous Multi-View SLAM. (arXiv:2101.06562v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yunlong_Z/0/1/0/all/0/1\">Zhang Yunlong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenxin_L/0/1/0/all/0/1\">Li Chenxin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_L/0/1/0/all/0/1\">Lin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liyan_S/0/1/0/all/0/1\">Sun Liyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yihong_Z/0/1/0/all/0/1\">Zhuang Yihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1\">Huang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinghao_D/0/1/0/all/0/1\">Ding Xinghao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaoqing_L/0/1/0/all/0/1\">Liu Xiaoqing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yizhou_Y/0/1/0/all/0/1\">Yu Yizhou</a>",
          "description": "This paper investigates the problem of pseudo-healthy synthesis that is\ndefined as synthesizing a subject-specific pathology-free image from a\npathological one. Recent approaches based on Generative Adversarial Network\n(GAN) have been developed for this task. However, these methods will inevitably\nfall into the trade-off between preserving the subject-specific identity and\ngenerating healthy-like appearances. To overcome this challenge, we propose a\nnovel adversarial training regime, Generator versus Segmentor (GVS), to\nalleviate this trade-off by a divide-and-conquer strategy. We further consider\nthe deteriorating generalization performance of the segmentor throughout the\ntraining and develop a pixel-wise weighted loss by muting the well-transformed\npixels to promote it. Moreover, we propose a new metric to measure how healthy\nthe synthetic images look. The qualitative and quantitative experiments on the\npublic dataset BraTS demonstrate that the proposed method outperforms the\nexisting methods. Besides, we also certify the effectiveness of our method on\ndatasets LiTS. Our implementation and pre-trained networks are publicly\navailable at https://github.com/Au3C2/Generator-Versus-Segmentor.",
          "link": "http://arxiv.org/abs/2009.05722",
          "publishedOn": "2021-07-16T00:48:24.080Z",
          "wordCount": 644,
          "title": "Generator Versus Segmentor: Pseudo-healthy Synthesis. (arXiv:2009.05722v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arun_N/0/1/0/all/0/1\">Nishanth Arun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaw_N/0/1/0/all/0/1\">Nathan Gaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praveer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Mehak Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bryan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sharut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1\">Jay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidwani_M/0/1/0/all/0/1\">Mishka Gidwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adebayo_J/0/1/0/all/0/1\">Julius Adebayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Matthew D. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>",
          "description": "Saliency maps have become a widely used method to make deep learning models\nmore interpretable by providing post-hoc explanations of classifiers through\nidentification of the most pertinent areas of the input medical image. They are\nincreasingly being used in medical imaging to provide clinically plausible\nexplanations for the decisions the neural network makes. However, the utility\nand robustness of these visualization maps has not yet been rigorously examined\nin the context of medical imaging. We posit that trustworthiness in this\ncontext requires 1) localization utility, 2) sensitivity to model weight\nrandomization, 3) repeatability, and 4) reproducibility. Using the localization\ninformation available in two large public radiology datasets, we quantify the\nperformance of eight commonly used saliency map approaches for the above\ncriteria using area under the precision-recall curves (AUPRC) and structural\nsimilarity index (SSIM), comparing their performance to various baseline\nmeasures. Using our framework to quantify the trustworthiness of saliency maps,\nwe show that all eight saliency map techniques fail at least one of the\ncriteria and are, in most cases, less trustworthy when compared to the\nbaselines. We suggest that their usage in the high-risk domain of medical\nimaging warrants additional scrutiny and recommend that detection or\nsegmentation models be used if localization is the desired output of the\nnetwork. Additionally, to promote reproducibility of our findings, we provide\nthe code we used for all tests performed in this work at this link:\nhttps://github.com/QTIM-Lab/Assessing-Saliency-Maps.",
          "link": "http://arxiv.org/abs/2008.02766",
          "publishedOn": "2021-07-16T00:48:24.064Z",
          "wordCount": 735,
          "title": "Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging. (arXiv:2008.02766v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kafri_O/0/1/0/all/0/1\">Omer Kafri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1\">Or Patashnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1\">Yuval Alaluf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>",
          "description": "We present StyleFusion, a new mapping architecture for StyleGAN, which takes\nas input a number of latent codes and fuses them into a single style code.\nInserting the resulting style code into a pre-trained StyleGAN generator\nresults in a single harmonized image in which each semantic region is\ncontrolled by one of the input latent codes. Effectively, StyleFusion yields a\ndisentangled representation of the image, providing fine-grained control over\neach region of the generated image. Moreover, to help facilitate global control\nover the generated image, a special input latent code is incorporated into the\nfused representation. StyleFusion operates in a hierarchical manner, where each\nlevel is tasked with learning to disentangle a pair of image regions (e.g., the\ncar body and wheels). The resulting learned disentanglement allows one to\nmodify both local, fine-grained semantics (e.g., facial features) as well as\nmore global features (e.g., pose and background), providing improved\nflexibility in the synthesis process. As a natural extension, StyleFusion\nenables one to perform semantically-aware cross-image mixing of regions that\nare not necessarily aligned. Finally, we demonstrate how StyleFusion can be\npaired with existing editing techniques to more faithfully constrain the edit\nto the user's region of interest.",
          "link": "http://arxiv.org/abs/2107.07437",
          "publishedOn": "2021-07-16T00:48:24.050Z",
          "wordCount": 641,
          "title": "StyleFusion: A Generative Model for Disentangling Spatial Segments. (arXiv:2107.07437v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.05383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-An Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Siang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Ya-Liang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "We study the XAI (explainable AI) on the face recognition task, particularly\nthe face verification here. Face verification is a crucial task in recent days\nand it has been deployed to plenty of applications, such as access control,\nsurveillance, and automatic personal log-on for mobile devices. With the\nincreasing amount of data, deep convolutional neural networks can achieve very\nhigh accuracy for the face verification task. Beyond exceptional performances,\ndeep face verification models need more interpretability so that we can trust\nthe results they generate. In this paper, we propose a novel similarity metric,\ncalled explainable cosine ($xCos$), that comes with a learnable module that can\nbe plugged into most of the verification models to provide meaningful\nexplanations. With the help of $xCos$, we can see which parts of the two input\nfaces are similar, where the model pays its attention to, and how the local\nsimilarities are weighted to form the output $xCos$ score. We demonstrate the\neffectiveness of our proposed method on LFW and various competitive benchmarks,\nresulting in not only providing novel and desiring model interpretability for\nface verification but also ensuring the accuracy as plugging into existing face\nrecognition models.",
          "link": "http://arxiv.org/abs/2003.05383",
          "publishedOn": "2021-07-16T00:48:24.013Z",
          "wordCount": 699,
          "title": "xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wisth_D/0/1/0/all/0/1\">David Wisth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camurri_M/0/1/0/all/0/1\">Marco Camurri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1\">Maurice Fallon</a>",
          "description": "We present VILENS (Visual Inertial Lidar Legged Navigation System), an\nodometry system for legged robots based on factor graphs. The key novelty is\nthe tight fusion of four different sensor modalities to achieve reliable\noperation when the individual sensors would otherwise produce degenerate\nestimation. To minimize leg odometry drift, we extend the robot's state with a\nlinear velocity bias term which is estimated online. This bias is only\nobservable because of the tight fusion of this preintegrated velocity factor\nwith vision, lidar, and IMU factors. Extensive experimental validation on the\nANYmal quadruped robots is presented, for a total duration of 2 h and 1.8 km\ntraveled. The experiments involved dynamic locomotion over loose rocks, slopes,\nand mud; these included perceptual challenges, such as dark and dusty\nunderground caverns or open, feature-deprived areas, as well as mobility\nchallenges such as slipping and terrain deformation. We show an average\nimprovement of 62% translational and 51% rotational errors compared to a\nstate-of-the-art loosely coupled approach. To demonstrate its robustness,\nVILENS was also integrated with a perceptive controller and a local path\nplanner.",
          "link": "http://arxiv.org/abs/2107.07243",
          "publishedOn": "2021-07-16T00:48:23.997Z",
          "wordCount": 623,
          "title": "VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots. (arXiv:2107.07243v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2008.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In view of the difficulty in reconstructing object details in point cloud\ncompletion, we propose a shape prior learning method for object completion. The\nshape priors include geometric information in both complete and the partial\npoint clouds. We design a feature alignment strategy to learn the shape prior\nfrom complete points, and a coarse to fine strategy to incorporate partial\nprior in the fine stage. To learn the complete objects prior, we first train a\npoint cloud auto-encoder to extract the latent embeddings from complete points.\nThen we learn a mapping to transfer the point features from partial points to\nthat of the complete points by optimizing feature alignment losses. The feature\nalignment losses consist of a L2 distance and an adversarial loss obtained by\nMaximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2\ndistance optimizes the partial features towards the complete ones in the\nfeature space, and MMD-GAN decreases the statistical distance of two point\nfeatures in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art\nperformances on the point cloud completion task. Our code is available at\nhttps://github.com/xiaogangw/point-cloud-completion-shape-prior.",
          "link": "http://arxiv.org/abs/2008.00394",
          "publishedOn": "2021-07-16T00:48:23.969Z",
          "wordCount": 661,
          "title": "Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:23.960Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1\">Lalith Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_G/0/1/0/all/0/1\">Gabriele Romano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehler_S/0/1/0/all/0/1\">Sven Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelm_H/0/1/0/all/0/1\">Halvar Kelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karck_M/0/1/0/all/0/1\">Matthias Karck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simone_R/0/1/0/all/0/1\">Raffaele De Simone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1\">Sandy Engelhardt</a>",
          "description": "The CycleGAN framework allows for unsupervised image-to-image translation of\nunpaired data. In a scenario of surgical training on a physical surgical\nsimulator, this method can be used to transform endoscopic images of phantoms\ninto images which more closely resemble the intra-operative appearance of the\nsame surgical target structure. This can be viewed as a novel augmented reality\napproach, which we coined Hyperrealism in previous work. In this use case, it\nis of paramount importance to display objects like needles, sutures or\ninstruments consistent in both domains while altering the style to a more\ntissue-like appearance. Segmentation of these objects would allow for a direct\ntransfer, however, contouring of these, partly tiny and thin foreground objects\nis cumbersome and perhaps inaccurate. Instead, we propose to use landmark\ndetection on the points when sutures pass into the tissue. This objective is\ndirectly incorporated into a CycleGAN framework by treating the performance of\npre-trained detector models as an additional optimization goal. We show that a\ntask defined on these sparse landmark labels improves consistency of synthesis\nby the generator network in both domains. Comparing a baseline CycleGAN\narchitecture to our proposed extension (DetCycleGAN), mean precision (PPV)\nimproved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by\n+0.4743. Furthermore, it could be shown that by dataset fusion, generated\nintra-operative images can be leveraged as additional training data for the\ndetection network itself. The data is released within the scope of the AdaptOR\nMICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at\nhttps://github.com/Cardio-AI/detcyclegan_pytorch.",
          "link": "http://arxiv.org/abs/2107.06941",
          "publishedOn": "2021-07-16T00:48:23.952Z",
          "wordCount": 720,
          "title": "Mutually improved endoscopic image synthesis and landmark detection in unpaired image-to-image translation. (arXiv:2107.06941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">Jay Roberts</a>",
          "description": "Deep neural networks are easily fooled by small perturbations known as\nadversarial attacks. Adversarial Training (AT) is a technique that\napproximately solves a robust optimization problem to minimize the worst-case\nloss and is widely regarded as the most effective defense against such attacks.\nWe develop a theoretical framework for adversarial training with FW\noptimization (FW-AT) that reveals a geometric connection between the loss\nlandscape and the distortion of $\\ell_\\infty$ FW attacks (the attack's $\\ell_2$\nnorm). Specifically, we show that high distortion of FW attacks is equivalent\nto low variation along the attack path. It is then experimentally demonstrated\non various deep neural network architectures that $\\ell_\\infty$ attacks against\nrobust models achieve near maximal $\\ell_2$ distortion. This mathematical\ntransparency differentiates FW from the more popular Projected Gradient Descent\n(PGD) optimization. To demonstrate the utility of our theoretical framework we\ndevelop FW-Adapt, a novel adversarial training algorithm which uses simple\ndistortion measure to adaptively change number of attack steps during training.\nFW-Adapt provides strong robustness at lower training times in comparison to\nPGD-AT for a variety of white-box and black-box attacks.",
          "link": "http://arxiv.org/abs/2012.12368",
          "publishedOn": "2021-07-16T00:48:23.934Z",
          "wordCount": 649,
          "title": "Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1\">Sudhanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With the advancement in the technology sector spanning over every field, a\nhuge influx of information is inevitable. Among all the opportunities that the\nadvancements in the technology have brought, one of them is to propose\nefficient solutions for data retrieval. This means that from an enormous pile\nof data, the retrieval methods should allow the users to fetch the relevant and\nrecent data over time. In the field of entertainment and e-commerce,\nrecommender systems have been functioning to provide the aforementioned.\nEmploying the same systems in the medical domain could definitely prove to be\nuseful in variety of ways. Following this context, the goal of this paper is to\npropose collaborative filtering based recommender system in the healthcare\nsector to recommend remedies based on the symptoms experienced by the patients.\nFurthermore, a new dataset is developed consisting of remedies concerning\nvarious diseases to address the limited availability of the data. The proposed\nrecommender system accepts the prognostic markers of a patient as the input and\ngenerates the best remedy course. With several experimental trials, the\nproposed model achieved promising results in recommending the possible remedy\nfor given prognostic markers.",
          "link": "http://arxiv.org/abs/2107.07500",
          "publishedOn": "2021-07-16T00:48:23.918Z",
          "wordCount": 640,
          "title": "Recommending best course of treatment based on similarities of prognostic markers\\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Migdal_P/0/1/0/all/0/1\">Piotr Migda&#x142;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olechno_B/0/1/0/all/0/1\">Bart&#x142;omiej Olechno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podgorski_B/0/1/0/all/0/1\">B&#x142;a&#x17c;ej Podg&#xf3;rski</a>",
          "description": "We present practical approaches of using deep learning to create and enhance\nlevel maps and textures for video games -- desktop, mobile, and web. We aim to\npresent new possibilities for game developers and level artists. The task of\ndesigning levels and filling them with details is challenging. It is both\ntime-consuming and takes effort to make levels rich, complex, and with a\nfeeling of being natural. Fortunately, recent progress in deep learning\nprovides new tools to accompany level designers and visual artists. Moreover,\nthey offer a way to generate infinite worlds for game replayability and adjust\neducational games to players' needs. We present seven approaches to create\nlevel maps, each using statistical methods, machine learning, or deep learning.\nIn particular, we include:\n\n- Generative Adversarial Networks for creating new images from existing\nexamples (e.g. ProGAN).\n\n- Super-resolution techniques for upscaling images while preserving crisp\ndetail (e.g. ESRGAN).\n\n- Neural style transfer for changing visual themes.\n\n- Image translation - turning semantic maps into images (e.g. GauGAN).\n\n- Semantic segmentation for turning images into semantic masks (e.g. U-Net).\n\n- Unsupervised semantic segmentation for extracting semantic features (e.g.\nTile2Vec).\n\n- Texture synthesis - creating large patterns based on a smaller sample (e.g.\nInGAN).",
          "link": "http://arxiv.org/abs/2107.07397",
          "publishedOn": "2021-07-16T00:48:23.875Z",
          "wordCount": 666,
          "title": "Level generation and style enhancement -- deep learning for game development overview. (arXiv:2107.07397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07271",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1\">Andrew Moyes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1\">Richard Gault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1\">Ji Ming</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1\">Danny Crookes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>",
          "description": "Domain shift is a problem commonly encountered when developing automated\nhistopathology pipelines. The performance of machine learning models such as\nconvolutional neural networks within automated histopathology pipelines is\noften diminished when applying them to novel data domains due to factors\narising from differing staining and scanning protocols. The Dual-Channel\nAuto-Encoder (DCAE) model was previously shown to produce feature\nrepresentations that are less sensitive to appearance variation introduced by\ndifferent digital slide scanners. In this work, the Multi-Channel Auto-Encoder\n(MCAE) model is presented as an extension to DCAE which learns from more than\ntwo domains of data. Additionally, a synthetic dataset is generated using\nCycleGANs that contains aligned tissue images that have had their appearance\nsynthetically modified. Experimental results show that the MCAE model produces\nfeature representations that are less sensitive to inter-domain variations than\nthe comparative StaNoSA method when tested on the novel synthetic data.\nAdditionally, the MCAE and StaNoSA models are tested on a novel tissue\nclassification task. The results of this experiment show the MCAE model out\nperforms the StaNoSA model by 5 percentage-points in the f1-score. These\nresults show that the MCAE model is able to generalise better to novel data and\ntasks than existing approaches by actively learning normalised feature\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.07271",
          "publishedOn": "2021-07-16T00:48:23.866Z",
          "wordCount": 669,
          "title": "Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miah_M/0/1/0/all/0/1\">Mehdi Miah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>",
          "description": "We propose a method for multi-object tracking and segmentation (MOTS) that\ndoes not require fine-tuning or per benchmark hyperparameter selection. The\nproposed method addresses particularly the data association problem. Indeed,\nthe recently introduced HOTA metric, that has a better alignment with the human\nvisual assessment by evenly balancing detections and associations quality, has\nshown that improvements are still needed for data association. After creating\ntracklets using instance segmentation and optical flow, the proposed method\nrelies on a space-time memory network (STM) developed for one-shot video object\nsegmentation to improve the association of tracklets with temporal gaps. To the\nbest of our knowledge, our method, named MeNToS, is the first to use the STM\nnetwork to track object masks for MOTS. We took the 4th place in the RobMOTS\nchallenge. The project page is https://mehdimiah.com/mentos.html.",
          "link": "http://arxiv.org/abs/2107.07067",
          "publishedOn": "2021-07-16T00:48:23.860Z",
          "wordCount": 585,
          "title": "MeNToS: Tracklets Association with a Space-Time Memory Network. (arXiv:2107.07067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_M/0/1/0/all/0/1\">Manuel Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1\">Ruben Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masia_B/0/1/0/all/0/1\">Belen Masia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_D/0/1/0/all/0/1\">Diego Gutierrez</a>",
          "description": "We present a single-image data-driven method to automatically relight images\nwith full-body humans in them. Our framework is based on a realistic scene\ndecomposition leveraging precomputed radiance transfer (PRT) and spherical\nharmonics (SH) lighting. In contrast to previous work, we lift the assumptions\non Lambertian materials and explicitly model diffuse and specular reflectance\nin our data. Moreover, we introduce an additional light-dependent residual term\nthat accounts for errors in the PRT-based image reconstruction. We propose a\nnew deep learning architecture, tailored to the decomposition performed in PRT,\nthat is trained using a combination of L1, logarithmic, and rendering losses.\nOur model outperforms the state of the art for full-body human relighting both\nwith synthetic images and photographs.",
          "link": "http://arxiv.org/abs/2107.07259",
          "publishedOn": "2021-07-16T00:48:23.853Z",
          "wordCount": 570,
          "title": "Single-image Full-body Human Relighting. (arXiv:2107.07259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1\">Ivona Najdenkoska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Automating report generation for medical imaging promises to reduce workload\nand assist diagnosis in clinical practice. Recent work has shown that deep\nlearning models can successfully caption natural images. However, learning from\nmedical data is challenging due to the diversity and uncertainty inherent in\nthe reports written by different radiologists with discrepant expertise and\nexperience. To tackle these challenges, we propose variational topic inference\nfor automatic report generation. Specifically, we introduce a set of topics as\nlatent variables to guide sentence generation by aligning image and language\nmodalities in a latent space. The topics are inferred in a conditional\nvariational inference framework, with each topic governing the generation of a\nsentence in the report. Further, we adopt a visual attention module that\nenables the model to attend to different locations in the image and generate\nmore informative descriptions. We conduct extensive experiments on two\nbenchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results\ndemonstrate that our proposed variational topic inference method can generate\nnovel reports rather than mere copies of reports used in training, while still\nachieving comparable performance to state-of-the-art methods in terms of\nstandard language generation criteria.",
          "link": "http://arxiv.org/abs/2107.07314",
          "publishedOn": "2021-07-16T00:48:23.836Z",
          "wordCount": 653,
          "title": "Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sangmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Junhyug Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangil Kim</a>",
          "description": "Identifying relations between objects is central to understanding the scene.\nWhile several works have been proposed for relation modeling in the image\ndomain, there have been many constraints in the video domain due to challenging\ndynamics of spatio-temporal interactions (e.g., Between which objects are there\nan interaction? When do relations occur and end?). To date, two representative\nmethods have been proposed to tackle Video Visual Relation Detection (VidVRD):\nsegment-based and window-based. We first point out the limitations these two\nmethods have and propose Temporal Span Proposal Network (TSPN), a novel method\nwith two advantages in terms of efficiency and effectiveness. 1) TSPN tells\nwhat to look: it sparsifies relation search space by scoring relationness\n(i.e., confidence score for the existence of a relation between pair of\nobjects) of object pair. 2) TSPN tells when to look: it leverages the full\nvideo context to simultaneously predict the temporal span and categories of the\nentire relations. TSPN demonstrates its effectiveness by achieving new\nstate-of-the-art by a significant margin on two VidVRD benchmarks\n(ImageNet-VidVDR and VidOR) while also showing lower time complexity than\nexisting methods - in particular, twice as efficient as a popular segment-based\napproach.",
          "link": "http://arxiv.org/abs/2107.07154",
          "publishedOn": "2021-07-16T00:48:23.829Z",
          "wordCount": 669,
          "title": "What and When to Look?: Temporal Span Proposal Network for Video Visual Relation Detection. (arXiv:2107.07154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Vision and Language Navigation (VLN) requires an agent to navigate to a\ntarget location by following natural language instructions. Most of existing\nworks represent a navigation candidate by the feature of the corresponding\nsingle view where the candidate lies in. However, an instruction may mention\nlandmarks out of the single view as references, which might lead to failures of\ntextual-visual matching of existing methods. In this work, we propose a\nmulti-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate\nvisual contexts from neighbor views for better textual-visual matching.\nSpecifically, our NvEM utilizes a subject module and a reference module to\ncollect contexts from neighbor views. The subject module fuses neighbor views\nat a global level, and the reference module fuses neighbor objects at a local\nlevel. Subjects and references are adaptively determined via attention\nmechanisms. Our model also includes an action module to utilize the strong\norientation guidance (e.g., ``turn left'') in instructions. Each module\npredicts navigation action separately and their weighted sum is used for\npredicting the final action. Extensive experimental results demonstrate the\neffectiveness of the proposed method on the R2R and R4R benchmarks against\nseveral state-of-the-art navigators, and NvEM even beats some pre-training\nones. Our code is available at https://github.com/MarSaKi/NvEM.",
          "link": "http://arxiv.org/abs/2107.07201",
          "publishedOn": "2021-07-16T00:48:23.822Z",
          "wordCount": 643,
          "title": "Neighbor-view Enhanced Model for Vision and Language Navigation. (arXiv:2107.07201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xunli Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Amodal segmentation is a new direction of instance segmentation while\nconsidering the segmentation of the visible and occluded parts of the instance.\nThe existing state-of-the-art method uses multi-task branches to predict the\namodal part and the visible part separately and subtract the visible part from\nthe amodal part to obtain the occluded part. However, the amodal part contains\nvisible information. Therefore, the separated prediction method will generate\nduplicate information. Different from this method, we propose a method of\namodal segmentation based on the idea of the jigsaw. The method uses multi-task\nbranches to predict the two naturally decoupled parts of visible and occluded,\nwhich is like getting two matching jigsaw pieces. Then put the two jigsaw\npieces together to get the amodal part. This makes each branch focus on the\nmodeling of the object. And we believe that there are certain rules in the\nocclusion relationship in the real world. This is a kind of occlusion context\ninformation. This jigsaw method can better model the occlusion relationship and\nuse the occlusion context information, which is important for amodal\nsegmentation. Experiments on two widely used amodally annotated datasets prove\nthat our method exceeds existing state-of-the-art methods. The source code of\nthis work will be made public soon.",
          "link": "http://arxiv.org/abs/2107.07464",
          "publishedOn": "2021-07-16T00:48:23.816Z",
          "wordCount": 636,
          "title": "Amodal segmentation just like doing a jigsaw. (arXiv:2107.07464v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sourav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Suresh Sundaram</a>",
          "description": "In this paper, a novel confidence conditioned knowledge distillation (CCKD)\nscheme for transferring the knowledge from a teacher model to a student model\nis proposed. Existing state-of-the-art methods employ fixed loss functions for\nthis purpose and ignore the different levels of information that need to be\ntransferred for different samples. In addition to that, these methods are also\ninefficient in terms of data usage. CCKD addresses these issues by leveraging\nthe confidence assigned by the teacher model to the correct class to devise\nsample-specific loss functions (CCKD-L formulation) and targets (CCKD-T\nformulation). Further, CCKD improves the data efficiency by employing\nself-regulation to stop those samples from participating in the distillation\nprocess on which the student model learns faster. Empirical evaluations on\nseveral benchmark datasets show that CCKD methods achieve at least as much\ngeneralization performance levels as other state-of-the-art methods while being\ndata efficient in the process. Student models trained through CCKD methods do\nnot retain most of the misclassifications commited by the teacher model on the\ntraining set. Distillation through CCKD methods improves the resilience of the\nstudent models against adversarial attacks compared to the conventional KD\nmethod. Experiments show at least 3% increase in performance against\nadversarial attacks for the MNIST and the Fashion MNIST datasets, and at least\n6% increase for the CIFAR10 dataset.",
          "link": "http://arxiv.org/abs/2107.06993",
          "publishedOn": "2021-07-16T00:48:23.808Z",
          "wordCount": 650,
          "title": "Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sobh_I/0/1/0/all/0/1\">Ibrahim Sobh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamed_A/0/1/0/all/0/1\">Ahmed Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Deep neural networks (DNNs) have accomplished impressive success in various\napplications, including autonomous driving perception tasks, in recent years.\nOn the other hand, current deep neural networks are easily fooled by\nadversarial attacks. This vulnerability raises significant concerns,\nparticularly in safety-critical applications. As a result, research into\nattacking and defending DNNs has gained much coverage. In this work, detailed\nadversarial attacks are applied on a diverse multi-task visual perception deep\nnetwork across distance estimation, semantic segmentation, motion detection,\nand object detection. The experiments consider both white and black box attacks\nfor targeted and un-targeted cases, while attacking a task and inspecting the\neffect on all the others, in addition to inspecting the effect of applying a\nsimple defense method. We conclude this paper by comparing and discussing the\nexperimental results, proposing insights and future work. The visualizations of\nthe attacks are available at https://youtu.be/R3JUV41aiPY.",
          "link": "http://arxiv.org/abs/2107.07449",
          "publishedOn": "2021-07-16T00:48:23.791Z",
          "wordCount": 585,
          "title": "Adversarial Attacks on Multi-task Visual Perception for Autonomous Driving. (arXiv:2107.07449v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1\">Puneet Mangla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandhok_S/0/1/0/all/0/1\">Shivam Chandhok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>",
          "description": "Recent progress towards designing models that can generalize to unseen\ndomains (i.e domain generalization) or unseen classes (i.e zero-shot learning)\nhas embarked interest towards building models that can tackle both domain-shift\nand semantic shift simultaneously (i.e zero-shot domain generalization). For\nmodels to generalize to unseen classes in unseen domains, it is crucial to\nlearn feature representation that preserves class-level (domain-invariant) as\nwell as domain-specific information. Motivated from the success of generative\nzero-shot approaches, we propose a feature generative framework integrated with\na COntext COnditional Adaptive (COCOA) Batch-Normalization to seamlessly\nintegrate class-level semantic and domain-specific information. The generated\nvisual features better capture the underlying data distribution enabling us to\ngeneralize to unseen classes and domains at test-time. We thoroughly evaluate\nand analyse our approach on established large-scale benchmark - DomainNet and\ndemonstrate promising performance over baselines and state-of-art methods.",
          "link": "http://arxiv.org/abs/2107.07497",
          "publishedOn": "2021-07-16T00:48:23.785Z",
          "wordCount": 577,
          "title": "Context-Conditional Adaptation for Recognizing Unseen Classes in Unseen Domains. (arXiv:2107.07497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07468",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bertoldo_J/0/1/0/all/0/1\">Jo&#xe3;o P C Bertoldo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Decenciere_E/0/1/0/all/0/1\">Etienne Decenci&#xe8;re</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ryckelynck_D/0/1/0/all/0/1\">David Ryckelynck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Proudhon_H/0/1/0/all/0/1\">Henry Proudhon</a>",
          "description": "X-ray Computed Tomography (XCT) techniques have evolved to a point that\nhigh-resolution data can be acquired so fast that classic segmentation methods\nare prohibitively cumbersome, demanding automated data pipelines capable of\ndealing with non-trivial 3D images. Deep learning has demonstrated success in\nmany image processing tasks, including material science applications, showing a\npromising alternative for a humanfree segmentation pipeline. In this paper a\nmodular interpretation of UNet (Modular U-Net) is proposed and trained to\nsegment 3D tomography images of a three-phased glass fiber-reinforced Polyamide\n66. We compare 2D and 3D versions of our model, finding that the former is\nslightly better than the latter. We observe that human-comparable results can\nbe achievied even with only 10 annotated layers and using a shallow U-Net\nyields better results than a deeper one. As a consequence, Neural Network (NN)\nshow indeed a promising venue to automate XCT data processing pipelines needing\nno human, adhoc intervention.",
          "link": "http://arxiv.org/abs/2107.07468",
          "publishedOn": "2021-07-16T00:48:23.779Z",
          "wordCount": 630,
          "title": "A modular U-Net for automated segmentation of X-ray tomography images in composite materials. (arXiv:2107.07468v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1\">Samet Akcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>",
          "description": "Identifying potential threats concealed within the baggage is of prime\nconcern for the security staff. Many researchers have developed frameworks that\ncan detect baggage threats from X-ray scans. However, to the best of our\nknowledge, all of these frameworks require extensive training on large-scale\nand well-annotated datasets, which are hard to procure in the real world. This\npaper presents a novel unsupervised anomaly instance segmentation framework\nthat recognizes baggage threats, in X-ray scans, as anomalies without requiring\nany ground truth labels. Furthermore, thanks to its stylization capacity, the\nframework is trained only once, and at the inference stage, it detects and\nextracts contraband items regardless of their scanner specifications. Our\none-staged approach initially learns to reconstruct normal baggage content via\nan encoder-decoder network utilizing a proposed stylization loss function. The\nmodel subsequently identifies the abnormal regions by analyzing the disparities\nwithin the original and the reconstructed scans. The anomalous regions are then\nclustered and post-processed to fit a bounding box for their localization. In\naddition, an optional classifier can also be appended with the proposed\nframework to recognize the categories of these extracted anomalies. A thorough\nevaluation of the proposed system on four public baggage X-ray datasets,\nwithout any re-training, demonstrates that it achieves competitive performance\nas compared to the conventional fully supervised methods (i.e., the mean\naverage precision score of 0.7941 on SIXray, 0.8591 on GDXray, 0.7483 on\nOPIXray, and 0.5439 on COMPASS-XP dataset) while outperforming state-of-the-art\nsemi-supervised and unsupervised baggage threat detection frameworks by 67.37%,\n32.32%, 47.19%, and 45.81% in terms of F1 score across SIXray, GDXray, OPIXray,\nand COMPASS-XP datasets, respectively.",
          "link": "http://arxiv.org/abs/2107.07333",
          "publishedOn": "2021-07-16T00:48:23.766Z",
          "wordCount": 708,
          "title": "Unsupervised Anomaly Instance Segmentation for Baggage Threat Recognition. (arXiv:2107.07333v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07436",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1\">Neil Jethani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1\">Mukund Sudarshan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Shapley values are widely used to explain black-box models, but they are\ncostly to calculate because they require many model evaluations. We introduce\nFastSHAP, a method for estimating Shapley values in a single forward pass using\na learned explainer model. FastSHAP amortizes the cost of explaining many\ninputs via a learning approach inspired by the Shapley value's weighted least\nsquares characterization, and it can be trained using standard stochastic\ngradient optimization. We compare FastSHAP to existing estimation approaches,\nrevealing that it generates high-quality explanations with orders of magnitude\nspeedup.",
          "link": "http://arxiv.org/abs/2107.07436",
          "publishedOn": "2021-07-16T00:48:23.748Z",
          "wordCount": 533,
          "title": "FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">D. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">J. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">J. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">K. Lee</a>",
          "description": "In the process of intelligently segmenting foods in images using deep neural\nnetworks for diet management, data collection and labeling for network training\nare very important but labor-intensive tasks. In order to solve the\ndifficulties of data collection and annotations, this paper proposes a food\nsegmentation method applicable to real-world through synthetic data. To perform\nfood segmentation on healthcare robot systems, such as meal assistance robot\narm, we generate synthetic data using the open-source 3D graphics software\nBlender placing multiple objects on meal plate and train Mask R-CNN for\ninstance segmentation. Also, we build a data collection system and verify our\nsegmentation model on real-world food data. As a result, on our real-world\ndataset, the model trained only synthetic data is available to segment food\ninstances that are not trained with 52.2% mask AP@all, and improve performance\nby +6.4%p after fine-tuning comparing to the model trained from scratch. In\naddition, we also confirm the possibility and performance improvement on the\npublic dataset for fair analysis. Our code and pre-trained weights are\navaliable online at: https://github.com/gist-ailab/Food-Instance-Segmentation",
          "link": "http://arxiv.org/abs/2107.07191",
          "publishedOn": "2021-07-16T00:48:23.735Z",
          "wordCount": 621,
          "title": "Deep Learning based Food Instance Segmentation using Synthetic Data. (arXiv:2107.07191v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1\">Amirreza Yousefzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1\">Manolis Sifalakis</a>",
          "description": "Activation sparsity improves compute efficiency and resource utilization in\nsparsity-aware neural network accelerators. As the predominant operation in\nDNNs is multiply-accumulate (MAC) of activations with weights to compute inner\nproducts, skipping operations where (at least) one of the two operands is zero\ncan make inference more efficient in terms of latency and power. Spatial\nsparsification of activations is a popular topic in DNN literature and several\nmethods have already been established to bias a DNN for it. On the other hand,\ntemporal sparsity is an inherent feature of bio-inspired spiking neural\nnetworks (SNNs), which neuromorphic processing exploits for hardware\nefficiency. Introducing and exploiting spatio-temporal sparsity, is a topic\nmuch less explored in DNN literature, but in perfect resonance with the trend\nin DNN, to shift from static signal processing to more streaming signal\nprocessing. Towards this goal, in this paper we introduce a new DNN layer\n(called Delta Activation Layer), whose sole purpose is to promote temporal\nsparsity of activations during training. A Delta Activation Layer casts\ntemporal sparsity into spatial activation sparsity to be exploited when\nperforming sparse tensor multiplications in hardware. By employing delta\ninference and ``the usual'' spatial sparsification heuristics during training,\nthe resulting model learns to exploit not only spatial but also temporal\nactivation sparsity (for a given input data distribution). One may use the\nDelta Activation Layer either during vanilla training or during a refinement\nphase. We have implemented Delta Activation Layer as an extension of the\nstandard Tensoflow-Keras library, and applied it to train deep neural networks\non the Human Action Recognition (UCF101) dataset. We report an almost 3x\nimprovement of activation sparsity, with recoverable loss of model accuracy\nafter longer training.",
          "link": "http://arxiv.org/abs/2107.07305",
          "publishedOn": "2021-07-16T00:48:23.725Z",
          "wordCount": 728,
          "title": "Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1\">Gereon Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1\">Mohamed Elgharib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "Generative adversarial models (GANs) continue to produce advances in terms of\nthe visual quality of still images, as well as the learning of temporal\ncorrelations. However, few works manage to combine these two interesting\ncapabilities for the synthesis of video content: Most methods require an\nextensive training dataset in order to learn temporal correlations, while being\nrather limited in the resolution and visual quality of their output frames. In\nthis paper, we present a novel approach to the video synthesis problem that\nhelps to greatly improve visual quality and drastically reduce the amount of\ntraining data and resources necessary for generating video content. Our\nformulation separates the spatial domain, in which individual frames are\nsynthesized, from the temporal domain, in which motion is generated. For the\nspatial domain we make use of a pre-trained StyleGAN network, the latent space\nof which allows control over the appearance of the objects it was trained for.\nThe expressive power of this model allows us to embed our training videos in\nthe StyleGAN latent space. Our temporal architecture is then trained not on\nsequences of RGB frames, but on sequences of StyleGAN latent codes. The\nadvantageous properties of the StyleGAN space simplify the discovery of\ntemporal correlations. We demonstrate that it suffices to train our temporal\narchitecture on only 10 minutes of footage of 1 subject for about 6 hours.\nAfter training, our model can not only generate new portrait videos for the\ntraining subject, but also for any random subject which can be embedded in the\nStyleGAN space.",
          "link": "http://arxiv.org/abs/2107.07224",
          "publishedOn": "2021-07-16T00:48:23.715Z",
          "wordCount": 693,
          "title": "StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN. (arXiv:2107.07224v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yikun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>",
          "description": "Online advertising is ubiquitous in web business. Image displaying is\nconsidered as one of the most commonly used formats to interact with customers.\nContextual multi-armed bandit has shown success in the application of\nadvertising to solve the exploration-exploitation dilemma existed in the\nrecommendation procedure. Inspired by the visual-aware advertising, in this\npaper, we propose a contextual bandit algorithm, where the convolutional neural\nnetwork (CNN) is utilized to learn the reward function along with an upper\nconfidence bound (UCB) for exploration. We also prove a near-optimal regret\nbound $\\tilde{\\mathcal{O}}(\\sqrt{T})$ when the network is over-parameterized\nand establish strong connections with convolutional neural tangent kernel\n(CNTK). Finally, we evaluate the empirical performance of the proposed\nalgorithm and show that it outperforms other state-of-the-art UCB-based bandit\nalgorithms on real-world image data sets.",
          "link": "http://arxiv.org/abs/2107.07438",
          "publishedOn": "2021-07-16T00:48:23.709Z",
          "wordCount": 565,
          "title": "Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deane_J/0/1/0/all/0/1\">Jake Deane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearney_S/0/1/0/all/0/1\">Sinead Kearney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwang In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1\">Darren Cosker</a>",
          "description": "Synthetic data is becoming increasingly common for training computer vision\nmodels for a variety of tasks. Notably, such data has been applied in tasks\nrelated to humans such as 3D pose estimation where data is either difficult to\ncreate or obtain in realistic settings. Comparatively, there has been less work\ninto synthetic animal data and it's uses for training models. Consequently, we\nintroduce a parametric canine model, DynaDog+T, for generating synthetic canine\nimages and data which we use for a common computer vision task, binary\nsegmentation, which would otherwise be difficult due to the lack of available\ndata.",
          "link": "http://arxiv.org/abs/2107.07330",
          "publishedOn": "2021-07-16T00:48:23.691Z",
          "wordCount": 544,
          "title": "DynaDog+T: A Parametric Animal Model for Synthetic Canine Image Generation. (arXiv:2107.07330v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Nico Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>",
          "description": "The increasing demand for commodities is leading to changes in land use\nworldwide. In the tropics, deforestation, which causes high carbon emissions\nand threatens biodiversity, is often linked to agricultural expansion. While\nthe need for deforestation-free global supply chains is widely recognized,\nmaking progress in practice remains a challenge. Here, we propose an automated\napproach that aims to support conservation and sustainable land use planning\ndecisions by mapping tropical landscapes at large scale and high spatial\nresolution following the High Carbon Stock (HCS) approach. A deep learning\napproach is developed that estimates canopy height for each 10 m Sentinel-2\npixel by learning from sparse GEDI LIDAR reference data, achieving an overall\nRMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are\npredictive for classifying HCS forests and degraded areas with an overall\naccuracy of 86 % and produce a first high carbon stock map for Indonesia,\nMalaysia, and the Philippines.",
          "link": "http://arxiv.org/abs/2107.07431",
          "publishedOn": "2021-07-16T00:48:23.675Z",
          "wordCount": 606,
          "title": "High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The increasing population of elderly people is associated with the need to\nmeet their increasing requirements and to provide solutions that can improve\ntheir quality of life in a smart home. In addition to fear and anxiety towards\ninterfacing with systems; cognitive disabilities, weakened memory, disorganized\nbehavior and even physical limitations are some of the problems that elderly\npeople tend to face with increasing age. The essence of providing\ntechnology-based solutions to address these needs of elderly people and to\ncreate smart and assisted living spaces for the elderly; lies in developing\nsystems that can adapt by addressing their diversity and can augment their\nperformances in the context of their day to day goals. Therefore, this work\nproposes a framework for development of a Personalized Intelligent Assistant to\nhelp elderly people perform Activities of Daily Living (ADLs) in a smart and\nconnected Internet of Things (IoT) based environment. This Personalized\nIntelligent Assistant can analyze different tasks performed by the user and\nrecommend activities by considering their daily routine, current affective\nstate and the underlining user experience. To uphold the efficacy of this\nproposed framework, it has been tested on a couple of datasets for modelling an\naverage user and a specific user respectively. The results presented show that\nthe model achieves a performance accuracy of 73.12% when modelling a specific\nuser, which is considerably higher than its performance while modelling an\naverage user, this upholds the relevance for development and implementation of\nthis proposed framework.",
          "link": "http://arxiv.org/abs/2107.07344",
          "publishedOn": "2021-07-16T00:48:23.649Z",
          "wordCount": 731,
          "title": "Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Automatic image matting (AIM) refers to estimating the soft foreground from\nan arbitrary natural image without any auxiliary input like trimap, which is\nuseful for image editing. Prior methods try to learn semantic features to aid\nthe matting process while being limited to images with salient opaque\nforegrounds such as humans and animals. In this paper, we investigate the\ndifficulties when extending them to natural images with salient\ntransparent/meticulous foregrounds or non-salient foregrounds. To address the\nproblem, a novel end-to-end matting network is proposed, which can predict a\ngeneralized trimap for any image of the above types as a unified semantic\nrepresentation. Simultaneously, the learned semantic features guide the matting\nnetwork to focus on the transition areas via an attention mechanism. We also\nconstruct a test set AIM-500 that contains 500 diverse natural images covering\nall types along with manually labeled alpha mattes, making it feasible to\nbenchmark the generalization ability of AIM models. Results of the experiments\ndemonstrate that our network trained on available composite matting datasets\noutperforms existing methods both objectively and subjectively. The source code\nand dataset are available at https://github.com/JizhiziLi/AIM.",
          "link": "http://arxiv.org/abs/2107.07235",
          "publishedOn": "2021-07-16T00:48:23.587Z",
          "wordCount": 630,
          "title": "Deep Automatic Natural Image Matting. (arXiv:2107.07235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Corcoll_O/0/1/0/all/0/1\">Oriol Corcoll</a>",
          "description": "Automatic image cropping techniques are commonly used to enhance the\naesthetic quality of an image; they do it by detecting the most beautiful or\nthe most salient parts of the image and removing the unwanted content to have a\nsmaller image that is more visually pleasing. In this thesis, I introduce an\nadditional dimension to the problem of cropping, semantics. I argue that image\ncropping can also enhance the image's relevancy for a given entity by using the\nsemantic information contained in the image. I call this problem, Semantic\nImage Cropping. To support my argument, I provide a new dataset containing 100\nimages with at least two different entities per image and four ground truth\ncroppings collected using Amazon Mechanical Turk. I use this dataset to show\nthat state-of-the-art cropping algorithms that only take into account\naesthetics do not perform well in the problem of semantic image cropping.\nAdditionally, I provide a new deep learning system that takes not just\naesthetics but also semantics into account to generate image croppings, and I\nevaluate its performance using my new semantic cropping dataset, showing that\nusing the semantic information of an image can help to produce better\ncroppings.",
          "link": "http://arxiv.org/abs/2107.07153",
          "publishedOn": "2021-07-16T00:48:23.563Z",
          "wordCount": 615,
          "title": "Semantic Image Cropping. (arXiv:2107.07153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yakun Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_M/0/1/0/all/0/1\">Muwei Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shaoxiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>",
          "description": "The goal of photometric stereo is to measure the precise surface normal of a\n3D object from observations with various shading cues. However, non-Lambertian\nsurfaces influence the measurement accuracy due to irregular shading cues.\nDespite deep neural networks have been employed to simulate the performance of\nnon-Lambertian surfaces, the error in specularities, shadows, and crinkle\nregions is hard to be reduced. In order to address this challenge, we here\npropose a photometric stereo network that incorporates Lambertian priors to\nbetter measure the surface normal. In this paper, we use the initial normal\nunder the Lambertian assumption as the prior information to refine the normal\nmeasurement, instead of solely applying the observed shading cues to deriving\nthe surface normal. Our method utilizes the Lambertian information to\nreparameterize the network weights and the powerful fitting ability of deep\nneural networks to correct these errors caused by general reflectance\nproperties. Our explorations include: the Lambertian priors (1) reduce the\nlearning hypothesis space, making our method learn the mapping in the same\nsurface normal space and improving the accuracy of learning, and (2) provides\nthe differential features learning, improving the surfaces reconstruction of\ndetails. Extensive experiments verify the effectiveness of the proposed\nLambertian prior photometric stereo network in accurate surface normal\nmeasurement, on the challenging benchmark dataset.",
          "link": "http://arxiv.org/abs/2107.07192",
          "publishedOn": "2021-07-16T00:48:23.554Z",
          "wordCount": 653,
          "title": "Incorporating Lambertian Priors into Surface Normals Measurement. (arXiv:2107.07192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruohua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kazuki Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>",
          "description": "Multi-pedestrian trajectory prediction is an indispensable safety element of\nautonomous systems that interact with crowds in unstructured environments. Many\nrecent efforts have developed trajectory prediction algorithms with focus on\nunderstanding social norms behind pedestrian motions. Yet we observe these\nworks usually hold two assumptions that prevent them from being smoothly\napplied to robot applications: positions of all pedestrians are consistently\ntracked; the target agent pays attention to all pedestrians in the scene. The\nfirst assumption leads to biased interaction modeling with incomplete\npedestrian data, and the second assumption introduces unnecessary disturbances\nand leads to the freezing robot problem. Thus, we propose Gumbel Social\nTransformer, in which an Edge Gumbel Selector samples a sparse interaction\ngraph of partially observed pedestrians at each time step. A Node Transformer\nEncoder and a Masked LSTM encode the pedestrian features with the sampled\nsparse graphs to predict trajectories. We demonstrate that our model overcomes\nthe potential problems caused by the assumptions, and our approach outperforms\nthe related works in benchmark evaluation.",
          "link": "http://arxiv.org/abs/2107.07056",
          "publishedOn": "2021-07-16T00:48:23.530Z",
          "wordCount": 612,
          "title": "Learning Sparse Interaction Graphs of Partially Observed Pedestrians for Trajectory Prediction. (arXiv:2107.07056v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>",
          "description": "We present a generic method for recurrently using the same parameters for\nmany different convolution layers to build a deep network. Specifically, for a\nnetwork, we create a recurrent parameter generator (RPG), from which the\nparameters of each convolution layer are generated. Though using recurrent\nmodels to build a deep convolutional neural network (CNN) is not entirely new,\nour method achieves significant performance gain compared to the existing\nworks. We demonstrate how to build a one-layer neural network to achieve\nsimilar performance compared to other traditional CNN models on various\napplications and datasets. Such a method allows us to build an arbitrarily\ncomplex neural network with any amount of parameters. For example, we build a\nResNet34 with model parameters reduced by more than $400$ times, which still\nachieves $41.6\\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG\ncan be applied at different scales, such as layers, blocks, or even\nsub-networks. Specifically, we use the RPG to build a ResNet18 network with the\nnumber of weights equivalent to one convolutional layer of a conventional\nResNet and show this model can achieve $67.2\\%$ ImageNet top-1 accuracy. The\nproposed method can be viewed as an inverse approach to model compression.\nRather than removing the unused parameters from a large model, it aims to\nsqueeze more information into a small number of parameters. Extensive\nexperiment results are provided to demonstrate the power of the proposed\nrecurrent parameter generator.",
          "link": "http://arxiv.org/abs/2107.07110",
          "publishedOn": "2021-07-16T00:48:23.523Z",
          "wordCount": 666,
          "title": "Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kilic_V/0/1/0/all/0/1\">Velat Kilic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_D/0/1/0/all/0/1\">Deepti Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath Sindagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">A. Brinton Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_M/0/1/0/all/0/1\">Mark A. Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Lidar-based object detectors are critical parts of the 3D perception pipeline\nin autonomous navigation systems such as self-driving cars. However, they are\nknown to be sensitive to adverse weather conditions such as rain, snow and fog\ndue to reduced signal-to-noise ratio (SNR) and signal-to-background ratio\n(SBR). As a result, lidar-based object detectors trained on data captured in\nnormal weather tend to perform poorly in such scenarios. However, collecting\nand labelling sufficient training data in a diverse range of adverse weather\nconditions is laborious and prohibitively expensive. To address this issue, we\npropose a physics-based approach to simulate lidar point clouds of scenes in\nadverse weather conditions. These augmented datasets can then be used to train\nlidar-based detectors to improve their all-weather reliability. Specifically,\nwe introduce a hybrid Monte-Carlo based approach that treats (i) the effects of\nlarge particles by placing them randomly and comparing their back reflected\npower against the target, and (ii) attenuation effects on average through\ncalculation of scattering efficiencies from the Mie theory and particle size\ndistributions. Retraining networks with this augmented data improves mean\naverage precision evaluated on real world rainy scenes and we observe greater\nimprovement in performance with our model relative to existing models from the\nliterature. Furthermore, we evaluate recent state-of-the-art detectors on the\nsimulated weather conditions and present an in-depth analysis of their\nperformance.",
          "link": "http://arxiv.org/abs/2107.07004",
          "publishedOn": "2021-07-16T00:48:23.517Z",
          "wordCount": 683,
          "title": "Lidar Light Scattering Augmentation (LISA): Physics-based Simulation of Adverse Weather Conditions for 3D Object Detection. (arXiv:2107.07004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaomeng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leake_D/0/1/0/all/0/1\">David Leake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xizi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>",
          "description": "The case difference heuristic (CDH) approach is a knowledge-light method for\nlearning case adaptation knowledge from the case base of a case-based reasoning\nsystem. Given a pair of cases, the CDH approach attributes the difference in\ntheir solutions to the difference in the problems they solve, and generates\nadaptation rules to adjust solutions accordingly when a retrieved case and new\nquery have similar problem differences. As an alternative to learning\nadaptation rules, several researchers have applied neural networks to learn to\npredict solution differences from problem differences. Previous work on such\napproaches has assumed that the feature set describing problems is predefined.\nThis paper investigates a two-phase process combining deep learning for feature\nextraction and neural network based adaptation learning from extracted\nfeatures. Its performance is demonstrated in a regression task on an image\ndata: predicting age given the image of a face. Results show that the combined\nprocess can successfully learn adaptation knowledge applicable to nonsymbolic\ndifferences in cases. The CBR system achieves slightly lower performance\noverall than a baseline deep network regressor, but better performance than the\nbaseline on novel queries.",
          "link": "http://arxiv.org/abs/2107.07095",
          "publishedOn": "2021-07-16T00:48:23.509Z",
          "wordCount": 654,
          "title": "Applying the Case Difference Heuristic to Learn Adaptations from Deep Network Features. (arXiv:2107.07095v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_D/0/1/0/all/0/1\">Di You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jingfen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>",
          "description": "Recent deep network-based compressive sensing (CS) methods have achieved\ngreat success. However, most of them regard different sampling matrices as\ndifferent independent tasks and need to train a specific model for each target\nsampling matrix. Such practices give rise to inefficiency in computing and\nsuffer from poor generalization ability. In this paper, we propose a novel\nCOntrollable Arbitrary-Sampling neTwork, dubbed COAST, to solve CS problems of\narbitrary-sampling matrices (including unseen sampling matrices) with one\nsingle model. Under the optimization-inspired deep unfolding framework, our\nCOAST exhibits good interpretability. In COAST, a random projection\naugmentation (RPA) strategy is proposed to promote the training diversity in\nthe sampling space to enable arbitrary sampling, and a controllable proximal\nmapping module (CPMM) and a plug-and-play deblocking (PnP-D) strategy are\nfurther developed to dynamically modulate the network features and effectively\neliminate the blocking artifacts, respectively. Extensive experiments on widely\nused benchmark datasets demonstrate that our proposed COAST is not only able to\nhandle arbitrary sampling matrices with one single model but also to achieve\nstate-of-the-art performance with fast speed. The source code is available on\nhttps://github.com/jianzhangcs/COAST.",
          "link": "http://arxiv.org/abs/2107.07225",
          "publishedOn": "2021-07-16T00:48:23.418Z",
          "wordCount": 647,
          "title": "COAST: COntrollable Arbitrary-Sampling NeTwork for Compressive Sensing. (arXiv:2107.07225v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Feng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chonghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muralidhar_S/0/1/0/all/0/1\">Shivran Muralidhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vijaykrishnan Narayanan</a>",
          "description": "The cognitive system for human action and behavior has evolved into a deep\nlearning regime, and especially the advent of Graph Convolution Networks has\ntransformed the field in recent years. However, previous works have mainly\nfocused on over-parameterized and complex models based on dense graph\nconvolution networks, resulting in low efficiency in training and inference.\nMeanwhile, the Transformer architecture-based model has not yet been well\nexplored for cognitive application in human action and behavior estimation.\nThis work proposes a novel skeleton-based human action recognition model with\nsparse attention on the spatial dimension and segmented linear attention on the\ntemporal dimension of data. Our model can also process the variable length of\nvideo clips grouped as a single batch. Experiments show that our model can\nachieve comparable performance while utilizing much less trainable parameters\nand achieve high speed in training and inference. Experiments show that our\nmodel achieves 4~18x speedup and 1/7~1/15 model size compared with the baseline\nmodels at competitive accuracy.",
          "link": "http://arxiv.org/abs/2107.07089",
          "publishedOn": "2021-07-16T00:48:23.377Z",
          "wordCount": 605,
          "title": "STAR: Sparse Transformer-based Action Recognition. (arXiv:2107.07089v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1\">Vincenzo Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1\">Kaveh Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1\">Daniele Grandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1\">Bryony DuPont</a>",
          "description": "Function is defined as the ensemble of tasks that enable the product to\ncomplete the designed purpose. Functional tools, such as functional modeling,\noffer decision guidance in the early phase of product design, where explicit\ndesign decisions are yet to be made. Function-based design data is often sparse\nand grounded in individual interpretation. As such, function-based design tools\ncan benefit from automatic function classification to increase data fidelity\nand provide function representation models that enable function-based\nintelligent design agents. Function-based design data is commonly stored in\nmanually generated design repositories. These design repositories are a\ncollection of expert knowledge and interpretations of function in product\ndesign bounded by function-flow and component taxonomies. In this work, we\nrepresent a structured taxonomy-based design repository as assembly-flow\ngraphs, then leverage a graph neural network (GNN) model to perform automatic\nfunction classification. We support automated function classification by\nlearning from repository data to establish the ground truth of component\nfunction assignment. Experimental results show that our GNN model achieves a\nmicro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and\n0.783 for tier 3 (specific) functions. Given the imbalance of data features,\nthe results are encouraging. Our efforts in this paper can be a starting point\nfor more sophisticated applications in knowledge-based CAD systems and\nDesign-for-X consideration in function-based design.",
          "link": "http://arxiv.org/abs/2107.07042",
          "publishedOn": "2021-07-16T00:48:23.370Z",
          "wordCount": 660,
          "title": "Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shi-Yao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chung-Yen Su</a>",
          "description": "Nowadays, due to the rapid population expansion, food shortage has become a\ncritical issue. In order to stabilizing the food source production, preventing\ncrops from being attacked by pests is very important. In generally, farmers use\npesticides to kill pests, however, improperly using pesticides will also kill\nsome insects which is beneficial to crops, such as bees. If the number of bees\nis too few, the supplement of food in the world will be in short. Besides,\nexcessive pesticides will seriously pollute the environment. Accordingly,\nfarmers need a machine which can automatically recognize the pests. Recently,\ndeep learning is popular because its effectiveness in the field of image\nclassification. In this paper, we propose a small and efficient model called\nExquisiteNet to complete the task of recognizing the pests and we expect to\napply our model on mobile devices. ExquisiteNet mainly consists of two blocks.\nOne is double fusion with squeeze-and-excitation-bottleneck block (DFSEB\nblock), and the other is max feature expansion block (ME block). ExquisiteNet\nonly has 0.98M parameters and its computing speed is very fast almost the same\nas SqueezeNet. In order to evaluate our model's performance, we test our model\non a benchmark pest dataset called IP102. Compared to many state-of-the-art\nmodels, such as ResNet101, ShuffleNetV2, MobileNetV3-large and EfficientNet\netc., our model achieves higher accuracy, that is, 52.32% on the test set of\nIP102 without any data augmentation.",
          "link": "http://arxiv.org/abs/2107.07167",
          "publishedOn": "2021-07-16T00:48:23.363Z",
          "wordCount": 675,
          "title": "An Efficient and Small Convolutional Neural Network for Pest Recognition -- ExquisiteNet. (arXiv:2107.07167v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ufer_N/0/1/0/all/0/1\">Nikolai Ufer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_S/0/1/0/all/0/1\">Sabine Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "The search for specific objects or motifs is essential to art history as both\nassist in decoding the meaning of artworks. Digitization has produced large art\ncollections, but manual methods prove to be insufficient to analyze them. In\nthe following, we introduce an algorithm that allows users to search for image\nregions containing specific motifs or objects and find similar regions in an\nextensive dataset, helping art historians to analyze large digitized art\ncollections. Computer vision has presented efficient methods for visual\ninstance retrieval across photographs. However, applied to art collections,\nthey reveal severe deficiencies because of diverse motifs and massive domain\nshifts induced by differences in techniques, materials, and styles. In this\npaper, we present a multi-style feature fusion approach that successfully\nreduces the domain gap and improves retrieval results without labelled data or\ncurated image collections. Our region-based voting with GPU-accelerated\napproximate nearest-neighbour search allows us to find and localize even small\nmotifs within an extensive dataset in a few seconds. We obtain state-of-the-art\nresults on the Brueghel dataset and demonstrate its generalization to\ninhomogeneous collections with a large number of distractors.",
          "link": "http://arxiv.org/abs/2107.06935",
          "publishedOn": "2021-07-16T00:48:23.356Z",
          "wordCount": 647,
          "title": "Object Retrieval and Localization in Large Art Collections using Deep Multi-Style Feature Fusion and Iterative Voting. (arXiv:2107.06935v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-16T00:48:23.350Z",
          "wordCount": 707,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengjie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaoqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiyu Song</a>",
          "description": "Up-to-date High-Definition (HD) maps are essential for self-driving cars. To\nachieve constantly updated HD maps, we present a deep neural network (DNN),\nDiff-Net, to detect changes in them. Compared to traditional methods based on\nobject detectors, the essential design in our work is a parallel feature\ndifference calculation structure that infers map changes by comparing features\nextracted from the camera and rasterized images. To generate these rasterized\nimages, we project map elements onto images in the camera view, yielding\nmeaningful map representations that can be consumed by a DNN accordingly. As we\nformulate the change detection task as an object detection problem, we leverage\nthe anchor-based structure that predicts bounding boxes with different change\nstatus categories. Furthermore, rather than relying on single frame input, we\nintroduce a spatio-temporal fusion module that fuses features from history\nframes into the current, thus improving the overall performance. Finally, we\ncomprehensively validate our method's effectiveness using freshly collected\ndatasets. Results demonstrate that our Diff-Net achieves better performance\nthan the baseline methods and is ready to be integrated into a map production\npipeline maintaining an up-to-date HD map.",
          "link": "http://arxiv.org/abs/2107.07030",
          "publishedOn": "2021-07-16T00:48:23.333Z",
          "wordCount": 631,
          "title": "Diff-Net: Image Feature Difference based High-Definition Map Change Detection. (arXiv:2107.07030v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1\">Mauricio Mendez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1\">Ivan Garcia Jorge Gonzalez-Zapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1\">Andres Mendez-Vazquez</a>",
          "description": "Few-shot learning is a relatively new technique that specializes in problems\nwhere we have little amounts of data. The goal of these methods is to classify\ncategories that have not been seen before with just a handful of samples.\nRecent approaches, such as metric learning, adopt the meta-learning strategy in\nwhich we have episodic tasks conformed by support (training) data and query\n(test) data. Metric learning methods have demonstrated that simple models can\nachieve good performance by learning a similarity function to compare the\nsupport and the query data. However, the feature space learned by a given\nmetric learning approach may not exploit the information given by a specific\nfew-shot task. In this work, we explore the use of dimension reduction\ntechniques as a way to find task-significant features helping to make better\npredictions. We measure the performance of the reduced features by assigning a\nscore based on the intra-class and inter-class distance, and selecting a\nfeature reduction method in which instances of different classes are far away\nand instances of the same class are close. This module helps to improve the\naccuracy performance by allowing the similarity function, given by the metric\nlearning method, to have more discriminative features for the classification.\nOur method outperforms the metric learning baselines in the miniImageNet\ndataset by around 2% in accuracy performance.",
          "link": "http://arxiv.org/abs/2107.06992",
          "publishedOn": "2021-07-16T00:48:23.325Z",
          "wordCount": 677,
          "title": "Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1\">Tomasz W&#x142;odarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1\">Adam Klasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "In this paper, we propose an end-to-end multi-task neural network called\nFetalNet with an attention mechanism and stacked module for spatio-temporal\nfetal ultrasound scan video analysis. Fetal biometric measurement is a standard\nexamination during pregnancy used for the fetus growth monitoring and\nestimation of gestational age and fetal weight. The main goal in fetal\nultrasound scan video analysis is to find proper standard planes to measure the\nfetal head, abdomen and femur. Due to natural high speckle noise and shadows in\nultrasound data, medical expertise and sonographic experience are required to\nfind the appropriate acquisition plane and perform accurate measurements of the\nfetus. In addition, existing computer-aided methods for fetal US biometric\nmeasurement address only one single image frame without considering temporal\nfeatures. To address these shortcomings, we propose an end-to-end multi-task\nneural network for spatio-temporal ultrasound scan video analysis to\nsimultaneously localize, classify and measure the fetal body parts. We propose\na new encoder-decoder segmentation architecture that incorporates a\nclassification branch. Additionally, we employ an attention mechanism with a\nstacked module to learn salient maps to suppress irrelevant US regions and\nefficient scan plane localization. We trained on the fetal ultrasound video\ncomes from routine examinations of 700 different patients. Our method called\nFetalNet outperforms existing state-of-the-art methods in both classification\nand segmentation in fetal ultrasound video recordings.",
          "link": "http://arxiv.org/abs/2107.06943",
          "publishedOn": "2021-07-16T00:48:23.318Z",
          "wordCount": 682,
          "title": "FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinglu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yinyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jian Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Jun Zhang</a>",
          "description": "Automatic surgical instruction generation is a prerequisite towards\nintra-operative context-aware surgical assistance. However, generating\ninstructions from surgical scenes is challenging, as it requires jointly\nunderstanding the surgical activity of current view and modelling relationships\nbetween visual information and textual description. Inspired by the neural\nmachine translation and imaging captioning tasks in open domain, we introduce a\ntransformer-backboned encoder-decoder network with self-critical reinforcement\nlearning to generate instructions from surgical images. We evaluate the\neffectiveness of our method on DAISI dataset, which includes 290 procedures\nfrom various medical disciplines. Our approach outperforms the existing\nbaseline over all caption evaluation metrics. The results demonstrate the\nbenefits of the encoder-decoder structure backboned by transformer in handling\nmultimodal context.",
          "link": "http://arxiv.org/abs/2107.06964",
          "publishedOn": "2021-07-16T00:48:23.301Z",
          "wordCount": 550,
          "title": "Surgical Instruction Generation with Transformers. (arXiv:2107.06964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zona Kostic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1\">Aleksandar Jevremovic</a>",
          "description": "The attractiveness of a property is one of the most interesting, yet\nchallenging, categories to model. Image characteristics are used to describe\ncertain attributes, and to examine the influence of visual factors on the price\nor timeframe of the listing. In this paper, we propose a set of techniques for\nthe extraction of visual features for efficient numerical inclusion in\nmodern-day predictive algorithms. We discuss techniques such as Shannon's\nentropy, calculating the center of gravity, employing image segmentation, and\nusing Convolutional Neural Networks. After comparing these techniques as\napplied to a set of property-related images (indoor, outdoor, and satellite),\nwe conclude the following: (i) the entropy is the most efficient single-digit\nvisual measure for housing price prediction; (ii) image segmentation is the\nmost important visual feature for the prediction of housing lifespan; and (iii)\ndeep image features can be used to quantify interior characteristics and\ncontribute to captivation modeling. The set of 40 image features selected here\ncarries a significant amount of predictive power and outperforms some of the\nstrongest metadata predictors. Without any need to replace a human expert in a\nreal-estate appraisal process, we conclude that the techniques presented in\nthis paper can efficiently describe visible characteristics, thus introducing\nperceived attractiveness as a quantitative measure into the predictive modeling\nof housing.",
          "link": "http://arxiv.org/abs/2107.07148",
          "publishedOn": "2021-07-16T00:48:23.284Z",
          "wordCount": 649,
          "title": "What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bohong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "The mainstream approach for filter pruning is usually either to force a\nhard-coded importance estimation upon a computation-heavy pretrained model to\nselect \"important\" filters, or to impose a hyperparameter-sensitive sparse\nconstraint on the loss objective to regularize the network training. In this\npaper, we present a novel filter pruning method, dubbed dynamic-coded filter\nfusion (DCFF), to derive compact CNNs in a computation-economical and\nregularization-free manner for efficient image classification. Each filter in\nour DCFF is firstly given an inter-similarity distribution with a temperature\nparameter as a filter proxy, on top of which, a fresh Kullback-Leibler\ndivergence based dynamic-coded criterion is proposed to evaluate the filter\nimportance. In contrast to simply keeping high-score filters in other methods,\nwe propose the concept of filter fusion, i.e., the weighted averages using the\nassigned proxies, as our preserved filters. We obtain a one-hot\ninter-similarity distribution as the temperature parameter approaches infinity.\nThus, the relative importance of each filter can vary along with the training\nof the compact CNN, leading to dynamically changeable fused filters without\nboth the dependency on the pretrained model and the introduction of sparse\nconstraints. Extensive experiments on classification benchmarks demonstrate the\nsuperiority of our DCFF over the compared counterparts. For example, our DCFF\nderives a compact VGGNet-16 with only 72.77M FLOPs and 1.06M parameters while\nreaching top-1 accuracy of 93.47% on CIFAR-10. A compact ResNet-50 is obtained\nwith 63.8% FLOPs and 58.6% parameter reductions, retaining 75.60% top-1\naccuracy on ILSVRC-2012. Our code, narrower models and training logs are\navailable at https://github.com/lmbxmu/DCFF.",
          "link": "http://arxiv.org/abs/2107.06916",
          "publishedOn": "2021-07-16T00:48:23.278Z",
          "wordCount": 705,
          "title": "Training Compact CNNs for Image Classification using Dynamic-coded Filter Fusion. (arXiv:2107.06916v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, in the last few years, a large research effort\nhas been devoted to image captioning, i.e. the task of describing images with\nsyntactically and semantically meaningful sentences. Starting from 2015 the\ntask has generally been addressed with pipelines composed of a visual encoding\nstep and a language model for text generation. During these years, both\ncomponents have evolved considerably through the exploitation of object\nregions, attributes, and relationships and the introduction of multi-modal\nconnections, fully-attentive approaches, and BERT-like early-fusion strategies.\nHowever, regardless of the impressive results obtained, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview and categorization of image captioning approaches,\nfrom visual encoding and text generation to training strategies, used datasets,\nand evaluation metrics. In this respect, we quantitatively compare many\nrelevant state-of-the-art approaches to identify the most impactful technical\ninnovations in image captioning architectures and training strategies.\nMoreover, many variants of the problem and its open challenges are analyzed and\ndiscussed. The final goal of this work is to serve as a tool for understanding\nthe existing state-of-the-art and highlighting the future directions for an\narea of research where Computer Vision and Natural Language Processing can find\nan optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-07-16T00:48:23.264Z",
          "wordCount": 664,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langlois_T/0/1/0/all/0/1\">Thomas A. Langlois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">H. Charles Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1\">Erin Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>",
          "description": "Developments in machine learning interpretability techniques over the past\ndecade have provided new tools to observe the image regions that are most\ninformative for classification and localization in artificial neural networks\n(ANNs). Are the same regions similarly informative to human observers? Using\ndata from 78 new experiments and 6,610 participants, we show that passive\nattention techniques reveal a significant overlap with human visual selectivity\nestimates derived from 6 distinct behavioral tasks including visual\ndiscrimination, spatial localization, recognizability, free-viewing,\ncued-object search, and saliency search fixations. We find that input\nvisualizations derived from relatively simple ANN architectures probed using\nguided backpropagation methods are the best predictors of a shared component in\nthe joint variability of the human measures. We validate these correlational\nresults with causal manipulations using recognition experiments. We show that\nimages masked with ANN attention maps were easier for humans to classify than\ncontrol masks in a speeded recognition experiment. Similarly, we find that\nrecognition performance in the same ANN models was likewise influenced by\nmasking input images using human visual selectivity maps. This work contributes\na new approach to evaluating the biological and psychological validity of\nleading ANNs as models of human vision: by examining their similarities and\ndifferences in terms of their visual selectivity to the information contained\nin images.",
          "link": "http://arxiv.org/abs/2107.07013",
          "publishedOn": "2021-07-16T00:48:23.237Z",
          "wordCount": 658,
          "title": "Passive attention in artificial neural networks predicts human visual selectivity. (arXiv:2107.07013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kakaletsis_E/0/1/0/all/0/1\">Efstratios Kakaletsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1\">Nikos Nikolaidis</a>",
          "description": "In this paper, a simple technique for Unmanned Aerial Vehicles (UAVs)\npotential landing site detection using terrain information through\nidentification of flat areas, is presented. The algorithm utilizes digital\nelevation models (DEM) that represent the height distribution of an area. Flat\nareas which constitute appropriate landing zones for UAVs in normal or\nemergency situations result by thresholding the image gradient magnitude of the\ndigital surface model (DSM). The proposed technique also uses connected\ncomponents evaluation on the thresholded gradient image in order to discover\nconnected regions of sufficient size for landing. Moreover, man-made structures\nand vegetation areas are detected and excluded from the potential landing\nsites. Quantitative performance evaluation of the proposed landing site\ndetection algorithm in a number of areas on real world and synthetic datasets,\naccompanied by a comparison with a state-of-the-art algorithm, proves its\nefficiency and superiority.",
          "link": "http://arxiv.org/abs/2107.06921",
          "publishedOn": "2021-07-16T00:48:23.157Z",
          "wordCount": 604,
          "title": "Potential UAV Landing Sites Detection through Digital Elevation Models Analysis. (arXiv:2107.06921v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1\">Janis Postels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1\">Mattia Segu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "A set of novel approaches for estimating epistemic uncertainty in deep neural\nnetworks with a single forward pass has recently emerged as a valid alternative\nto Bayesian Neural Networks. On the premise of informative representations,\nthese deterministic uncertainty methods (DUMs) achieve strong performance on\ndetecting out-of-distribution (OOD) data while adding negligible computational\ncosts at inference time. However, it remains unclear whether DUMs are well\ncalibrated and can seamlessly scale to real-world applications - both\nprerequisites for their practical deployment. To this end, we first provide a\ntaxonomy of DUMs, evaluate their calibration under continuous distributional\nshifts and their performance on OOD detection for image classification tasks.\nThen, we extend the most promising approaches to semantic segmentation. We find\nthat, while DUMs scale to realistic vision tasks and perform well on OOD\ndetection, the practicality of current methods is undermined by poor\ncalibration under realistic distributional shifts.",
          "link": "http://arxiv.org/abs/2107.00649",
          "publishedOn": "2021-07-15T01:59:04.389Z",
          "wordCount": 602,
          "title": "On the Practicality of Deterministic Epistemic Uncertainty. (arXiv:2107.00649v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06808",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_Q/0/1/0/all/0/1\">Qi Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yong Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "As a common weather, rain streaks adversely degrade the image quality. Hence,\nremoving rains from an image has become an important issue in the field. To\nhandle such an ill-posed single image deraining task, in this paper, we\nspecifically build a novel deep architecture, called rain convolutional\ndictionary network (RCDNet), which embeds the intrinsic priors of rain streaks\nand has clear interpretability. In specific, we first establish a RCD model for\nrepresenting rain streaks and utilize the proximal gradient descent technique\nto design an iterative algorithm only containing simple operators for solving\nthe model. By unfolding it, we then build the RCDNet in which every network\nmodule has clear physical meanings and corresponds to each operation involved\nin the algorithm. This good interpretability greatly facilitates an easy\nvisualization and analysis on what happens inside the network and why it works\nwell in inference process. Moreover, taking into account the domain gap issue\nin real scenarios, we further design a novel dynamic RCDNet, where the rain\nkernels can be dynamically inferred corresponding to input rainy images and\nthen help shrink the space for rain layer estimation with few rain maps so as\nto ensure a fine generalization performance in the inconsistent scenarios of\nrain types between training and testing data. By end-to-end training such an\ninterpretable network, all involved rain kernels and proximal operators can be\nautomatically extracted, faithfully characterizing the features of both rain\nand clean background layers, and thus naturally lead to better deraining\nperformance. Comprehensive experiments substantiate the superiority of our\nmethod, especially on its well generality to diverse testing scenarios and good\ninterpretability for all its modules. Code is available in\n\\emph{\\url{https://github.com/hongwang01/DRCDNet}}.",
          "link": "http://arxiv.org/abs/2107.06808",
          "publishedOn": "2021-07-15T01:59:04.260Z",
          "wordCount": 730,
          "title": "RCDNet: An Interpretable Rain Convolutional Dictionary Network for Single Image Deraining. (arXiv:2107.06808v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>",
          "description": "As an important step towards visual reasoning, visual grounding (e.g., phrase\nlocalization, referring expression comprehension/segmentation) has been widely\nexplored Previous approaches to referring expression comprehension (REC) or\nsegmentation (RES) either suffer from limited performance, due to a two-stage\nsetup, or require the designing of complex task-specific one-stage\narchitectures. In this paper, we propose a simple one-stage multi-task\nframework for visual grounding tasks. Specifically, we leverage a transformer\narchitecture, where two modalities are fused in a visual-lingual encoder. In\nthe decoder, the model learns to generate contextualized lingual queries which\nare then decoded and used to directly regress the bounding box and produce a\nsegmentation mask for the corresponding referred regions. With this simple but\nhighly contextualized model, we outperform state-of-the-arts methods by a large\nmargin on both REC and RES tasks. We also show that a simple pre-training\nschedule (on an external dataset) further improves the performance. Extensive\nexperiments and ablations illustrate that our model benefits greatly from\ncontextualized information and multi-task training.",
          "link": "http://arxiv.org/abs/2106.03089",
          "publishedOn": "2021-07-15T01:59:03.915Z",
          "wordCount": 624,
          "title": "Referring Transformer: A One-step Approach to Multi-task Visual Grounding. (arXiv:2106.03089v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1\">Silvia Bucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1\">Francesco Cappio Borlino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1\">Tatiana Tommasi</a>",
          "description": "Vision systems trained in closed-world scenarios will inevitably fail when\npresented with new environmental conditions, new data distributions and novel\nclasses at deployment time. How to move towards open-world learning is a long\nstanding research question, but the existing solutions mainly focus on specific\naspects of the problem (single domain Open-Set, multi-domain Closed-Set), or\npropose complex strategies which combine multiple losses and manually tuned\nhyperparameters. In this work we tackle multi-source Open-Set domain adaptation\nby introducing HyMOS: a straightforward supervised model that exploits the\npower of contrastive learning and the properties of its hyperspherical feature\nspace to correctly predict known labels on the target, while rejecting samples\nbelonging to any unknown class. HyMOS includes a tailored data balancing to\nenforce cross-source alignment and introduces style transfer among the instance\ntransformations of contrastive learning for source-target adaptation, avoiding\nthe risk of negative transfer. Finally a self-training strategy refines the\nmodel without the need for handcrafted thresholds. We validate our method over\nthree challenging datasets and provide an extensive quantitative and\nqualitative experimental analysis. The obtained results show that HyMOS\noutperforms several Open-Set and universal domain adaptation approaches,\ndefining the new state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.02067",
          "publishedOn": "2021-07-15T01:59:03.857Z",
          "wordCount": 644,
          "title": "Distance-based Hyperspherical Classification for Multi-source Open-Set Domain Adaptation. (arXiv:2107.02067v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_L/0/1/0/all/0/1\">Lingjie Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Cartoon face recognition is challenging as they typically have smooth color\nregions and emphasized edges, the key to recognize cartoon faces is to\nprecisely perceive their sparse and critical shape patterns. However, it is\nquite difficult to learn a shape-oriented representation for cartoon face\nrecognition with convolutional neural networks (CNNs). To mitigate this issue,\nwe propose the GraphJigsaw that constructs jigsaw puzzles at various stages in\nthe classification network and solves the puzzles with the graph convolutional\nnetwork (GCN) in a progressive manner. Solving the puzzles requires the model\nto spot the shape patterns of the cartoon faces as the texture information is\nquite limited. The key idea of GraphJigsaw is constructing a jigsaw puzzle by\nrandomly shuffling the intermediate convolutional feature maps in the spatial\ndimension and exploiting the GCN to reason and recover the correct layout of\nthe jigsaw fragments in a self-supervised manner. The proposed GraphJigsaw\navoids training the classification model with the deconstructed images that\nwould introduce noisy patterns and are harmful for the final classification.\nSpecially, GraphJigsaw can be incorporated at various stages in a top-down\nmanner within the classification model, which facilitates propagating the\nlearned shape patterns gradually. GraphJigsaw does not rely on any extra manual\nannotation during the training process and incorporates no extra computation\nburden at inference time. Both quantitative and qualitative experimental\nresults have verified the feasibility of our proposed GraphJigsaw, which\nconsistently outperforms other face recognition or jigsaw-based methods on two\npopular cartoon face datasets with considerable improvements.",
          "link": "http://arxiv.org/abs/2107.06532",
          "publishedOn": "2021-07-15T01:59:03.810Z",
          "wordCount": 683,
          "title": "Graph Jigsaw Learning for Cartoon Face Recognition. (arXiv:2107.06532v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Sho Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonsalves_T/0/1/0/all/0/1\">Tad Gonsalves</a>",
          "description": "Unpaired image-to-image translation using Generative Adversarial Networks\n(GAN) is successful in converting images among multiple domains. Moreover,\nrecent studies have shown a way to diversify the outputs of the generator.\nHowever, since there are no restrictions on how the generator diversifies the\nresults, it is likely to translate some unexpected features. In this paper, we\npropose Style-Restricted GAN (SRGAN) to demonstrate the importance of\ncontrolling the encoded features used in style diversifying process. More\nspecifically, instead of KL divergence loss, we adopt three new losses to\nrestrict the distribution of the encoded features: batch KL divergence loss,\ncorrelation loss, and histogram imitation loss. Further, the encoder is\npre-trained with classification tasks before being used in translation process.\nThe study reports quantitative as well as qualitative results with Precision,\nRecall, Density, and Coverage. The proposed three losses lead to the\nenhancement of the level of diversity compared to the conventional KL loss. In\nparticular, SRGAN is found to be successful in translating with higher\ndiversity and without changing the class-unrelated features in the CelebA face\ndataset. To conclude, the importance of the encoded features being\nwell-regulated was proven with two experiments. Our implementation is available\nat https://github.com/shinshoji01/Style-Restricted_GAN.",
          "link": "http://arxiv.org/abs/2105.07621",
          "publishedOn": "2021-07-15T01:59:03.804Z",
          "wordCount": 674,
          "title": "Style-Restricted GAN: Multi-Modal Translation with Style Restriction Using Generative Adversarial Networks. (arXiv:2105.07621v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Vision transformer has achieved competitive performance on a variety of\ncomputer vision applications. However, their storage, run-time memory, and\ncomputational demands are hindering the deployment to mobile devices. Here we\npresent a vision transformer pruning approach, which identifies the impacts of\ndimensions in each layer of transformer and then executes pruning accordingly.\nBy encouraging dimension-wise sparsity in the transformer, important dimensions\nautomatically emerge. A great number of dimensions with small importance scores\ncan be discarded to achieve a high pruning ratio without significantly\ncompromising accuracy. The pipeline for vision transformer pruning is as\nfollows: 1) training with sparsity regularization; 2) pruning dimensions of\nlinear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of\nthe proposed algorithm are well evaluated and analyzed on ImageNet dataset to\ndemonstrate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2104.08500",
          "publishedOn": "2021-07-15T01:59:03.447Z",
          "wordCount": 607,
          "title": "Visual Transformer Pruning. (arXiv:2104.08500v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiao Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tingzhang Zhao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojun Ye</a> (2) ((1) Department of Computer Science, University of Science and Technology Beijing (2) School of Software, Tsinghua University)",
          "description": "Feature selection, an effective technique for dimensionality reduction, plays\nan important role in many machine learning systems. Supervised knowledge can\nsignificantly improve the performance. However, faced with the rapid growth of\nnewly emerging concepts, existing supervised methods might easily suffer from\nthe scarcity and validity of labeled data for training. In this paper, the\nauthors study the problem of zero-shot feature selection (i.e., building a\nfeature selection model that generalizes well to \"unseen\" concepts with limited\ntraining data of \"seen\" concepts). Specifically, they adopt class-semantic\ndescriptions (i.e., attributes) as supervision for feature selection, so as to\nutilize the supervised knowledge transferred from the seen concepts. For more\nreliable discriminative features, they further propose the\ncenter-characteristic loss which encourages the selected features to capture\nthe central characteristics of seen concepts. Extensive experiments conducted\non various real-world datasets demonstrate the effectiveness of the method.",
          "link": "http://arxiv.org/abs/1908.03464",
          "publishedOn": "2021-07-15T01:59:03.372Z",
          "wordCount": 643,
          "title": "Zero-Shot Feature Selection via Transferring Supervised Knowledge. (arXiv:1908.03464v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>",
          "description": "While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4%/16.3% of\nResNet-101/ResNet-50.",
          "link": "http://arxiv.org/abs/2104.09124",
          "publishedOn": "2021-07-15T01:59:03.232Z",
          "wordCount": 683,
          "title": "DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1\">Andrei Atanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Averaging predictions over a set of models -- an ensemble -- is widely used\nto improve predictive performance and uncertainty estimation of deep learning\nmodels. At the same time, many machine learning systems, such as search,\nmatching, and recommendation systems, heavily rely on embeddings.\nUnfortunately, due to misalignment of features of independently trained models,\nembeddings, cannot be improved with a naive deep ensemble like approach. In\nthis work, we look at the ensembling of representations and propose mean\nembeddings with test-time augmentation (MeTTA) simple yet well-performing\nrecipe for ensembling representations. Empirically we demonstrate that MeTTA\nsignificantly boosts the quality of linear evaluation on ImageNet for both\nsupervised and self-supervised models. Even more exciting, we draw connections\nbetween MeTTA, image retrieval, and transformation invariant models. We believe\nthat spreading the success of ensembles to inference higher-quality\nrepresentations is the important step that will open many new applications of\nensembling.",
          "link": "http://arxiv.org/abs/2106.08038",
          "publishedOn": "2021-07-15T01:59:03.226Z",
          "wordCount": 615,
          "title": "Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Transformers have shown great potential in various computer vision tasks\nowing to their strong capability in modeling long-range dependency using the\nself-attention mechanism. Nevertheless, vision transformers treat an image as\n1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in\nmodeling local visual structures and dealing with scale variance.\nAlternatively, they require large-scale training data and longer training\nschedules to learn the IB implicitly. In this paper, we propose a novel Vision\nTransformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE.\nTechnically, ViTAE has several spatial pyramid reduction modules to downsample\nand embed the input image into tokens with rich multi-scale context by using\nmultiple convolutions with different dilation rates. In this way, it acquires\nan intrinsic scale invariance IB and is able to learn robust feature\nrepresentation for objects at various scales. Moreover, in each transformer\nlayer, ViTAE has a convolution block in parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. Experiments on ImageNet\nas well as downstream tasks prove the superiority of ViTAE over the baseline\ntransformer and concurrent works. Source code and pretrained models will be\navailable at GitHub.",
          "link": "http://arxiv.org/abs/2106.03348",
          "publishedOn": "2021-07-15T01:59:03.220Z",
          "wordCount": 694,
          "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias. (arXiv:2106.03348v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Dan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1\">Bastian Leibe</a>",
          "description": "In this preliminary work we attempt to apply submanifold sparse convolution\nto the task of 3D person detection. In particular, we present Person-MinkUNet,\na single-stage 3D person detection network based on Minkowski Engine with U-Net\narchitecture. The network achieves a 76.4% average precision (AP) on the JRDB\n3D detection benchmark.",
          "link": "http://arxiv.org/abs/2107.06780",
          "publishedOn": "2021-07-15T01:59:03.202Z",
          "wordCount": 497,
          "title": "Person-MinkUNet: 3D Person Detection with LiDAR Point Cloud. (arXiv:2107.06780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shubin_D/0/1/0/all/0/1\">Dmitrii Shubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eytan_D/0/1/0/all/0/1\">Danny Eytan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodfellow_S/0/1/0/all/0/1\">Sebastian D. Goodfellow</a>",
          "description": "Self-supervised learning methods for computer vision have demonstrated the\neffectiveness of pre-training feature representations, resulting in\nwell-generalizing Deep Neural Networks, even if the annotated data are limited.\nHowever, representation learning techniques require a significant amount of\ntime for model training, with most of it time spent on precise hyper-parameter\noptimization and selection of augmentation techniques. We hypothesized that if\nthe annotated dataset has enough morphological diversity to capture the general\npopulation's as is common in medical imaging, for example, due to conserved\nsimilarities of tissue mythologies, the variance error of the trained model is\nthe prevalent component of the Bias-Variance Trade-off. We propose the Variance\nAware Training (VAT) method that exploits this property by introducing the\nvariance error into the model loss function, i.e., enabling minimizing the\nvariance explicitly. Additionally, we provide the theoretical formulation and\nproof of the proposed method to aid in interpreting the approach. Our method\nrequires selecting only one hyper-parameter and was able to match or improve\nthe state-of-the-art performance of self-supervised methods while achieving an\norder of magnitude reduction in the GPU training time. We validated VAT on\nthree medical imaging datasets from diverse domains and various learning\nobjectives. These included a Magnetic Resonance Imaging (MRI) dataset for the\nheart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography\ndataset for ordinary regression of diabetic retinopathy progression (Kaggle\n2019 APTOS Blindness Detection challenge), and classification of\nhistopathologic scans of lymph node sections (PatchCamelyon dataset).",
          "link": "http://arxiv.org/abs/2105.14117",
          "publishedOn": "2021-07-15T01:59:03.195Z",
          "wordCount": 724,
          "title": "About Explicit Variance Minimization: Training Neural Networks for Medical Imaging With Limited Data Annotations. (arXiv:2105.14117v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaghouri_A/0/1/0/all/0/1\">Anas Al Shaghouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhatib_R/0/1/0/all/0/1\">Rami Alkhatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berjaoui_S/0/1/0/all/0/1\">Samir Berjaoui</a>",
          "description": "Roads are connecting line between different places, and used daily. Roads'\nperiodic maintenance keeps them safe and functional. Detecting and reporting\nthe existence of potholes to responsible departments can help in eliminating\nthem. This study deployed and tested on different deep learning architecture to\ndetect potholes. The images used for training were collected by cellphone\nmounted on the windshield of the car, in addition to many images downloaded\nfrom the internet to increase the size and variability of the database. Second,\nvarious object detection algorithms are employed and compared to detect\npotholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53.\nYOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39%\nmean Average Precision (mAP). The speed of processing was 20 frame per second.\nThe system was able to detect potholes from a range on 100 meters away from the\ncamera. The system can increase the safety of drivers and improve the\nperformance of self-driving cars by detecting pothole time ahead.",
          "link": "http://arxiv.org/abs/2107.06356",
          "publishedOn": "2021-07-15T01:59:03.187Z",
          "wordCount": 606,
          "title": "Real-Time Pothole Detection Using Deep Learning. (arXiv:2107.06356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pei_M/0/1/0/all/0/1\">Meiqi Pei</a>",
          "description": "In recent years, monocular depth estimation is applied to understand the\nsurrounding 3D environment and has made great progress. However, there is an\nill-posed problem on how to gain depth information directly from a single\nimage. With the rapid development of deep learning, this problem is possible to\nbe solved. Although more and more approaches are proposed one after another,\nmost of existing methods inevitably lost details due to continuous downsampling\nwhen mapping from RGB space to depth space. To the end, we design a Multi-scale\nFeatures Network (MSFNet), which consists of Enhanced Diverse Attention (EDA)\nmodule and Upsample-Stage Fusion (USF) module. The EDA module employs the\nspatial attention method to learn significant spatial information, while USF\nmodule complements low-level detail information with high-level semantic\ninformation from the perspective of multi-scale feature fusion to improve the\npredicted effect. In addition, since the simple samples are always trained to a\nbetter effect first, the hard samples are difficult to converge. Therefore, we\ndesign a batch-loss to assign large loss factors to the harder samples in a\nbatch. Experiments on NYU-Depth V2 dataset and KITTI dataset demonstrate that\nour proposed approach is more competitive with the state-of-the-art methods in\nboth qualitative and quantitative evaluation.",
          "link": "http://arxiv.org/abs/2107.06445",
          "publishedOn": "2021-07-15T01:59:03.179Z",
          "wordCount": 634,
          "title": "MSFNet:Multi-scale features network for monocular depth estimation. (arXiv:2107.06445v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Trinh Man Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjia Zhou</a>",
          "description": "COVID-19 leads to the high demand for remote interactive systems ever seen.\nOne of the key elements of these systems is video streaming, which requires a\nvery high network bandwidth due to its specific real-time demand, especially\nwith high-resolution video. Existing video compression methods are struggling\nin the trade-off between video quality and the speed requirement. Addressed\nthat the background information rarely changes in most remote meeting cases, we\nintroduce a Region-Of-Interests (ROI) based video compression framework (named\nRCLC) that leverages the cutting-edge learning-based and conventional\ntechnologies. In RCLC, each coming frame is marked as a background-updating\n(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the\nBU frame is compressed with low-quality and high-compression, while the ROI\nfrom RU-frame is compressed with high-quality and low-compression. The\nlearning-based methods are applied to detect the ROI, blend background-ROI, and\nenhance video quality. The experimental results show that our RCLC can reduce\nup to 32.55\\% BD-rate for the ROI region compared to H.265 video codec under a\nsimilar compression time with 1080p resolution.",
          "link": "http://arxiv.org/abs/2107.06492",
          "publishedOn": "2021-07-15T01:59:03.173Z",
          "wordCount": 663,
          "title": "RCLC: ROI-based joint conventional and learning video compression. (arXiv:2107.06492v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyulim Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">JeongSoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Seungri Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun-Ho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_C/0/1/0/all/0/1\">Chulmin Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>",
          "description": "A significant amount of work has been done on adversarial attacks that inject\nimperceptible noise to images to deteriorate the image classification\nperformance of deep models. However, most of the existing studies consider\nattacks in the digital (pixel) domain where an image acquired by an image\nsensor with sampling and quantization has been recorded. This paper, for the\nfirst time, introduces an optical adversarial attack, which physically alters\nthe light field information arriving at the image sensor so that the\nclassification model yields misclassification. More specifically, we modulate\nthe phase of the light in the Fourier domain using a spatial light modulator\nplaced in the photographic system. The operative parameters of the modulator\nare obtained by gradient-based optimization to maximize cross-entropy and\nminimize distortions. We present experiments based on both simulation and a\nreal hardware optical system, from which the feasibility of the proposed\noptical attack is demonstrated. It is also verified that the proposed attack is\ncompletely different from common optical-domain distortions such as spherical\naberration, defocus, and astigmatism in terms of both perturbation patterns and\nclassification results.",
          "link": "http://arxiv.org/abs/2106.09908",
          "publishedOn": "2021-07-15T01:59:03.157Z",
          "wordCount": 654,
          "title": "Light Lies: Optical Adversarial Attack. (arXiv:2106.09908v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>",
          "description": "Camera calibration is an important prerequisite towards the solution of 3D\ncomputer vision problems. Traditional methods rely on static images of a\ncalibration pattern. This raises interesting challenges towards the practical\nusage of event cameras, which notably require image change to produce\nsufficient measurements. The current standard for event camera calibration\ntherefore consists of using flashing patterns. They have the advantage of\nsimultaneously triggering events in all reprojected pattern feature locations,\nbut it is difficult to construct or use such patterns in the field. We present\nthe first dynamic event camera calibration algorithm. It calibrates directly\nfrom events captured during relative motion between camera and calibration\npattern. The method is propelled by a novel feature extraction mechanism for\ncalibration patterns, and leverages existing calibration tools before\noptimizing all parameters through a multi-segment continuous-time formulation.\nAs demonstrated through our results on real data, the obtained calibration\nmethod is highly convenient and reliably calibrates from data sequences\nspanning less than 10 seconds.",
          "link": "http://arxiv.org/abs/2107.06749",
          "publishedOn": "2021-07-15T01:59:03.151Z",
          "wordCount": 603,
          "title": "Dynamic Event Camera Calibration. (arXiv:2107.06749v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yinan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1\">Bei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1\">Guojun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1\">Lu Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "The rapid progress of photorealistic synthesis techniques has reached at a\ncritical point where the boundary between real and manipulated images starts to\nblur. Thus, benchmarking and advancing digital forgery analysis have become a\npressing issue. However, existing face forgery datasets either have limited\ndiversity or only support coarse-grained analysis. To counter this emerging\nthreat, we construct the ForgeryNet dataset, an extremely large face forgery\ndataset with unified annotations in image- and video-level data across four\ntasks: 1) Image Forgery Classification, including two-way (real / fake),\nthree-way (real / fake with identity-replaced forgery approaches / fake with\nidentity-remained forgery approaches), and n-way (real and 15 respective\nforgery approaches) classification. 2) Spatial Forgery Localization, which\nsegments the manipulated area of fake images compared to their corresponding\nsource real images. 3) Video Forgery Classification, which re-defines the\nvideo-level forgery classification with manipulated frames in random positions.\nThis task is important because attackers in real world are free to manipulate\nany target frame. and 4) Temporal Forgery Localization, to localize the\ntemporal segments which are manipulated. ForgeryNet is by far the largest\npublicly available deep face forgery dataset in terms of data-scale (2.9\nmillion images, 221,247 videos), manipulations (7 image-level approaches, 8\nvideo-level approaches), perturbations (36 independent and more mixed\nperturbations) and annotations (6.3 million classification labels, 2.9 million\nmanipulated area annotations and 221,247 temporal forgery segment labels). We\nperform extensive benchmarking and studies of existing face forensics methods\nand obtain several valuable observations.",
          "link": "http://arxiv.org/abs/2103.05630",
          "publishedOn": "2021-07-15T01:59:03.144Z",
          "wordCount": 735,
          "title": "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. (arXiv:2103.05630v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_C/0/1/0/all/0/1\">Christian Bartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratz_H/0/1/0/all/0/1\">Hendrik R&#xe4;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haojin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_J/0/1/0/all/0/1\">Joseph Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1\">Christoph Meinel</a>",
          "description": "One of the most pressing problems in the automated analysis of historical\ndocuments is the availability of annotated training data. In this paper, we\npropose a novel method for the synthesis of training data for semantic\nsegmentation of document images. We utilize clusters found in intermediate\nfeatures of a StyleGAN generator for the synthesis of RGB and label images at\nthe same time. Our model can be applied to any dataset of scanned documents\nwithout the need for manual annotation of individual images, as each model is\ncustom-fit to the dataset. In our experiments, we show that models trained on\nour synthetic data can reach competitive performance on open benchmark datasets\nfor line segmentation.",
          "link": "http://arxiv.org/abs/2107.06777",
          "publishedOn": "2021-07-15T01:59:03.136Z",
          "wordCount": 569,
          "title": "Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data. (arXiv:2107.06777v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roxo_T/0/1/0/all/0/1\">Tiago Roxo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proenca_H/0/1/0/all/0/1\">Hugo Proen&#xe7;a</a>",
          "description": "Soft biometrics inference in surveillance scenarios is a topic of interest\nfor various applications, particularly in security-related areas. However, soft\nbiometric analysis is not extensively reported in wild conditions. In\nparticular, previous works on gender recognition report their results in face\ndatasets, with relatively good image quality and frontal poses. Given the\nuncertainty of the availability of the facial region in wild conditions, we\nconsider that these methods are not adequate for surveillance settings. To\novercome these limitations, we: 1) present frontal and wild face versions of\nthree well-known surveillance datasets; and 2) propose a model that effectively\nand dynamically combines facial and body information, which makes it suitable\nfor gender recognition in wild conditions. The frontal and wild face datasets\nderive from widely used Pedestrian Attribute Recognition (PAR) sets (PETA,\nPA-100K, and RAP), using a pose-based approach to filter the frontal samples\nand facial regions. This approach retrieves the facial region of images with\nvarying image/subject conditions, where the state-of-the-art face detectors\noften fail. Our model combines facial and body information through a learnable\nfusion matrix and a channel-attention sub-network, focusing on the most\ninfluential body parts according to the specific image/subject features. We\ncompare it with five PAR methods, consistently obtaining state-of-the-art\nresults on gender recognition, and reducing the prediction errors by up to 24%\nin frontal samples. The announced PAR datasets versions and model serve as the\nbasis for wild soft biometrics classification and are available in\nhttps://github.com/Tiago-Roxo.",
          "link": "http://arxiv.org/abs/2107.06847",
          "publishedOn": "2021-07-15T01:59:03.128Z",
          "wordCount": 678,
          "title": "Faces in the Wild: Efficient Gender Recognition in Surveillance Conditions. (arXiv:2107.06847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yang-tian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao-zhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>",
          "description": "Pose transfer of human videos aims to generate a high fidelity video of a\ntarget person imitating actions of a source person. A few studies have made\ngreat progress either through image translation with deep latent features or\nneural rendering with explicit 3D features. However, both of them rely on large\namounts of training data to generate realistic results, and the performance\ndegrades on more accessible internet videos due to insufficient training\nframes. In this paper, we demonstrate that the dynamic details can be preserved\neven trained from short monocular videos. Overall, we propose a neural video\nrendering framework coupled with an image-translation-based dynamic details\ngeneration network (D2G-Net), which fully utilizes both the stability of\nexplicit 3D features and the capacity of learning components. To be specific, a\nnovel texture representation is presented to encode both the static and\npose-varying appearance characteristics, which is then mapped to the image\nspace and rendered as a detail-rich frame in the neural rendering stage.\nMoreover, we introduce a concise temporal loss in the training stage to\nsuppress the detail flickering that is made more visible due to high-quality\ndynamic details generated by our method. Through extensive comparisons, we\ndemonstrate that our neural human video renderer is capable of achieving both\nclearer dynamic details and more robust performance even on accessible short\nvideos with only 2k - 4k frames.",
          "link": "http://arxiv.org/abs/2106.14132",
          "publishedOn": "2021-07-15T01:59:03.111Z",
          "wordCount": 707,
          "title": "Robust Pose Transfer with Dynamic Details using Neural Video Rendering. (arXiv:2106.14132v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaofu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaojie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuebin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>",
          "description": "Long-range context information is crucial for the semantic segmentation of\nHigh-Resolution (HR) Remote Sensing Images (RSIs). The image cropping\noperations, commonly used for training neural networks, limit the perception of\nlong-range context information in large RSIs. To break this limitation, we\npropose a Wide-Context Network (WiCoNet) for the semantic segmentation of HR\nRSIs. In the WiCoNet, apart from a conventional feature extraction network that\naggregates the local information, an extra context branch is designed to\nexplicitly model the spatial information in a larger image area. The\ninformation between the two branches is communicated through a Context\nTransformer, which is a novel design derived from the Vision Transformer to\nmodel the long-range context correlations. Ablation studies and comparative\nexperiments conducted on several benchmark datasets prove the effectiveness of\nthe proposed method. In addition, we present a new Beijing Land-Use (BLU)\ndataset. This is a large-scale HR satellite dataset provided with high-quality\nand fine-grained reference labels, which can boost future studies in this\nfield.",
          "link": "http://arxiv.org/abs/2106.15754",
          "publishedOn": "2021-07-15T01:59:03.100Z",
          "wordCount": 646,
          "title": "Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reinhold_J/0/1/0/all/0/1\">Jacob C. Reinhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carass_A/0/1/0/all/0/1\">Aaron Carass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>",
          "description": "Precision medicine involves answering counterfactual questions such as \"Would\nthis patient respond better to treatment A or treatment B?\" These types of\nquestions are causal in nature and require the tools of causal inference to be\nanswered, e.g., with a structural causal model (SCM). In this work, we develop\nan SCM that models the interaction between demographic information, disease\ncovariates, and magnetic resonance (MR) images of the brain for people with\nmultiple sclerosis. Inference in the SCM generates counterfactual images that\nshow what an MR image of the brain would look like if demographic or disease\ncovariates are changed. These images can be used for modeling disease\nprogression or used for image processing tasks where controlling for\nconfounders is necessary.",
          "link": "http://arxiv.org/abs/2103.03158",
          "publishedOn": "2021-07-15T01:59:03.094Z",
          "wordCount": 616,
          "title": "A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lukyanenko_S/0/1/0/all/0/1\">Stanislav Lukyanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Won-Dong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Donglai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struyven_R/0/1/0/all/0/1\">Robbert Struyven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leahy_B/0/1/0/all/0/1\">Brian Leahy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Helen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Yosef_D/0/1/0/all/0/1\">Dalit Ben-Yosef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Needleman_D/0/1/0/all/0/1\">Daniel Needleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "The developmental process of embryos follows a monotonic order. An embryo can\nprogressively cleave from one cell to multiple cells and finally transform to\nmorula and blastocyst. For time-lapse videos of embryos, most existing\ndevelopmental stage classification methods conduct per-frame predictions using\nan image frame at each time step. However, classification using only images\nsuffers from overlapping between cells and imbalance between stages. Temporal\ninformation can be valuable in addressing this problem by capturing movements\nbetween neighboring frames. In this work, we propose a two-stream model for\ndevelopmental stage classification. Unlike previous methods, our two-stream\nmodel accepts both temporal and image information. We develop a linear-chain\nconditional random field (CRF) on top of neural network features extracted from\nthe temporal and image streams to make use of both modalities. The linear-chain\nCRF formulation enables tractable training of global sequential models over\nmultiple frames while also making it possible to inject monotonic development\norder constraints into the learning process explicitly. We demonstrate our\nalgorithm on two time-lapse embryo video datasets: i) mouse and ii) human\nembryo datasets. Our method achieves 98.1 % and 80.6 % for mouse and human\nembryo stage classification, respectively. Our approach will enable more\nprofound clinical and biological studies and suggests a new direction for\ndevelopmental stage classification by utilizing temporal information.",
          "link": "http://arxiv.org/abs/2107.06360",
          "publishedOn": "2021-07-15T01:59:03.089Z",
          "wordCount": 683,
          "title": "Developmental Stage Classification of EmbryosUsing Two-Stream Neural Network with Linear-Chain Conditional Random Field. (arXiv:2107.06360v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Ning Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jiajun Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sheng Zhou</a>",
          "description": "Present domain adaptation methods usually perform explicit representation\nalignment by simultaneously accessing the source data and target data. However,\nthe source data are not always available due to the privacy preserving\nconsideration or bandwidth limitation. Source-free domain adaptation aims to\nsolve the above problem by performing domain adaptation without accessing the\nsource data. The adaptation paradigm is receiving more and more attention in\nrecent years, and multiple works have been proposed for unsupervised\nsource-free domain adaptation. However, without utilizing any supervised signal\nand source data at the adaptation stage, the optimization of the target model\nis unstable and fragile. To alleviate the problem, we focus on semi-supervised\ndomain adaptation under source-free setting. More specifically, we propose\nuncertainty-guided Mixup to reduce the representation's intra-domain\ndiscrepancy and perform inter-domain alignment without directly accessing the\nsource data. Finally, we conduct extensive semi-supervised domain adaptation\nexperiments on various datasets. Our method outperforms the recent\nsemi-supervised baselines and the unsupervised variant also achieves\ncompetitive performance. The experiment codes will be released in the future.",
          "link": "http://arxiv.org/abs/2107.06707",
          "publishedOn": "2021-07-15T01:59:03.083Z",
          "wordCount": 606,
          "title": "Uncertainty-Guided Mixup for Semi-Supervised Domain Adaptation without Source Data. (arXiv:2107.06707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinda Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>",
          "description": "Fine-grained image recognition is challenging because discriminative clues\nare usually fragmented, whether from a single image or multiple images. Despite\ntheir significant improvements, most existing methods still focus on the most\ndiscriminative parts from a single image, ignoring informative details in other\nregions and lacking consideration of clues from other associated images. In\nthis paper, we analyze the difficulties of fine-grained image recognition from\na new perspective and propose a transformer architecture with the peak\nsuppression module and knowledge guidance module, which respects the\ndiversification of discriminative features in a single image and the\naggregation of discriminative clues among multiple images. Specifically, the\npeak suppression module first utilizes a linear projection to convert the input\nimage into sequential tokens. It then blocks the token based on the attention\nresponse generated by the transformer encoder. This module penalizes the\nattention to the most discriminative parts in the feature learning process,\ntherefore, enhancing the information exploitation of the neglected regions. The\nknowledge guidance module compares the image-based representation generated\nfrom the peak suppression module with the learnable knowledge embedding set to\nobtain the knowledge response coefficients. Afterwards, it formalizes the\nknowledge learning as a classification problem using response coefficients as\nthe classification scores. Knowledge embeddings and image-based representations\nare updated during training so that the knowledge embedding includes\ndiscriminative clues for different images. Finally, we incorporate the acquired\nknowledge embeddings into the image-based representations as comprehensive\nrepresentations, leading to significantly higher performance. Extensive\nevaluations on the six popular datasets demonstrate the advantage of the\nproposed method.",
          "link": "http://arxiv.org/abs/2107.06538",
          "publishedOn": "2021-07-15T01:59:03.066Z",
          "wordCount": 693,
          "title": "Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition. (arXiv:2107.06538v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doughty_M/0/1/0/all/0/1\">Mitchell Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Karan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghugre_N/0/1/0/all/0/1\">Nilesh R. Ghugre</a>",
          "description": "We present SurgeonAssist-Net: a lightweight framework making\naction-and-workflow-driven virtual assistance, for a set of predefined surgical\ntasks, accessible to commercially available optical see-through head-mounted\ndisplays (OST-HMDs). On a widely used benchmark dataset for laparoscopic\nsurgical workflow, our implementation competes with state-of-the-art approaches\nin prediction accuracy for automated task recognition, and yet requires 7.4x\nfewer parameters, 10.2x fewer floating point operations per second (FLOPS), is\n7.0x faster for inference on a CPU, and is capable of near real-time\nperformance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use\nof an efficient convolutional neural network (CNN) backbone to extract\ndiscriminative features from image data, and a low-parameter recurrent neural\nnetwork (RNN) architecture to learn long-term temporal dependencies. To\ndemonstrate the feasibility of our approach for inference on the HoloLens 2 we\ncreated a sample dataset that included video of several surgical tasks recorded\nfrom a user-centric point-of-view. After training, we deployed our model and\ncataloged its performance in an online simulated surgical scenario for the\nprediction of the current surgical task. The utility of our approach is\nexplored in the discussion of several relevant clinical use-cases. Our code is\npublicly available at https://github.com/doughtmw/surgeon-assist-net.",
          "link": "http://arxiv.org/abs/2107.06397",
          "publishedOn": "2021-07-15T01:59:03.060Z",
          "wordCount": 644,
          "title": "SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based Augmented Reality for Surgical Guidance. (arXiv:2107.06397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reichardt_L/0/1/0/all/0/1\">Laurenz Reichardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangat_P/0/1/0/all/0/1\">Patrick Mangat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1\">Oliver Wasenm&#xfc;ller</a>",
          "description": "LiDAR depth maps provide environmental guidance in a variety of applications.\nHowever, such depth maps are typically sparse and insufficient for complex\ntasks such as autonomous navigation. State of the art methods use image guided\nneural networks for dense depth completion. We develop a guided convolutional\nneural network focusing on gathering dense and valid information from sparse\ndepth maps. To this end, we introduce a novel layer with spatially variant and\ncontent-depended dilation to include additional data from sparse input.\nFurthermore, we propose a sparsity invariant residual bottleneck block. We\nevaluate our Dense Validity Mask Network (DVMN) on the KITTI depth completion\nbenchmark and achieve state of the art results. At the time of submission, our\nnetwork is the leading method using sparsity invariant convolution.",
          "link": "http://arxiv.org/abs/2107.06709",
          "publishedOn": "2021-07-15T01:59:03.055Z",
          "wordCount": 580,
          "title": "DVMN: Dense Validity Mask Network for Depth Completion. (arXiv:2107.06709v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06463",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Haisheng Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jianping Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jie Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guohe Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_C/0/1/0/all/0/1\">Chengjie Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Jingning Han</a>",
          "description": "Recently deep learning-based image compression methods have achieved\nsignificant achievements and gradually outperformed traditional approaches\nincluding the latest standard Versatile Video Coding (VVC) in both PSNR and\nMS-SSIM metrics. Two key components of learned image compression frameworks are\nthe entropy model of the latent representations and the encoding/decoding\nnetwork architectures. Various models have been proposed, such as\nautoregressive, softmax, logistic mixture, Gaussian mixture, and Laplacian.\nExisting schemes only use one of these models. However, due to the vast\ndiversity of images, it is not optimal to use one model for all images, even\ndifferent regions of one image. In this paper, we propose a more flexible\ndiscretized Gaussian-Laplacian-Logistic mixture model (GLLMM) for the latent\nrepresentations, which can adapt to different contents in different images and\ndifferent regions of one image more accurately. Besides, in the\nencoding/decoding network design part, we propose a concatenated residual\nblocks (CRB), where multiple residual blocks are serially connected with\nadditional shortcut connections. The CRB can improve the learning ability of\nthe network, which can further improve the compression performance.\nExperimental results using the Kodak and Tecnick datasets show that the\nproposed scheme outperforms all the state-of-the-art learning-based methods and\nexisting compression standards including VVC intra coding (4:4:4 and 4:2:0) in\nterms of the PSNR and MS-SSIM.",
          "link": "http://arxiv.org/abs/2107.06463",
          "publishedOn": "2021-07-15T01:59:03.048Z",
          "wordCount": 683,
          "title": "Learned Image Compression with Discretized Gaussian-Laplacian-Logistic Mixture Model and Concatenated Residual Modules. (arXiv:2107.06463v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Windsor_R/0/1/0/all/0/1\">Rhydian Windsor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1\">Amir Jamaludin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1\">Timor Kadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "This paper explores the use of self-supervised deep learning in medical\nimaging in cases where two scan modalities are available for the same subject.\nSpecifically, we use a large publicly-available dataset of over 20,000 subjects\nfrom the UK Biobank with both whole body Dixon technique magnetic resonance\n(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three\ncontributions: (i) We introduce a multi-modal image-matching contrastive\nframework, that is able to learn to match different-modality scans of the same\nsubject with high accuracy. (ii) Without any adaption, we show that the\ncorrespondences learnt during this contrastive training step can be used to\nperform automatic cross-modal scan registration in a completely unsupervised\nmanner. (iii) Finally, we use these registrations to transfer segmentation maps\nfrom the DXA scans to the MR scans where they are used to train a network to\nsegment anatomical regions without requiring ground-truth MR examples. To aid\nfurther research, our code will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.06652",
          "publishedOn": "2021-07-15T01:59:03.042Z",
          "wordCount": 613,
          "title": "Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging. (arXiv:2107.06652v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xuguang Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>",
          "description": "Human motion prediction is an important and challenging topic that has\npromising prospects in efficient and safe human-robot-interaction systems.\nCurrently, the majority of the human motion prediction algorithms are based on\ndeterministic models, which may lead to risky decisions for robots. To solve\nthis problem, we propose a probabilistic model for human motion prediction in\nthis paper. The key idea of our approach is to extend the conventional\ndeterministic motion prediction neural network to a Bayesian one. On one hand,\nour model could generate several future motions when given an observed motion\nsequence. On the other hand, by calculating the Epistemic Uncertainty and the\nHeteroscedastic Aleatoric Uncertainty, our model could tell the robot if the\nobservation has been seen before and also give the optimal result among all\npossible predictions. We extensively validate our approach on a large scale\nbenchmark dataset Human3.6m. The experiments show that our approach performs\nbetter than deterministic methods. We further evaluate our approach in a\nHuman-Robot-Interaction (HRI) scenario. The experimental results show that our\napproach makes the interaction more efficient and safer.",
          "link": "http://arxiv.org/abs/2107.06564",
          "publishedOn": "2021-07-15T01:59:03.027Z",
          "wordCount": 619,
          "title": "Probabilistic Human Motion Prediction via A Bayesian Neural Network. (arXiv:2107.06564v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sangtae Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asma_E/0/1/0/all/0/1\">Evren Asma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandler_A/0/1/0/all/0/1\">Adam Chandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihsani_A/0/1/0/all/0/1\">Alvin Ihsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevrhal_S/0/1/0/all/0/1\">Sven Prevrhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saboury_B/0/1/0/all/0/1\">Babak Saboury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thielemans_K/0/1/0/all/0/1\">Kris Thielemans</a>",
          "description": "Artificial intelligence (AI) has significant potential to positively impact\nand advance medical imaging, including positron emission tomography (PET)\nimaging applications. AI has the ability to enhance and optimize all aspects of\nthe PET imaging chain from patient scheduling, patient setup, protocoling, data\nacquisition, detector signal processing, reconstruction, image processing and\ninterpretation. AI poses industry-specific challenges which will need to be\naddressed and overcome to maximize the future potentials of AI in PET. This\npaper provides an overview of these industry-specific challenges for the\ndevelopment, standardization, commercialization, and clinical adoption of AI,\nand explores the potential enhancements to PET imaging brought on by AI in the\nnear future. In particular, the combination of on-demand image reconstruction,\nAI, and custom designed data processing workflows may open new possibilities\nfor innovation which would positively impact the industry and ultimately\npatients.",
          "link": "http://arxiv.org/abs/2107.06747",
          "publishedOn": "2021-07-15T01:59:03.021Z",
          "wordCount": 584,
          "title": "Artificial Intelligence in PET: an Industry Perspective. (arXiv:2107.06747v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1\">Patryk Orzechowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1\">Jason H. Moore</a>",
          "description": "Understanding the strengths and weaknesses of machine learning (ML)\nalgorithms is crucial for determine their scope of application. Here, we\nintroduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of\nsynthetic datasets for comprehensive, reproducible, and interpretable\nbenchmarking of machine learning algorithms for classification of binary\noutcomes. The DIGEN resource consists of 40 mathematical functions which map\ncontinuous features to discrete endpoints for creating synthetic datasets.\nThese 40 functions were discovered using a heuristic algorithm designed to\nmaximize the diversity of performance among multiple popular machine learning\nalgorithms thus providing a useful test suite for evaluating and comparing new\nmethods. Access to the generative functions facilitates understanding of why a\nmethod performs poorly compared to other algorithms thus providing ideas for\nimprovement. The resource with extensive documentation and analyses is\nopen-source and available on GitHub.",
          "link": "http://arxiv.org/abs/2107.06475",
          "publishedOn": "2021-07-15T01:59:03.015Z",
          "wordCount": 615,
          "title": "Generative and reproducible benchmarks for comprehensive evaluation of machine learning classifiers. (arXiv:2107.06475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskare_P/0/1/0/all/0/1\">Pranjal Bhaskare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "This paper presents a deep learning approach for the classification of\nEngineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to\nthe availability of large annotated datasets and also enough computational\npower in the form of GPUs, many deep learning-based solutions for object\nclassification have been proposed of late, especially in the domain of images\nand graphical models. Nevertheless, very few solutions have been proposed for\nthe task of functional classification of CAD models. Hence, for this research,\nCAD models have been collected from Engineering Shape Benchmark (ESB), National\nDesign Repository (NDR) and augmented with newer models created using a\nmodelling software to form a dataset - 'CADNET'. It is proposed to use a\nresidual network architecture for CADNET, inspired by the popular ResNet. A\nweighted Light Field Descriptor (LFD) scheme is chosen as the method of feature\nextraction, and the generated images are fed as inputs to the CNN. The problem\nof class imbalance in the dataset is addressed using a class weights approach.\nExperiments have been conducted with other signatures such as geodesic distance\netc. using deep networks as well as other network architectures on the CADNET.\nThe LFD-based CNN approach using the proposed network architecture, along with\ngradient boosting yielded the best classification accuracy on CADNET.",
          "link": "http://arxiv.org/abs/2107.06481",
          "publishedOn": "2021-07-15T01:59:03.008Z",
          "wordCount": 671,
          "title": "A Convolutional Neural Network Approach to the Classification of Engineering Models. (arXiv:2107.06481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qimeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>",
          "description": "Temporal action detection (TAD) aims to determine the semantic label and the\nboundaries of every action instance in an untrimmed video. It is a fundamental\nand challenging task in video understanding and significant progress has been\nmade. Previous methods involve multiple stages or networks and hand-designed\nrules or operations, which fall short in efficiency and flexibility. In this\npaper, we propose an end-to-end framework for TAD upon Transformer, termed\n\\textit{TadTR}, which maps a set of learnable embeddings to action instances in\nparallel. TadTR is able to adaptively extract temporal context information\nrequired for making action predictions, by selectively attending to a sparse\nset of snippets in a video. As a result, it simplifies the pipeline of TAD and\nrequires lower computation cost than previous detectors, while preserving\nremarkable detection performance. TadTR achieves state-of-the-art performance\non HACS Segments (+3.35% average mAP). As a single-network detector, TadTR runs\n10$\\times$ faster than its comparable competitor. It outperforms existing\nsingle-network detectors by a large margin on THUMOS14 (+5.0% average mAP) and\nActivityNet (+7.53% average mAP). When combined with other detectors, it\nreports 54.1% mAP at IoU=0.5 on THUMOS14, and 34.55% average mAP on\nActivityNet-1.3. Our code will be released at\n\\url{https://github.com/xlliu7/TadTR}.",
          "link": "http://arxiv.org/abs/2106.10271",
          "publishedOn": "2021-07-15T01:59:03.002Z",
          "wordCount": 667,
          "title": "End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yijie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>",
          "description": "Image hazing aims to render a hazy image from a given clean one, which could\nbe applied to a variety of practical applications such as gaming, filming,\nphotographic filtering, and image dehazing. To generate plausible haze, we\nstudy two less-touched but challenging problems in hazy image rendering,\nnamely, i) how to estimate the transmission map from a single image without\nauxiliary information, and ii) how to adaptively learn the airlight from\nexemplars, i.e., unpaired real hazy images. To this end, we propose a neural\nrendering method for image hazing, dubbed as HazeGEN. To be specific, HazeGEN\nis a knowledge-driven neural network which estimates the transmission map by\nleveraging a new prior, i.e., there exists the structure similarity (e.g.,\ncontour and luminance) between the transmission map and the input clean image.\nTo adaptively learn the airlight, we build a neural module based on another new\nprior, i.e., the rendered hazy image and the exemplar are similar in the\nairlight distribution. To the best of our knowledge, this could be the first\nattempt to deeply rendering hazy images in an unsupervised fashion. Comparing\nwith existing haze generation methods, HazeGEN renders the hazy images in an\nunsupervised, learnable, and controllable manner, thus avoiding the\nlabor-intensive efforts in paired data collection and the domain-shift issue in\nhaze generation. Extensive experiments show the promising performance of our\nmethod comparing with some baselines in both qualitative and quantitative\ncomparisons. The code will be released on GitHub after acceptance.",
          "link": "http://arxiv.org/abs/2107.06681",
          "publishedOn": "2021-07-15T01:59:02.996Z",
          "wordCount": 678,
          "title": "Unsupervised Neural Rendering for Image Hazing. (arXiv:2107.06681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1\">Jamie Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Prisacariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brostow_G/0/1/0/all/0/1\">Gabriel Brostow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firman_M/0/1/0/all/0/1\">Michael Firman</a>",
          "description": "Self-supervised monocular depth estimation networks are trained to predict\nscene depth using nearby frames as a supervision signal during training.\nHowever, for many applications, sequence information in the form of video\nframes is also available at test time. The vast majority of monocular networks\ndo not make use of this extra signal, thus ignoring valuable information that\ncould be used to improve the predicted depth. Those that do, either use\ncomputationally expensive test-time refinement techniques or off-the-shelf\nrecurrent networks, which only indirectly make use of the geometric information\nthat is inherently available.\n\nWe propose ManyDepth, an adaptive approach to dense depth estimation that can\nmake use of sequence information at test time, when it is available. Taking\ninspiration from multi-view stereo, we propose a deep end-to-end cost volume\nbased approach that is trained using self-supervision only. We present a novel\nconsistency loss that encourages the network to ignore the cost volume when it\nis deemed unreliable, e.g. in the case of moving objects, and an augmentation\nscheme to cope with static cameras. Our detailed experiments on both KITTI and\nCityscapes show that we outperform all published self-supervised baselines,\nincluding those that use single or multiple frames at test time.",
          "link": "http://arxiv.org/abs/2104.14540",
          "publishedOn": "2021-07-15T01:59:02.978Z",
          "wordCount": 670,
          "title": "The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth. (arXiv:2104.14540v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_B/0/1/0/all/0/1\">Baolian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chengwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huiguang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>",
          "description": "Locating diseases in chest X-ray images with few careful annotations saves\nlarge human effort. Recent works approached this task with innovative\nweakly-supervised algorithms such as multi-instance learning (MIL) and class\nactivation maps (CAM), however, these methods often yield inaccurate or\nincomplete regions. One of the reasons is the neglection of the pathological\nimplications hidden in the relationship across anatomical regions within each\nimage and the relationship across images. In this paper, we argue that the\ncross-region and cross-image relationship, as contextual and compensating\ninformation, is vital to obtain more consistent and integral regions. To model\nthe relationship, we propose the Graph Regularized Embedding Network (GREN),\nwhich leverages the intra-image and inter-image information to locate diseases\non chest X-ray images. GREN uses a pre-trained U-Net to segment the lung lobes,\nand then models the intra-image relationship between the lung lobes using an\nintra-image graph to compare different regions. Meanwhile, the relationship\nbetween in-batch images is modeled by an inter-image graph to compare multiple\nimages. This process mimics the training and decision-making process of a\nradiologist: comparing multiple regions and images for diagnosis. In order for\nthe deep embedding layers of the neural network to retain structural\ninformation (important in the localization task), we use the Hash coding and\nHamming distance to compute the graphs, which are used as regularizers to\nfacilitate training. By means of this, our approach achieves the\nstate-of-the-art result on NIH chest X-ray dataset for weakly-supervised\ndisease localization. Our codes are accessible online.",
          "link": "http://arxiv.org/abs/2107.06442",
          "publishedOn": "2021-07-15T01:59:02.971Z",
          "wordCount": 703,
          "title": "GREN: Graph-Regularized Embedding Network for Weakly-Supervised Disease Localization in X-ray images. (arXiv:2107.06442v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bernhardt_M/0/1/0/all/0/1\">Melanie Bernhardt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1\">Rajesh Jena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nushi_B/0/1/0/all/0/1\">Besmira Nushi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wadhwani_S/0/1/0/all/0/1\">Sharan Wadhwani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Natarajan_K/0/1/0/all/0/1\">Kal Natarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashraf_S/0/1/0/all/0/1\">Shazad Ashraf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>",
          "description": "Chest radiography has been a recommended procedure for patient triaging and\nresource management in intensive care units (ICUs) throughout the COVID-19\npandemic. The machine learning efforts to augment this workflow have been long\nchallenged due to deficiencies in reporting, model evaluation, and failure mode\nanalysis. To address some of those shortcomings, we model radiological features\nwith a human-interpretable class hierarchy that aligns with the radiological\ndecision process. Also, we propose the use of a data-driven error analysis\nmethodology to uncover the blind spots of our model, providing further\ntransparency on its clinical utility. For example, our experiments show that\nmodel failures highly correlate with ICU imaging conditions and with the\ninherent difficulty in distinguishing certain types of radiological features.\nAlso, our hierarchical interpretation and analysis facilitates the comparison\nwith respect to radiologists' findings and inter-variability, which in return\nhelps us to better assess the clinical applicability of models.",
          "link": "http://arxiv.org/abs/2107.06618",
          "publishedOn": "2021-07-15T01:59:02.965Z",
          "wordCount": 673,
          "title": "Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs. (arXiv:2107.06618v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suneung-Kim/0/1/0/all/0/1\">Suneung-Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Due to the recent outbreak of COVID-19, many classes, exams, and meetings\nhave been conducted non-face-to-face. However, the foundation for video\nconferencing solutions is still insufficient. So this technology has become an\nimportant issue. In particular, these technologies are essential for\nnon-face-to-face testing, and technology dissemination is urgent. In this\npaper, we present a single video conferencing solution using gaze estimation in\npreparation for these problems. Gaze is an important cue for the tasks such as\nanalysis of human behavior. Hence, numerous studies have been proposed to solve\ngaze estimation using deep learning, which is one of the most prominent methods\nup to date. We use these gaze estimation methods to detect abnormal behavior of\nvideo conferencing participants. Our contribution is as follows. i) We find and\napply the optimal network for the gaze estimation method and apply a\nself-supervised method to improve accuracy. ii) For anomaly detection, we\npresent a new dataset that aggregates the values of a new gaze, head pose, etc.\niii) We train newly created data on Multi Layer Perceptron (MLP) models to\ndetect anomaly behavior based on deep learning. We demonstrate the robustness\nof our method through experiments.",
          "link": "http://arxiv.org/abs/2107.06530",
          "publishedOn": "2021-07-15T01:59:02.959Z",
          "wordCount": 685,
          "title": "Detection of Abnormal Behavior with Self-Supervised Gaze Estimation. (arXiv:2107.06530v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>",
          "description": "As a key component of talking face generation, lip movements generation\ndetermines the naturalness and coherence of the generated talking face video.\nPrior literature mainly focuses on speech-to-lip generation while there is a\npaucity in text-to-lip (T2L) generation. T2L is a challenging task and existing\nend-to-end works depend on the attention mechanism and autoregressive (AR)\ndecoding manner. However, the AR decoding manner generates current lip frame\nconditioned on frames generated previously, which inherently hinders the\ninference speed, and also has a detrimental effect on the quality of generated\nlip frames due to error propagation. This encourages the research of parallel\nT2L generation. In this work, we propose a novel parallel decoding model for\nhigh-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we\npredict the duration of the encoded linguistic features and model the target\nlip frames conditioned on the encoded linguistic features with their duration\nin a non-autoregressive manner. Furthermore, we incorporate the structural\nsimilarity index loss and adversarial learning to improve perceptual quality of\ngenerated lip frames and alleviate the blurry prediction problem. Extensive\nexperiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L\ngenerates lip movements with competitive quality compared with the\nstate-of-the-art AR T2L model DualLip and exceeds the baseline AR model\nTransformerT2L by a notable margin benefiting from the mitigation of the error\npropagation problem; and 2) exhibits distinct superiority in inference speed\n(an average speedup of 19$\\times$ than DualLip on TCD-TIMIT).",
          "link": "http://arxiv.org/abs/2107.06831",
          "publishedOn": "2021-07-15T01:59:02.942Z",
          "wordCount": 668,
          "title": "High-Speed and High-Quality Text-to-Lip Generation. (arXiv:2107.06831v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">YiMin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yingjie Xi</a>",
          "description": "Detection faults in seismic data is a crucial step for seismic structural\ninterpretation, reservoir characterization and well placement. Some recent\nworks regard it as an image segmentation task. The task of image segmentation\nrequires huge labels, especially 3D seismic data, which has a complex structure\nand lots of noise. Therefore, its annotation requires expert experience and a\nhuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to\neffectively train 3D-CNN by some slices from 3D seismic data, so that the model\ncan learn the segmentation of 3D seismic data from a few 2D slices. In order to\nfully extract information from limited data and suppress seismic noise, we\npropose an attention module that can be used for active supervision training\nand embedded in the network. The attention heatmap label is generated by the\noriginal label, and letting it supervise the attention module using the\nlambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss\nfunction, the method can extract 3D seismic features from a few 2D slice\nlabels. And it also shows the advanced performance of the attention module,\nwhich can significantly suppress the noise in the seismic data while increasing\nthe model's sensitivity to the foreground. Finally, on the public test set, we\nonly use the 2D slice labels training that accounts for 3.3% of the 3D volume\nlabel, and achieve similar performance to the 3D volume label training.",
          "link": "http://arxiv.org/abs/2105.03857",
          "publishedOn": "2021-07-15T01:59:02.936Z",
          "wordCount": 754,
          "title": "Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels. (arXiv:2105.03857v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1\">Akshay L Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sai Vikas Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Active Learning (AL) techniques aim to minimize the training data required to\ntrain a model for a given task. Pool-based AL techniques start with a small\ninitial labeled pool and then iteratively pick batches of the most informative\nsamples for labeling. Generally, the initial pool is sampled randomly and\nlabeled to seed the AL iterations. While recent studies have focused on\nevaluating the robustness of various query functions in AL, little to no\nattention has been given to the design of the initial labeled pool for deep\nactive learning. Given the recent successes of learning representations in\nself-supervised/unsupervised ways, we study if an intelligently sampled initial\nlabeled pool can improve deep AL performance. We investigate the effect of\nintelligently sampled initial labeled pools, including the use of\nself-supervised and unsupervised strategies, on deep AL methods. The setup,\nhypotheses, methodology, and implementation details were evaluated by peer\nreview before experiments were conducted. Experimental results could not\nconclusively prove that intelligently sampled initial pools are better for AL\nthan random initial pools in the long run, although a Variational\nAutoencoder-based initial pool sampling strategy showed interesting trends that\nmerit deeper investigation.",
          "link": "http://arxiv.org/abs/2011.14696",
          "publishedOn": "2021-07-15T01:59:02.929Z",
          "wordCount": 683,
          "title": "On Initial Pools for Deep Active Learning. (arXiv:2011.14696v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sandip Roy</a>",
          "description": "Terse representation of high-dimensional weather scene data is explored, in\nsupport of strategic air traffic flow management objectives. Specifically, we\nconsider whether aviation-relevant weather scenes are compressible, in the\nsense that each scene admits a possibly-different sparse representation in a\nbasis of interest. Here, compression of weather scenes extracted from METAR\ndata (including temperature, flight categories, and visibility profiles for the\ncontiguous United States) is examined, for the graph-spectral basis. The scenes\nare found to be compressible, with 75-95% of the scene content captured using\n0.5-4% of the basis vectors. Further, the dominant basis vectors for each scene\nare seen to identify time-varying spatial characteristics of the weather, and\nreconstruction from the compressed representation is demonstrated. Finally,\npotential uses of the compressive representations in strategic TFM design are\nbriefly scoped.",
          "link": "http://arxiv.org/abs/2107.06394",
          "publishedOn": "2021-07-15T01:59:02.923Z",
          "wordCount": 572,
          "title": "Compressive Representations of Weather Scenes for Strategic Air Traffic Flow Management. (arXiv:2107.06394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1\">Rajat Koner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildebrandt_M/0/1/0/all/0/1\">Marcel Hildebrandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Deepan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Visual Question Answering (VQA) is concerned with answering free-form\nquestions about an image. Since it requires a deep semantic and linguistic\nunderstanding of the question and the ability to associate it with various\nobjects that are present in the image, it is an ambitious task and requires\nmulti-modal reasoning from both computer vision and natural language\nprocessing. We propose Graphhopper, a novel method that approaches the task by\nintegrating knowledge graph reasoning, computer vision, and natural language\nprocessing techniques. Concretely, our method is based on performing\ncontext-driven, sequential reasoning based on the scene entities and their\nsemantic and spatial relationships. As a first step, we derive a scene graph\nthat describes the objects in the image, as well as their attributes and their\nmutual relationships. Subsequently, a reinforcement learning agent is trained\nto autonomously navigate in a multi-hop manner over the extracted scene graph\nto generate reasoning paths, which are the basis for deriving answers. We\nconduct an experimental study on the challenging dataset GQA, based on both\nmanually curated and automatically generated scene graphs. Our results show\nthat we keep up with a human performance on manually curated scene graphs.\nMoreover, we find that Graphhopper outperforms another state-of-the-art scene\ngraph reasoning model on both manually curated and automatically generated\nscene graphs by a significant margin.",
          "link": "http://arxiv.org/abs/2107.06325",
          "publishedOn": "2021-07-15T01:59:02.917Z",
          "wordCount": 665,
          "title": "Graphhopper: Multi-Hop Scene Graph Reasoning for Visual Question Answering. (arXiv:2107.06325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deguerre_B/0/1/0/all/0/1\">Benjamin Deguerre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1\">Clement Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1\">Gilles Gasso</a>",
          "description": "Object detection in images has reached unprecedented performances. The\nstate-of-the-art methods rely on deep architectures that extract salient\nfeatures and predict bounding boxes enclosing the objects of interest. These\nmethods essentially run on RGB images. However, the RGB images are often\ncompressed by the acquisition devices for storage purpose and transfer\nefficiency. Hence, their decompression is required for object detectors. To\ngain in efficiency, this paper proposes to take advantage of the compressed\nrepresentation of images to carry out object detection usable in constrained\nresources conditions.\n\nSpecifically, we focus on JPEG images and propose a thorough analysis of\ndetection architectures newly designed in regard of the peculiarities of the\nJPEG norm. This leads to a $\\times 1.7$ speed up in comparison with a standard\nRGB-based architecture, while only reducing the detection performance by 5.5%.\nAdditionally, our empirical findings demonstrate that only part of the\ncompressed JPEG information, namely the luminance component, may be required to\nmatch detection accuracy of the full input methods.",
          "link": "http://arxiv.org/abs/2006.05732",
          "publishedOn": "2021-07-15T01:59:02.911Z",
          "wordCount": 645,
          "title": "Object Detection in the DCT Domain: is Luminance the Solution?. (arXiv:2006.05732v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haimin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>",
          "description": "Recent neural rendering approaches for human activities achieve remarkable\nview synthesis results, but still rely on dense input views or dense training\nwith all the capture frames, leading to deployment difficulty and inefficient\ntraining overload. However, existing advances will be ill-posed if the input is\nboth spatially and temporally sparse. To fill this gap, in this paper we\npropose a few-shot neural human rendering approach (FNHR) from only sparse RGBD\ninputs, which exploits the temporal and spatial redundancy to generate\nphoto-realistic free-view output of human activities. Our FNHR is trained only\non the key-frames which expand the motion manifold in the input sequences. We\nintroduce a two-branch neural blending to combine the neural point render and\nclassical graphics texturing pipeline, which integrates reliable observations\nover sparse key-frames. Furthermore, we adopt a patch-based adversarial\ntraining process to make use of the local redundancy and avoids over-fitting to\nthe key-frames, which generates fine-detailed rendering results. Extensive\nexperiments demonstrate the effectiveness of our approach to generate\nhigh-quality free view-point results for challenging human performances under\nthe sparse setting.",
          "link": "http://arxiv.org/abs/2107.06505",
          "publishedOn": "2021-07-15T01:59:02.895Z",
          "wordCount": 625,
          "title": "Few-shot Neural Human Performance Rendering from Sparse RGBD Videos. (arXiv:2107.06505v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Duhun Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eunjung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1\">Wonjong Rhee</a>",
          "description": "We propose an AID-purifier that can boost the robustness of\nadversarially-trained networks by purifying their inputs. AID-purifier is an\nauxiliary network that works as an add-on to an already trained main\nclassifier. To keep it computationally light, it is trained as a discriminator\nwith a binary cross-entropy loss. To obtain additionally useful information\nfrom the adversarial examples, the architecture design is closely related to\ninformation maximization principles where two layers of the main classification\nnetwork are piped to the auxiliary network. To assist the iterative\noptimization procedure of purification, the auxiliary network is trained with\nAVmixup. AID-purifier can be used together with other purifiers such as\nPixelDefend for an extra enhancement. The overall results indicate that the\nbest performing adversarially-trained networks can be enhanced by the best\nperforming purification networks, where AID-purifier is a competitive candidate\nthat is light and robust.",
          "link": "http://arxiv.org/abs/2107.06456",
          "publishedOn": "2021-07-15T01:59:02.889Z",
          "wordCount": 590,
          "title": "AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense. (arXiv:2107.06456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06449",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Hengtao Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xuanang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Sheng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wood_B/0/1/0/all/0/1\">Bradford J. Wood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>",
          "description": "Fusing intra-operative 2D transrectal ultrasound (TRUS) image with\npre-operative 3D magnetic resonance (MR) volume to guide prostate biopsy can\nsignificantly increase the yield. However, such a multimodal 2D/3D registration\nproblem is a very challenging task. In this paper, we propose an end-to-end\nframe-to-volume registration network (FVR-Net), which can efficiently bridge\nthe previous research gaps by aligning a 2D TRUS frame with a 3D TRUS volume\nwithout requiring hardware tracking. The proposed FVR-Net utilizes a\ndual-branch feature extraction module to extract the information from TRUS\nframe and volume to estimate transformation parameters. We also introduce a\ndifferentiable 2D slice sampling module which allows gradients backpropagating\nfrom an unsupervised image similarity loss for content correspondence learning.\nOur model shows superior efficiency for real-time interventional guidance with\nhighly competitive registration accuracy.",
          "link": "http://arxiv.org/abs/2107.06449",
          "publishedOn": "2021-07-15T01:59:02.883Z",
          "wordCount": 576,
          "title": "End-to-end Ultrasound Frame to Volume Registration. (arXiv:2107.06449v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1\">Weikai Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">Geguang Pu</a>",
          "description": "High-level representation-guided pixel denoising and adversarial training are\nindependent solutions to enhance the robustness of CNNs against adversarial\nattacks by pre-processing input data and re-training models, respectively. Most\nrecently, adversarial training techniques have been widely studied and improved\nwhile the pixel denoising-based method is getting less attractive. However, it\nis still questionable whether there exists a more advanced pixel\ndenoising-based method and whether the combination of the two solutions\nbenefits each other. To this end, we first comprehensively investigate two\nkinds of pixel denoising methods for adversarial robustness enhancement (i.e.,\nexisting additive-based and unexplored filtering-based methods) under the loss\nfunctions of image-level and semantic-level restorations, respectively, showing\nthat pixel-wise filtering can obtain much higher image quality (e.g., higher\nPSNR) as well as higher robustness (e.g., higher accuracy on adversarial\nexamples) than existing pixel-wise additive-based method. However, we also\nobserve that the robustness results of the filtering-based method rely on the\nperturbation amplitude of adversarial examples used for training. To address\nthis problem, we propose predictive perturbation-aware pixel-wise filtering,\nwhere dual-perturbation filtering and an uncertainty-aware fusion module are\ndesigned and employed to automatically perceive the perturbation amplitude\nduring the training and testing process. The proposed method is termed as\nAdvFilter. Moreover, we combine adversarial pixel denoising methods with three\nadversarial training-based methods, hinting that considering data and models\njointly is able to achieve more robust CNNs. The experiments conduct on\nNeurIPS-2017DEV, SVHN, and CIFAR10 datasets and show the advantages over\nenhancing CNNs' robustness, high generalization to different models, and noise\nlevels.",
          "link": "http://arxiv.org/abs/2107.06501",
          "publishedOn": "2021-07-15T01:59:02.877Z",
          "wordCount": 716,
          "title": "AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teutscher_D/0/1/0/all/0/1\">Dennis Teutscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangat_P/0/1/0/all/0/1\">Patrick Mangat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1\">Oliver Wasenm&#xfc;ller</a>",
          "description": "Depth completion from sparse LiDAR and high-resolution RGB data is one of the\nfoundations for autonomous driving techniques. Current approaches often rely on\nCNN-based methods with several known drawbacks: flying pixel at depth\ndiscontinuities, overfitting to both a given data set as well as error metric,\nand many more. Thus, we propose our novel Piecewise Depth Completion (PDC),\nwhich works completely without deep learning. PDC segments the RGB image into\nsuperpixels corresponding the regions with similar depth value. Superpixels\ncorresponding to same objects are gathered using a cost map. At the end, we\nreceive detailed depth images with state of the art accuracy. In our\nevaluation, we can show both the influence of the individual proposed\nprocessing steps and the overall performance of our method on the challenging\nKITTI dataset.",
          "link": "http://arxiv.org/abs/2107.06711",
          "publishedOn": "2021-07-15T01:59:02.861Z",
          "wordCount": 574,
          "title": "PDC: Piecewise Depth Completion utilizing Superpixels. (arXiv:2107.06711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1\">Larisa Markeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1\">Ilya Tolstikhin</a>",
          "description": "We introduce a generalization to the lottery ticket hypothesis in which the\nnotion of \"sparsity\" is relaxed by choosing an arbitrary basis in the space of\nparameters. We present evidence that the original results reported for the\ncanonical basis continue to hold in this broader setting. We describe how\nstructured pruning methods, including pruning units or factorizing\nfully-connected layers into products of low-rank matrices, can be cast as\nparticular instances of this \"generalized\" lottery ticket hypothesis. The\ninvestigations reported here are preliminary and are provided to encourage\nfurther research along this direction.",
          "link": "http://arxiv.org/abs/2107.06825",
          "publishedOn": "2021-07-15T01:59:02.855Z",
          "wordCount": 542,
          "title": "A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+More_A/0/1/0/all/0/1\">Amit More</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Predicting novel views of a scene from real-world images has always been a\nchallenging task. In this work, we propose a deep convolutional neural network\n(CNN) which learns to predict novel views of a scene from given collection of\nimages. In comparison to prior deep learning based approaches, which can handle\nonly a fixed number of input images to predict novel view, proposed approach\nworks with different numbers of input images. The proposed model explicitly\nperforms feature extraction and matching from a given pair of input images and\nestimates, at each pixel, the probability distribution (pdf) over possible\ndepth levels in the scene. This pdf is then used for estimating the novel view.\nThe model estimates multiple predictions of novel view, one estimate per input\nimage pair, from given image collection. The model also estimates an occlusion\nmask and combines multiple novel view estimates in to a single optimal\nprediction. The finite number of depth levels used in the analysis may cause\noccasional blurriness in the estimated view. We mitigate this issue with simple\nmulti-resolution analysis which improves the quality of the estimates. We\nsubstantiate the performance on different datasets and show competitive\nperformance.",
          "link": "http://arxiv.org/abs/2107.06812",
          "publishedOn": "2021-07-15T01:59:02.849Z",
          "wordCount": 621,
          "title": "Deep Learning based Novel View Synthesis. (arXiv:2107.06812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Ning Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jiajun Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lixian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jun Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>",
          "description": "Domain Adaptation has been widely used to deal with the distribution shift in\nvision, language, multimedia etc. Most domain adaptation methods learn\ndomain-invariant features with data from both domains available. However, such\na strategy might be infeasible in practice when source data are unavailable due\nto data-privacy concerns. To address this issue, we propose a novel adaptation\nmethod via hypothesis transfer without accessing source data at adaptation\nstage. In order to fully use the limited target data, a semi-supervised mutual\nenhancement method is proposed, in which entropy minimization and augmented\nlabel propagation are used iteratively to perform inter-domain and intra-domain\nalignments. Compared with state-of-the-art methods, the experimental results on\nthree public datasets demonstrate that our method gets up to 19.9% improvements\non semi-supervised adaptation tasks.",
          "link": "http://arxiv.org/abs/2107.06735",
          "publishedOn": "2021-07-15T01:59:02.843Z",
          "wordCount": 564,
          "title": "Semi-Supervised Hypothesis Transfer for Source-Free Domain Adaptation. (arXiv:2107.06735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1\">Vanessa D&#x27;Amario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Datasets often contain input dimensions that are unnecessary to predict the\noutput label, e.g. background in object recognition, which lead to more\ntrainable parameters. Deep Neural Networks (DNNs) are robust to increasing the\nnumber of parameters in the hidden layers, but it is unclear whether this holds\ntrue for the input layer. In this letter, we investigate the impact of\nunnecessary input dimensions on a central issue of DNNs: their data efficiency,\nie. the amount of examples needed to achieve certain generalization\nperformance. Our results show that unnecessary input dimensions that are\ntask-unrelated substantially degrade data efficiency. This highlights the need\nfor mechanisms that remove {task-unrelated} dimensions to enable data\nefficiency gains.",
          "link": "http://arxiv.org/abs/2107.06409",
          "publishedOn": "2021-07-15T01:59:02.836Z",
          "wordCount": 554,
          "title": "The Foes of Neural Network's Data Efficiency Among Unnecessary Input Dimensions. (arXiv:2107.06409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06281",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Mhiri_I/0/1/0/all/0/1\">Islem Mhiri</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nebli_A/0/1/0/all/0/1\">Ahmed Nebli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mahjoub_M/0/1/0/all/0/1\">Mohamed Ali Mahjoub</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>",
          "description": "Brain graph synthesis marked a new era for predicting a target brain graph\nfrom a source one without incurring the high acquisition cost and processing\ntime of neuroimaging data. However, existing multi-modal graph synthesis\nframeworks have several limitations. First, they mainly focus on generating\ngraphs from the same domain (intra-modality), overlooking the rich multimodal\nrepresentations of brain connectivity (inter-modality). Second, they can only\nhandle isomorphic graph generation tasks, limiting their generalizability to\nsynthesizing target graphs with a different node size and topological structure\nfrom those of the source one. More importantly, both target and source domains\nmight have different distributions, which causes a domain fracture between them\n(i.e., distribution misalignment). To address such challenges, we propose an\ninter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to\ninfer a target graph modality based on a given modality. Our three core\ncontributions lie in (i) predicting a target graph (e.g., functional) from a\nsource graph (e.g., morphological) based on a novel graph generative\nadversarial network (gGAN); (ii) using non-isomorphic graphs for both source\nand target domains with a different number of nodes, edges and structure; and\n(iii) enforcing the predicted target distribution to match that of the ground\ntruth graphs using a graph autoencoder to relax the designed loss oprimization.\nTo handle the unstable behavior of gGAN, we design a new Ground\nTruth-Preserving (GT-P) loss function to guide the generator in learning the\ntopological structure of ground truth brain graphs. Our comprehensive\nexperiments on predicting functional from morphological graphs demonstrate the\noutperformance of IMANGraphNet in comparison with its variants. This can be\nfurther leveraged for integrative and holistic brain mapping in health and\ndisease.",
          "link": "http://arxiv.org/abs/2107.06281",
          "publishedOn": "2021-07-15T01:59:02.828Z",
          "wordCount": 719,
          "title": "Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic Brain Mapping. (arXiv:2107.06281v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baoquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhijian Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "Different environments pose a great challenge on the outdoor robust visual\nperception for long-term autonomous driving and the generalization of\nlearning-based algorithms on different environmental effects is still an open\nproblem. Although monocular depth prediction has been well studied recently,\nthere is few work focusing on the robust learning-based depth prediction across\ndifferent environments, e.g., changing illumination and seasons, owing to the\nlack of such a multi-environment real-world dataset and benchmark. To this end,\nthe first cross-season monocular depth prediction dataset and benchmark\nSeasonDepth (available on https://seasondepth.github.io/) is built based on CMU\nVisual Localization dataset. To benchmark the depth estimation performance\nunder different environments, we investigate representative and recent\nstate-of-the-art open-source supervised, self-supervised and domain adaptation\ndepth prediction methods from KITTI benchmark using several newly-formulated\nmetrics. Through extensive experimental evaluation on the proposed dataset, the\ninfluence of multiple environments on performance and robustness is analyzed\nboth qualitatively and quantitatively, showing that the long-term monocular\ndepth prediction is far from solved even with fine-tuning. We further give\npromising avenues that self-supervised training and stereo geometry constraint\nhelp to enhance the robustness to changing environments.",
          "link": "http://arxiv.org/abs/2011.04408",
          "publishedOn": "2021-07-15T01:59:02.821Z",
          "wordCount": 668,
          "title": "SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06536",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1\">Meng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Jiasong Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xiuping Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>",
          "description": "Image super-resolution (SR) methods can generate remote sensing images with\nhigh spatial resolution without increasing the cost, thereby providing a\nfeasible way to acquire high-resolution remote sensing images, which are\ndifficult to obtain due to the high cost of acquisition equipment and complex\nweather. Clearly, image super-resolution is a severe ill-posed problem.\nFortunately, with the development of deep learning, the powerful fitting\nability of deep neural networks has solved this problem to some extent. In this\npaper, we propose a network based on the generative adversarial network (GAN)\nto generate high resolution remote sensing images, named the multi-attention\ngenerative adversarial network (MA-GAN). We first designed a GAN-based\nframework for the image SR task. The core to accomplishing the SR task is the\nimage generator with post-upsampling that we designed. The main body of the\ngenerator contains two blocks; one is the pyramidal convolution in the\nresidual-dense block (PCRDB), and the other is the attention-based upsample\n(AUP) block. The attentioned pyramidal convolution (AttPConv) in the PCRDB\nblock is a module that combines multi-scale convolution and channel attention\nto automatically learn and adjust the scaling of the residuals for better\nresults. The AUP block is a module that combines pixel attention (PA) to\nperform arbitrary multiples of upsampling. These two blocks work together to\nhelp generate better quality images. For the loss function, we design a loss\nfunction based on pixel loss and introduce both adversarial loss and feature\nloss to guide the generator learning. We have compared our method with several\nstate-of-the-art methods on a remote sensing scene image dataset, and the\nexperimental results consistently demonstrate the effectiveness of the proposed\nMA-GAN.",
          "link": "http://arxiv.org/abs/2107.06536",
          "publishedOn": "2021-07-15T01:59:02.814Z",
          "wordCount": 720,
          "title": "Multi-Attention Generative Adversarial Network for Remote Sensing Image Super-Resolution. (arXiv:2107.06536v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05255",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Ce Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haimiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shang_K/0/1/0/all/0/1\">Kun Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Y/0/1/0/all/0/1\">Yuanyuan Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>",
          "description": "Computed tomography (CT) reconstruction from X-ray projections acquired\nwithin a limited angle range is challenging, especially when the angle range is\nextremely small. Both analytical and iterative models need more projections for\neffective modeling. Deep learning methods have gained prevalence due to their\nexcellent reconstruction performances, but such success is mainly limited\nwithin the same dataset and does not generalize across datasets with different\ndistributions. Hereby we propose ExtraPolationNetwork for limited-angle CT\nreconstruction via the introduction of a sinogram extrapolation module, which\nis theoretically justified. The module complements extra sinogram information\nand boots model generalizability. Extensive experimental results show that our\nreconstruction model achieves state-of-the-art performance on NIH-AAPM dataset,\nsimilar to existing approaches. More importantly, we show that using such a\nsinogram extrapolation module significantly improves the generalization\ncapability of the model on unseen datasets (e.g., COVID-19 and LIDC datasets)\nwhen compared to existing approaches.",
          "link": "http://arxiv.org/abs/2103.05255",
          "publishedOn": "2021-07-15T01:59:02.789Z",
          "wordCount": 670,
          "title": "Improving Generalizability in Limited-Angle CT Reconstruction with Sinogram Extrapolation. (arXiv:2103.05255v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayat_N/0/1/0/all/0/1\">Nasir Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lashen_H/0/1/0/all/0/1\">Hazem Lashen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>",
          "description": "Despite the success of deep neural networks in chest X-ray (CXR) diagnosis,\nsupervised learning only allows the prediction of disease classes that were\nseen during training. At inference, these networks cannot predict an unseen\ndisease class. Incorporating a new class requires the collection of labeled\ndata, which is not a trivial task, especially for less frequently-occurring\ndiseases. As a result, it becomes inconceivable to build a model that can\ndiagnose all possible disease classes. Here, we propose a multi-label\ngeneralized zero shot learning (CXR-ML-GZSL) network that can simultaneously\npredict multiple seen and unseen diseases in CXR images. Given an input image,\nCXR-ML-GZSL learns a visual representation guided by the input's corresponding\nsemantics extracted from a rich medical text corpus. Towards this ambitious\ngoal, we propose to map both visual and semantic modalities to a latent feature\nspace using a novel learning objective. The objective ensures that (i) the most\nrelevant labels for the query image are ranked higher than irrelevant labels,\n(ii) the network learns a visual representation that is aligned with its\nsemantics in the latent feature space, and (iii) the mapped semantics preserve\ntheir original inter-class representation. The network is end-to-end trainable\nand requires no independent pre-training for the offline feature extractor.\nExperiments on the NIH Chest X-ray dataset show that our network outperforms\ntwo strong baselines in terms of recall, precision, f1 score, and area under\nthe receiver operating characteristic curve. Our code is publicly available at:\nhttps://github.com/nyuad-cai/CXR-ML-GZSL.git",
          "link": "http://arxiv.org/abs/2107.06563",
          "publishedOn": "2021-07-15T01:59:02.775Z",
          "wordCount": 698,
          "title": "Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs. (arXiv:2107.06563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_X/0/1/0/all/0/1\">Xiaofei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangtao Xie</a>",
          "description": "This is a short technical report introducing the solution of Team Rat for\nShort-video Parsing Face Parsing Track of The 3rd Person in Context (PIC)\nWorkshop and Challenge at CVPR 2021.\n\nIn this report, we propose an Edge-Aware Network (EANet) that uses edge\ninformation to refine the segmentation edge. To further obtain the finer edge\nresults, we introduce edge attention loss that only compute cross entropy on\nthe edges, it can effectively reduce the classification error around edge and\nget more smooth boundary. Benefiting from the edge information and edge\nattention loss, the proposed EANet achieves 86.16\\% accuracy in the Short-video\nFace Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge,\nranked the third place.",
          "link": "http://arxiv.org/abs/2106.07409",
          "publishedOn": "2021-07-15T01:59:02.759Z",
          "wordCount": 588,
          "title": "3rd Place Solution for Short-video Face Parsing Challenge. (arXiv:2106.07409v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1\">Maxim Rakhuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We study low-rank parameterizations of weight matrices with embedded spectral\nproperties in the Deep Learning context. The low-rank property leads to\nparameter efficiency and permits taking computational shortcuts when computing\nmappings. Spectral properties are often subject to constraints in optimization\nproblems, leading to better models and stability of optimization. We start by\nlooking at the compact SVD parameterization of weight matrices and identifying\nredundancy sources in the parameterization. We further apply the Tensor Train\n(TT) decomposition to the compact SVD components, and propose a non-redundant\ndifferentiable parameterization of fixed TT-rank tensor manifolds, termed the\nSpectral Tensor Train Parameterization (STTP). We demonstrate the effects of\nneural network compression in the image classification setting and both\ncompression and improved training stability in the generative adversarial\ntraining setting.",
          "link": "http://arxiv.org/abs/2103.04217",
          "publishedOn": "2021-07-15T01:59:02.743Z",
          "wordCount": 609,
          "title": "Spectral Tensor Train Parameterization of Deep Learning Layers. (arXiv:2103.04217v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guochen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1\">Qiang Ling</a>",
          "description": "Semi-supervised Fine-Grained Recognition is a challenge task due to the\ndifficulty of data imbalance, high inter-class similarity and domain mismatch.\nRecent years, this field has witnessed great progress and many methods has\ngained great performance. However, these methods can hardly generalize to the\nlarge-scale datasets, such as Semi-iNat, as they are prone to suffer from noise\nin unlabeled data and the incompetence for learning features from imbalanced\nfine-grained data. In this work, we propose Bilateral-Branch Self-Training\nFramework (BiSTF), a simple yet effective framework to improve existing\nsemi-supervised learning methods on class-imbalanced and domain-shifted\nfine-grained data. By adjusting the update frequency through stochastic epoch\nupdate, BiSTF iteratively retrains a baseline SSL model with a labeled set\nexpanded by selectively adding pseudo-labeled samples from an unlabeled set,\nwhere the distribution of pseudo-labeled samples are the same as the labeled\ndata. We show that BiSTF outperforms the existing state-of-the-art SSL\nalgorithm on Semi-iNat dataset.",
          "link": "http://arxiv.org/abs/2107.06768",
          "publishedOn": "2021-07-15T01:59:02.736Z",
          "wordCount": 600,
          "title": "BiSTF: Bilateral-Branch Self-Training Framework for Semi-Supervised Large-scale Fine-Grained Recognition. (arXiv:2107.06768v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Eun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Face anti-spoofing (FAS) plays an important role in protecting face\nrecognition systems from face representation attacks. Many recent studies in\nFAS have approached this problem with domain generalization technique. Domain\ngeneralization aims to increase generalization performance to better detect\nvarious types of attacks and unseen attacks. However, previous studies in this\narea have defined each domain simply as an anti-spoofing datasets and focused\non developing learning techniques. In this paper, we proposed a method that\nenables network to judge its domain by itself with the clustered convolutional\nfeature statistics from intermediate layers of the network, without labeling\ndomains as datasets. We obtained pseudo-domain labels by not only using the\nnetwork extracting features, but also using depth estimators, which were\npreviously used only as an auxiliary task in FAS. In our experiments, we\ntrained with three datasets and evaluated the performance with the remaining\none dataset to demonstrate the effectiveness of the proposed method by\nconducting a total of four sets of experiments.",
          "link": "http://arxiv.org/abs/2107.06552",
          "publishedOn": "2021-07-15T01:59:02.730Z",
          "wordCount": 600,
          "title": "Domain Generalization with Pseudo-Domain Label for Face Anti-Spoofing. (arXiv:2107.06552v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>",
          "description": "Understanding the behavior and vulnerability of pre-trained deep neural\nnetworks (DNNs) can help to improve them. Analysis can be performed via\nreversing the network's flow to generate inputs from internal representations.\nMost existing work relies on priors or data-intensive optimization to invert a\nmodel, yet struggles to scale to deep architectures and complex datasets. This\npaper presents a zero-shot direct model inversion framework that recovers the\ninput to the trained model given only the internal representation. The crux of\nour method is to inverse the DNN in a divide-and-conquer manner while\nre-syncing the inverted layers via cycle-consistency guidance with the help of\nsynthesized data. As a result, we obtain a single feed-forward model capable of\ninversion with a single forward pass without seeing any real data of the\noriginal task. With the proposed approach, we scale zero-shot direct inversion\nto deep architectures and complex datasets. We empirically show that modern\nclassification models on ImageNet can, surprisingly, be inverted, allowing an\napproximate recovery of the original 224x224px images from a representation\nafter more than 20 layers. Moreover, inversion of generators in GANs unveils\nlatent code of a given synthesized face image at 128x128px, which can even, in\nturn, improve defective synthesized images from GANs.",
          "link": "http://arxiv.org/abs/2107.06304",
          "publishedOn": "2021-07-15T01:59:02.724Z",
          "wordCount": 673,
          "title": "Deep Neural Networks are Surprisingly Reversible: A Baseline for Zero-Shot Inversion. (arXiv:2107.06304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1\">Tuomo Lahtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1\">Hannu Turtiainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1\">Andrei Costin</a>",
          "description": "Image annotation and large annotated datasets are crucial parts within the\nComputer Vision and Artificial Intelligence fields.At the same time, it is\nwell-known and acknowledged by the research community that the image annotation\nprocess is challenging, time-consuming and hard to scale. Therefore, the\nresearchers and practitioners are always seeking ways to perform the\nannotations easier, faster, and at higher quality. Even though several widely\nused tools exist and the tools' landscape evolved considerably, most of the\ntools still require intricate technical setups and high levels of technical\nsavviness from its operators and crowdsource contributors.\n\nIn order to address such challenges, we develop and present BRIMA -- a\nflexible and open-source browser extension that allows BRowser-only IMage\nAnnotation at considerably lower overheads. Once added to the browser, it\ninstantly allows the user to annotate images easily and efficiently directly\nfrom the browser without any installation or setup on the client-side. It also\nfeatures cross-browser and cross-platform functionality thus presenting itself\nas a neat tool for researchers within the Computer Vision, Artificial\nIntelligence, and privacy-related fields.",
          "link": "http://arxiv.org/abs/2107.06351",
          "publishedOn": "2021-07-15T01:59:02.718Z",
          "wordCount": 612,
          "title": "BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint). (arXiv:2107.06351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tuan Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1\">Katherine M. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1\">Luke Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1\">Kevin Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Siddharth N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1\">Samuel J. Gershman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>",
          "description": "Modeling complex phenomena typically involves the use of both discrete and\ncontinuous variables. Such a setting applies across a wide range of problems,\nfrom identifying trends in time-series data to performing effective\ncompositional scene understanding in images. Here, we propose Hybrid Memoised\nWake-Sleep (HMWS), an algorithm for effective inference in such hybrid\ndiscrete-continuous models. Prior approaches to learning suffer as they need to\nperform repeated expensive inner-loop discrete inference. We build on a recent\napproach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by\nmemoising discrete variables, and extend it to allow for a principled and\neffective way to handle continuous variables by learning a separate recognition\nmodel used for importance-sampling based approximate inference and\nmarginalization. We evaluate HMWS in the GP-kernel learning and 3D scene\nunderstanding domains, and show that it outperforms current state-of-the-art\ninference methods.",
          "link": "http://arxiv.org/abs/2107.06393",
          "publishedOn": "2021-07-15T01:59:02.702Z",
          "wordCount": 595,
          "title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual\nencoders, using a relatively small set of manually-annotated data (as compared\nto web-crawled data), to perceive the visual world. However, it has been\nobserved that large-scale pretraining usually can result in better\ngeneralization performance, e.g., CLIP (Contrastive Language-Image\nPre-training), trained on a massive amount of image-caption pairs, has shown a\nstrong zero-shot capability on various vision tasks. To further study the\nadvantage brought by CLIP, we propose to use CLIP as the visual encoder in\nvarious V&L models in two typical scenarios: 1) plugging CLIP into\ntask-specific fine-tuning; 2) combining CLIP with V&L pre-training and\ntransferring to downstream tasks. We show that CLIP significantly outperforms\nwidely-used visual encoders trained with in-domain annotated data, such as\nBottomUp-TopDown. We achieve competitive or better results on diverse V&L\ntasks, while establishing new state-of-the-art results on Visual Question\nAnswering, Visual Entailment, and V&L Navigation tasks. We release our code at\nhttps://github.com/clip-vil/CLIP-ViL.",
          "link": "http://arxiv.org/abs/2107.06383",
          "publishedOn": "2021-07-15T01:59:02.694Z",
          "wordCount": 618,
          "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "High-definition map (HD map) construction is a crucial problem for autonomous\ndriving. This problem typically involves collecting high-quality point clouds,\nfusing multiple point clouds of the same scene, annotating map elements, and\nupdating maps constantly. This pipeline, however, requires a vast amount of\nhuman efforts and resources which limits its scalability. Additionally,\ntraditional HD maps are coupled with centimeter-level accurate localization\nwhich is unreliable in many scenarios. In this paper, we argue that online map\nlearning, which dynamically constructs the HD maps based on local sensor\nobservations, is a more scalable way to provide semantic and geometry priors to\nself-driving vehicles than traditional pre-annotated HD maps. Meanwhile, we\nintroduce an online map learning method, titled HDMapNet. It encodes image\nfeatures from surrounding cameras and/or point clouds from LiDAR, and predicts\nvectorized map elements in the bird's-eye view. We benchmark HDMapNet on the\nnuScenes dataset and show that in all settings, it performs better than\nbaseline methods. Of note, our fusion-based HDMapNet outperforms existing\nmethods by more than 50% in all metrics. To accelerate future research, we\ndevelop customized metrics to evaluate map learning performance, including both\nsemantic-level and instance-level ones. By introducing this method and metrics,\nwe invite the community to study this novel map learning problem. We will\nrelease our code and evaluation kit to facilitate future development.",
          "link": "http://arxiv.org/abs/2107.06307",
          "publishedOn": "2021-07-15T01:59:02.688Z",
          "wordCount": 660,
          "title": "HDMapNet: An Online HD Map Construction and Evaluation Framework. (arXiv:2107.06307v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1\">Alexander Mordvintsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1\">Ettore Randazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1\">Eyvind Niklasson</a>",
          "description": "Reaction-Diffusion (RD) systems provide a computational framework that\ngoverns many pattern formation processes in nature. Current RD system design\npractices boil down to trial-and-error parameter search. We propose a\ndifferentiable optimization method for learning the RD system parameters to\nperform example-based texture synthesis on a 2D plane. We do this by\nrepresenting the RD system as a variant of Neural Cellular Automata and using\ntask-specific differentiable loss functions. RD systems generated by our method\nexhibit robust, non-trivial 'life-like' behavior.",
          "link": "http://arxiv.org/abs/2107.06862",
          "publishedOn": "2021-07-15T01:59:02.641Z",
          "wordCount": 522,
          "title": "Differentiable Programming of Reaction-Diffusion Patterns. (arXiv:2107.06862v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2009.03871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foti_S/0/1/0/all/0/1\">Simone Foti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1\">Bongjin Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowrick_T/0/1/0/all/0/1\">Thomas Dowrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalhinho_J/0/1/0/all/0/1\">Joao Ramalhinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allam_M/0/1/0/all/0/1\">Moustafa Allam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1\">Brian Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>",
          "description": "In this work we propose a method based on geometric deep learning to predict\nthe complete surface of the liver, given a partial point cloud of the organ\nobtained during the surgical laparoscopic procedure. We introduce a new data\naugmentation technique that randomly perturbs shapes in their frequency domain\nto compensate the limited size of our dataset. The core of our method is a\nvariational autoencoder (VAE) that is trained to learn a latent space for\ncomplete shapes of the liver. At inference time, the generative part of the\nmodel is embedded in an optimisation procedure where the latent representation\nis iteratively updated to generate a model that matches the intraoperative\npartial point cloud. The effect of this optimisation is a progressive non-rigid\ndeformation of the initially generated shape. Our method is qualitatively\nevaluated on real data and quantitatively evaluated on synthetic data. We\ncompared with a state-of-the-art rigid registration algorithm, that our method\noutperformed in visible areas.",
          "link": "http://arxiv.org/abs/2009.03871",
          "publishedOn": "2021-07-14T01:41:50.599Z",
          "wordCount": 640,
          "title": "Intraoperative Liver Surface Completion with Graph Convolutional VAE. (arXiv:2009.03871v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1\">Kemal Oksuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cam_B/0/1/0/all/0/1\">Baris Can Cam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1\">Sinan Kalkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>",
          "description": "Despite being widely used as a performance measure for visual detection\ntasks, Average Precision (AP) is limited in reflecting localisation quality,\n(ii) interpretability and (iii) robustness to the design choices regarding its\ncomputation, and its applicability to outputs without confidence scores.\nPanoptic Quality (PQ), a measure proposed for evaluating panoptic segmentation\n(Kirillov et al., 2019), does not suffer from these limitations but is limited\nto panoptic segmentation. In this paper, we propose Localisation Recall\nPrecision (LRP) Error as the performance measure for all visual detection\ntasks. LRP Error, initially proposed only for object detection by Oksuz et al.\n(2018), does not suffer from the aforementioned limitations and is applicable\nto all visual detection tasks. We also introduce Optimal LRP (oLRP) Error as\nthe minimum LRP error obtained over confidence scores to evaluate visual\ndetectors and obtain optimal thresholds for deployment. We provide a detailed\ncomparative analysis of LRP with AP and PQ, and use nearly 100 state-of-the-art\nvisual detectors from seven visual detection tasks (i.e. object detection,\nkeypoint detection, instance segmentation, panoptic segmentation, visual\nrelationship detection, zero-shot detection and generalised zero-shot\ndetection) using ten datasets (i.e. different COCO variants, LVIS, Open Images,\nPascal, ILSVRC) to empirically show that LRP provides richer and more\ndiscriminative information than its counterparts. Code available at:\nhttps://github.com/kemaloksuz/LRP-Error",
          "link": "http://arxiv.org/abs/2011.10772",
          "publishedOn": "2021-07-14T01:41:50.468Z",
          "wordCount": 701,
          "title": "One Metric to Measure them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks. (arXiv:2011.10772v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chaoyou Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>",
          "description": "Combinatorial optimization (CO) has been a hot research topic because of its\ntheoretic and practical importance. As a classic CO problem, deep hashing aims\nto find an optimal code for each data from finite discrete possibilities, while\nthe discrete nature brings a big challenge to the optimization process.\nPrevious methods usually mitigate this challenge by binary approximation,\nsubstituting binary codes for real-values via activation functions or\nregularizations. However, such approximation leads to uncertainty between\nreal-values and binary ones, degrading retrieval performance. In this paper, we\npropose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly\nestimates the uncertainty during training and leverages the uncertainty\ninformation to guide the approximation process. Specifically, we model\nbit-level uncertainty via measuring the discrepancy between the output of a\nhashing network and that of a momentum-updated network. The discrepancy of each\nbit indicates the uncertainty of the hashing network to the approximate output\nof that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can\nbe regarded as image-level uncertainty. It embodies the uncertainty of the\nhashing network to the corresponding input image. The hashing bit and image\nwith higher uncertainty are paid more attention during optimization. To the\nbest of our knowledge, this is the first work to study the uncertainty in\nhashing bits. Extensive experiments are conducted on four datasets to verify\nthe superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a\nmillion-scale dataset Clothing1M. Our method achieves the best performance on\nall of the datasets and surpasses existing state-of-the-art methods by a large\nmargin.",
          "link": "http://arxiv.org/abs/2009.08012",
          "publishedOn": "2021-07-14T01:41:50.382Z",
          "wordCount": 725,
          "title": "Deep Momentum Uncertainty Hashing. (arXiv:2009.08012v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "It is well noted that coordinate based MLPs benefit greatly -- in terms of\npreserving high-frequency information -- through the encoding of coordinate\npositions as an array of Fourier features. Hitherto, the rationale for the\neffectiveness of these positional encodings has been solely studied through a\nFourier lens. In this paper, we strive to broaden this understanding by showing\nthat alternative non-Fourier embedding functions can indeed be used for\npositional encoding. Moreover, we show that their performance is entirely\ndetermined by a trade-off between the stable rank of the embedded matrix and\nthe distance preservation between embedded coordinates. We further establish\nthat the now ubiquitous Fourier feature mapping of position is a special case\nthat fulfills these conditions. Consequently, we present a more general theory\nto analyze positional encoding in terms of shifted basis functions. To this\nend, we develop the necessary theoretical formulae and empirically verify that\nour theoretical claims hold in practice. Codes available at\nhttps://github.com/osiriszjq/Rethinking-positional-encoding.",
          "link": "http://arxiv.org/abs/2107.02561",
          "publishedOn": "2021-07-14T01:41:50.364Z",
          "wordCount": 603,
          "title": "Rethinking Positional Encoding. (arXiv:2107.02561v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parashar_S/0/1/0/all/0/1\">Shaifali Parashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yuxuan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "A recent trend in Non-Rigid Structure-from-Motion (NRSfM) is to express\nlocal, differential constraints between pairs of images, from which the surface\nnormal at any point can be obtained by solving a system of polynomial\nequations. The systems of equations derived in previous work, however, are of\nhigh degree, having up to five real solutions, thus requiring a computationally\nexpensive strategy to select a unique solution. Furthermore, they suffer from\ndegeneracies that make the resulting estimates unreliable, without any\nmechanism to identify this situation.\n\nIn this paper, we show that, under widely applicable assumptions, we can\nderive a new system of equation in terms of the surface normals whose two\nsolutions can be obtained in closed-form and can easily be disambiguated\nlocally. Our formalism further allows us to assess how reliable the estimated\nlocal normals are and, hence, to discard them if they are not. Our experiments\nshow that our reconstructions, obtained from two or more views, are\nsignificantly more accurate than those of state-of-the-art methods, while also\nbeing faster.",
          "link": "http://arxiv.org/abs/2011.11567",
          "publishedOn": "2021-07-14T01:41:50.356Z",
          "wordCount": 630,
          "title": "A Closed-Form Solution to Local Non-Rigid Structure-from-Motion. (arXiv:2011.11567v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenlong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_J/0/1/0/all/0/1\">James M. Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_J/0/1/0/all/0/1\">Jeffrey Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bezdek_J/0/1/0/all/0/1\">James C. Bezdek</a>",
          "description": "Examining most streaming clustering algorithms leads to the understanding\nthat they are actually incremental classification models. They model existing\nand newly discovered structures via summary information that we call\nfootprints. Incoming data is normally assigned a crisp label (into one of the\nstructures) and that structure's footprint is incrementally updated. There is\nno reason that these assignments need to be crisp. In this paper, we propose a\nnew streaming classification algorithm that uses Neural Gas prototypes as\nfootprints and produces a possibilistic label vector (of typicalities) for each\nincoming vector. These typicalities are generated by a modified possibilistic\nk-nearest neighbor algorithm. The approach is tested on synthetic and real\nimage datasets. We compare our approach to three other streaming classifiers\nbased on the Adaptive Random Forest, Very Fast Decision Rules, and the\nDenStream algorithm with excellent results.",
          "link": "http://arxiv.org/abs/2010.00635",
          "publishedOn": "2021-07-14T01:41:50.317Z",
          "wordCount": 606,
          "title": "StreamSoNG: A Soft Streaming Classification Approach. (arXiv:2010.00635v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiquan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total\nof 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,\n120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to\nrealize the function of valuating image classification. In order to prove that\nthe methods of different periods in the field of image classification have\ndiscrepancies on GasHisSDB, we select a variety of classifiers for evaluation.\nSeven classical machine learning classifiers, three CNN classifiers and a novel\ntransformer-based classifier are selected for testing on image classification\ntasks. GasHisSDB is available at the\nURL:https://github.com/NEUhwm/GasHisSDB.git.",
          "link": "http://arxiv.org/abs/2106.02473",
          "publishedOn": "2021-07-14T01:41:50.302Z",
          "wordCount": 614,
          "title": "A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jizong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanfeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Caiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>",
          "description": "Despite their outstanding accuracy, semi-supervised segmentation methods\nbased on deep neural networks can still yield predictions that are considered\nanatomically impossible by clinicians, for instance, containing holes or\ndisconnected regions. To solve this problem, we present a Context-aware Virtual\nAdversarial Training (CaVAT) method for generating anatomically plausible\nsegmentation. Unlike approaches focusing solely on accuracy, our method also\nconsiders complex topological constraints like connectivity which cannot be\neasily modeled in a differentiable loss function. We use adversarial training\nto generate examples violating the constraints, so the network can learn to\navoid making such incorrect predictions on new examples, and employ the\nReinforce algorithm to handle non-differentiable segmentation constraints. The\nproposed method offers a generic and efficient way to add any constraint on top\nof any segmentation network. Experiments on two clinically-relevant datasets\nshow our method to produce segmentations that are both accurate and\nanatomically-plausible in terms of region connectivity.",
          "link": "http://arxiv.org/abs/2107.05532",
          "publishedOn": "2021-07-14T01:41:50.280Z",
          "wordCount": 610,
          "title": "Context-aware virtual adversarial training for anatomically-plausible segmentation. (arXiv:2107.05532v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Abdurrahim Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_R/0/1/0/all/0/1\">Rahmetullah Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goktay_F/0/1/0/all/0/1\">Fatih Goktay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencoglan_G/0/1/0/all/0/1\">Gulsum Gencoglan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demircali_A/0/1/0/all/0/1\">Ali Anil Demircali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilsizoglu_B/0/1/0/all/0/1\">Berk Dilsizoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uvet_H/0/1/0/all/0/1\">Huseyin Uvet</a>",
          "description": "Clinical dermatology, still relies heavily on manual introspection of fungi\nwithin a Potassium Hydroxide (KOH) solution using a brightfield microscope.\nHowever, this method takes a long time, is based on the experience of the\nclinician, and has a low accuracy. With the increase of neural network\napplications in the field of clinical microscopy it is now possible to automate\nsuch manual processes increasing both efficiency and accuracy. This study\npresents a deep neural network structure that enables the rapid solutions for\nthese problems and can perform automatic fungi detection in grayscale images\nwithout colorants. Microscopic images of 81 fungi and 235 ceratine were\ncollected. Then, smaller patches were extracted containing 2062 fungi and 2142\nceratine. In order to detect fungus and ceratine, two models were created one\nof which was a custom neural network and the other was based on the VGG16\narchitecture. The developed custom model had 99.84% accuracy, and an area under\nthe curve (AUC) value of 1.00, while the VGG16 model had 98.89% accuracy and an\nAUC value of 0.99. However, average accuracy and AUC value of clinicians is\n72.8% and 0.87 respectively. This deep learning model allows the development of\nan automated system that can detect fungi within microscopic images.",
          "link": "http://arxiv.org/abs/2106.16139",
          "publishedOn": "2021-07-14T01:41:50.272Z",
          "wordCount": 726,
          "title": "Automated Onychomycosis Detection Using Deep Neural Networks. (arXiv:2106.16139v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1\">Ramon Manzorro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1\">Joshua L. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Binh Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1\">Dev Yashpal Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1\">Eero P. Simoncelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteson_D/0/1/0/all/0/1\">David S. Matteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1\">Peter A. Crozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>",
          "description": "Denoising is a fundamental challenge in scientific imaging. Deep\nconvolutional neural networks (CNNs) provide the current state of the art in\ndenoising natural images, where they produce impressive results. However, their\npotential has barely been explored in the context of scientific imaging.\nDenoising CNNs are typically trained on real natural images artificially\ncorrupted with simulated noise. In contrast, in scientific applications,\nnoiseless ground-truth images are usually not available. To address this issue,\nwe propose a simulation-based denoising (SBD) framework, in which CNNs are\ntrained on simulated images. We test the framework on data obtained from\ntransmission electron microscopy (TEM), an imaging technique with widespread\napplications in material science, biology, and medicine. SBD outperforms\nexisting techniques by a wide margin on a simulated benchmark dataset, as well\nas on real data. Apart from the denoised images, SBD generates likelihood maps\nto visualize the agreement between the structure of the denoised image and the\nobserved data. Our results reveal shortcomings of state-of-the-art denoising\narchitectures, such as their small field-of-view: substantially increasing the\nfield-of-view of the CNNs allows them to exploit non-local periodic patterns in\nthe data, which is crucial at high noise levels. In addition, we analyze the\ngeneralization capability of SBD, demonstrating that the trained networks are\nrobust to variations of imaging parameters and of the underlying signal\nstructure. Finally, we release the first publicly available benchmark dataset\nof TEM images, containing 18,000 examples.",
          "link": "http://arxiv.org/abs/2010.12970",
          "publishedOn": "2021-07-14T01:41:50.265Z",
          "wordCount": 750,
          "title": "Deep Denoising For Scientific Discovery: A Case Study In Electron Microscopy. (arXiv:2010.12970v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Huijie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wentao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yandong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>",
          "description": "Accurate labeling is essential for supervised deep learning methods. In this\npaper, to accurately segment images of multiple overlapping cervical cells with\ndeep learning models, we propose an automatic label correction algorithm to\nimprove the edge positioning accuracy of overlapping cervical cells in manual\nlabeling. Our algorithm is designed based on gradient guidance, and can\nautomatically correct edge positions for overlapping cervical cells and\ndifferences among manual labeling with different annotators. Using the proposed\nalgorithm, we constructed an open cervical cell edge detection dataset (CCEDD)\nwith high labeling accuracy. The experiments on the dataset for training show\nthat our automatic label correction algorithm can improve the accuracy of\nmanual labels and further improve the positioning accuracy of overlapping cells\nwith deep learning models. We have released the dataset and code at\nhttps://github.com/nachifur/automatic-label-correction-CCEDD.",
          "link": "http://arxiv.org/abs/2010.01919",
          "publishedOn": "2021-07-14T01:41:50.257Z",
          "wordCount": 623,
          "title": "Automatic Label Correction for the Accurate Edge Detection of Overlapping Cervical Cells. (arXiv:2010.01919v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weixiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boom_B/0/1/0/all/0/1\">Bas Boom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ledoux_H/0/1/0/all/0/1\">Hugo Ledoux</a>",
          "description": "Recent developments in data acquisition technology allow us to collect 3D\ntexture meshes quickly. Those can help us understand and analyse the urban\nenvironment, and as a consequence are useful for several applications like\nspatial analysis and urban planning. Semantic segmentation of texture meshes\nthrough deep learning methods can enhance this understanding, but it requires a\nlot of labelled data. The contributions of this work are threefold: (1) a new\nbenchmark dataset of semantic urban meshes, (2) a novel semi-automatic\nannotation framework, and (3) an annotation tool for 3D meshes. In particular,\nour dataset covers about 4 km2 in Helsinki (Finland), with six classes, and we\nestimate that we save about 600 hours of labelling work using our annotation\nframework, which includes initial segmentation and interactive refinement. We\nalso compare the performance of several state-of-theart 3D semantic\nsegmentation methods on the new benchmark dataset. Other researchers can use\nour results to train their networks: the dataset is publicly available, and the\nannotation tool is released as open-source.",
          "link": "http://arxiv.org/abs/2103.00355",
          "publishedOn": "2021-07-14T01:41:50.250Z",
          "wordCount": 640,
          "title": "SUM: A Benchmark Dataset of Semantic Urban Meshes. (arXiv:2103.00355v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yidong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>",
          "description": "Machine learning systems generally assume that the training and testing\ndistributions are the same. To this end, a key requirement is to develop models\nthat can generalize to unseen distributions. Domain generalization (DG), i.e.,\nout-of-distribution generalization, has attracted increasing interests in\nrecent years. Domain generalization deals with a challenging setting where one\nor several different but related domain(s) are given, and the goal is to learn\na model that can generalize to an unseen test domain. Great progress has been\nmade in the area of domain generalization for years. This paper presents the\nfirst review of recent advances in this area. First, we provide a formal\ndefinition of domain generalization and discuss several related fields. We then\nthoroughly review the theories related to domain generalization and carefully\nanalyze the theory behind generalization. We categorize recent algorithms into\nthree classes: data manipulation, representation learning, and learning\nstrategy, and present several popular algorithms in detail for each category.\nThird, we introduce the commonly used datasets and applications. Finally, we\nsummarize existing literature and present some potential research topics for\nthe future.",
          "link": "http://arxiv.org/abs/2103.03097",
          "publishedOn": "2021-07-14T01:41:50.230Z",
          "wordCount": 690,
          "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fei-Fei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaitanya_K/0/1/0/all/0/1\">Krishna Chaitanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengda Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1\">Benedikt Wiestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>",
          "description": "Radiomic representations can quantify properties of regions of interest in\nmedical image data. Classically, they account for pre-defined statistics of\nshape, texture, and other low-level image features. Alternatively, deep\nlearning-based representations are derived from supervised learning but require\nexpensive annotations from experts and often suffer from overfitting and data\nimbalance issues. In this work, we address the challenge of learning\nrepresentations of 3D medical images for an effective quantification under data\nimbalance. We propose a \\emph{self-supervised} representation learning\nframework to learn high-level features of 3D volumes as a complement to\nexisting radiomics features. Specifically, we demonstrate how to learn image\nrepresentations in a self-supervised fashion using a 3D Siamese network. More\nimportantly, we deal with data imbalance by exploiting two unsupervised\nstrategies: a) sample re-weighting, and b) balancing the composition of\ntraining batches. When combining our learned self-supervised feature with\ntraditional radiomics, we show significant improvement in brain tumor\nclassification and lung cancer staging tasks covering MRI and CT imaging\nmodalities.",
          "link": "http://arxiv.org/abs/2103.04167",
          "publishedOn": "2021-07-14T01:41:50.223Z",
          "wordCount": 644,
          "title": "Imbalance-Aware Self-Supervised Learning for 3D Radiomic Representations. (arXiv:2103.04167v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1\">Maxim Rakhuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We introduce T-Basis, a novel concept for a compact representation of a set\nof tensors, each of an arbitrary shape, which is often seen in Neural Networks.\nEach of the tensors in the set is modeled using Tensor Rings, though the\nconcept applies to other Tensor Networks. Owing its name to the T-shape of\nnodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally\nshaped three-dimensional tensors, used to represent Tensor Ring nodes. Such\nrepresentation allows us to parameterize the tensor set with a small number of\nparameters (coefficients of the T-Basis tensors), scaling logarithmically with\neach tensor's size in the set and linearly with the dimensionality of T-Basis.\nWe evaluate the proposed approach on the task of neural network compression and\ndemonstrate that it reaches high compression rates at acceptable performance\ndrops. Finally, we analyze memory and operation requirements of the compressed\nnetworks and conclude that T-Basis networks are equally well suited for\ntraining and inference in resource-constrained environments and usage on the\nedge devices.",
          "link": "http://arxiv.org/abs/2007.06631",
          "publishedOn": "2021-07-14T01:41:50.214Z",
          "wordCount": 650,
          "title": "T-Basis: a Compact Representation for Neural Networks. (arXiv:2007.06631v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hesen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "Data augmentation is a commonly used approach to improving the generalization\nof deep learning models. Recent works show that learned data augmentation\npolicies can achieve better generalization than hand-crafted ones. However,\nmost of these works use unified augmentation policies for all samples in a\ndataset, which is observed not necessarily beneficial for all labels in\nmulti-label classification tasks, i.e., some policies may have negative impacts\non some labels while benefitting the others. To tackle this problem, we propose\na novel Label-Based AutoAugmentation (LB-Aug) method for multi-label scenarios,\nwhere augmentation policies are generated with respect to labels by an\naugmentation-policy network. The policies are learned via reinforcement\nlearning using policy gradient methods, providing a mapping from instance\nlabels to their optimal augmentation policies. Numerical experiments show that\nour LB-Aug outperforms previous state-of-the-art augmentation methods by large\nmargins in multiple benchmarks on image and video classification.",
          "link": "http://arxiv.org/abs/2107.05384",
          "publishedOn": "2021-07-14T01:41:50.206Z",
          "wordCount": 598,
          "title": "Fine-Grained AutoAugmentation for Multi-Label Classification. (arXiv:2107.05384v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00138",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grodecki_K/0/1/0/all/0/1\">Kajetan Grodecki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Killekar_A/0/1/0/all/0/1\">Aditya Killekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1\">Andrew Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cadet_S/0/1/0/all/0/1\">Sebastien Cadet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McElhinney_P/0/1/0/all/0/1\">Priscilla McElhinney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razipour_A/0/1/0/all/0/1\">Aryabod Razipour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Cato Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pressman_B/0/1/0/all/0/1\">Barry D. Pressman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Julien_P/0/1/0/all/0/1\">Peter Julien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simon_J/0/1/0/all/0/1\">Judit Simon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maurovich_Horvat_P/0/1/0/all/0/1\">Pal Maurovich-Horvat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaibazzi_N/0/1/0/all/0/1\">Nicola Gaibazzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thakur_U/0/1/0/all/0/1\">Udit Thakur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mancini_E/0/1/0/all/0/1\">Elisabetta Mancini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agalbato_C/0/1/0/all/0/1\">Cecilia Agalbato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munechika_J/0/1/0/all/0/1\">Jiro Munechika</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matsumoto_H/0/1/0/all/0/1\">Hidenari Matsumoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mene_R/0/1/0/all/0/1\">Roberto Men&#xe8;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parati_G/0/1/0/all/0/1\">Gianfranco Parati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cernigliaro_F/0/1/0/all/0/1\">Franco Cernigliaro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nerlekar_N/0/1/0/all/0/1\">Nitesh Nerlekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torlasco_C/0/1/0/all/0/1\">Camilla Torlasco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pontone_G/0/1/0/all/0/1\">Gianluca Pontone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_D/0/1/0/all/0/1\">Damini Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slomka_P/0/1/0/all/0/1\">Piotr J. Slomka</a>",
          "description": "Quantitative lung measures derived from computed tomography (CT) have been\ndemonstrated to improve prognostication in coronavirus disease (COVID-19)\npatients, but are not part of the clinical routine since required manual\nsegmentation of lung lesions is prohibitively time-consuming. We propose a new\nfully automated deep learning framework for rapid quantification and\ndifferentiation between lung lesions in COVID-19 pneumonia from both contrast\nand non-contrast CT images using convolutional Long Short-Term Memory\n(ConvLSTM) networks. Utilizing the expert annotations, model training was\nperformed 5 times with separate hold-out sets using 5-fold cross-validation to\nsegment ground-glass opacity and high opacity (including consolidation and\npleural effusion). The performance of the method was evaluated on CT data sets\nfrom 197 patients with positive reverse transcription polymerase chain reaction\ntest result for SARS-CoV-2. Strong agreement between expert manual and\nautomatic segmentation was obtained for lung lesions with a Dice score\ncoefficient of 0.876 $\\pm$ 0.005; excellent correlations of 0.978 and 0.981 for\nground-glass opacity and high opacity volumes. In the external validation set\nof 67 patients, there was dice score coefficient of 0.767 $\\pm$ 0.009 as well\nas excellent correlations of 0.989 and 0.996 for ground-glass opacity and high\nopacity volumes. Computations for a CT scan comprising 120 slices were\nperformed under 2 seconds on a personal computer equipped with NVIDIA Titan RTX\ngraphics processing unit. Therefore, our deep learning-based method allows\nrapid fully-automated quantitative measurement of pneumonia burden from CT and\nmay generate results with an accuracy similar to the expert readers.",
          "link": "http://arxiv.org/abs/2104.00138",
          "publishedOn": "2021-07-14T01:41:50.199Z",
          "wordCount": 824,
          "title": "Rapid quantification of COVID-19 pneumonia burden from computed tomography with convolutional LSTM networks. (arXiv:2104.00138v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.07936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bozkir_E/0/1/0/all/0/1\">Efe Bozkir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1\">Ali Burak &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1\">Mete Akg&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1\">Nico Pfeifer</a>",
          "description": "Eye tracking is handled as one of the key technologies for applications that\nassess and evaluate human attention, behavior, and biometrics, especially using\ngaze, pupillary, and blink behaviors. One of the challenges with regard to the\nsocial acceptance of eye tracking technology is however the preserving of\nsensitive and personal information. To tackle this challenge, we employ a\nprivacy-preserving framework based on randomized encoding to train a Support\nVector Regression model using synthetic eye images privately to estimate the\nhuman gaze. During the computation, none of the parties learn about the data or\nthe result that any other party has. Furthermore, the party that trains the\nmodel cannot reconstruct pupil, blinks or visual scanpath. The experimental\nresults show that our privacy-preserving framework is capable of working in\nreal-time, with the same accuracy as compared to non-private version and could\nbe extended to other eye tracking related problems.",
          "link": "http://arxiv.org/abs/1911.07936",
          "publishedOn": "2021-07-14T01:41:50.178Z",
          "wordCount": 688,
          "title": "Privacy Preserving Gaze Estimation using Synthetic Images via a Randomized Encoding Based Framework. (arXiv:1911.07936v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+QingsongYao/0/1/0/all/0/1\">QingsongYao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S.kevin Zhou</a>",
          "description": "Detecting anatomical landmarks in medical images plays an essential role in\nunderstanding the anatomy and planning automated processing. In recent years, a\nvariety of deep neural network methods have been developed to detect landmarks\nautomatically. However, all of those methods are unary in the sense that a\nhighly specialized network is trained for a single task say associated with a\nparticular anatomical region. In this work, for the first time, we investigate\nthe idea of ``You Only Learn Once (YOLO)'' and develop a universal anatomical\nlandmark detection model to realize multiple landmark detection tasks with\nend-to-end training based on mixed datasets. The model consists of a local\nnetwork and a global network: The local network is built upon the idea of\nuniversal U-Net to learn multi-domain local features and the global network is\na parallelly-duplicated sequential of dilated convolutions that extract global\nfeatures to further disambiguate the landmark locations. It is worth mentioning\nthat the new model design requires much fewer parameters than models with\nstandard convolutions to train. We evaluate our YOLO model on three X-ray\ndatasets of 1,588 images on the head, hand, and chest, collectively\ncontributing 62 landmarks. The experimental results show that our proposed\nuniversal model behaves largely better than any previous models trained on\nmultiple datasets. It even beats the performance of the model that is trained\nseparately for every single dataset.",
          "link": "http://arxiv.org/abs/2103.04657",
          "publishedOn": "2021-07-14T01:41:50.170Z",
          "wordCount": 698,
          "title": "You Only Learn Once: Universal Anatomical Landmark Detection. (arXiv:2103.04657v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiluan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philion_J/0/1/0/all/0/1\">Jonah Philion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>",
          "description": "A high-performing object detection system plays a crucial role in autonomous\ndriving (AD). The performance, typically evaluated in terms of mean Average\nPrecision, does not take into account orientation and distance of the actors in\nthe scene, which are important for the safe AD. It also ignores environmental\ncontext. Recently, Philion et al. proposed a neural planning metric (PKL),\nbased on the KL divergence of a planner's trajectory and the groundtruth route,\nto accommodate these requirements. In this paper, we use this neural planning\nmetric to score all submissions of the nuScenes detection challenge and analyze\nthe results. We find that while somewhat correlated with mAP, the PKL metric\nshows different behavior to increased traffic density, ego velocity, road\ncurvature and intersections. Finally, we propose ideas to extend the neural\nplanning metric.",
          "link": "http://arxiv.org/abs/2010.09350",
          "publishedOn": "2021-07-14T01:41:50.160Z",
          "wordCount": 630,
          "title": "The efficacy of Neural Planning Metrics: A meta-analysis of PKL on nuScenes. (arXiv:2010.09350v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06211",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zaifeng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_T/0/1/0/all/0/1\">Tsz Nam Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chau_L/0/1/0/all/0/1\">Lap-Pui Chau</a>",
          "description": "High Dynamic Range (HDR) imaging via multi-exposure fusion is an important\ntask for most modern imaging platforms. In spite of recent developments in both\nhardware and algorithm innovations, challenges remain over content association\nambiguities caused by saturation, motion, and various artifacts introduced\nduring multi-exposure fusion such as ghosting, noise, and blur. In this work,\nwe propose an Attention-guided Progressive Neural Texture Fusion (APNT-Fusion)\nHDR restoration model which aims to address these issues within one framework.\nAn efficient two-stream structure is proposed which separately focuses on\ntexture feature transfer over saturated regions and multi-exposure tonal and\ntexture feature fusion. A neural feature transfer mechanism is proposed which\nestablishes spatial correspondence between different exposures based on\nmulti-scale VGG features in the masked saturated HDR domain for discriminative\ncontextual clues over the ambiguous image areas. A progressive texture blending\nmodule is designed to blend the encoded two-stream features in a multi-scale\nand progressive manner. In addition, we introduce several novel attention\nmechanisms, i.e., the motion attention module detects and suppresses the\ncontent discrepancies among the reference images; the saturation attention\nmodule facilitates differentiating the misalignment caused by saturation from\nthose caused by motion; and the scale attention module ensures texture blending\nconsistency between different coder/decoder scales. We carry out comprehensive\nqualitative and quantitative evaluations and ablation studies, which validate\nthat these novel modules work coherently under the same framework and\noutperform state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.06211",
          "publishedOn": "2021-07-14T01:41:50.153Z",
          "wordCount": 686,
          "title": "Attention-Guided Progressive Neural Texture Fusion for High Dynamic Range Image Restoration. (arXiv:2107.06211v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fain_M/0/1/0/all/0/1\">Mikhail Fain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1\">Niall Twomey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponikar_A/0/1/0/all/0/1\">Andrey Ponikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_R/0/1/0/all/0/1\">Ryan Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>",
          "description": "We propose a novel non-parametric method for cross-modal recipe retrieval\nwhich is applied on top of precomputed image and text embeddings. By combining\nour method with standard approaches for building image and text encoders,\ntrained independently with a self-supervised classification objective, we\ncreate a baseline model which outperforms most existing methods on a\nchallenging image-to-recipe task. We also use our method for comparing image\nand text encoders trained using different modern approaches, thus addressing\nthe issues hindering the development of novel methods for cross-modal recipe\nretrieval. We demonstrate how to use the insights from model comparison and\nextend our baseline model with standard triplet loss that improves\nstate-of-the-art on the Recipe1M dataset by a large margin, while using only\nprecomputed features and with much less complexity than existing methods.\nFurther, our approach readily generalizes beyond recipe retrieval to other\nchallenging domains, achieving state-of-the-art performance on Politics and\nGoodNews cross-modal retrieval tasks.",
          "link": "http://arxiv.org/abs/1911.12763",
          "publishedOn": "2021-07-14T01:41:50.146Z",
          "wordCount": 623,
          "title": "Dividing and Conquering Cross-Modal Recipe Retrieval: from Nearest Neighbours Baselines to SoTA. (arXiv:1911.12763v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bowen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>",
          "description": "Modern approaches typically formulate semantic segmentation as a per-pixel\nclassification task, while instance-level segmentation is handled with an\nalternative mask classification. Our key insight: mask classification is\nsufficiently general to solve both semantic- and instance-level segmentation\ntasks in a unified manner using the exact same model, loss, and training\nprocedure. Following this observation, we propose MaskFormer, a simple mask\nclassification model which predicts a set of binary masks, each associated with\na single global class label prediction. Overall, the proposed mask\nclassification-based method simplifies the landscape of effective approaches to\nsemantic and panoptic segmentation tasks and shows excellent empirical results.\nIn particular, we observe that MaskFormer outperforms per-pixel classification\nbaselines when the number of classes is large. Our mask classification-based\nmethod outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K)\nand panoptic segmentation (52.7 PQ on COCO) models.",
          "link": "http://arxiv.org/abs/2107.06278",
          "publishedOn": "2021-07-14T01:41:50.128Z",
          "wordCount": 583,
          "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation. (arXiv:2107.06278v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pilato_A/0/1/0/all/0/1\">Antonio Di Pilato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taggio_N/0/1/0/all/0/1\">Nicol&#xf2; Taggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pompili_A/0/1/0/all/0/1\">Alexis Pompili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobellis_M/0/1/0/all/0/1\">Michele Iacobellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florio_A/0/1/0/all/0/1\">Adriano Di Florio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passarelli_D/0/1/0/all/0/1\">Davide Passarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samarelli_S/0/1/0/all/0/1\">Sergio Samarelli</a>",
          "description": "The interest for change detection in the field of remote sensing has\nincreased in the last few years. Searching for changes in satellite images has\nmany useful applications, ranging from land cover and land use analysis to\nanomaly detection. In particular, urban change detection provides an efficient\ntool to study urban spread and growth through several years of observation. At\nthe same time, change detection is often a computationally challenging and\ntime-consuming task, which requires innovative methods to guarantee optimal\nresults with unquestionable value and within reasonable time. In this paper we\npresent two different approaches to change detection (semantic segmentation and\nclassification) that both exploit convolutional neural networks to achieve good\nresults, which can be further refined and used in a post-processing workflow\nfor a large variety of applications.",
          "link": "http://arxiv.org/abs/2107.06132",
          "publishedOn": "2021-07-14T01:41:50.121Z",
          "wordCount": 578,
          "title": "Deep learning approaches to Earth Observation change detection. (arXiv:2107.06132v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piva_F/0/1/0/all/0/1\">Fabrizio J. Piva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1\">Gijs Dubbelman</a>",
          "description": "We introduce an unsupervised domain adaption (UDA) strategy that combines\nmultiple image translations, ensemble learning and self-supervised learning in\none coherent approach. We focus on one of the standard tasks of UDA in which a\nsemantic segmentation model is trained on labeled synthetic data together with\nunlabeled real-world data, aiming to perform well on the latter. To exploit the\nadvantage of using multiple image translations, we propose an ensemble learning\napproach, where three classifiers calculate their prediction by taking as input\nfeatures of different image translations, making each classifier learn\nindependently, with the purpose of combining their outputs by sparse\nMultinomial Logistic Regression. This regression layer known as meta-learner\nhelps to reduce the bias during pseudo label generation when performing\nself-supervised learning and improves the generalizability of the model by\ntaking into consideration the contribution of each classifier. We evaluate our\nmethod on the standard UDA benchmarks, i.e. adapting GTA V and Synthia to\nCityscapes, and achieve state-of-the-art results in the mean intersection over\nunion metric. Extensive ablation experiments are reported to highlight the\nadvantageous properties of our proposed UDA strategy.",
          "link": "http://arxiv.org/abs/2107.06235",
          "publishedOn": "2021-07-14T01:41:50.113Z",
          "wordCount": 632,
          "title": "Exploiting Image Translations via Ensemble Self-Supervised Learning for Unsupervised Domain Adaptation. (arXiv:2107.06235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Vision transformers have been successfully applied to image recognition tasks\ndue to their ability to capture long-range dependencies within an image.\nHowever, there are still gaps in both performance and computational cost\nbetween transformers and existing convolutional neural networks (CNNs). In this\npaper, we aim to address this issue and develop a network that can outperform\nnot only the canonical transformers, but also the high-performance\nconvolutional models. We propose a new transformer based hybrid network by\ntaking advantage of transformers to capture long-range dependencies, and of\nCNNs to model local features. Furthermore, we scale it to obtain a family of\nmodels, called CMTs, obtaining much better accuracy and efficiency than\nprevious convolution and transformer based models. In particular, our CMT-S\nachieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on\nFLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S\nalso generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%),\nand other challenging vision datasets such as COCO (44.3% mAP), with\nconsiderably less computational cost.",
          "link": "http://arxiv.org/abs/2107.06263",
          "publishedOn": "2021-07-14T01:41:50.107Z",
          "wordCount": 609,
          "title": "CMT: Convolutional Neural Networks Meet Vision Transformers. (arXiv:2107.06263v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoxin Liu</a>",
          "description": "Domain generalization (DG) aims to help models trained on a set of source\ndomains generalize better on unseen target domains. The performances of current\nDG methods largely rely on sufficient labeled data, which however are usually\ncostly or unavailable. While unlabeled data are far more accessible, we seek to\nexplore how unsupervised learning can help deep models generalizes across\ndomains. Specifically, we study a novel generalization problem called\nunsupervised domain generalization, which aims to learn generalizable models\nwith unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised\nLearning (DIUL) method to cope with the significant and misleading\nheterogeneity within unlabeled data and severe distribution shifts between\nsource and target data. Surprisingly we observe that DIUL can not only\ncounterbalance the scarcity of labeled data but also further strengthen the\ngeneralization ability of models when the labeled data are sufficient. As a\npretraining approach, DIUL shows superior to ImageNet pretraining protocol even\nwhen the available data are unlabeled and of a greatly smaller amount compared\nto ImageNet. Extensive experiments clearly demonstrate the effectiveness of our\nmethod compared with state-of-the-art unsupervised learning counterparts.",
          "link": "http://arxiv.org/abs/2107.06219",
          "publishedOn": "2021-07-14T01:41:50.100Z",
          "wordCount": 625,
          "title": "Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization. (arXiv:2107.06219v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.13751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "Recent works on plug-and-play image restoration have shown that a denoiser\ncan implicitly serve as the image prior for model-based methods to solve many\ninverse problems. Such a property induces considerable advantages for\nplug-and-play image restoration (e.g., integrating the flexibility of\nmodel-based method and effectiveness of learning-based methods) when the\ndenoiser is discriminatively learned via deep convolutional neural network\n(CNN) with large modeling capacity. However, while deeper and larger CNN models\nare rapidly gaining popularity, existing plug-and-play image restoration\nhinders its performance due to the lack of suitable denoiser prior. In order to\npush the limits of plug-and-play image restoration, we set up a benchmark deep\ndenoiser prior by training a highly flexible and effective CNN denoiser. We\nthen plug the deep denoiser prior as a modular part into a half quadratic\nsplitting based iterative algorithm to solve various image restoration\nproblems. We, meanwhile, provide a thorough analysis of parameter setting,\nintermediate results and empirical convergence to better understand the working\nmechanism. Experimental results on three representative image restoration\ntasks, including deblurring, super-resolution and demosaicing, demonstrate that\nthe proposed plug-and-play image restoration with deep denoiser prior not only\nsignificantly outperforms other state-of-the-art model-based methods but also\nachieves competitive or even superior performance against state-of-the-art\nlearning-based methods. The source code is available at\nhttps://github.com/cszn/DPIR.",
          "link": "http://arxiv.org/abs/2008.13751",
          "publishedOn": "2021-07-14T01:41:50.080Z",
          "wordCount": 692,
          "title": "Plug-and-Play Image Restoration with Deep Denoiser Prior. (arXiv:2008.13751v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shihua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Cheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Changxiao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "Despite the remarkable successes of Convolutional Neural Networks (CNNs) in\ncomputer vision, it is time-consuming and error-prone to manually design a CNN.\nAmong various Neural Architecture Search (NAS) methods that are motivated to\nautomate designs of high-performance CNNs, the differentiable NAS and\npopulation-based NAS are attracting increasing interests due to their unique\ncharacters. To benefit from the merits while overcoming the deficiencies of\nboth, this work proposes a novel NAS method, RelativeNAS. As the key to\nefficient search, RelativeNAS performs joint learning between fast-learners\n(i.e. networks with relatively higher accuracy) and slow-learners in a pairwise\nmanner. Moreover, since RelativeNAS only requires low-fidelity performance\nestimation to distinguish each pair of fast-learner and slow-learner, it saves\ncertain computation costs for training the candidate architectures. The\nproposed RelativeNAS brings several unique advantages: (1) it achieves\nstate-of-the-art performance on ImageNet with top-1 error rate of 24.88%, i.e.\noutperforming DARTS and AmoebaNet-B by 1.82% and 1.12% respectively; (2) it\nspends only nine hours with a single 1080Ti GPU to obtain the discovered cells,\ni.e. 3.75x and 7875x faster than DARTS and AmoebaNet respectively; (3) it\nprovides that the discovered cells obtained on CIFAR-10 can be directly\ntransferred to object detection, semantic segmentation, and keypoint detection,\nyielding competitive results of 73.1% mAP on PASCAL VOC, 78.7% mIoU on\nCityscapes, and 68.5% AP on MSCOCO, respectively. The implementation of\nRelativeNAS is available at https://github.com/EMI-Group/RelativeNAS",
          "link": "http://arxiv.org/abs/2009.06193",
          "publishedOn": "2021-07-14T01:41:50.073Z",
          "wordCount": 710,
          "title": "RelativeNAS: Relative Neural Architecture Search via Slow-Fast Learning. (arXiv:2009.06193v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Junbao Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Due to the domain discrepancy in visual domain adaptation, the performance of\nsource model degrades when bumping into the high data density near decision\nboundary in target domain. A common solution is to minimize the Shannon Entropy\nto push the decision boundary away from the high density area. However, entropy\nminimization also leads to severe reduction of prediction diversity, and\nunfortunately brings harm to the domain adaptation. In this paper, we\ninvestigate the prediction discriminability and diversity by studying the\nstructure of the classification output matrix of a randomly selected data\nbatch. We find by theoretical analysis that the prediction discriminability and\ndiversity could be separately measured by the Frobenius-norm and rank of the\nbatch output matrix. The nuclear-norm is an upperbound of the former, and a\nconvex approximation of the latter. Accordingly, we propose Batch Nuclear-norm\nMaximization and Minimization, which performs nuclear-norm maximization on the\ntarget output matrix to enhance the target prediction ability, and nuclear-norm\nminimization on the source batch output matrix to increase applicability of the\nsource domain knowledge. We further approximate the nuclear-norm by\nL_{1,2}-norm, and design multi-batch optimization for stable solution on large\nnumber of categories. The fast approximation method achieves O(n^2)\ncomputational complexity and better convergence property. Experiments show that\nour method could boost the adaptation accuracy and robustness under three\ntypical domain adaptation scenarios. The code is available at\nhttps://github.com/cuishuhao/BNM.",
          "link": "http://arxiv.org/abs/2107.06154",
          "publishedOn": "2021-07-14T01:41:50.066Z",
          "wordCount": 683,
          "title": "Fast Batch Nuclear-norm Maximization and Minimization for Robust Domain Adaptation. (arXiv:2107.06154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Mai Lan Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1\">Volker Blanz</a>",
          "description": "We propose a simple modification from a fixed margin triplet loss to an\nadaptive margin triplet loss. While the original triplet loss is used widely in\nclassification problems such as face recognition, face re-identification and\nfine-grained similarity, our proposed loss is well suited for rating datasets\nin which the ratings are continuous values. In contrast to original triplet\nloss where we have to sample data carefully, in out method, we can generate\ntriplets using the whole dataset, and the optimization can still converge\nwithout frequently running into a model collapsing issue. The adaptive margins\nonly need to be computed once before the training, which is much less expensive\nthan generating triplets after every epoch as in the fixed margin case. Besides\nsubstantially improved training stability (the proposed model never collapsed\nin our experiments compared to a couple of times that the training collapsed on\nexisting triplet loss), we achieved slightly better performance than the\noriginal triplet loss on various rating datasets and network architectures.",
          "link": "http://arxiv.org/abs/2107.06187",
          "publishedOn": "2021-07-14T01:41:50.059Z",
          "wordCount": 599,
          "title": "Deep Ranking with Adaptive Margin Triplet Loss. (arXiv:2107.06187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1\">Syed Muhammad Arsalan Bashir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mahrukh Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yilong Niu</a>",
          "description": "Image super-resolution (SR) is one of the vital image processing methods that\nimprove the resolution of an image in the field of computer vision. In the last\ntwo decades, significant progress has been made in the field of\nsuper-resolution, especially by utilizing deep learning methods. This survey is\nan effort to provide a detailed survey of recent progress in single-image\nsuper-resolution in the perspective of deep learning while also informing about\nthe initial classical methods used for image super-resolution. The survey\nclassifies the image SR methods into four categories, i.e., classical methods,\nsupervised learning-based methods, unsupervised learning-based methods, and\ndomain-specific SR methods. We also introduce the problem of SR to provide\nintuition about image quality metrics, available reference datasets, and SR\nchallenges. Deep learning-based approaches of SR are evaluated using a\nreference dataset. Some of the reviewed state-of-the-art image SR methods\ninclude the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN),\nmultiscale residual network (MSRN), meta residual dense network (Meta-RDN),\nrecurrent back-projection network (RBPN), second-order attention network (SAN),\nSR feedback network (SRFBN) and the wavelet-based residual attention network\n(WRAN). Finally, this survey is concluded with future directions and trends in\nSR and open problems in SR to be addressed by the researchers.",
          "link": "http://arxiv.org/abs/2102.09351",
          "publishedOn": "2021-07-14T01:41:50.052Z",
          "wordCount": 699,
          "title": "A Comprehensive Review of Deep Learning-based Single Image Super-resolution. (arXiv:2102.09351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06179",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mojrian_S/0/1/0/all/0/1\">Sanaz Mojrian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nodehi_I/0/1/0/all/0/1\">Issa Nodehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mashmool_A/0/1/0/all/0/1\">Amir Mashmool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zadegan_Z/0/1/0/all/0/1\">Zeynab Kiani Zadegan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirkharkolaie_S/0/1/0/all/0/1\">Sahar Khanjani Shirkharkolaie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tamadon_T/0/1/0/all/0/1\">Tahereh Tamadon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_S/0/1/0/all/0/1\">Samiyeh Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akbari_M/0/1/0/all/0/1\">Mitra Akbari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassannataj_E/0/1/0/all/0/1\">Edris Hassannataj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>",
          "description": "Myocardial infarction disease (MID) is caused to the rapid progress of\nundiagnosed coronary artery disease (CAD) that indicates the injury of a heart\ncell by decreasing the blood flow to the cardiac muscles. MID is the leading\ncause of death in middle-aged and elderly subjects all over the world. In\ngeneral, raw Electrocardiogram (ECG) signals are tested for MID identification\nby clinicians that is exhausting, time-consuming, and expensive. Artificial\nintelligence-based methods are proposed to handle the problems to diagnose MID\non the ECG signals automatically. Hence, in this survey paper, artificial\nintelligence-based methods, including machine learning and deep learning, are\nreview for MID diagnosis on the ECG signals. Using the methods demonstrate that\nthe feature extraction and selection of ECG signals required to be handcrafted\nin the ML methods. In contrast, these tasks are explored automatically in the\nDL methods. Based on our best knowledge, Deep Convolutional Neural Network\n(DCNN) methods are highly required methods developed for the early diagnosis of\nMID on the ECG signals. Most researchers have tended to use DCNN methods, and\nno studies have surveyed using artificial intelligence methods for MID\ndiagnosis on the ECG signals.",
          "link": "http://arxiv.org/abs/2107.06179",
          "publishedOn": "2021-07-14T01:41:50.033Z",
          "wordCount": 664,
          "title": "A Survey of Applications of Artificial Intelligence for Myocardial Infarction Disease Diagnosis. (arXiv:2107.06179v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>",
          "description": "We present MaX-DeepLab, the first end-to-end model for panoptic segmentation.\nOur approach simplifies the current pipeline that depends heavily on surrogate\nsub-tasks and hand-designed components, such as box detection, non-maximum\nsuppression, thing-stuff merging, etc. Although these sub-tasks are tackled by\narea experts, they fail to comprehensively solve the target task. By contrast,\nour MaX-DeepLab directly predicts class-labeled masks with a mask transformer,\nand is trained with a panoptic quality inspired loss via bipartite matching.\nOur mask transformer employs a dual-path architecture that introduces a global\nmemory path in addition to a CNN path, allowing direct communication with any\nCNN layers. As a result, MaX-DeepLab shows a significant 7.1% PQ gain in the\nbox-free regime on the challenging COCO dataset, closing the gap between\nbox-based and box-free methods for the first time. A small variant of\nMaX-DeepLab improves 3.0% PQ over DETR with similar parameters and M-Adds.\nFurthermore, MaX-DeepLab, without test time augmentation, achieves new\nstate-of-the-art 51.3% PQ on COCO test-dev set. Code is available at\nhttps://github.com/google-research/deeplab2.",
          "link": "http://arxiv.org/abs/2012.00759",
          "publishedOn": "2021-07-14T01:41:50.026Z",
          "wordCount": 647,
          "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. (arXiv:2012.00759v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05990",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolf_T/0/1/0/all/0/1\">Tom Nuno Wolf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>",
          "description": "Prior work on diagnosing Alzheimer's disease from magnetic resonance images\nof the brain established that convolutional neural networks (CNNs) can leverage\nthe high-dimensional image information for classifying patients. However,\nlittle research focused on how these models can utilize the usually\nlow-dimensional tabular information, such as patient demographics or laboratory\nmeasurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a\ngeneral-purpose module for CNNs that dynamically rescales and shifts the\nfeature maps of a convolutional layer, conditional on a patient's tabular\nclinical information. We show that DAFT is highly effective in combining 3D\nimage and tabular information for diagnosis and time-to-dementia prediction,\nwhere it outperforms competing CNNs with a mean balanced accuracy of 0.622 and\nmean c-index of 0.748, respectively. Our extensive ablation study provides\nvaluable insights into the architectural properties of DAFT. Our implementation\nis available at https://github.com/ai-med/DAFT.",
          "link": "http://arxiv.org/abs/2107.05990",
          "publishedOn": "2021-07-14T01:41:50.018Z",
          "wordCount": 614,
          "title": "Combining 3D Image and Tabular Data via the Dynamic Affine Feature Map Transform. (arXiv:2107.05990v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1\">Pierre Marza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1\">Laetitia Matignon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1\">Olivier Simonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>",
          "description": "In the context of visual navigation, the capacity to map a novel environment\nis necessary for an agent to exploit its observation history in the considered\nplace and efficiently reach known goals. This ability can be associated with\nspatial reasoning, where an agent is able to perceive spatial relationships and\nregularities, and discover object affordances. In classical Reinforcement\nLearning (RL) setups, this capacity is learned from reward alone. We introduce\nsupplementary supervision in the form of auxiliary tasks designed to favor the\nemergence of spatial perception capabilities in agents trained for a\ngoal-reaching downstream objective. We show that learning to estimate metrics\nquantifying the spatial relationships between an agent at a given location and\na goal to reach has a high positive impact in Multi-Object Navigation settings.\nOur method significantly improves the performance of different baseline agents,\nthat either build an explicit or implicit representation of the environment,\neven matching the performance of incomparable oracle agents taking ground-truth\nmaps as input.",
          "link": "http://arxiv.org/abs/2107.06011",
          "publishedOn": "2021-07-14T01:41:50.011Z",
          "wordCount": 608,
          "title": "Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jinqing Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>",
          "description": "Recently, with the advance of deep Convolutional Neural Networks (CNNs),\nperson Re-Identification (Re-ID) has witnessed great success in various\napplications. However, with limited receptive fields of CNNs, it is still\nchallenging to extract discriminative representations in a global view for\npersons under non-overlapped cameras. Meanwhile, Transformers demonstrate\nstrong abilities of modeling long-range dependencies for spatial and sequential\ndata. In this work, we take advantages of both CNNs and Transformers, and\npropose a novel learning framework named Hierarchical Aggregation Transformer\n(HAT) for image-based person Re-ID with high performance. To achieve this goal,\nwe first propose a Deeply Supervised Aggregation (DSA) to recurrently aggregate\nhierarchical features from CNN backbones. With multi-granularity supervisions,\nthe DSA can enhance multi-scale features for person retrieval, which is very\ndifferent from previous methods. Then, we introduce a Transformer-based Feature\nCalibration (TFC) to integrate low-level detail information as the global prior\nfor high-level semantic information. The proposed TFC is inserted to each level\nof hierarchical features, resulting in great performance improvements. To our\nbest knowledge, this work is the first to take advantages of both CNNs and\nTransformers for image-based person Re-ID. Comprehensive experiments on four\nlarge-scale Re-ID benchmarks demonstrate that our method shows better results\nthan several state-of-the-art methods. The code is released at\nhttps://github.com/AI-Zhpp/HAT.",
          "link": "http://arxiv.org/abs/2107.05946",
          "publishedOn": "2021-07-14T01:41:50.004Z",
          "wordCount": 685,
          "title": "HAT: Hierarchical Aggregation Transformers for Person Re-identification. (arXiv:2107.05946v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1\">Tao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhouhui Lian</a>",
          "description": "Arbitrary-shaped text detection has recently attracted increasing interests\nand witnessed rapid development with the popularity of deep learning\nalgorithms. Nevertheless, existing approaches often obtain inaccurate detection\nresults, mainly due to the relatively weak ability to utilize context\ninformation and the inappropriate choice of offset references. This paper\npresents a novel text instance expression which integrates both foreground and\nbackground information into the pipeline, and naturally uses the pixels near\ntext boundaries as the offset starts. Besides, a corresponding post-processing\nalgorithm is also designed to sequentially combine the four prediction results\nand reconstruct the text instance accurately. We evaluate our method on several\nchallenging scene text benchmarks, including both curved and multi-oriented\ntext datasets. Experimental results demonstrate that the proposed approach\nobtains superior or competitive performance compared to other state-of-the-art\nmethods, e.g., 83.4% F-score for Total-Text, 82.4% F-score for MSRA-TD500, etc.",
          "link": "http://arxiv.org/abs/2107.06129",
          "publishedOn": "2021-07-14T01:41:49.985Z",
          "wordCount": 575,
          "title": "Bidirectional Regression for Arbitrary-Shaped Text Detection. (arXiv:2107.06129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goswami_S/0/1/0/all/0/1\">Suranjan Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member_I/0/1/0/all/0/1\">IEEE Student Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member_S/0/1/0/all/0/1\">Senior Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_B/0/1/0/all/0/1\">Bidyut B. Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fellow_L/0/1/0/all/0/1\">Life Fellow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>",
          "description": "Thermal Images profile the passive radiation of objects and capture them in\ngrayscale images. Such images have a very different distribution of data\ncompared to optical colored images. We present here a work that produces a\ngrayscale thermo-optical fused mask given a thermal input. This is a deep\nlearning based pioneering work since to the best of our knowledge, there exists\nno other work on thermal-optical grayscale fusion. Our method is also unique in\nthe sense that the deep learning method we are proposing here works on the\nDiscrete Wavelet Transform (DWT) domain instead of the gray level domain. As a\npart of this work, we also present a new and unique database for obtaining the\nregion of interest in thermal images based on an existing thermal visual paired\ndatabase, containing the Region of Interest on 5 different classes of data.\nFinally, we are proposing a simple low cost overhead statistical measure for\nidentifying the region of interest in the fused images, which we call as the\nRegion of Fusion (RoF). Experiments on the database show encouraging results in\nidentifying the region of interest in the fused images. We also show that they\ncan be processed better in the mixed form rather than with only thermal images.",
          "link": "http://arxiv.org/abs/2107.05942",
          "publishedOn": "2021-07-14T01:41:49.977Z",
          "wordCount": 658,
          "title": "A Novel Deep Learning Method for Thermal to Annotated Thermal-Optical Fused Images. (arXiv:2107.05942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wen-Sheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>",
          "description": "We present Retrieve in Style (RIS), an unsupervised framework for\nfine-grained facial feature transfer and retrieval on real images. Recent work\nshows that it is possible to learn a catalog that allows local semantic\ntransfers of facial features on generated images by capitalizing on the\ndisentanglement property of the StyleGAN latent space. RIS improves existing\nart on: 1) feature disentanglement and allows for challenging transfers (i.e.,\nhair and pose) that were not shown possible in SoTA methods. 2) eliminating the\nneed for per-image hyperparameter tuning, and for computing a catalog over a\nlarge batch of images. 3) enabling face retrieval using the proposed facial\nfeatures (e.g., eyes), and to our best knowledge, is the first work to retrieve\nface images at the fine-grained level. 4) robustness and natural application to\nreal images. Our qualitative and quantitative analyses show RIS achieves both\nhigh-fidelity feature transfers and accurate fine-grained retrievals on real\nimages. We discuss the responsible application of RIS.",
          "link": "http://arxiv.org/abs/2107.06256",
          "publishedOn": "2021-07-14T01:41:49.969Z",
          "wordCount": 607,
          "title": "Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval. (arXiv:2107.06256v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06276",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Suman_S/0/1/0/all/0/1\">Sudhir Suman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sakla_N/0/1/0/all/0/1\">Nicole Sakla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "With more than 60,000 deaths annually in the United States, Pulmonary\nEmbolism (PE) is among the most fatal cardiovascular diseases. It is caused by\nan artery blockage in the lung; confirming its presence is time-consuming and\nis prone to over-diagnosis. The utilization of automated PE detection systems\nis critical for diagnostic accuracy and efficiency. In this study we propose a\ntwo-stage attention-based CNN-LSTM network for predicting PE, its associated\ntype (chronic, acute) and corresponding location (leftsided, rightsided or\ncentral) on computed tomography (CT) examinations. We trained our model on the\nlargest available public Computed Tomography Pulmonary Angiogram PE dataset\n(RSNA-STR Pulmonary Embolism CT (RSPECT) Dataset, N=7279 CT studies) and tested\nit on an in-house curated dataset of N=106 studies. Our framework mirrors the\nradiologic diagnostic process via a multi-slice approach so that the accuracy\nand pathologic sequela of true pulmonary emboli may be meticulously assessed,\nenabling physicians to better appraise the morbidity of a PE when present. Our\nproposed method outperformed a baseline CNN classifier and a single-stage\nCNN-LSTM network, achieving an AUC of 0.95 on the test set for detecting the\npresence of PE in the study.",
          "link": "http://arxiv.org/abs/2107.06276",
          "publishedOn": "2021-07-14T01:41:49.962Z",
          "wordCount": 663,
          "title": "Attention based CNN-LSTM Network for Pulmonary Embolism Prediction on Chest Computed Tomography Pulmonary Angiograms. (arXiv:2107.06276v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qingyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargteil_A/0/1/0/all/0/1\">Adam Bargteil</a>",
          "description": "We explore computational approaches for visual guidance to aid in creating\naesthetically pleasing art and graphic design. Our work complements and builds\non previous work that developed models for how humans look at images. Our\napproach comprises three steps. First, we collected a dataset of art\nmasterpieces and labeled the visual fixations with state-of-art vision models.\nSecond, we clustered the visual guidance templates of the art masterpieces with\nunsupervised learning. Third, we developed a pipeline using generative\nadversarial networks to learn the principles of visual guidance and that can\nproduce aesthetically pleasing layouts. We show that the aesthetic visual\nguidance principles can be learned and integrated into a high-dimensional model\nand can be queried by the features of graphic elements. We evaluate our\napproach by generating layouts on various drawings and graphic designs.\nMoreover, our model considers the color and structure of graphic elements when\ngenerating layouts. Consequently, we believe our tool, which generates multiple\naesthetic layout options in seconds, can help artists create beautiful art and\ngraphic designs.",
          "link": "http://arxiv.org/abs/2107.06262",
          "publishedOn": "2021-07-14T01:41:49.954Z",
          "wordCount": 607,
          "title": "Learning Aesthetic Layouts via Visual Guidance. (arXiv:2107.06262v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1\">Shubham Dhayarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1\">V.K. Viekash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.",
          "link": "http://arxiv.org/abs/2107.06212",
          "publishedOn": "2021-07-14T01:41:49.934Z",
          "wordCount": 684,
          "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06028",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bauermeister_H/0/1/0/all/0/1\">Hartmut Bauermeister</a>, <a href=\"http://arxiv.org/find/math/1/au:+Laude_E/0/1/0/all/0/1\">Emanuel Laude</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mollenhoff_T/0/1/0/all/0/1\">Thomas M&#xf6;llenhoff</a>, <a href=\"http://arxiv.org/find/math/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>",
          "description": "Dual decomposition approaches in nonconvex optimization may suffer from a\nduality gap. This poses a challenge when applying them directly to nonconvex\nproblems such as MAP-inference in a Markov random field (MRF) with continuous\nstate spaces. To eliminate such gaps, this paper considers a reformulation of\nthe original nonconvex task in the space of measures. This infinite-dimensional\nreformulation is then approximated by a semi-infinite one, which is obtained\nvia a piecewise polynomial discretization in the dual. We provide a geometric\nintuition behind the primal problem induced by the dual discretization and draw\nconnections to optimization over moment spaces. In contrast to existing\ndiscretizations which suffer from a grid bias, we show that a piecewise\npolynomial discretization better preserves the continuous nature of our\nproblem. Invoking results from optimal transport theory and convex algebraic\ngeometry we reduce the semi-infinite program to a finite one and provide a\npractical implementation based on semidefinite programming. We show,\nexperimentally and in theory, that the approach successfully reduces the\nduality gap. To showcase the scalability of our approach, we apply it to the\nstereo matching problem between two images.",
          "link": "http://arxiv.org/abs/2107.06028",
          "publishedOn": "2021-07-14T01:41:49.925Z",
          "wordCount": 638,
          "title": "Lifting the Convex Conjugate in Lagrangian Relaxations: A Tractable Approach for Continuous Markov Random Fields. (arXiv:2107.06028v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sulzer_R/0/1/0/all/0/1\">Raphael Sulzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallet_B/0/1/0/all/0/1\">Bruno Vallet</a>",
          "description": "We introduce a novel learning-based, visibility-aware, surface reconstruction\nmethod for large-scale, defect-laden point clouds. Our approach can cope with\nthe scale and variety of point cloud defects encountered in real-life\nMulti-View Stereo (MVS) acquisitions. Our method relies on a 3D Delaunay\ntetrahedralization whose cells are classified as inside or outside the surface\nby a graph neural network and an energy model solvable with a graph cut. Our\nmodel, making use of both local geometric attributes and line-of-sight\nvisibility information, is able to learn a visibility model from a small amount\nof synthetic training data and generalizes to real-life acquisitions. Combining\nthe efficiency of deep learning methods and the scalability of energy based\nmodels, our approach outperforms both learning and non learning-based\nreconstruction algorithms on two publicly available reconstruction benchmarks.",
          "link": "http://arxiv.org/abs/2107.06130",
          "publishedOn": "2021-07-14T01:41:49.918Z",
          "wordCount": 593,
          "title": "Scalable Surface Reconstruction with Delaunay-Graph Neural Networks. (arXiv:2107.06130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05975",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gotkowski_K/0/1/0/all/0/1\">Karol Gotkowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bucher_A/0/1/0/all/0/1\">Andreas Bucher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischbach_R/0/1/0/all/0/1\">Ricarda Fischbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaltenborn_I/0/1/0/all/0/1\">Isabel Kaltenborn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Automatic segmentation of lung lesions in computer tomography has the\npotential to ease the burden of clinicians during the Covid-19 pandemic. Yet\npredictive deep learning models are not trusted in the clinical routine due to\nfailing silently in out-of-distribution (OOD) data. We propose a lightweight\nOOD detection method that exploits the Mahalanobis distance in the feature\nspace. The proposed approach can be seamlessly integrated into state-of-the-art\nsegmentation pipelines without requiring changes in model architecture or\ntraining procedure, and can therefore be used to assess the suitability of\npre-trained models to new data. We validate our method with a patch-based\nnnU-Net architecture trained with a multi-institutional dataset and find that\nit effectively detects samples that the model segments incorrectly.",
          "link": "http://arxiv.org/abs/2107.05975",
          "publishedOn": "2021-07-14T01:41:49.911Z",
          "wordCount": 617,
          "title": "Detecting when pre-trained nnU-Net models fail silently for Covid-19. (arXiv:2107.05975v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Daniel Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oort_C/0/1/0/all/0/1\">Colin Van Oort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1\">Jonathan Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wshah_S/0/1/0/all/0/1\">Safwan Wshah</a>",
          "description": "Geo-localizing static objects from street images is challenging but also very\nimportant for road asset mapping and autonomous driving. In this paper we\npresent a two-stage framework that detects and geolocalizes traffic signs from\nlow frame rate street videos. Our proposed system uses a modified version of\nRetinaNet (GPS-RetinaNet), which predicts a positional offset for each sign\nrelative to the camera, in addition to performing the standard classification\nand bounding box regression. Candidate sign detections from GPS-RetinaNet are\ncondensed into geolocalized signs by our custom tracker, which consists of a\nlearned metric network and a variant of the Hungarian Algorithm. Our metric\nnetwork estimates the similarity between pairs of detections, then the\nHungarian Algorithm matches detections across images using the similarity\nscores provided by the metric network. Our models were trained using an updated\nversion of the ARTS dataset, which contains 25,544 images and 47.589 sign\nannotations ~\\cite{arts}. The proposed dataset covers a diverse set of\nenvironments gathered from a broad selection of roads. Each annotaiton contains\na sign class label, its geospatial location, an assembly label, a side of road\nindicator, and unique identifiers that aid in the evaluation. This dataset will\nsupport future progress in the field, and the proposed system demonstrates how\nto take advantage of some of the unique characteristics of a realistic\ngeolocalization dataset.",
          "link": "http://arxiv.org/abs/2107.06257",
          "publishedOn": "2021-07-14T01:41:49.904Z",
          "wordCount": 677,
          "title": "Object Tracking and Geo-localization from Street Images. (arXiv:2107.06257v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigner_C/0/1/0/all/0/1\">Christina Aigner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>",
          "description": "Deep Neural Networks (DNNs) have an enormous potential to learn from complex\nbiomedical data. In particular, DNNs have been used to seamlessly fuse\nheterogeneous information from neuroanatomy, genetics, biomarkers, and\nneuropsychological tests for highly accurate Alzheimer's disease diagnosis. On\nthe other hand, their black-box nature is still a barrier for the adoption of\nsuch a system in the clinic, where interpretability is absolutely essential. We\npropose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for\nexplaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of\nthe neuroanatomy and tabular biomarkers. Our explanations are based on the\nShapley value, which is the unique method that satisfies all fundamental axioms\nfor local explanations previously established in the literature. Thus, SVEHNN\nhas many desirable characteristics that previous work on interpretability for\nmedical decision making is lacking. To avoid the exponential time complexity of\nthe Shapley value, we propose to transform a given DNN into a Lightweight\nProbabilistic Deep Network without re-training, thus achieving a complexity\nonly quadratic in the number of features. In our experiments on synthetic and\nreal data, we show that we can closely approximate the exact Shapley value with\na dramatically reduced runtime and can reveal the hidden knowledge the network\nhas learned from the data.",
          "link": "http://arxiv.org/abs/2107.05997",
          "publishedOn": "2021-07-14T01:41:49.897Z",
          "wordCount": 669,
          "title": "Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from Heterogeneous Data. (arXiv:2107.05997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06149",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haocheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiaxiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>",
          "description": "With the rapid development of data-driven techniques, data has played an\nessential role in various computer vision tasks. Many realistic and synthetic\ndatasets have been proposed to address different problems. However, there are\nlots of unresolved challenges: (1) the creation of dataset is usually a tedious\nprocess with manual annotations, (2) most datasets are only designed for a\nsingle specific task, (3) the modification or randomization of the 3D scene is\ndifficult, and (4) the release of commercial 3D data may encounter copyright\nissue.\n\nThis paper presents MINERVAS, a Massive INterior EnviRonments VirtuAl\nSynthesis system, to facilitate the 3D scene modification and the 2D image\nsynthesis for various vision tasks. In particular, we design a programmable\npipeline with Domain-Specific Language, allowing users to (1) select scenes\nfrom the commercial indoor scene database, (2) synthesize scenes for different\ntasks with customized rules, and (3) render various imagery data, such as\nvisual color, geometric structures, semantic label. Our system eases the\ndifficulty of customizing massive numbers of scenes for different tasks and\nrelieves users from manipulating fine-grained scene configurations by providing\nuser-controllable randomness using multi-level samplers. Most importantly, it\nempowers users to access commercial scene databases with millions of indoor\nscenes and protects the copyright of core data assets, e.g., 3D CAD models. We\ndemonstrate the validity and flexibility of our system by using our synthesized\ndata to improve the performance on different kinds of computer vision tasks.",
          "link": "http://arxiv.org/abs/2107.06149",
          "publishedOn": "2021-07-14T01:41:49.878Z",
          "wordCount": 686,
          "title": "MINERVAS: Massive INterior EnviRonments VirtuAl Synthesis. (arXiv:2107.06149v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lemkhenter_A/0/1/0/all/0/1\">Abdelhak Lemkhenter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1\">Adam Bielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_A/0/1/0/all/0/1\">Alp Eren Sari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "We introduce Kernel Density Discrimination GAN (KDD GAN), a novel method for\ngenerative adversarial learning. KDD GAN formulates the training as a\nlikelihood ratio optimization problem where the data distributions are written\nexplicitly via (local) Kernel Density Estimates (KDE). This is inspired by the\nrecent progress in contrastive learning and its relation to KDE. We define the\nKDEs directly in feature space and forgo the requirement of invertibility of\nthe kernel feature mappings. In our approach, features are no longer optimized\nfor linear separability, as in the original GAN formulation, but for the more\ngeneral discrimination of distributions in the feature space. We analyze the\ngradient of our loss with respect to the feature representation and show that\nit is better behaved than that of the original hinge loss. We perform\nexperiments with the proposed KDE-based loss, used either as a training loss or\na regularization term, on both CIFAR10 and scaled versions of ImageNet. We use\nBigGAN/SA-GAN as a backbone and baseline, since our focus is not to design the\narchitecture of the networks. We show a boost in the quality of generated\nsamples with respect to FID from 10% to 40% compared to the baseline. Code will\nbe made available.",
          "link": "http://arxiv.org/abs/2107.06197",
          "publishedOn": "2021-07-14T01:41:49.871Z",
          "wordCount": 638,
          "title": "Generative Adversarial Learning via Kernel Density Discrimination. (arXiv:2107.06197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandt_I/0/1/0/all/0/1\">Irma van den Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fok_F/0/1/0/all/0/1\">Floris Fok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulders_B/0/1/0/all/0/1\">Bas Mulders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1\">Joaquin Vanschoren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>",
          "description": "Transfer learning is a commonly used strategy for medical image\nclassification, especially via pretraining on source data and fine-tuning on\ntarget data. There is currently no consensus on how to choose appropriate\nsource data, and in the literature we can find both evidence of favoring large\nnatural image datasets such as ImageNet, and evidence of favoring more\nspecialized medical datasets. In this paper we perform a systematic study with\nnine source datasets with natural or medical images, and three target medical\ndatasets, all with 2D images. We find that ImageNet is the source leading to\nthe highest performances, but also that larger datasets are not necessarily\nbetter. We also study different definitions of data similarity. We show that\ncommon intuitions about similarity may be inaccurate, and therefore not\nsufficient to predict an appropriate source a priori. Finally, we discuss\nseveral steps needed for further research in this field, especially with regard\nto other types (for example 3D) medical images. Our experiments and pretrained\nmodels are available via \\url{https://www.github.com/vcheplygina/cats-scans}",
          "link": "http://arxiv.org/abs/2107.05940",
          "publishedOn": "2021-07-14T01:41:49.864Z",
          "wordCount": 627,
          "title": "Cats, not CAT scans: a study of dataset similarity in transfer learning for 2D medical image classification. (arXiv:2107.05940v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lanqing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>",
          "description": "Low-light image enhancement (LLIE) is a pervasive yet challenging problem,\nsince: 1) low-light measurements may vary due to different imaging conditions\nin practice; 2) images can be enlightened subjectively according to diverse\npreferences by each individual. To tackle these two challenges, this paper\npresents a novel deep reinforcement learning based method, dubbed ReLLIE, for\ncustomized low-light enhancement. ReLLIE models LLIE as a markov decision\nprocess, i.e., estimating the pixel-wise image-specific curves sequentially and\nrecurrently. Given the reward computed from a set of carefully crafted\nnon-reference loss functions, a lightweight network is proposed to estimate the\ncurves for enlightening of a low-light image input. As ReLLIE learns a policy\ninstead of one-one image translation, it can handle various low-light\nmeasurements and provide customized enhanced outputs by flexibly applying the\npolicy different times. Furthermore, ReLLIE can enhance real-world images with\nhybrid corruptions, e.g., noise, by using a plug-and-play denoiser easily.\nExtensive experiments on various benchmarks demonstrate the advantages of\nReLLIE, comparing to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.05830",
          "publishedOn": "2021-07-14T01:41:49.857Z",
          "wordCount": 607,
          "title": "ReLLIE: Deep Reinforcement Learning for Customized Low-Light Image Enhancement. (arXiv:2107.05830v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of obese human mesh recovery, i.e., fitting a\nparametric human mesh to images of obese people. Despite obese person mesh\nfitting being an important problem with numerous applications (e.g.,\nhealthcare), much recent progress in mesh recovery has been restricted to\nimages of non-obese people. In this work, we identify this crucial gap in the\ncurrent literature by presenting and discussing limitations of existing\nalgorithms. Next, we present a simple baseline to address this problem that is\nscalable and can be easily used in conjunction with existing algorithms to\nimprove their performance. Finally, we present a generalized human mesh\noptimization algorithm that substantially improves the performance of existing\nmethods on both obese person images as well as community-standard benchmark\ndatasets. A key innovation of this technique is that it does not rely on\nsupervision from expensive-to-create mesh parameters. Instead, starting from\nwidely and cheaply available 2D keypoints annotations, our method automatically\ngenerates mesh parameters that can in turn be used to re-train and fine-tune\nany existing mesh estimation algorithm. This way, we show our method acts as a\ndrop-in to improve the performance of a wide variety of contemporary mesh\nestimation methods. We conduct extensive experiments on multiple datasets\ncomprising both standard and obese person images and demonstrate the efficacy\nof our proposed techniques.",
          "link": "http://arxiv.org/abs/2107.06239",
          "publishedOn": "2021-07-14T01:41:49.839Z",
          "wordCount": 677,
          "title": "Everybody Is Unique: Towards Unbiased Human Mesh Recovery. (arXiv:2107.06239v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Mai Lan Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1\">Volker Blanz</a>",
          "description": "Discriminative features play an important role in image and object\nclassification and also in other fields of research such as semi-supervised\nlearning, fine-grained classification, out of distribution detection. Inspired\nby Linear Discriminant Analysis (LDA), we propose an optimization called Neural\nDiscriminant Analysis (NDA) for Deep Convolutional Neural Networks (DCNNs). NDA\ntransforms deep features to become more discriminative and, therefore, improves\nthe performances in various tasks. Our proposed optimization has two primary\ngoals for inter- and intra-class variances. The first one is to minimize\nvariances within each individual class. The second goal is to maximize pairwise\ndistances between features coming from different classes. We evaluate our NDA\noptimization in different research fields: general supervised classification,\nfine-grained classification, semi-supervised learning, and out of distribution\ndetection. We achieve performance improvements in all the fields compared to\nbaseline methods that do not use NDA. Besides, using NDA, we also surpass the\nstate of the art on the four tasks on various testing datasets.",
          "link": "http://arxiv.org/abs/2107.06209",
          "publishedOn": "2021-07-14T01:41:49.831Z",
          "wordCount": 602,
          "title": "Learning a Discriminant Latent Space with Neural Discriminant Analysis. (arXiv:2107.06209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matveev_A/0/1/0/all/0/1\">Albert Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "We present a pipeline for parametric wireframe extraction from densely\nsampled point clouds. Our approach processes a scalar distance field that\nrepresents proximity to the nearest sharp feature curve. In intermediate\nstages, it detects corners, constructs curve segmentation, and builds a\ntopological graph fitted to the wireframe. As an output, we produce parametric\nspline curves that can be edited and sampled arbitrarily. We evaluate our\nmethod on 50 complex 3D shapes and compare it to the novel deep learning-based\ntechnique, demonstrating superior quality.",
          "link": "http://arxiv.org/abs/2107.06165",
          "publishedOn": "2021-07-14T01:41:49.824Z",
          "wordCount": 521,
          "title": "3D Parametric Wireframe Extraction Based on Distance Fields. (arXiv:2107.06165v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1\">Tao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhouhui Lian</a>",
          "description": "Scene text detection remains a grand challenge due to the variation in text\ncurvatures, orientations, and aspect ratios. One of the most intractable\nproblems is how to represent text instances of arbitrary shapes. Although many\nstate-of-the-art methods have been proposed to model irregular texts in a\nflexible manner, most of them lose simplicity and robustness. Their complicated\npost-processings and the regression under Dirac delta distribution undermine\nthe detection performance and the generalization ability. In this paper, we\npropose an efficient text instance representation named CentripetalText (CT),\nwhich decomposes text instances into the combination of text kernels and\ncentripetal shifts. Specifically, we utilize the centripetal shifts to\nimplement the pixel aggregation, which guide the external text pixels to the\ninternal text kernels. The relaxation operation is integrated into the dense\nregression for centripetal shifts, allowing the correct prediction in a range,\nnot a specific value. The convenient reconstruction of the text contours and\nthe tolerance of the prediction errors in our method guarantee the high\ndetection accuracy and the fast inference speed respectively. Besides, we\nshrink our text detector into a proposal generation module, namely\nCentripetalText Proposal Network (CPN), replacing SPN in Mask TextSpotter v3\nand producing more accurate proposals. To validate the effectiveness of our\ndesigns, we conduct experiments on several commonly used scene text benchmarks,\nincluding both curved and multi-oriented text datasets. For the task of scene\ntext detection, our approach achieves superior or competitive performance\ncompared to other existing methods, e.g., F-measure of 86.3% at 40.0 FPS on\nTotal-Text, F-measure of 86.1% at 34.8 FPS on MSRA-TD500, etc. For the task of\nend-to-end scene text recognition, we outperform Mask TextSpotter v3 by 1.1% on\nTotal-Text.",
          "link": "http://arxiv.org/abs/2107.05945",
          "publishedOn": "2021-07-14T01:41:49.818Z",
          "wordCount": 714,
          "title": "CentripetalText: An Efficient Text Instance Representation for Scene Text Detection. (arXiv:2107.05945v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Filbrandt_G/0/1/0/all/0/1\">Gregory Filbrandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1\">Konstantinos Kamnitsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_D/0/1/0/all/0/1\">David Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Alexandra Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Scarcity of high quality annotated images remains a limiting factor for\ntraining accurate image segmentation models. While more and more annotated\ndatasets become publicly available, the number of samples in each individual\ndatabase is often small. Combining different databases to create larger amounts\nof training data is appealing yet challenging due to the heterogeneity as a\nresult of differences in data acquisition and annotation processes, often\nyielding incompatible or even conflicting information. In this paper, we\ninvestigate and propose several strategies for learning from partially\noverlapping labels in the context of abdominal organ segmentation. We find that\ncombining a semi-supervised approach with an adaptive cross entropy loss can\nsuccessfully exploit heterogeneously annotated data and substantially improve\nsegmentation accuracy compared to baseline and alternative approaches.",
          "link": "http://arxiv.org/abs/2107.05938",
          "publishedOn": "2021-07-14T01:41:49.811Z",
          "wordCount": 566,
          "title": "Learning from Partially Overlapping Labels: Image Segmentation under Annotation Shift. (arXiv:2107.05938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zudi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Donglai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkova_M/0/1/0/all/0/1\">Mariela D. Petkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuelong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zergham Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Krishna Swaroop K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Silin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_N/0/1/0/all/0/1\">Nils Wendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulanger_Weill_J/0/1/0/all/0/1\">Jonathan Boulanger-Weill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhanyasi_N/0/1/0/all/0/1\">Nagaraju Dhanyasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arganda_Carreras_I/0/1/0/all/0/1\">Ignacio Arganda-Carreras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engert_F/0/1/0/all/0/1\">Florian Engert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lichtman_J/0/1/0/all/0/1\">Jeff Lichtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "Segmenting 3D cell nuclei from microscopy image volumes is critical for\nbiological and clinical analysis, enabling the study of cellular expression\npatterns and cell lineages. However, current datasets for neuronal nuclei\nusually contain volumes smaller than $10^{\\text{-}3}\\ mm^3$ with fewer than 500\ninstances per volume, unable to reveal the complexity in large brain regions\nand restrict the investigation of neuronal structures. In this paper, we have\npushed the task forward to the sub-cubic millimeter scale and curated the NucMM\ndataset with two fully annotated volumes: one $0.1\\ mm^3$ electron microscopy\n(EM) volume containing nearly the entire zebrafish brain with around 170,000\nnuclei; and one $0.25\\ mm^3$ micro-CT (uCT) volume containing part of a mouse\nvisual cortex with about 7,000 nuclei. With two imaging modalities and\nsignificantly increased volume size and instance numbers, we discover a great\ndiversity of neuronal nuclei in appearance and density, introducing new\nchallenges to the field. We also perform a statistical analysis to illustrate\nthose challenges quantitatively. To tackle the challenges, we propose a novel\nhybrid-representation learning model that combines the merits of foreground\nmask, contour map, and signed distance transform to produce high-quality 3D\nmasks. The benchmark comparisons on the NucMM dataset show that our proposed\nmethod significantly outperforms state-of-the-art nuclei segmentation\napproaches. Code and data are available at\nhttps://connectomics-bazaar.github.io/proj/nucMM/index.html.",
          "link": "http://arxiv.org/abs/2107.05840",
          "publishedOn": "2021-07-14T01:41:49.804Z",
          "wordCount": 698,
          "title": "NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale. (arXiv:2107.05840v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fiege_E/0/1/0/all/0/1\">Eric Fiege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houta_S/0/1/0/all/0/1\">Salima Houta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisgin_P/0/1/0/all/0/1\">Pinar Bisgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surges_R/0/1/0/all/0/1\">Rainer Surges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howar_F/0/1/0/all/0/1\">Falk Howar</a>",
          "description": "Documentation of epileptic seizures plays an essential role in planning\nmedical therapy. Solutions for automated epileptic seizure detection can help\nimprove the current problem of incomplete and erroneous manual documentation of\nepileptic seizures. In recent years, a number of wearable sensors have been\ntested for this purpose. However, detecting seizures with subtle symptoms\nremains difficult and current solutions tend to have a high false alarm rate.\nSeizures can also affect the patient's arterial blood pressure, which has not\nyet been studied for detection with sensors. The pulse transit time (PTT)\nprovides a noninvasive estimate of arterial blood pressure. It can be obtained\nby using to two sensors, which are measuring the time differences between\narrivals of the pulse waves. Due to separated time chips a clock drift emerges,\nwhich is strongly influencing the PTT. In this work, we present an algorithm\nwhich responds to alterations in the PTT, considering the clock drift and\nenabling the noninvasive monitoring of blood pressure alterations using\nseparated sensors. Furthermore we investigated whether seizures can be detected\nusing the PTT. Our results indicate that using the algorithm, it is possible to\ndetect seizures with a Random Forest. Using the PTT along with other signals in\na multimodal approach, the detection of seizures with subtle symptoms could\nthereby be improved.",
          "link": "http://arxiv.org/abs/2107.05894",
          "publishedOn": "2021-07-14T01:41:49.785Z",
          "wordCount": 651,
          "title": "Automatic Seizure Detection Using the Pulse Transit Time. (arXiv:2107.05894v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sourya Dipta Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg A. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Saikat Dutta</a>",
          "description": "Deep image relighting allows photo enhancement by illumination-specific\nretouching without human effort and so it is getting much interest lately. Most\nof the existing popular methods available for relighting are run-time intensive\nand memory inefficient. Keeping these issues in mind, we propose the use of\nStacked Deep Multi-Scale Hierarchical Network, which aggregates features from\neach image at different scales. Our solution is differentiable and robust for\ntranslating image illumination setting from input image to target image.\nAdditionally, we have also shown that using a multi-step training approach to\nthis problem with two different loss functions can significantly boost\nperformance and can achieve a high quality reconstruction of a relighted image.",
          "link": "http://arxiv.org/abs/2107.06125",
          "publishedOn": "2021-07-14T01:41:49.778Z",
          "wordCount": 565,
          "title": "MSR-Net: Multi-Scale Relighting Network for One-to-One Relighting. (arXiv:2107.06125v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Webster_R/0/1/0/all/0/1\">Ryan Webster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabin_J/0/1/0/all/0/1\">Julien Rabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1\">Loic Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1\">Frederic Jurie</a>",
          "description": "Recently, generative adversarial networks (GANs) have achieved stunning\nrealism, fooling even human observers. Indeed, the popular tongue-in-cheek\nwebsite {\\small \\url{ this http URL}}, taunts users with\nGAN generated images that seem too real to believe. On the other hand, GANs do\nleak information about their training data, as evidenced by membership attacks\nrecently demonstrated in the literature. In this work, we challenge the\nassumption that GAN faces really are novel creations, by constructing a\nsuccessful membership attack of a new kind. Unlike previous works, our attack\ncan accurately discern samples sharing the same identity as training samples\nwithout being the same samples. We demonstrate the interest of our attack\nacross several popular face datasets and GAN training procedures. Notably, we\nshow that even in the presence of significant dataset diversity, an over\nrepresented person can pose a privacy concern.",
          "link": "http://arxiv.org/abs/2107.06018",
          "publishedOn": "2021-07-14T01:41:49.771Z",
          "wordCount": 584,
          "title": "This Person (Probably) Exists. Identity Membership Attacks Against GAN Generated Faces. (arXiv:2107.06018v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sumedha Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_S/0/1/0/all/0/1\">Stephen Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triantafillou_S/0/1/0/all/0/1\">Sofia Triantafillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Model explainability is essential for the creation of trustworthy Machine\nLearning models in healthcare. An ideal explanation resembles the\ndecision-making process of a domain expert and is expressed using concepts or\nterminology that is meaningful to the clinicians. To provide such an\nexplanation, we first associate the hidden units of the classifier to\nclinically relevant concepts. We take advantage of radiology reports\naccompanying the chest X-ray images to define concepts. We discover sparse\nassociations between concepts and hidden units using a linear sparse logistic\nregression. To ensure that the identified units truly influence the\nclassifier's outcome, we adopt tools from Causal Inference literature and, more\nspecifically, mediation analysis through counterfactual interventions. Finally,\nwe construct a low-depth decision tree to translate all the discovered concepts\ninto a straightforward decision rule, expressed to the radiologist. We\nevaluated our approach on a large chest x-ray dataset, where our model produces\na global explanation consistent with clinical knowledge.",
          "link": "http://arxiv.org/abs/2107.06098",
          "publishedOn": "2021-07-14T01:41:49.763Z",
          "wordCount": 595,
          "title": "Using Causal Analysis for Conceptual Deep Learning Explanation. (arXiv:2107.06098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiangbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_A/0/1/0/all/0/1\">An-Ti Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haro_A/0/1/0/all/0/1\">Antonio Haro</a>",
          "description": "Large-scale product recognition is one of the major applications of computer\nvision and machine learning in the e-commerce domain. Since the number of\nproducts is typically much larger than the number of categories of products,\nimage-based product recognition is often cast as a visual search rather than a\nclassification problem. It is also one of the instances of super fine-grained\nrecognition, where there are many products with slight or subtle visual\ndifferences. It has always been a challenge to create a benchmark dataset for\ntraining and evaluation on various visual search solutions in a real-world\nsetting. This motivated creation of eProduct, a dataset consisting of 2.5\nmillion product images towards accelerating development in the areas of\nself-supervised learning, weakly-supervised learning, and multimodal learning,\nfor fine-grained recognition. We present eProduct as a training set and an\nevaluation set, where the training set contains 1.3M+ listing images with\ntitles and hierarchical category labels, for model development, and the\nevaluation set includes 10,000 query and 1.1 million index images for visual\nsearch evaluation. We will present eProduct's construction steps, provide\nanalysis about its diversity and cover the performance of baseline models\ntrained on it.",
          "link": "http://arxiv.org/abs/2107.05856",
          "publishedOn": "2021-07-14T01:41:49.746Z",
          "wordCount": 648,
          "title": "eProduct: A Million-Scale Visual Search Benchmark to Address Product Recognition Challenges. (arXiv:2107.05856v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leng_G/0/1/0/all/0/1\">Guangjie Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yeku Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>",
          "description": "Empirical works suggest that various semantics emerge in the latent space of\nGenerative Adversarial Networks (GANs) when being trained to generate images.\nTo perform real image editing, it requires an accurate mapping from the real\nimage to the latent space to leveraging these learned semantics, which is\nimportant yet difficult. An in-domain GAN inversion approach is recently\nproposed to constraint the inverted code within the latent space by forcing the\nreconstructed image obtained from the inverted code within the real image\nspace. Empirically, we find that the inverted code by the in-domain GAN can\ndeviate from the latent space significantly. To solve this problem, we propose\na force-in-domain GAN based on the in-domain GAN, which utilizes a\ndiscriminator to force the inverted code within the latent space. The\nforce-in-domain GAN can also be interpreted by a cycle-GAN with slight\nmodification. Extensive experiments show that our force-in-domain GAN not only\nreconstructs the target image at the pixel level, but also align the inverted\ncode with the latent space well for semantic editing.",
          "link": "http://arxiv.org/abs/2107.06050",
          "publishedOn": "2021-07-14T01:41:49.733Z",
          "wordCount": 606,
          "title": "Force-in-domain GAN inversion. (arXiv:2107.06050v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets.",
          "link": "http://arxiv.org/abs/2107.05948",
          "publishedOn": "2021-07-14T01:41:49.726Z",
          "wordCount": 681,
          "title": "On Designing Good Representation Learning Models. (arXiv:2107.05948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1\">Aihua Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zihui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yaqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>",
          "description": "Point cloud upsampling aims to generate dense point clouds from given sparse\nones, which is a challenging task due to the irregular and unordered nature of\npoint sets. To address this issue, we present a novel deep learning-based\nmodel, called PU-Flow,which incorporates normalizing flows and feature\ninterpolation techniques to produce dense points uniformly distributed on the\nunderlying surface. Specifically, we formulate the upsampling process as point\ninterpolation in a latent space, where the interpolation weights are adaptively\nlearned from local geometric context, and exploit the invertible\ncharacteristics of normalizing flows to transform points between Euclidean and\nlatent spaces. We evaluate PU-Flow on a wide range of 3D models with sharp\nfeatures and high-frequency details. Qualitative and quantitative results show\nthat our method outperforms state-of-the-art deep learning-based approaches in\nterms of reconstruction quality, proximity-to-surface accuracy, and computation\nefficiency.",
          "link": "http://arxiv.org/abs/2107.05893",
          "publishedOn": "2021-07-14T01:41:49.718Z",
          "wordCount": 577,
          "title": "PU-Flow: a Point Cloud Upsampling Networkwith Normalizing Flows. (arXiv:2107.05893v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1\">Qirong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Ling Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenming Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1\">Xiuyan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaohua Huang</a>",
          "description": "Micro-expression recognition (\\textbf{MER}) has attracted lots of\nresearchers' attention in a decade. However, occlusion will occur for MER in\nreal-world scenarios. This paper deeply investigates an interesting but\nunexplored challenging issue in MER, \\ie, occlusion MER. First, to research MER\nunder real-world occlusion, synthetic occluded micro-expression databases are\ncreated by using various mask for the community. Second, to suppress the\ninfluence of occlusion, a \\underline{R}egion-inspired \\underline{R}elation\n\\underline{R}easoning \\underline{N}etwork (\\textbf{RRRN}) is proposed to model\nrelations between various facial regions. RRRN consists of a backbone network,\nthe Region-Inspired (\\textbf{RI}) module and Relation Reasoning (\\textbf{RR})\nmodule. More specifically, the backbone network aims at extracting feature\nrepresentations from different facial regions, RI module computing an adaptive\nweight from the region itself based on attention mechanism with respect to the\nunobstructedness and importance for suppressing the influence of occlusion, and\nRR module exploiting the progressive interactions among these regions by\nperforming graph convolutions. Experiments are conducted on handout-database\nevaluation and composite database evaluation tasks of MEGC 2018 protocol.\nExperimental results show that RRRN can significantly explore the importance of\nfacial regions and capture the cooperative complementary relationship of facial\nregions for MER. The results also demonstrate RRRN outperforms the\nstate-of-the-art approaches, especially on occlusion, and RRRN acts more robust\nto occlusion.",
          "link": "http://arxiv.org/abs/2107.05904",
          "publishedOn": "2021-07-14T01:41:49.705Z",
          "wordCount": 656,
          "title": "Region attention and graph embedding network for occlusion objective class-based micro-expression recognition. (arXiv:2107.05904v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>",
          "description": "We propose ST-DETR, a Spatio-Temporal Transformer-based architecture for\nobject detection from a sequence of temporal frames. We treat the temporal\nframes as sequences in both space and time and employ the full attention\nmechanisms to take advantage of the features correlations over both dimensions.\nThis treatment enables us to deal with frames sequence as temporal object\nfeatures traces over every location in the space. We explore two possible\napproaches; the early spatial features aggregation over the temporal dimension,\nand the late temporal aggregation of object query spatial features. Moreover,\nwe propose a novel Temporal Positional Embedding technique to encode the time\nsequence information. To evaluate our approach, we choose the Moving Object\nDetection (MOD)task, since it is a perfect candidate to showcase the importance\nof the temporal dimension. Results show a significant 5% mAP improvement on the\nKITTI MOD dataset over the 1-step spatial baseline.",
          "link": "http://arxiv.org/abs/2107.05887",
          "publishedOn": "2021-07-14T01:41:49.692Z",
          "wordCount": 584,
          "title": "ST-DETR: Spatio-Temporal Object Traces Attention Detection Transformer. (arXiv:2107.05887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>",
          "description": "Background: The deployment of various networks (e.g., Internet of Things\n(IoT) and mobile networks) and databases (e.g., nutrition tables and food\ncompositional databases) in the food system generates massive information silos\ndue to the well-known data harmonization problem. The food knowledge graph\nprovides a unified and standardized conceptual terminology and their\nrelationships in a structured form and thus can transform these information\nsilos across the whole food system to a more reusable globally digitally\nconnected Internet of Food, enabling every stage of the food system from\nfarm-to-fork.\n\nScope and approach: We review the evolution of food knowledge organization,\nfrom food classification, food ontology to food knowledge graphs. We then\ndiscuss the progress in food knowledge graphs from several representative\napplications. We finally discuss the main challenges and future directions.\n\nKey findings and conclusions: Our comprehensive summary of current research\non food knowledge graphs shows that food knowledge graphs play an important\nrole in food-oriented applications, including food search and Question\nAnswering (QA), personalized dietary recommendation, food analysis and\nvisualization, food traceability, and food machinery intelligent manufacturing.\nFuture directions for food knowledge graphs cover several fields such as\nmultimodal food knowledge graphs and food intelligence.",
          "link": "http://arxiv.org/abs/2107.05869",
          "publishedOn": "2021-07-14T01:41:49.683Z",
          "wordCount": 637,
          "title": "Towards Building a Food Knowledge Graph for Internet of Food. (arXiv:2107.05869v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kherchouche_A/0/1/0/all/0/1\">Anouar Kherchouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fezza_S/0/1/0/all/0/1\">Sid Ahmed Fezza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>",
          "description": "Despite the enormous performance of deepneural networks (DNNs), recent\nstudies have shown theirvulnerability to adversarial examples (AEs), i.e.,\ncare-fully perturbed inputs designed to fool the targetedDNN. Currently, the\nliterature is rich with many ef-fective attacks to craft such AEs. Meanwhile,\nmany de-fenses strategies have been developed to mitigate thisvulnerability.\nHowever, these latter showed their effec-tiveness against specific attacks and\ndoes not general-ize well to different attacks. In this paper, we proposea\nframework for defending DNN classifier against ad-versarial samples. The\nproposed method is based on atwo-stage framework involving a separate detector\nanda denoising block. The detector aims to detect AEs bycharacterizing them\nthrough the use of natural scenestatistic (NSS), where we demonstrate that\nthese statis-tical features are altered by the presence of\nadversarialperturbations. The denoiser is based on block matching3D (BM3D)\nfilter fed by an optimum threshold valueestimated by a convolutional neural\nnetwork (CNN) toproject back the samples detected as AEs into theirdata\nmanifold. We conducted a complete evaluation onthree standard datasets namely\nMNIST, CIFAR-10 andTiny-ImageNet. The experimental results show that\ntheproposed defense method outperforms the state-of-the-art defense techniques\nby improving the robustnessagainst a set of attacks under black-box, gray-box\nand white-box settings. The source code is available at:\nhttps://github.com/kherchouche-anouar/2DAE",
          "link": "http://arxiv.org/abs/2107.05780",
          "publishedOn": "2021-07-14T01:41:49.666Z",
          "wordCount": 665,
          "title": "Detect and Defense Against Adversarial Examples in Deep Learning using Natural Scene Statistics and Adaptive Denoising. (arXiv:2107.05780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiabao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liangli Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1\">Cuizhu Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jupeng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "High-capacity image steganography, aimed at concealing a secret image in a\ncover image, is a technique to preserve sensitive data, e.g., faces and\nfingerprints. Previous methods focus on the security during transmission and\nsubsequently run a risk of privacy leakage after the restoration of secret\nimages at the receiving end. To address this issue, we propose a framework,\ncalled Multitask Identity-Aware Image Steganography (MIAIS), to achieve direct\nrecognition on container images without restoring secret images. The key issue\nof the direct recognition is to preserve identity information of secret images\ninto container images and make container images look similar to cover images at\nthe same time. Thus, we introduce a simple content loss to preserve the\nidentity information, and design a minimax optimization to deal with the\ncontradictory aspects. We demonstrate that the robustness results can be\ntransferred across different cover datasets. In order to be flexible for the\nsecret image restoration in some cases, we incorporate an optional restoration\nnetwork into our method, providing a multitask framework. The experiments under\nthe multitask scenario show the effectiveness of our framework compared with\nother visual information hiding methods and state-of-the-art high-capacity\nimage steganography methods.",
          "link": "http://arxiv.org/abs/2107.05819",
          "publishedOn": "2021-07-14T01:41:49.617Z",
          "wordCount": 640,
          "title": "Multitask Identity-Aware Image Steganography via Minimax Optimization. (arXiv:2107.05819v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Siyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>",
          "description": "Self-Attention has become prevalent in computer vision models. Inspired by\nfully connected Conditional Random Fields (CRFs), we decompose it into local\nand context terms. They correspond to the unary and binary terms in CRF and are\nimplemented by attention mechanisms with projection matrices. We observe that\nthe unary terms only make small contributions to the outputs, and meanwhile\nstandard CNNs that rely solely on the unary terms achieve great performances on\na variety of tasks. Therefore, we propose Locally Enhanced Self-Attention\n(LESA), which enhances the unary term by incorporating it with convolutions,\nand utilizes a fusion module to dynamically couple the unary and binary\noperations. In our experiments, we replace the self-attention modules with\nLESA. The results on ImageNet and COCO show the superiority of LESA over\nconvolution and self-attention baselines for the tasks of image recognition,\nobject detection, and instance segmentation. The code is made publicly\navailable.",
          "link": "http://arxiv.org/abs/2107.05637",
          "publishedOn": "2021-07-14T01:41:49.577Z",
          "wordCount": 588,
          "title": "Locally Enhanced Self-Attention: Rethinking Self-Attention as Local and Context Terms. (arXiv:2107.05637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengsheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1\">Miguel Angel Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1\">Alex Colburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulbricht_D/0/1/0/all/0/1\">Daniel Ulbricht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1\">Qi Shan</a>",
          "description": "We study the problem of novel view synthesis of a scene comprised of 3D\nobjects. We propose a simple yet effective approach that is neither continuous\nnor implicit, challenging recent trends on view synthesis. We demonstrate that\nalthough continuous radiance field representations have gained a lot of\nattention due to their expressive power, our simple approach obtains comparable\nor even better novel view reconstruction quality comparing with\nstate-of-the-art baselines while increasing rendering speed by over 400x. Our\nmodel is trained in a category-agnostic manner and does not require\nscene-specific optimization. Therefore, it is able to generalize novel view\nsynthesis to object categories not seen during training. In addition, we show\nthat with our simple formulation, we can use view synthesis as a\nself-supervision signal for efficient learning of 3D geometry without explicit\n3D supervision.",
          "link": "http://arxiv.org/abs/2107.05775",
          "publishedOn": "2021-07-14T01:41:49.558Z",
          "wordCount": 581,
          "title": "Fast and Explicit Neural View Synthesis. (arXiv:2107.05775v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun%2A_S/0/1/0/all/0/1\">Shuyang Sun*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue%2A_X/0/1/0/all/0/1\">Xiaoyu Yue*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>",
          "description": "Human vision is able to capture the part-whole hierarchical information from\nthe entire scene. This paper presents the Visual Parser (ViP) that explicitly\nconstructs such a hierarchy with transformers. ViP divides visual\nrepresentations into two levels, the part level and the whole level.\nInformation of each part represents a combination of several independent\nvectors within the whole. To model the representations of the two levels, we\nfirst encode the information from the whole into part vectors through an\nattention mechanism, then decode the global information within the part vectors\nback into the whole representation. By iteratively parsing the two levels with\nthe proposed encoder-decoder interaction, the model can gradually refine the\nfeatures on both levels. Experimental results demonstrate that ViP can achieve\nvery competitive performance on three major tasks e.g. classification,\ndetection and instance segmentation. In particular, it can surpass the previous\nstate-of-the-art CNN backbones by a large margin on object detection. The tiny\nmodel of the ViP family with $7.2\\times$ fewer parameters and $10.9\\times$\nfewer FLOPS can perform comparably with the largest model\nResNeXt-101-64$\\times$4d of ResNe(X)t family. Visualization results also\ndemonstrate that the learnt parts are highly informative of the predicting\nclass, making ViP more explainable than previous fundamental architectures.\nCode is available at https://github.com/kevin-ssy/ViP.",
          "link": "http://arxiv.org/abs/2107.05790",
          "publishedOn": "2021-07-14T01:41:49.550Z",
          "wordCount": 642,
          "title": "Visual Parser: Representing Part-whole Hierarchies with Transformers. (arXiv:2107.05790v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>",
          "description": "The technological advancements of deep learning have enabled sophisticated\nface manipulation schemes, raising severe trust issues and security concerns in\nmodern society. Generally speaking, detecting manipulated faces and locating\nthe potentially altered regions are challenging tasks. Herein, we propose a\nconceptually simple but effective method to efficiently detect forged faces in\nan image while simultaneously locating the manipulated regions. The proposed\nscheme relies on a segmentation map that delivers meaningful high-level\nsemantic information clues about the image. Furthermore, a noise map is\nestimated, playing a complementary role in capturing low-level clues and\nsubsequently empowering decision-making. Finally, the features from these two\nmodules are combined to distinguish fake faces. Extensive experiments show that\nthe proposed model achieves state-of-the-art detection accuracy and remarkable\nlocalization performance.",
          "link": "http://arxiv.org/abs/2107.05821",
          "publishedOn": "2021-07-14T01:41:49.543Z",
          "wordCount": 576,
          "title": "Detect and Locate: A Face Anti-Manipulation Approach with Semantic and Noise-level Supervision. (arXiv:2107.05821v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>",
          "description": "Deep neural networks suffer from catastrophic forgetting when learning\nmultiple knowledge sequentially, and a growing number of approaches have been\nproposed to mitigate this problem. Some of these methods achieved considerable\nperformance by associating the flat local minima with forgetting mitigation in\ncontinual learning. However, they inevitably need (1) tedious hyperparameters\ntuning, and (2) additional computational cost. To alleviate these problems, in\nthis paper, we propose a simple yet effective optimization method, called\nAlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we\nconduct gradient descent and ascent alternatively when the network tends to\nconverge at each session of learning new knowledge. Moreover, we theoretically\nprove that such a strategy can encourage the optimization to converge to a flat\nminima. We verify AlterSGD on continual learning benchmark for semantic\nsegmentation and the empirical results show that we can significantly mitigate\nthe forgetting and outperform the state-of-the-art methods with a large margin\nunder challenging continual learning protocols.",
          "link": "http://arxiv.org/abs/2107.05804",
          "publishedOn": "2021-07-14T01:41:49.519Z",
          "wordCount": 605,
          "title": "AlterSGD: Finding Flat Minima for Continual Learning by Alternative Training. (arXiv:2107.05804v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devgon_S/0/1/0/all/0/1\">Shivin Devgon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1\">Jeffrey Ichnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danielczuk_M/0/1/0/all/0/1\">Michael Danielczuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Daniel S. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1\">Ashwin Balakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shirin Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_E/0/1/0/all/0/1\">Eduardo M. C. Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solowjow_E/0/1/0/all/0/1\">Eugen Solowjow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>",
          "description": "In industrial part kitting, 3D objects are inserted into cavities for\ntransportation or subsequent assembly. Kitting is a critical step as it can\ndecrease downstream processing and handling times and enable lower storage and\nshipping costs. We present Kit-Net, a framework for kitting previously unseen\n3D objects into cavities given depth images of both the target cavity and an\nobject held by a gripper in an unknown initial orientation. Kit-Net uses\nself-supervised deep learning and data augmentation to train a convolutional\nneural network (CNN) to robustly estimate 3D rotations between objects and\nmatching concave or convex cavities using a large training dataset of simulated\ndepth images pairs. Kit-Net then uses the trained CNN to implement a controller\nto orient and position novel objects for insertion into novel prismatic and\nconformal 3D cavities. Experiments in simulation suggest that Kit-Net can\norient objects to have a 98.9% average intersection volume between the object\nmesh and that of the target cavity. Physical experiments with industrial\nobjects succeed in 18% of trials using a baseline method and in 63% of trials\nwith Kit-Net. Video, code, and data are available at\nhttps://github.com/BerkeleyAutomation/Kit-Net.",
          "link": "http://arxiv.org/abs/2107.05789",
          "publishedOn": "2021-07-14T01:41:49.511Z",
          "wordCount": 658,
          "title": "Kit-Net: Self-Supervised Learning to Kit Novel 3D Objects into Novel 3D Cavities. (arXiv:2107.05789v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1\">Hawzhin Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1\">Tolulope A. Odetola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1\">Nan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Syed Rafay Hasan</a>",
          "description": "In this paper, dynamic deployment of Convolutional Neural Network (CNN)\narchitecture is proposed utilizing only IoT-level devices. By partitioning and\npipelining the CNN, it horizontally distributes the computation load among\nresource-constrained devices (called horizontal collaboration), which in turn\nincreases the throughput. Through partitioning, we can decrease the computation\nand energy consumption on individual IoT devices and increase the throughput\nwithout sacrificing accuracy. Also, by processing the data at the generation\npoint, data privacy can be achieved. The results show that throughput can be\nincreased by 1.55x to 1.75x for sharing the CNN into two and three\nresource-constrained devices, respectively.",
          "link": "http://arxiv.org/abs/2107.05828",
          "publishedOn": "2021-07-14T01:41:49.503Z",
          "wordCount": 555,
          "title": "Dynamic Distribution of Edge Intelligence at the Node Level for Internet of Things. (arXiv:2107.05828v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toichkin_D/0/1/0/all/0/1\">Dmitry Toichkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1\">Yansong Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qinghai Guo</a>",
          "description": "State-of-the-art artificial neural networks (ANNs) require labelled data or\nfeedback between layers, are often biologically implausible, and are vulnerable\nto adversarial attacks that humans are not susceptible to. On the other hand,\nHebbian learning in winner-take-all (WTA) networks, is unsupervised,\nfeed-forward, and biologically plausible. However, an objective optimization\ntheory for WTA networks has been missing, except under very limiting\nassumptions. Here we derive formally such a theory, based on biologically\nplausible but generic ANN elements. Through Hebbian learning, network\nparameters maintain a Bayesian generative model of the data. There is no\nsupervisory loss function, but the network does minimize cross-entropy between\nits activations and the input distribution. The key is a \"soft\" WTA where there\nis no absolute \"hard\" winner neuron, and a specific type of Hebbian-like\nplasticity of weights and biases. We confirm our theory in practice, where, in\nhandwritten digit (MNIST) recognition, our Hebbian algorithm, SoftHebb,\nminimizes cross-entropy without having access to it, and outperforms the more\nfrequently used, hard-WTA-based method. Strikingly, it even outperforms\nsupervised end-to-end backpropagation, under certain conditions. Specifically,\nin a two-layered network, SoftHebb outperforms backpropagation when the\ntraining dataset is only presented once, when the testing data is noisy, and\nunder gradient-based adversarial attacks. Adversarial attacks that confuse\nSoftHebb are also confusing to the human eye. Finally, the model can generate\ninterpolations of objects from its input distribution.",
          "link": "http://arxiv.org/abs/2107.05747",
          "publishedOn": "2021-07-14T01:41:49.454Z",
          "wordCount": 679,
          "title": "SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all networks. (arXiv:2107.05747v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1\">Ali Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1\">Madhusudhanan Balasubramanian</a>",
          "description": "Dense optical flow estimation is challenging when there are large\ndisplacements in a scene with heterogeneous motion dynamics, occlusion, and\nscene homogeneity. Traditional approaches to handle these challenges include\nhierarchical and multiresolution processing methods. Learning-based optical\nflow methods typically use a multiresolution approach with image warping when a\nbroad range of flow velocities and heterogeneous motion is present. Accuracy of\nsuch coarse-to-fine methods is affected by the ghosting artifacts when images\nare warped across multiple resolutions and by the vanishing problem in smaller\nscene extents with higher motion contrast. Previously, we devised strategies\nfor building compact dense prediction networks guided by the effective\nreceptive field (ERF) characteristics of the network (DDCNet). The DDCNet\ndesign was intentionally simple and compact allowing it to be used as a\nbuilding block for designing more complex yet compact networks. In this work,\nwe extend the DDCNet strategies to handle heterogeneous motion dynamics by\ncascading DDCNet based sub-nets with decreasing extents of their ERF. Our\nDDCNet with multiresolution capability (DDCNet-Multires) is compact without any\nspecialized network layers. We evaluate the performance of the DDCNet-Multires\nnetwork using standard optical flow benchmark datasets. Our experiments\ndemonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and\nprovides optical flow estimates with accuracy comparable to similar lightweight\nlearning-based methods.",
          "link": "http://arxiv.org/abs/2107.05634",
          "publishedOn": "2021-07-14T01:41:49.412Z",
          "wordCount": 664,
          "title": "DDCNet-Multires: Effective Receptive Field Guided Multiresolution CNN for Dense Prediction. (arXiv:2107.05634v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahiner_A/0/1/0/all/0/1\">Arda Sahiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1\">Tolga Ergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1\">Batu Ozturkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartan_B/0/1/0/all/0/1\">Burak Bartan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1\">Morteza Mardani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>",
          "description": "Generative Adversarial Networks (GANs) are commonly used for modeling complex\ndistributions of data. Both the generators and discriminators of GANs are often\nmodeled by neural networks, posing a non-transparent optimization problem which\nis non-convex and non-concave over the generator and discriminator,\nrespectively. Such networks are often heuristically optimized with gradient\ndescent-ascent (GDA), but it is unclear whether the optimization problem\ncontains any saddle points, or whether heuristic methods can find them in\npractice. In this work, we analyze the training of Wasserstein GANs with\ntwo-layer neural network discriminators through the lens of convex duality, and\nfor a variety of generators expose the conditions under which Wasserstein GANs\ncan be solved exactly with convex optimization approaches, or can be\nrepresented as convex-concave games. Using this convex duality interpretation,\nwe further demonstrate the impact of different activation functions of the\ndiscriminator. Our observations are verified with numerical results\ndemonstrating the power of the convex interpretation, with applications in\nprogressive training of convex architectures corresponding to linear generators\nand quadratic-activation discriminators for CelebA image generation. The code\nfor our experiments is available at https://github.com/ardasahiner/ProCoGAN.",
          "link": "http://arxiv.org/abs/2107.05680",
          "publishedOn": "2021-07-14T01:41:49.405Z",
          "wordCount": 662,
          "title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1\">Darshan Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1\">S Balasubramanian</a>",
          "description": "Facial expression recognition (FER) in the wild is crucial for building\nreliable human-computer interactive systems. However, annotations of large\nscale datasets in FER has been a key challenge as these datasets suffer from\nnoise due to various factors like crowd sourcing, subjectivity of annotators,\npoor quality of images, automatic labelling based on key word search etc. Such\nnoisy annotations impede the performance of FER due to the memorization ability\nof deep networks. During early learning stage, deep networks fit on clean data.\nThen, eventually, they start overfitting on noisy labels due to their\nmemorization ability, which limits FER performance. This report presents\nConsensual Collaborative Training (CCT) framework used in our submission to\nexpression recognition track of the Affective Behaviour Analysis in-the-wild\n(ABAW) 2021 competition. CCT co-trains three networks jointly using a convex\ncombination of supervision loss and consistency loss, without making any\nassumption about the noise distribution. A dynamic transition mechanism is used\nto move from supervision loss in early learning to consistency loss for\nconsensus of predictions among networks in the later stage. Co-training reduces\noverall error, and consistency loss prevents overfitting to noisy samples. The\nperformance of the model is validated on challenging Aff-Wild2 dataset for\ncategorical expression classification. Our code is made publicly available at\nhttps://github.com/1980x/ABAW2021DMACS.",
          "link": "http://arxiv.org/abs/2107.05736",
          "publishedOn": "2021-07-14T01:41:49.362Z",
          "wordCount": 679,
          "title": "Affect Expression Behaviour Analysis in the Wild using Consensual Collaborative Training. (arXiv:2107.05736v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>",
          "description": "This paper presents a novel hierarchical Bayesian model for unbiased atlas\nbuilding with subject-specific regularizations of image registration. We\ndevelop an atlas construction process that automatically selects parameters to\ncontrol the smoothness of diffeomorphic transformation according to individual\nimage data. To achieve this, we introduce a hierarchical prior distribution on\nregularization parameters that allows multiple penalties on images with various\ndegrees of geometric transformations. We then treat the regularization\nparameters as latent variables and integrate them out from the model by using\nthe Monte Carlo Expectation Maximization (MCEM) algorithm. Another advantage of\nour algorithm is that it eliminates the need for manual parameter tuning, which\ncan be tedious and infeasible. We demonstrate the effectiveness of our model on\n3D brain MR images. Experimental results show that our model provides a sharper\natlas compared to the current atlas building algorithms with single-penalty\nregularizations. Our code is publicly available at\nhttps://github.com/jw4hv/HierarchicalBayesianAtlasBuild.",
          "link": "http://arxiv.org/abs/2107.05698",
          "publishedOn": "2021-07-14T01:41:49.345Z",
          "wordCount": 616,
          "title": "Bayesian Atlas Building with Hierarchical Priors for Subject-specific Regularization. (arXiv:2107.05698v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>",
          "description": "Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.",
          "link": "http://arxiv.org/abs/2107.05768",
          "publishedOn": "2021-07-14T01:41:49.311Z",
          "wordCount": 664,
          "title": "Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilie_A/0/1/0/all/0/1\">Andrei Ilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanescu_A/0/1/0/all/0/1\">Alin Stefanescu</a>",
          "description": "Recent work has shown how easily white-box adversarial attacks can be applied\nto state-of-the-art image classifiers. However, real-life scenarios resemble\nmore the black-box adversarial conditions, lacking transparency and usually\nimposing natural, hard constraints on the query budget.\n\nWe propose $\\textbf{EvoBA}$, a black-box adversarial attack based on a\nsurprisingly simple evolutionary search strategy. $\\textbf{EvoBA}$ is\nquery-efficient, minimizes $L_0$ adversarial perturbations, and does not\nrequire any form of training.\n\n$\\textbf{EvoBA}$ shows efficiency and efficacy through results that are in\nline with much more complex state-of-the-art black-box attacks such as\n$\\textbf{AutoZOOM}$. It is more query-efficient than $\\textbf{SimBA}$, a simple\nand powerful baseline black-box attack, and has a similar level of complexity.\nTherefore, we propose it both as a new strong baseline for black-box\nadversarial attacks and as a fast and general tool for gaining empirical\ninsight into how robust image classifiers are with respect to $L_0$ adversarial\nperturbations.\n\nThere exist fast and reliable $L_2$ black-box attacks, such as\n$\\textbf{SimBA}$, and $L_{\\infty}$ black-box attacks, such as\n$\\textbf{DeepSearch}$. We propose $\\textbf{EvoBA}$ as a query-efficient $L_0$\nblack-box adversarial attack which, together with the aforementioned methods,\ncan serve as a generic tool to assess the empirical robustness of image\nclassifiers. The main advantages of such methods are that they run fast, are\nquery-efficient, and can easily be integrated in image classifiers development\npipelines.\n\nWhile our attack minimises the $L_0$ adversarial perturbation, we also report\n$L_2$, and notice that we compare favorably to the state-of-the-art $L_2$\nblack-box attack, $\\textbf{AutoZOOM}$, and of the $L_2$ strong baseline,\n$\\textbf{SimBA}$.",
          "link": "http://arxiv.org/abs/2107.05754",
          "publishedOn": "2021-07-14T01:41:49.277Z",
          "wordCount": 694,
          "title": "EvoBA: An Evolution Strategy as a Strong Baseline forBlack-Box Adversarial Attacks. (arXiv:2107.05754v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1\">Ajey Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1\">Nisarg Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Prasenjit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1\">Govind Makharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>",
          "description": "Contrastive Learning (CL) is a recent representation learning approach, which\nencourages inter-class separability and intra-class compactness in learned\nimage representations. Since medical images often contain multiple semantic\nclasses in an image, using CL to learn representations of local features (as\nopposed to global) is important. In this work, we present a novel\nsemi-supervised 2D medical segmentation solution that applies CL on image\npatches, instead of full images. These patches are meaningfully constructed\nusing the semantic information of different classes obtained via pseudo\nlabeling. We also propose a novel consistency regularization (CR) scheme, which\nworks in synergy with CL. It addresses the problem of confirmation bias, and\nencourages better clustering in the feature space. We evaluate our method on\nfour public medical segmentation datasets and a novel histopathology dataset\nthat we introduce. Our method obtains consistent improvements over\nstate-of-the-art semi-supervised segmentation approaches for all datasets.",
          "link": "http://arxiv.org/abs/2106.06801",
          "publishedOn": "2021-07-13T01:59:35.482Z",
          "wordCount": 613,
          "title": "Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaofei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>",
          "description": "Scene understanding based on LiDAR point cloud is an essential task for\nautonomous cars to drive safely, which often employs spherical projection to\nmap 3D point cloud into multi-channel 2D images for semantic segmentation. Most\nexisting methods simply stack different point attributes/modalities (e.g.\ncoordinates, intensity, depth, etc.) as image channels to increase information\ncapacity, but ignore distinct characteristics of point attributes in different\nimage channels. We design FPS-Net, a convolutional fusion network that exploits\nthe uniqueness and discrepancy among the projected image channels for optimal\npoint cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead\nof simply stacking multiple channel images as a single input, we group them\ninto different modalities to first learn modality-specific features separately\nand then map the learned features into a common high-dimensional feature space\nfor pixel-level fusion and learning. Specifically, we design a residual dense\nblock with multiple receptive fields as a building block in the encoder which\npreserves detailed information in each modality and learns hierarchical\nmodality-specific and fused features effectively. In the FPS-Net decoder, we\nuse a recurrent convolution block likewise to hierarchically decode fused\nfeatures into output space for pixel-level classification. Extensive\nexperiments conducted on two widely adopted point cloud datasets show that\nFPS-Net achieves superior semantic segmentation as compared with\nstate-of-the-art projection-based methods. In addition, the proposed modality\nfusion idea is compatible with typical projection-based methods and can be\nincorporated into them with consistent performance improvements.",
          "link": "http://arxiv.org/abs/2103.00738",
          "publishedOn": "2021-07-13T01:59:35.470Z",
          "wordCount": 697,
          "title": "FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation. (arXiv:2103.00738v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.02159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tone_D/0/1/0/all/0/1\">Daiki Tone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1\">Daisuke Iwai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiura_S/0/1/0/all/0/1\">Shinsaku Hiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1\">Kosuke Sato</a>",
          "description": "This paper presents a novel active marker for dynamic projection mapping (PM)\nthat emits a temporal blinking pattern of infrared (IR) light representing its\nID. We used a multi-material three dimensional (3D) printer to fabricate a\nprojection object with optical fibers that can guide IR light from LEDs\nattached on the bottom of the object. The aperture of an optical fiber is\ntypically very small; thus, it is unnoticeable to human observers under\nprojection and can be placed on a strongly curved part of a projection surface.\nIn addition, the working range of our system can be larger than previous\nmarker-based methods as the blinking patterns can theoretically be recognized\nby a camera placed at a wide range of distances from markers. We propose an\nautomatic marker placement algorithm to spread multiple active markers over the\nsurface of a projection object such that its pose can be robustly estimated\nusing captured images from arbitrary directions. We also propose an\noptimization framework for determining the routes of the optical fibers in such\na way that collisions of the fibers can be avoided while minimizing the loss of\nlight intensity in the fibers. Through experiments conducted using three\nfabricated objects containing strongly curved surfaces, we confirmed that the\nproposed method can achieve accurate dynamic PMs in a significantly wide\nworking range.",
          "link": "http://arxiv.org/abs/2002.02159",
          "publishedOn": "2021-07-13T01:59:35.429Z",
          "wordCount": 693,
          "title": "FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers in Dynamic Projection Mapping. (arXiv:2002.02159v1 [cs.GR] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "This paper jointly resolves two problems in vision transformer: i) the\ncomputation of Multi-Head Self-Attention (MHSA) has high computational/space\ncomplexity; ii) recent vision transformer networks are overly tuned for image\nclassification, ignoring the difference between image classification (simple\nscenarios, more similar to NLP) and downstream scene understanding tasks\n(complicated scenarios, rich structural and contextual information). To this\nend, we note that pyramid pooling has been demonstrated to be effective in\nvarious vision tasks owing to its powerful ability in context abstraction, and\nits natural property of spatial invariance is also suitable to address the loss\nof structural information (problem ii)). Hence, we propose to adapt pyramid\npooling to MHSA for alleviating its high requirement on computational resources\n(problem i)). In this way, this pooling-based MHSA can well address the above\ntwo problems and is thus flexible and powerful for downstream scene\nunderstanding tasks. Plugged with our pooling-based MHSA, we build a\ndownstream-task-oriented transformer network, dubbed Pyramid Pooling\nTransformer (P2T). Extensive experiments demonstrate that, when applied P2T as\nthe backbone network, it shows substantial superiority in various downstream\nscene understanding tasks such as semantic segmentation, object detection,\ninstance segmentation, and visual saliency detection, compared to previous CNN-\nand transformer-based networks. The code will be released at\nhttps://github.com/yuhuan-wu/P2T.",
          "link": "http://arxiv.org/abs/2106.12011",
          "publishedOn": "2021-07-13T01:59:35.406Z",
          "wordCount": 678,
          "title": "P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Child_T/0/1/0/all/0/1\">Travers B. Child</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>",
          "description": "Visual Commonsense Reasoning (VCR) predicts an answer with corresponding\nrationale, given a question-image input. VCR is a recently introduced visual\nscene understanding task with a wide range of applications, including visual\nquestion answering, automated vehicle systems, and clinical decision support.\nPrevious approaches to solving the VCR task generally rely on pre-training or\nexploiting memory with long dependency relationship encoded models. However,\nthese approaches suffer from a lack of generalizability and prior knowledge. In\nthis paper we propose a dynamic working memory based cognitive VCR network,\nwhich stores accumulated commonsense between sentences to provide prior\nknowledge for inference. Extensive experiments show that the proposed model\nyields significant improvements over existing methods on the benchmark VCR\ndataset. Moreover, the proposed model provides intuitive interpretation into\nvisual commonsense reasoning. A Python implementation of our mechanism is\npublicly available at https://github.com/tanjatang/DMVCR",
          "link": "http://arxiv.org/abs/2107.01671",
          "publishedOn": "2021-07-13T01:59:35.394Z",
          "wordCount": 601,
          "title": "Cognitive Visual Commonsense Reasoning Using Dynamic Working Memory. (arXiv:2107.01671v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Changshun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcone_Y/0/1/0/all/0/1\">Yli&#xe8;s Falcone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensalem_S/0/1/0/all/0/1\">Saddek Bensalem</a>",
          "description": "Classification neural networks fail to detect inputs that do not fall inside\nthe classes they have been trained for. Runtime monitoring techniques on the\nneuron activation pattern can be used to detect such inputs. We present an\napproach for monitoring classification systems via data abstraction. Data\nabstraction relies on the notion of box with a resolution. Box-based\nabstraction consists in representing a set of values by its minimal and maximal\nvalues in each dimension. We augment boxes with a notion of resolution and\ndefine their clustering coverage, which is intuitively a quantitative metric\nthat indicates the abstraction quality. This allows studying the effect of\ndifferent clustering parameters on the constructed boxes and estimating an\ninterval of sub-optimal parameters. Moreover, we automatically construct\nmonitors that leverage both the correct and incorrect behaviors of a system.\nThis allows checking the size of the monitor abstractions and analyzing the\nseparability of the network. Monitors are obtained by combining the\nsub-monitors of each class of the system placed at some selected layers. Our\nexperiments demonstrate the effectiveness of our clustering coverage estimation\nand show how to assess the effectiveness and precision of monitors according to\nthe selected clustering parameter and monitored layers.",
          "link": "http://arxiv.org/abs/2104.14435",
          "publishedOn": "2021-07-13T01:59:35.388Z",
          "wordCount": 668,
          "title": "Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes. (arXiv:2104.14435v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tingting Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Modern top-performing object detectors depend heavily on backbone networks,\nwhose advances bring consistent performance gains through exploring more\neffective network structures. In this paper, we propose a novel and flexible\nbackbone framework, namely CBNetV2, to better train existing open-sourced\npre-trained backbones under the pre-training fine-tuning protocol. In\nparticular, CBNetV2 architecture groups multiple identical backbones, which are\nconnected through composite connections. Specifically, it integrates the high-\nand low-level features of multiple backbone networks and gradually expands the\nreceptive field to more efficiently perform object detection. We also propose a\nbetter training strategy with assistant supervision for CBNet-based detectors.\nCBNetV2 has strong generalization capabilities for different backbones and head\ndesigns of the detector architecture. Without additional pre-training, CBNetV2\ncan be adapted to various backbones, including manual-based and NAS-based, as\nwell as CNN-based and Transformer-based ones. Experiments provide strong\nevidence showing that composite backbones are more efficient, effective, and\nresource-friendly than wider and deeper networks. CBNetV2 is compatible with\nthe head designs of most mainstream detectors, including one-stage and\ntwo-stage detectors, as well as anchor-based and anchor-free-based ones, and\nsignificantly improve their performances by more than 3.0% AP over the baseline\non COCO. Particularly, under the single-model and single-scale testing\nprotocol, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO\ntest-dev, which is significantly better than the state-of-the-art result (i.e.,\n57.7% box AP and 50.2% mask AP). Code is available at\nhttps://github.com/VDIGPKU/CBNetV2.",
          "link": "http://arxiv.org/abs/2107.00420",
          "publishedOn": "2021-07-13T01:59:35.364Z",
          "wordCount": 719,
          "title": "CBNetV2: A Composite Backbone Network Architecture for Object Detection. (arXiv:2107.00420v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1\">Dimitrios Vytiniotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1\">Grzegorz Swirszcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1\">Viorica Patraucean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.",
          "link": "http://arxiv.org/abs/2106.08318",
          "publishedOn": "2021-07-13T01:59:35.358Z",
          "wordCount": 668,
          "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_M/0/1/0/all/0/1\">Mingzhe Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhengjun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Facial Expression Recognition (FER) in the wild is an extremely challenging\ntask in computer vision due to variant backgrounds, low-quality facial images,\nand the subjectiveness of annotators. These uncertainties make it difficult for\nneural networks to learn robust features on limited-scale datasets. Moreover,\nthe networks can be easily distributed by the above factors and perform\nincorrect decisions. Recently, vision transformer (ViT) and data-efficient\nimage transformers (DeiT) present their significant performance in traditional\nclassification tasks. The self-attention mechanism makes transformers obtain a\nglobal receptive field in the first layer which dramatically enhances the\nfeature extraction capability. In this work, we first propose a novel pure\ntransformer-based mask vision transformer (MVT) for FER in the wild, which\nconsists of two modules: a transformer-based mask generation network (MGN) to\ngenerate a mask that can filter out complex backgrounds and occlusion of face\nimages, and a dynamic relabeling module to rectify incorrect labels in FER\ndatasets in the wild. Extensive experimental results demonstrate that our MVT\noutperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with\n89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable\nresult on AffectNet-8 with 61.40%.",
          "link": "http://arxiv.org/abs/2106.04520",
          "publishedOn": "2021-07-13T01:59:35.352Z",
          "wordCount": 666,
          "title": "MVT: Mask Vision Transformer for Facial Expression Recognition in the wild. (arXiv:2106.04520v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangrui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "Place recognition gives a SLAM system the ability to correct cumulative\nerrors. Unlike images that contain rich texture features, point clouds are\nalmost pure geometric information which makes place recognition based on point\nclouds challenging. Existing works usually encode low-level features such as\ncoordinate, normal, reflection intensity, etc., as local or global descriptors\nto represent scenes. Besides, they often ignore the translation between point\nclouds when matching descriptors. Different from most existing methods, we\nexplore the use of high-level features, namely semantics, to improve the\ndescriptor's representation ability. Also, when matching descriptors, we try to\ncorrect the translation between point clouds to improve accuracy. Concretely,\nwe propose a novel global descriptor, Semantic Scan Context, which explores\nsemantic information to represent scenes more effectively. We also present a\ntwo-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align\nthe point cloud to improve matching performance. Our experiments on the KITTI\ndataset show that our approach outperforms the state-of-the-art methods with a\nlarge margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.",
          "link": "http://arxiv.org/abs/2107.00382",
          "publishedOn": "2021-07-13T01:59:35.345Z",
          "wordCount": 638,
          "title": "SSC: Semantic Scan Context for Large-Scale Place Recognition. (arXiv:2107.00382v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhananjaya_M/0/1/0/all/0/1\">Mahesh M Dhananjaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Autonomous driving is rapidly advancing, and Level 2 functions are becoming a\nstandard feature. One of the foremost outstanding hurdles is to obtain robust\nvisual perception in harsh weather and low light conditions where accuracy\ndegradation is severe. It is critical to have a weather classification model to\ndecrease visual perception confidence during these scenarios. Thus, we have\nbuilt a new dataset for weather (fog, rain, and snow) classification and light\nlevel (bright, moderate, and low) classification. Furthermore, we provide\nstreet type (asphalt, grass, and cobblestone) classification, leading to 9\nlabels. Each image has three labels corresponding to weather, light level, and\nstreet type. We recorded the data utilizing an industrial front camera of RCCC\n(red/clear) format with a resolution of $1024\\times1084$. We collected 15k\nvideo sequences and sampled 60k images. We implement an active learning\nframework to reduce the dataset's redundancy and find the optimal set of frames\nfor training a model. We distilled the 60k images further to 1.1k images, which\nwill be shared publicly after privacy anonymization. There is no public dataset\nfor weather and light level classification focused on autonomous driving to the\nbest of our knowledge. The baseline ResNet18 network used for weather\nclassification achieves state-of-the-art results in two non-automotive weather\nclassification public datasets but significantly lower accuracy on our proposed\ndataset, demonstrating it is not saturated and needs further research.",
          "link": "http://arxiv.org/abs/2104.14042",
          "publishedOn": "2021-07-13T01:59:35.338Z",
          "wordCount": 715,
          "title": "Weather and Light Level Classification for Autonomous Driving: Dataset, Baseline and Active Learning. (arXiv:2104.14042v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianbu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>",
          "description": "The monitoring of coastal wetlands is of great importance to the protection\nof marine and terrestrial ecosystems. However, due to the complex environment,\nsevere vegetation mixture, and difficulty of access, it is impossible to\naccurately classify coastal wetlands and identify their species with\ntraditional classifiers. Despite the integration of multisource remote sensing\ndata for performance enhancement, there are still challenges with acquiring and\nexploiting the complementary merits from multisource data. In this paper, the\nDeepwise Feature Interaction Network (DFINet) is proposed for wetland\nclassification. A depthwise cross attention module is designed to extract\nself-correlation and cross-correlation from multisource feature pairs. In this\nway, meaningful complementary information is emphasized for classification.\nDFINet is optimized by coordinating consistency loss, discrimination loss, and\nclassification loss. Accordingly, DFINet reaches the standard solution-space\nunder the regularity of loss functions, while the spatial consistency and\nfeature discrimination are preserved. Comprehensive experimental results on two\nhyperspectral and multispectral wetland datasets demonstrate that the proposed\nDFINet outperforms other competitive methods in terms of overall accuracy.",
          "link": "http://arxiv.org/abs/2106.06896",
          "publishedOn": "2021-07-13T01:59:35.315Z",
          "wordCount": 656,
          "title": "Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yaya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>",
          "description": "By adding human-imperceptible perturbations to images, DNNs can be easily\nfooled. As one of the mainstream methods, feature space targeted attacks\nperturb images by modulating their intermediate feature maps, for the\ndiscrepancy between the intermediate source and target features is minimized.\nHowever, the current choice of pixel-wise Euclidean Distance to measure the\ndiscrepancy is questionable because it unreasonably imposes a\nspatial-consistency constraint on the source and target features. Intuitively,\nan image can be categorized as \"cat\" no matter the cat is on the left or right\nof the image. To address this issue, we propose to measure this discrepancy\nusing statistic alignment. Specifically, we design two novel approaches called\nPair-wise Alignment Attack and Global-wise Alignment Attack, which attempt to\nmeasure similarities between feature maps by high-order statistics with\ntranslation invariance. Furthermore, we systematically analyze the layer-wise\ntransferability with varied difficulties to obtain highly reliable attacks.\nExtensive experiments verify the effectiveness of our proposed method, and it\noutperforms the state-of-the-art algorithms by a large margin. Our code is\npublicly available at https://github.com/yaya-cheng/PAA-GAA.",
          "link": "http://arxiv.org/abs/2105.11645",
          "publishedOn": "2021-07-13T01:59:35.308Z",
          "wordCount": 637,
          "title": "Feature Space Targeted Attacks by Statistic Alignment. (arXiv:2105.11645v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zamanitajeddin_N/0/1/0/all/0/1\">Neda Zamanitajeddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>",
          "description": "Digitization of histology images and the advent of new computational methods,\nlike deep learning, have helped the automatic grading of colorectal\nadenocarcinoma cancer (CRA). Present automated CRA grading methods, however,\nusually use tiny image patches and thus fail to integrate the entire tissue\nmicro-architecture for grading purposes. To tackle these challenges, we propose\nto use a statistical network analysis method to describe the complex structure\nof the tissue micro-environment by modelling nuclei and their connections as a\nnetwork. We show that by analyzing only the interactions between the cells in a\nnetwork, we can extract highly discriminative statistical features for CRA\ngrading. Unlike other deep learning or convolutional graph-based approaches,\nour method is highly scalable (can be used for cell networks consist of\nmillions of nodes), completely explainable, and computationally inexpensive. We\ncreate cell networks on a broad CRC histology image dataset, experiment with\nour method, and report state-of-the-art performance for the prediction of\nthree-class CRA grading.",
          "link": "http://arxiv.org/abs/2106.15299",
          "publishedOn": "2021-07-13T01:59:35.300Z",
          "wordCount": 624,
          "title": "Cells are Actors: Social Network Analysis with Classical ML for SOTA Histology Image Classification. (arXiv:2106.15299v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07961",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1\">Alan Q. Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+LaViolette_A/0/1/0/all/0/1\">Aaron K. LaViolette</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moon_L/0/1/0/all/0/1\">Leo Moon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chris Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>",
          "description": "Compressed sensing fluorescence microscopy (CS-FM) proposes a scheme whereby\nless measurements are collected during sensing and reconstruction is performed\nto recover the image. Much work has gone into optimizing the sensing and\nreconstruction portions separately. We propose a method of jointly optimizing\nboth sensing and reconstruction end-to-end under a total measurement\nconstraint, enabling learning of the optimal sensing scheme concurrently with\nthe parameters of a neural network-based reconstruction network. We train our\nmodel on a rich dataset of confocal, two-photon, and wide-field microscopy\nimages comprising of a variety of biological samples. We show that our method\noutperforms several baseline sensing schemes and a regularized regression\nreconstruction algorithm.",
          "link": "http://arxiv.org/abs/2105.07961",
          "publishedOn": "2021-07-13T01:59:35.295Z",
          "wordCount": 586,
          "title": "Joint Optimization of Hadamard Sensing and Reconstruction in Compressed Sensing Fluorescence Microscopy. (arXiv:2105.07961v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1\">Michal Uricar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahiaoui_L/0/1/0/all/0/1\">Lucie Yahiaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Manual annotation of soiling on surround view cameras is a very challenging\nand expensive task. The unclear boundary for various soiling categories like\nwater drops or mud particles usually results in a large variance in the\nannotation quality. As a result, the models trained on such poorly annotated\ndata are far from being optimal. In this paper, we focus on handling such noisy\nannotations via pseudo-label driven ensemble model which allow us to quickly\nspot problematic annotations and in most cases also sufficiently fixing them.\nWe train a soiling segmentation model on both noisy and refined labels and\ndemonstrate significant improvements using the refined annotations. It also\nillustrates that it is possible to effectively refine lower cost coarse\nannotations.",
          "link": "http://arxiv.org/abs/2105.07930",
          "publishedOn": "2021-07-13T01:59:35.275Z",
          "wordCount": 601,
          "title": "Ensemble-based Semi-supervised Learning to Improve Noisy Soiling Annotations in Autonomous Driving. (arXiv:2105.07930v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14844",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yucheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>",
          "description": "Low-light imaging on mobile devices is typically challenging due to\ninsufficient incident light coming through the relatively small aperture,\nresulting in a low signal-to-noise ratio. Most of the previous works on\nlow-light image processing focus either only on a single task such as\nillumination adjustment, color enhancement, or noise removal; or on a joint\nillumination adjustment and denoising task that heavily relies on short-long\nexposure image pairs collected from specific camera models, and thus these\napproaches are less practical and generalizable in real-world settings where\ncamera-specific joint enhancement and restoration is required. To tackle this\nproblem, in this paper, we propose a low-light image processing framework that\nperforms joint illumination adjustment, color enhancement, and denoising.\nConsidering the difficulty in model-specific data collection and the ultra-high\ndefinition of the captured images, we design two branches: a coefficient\nestimation branch as well as a joint enhancement and denoising branch. The\ncoefficient estimation branch works in a low-resolution space and predicts the\ncoefficients for enhancement via bilateral learning, whereas the joint\nenhancement and denoising branch works in a full-resolution space and\nprogressively performs joint enhancement and denoising. In contrast to existing\nmethods, our framework does not need to recollect massive data when being\nadapted to another camera model, which significantly reduces the efforts\nrequired to fine-tune our approach for practical usage. Through extensive\nexperiments, we demonstrate its great potential in real-world low-light imaging\napplications when compared with current state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.14844",
          "publishedOn": "2021-07-13T01:59:35.258Z",
          "wordCount": 708,
          "title": "Progressive Joint Low-light Enhancement and Noise Removal for Raw Images. (arXiv:2106.14844v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weina Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>",
          "description": "Being able to explain the prediction to clinical end-users is a necessity to\nleverage the power of AI models for clinical decision support. For medical\nimages, saliency maps are the most common form of explanation. The maps\nhighlight important features for AI model's prediction. Although many saliency\nmap methods have been proposed, it is unknown how well they perform on\nexplaining decisions on multi-modal medical images, where each modality/channel\ncarries distinct clinical meanings of the same underlying biomedical\nphenomenon. Understanding such modality-dependent features is essential for\nclinical users' interpretation of AI decisions. To tackle this clinically\nimportant but technically ignored problem, we propose the MSFI\n(Modality-Specific Feature Importance) metric to examine whether saliency maps\ncan highlight modality-specific important features. MSFI encodes the clinical\nrequirements on modality prioritization and modality-specific feature\nlocalization. Our evaluations on 16 commonly used saliency map methods,\nincluding a clinician user study, show that although most saliency map methods\ncaptured modality importance information in general, most of them failed to\nhighlight modality-specific important features consistently and precisely. The\nevaluation results guide the choices of saliency map methods and provide\ninsights to propose new ones targeting clinical applications.",
          "link": "http://arxiv.org/abs/2107.05047",
          "publishedOn": "2021-07-13T01:59:35.239Z",
          "wordCount": 646,
          "title": "One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images. (arXiv:2107.05047v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuanhao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>",
          "description": "This technical report presents our solution to the HACS Temporal Action\nLocalization Challenge 2021, Weakly-Supervised Learning Track. The goal of\nweakly-supervised temporal action localization is to temporally locate and\nclassify action of interest in untrimmed videos given only video-level labels.\nWe adopt the two-stream consensus network (TSCN) as the main framework in this\nchallenge. The TSCN consists of a two-stream base model training procedure and\na pseudo ground truth learning procedure. The base model training encourages\nthe model to predict reliable predictions based on single modality (i.e., RGB\nor optical flow), based on the fusion of which a pseudo ground truth is\ngenerated and in turn used as supervision to train the base models. On the HACS\nv1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our\nmethod achieves 22.20% on the validation set and 21.68% on the testing set in\nterms of average mAP. Our solution ranked the 2rd in this challenge, and we\nhope our method can serve as a baseline for future academic research.",
          "link": "http://arxiv.org/abs/2106.10829",
          "publishedOn": "2021-07-13T01:59:35.233Z",
          "wordCount": 654,
          "title": "Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track. (arXiv:2106.10829v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1\">Juraj Kabzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kok Seang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1\">Eric Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1\">Alex Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1\">Luke Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1\">Sammy Omari</a>",
          "description": "In this work, we propose the world's first closed-loop ML-based planning\nbenchmark for autonomous driving. While there is a growing body of ML-based\nmotion planners, the lack of established datasets and metrics has limited the\nprogress in this area. Existing benchmarks for autonomous vehicle motion\nprediction have focused on short-term motion forecasting, rather than long-term\nplanning. This has led previous works to use open-loop evaluation with L2-based\nmetrics, which are not suitable for fairly evaluating long-term planning. Our\nbenchmark overcomes these limitations by introducing a large-scale driving\ndataset, lightweight closed-loop simulator, and motion-planning-specific\nmetrics. We provide a high-quality dataset with 1500h of human driving data\nfrom 4 cities across the US and Asia with widely varying traffic patterns\n(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop\nsimulation framework with reactive agents and provide a large set of both\ngeneral and scenario-specific planning metrics. We plan to release the dataset\nat NeurIPS 2021 and organize benchmark challenges starting in early 2022.",
          "link": "http://arxiv.org/abs/2106.11810",
          "publishedOn": "2021-07-13T01:59:35.202Z",
          "wordCount": 650,
          "title": "NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoxiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a novel method for single image depth estimation using surface\nnormal constraints. Existing depth estimation methods either suffer from the\nlack of geometric constraints, or are limited to the difficulty of reliably\ncapturing geometric context, which leads to a bottleneck of depth estimation\nquality. We therefore introduce a simple yet effective method, named Adaptive\nSurface Normal (ASN) constraint, to effectively correlate the depth estimation\nwith geometric consistency. Our key idea is to adaptively determine the\nreliable local geometry from a set of randomly sampled candidates to derive\nsurface normal constraint, for which we measure the consistency of the\ngeometric contextual features. As a result, our method can faithfully\nreconstruct the 3D geometry and is robust to local shape variations, such as\nboundaries, sharp corners and noises. We conduct extensive evaluations and\ncomparisons using public datasets. The experimental results demonstrate our\nmethod outperforms the state-of-the-art methods and has superior efficiency and\nrobustness.",
          "link": "http://arxiv.org/abs/2103.15483",
          "publishedOn": "2021-07-13T01:59:35.196Z",
          "wordCount": 623,
          "title": "Adaptive Surface Normal Constraint for Depth Estimation. (arXiv:2103.15483v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1\">Sambit Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotzig_H/0/1/0/all/0/1\">Heinrich Gotzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1\">Stefan Milz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1\">Patrick Mader</a>",
          "description": "3D object detection based on LiDAR point clouds is a crucial module in\nautonomous driving particularly for long range sensing. Most of the research is\nfocused on achieving higher accuracy and these models are not optimized for\ndeployment on embedded systems from the perspective of latency and power\nefficiency. For high speed driving scenarios, latency is a crucial parameter as\nit provides more time to react to dangerous situations. Typically a voxel or\npoint-cloud based 3D convolution approach is utilized for this module. Firstly,\nthey are inefficient on embedded platforms as they are not suitable for\nefficient parallelization. Secondly, they have a variable runtime due to level\nof sparsity of the scene which is against the determinism needed in a safety\nsystem. In this work, we aim to develop a very low latency algorithm with fixed\nruntime. We propose a novel semantic segmentation architecture as a single\nunified model for object center detection using key points, box predictions and\norientation prediction using binned classification in a simpler Bird's Eye View\n(BEV) 2D representation. The proposed architecture can be trivially extended to\ninclude semantic segmentation classes like road without any additional\ncomputation. The proposed model has a latency of 4 ms on the embedded Nvidia\nXavier platform. The model is 5X faster than other top accuracy models with a\nminimal accuracy degradation of 2% in Average Precision at IoU=0.5 on KITTI\ndataset.",
          "link": "http://arxiv.org/abs/2104.10780",
          "publishedOn": "2021-07-13T01:59:35.184Z",
          "wordCount": 725,
          "title": "BEVDetNet: Bird's Eye View LiDAR Point Cloud based Real-time 3D Object Detection for Autonomous Driving. (arXiv:2104.10780v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1\">Waqas Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_A/0/1/0/all/0/1\">Amir Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Neeraj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RehmanJaved_A/0/1/0/all/0/1\">Abdul RehmanJaved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1\">Thippa Reddy Gadekallu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalil_Z/0/1/0/all/0/1\">Zunera Jalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryvinska_N/0/1/0/all/0/1\">Natalia Kryvinska</a>",
          "description": "Cash payment is still king in several markets, accounting for more than 90\\\nof the payments in almost all the developing countries. The usage of mobile\nphones is pretty ordinary in this present era. Mobile phones have become an\ninseparable friend for many users, serving much more than just communication\ntools. Every subsequent person is heavily relying on them due to multifaceted\nusage and affordability. Every person wants to manage his/her daily\ntransactions and related issues by using his/her mobile phone. With the rise\nand advancements of mobile-specific security, threats are evolving as well. In\nthis paper, we provide a survey of various security models for mobile phones.\nWe explore multiple proposed models of the mobile payment system (MPS), their\ntechnologies and comparisons, payment methods, different security mechanisms\ninvolved in MPS, and provide analysis of the encryption technologies,\nauthentication methods, and firewall in MPS. We also present current challenges\nand future directions of mobile phone security.",
          "link": "http://arxiv.org/abs/2105.12097",
          "publishedOn": "2021-07-13T01:59:35.166Z",
          "wordCount": 634,
          "title": "Security in Next Generation Mobile Payment Systems: A Comprehensive Survey. (arXiv:2105.12097v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1\">Serban Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>",
          "description": "Convolutional neural networks (CNNs) have led to significant improvements in\ntasks involving semantic segmentation of images. CNNs are vulnerable in the\narea of biomedical image segmentation because of distributional gap between two\nsource and target domains with different data modalities which leads to domain\nshift. Domain shift makes data annotations in new modalities necessary because\nmodels must be retrained from scratch. Unsupervised domain adaptation (UDA) is\nproposed to adapt a model to new modalities using solely unlabeled target\ndomain data. Common UDA algorithms require access to data points in the source\ndomain which may not be feasible in medical imaging due to privacy concerns. In\nthis work, we develop an algorithm for UDA in a privacy-constrained setting,\nwhere the source domain data is inaccessible. Our idea is based on encoding the\ninformation from the source samples into a prototypical distribution that is\nused as an intermediate distribution for aligning the target domain\ndistribution with the source domain distribution. We demonstrate the\neffectiveness of our algorithm by comparing it to state-of-the-art medical\nimage semantic segmentation approaches on two medical image semantic\nsegmentation datasets.",
          "link": "http://arxiv.org/abs/2101.00522",
          "publishedOn": "2021-07-13T01:59:35.157Z",
          "wordCount": 659,
          "title": "Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images. (arXiv:2101.00522v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>",
          "description": "Human behavior understanding with unmanned aerial vehicles (UAVs) is of great\nsignificance for a wide range of applications, which simultaneously brings an\nurgent demand of large, challenging, and comprehensive benchmarks for the\ndevelopment and evaluation of UAV-based models. However, existing benchmarks\nhave limitations in terms of the amount of captured data, types of data\nmodalities, categories of provided tasks, and diversities of subjects and\nenvironments. Here we propose a new benchmark - UAVHuman - for human behavior\nunderstanding with UAVs, which contains 67,428 multi-modal video sequences and\n119 subjects for action recognition, 22,476 frames for pose estimation, 41,290\nframes and 1,144 identities for person re-identification, and 22,263 frames for\nattribute recognition. Our dataset was collected by a flying UAV in multiple\nurban and rural districts in both daytime and nighttime over three months,\nhence covering extensive diversities w.r.t subjects, backgrounds,\nilluminations, weathers, occlusions, camera motions, and UAV flying attitudes.\nSuch a comprehensive and challenging benchmark shall be able to promote the\nresearch of UAV-based human behavior understanding, including action\nrecognition, pose estimation, re-identification, and attribute recognition.\nFurthermore, we propose a fisheye-based action recognition method that\nmitigates the distortions in fisheye videos via learning unbounded\ntransformations guided by flat RGB videos. Experiments show the efficacy of our\nmethod on the UAV-Human dataset. The project page:\nhttps://github.com/SUTDCV/UAV-Human",
          "link": "http://arxiv.org/abs/2104.00946",
          "publishedOn": "2021-07-13T01:59:35.149Z",
          "wordCount": 707,
          "title": "UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles. (arXiv:2104.00946v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zipeng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>",
          "description": "Modern change detection (CD) has achieved remarkable success by the powerful\ndiscriminative ability of deep convolutions. However, high-resolution remote\nsensing CD remains challenging due to the complexity of objects in the scene.\nObjects with the same semantic concept may show distinct spectral\ncharacteristics at different times and spatial locations. Most recent CD\npipelines using pure convolutions are still struggling to relate long-range\nconcepts in space-time. Non-local self-attention approaches show promising\nperformance via modeling dense relations among pixels, yet are computationally\ninefficient. Here, we propose a bitemporal image transformer (BIT) to\nefficiently and effectively model contexts within the spatial-temporal domain.\nOur intuition is that the high-level concepts of the change of interest can be\nrepresented by a few visual words, i.e., semantic tokens. To achieve this, we\nexpress the bitemporal image into a few tokens, and use a transformer encoder\nto model contexts in the compact token-based space-time. The learned\ncontext-rich tokens are then feedback to the pixel-space for refining the\noriginal features via a transformer decoder. We incorporate BIT in a deep\nfeature differencing-based CD framework. Extensive experiments on three CD\ndatasets demonstrate the effectiveness and efficiency of the proposed method.\nNotably, our BIT-based model significantly outperforms the purely convolutional\nbaseline using only 3 times lower computational costs and model parameters.\nBased on a naive backbone (ResNet18) without sophisticated structures (e.g.,\nFPN, UNet), our model surpasses several state-of-the-art CD methods, including\nbetter than four recent attention-based methods in terms of efficiency and\naccuracy. Our code is available at https://github.com/justchenhao/BIT\\_CD.",
          "link": "http://arxiv.org/abs/2103.00208",
          "publishedOn": "2021-07-13T01:59:35.142Z",
          "wordCount": 731,
          "title": "Remote Sensing Image Change Detection with Transformers. (arXiv:2103.00208v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1\">Sebastian Monka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1\">Lavdim Halilaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1\">Stefan Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>",
          "description": "Traditional computer vision approaches, based on neural networks (NN), are\ntypically trained on a large amount of image data. By minimizing the\ncross-entropy loss between a prediction and a given class label, the NN and its\nvisual embedding space are learned to fulfill a given task. However, due to the\nsole dependence on the image data distribution of the training domain, these\nmodels tend to fail when applied to a target domain that differs from their\nsource domain. To learn a more robust NN to domain shifts, we propose the\nknowledge graph neural network (KG-NN), a neuro-symbolic approach that\nsupervises the training using image-data-invariant auxiliary knowledge. The\nauxiliary knowledge is first encoded in a knowledge graph with respective\nconcepts and their relationships, which is then transformed into a dense vector\nrepresentation via an embedding method. Using a contrastive loss function,\nKG-NN learns to adapt its visual embedding space and thus its weights according\nto the image-data invariant knowledge graph embedding space. We evaluate KG-NN\non visual transfer learning tasks for classification using the mini-ImageNet\ndataset and its derivatives, as well as road sign recognition datasets from\nGermany and China. The results show that a visual model trained with a\nknowledge graph as a trainer outperforms a model trained with cross-entropy in\nall experiments, in particular when the domain gap increases. Besides better\nperformance and stronger robustness to domain shifts, these KG-NN adapts to\nmultiple datasets and classes without suffering heavily from catastrophic\nforgetting.",
          "link": "http://arxiv.org/abs/2102.08747",
          "publishedOn": "2021-07-13T01:59:35.134Z",
          "wordCount": 728,
          "title": "Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yundong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huiye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiang Hu</a>",
          "description": "Medical image segmentation - the prerequisite of numerous clinical needs -\nhas been significantly prospered by recent advances in convolutional neural\nnetworks (CNNs). However, it exhibits general limitations on modeling explicit\nlong-range relation, and existing cures, resorting to building deep encoders\nalong with aggressive downsampling operations, leads to redundant deepened\nnetworks and loss of localized details. Hence, the segmentation task awaits a\nbetter solution to improve the efficiency of modeling global contexts while\nmaintaining a strong grasp of low-level details. In this paper, we propose a\nnovel parallel-in-branch architecture, TransFuse, to address this challenge.\nTransFuse combines Transformers and CNNs in a parallel style, where both global\ndependency and low-level spatial details can be efficiently captured in a much\nshallower manner. Besides, a novel fusion technique - BiFusion module is\ncreated to efficiently fuse the multi-level features from both branches.\nExtensive experiments demonstrate that TransFuse achieves the newest\nstate-of-the-art results on both 2D and 3D medical image sets including polyp,\nskin lesion, hip, and prostate segmentation, with significant parameter\ndecrease and inference speed improvement.",
          "link": "http://arxiv.org/abs/2102.08005",
          "publishedOn": "2021-07-13T01:59:35.127Z",
          "wordCount": 644,
          "title": "TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation. (arXiv:2102.08005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jian Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuge Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Licong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaona Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huabin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Desheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>",
          "description": "Objective: Breast cancer screening is of great significance in contemporary\nwomen's health prevention. The existing machines embedded in the AI system do\nnot reach the accuracy that clinicians hope. How to make intelligent systems\nmore reliable is a common problem. Methods: 1) Ultrasound image\nsuper-resolution: the SRGAN super-resolution network reduces the unclearness of\nultrasound images caused by the device itself and improves the accuracy and\ngeneralization of the detection model. 2) In response to the needs of medical\nimages, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI\nmodel: based on the respective advantages of different AI models, we employ two\nAI models to determine clinical resuls cross validation. And we accept the same\nresults and refuses others. Results: 1) With the help of the super-resolution\nmodel, the YOLOv4 model and the CenterNet model both increased the mAP score by\n9.6% and 13.8%. 2) Two methods for transforming the target model into a\nclassification model are proposed. And the unified output is in a specified\nformat to facilitate the call of the molti-AI model. 3) In the classification\nevaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%,\nspecificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity\n92.54%), the multi-AI model will refuse to make judgments on 23.55% of the\ninput data. Correspondingly, the performance has been greatly improved to\n95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work\nmakes the AI model more reliable in medical image diagnosis. Significance: 1)\nThe proposed method makes the target detection model more suitable for\ndiagnosing breast ultrasound images. 2) It provides a new idea for artificial\nintelligence in medical diagnosis, which can more conveniently introduce target\ndetection models from other fields to serve medical lesion screening.",
          "link": "http://arxiv.org/abs/2101.02639",
          "publishedOn": "2021-07-13T01:59:35.121Z",
          "wordCount": 774,
          "title": "More Reliable AI Solution: Breast Ultrasound Diagnosis Using Multi-AI Combination. (arXiv:2101.02639v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaojie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Real-time surgical phase recognition is a fundamental task in modern\noperating rooms. Previous works tackle this task relying on architectures\narranged in spatio-temporal order, however, the supportive benefits of\nintermediate spatial features are not considered. In this paper, we introduce,\nfor the first time in surgical workflow analysis, Transformer to reconsider the\nignored complementary effects of spatial and temporal features for accurate\nsurgical phase recognition. Our hybrid embedding aggregation Transformer fuses\ncleverly designed spatial and temporal embeddings by allowing for active\nqueries based on spatial information from temporal embedding sequences. More\nimportantly, our framework processes the hybrid embeddings in parallel to\nachieve a high inference speed. Our method is thoroughly validated on two large\nsurgical video datasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and\noutperforms the state-of-the-art approaches at a processing speed of 91 fps.",
          "link": "http://arxiv.org/abs/2103.09712",
          "publishedOn": "2021-07-13T01:59:35.095Z",
          "wordCount": 616,
          "title": "Trans-SVNet: Accurate Phase Recognition from Surgical Videos via Hybrid Embedding Aggregation Transformer. (arXiv:2103.09712v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Huayi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhimeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1\">Xiaorong Pu</a>",
          "description": "Multi-view clustering is an important research topic due to its capability to\nutilize complementary information from multiple views. However, there are few\nmethods to consider the negative impact caused by certain views with unclear\nclustering structures, resulting in poor multi-view clustering performance. To\naddress this drawback, we propose self-supervised discriminative feature\nlearning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders\nare applied to learn embedded features for each view independently. To leverage\nthe multi-view complementary information, we concatenate all views' embedded\nfeatures to form the global features, which can overcome the negative impact of\nsome views' unclear clustering structures. In a self-supervised manner,\npseudo-labels are obtained to build a unified target distribution to perform\nmulti-view discriminative feature learning. During this process, global\ndiscriminative information can be mined to supervise all views to learn more\ndiscriminative features, which in turn are used to update the target\ndistribution. Besides, this unified target distribution can make SDMVC learn\nconsistent cluster assignments, which accomplishes the clustering consistency\nof multiple views while preserving their features' diversity. Experiments on\nvarious types of multi-view datasets show that SDMVC achieves state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2103.15069",
          "publishedOn": "2021-07-13T01:59:35.075Z",
          "wordCount": 656,
          "title": "Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering. (arXiv:2103.15069v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04708",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jicong Zhang</a>",
          "description": "The success of deep learning methods in medical image segmentation tasks\nusually requires a large amount of labeled data. However, obtaining reliable\nannotations is expensive and time-consuming. Semi-supervised learning has\nattracted much attention in medical image segmentation by taking the advantage\nof unlabeled data which is much easier to acquire. In this paper, we propose a\nnovel dual-task mutual learning framework for semi-supervised medical image\nsegmentation. Our framework can be formulated as an integration of two\nindividual segmentation networks based on two tasks: learning region-based\nshape constraint and learning boundary-based surface mismatch. Different from\nthe one-way transfer between teacher and student networks, an ensemble of\ndual-task students can learn collaboratively and implicitly explore useful\nknowledge from each other during the training process. By jointly learning the\nsegmentation probability maps and signed distance maps of targets, our\nframework can enforce the geometric shape constraint and learn more reliable\ninformation. Experimental results demonstrate that our method achieves\nperformance gains by leveraging unlabeled data and outperforms the\nstate-of-the-art semi-supervised segmentation methods.",
          "link": "http://arxiv.org/abs/2103.04708",
          "publishedOn": "2021-07-13T01:59:35.060Z",
          "wordCount": 631,
          "title": "Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation. (arXiv:2103.04708v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruizhi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1\">Daniel Moyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Miriam Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quigley_K/0/1/0/all/0/1\">Keegan Quigley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkowitz_S/0/1/0/all/0/1\">Seth Berkowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horng_S/0/1/0/all/0/1\">Steven Horng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William M. Wells</a>",
          "description": "We propose and demonstrate a representation learning approach by maximizing\nthe mutual information between local features of images and text. The goal of\nthis approach is to learn useful image representations by taking advantage of\nthe rich information contained in the free text that describes the findings in\nthe image. Our method trains image and text encoders by encouraging the\nresulting representations to exhibit high local mutual information. We make use\nof recent advances in mutual information estimation with neural network\ndiscriminators. We argue that the sum of local mutual information is typically\na lower bound on the global mutual information. Our experimental results in the\ndownstream image classification tasks demonstrate the advantages of using local\nfeatures for image-text representation learning.",
          "link": "http://arxiv.org/abs/2103.04537",
          "publishedOn": "2021-07-13T01:59:35.053Z",
          "wordCount": 615,
          "title": "Multimodal Representation Learning via Maximization of Local Mutual Information. (arXiv:2103.04537v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "Object detection has recently achieved a breakthrough for removing the last\none non-differentiable component in the pipeline, Non-Maximum Suppression\n(NMS), and building up an end-to-end system. However, what makes for its\none-to-one prediction has not been well understood. In this paper, we first\npoint out that one-to-one positive sample assignment is the key factor, while,\none-to-many assignment in previous detectors causes redundant predictions in\ninference. Second, we surprisingly find that even training with one-to-one\nassignment, previous detectors still produce redundant predictions. We identify\nthat classification cost in matching cost is the main ingredient: (1) previous\ndetectors only consider location cost, (2) by additionally introducing\nclassification cost, previous detectors immediately produce one-to-one\nprediction during inference. We introduce the concept of score gap to explore\nthe effect of matching cost. Classification cost enlarges the score gap by\nchoosing positive samples as those of highest score in the training iteration\nand reducing noisy positive samples brought by only location cost. Finally, we\ndemonstrate the advantages of end-to-end object detection on crowded scenes.\nThe code is available at: \\url{https://github.com/PeizeSun/OneNet}.",
          "link": "http://arxiv.org/abs/2012.05780",
          "publishedOn": "2021-07-13T01:59:35.036Z",
          "wordCount": 648,
          "title": "What Makes for End-to-End Object Detection?. (arXiv:2012.05780v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roldao_L/0/1/0/all/0/1\">Luis Roldao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verroust_Blondet_A/0/1/0/all/0/1\">Anne Verroust-Blondet</a>",
          "description": "Semantic Scene Completion (SSC) aims to jointly estimate the complete\ngeometry and semantics of a scene, assuming partial sparse input. In the last\nyears following the multiplication of large-scale 3D datasets, SSC has gained\nsignificant momentum in the research community because it holds unresolved\nchallenges. Specifically, SSC lies in the ambiguous completion of large\nunobserved areas and the weak supervision signal of the ground truth. This led\nto a substantially increasing number of papers on the matter. This survey aims\nto identify, compare and analyze the techniques providing a critical analysis\nof the SSC literature on both methods and datasets. Throughout the paper, we\nprovide an in-depth analysis of the existing works covering all choices made by\nthe authors while highlighting the remaining avenues of research. SSC\nperformance of the SoA on the most popular datasets is also evaluated and\nanalyzed.",
          "link": "http://arxiv.org/abs/2103.07466",
          "publishedOn": "2021-07-13T01:59:35.030Z",
          "wordCount": 616,
          "title": "3D Semantic Scene Completion: a Survey. (arXiv:2103.07466v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desmarais_Y/0/1/0/all/0/1\">Yann Desmarais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottet_D/0/1/0/all/0/1\">Denis Mottet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slangen_P/0/1/0/all/0/1\">Pierre Slangen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesinos_P/0/1/0/all/0/1\">Philippe Montesinos</a>",
          "description": "Human pose estimation is a very active research field, stimulated by its\nimportant applications in robotics, entertainment or health and sports\nsciences, among others. Advances in convolutional networks triggered noticeable\nimprovements in 2D pose estimation, leading modern 3D markerless motion capture\ntechniques to an average error per joint of 20 mm. However, with the\nproliferation of methods, it is becoming increasingly difficult to make an\ninformed choice. Here, we review the leading human pose estimation methods of\nthe past five years, focusing on metrics, benchmarks and method structures. We\npropose a taxonomy based on accuracy, speed and robustness that we use to\nclassify de methods and derive directions for future research.",
          "link": "http://arxiv.org/abs/2010.06449",
          "publishedOn": "2021-07-13T01:59:35.023Z",
          "wordCount": 595,
          "title": "A review of 3D human pose estimation algorithms for markerless motion capture. (arXiv:2010.06449v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Honghong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Caili Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanjun Wang</a>",
          "description": "In spite of the great progress in human motion prediction, it is still a\nchallenging task to predict those aperiodic and complicated motions. We believe\nthat to capture the correlations among human body components is the key to\nunderstand the human motion. In this paper, we propose a novel multiscale graph\nconvolution network (MGCN) to address this problem. Firstly, we design an\nadaptive multiscale interactional encoding module (MIEM) which is composed of\ntwo sub modules: scale transformation module and scale interaction module to\nlearn the human body correlations. Secondly, we apply a coarse-to-fine decoding\nstrategy to decode the motions sequentially. We evaluate our approach on two\nstandard benchmark datasets for human motion prediction: Human3.6M and CMU\nmotion capture dataset. The experiments show that the proposed approach\nachieves the state-of-the-art performance for both short-term and long-term\nprediction especially in those complicated action category.",
          "link": "http://arxiv.org/abs/2103.10674",
          "publishedOn": "2021-07-13T01:59:35.017Z",
          "wordCount": 622,
          "title": "Learning Multiscale Correlations for Human Motion Prediction. (arXiv:2103.10674v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13557",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chao_H/0/1/0/all/0/1\">Hanqing Chao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xuanang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>",
          "description": "The extensive use of medical CT has raised a public concern over the\nradiation dose to the patient. Reducing the radiation dose leads to increased\nCT image noise and artifacts, which can adversely affect not only the\nradiologists judgement but also the performance of downstream medical image\nanalysis tasks. Various low-dose CT denoising methods, especially the recent\ndeep learning based approaches, have produced impressive results. However, the\nexisting denoising methods are all downstream-task-agnostic and neglect the\ndiverse needs of the downstream applications. In this paper, we introduce a\nnovel Task-Oriented Denoising Network (TOD-Net) with a task-oriented loss\nleveraging knowledge from the downstream tasks. Comprehensive empirical\nanalysis shows that the task-oriented loss complements other task agnostic\nlosses by steering the denoiser to enhance the image quality in the task\nrelated regions of interest. Such enhancement in turn brings general boosts on\nthe performance of various methods for the downstream task. The presented work\nmay shed light on the future development of context-aware image denoising\nmethods.",
          "link": "http://arxiv.org/abs/2103.13557",
          "publishedOn": "2021-07-13T01:59:35.010Z",
          "wordCount": 629,
          "title": "Task-Oriented Low-Dose CT Image Denoising. (arXiv:2103.13557v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_H/0/1/0/all/0/1\">Helen M. C. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milot_L/0/1/0/all/0/1\">Laurent Milot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>",
          "description": "Colorectal cancer is one of the most common and lethal cancers and colorectal\ncancer liver metastases (CRLM) is the major cause of death in patients with\ncolorectal cancer. Multifocality occurs frequently in CRLM, but is relatively\nunexplored in CRLM outcome prediction. Most existing clinical and imaging\nbiomarkers do not take the imaging features of all multifocal lesions into\naccount. In this paper, we present an end-to-end autoencoder-based multiple\ninstance neural network (AMINN) for the prediction of survival outcomes in\nmultifocal CRLM patients using radiomic features extracted from\ncontrast-enhanced MRIs. Specifically, we jointly train an autoencoder to\nreconstruct input features and a multiple instance network to make predictions\nby aggregating information from all tumour lesions of a patient. Also, we\nincorporate a two-step normalization technique to improve the training of deep\nneural networks, built on the observation that the distributions of radiomic\nfeatures are almost always severely skewed. Experimental results empirically\nvalidated our hypothesis that incorporating imaging features of all lesions\nimproves outcome prediction for multifocal cancer. The proposed AMINN framework\nachieved an area under the ROC curve (AUC) of 0.70, which is 11.4% higher than\nthe best baseline method. A risk score based on the outputs of AMINN achieved\nsuperior prediction in our multifocal CRLM cohort. The effectiveness of\nincorporating all lesions and applying two-step normalization is demonstrated\nby a series of ablation studies. A Keras implementation of AMINN is released.",
          "link": "http://arxiv.org/abs/2012.06875",
          "publishedOn": "2021-07-13T01:59:34.991Z",
          "wordCount": 712,
          "title": "AMINN: Autoencoder-based Multiple Instance Neural Network Improves Outcome Prediction of Multifocal Liver Metastases. (arXiv:2012.06875v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>",
          "description": "Visual object tracking aims to precisely estimate the bounding box for the\ngiven target, which is a challenging problem due to factors such as deformation\nand occlusion. Many recent trackers adopt the multiple-stage tracking strategy\nto improve the quality of bounding box estimation. These methods first coarsely\nlocate the target and then refine the initial prediction in the following\nstages. However, existing approaches still suffer from limited precision, and\nthe coupling of different stages severely restricts the method's\ntransferability. This work proposes a novel, flexible, and accurate refinement\nmodule called Alpha-Refine (AR), which can significantly improve the base\ntrackers' box estimation quality. By exploring a series of design options, we\nconclude that the key to successful refinement is extracting and maintaining\ndetailed spatial information as much as possible. Following this principle,\nAlpha-Refine adopts a pixel-wise correlation, a corner prediction head, and an\nauxiliary mask head as the core components. Comprehensive experiments on\nTrackingNet, LaSOT, GOT-10K, and VOT2020 benchmarks with multiple base trackers\nshow that our approach significantly improves the base trackers' performance\nwith little extra latency. The proposed Alpha-Refine method leads to a series\nof strengthened trackers, among which the ARSiamRPN (AR strengthened SiamRPNpp)\nand the ARDiMP50 (ARstrengthened DiMP50) achieve good efficiency-precision\ntrade-off, while the ARDiMPsuper (AR strengthened DiMP-super) achieves very\ncompetitive performance at a real-time speed. Code and pretrained models are\navailable at https://github.com/MasterBin-IIAU/AlphaRefine.",
          "link": "http://arxiv.org/abs/2012.06815",
          "publishedOn": "2021-07-13T01:59:34.985Z",
          "wordCount": 713,
          "title": "Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation. (arXiv:2012.06815v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>",
          "description": "Recently, many plug-and-play self-attention modules are proposed to enhance\nthe model generalization by exploiting the internal information of deep\nconvolutional neural networks (CNNs). Previous works lay an emphasis on the\ndesign of attention module for specific functionality, e.g., light-weighted or\ntask-oriented attention. However, they ignore the importance of where to plug\nin the attention module since they connect the modules individually with each\nblock of the entire CNN backbone for granted, leading to incremental\ncomputational cost and number of parameters with the growth of network depth.\nThus, we propose a framework called Efficient Attention Network (EAN) to\nimprove the efficiency for the existing attention modules. In EAN, we leverage\nthe sharing mechanism (Huang et al. 2020) to share the attention module within\nthe backbone and search where to connect the shared attention module via\nreinforcement learning. Finally, we obtain the attention network with sparse\nconnections between the backbone and modules, while (1) maintaining accuracy\n(2) reducing extra parameter increment and (3) accelerating inference.\nExtensive experiments on widely-used benchmarks and popular attention networks\nshow the effectiveness of EAN. Furthermore, we empirically illustrate that our\nEAN has the capacity of transferring to other tasks and capturing the\ninformative features. The code is available at\nhttps://github.com/gbup-group/EAN-efficient-attention-network.",
          "link": "http://arxiv.org/abs/2011.14058",
          "publishedOn": "2021-07-13T01:59:34.979Z",
          "wordCount": 685,
          "title": "Efficient Attention Network: Accelerate Attention by Searching Where to Plug. (arXiv:2011.14058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otsuzuki_T/0/1/0/all/0/1\">Takato Otsuzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Heon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hideaki Hayashi</a>",
          "description": "In convolutional neural network-based character recognition, pooling layers\nplay an important role in dimensionality reduction and deformation\ncompensation. However, their kernel shapes and pooling operations are\nempirically predetermined; typically, a fixed-size square kernel shape and max\npooling operation are used. In this paper, we propose a meta-learning framework\nfor pooling layers. As part of our framework, a parameterized pooling layer is\nproposed in which the kernel shape and pooling operation are trainable using\ntwo parameters, thereby allowing flexible pooling of the input data. We also\npropose a meta-learning algorithm for the parameterized pooling layer, which\nallows us to acquire a suitable pooling layer across multiple tasks. In the\nexperiment, we applied the proposed meta-learning framework to character\nrecognition tasks. The results demonstrate that a pooling layer that is\nsuitable across character recognition tasks was obtained via meta-learning, and\nthe obtained pooling layer improved the performance of the model in both\nfew-shot character recognition and noisy image recognition tasks.",
          "link": "http://arxiv.org/abs/2103.09528",
          "publishedOn": "2021-07-13T01:59:34.972Z",
          "wordCount": 627,
          "title": "Meta-learning of Pooling Layers for Character Recognition. (arXiv:2103.09528v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingquan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_G/0/1/0/all/0/1\">George Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>",
          "description": "Chest X-ray becomes one of the most common medical diagnoses due to its\nnoninvasiveness. The number of chest X-ray images has skyrocketed, but reading\nchest X-rays still have been manually performed by radiologists, which creates\nhuge burnouts and delays. Traditionally, radiomics, as a subfield of radiology\nthat can extract a large number of quantitative features from medical images,\ndemonstrates its potential to facilitate medical imaging diagnosis before the\ndeep learning era. In this paper, we develop an end-to-end framework,\nChexRadiNet, that can utilize the radiomics features to improve the abnormality\nclassification performance. Specifically, ChexRadiNet first applies a\nlight-weight but efficient triplet-attention mechanism to classify the chest\nX-rays and highlight the abnormal regions. Then it uses the generated class\nactivation map to extract radiomic features, which further guides our model to\nlearn more robust image features. After a number of iterations and with the\nhelp of radiomic features, our framework can converge to more accurate image\nregions. We evaluate the ChexRadiNet framework using three public datasets: NIH\nChestX-ray, CheXpert, and MIMIC-CXR. We find that ChexRadiNet outperforms the\nstate-of-the-art on both disease detection (0.843 in AUC) and localization\n(0.679 in T(IoU) = 0.1). We will make the code publicly available at\nhttps://github.com/bionlplab/lung_disease_detection_amia2021, with the hope\nthat this method can facilitate the development of automatic systems with a\nhigher-level understanding of the radiological world.",
          "link": "http://arxiv.org/abs/2011.12506",
          "publishedOn": "2021-07-13T01:59:34.967Z",
          "wordCount": 726,
          "title": "Using Radiomics as Prior Knowledge for Thorax Disease Classification and Localization in Chest X-rays. (arXiv:2011.12506v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bugata_P/0/1/0/all/0/1\">Peter Bugata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drotar_P/0/1/0/all/0/1\">Peter Drotar</a>",
          "description": "Feature selection is important step in machine learning since it has shown to\nimprove prediction accuracy while depressing the curse of dimensionality of\nhigh dimensional data. The neural networks have experienced tremendous success\nin solving many nonlinear learning problems. Here, we propose new\nneural-network based feature selection approach that introduces two constrains,\nthe satisfying of which leads to sparse FS layer. We have performed extensive\nexperiments on synthetic and real world data to evaluate performance of the\nproposed FS. In experiments we focus on the high dimension, low sample size\ndata since those represent the main challenge for feature selection. The\nresults confirm that proposed Feature Selection Based on Sparse Neural Network\nLayer with Normalizing Constraints (SNEL-FS) is able to select the important\nfeatures and yields superior performance compared to other conventional FS\nmethods.",
          "link": "http://arxiv.org/abs/2012.06365",
          "publishedOn": "2021-07-13T01:59:34.949Z",
          "wordCount": 610,
          "title": "Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints. (arXiv:2012.06365v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.11948",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xianhong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arce_G/0/1/0/all/0/1\">Gonzalo R. Arce</a>",
          "description": "Hyperspectral image classification (HIC) is an active research topic in\nremote sensing. Hyperspectral images typically generate large data cubes posing\nbig challenges in data acquisition, storage, transmission and processing. To\novercome these limitations, this paper develops a novel deep learning HIC\napproach based on compressive measurements of coded-aperture snapshot spectral\nimagers (CASSI), without reconstructing the complete hyperspectral data cube. A\nnew kind of deep learning strategy, namely 3D coded convolutional neural\nnetwork (3D-CCNN) is proposed to efficiently solve for the classification\nproblem, where the hardware-based coded aperture is regarded as a pixel-wise\nconnected network layer. An end-to-end training method is developed to jointly\noptimize the network parameters and the coded apertures with periodic\nstructures. The accuracy of classification is effectively improved by\nexploiting the synergy between the deep learning network and coded apertures.\nThe superiority of the proposed method is assessed over the state-of-the-art\nHIC methods on several hyperspectral datasets.",
          "link": "http://arxiv.org/abs/2009.11948",
          "publishedOn": "2021-07-13T01:59:34.943Z",
          "wordCount": 625,
          "title": "Compressive spectral image classification using 3D coded convolutional neural network. (arXiv:2009.11948v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Suncheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuzhuo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1\">Mengyuan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>",
          "description": "Employing clustering strategy to assign unlabeled target images with pseudo\nlabels has become a trend for person re-identification (re-ID) algorithms in\ndomain adaptation. A potential limitation of these clustering-based methods is\nthat they always tend to introduce noisy labels, which will undoubtedly hamper\nthe performance of our re-ID system. To handle this limitation, an intuitive\nsolution is to utilize collaborative training to purify the pseudo label\nquality. However, there exists a challenge that the complementarity of two\nnetworks, which inevitably share a high similarity, becomes weakened gradually\nas training process goes on; worse still, these approaches typically ignore to\nconsider the self-discrepancy of intra-class relations. To address this issue,\nin this paper, we propose a multiple co-teaching framework for domain adaptive\nperson re-ID, opening up a promising direction about self-discrepancy problem\nunder unsupervised condition. On top of that, a mean-teaching mechanism is\nleveraged to enlarge the difference and discover more complementary features.\nComprehensive experiments conducted on several large-scale datasets show that\nour method achieves competitive performance compared with the\nstate-of-the-arts.",
          "link": "http://arxiv.org/abs/2104.02265",
          "publishedOn": "2021-07-13T01:59:34.936Z",
          "wordCount": 636,
          "title": "Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoxiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a novel method for multi-view depth estimation from a single\nvideo, which is a critical task in various applications, such as perception,\nreconstruction and robot navigation. Although previous learning-based methods\nhave demonstrated compelling results, most works estimate depth maps of\nindividual video frames independently, without taking into consideration the\nstrong geometric and temporal coherence among the frames. Moreover, current\nstate-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for\ncost regularization and therefore require high computational cost, thus\nlimiting their deployment in real-world applications. Our method achieves\ntemporally coherent depth estimation results by using a novel Epipolar\nSpatio-Temporal (EST) transformer to explicitly associate geometric and\ntemporal correlation with multiple estimated depth maps. Furthermore, to reduce\nthe computational cost, inspired by recent Mixture-of-Experts models, we design\na compact hybrid network consisting of a 2D context-aware network and a 3D\nmatching network which learn 2D context information and 3D disparity cues\nseparately. Extensive experiments demonstrate that our method achieves higher\naccuracy in depth estimation and significant speedup than the SOTA methods.",
          "link": "http://arxiv.org/abs/2011.13118",
          "publishedOn": "2021-07-13T01:59:34.929Z",
          "wordCount": 646,
          "title": "Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks. (arXiv:2011.13118v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Variational autoencoders (VAEs) provide an effective and simple method for\nmodeling complex distributions. However, training VAEs often requires\nconsiderable hyperparameter tuning to determine the optimal amount of\ninformation retained by the latent variable. We study the impact of calibrated\ndecoders, which learn the uncertainty of the decoding distribution and can\ndetermine this amount of information automatically, on the VAE performance.\nWhile many methods for learning calibrated decoders have been proposed, many of\nthe recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc\nmodifications instead. We perform the first comprehensive comparative analysis\nof calibrated decoder and provide recommendations for simple and effective VAE\ntraining. Our analysis covers a range of image and video datasets and several\nsingle-image and sequential VAE models. We further propose a simple but novel\nmodification to the commonly used Gaussian decoder, which computes the\nprediction variance analytically. We observe empirically that using heuristic\nmodifications is not necessary with our method. Project website is at\nhttps://orybkin.github.io/sigma-vae/",
          "link": "http://arxiv.org/abs/2006.13202",
          "publishedOn": "2021-07-13T01:59:34.921Z",
          "wordCount": 661,
          "title": "Simple and Effective VAE Training with Calibrated Decoders. (arXiv:2006.13202v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaming Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "The past decade has witnessed significant progress on detecting objects in\naerial images that are often distributed with large scale variations and\narbitrary orientations. However most of existing methods rely on heuristically\ndefined anchors with different scales, angles and aspect ratios and usually\nsuffer from severe misalignment between anchor boxes and axis-aligned\nconvolutional features, which leads to the common inconsistency between the\nclassification score and localization accuracy. To address this issue, we\npropose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules:\na Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The\nFAM can generate high-quality anchors with an Anchor Refinement Network and\nadaptively align the convolutional features according to the anchor boxes with\na novel Alignment Convolution. The ODM first adopts active rotating filters to\nencode the orientation information and then produces orientation-sensitive and\norientation-invariant features to alleviate the inconsistency between\nclassification score and localization accuracy. Besides, we further explore the\napproach to detect objects in large-size images, which leads to a better\ntrade-off between speed and accuracy. Extensive experiments demonstrate that\nour method can achieve state-of-the-art performance on two commonly used aerial\nobjects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The\ncode is available at https://github.com/csuhan/s2anet.",
          "link": "http://arxiv.org/abs/2008.09397",
          "publishedOn": "2021-07-13T01:59:34.903Z",
          "wordCount": 675,
          "title": "Align Deep Features for Oriented Object Detection. (arXiv:2008.09397v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>",
          "description": "We present a new learning-based framework to recover vehicle pose in SO(3)\nfrom a single RGB image. In contrast to previous works that map from local\nappearance to observation angles, we explore a progressive approach by\nextracting meaningful Intermediate Geometrical Representations (IGRs) to\nestimate egocentric vehicle orientation. This approach features a deep model\nthat transforms perceived intensities to IGRs, which are mapped to a 3D\nrepresentation encoding object orientation in the camera coordinate system.\nCore problems are what IGRs to use and how to learn them more effectively. We\nanswer the former question by designing IGRs based on an interpolated cuboid\nthat derives from primitive 3D annotation readily. The latter question\nmotivates us to incorporate geometry knowledge with a new loss function based\non a projective invariant. This loss function allows unlabeled data to be used\nin the training stage to improve representation learning. Without additional\nlabels, our system outperforms previous monocular RGB-based methods for joint\nvehicle detection and pose estimation on the KITTI benchmark, achieving\nperformance even comparable to stereo methods. Code and pre-trained models are\navailable at this https URL.",
          "link": "http://arxiv.org/abs/2011.08464",
          "publishedOn": "2021-07-13T01:59:34.897Z",
          "wordCount": 687,
          "title": "Exploring intermediate representation for monocular vehicle pose estimation. (arXiv:2011.08464v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1\">Stephen J. Maybank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Extracting accurate foregrounds from natural images benefits many downstream\napplications such as film production and augmented reality. However, the furry\ncharacteristics and various appearance of the foregrounds, e.g., animal and\nportrait, challenge existing matting methods, which usually require extra user\ninputs such as trimap or scribbles. To resolve these problems, we study the\ndistinct roles of semantics and details for image matting and decompose the\ntask into two parallel sub-tasks: high-level semantic segmentation and\nlow-level details matting. Specifically, we propose a novel Glance and Focus\nMatting network (GFM), which employs a shared encoder and two separate decoders\nto learn both tasks in a collaborative manner for end-to-end natural image\nmatting. Besides, due to the limitation of available natural images in the\nmatting task, previous methods typically adopt composite images for training\nand evaluation, which result in limited generalization ability on real-world\nimages. In this paper, we investigate the domain gap issue between composite\nimages and real-world images systematically by conducting comprehensive\nanalyses of various discrepancies between foreground and background images. We\nfind that a carefully designed composition route RSSN that aims to reduce the\ndiscrepancies can lead to a better model with remarkable generalization\nability. Furthermore, we provide a benchmark containing 2,000 high-resolution\nreal-world animal images and 10,000 portrait images along with their manually\nlabeled alpha mattes to serve as a test bed for evaluating matting model's\ngeneralization ability on real-world images. Comprehensive empirical studies\nhave demonstrated that GFM outperforms state-of-the-art methods and effectively\nreduces the generalization error. The code and the dataset will be released.",
          "link": "http://arxiv.org/abs/2010.16188",
          "publishedOn": "2021-07-13T01:59:34.814Z",
          "wordCount": 749,
          "title": "Bridge Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miro_Nicolau_M/0/1/0/all/0/1\">Miquel Mir&#xf3;-Nicolau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moya_Alcover_B/0/1/0/all/0/1\">Biel Moy&#xe0;-Alcover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Hidalgo_M/0/1/0/all/0/1\">Manuel Gonz&#xe1;lez-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaume_i_Capo_A/0/1/0/all/0/1\">Antoni Jaume-i-Cap&#xf3;</a>",
          "description": "In this paper we propose a method to detect concave points as a first step to\nsegment overlapped objects on images. Given an image of an object cluster we\ncompute the curvature on each point of its contour. Then, we select regions\nwith the highest probability to contain an interest point, that is, regions\nwith higher curvature. Finally we obtain an interest point from each region and\nwe classify them between convex and concave. In order to evaluate the quality\nof the concave point detection algorithm we constructed a synthetic dataset to\nsimulate overlapping objects, providing the position of the concave points as a\nground truth. As a case study, the performance of a well-known application is\nevaluated, such as the splitting of overlapped cells in images of peripheral\nblood smears samples of patients with sickle cell anaemia. We used the proposed\nmethod to detect the concave points in clusters of cells and then we separate\nthis clusters by ellipse fitting. Experimentally we demonstrate that our\nproposal has a better performance than the state-of-the-art.",
          "link": "http://arxiv.org/abs/2008.00997",
          "publishedOn": "2021-07-13T01:59:34.807Z",
          "wordCount": 651,
          "title": "Segmenting overlapped cell clusters in biomedical images by concave point detection. (arXiv:2008.00997v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Di Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harakeh_A/0/1/0/all/0/1\">Ali Harakeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven Waslander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>",
          "description": "Capturing uncertainty in object detection is indispensable for safe\nautonomous driving. In recent years, deep learning has become the de-facto\napproach for object detection, and many probabilistic object detectors have\nbeen proposed. However, there is no summary on uncertainty estimation in deep\nobject detection, and existing methods are not only built with different\nnetwork architectures and uncertainty estimation methods, but also evaluated on\ndifferent datasets with a wide range of evaluation metrics. As a result, a\ncomparison among methods remains challenging, as does the selection of a model\nthat best suits a particular application. This paper aims to alleviate this\nproblem by providing a review and comparative study on existing probabilistic\nobject detection methods for autonomous driving applications. First, we provide\nan overview of generic uncertainty estimation in deep learning, and then\nsystematically survey existing methods and evaluation metrics for probabilistic\nobject detection. Next, we present a strict comparative study for probabilistic\nobject detection based on an image detector and three public autonomous driving\ndatasets. Finally, we present a discussion of the remaining challenges and\nfuture works. Code has been made available at\nhttps://github.com/asharakeh/pod_compare.git",
          "link": "http://arxiv.org/abs/2011.10671",
          "publishedOn": "2021-07-13T01:59:34.800Z",
          "wordCount": 674,
          "title": "A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving. (arXiv:2011.10671v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.13373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaeseok Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yeji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Data augmentation has greatly contributed to improving the performance in\nimage recognition tasks, and a lot of related studies have been conducted.\nHowever, data augmentation on 3D point cloud data has not been much explored.\n3D label has more sophisticated and rich structural information than the 2D\nlabel, so it enables more diverse and effective data augmentation. In this\npaper, we propose part-aware data augmentation (PA-AUG) that can better utilize\nrich information of 3D label to enhance the performance of 3D object detectors.\nPA-AUG divides objects into partitions and stochastically applies five\naugmentation methods to each local region. It is compatible with existing point\ncloud data augmentation methods and can be used universally regardless of the\ndetector's architecture. PA-AUG has improved the performance of\nstate-of-the-art 3D object detector for all classes of the KITTI dataset and\nhas the equivalent effect of increasing the train data by about 2.5$\\times$. We\nalso show that PA-AUG not only increases performance for a given dataset but\nalso is robust to corrupted data. The code is available at\nhttps://github.com/sky77764/pa-aug.pytorch",
          "link": "http://arxiv.org/abs/2007.13373",
          "publishedOn": "2021-07-13T01:59:34.782Z",
          "wordCount": 650,
          "title": "Part-Aware Data Augmentation for 3D Object Detection in Point Cloud. (arXiv:2007.13373v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tete Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mintun_E/0/1/0/all/0/1\">Eric Mintun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1\">Piotr Doll&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1\">Ross Girshick</a>",
          "description": "Vision transformer (ViT) models exhibit substandard optimizability. In\nparticular, they are sensitive to the choice of optimizer (AdamW vs. SGD),\noptimizer hyperparameters, and training schedule length. In comparison, modern\nconvolutional neural networks are far easier to optimize. Why is this the case?\nIn this work, we conjecture that the issue lies with the patchify stem of ViT\nmodels, which is implemented by a stride-p pxp convolution (p=16 by default)\napplied to the input image. This large-kernel plus large-stride convolution\nruns counter to typical design choices of convolutional layers in neural\nnetworks. To test whether this atypical design choice causes an issue, we\nanalyze the optimization behavior of ViT models with their original patchify\nstem versus a simple counterpart where we replace the ViT stem by a small\nnumber of stacked stride-two 3x3 convolutions. While the vast majority of\ncomputation in the two ViT designs is identical, we find that this small change\nin early visual processing results in markedly different training behavior in\nterms of the sensitivity to optimization settings as well as the final model\naccuracy. Using a convolutional stem in ViT dramatically increases optimization\nstability and also improves peak performance (by ~1-2% top-1 accuracy on\nImageNet-1k), while maintaining flops and runtime. The improvement can be\nobserved across the wide spectrum of model complexities (from 1G to 36G flops)\nand dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us\nto recommend using a standard, lightweight convolutional stem for ViT models as\na more robust architectural choice compared to the original ViT model design.",
          "link": "http://arxiv.org/abs/2106.14881",
          "publishedOn": "2021-07-13T01:59:34.770Z",
          "wordCount": 721,
          "title": "Early Convolutions Help Transformers See Better. (arXiv:2106.14881v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.00422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_D/0/1/0/all/0/1\">Deepak Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_V/0/1/0/all/0/1\">Vivek Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulluri_T/0/1/0/all/0/1\">Tarun Pulluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ussa_A/0/1/0/all/0/1\">Andres Ussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_P/0/1/0/all/0/1\">Pradeep Kumar Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_B/0/1/0/all/0/1\">Bharath Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Arindam Basu</a>",
          "description": "Neuromorphic vision sensors (NVS) have been recently explored to tackle\nscenarios where conventional sensors result in high data rate and processing\ntime. This paper presents a hybrid event-frame approach for detecting and\ntracking objects recorded by a stationary neuromorphic sensor, thereby\nexploiting the sparse NVS output in a low-power setting for traffic monitoring.\nSpecifically, we propose a hardware efficient processing pipeline that\noptimizes memory and computational needs. The usage of NVS gives the advantage\nof rejecting background while it has a unique disadvantage of fragmented\nobjects. To exploit the background removal, we propose an event-based binary\nimage creation that signals presence or absence of events in a frame duration.\nThis reduces memory requirement and enables usage of simple algorithms like\nmedian filtering and connected component labeling for denoise and region\nproposal respectively. To overcome the fragmentation issue, a YOLO-inspired\nneural network based detector and classifier to merge fragmented region\nproposals has been proposed. Finally, an overlap based tracker exploiting\noverlap between detections and tracks is proposed with heuristics to overcome\nocclusion. The proposed pipeline is evaluated with more than 5 hours of traffic\nrecording spanning three different locations on two different NVS and\ndemonstrate similar performance. Compared to existing event-based feature\ntrackers, our method provides similar accuracy while needing 6 times less\ncomputes. To the best of our knowledge, this is the first time a stationary NVS\nbased traffic monitoring solution is extensively compared to simultaneously\nrecorded RGB frame-methods while showing tremendous promise by outperforming\nstate-of-the-art deep learning solutions.",
          "link": "http://arxiv.org/abs/2006.00422",
          "publishedOn": "2021-07-13T01:59:34.740Z",
          "wordCount": 740,
          "title": "EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Neuromorphic Vision Sensors. (arXiv:2006.00422v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09646",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this paper, we present a novel network for high resolution video\ngeneration. Our network uses ideas from Wasserstein GANs by enforcing\nk-Lipschitz constraint on the loss term and Conditional GANs using class labels\nfor training and testing. We present Generator and Discriminator network\nlayerwise details along with the combined network architecture, optimization\ndetails and algorithm used in this work. Our network uses a combination of two\nloss terms: mean square pixel loss and an adversarial loss. The datasets used\nfor training and testing our network are UCF101, Golf and Aeroplane Datasets.\nUsing Inception Score and Fr\\'echet Inception Distance as the evaluation\nmetrics, our network outperforms previous state of the art networks on\nunsupervised video generation.",
          "link": "http://arxiv.org/abs/2008.09646",
          "publishedOn": "2021-07-13T01:59:34.734Z",
          "wordCount": 596,
          "title": "HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN. (arXiv:2008.09646v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.00845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoxiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a new learning-based method for multi-frame depth estimation from\na color video, which is a fundamental problem in scene understanding, robot\nnavigation or handheld 3D reconstruction. While recent learning-based methods\nestimate depth at high accuracy, 3D point clouds exported from their depth maps\noften fail to preserve important geometric feature (e.g., corners, edges,\nplanes) of man-made scenes. Widely-used pixel-wise depth errors do not\nspecifically penalize inconsistency on these features. These inaccuracies are\nparticularly severe when subsequent depth reconstructions are accumulated in an\nattempt to scan a full environment with man-made objects with this kind of\nfeatures. Our depth estimation algorithm therefore introduces a Combined Normal\nMap (CNM) constraint, which is designed to better preserve high-curvature\nfeatures and global planar regions. In order to further improve the depth\nestimation accuracy, we introduce a new occlusion-aware strategy that\naggregates initial depth predictions from multiple adjacent views into one\nfinal depth map and one occlusion probability map for the current reference\nview. Our method outperforms the state-of-the-art in terms of depth estimation\naccuracy, and preserves essential geometric features of man-made indoor scenes\nmuch better than other algorithms.",
          "link": "http://arxiv.org/abs/2004.00845",
          "publishedOn": "2021-07-13T01:59:34.728Z",
          "wordCount": 684,
          "title": "Occlusion-Aware Depth Estimation with Adaptive Normal Constraints. (arXiv:2004.00845v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12931",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_T/0/1/0/all/0/1\">Tashin Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabab_N/0/1/0/all/0/1\">Noor Hossain Nuri Sabab</a>",
          "description": "Climate change has been a common interest and the forefront of crucial\npolitical discussion and decision-making for many years. Shallow clouds play a\nsignificant role in understanding the Earth's climate, but they are challenging\nto interpret and represent in a climate model. By classifying these cloud\nstructures, there is a better possibility of understanding the physical\nstructures of the clouds, which would improve the climate model generation,\nresulting in a better prediction of climate change or forecasting weather\nupdate. Clouds organise in many forms, which makes it challenging to build\ntraditional rule-based algorithms to separate cloud features. In this paper,\nclassification of cloud organization patterns was performed using a new\nscaled-up version of Convolutional Neural Network (CNN) named as EfficientNet\nas the encoder and UNet as decoder where they worked as feature extractor and\nreconstructor of fine grained feature map and was used as a classifier, which\nwill help experts to understand how clouds will shape the future climate. By\nusing a segmentation model in a classification task, it was shown that with a\ngood encoder alongside UNet, it is possible to obtain good performance from\nthis dataset. Dice coefficient has been used for the final evaluation metric,\nwhich gave the score of 66.26\\% and 66.02\\% for public and private (test set)\nleaderboard on Kaggle competition respectively.",
          "link": "http://arxiv.org/abs/2009.12931",
          "publishedOn": "2021-07-13T01:59:34.716Z",
          "wordCount": 708,
          "title": "Classification and understanding of cloud structures via satellite images with EfficientUNet. (arXiv:2009.12931v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.13200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Compared with cheap addition operation, multiplication operation is of much\nhigher computation complexity. The widely-used convolutions in deep neural\nnetworks are exactly cross-correlation to measure the similarity between input\nfeature and convolution filters, which involves massive multiplications between\nfloat values. In this paper, we present adder networks (AdderNets) to trade\nthese massive multiplications in deep neural networks, especially convolutional\nneural networks (CNNs), for much cheaper additions to reduce computation costs.\nIn AdderNets, we take the $\\ell_1$-norm distance between filters and input\nfeature as the output response. The influence of this new similarity measure on\nthe optimization of neural network have been thoroughly analyzed. To achieve a\nbetter performance, we develop a special back-propagation approach for\nAdderNets by investigating the full-precision gradient. We then propose an\nadaptive learning rate strategy to enhance the training procedure of AdderNets\naccording to the magnitude of each neuron's gradient. As a result, the proposed\nAdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50\non the ImageNet dataset without any multiplication in convolution layer. The\ncodes are publicly available at: https://github.com/huaweinoah/AdderNet.",
          "link": "http://arxiv.org/abs/1912.13200",
          "publishedOn": "2021-07-13T01:59:34.692Z",
          "wordCount": 699,
          "title": "AdderNet: Do We Really Need Multiplications in Deep Learning?. (arXiv:1912.13200v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.14536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>",
          "description": "It is commonly believed that networks cannot be both accurate and robust,\nthat gaining robustness means losing accuracy. It is also generally believed\nthat, unless making networks larger, network architectural elements would\notherwise matter little in improving adversarial robustness. Here we present\nevidence to challenge these common beliefs by a careful study about adversarial\ntraining. Our key observation is that the widely-used ReLU activation function\nsignificantly weakens adversarial training due to its non-smooth nature. Hence\nwe propose smooth adversarial training (SAT), in which we replace ReLU with its\nsmooth approximations to strengthen adversarial training. The purpose of smooth\nactivation functions in SAT is to allow it to find harder adversarial examples\nand compute better gradient updates during adversarial training.\n\nCompared to standard adversarial training, SAT improves adversarial\nrobustness for \"free\", i.e., no drop in accuracy and no increase in\ncomputational cost. For example, without introducing additional computations,\nSAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while\nalso improving accuracy by 0.9% on ImageNet. SAT also works well with larger\nnetworks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%\nrobustness on ImageNet, outperforming the previous state-of-the-art defense by\n9.5% for accuracy and 11.6% for robustness. Models are available at\nhttps://github.com/cihangxie/SmoothAdversarialTraining.",
          "link": "http://arxiv.org/abs/2006.14536",
          "publishedOn": "2021-07-13T01:59:34.656Z",
          "wordCount": 674,
          "title": "Smooth Adversarial Training. (arXiv:2006.14536v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chenxu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "To encourage AI agents to conduct meaningful Visual Dialogue (VD), the use of\nReinforcement Learning has been proven potential. In Reinforcement Learning, it\nis crucial to represent states and assign rewards based on the action-caused\ntransitions of states. However, the state representation in previous Visual\nDialogue works uses the textual information only and its transitions are\nimplicit. In this paper, we propose Explicit Concerning States (ECS) to\nrepresent what visual contents are concerned at each round and what have been\nconcerned throughout the Visual Dialogue. ECS is modeled from multimodal\ninformation and is represented explicitly. Based on ECS, we formulate two\nintuitive and interpretable rewards to encourage the Visual Dialogue agents to\nconverse on diverse and informative visual information. Experimental results on\nthe VisDial v1.0 dataset show our method enables the Visual Dialogue agents to\ngenerate more visual coherent, less repetitive and more visual informative\ndialogues compared with previous methods, according to multiple automatic\nmetrics, human study and qualitative analysis.",
          "link": "http://arxiv.org/abs/2107.05250",
          "publishedOn": "2021-07-13T01:59:34.647Z",
          "wordCount": 604,
          "title": "Modeling Explicit Concerning States for Reinforcement Learning in Visual Dialogue. (arXiv:2107.05250v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanpeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">He Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yongming Tang</a>",
          "description": "Video super-resolution (VSR) technology excels in reconstructing low-quality\nvideo, avoiding unpleasant blur effect caused by interpolation-based\nalgorithms. However, vast computation complexity and memory occupation hampers\nthe edge of deplorability and the runtime inference in real-life applications,\nespecially for large-scale VSR task. This paper explores the possibility of\nreal-time VSR system and designs an efficient and generic VSR network, termed\nEGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for\ntemporal coherence. In order to pursue faster VSR processing ability up to 4K\nresolution, this paper tries to choose lightweight network structure and\nefficient upsampling method to reduce the computation required by EGVSR network\nunder the guarantee of high visual quality. Besides, we implement the batch\nnormalization computation fusion, convolutional acceleration algorithm and\nother neural network acceleration techniques on the actual hardware platform to\noptimize the inference process of EGVSR network. Finally, our EGVSR achieves\nthe real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the\nmost advanced VSR network at present, we achieve 85.04% reduction of\ncomputation density and 7.92x performance speedups. In terms of visual quality,\nthe proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP,\netc.) on the public test dataset Vid4 and surpasses other state-of-the-art\nmethods in overall performance score. The source code of this project can be\nfound on https://github.com/Thmen/EGVSR.",
          "link": "http://arxiv.org/abs/2107.05307",
          "publishedOn": "2021-07-13T01:59:34.634Z",
          "wordCount": 676,
          "title": "Real-Time Super-Resolution System of 4K-Video Based on Deep Learning. (arXiv:2107.05307v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1\">Ross Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1\">Pujitha Gunaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "With increasing automation in passenger vehicles, the study of safe and\nsmooth occupant-vehicle interaction and control transitions is key. In this\nstudy, we focus on the development of contextual, semantically meaningful\nrepresentations of the driver state, which can then be used to determine the\nappropriate timing and conditions for transfer of control between driver and\nvehicle. To this end, we conduct a large-scale real-world controlled data study\nwhere participants are instructed to take-over control from an autonomous agent\nunder different driving conditions while engaged in a variety of distracting\nactivities. These take-over events are captured using multiple driver-facing\ncameras, which when labelled result in a dataset of control transitions and\ntheir corresponding take-over times (TOTs). We then develop and train TOT\nmodels that operate sequentially on mid to high-level features produced by\ncomputer vision algorithms operating on different driver-facing camera views.\nThe proposed TOT model produces continuous estimates of take-over times without\ndelay, and shows promising qualitative and quantitative results in complex\nreal-world scenarios.",
          "link": "http://arxiv.org/abs/2104.11489",
          "publishedOn": "2021-07-13T01:59:34.627Z",
          "wordCount": 639,
          "title": "Autonomous Vehicles that Alert Humans to Take-Over Controls: Modeling with Real-World Data. (arXiv:2104.11489v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sallab_A/0/1/0/all/0/1\">Ahmad El Sallab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Moving object Detection (MOD) is a critical task in autonomous driving as\nmoving agents around the ego-vehicle need to be accurately detected for safe\ntrajectory planning. It also enables appearance agnostic detection of objects\nbased on motion cues. There are geometric challenges like motion-parallax\nambiguity which makes it a difficult problem. In this work, we aim to leverage\nthe vehicle motion information and feed it into the model to have an adaptation\nmechanism based on ego-motion. The motivation is to enable the model to\nimplicitly perform ego-motion compensation to improve performance. We convert\nthe six degrees of freedom vehicle motion into a pixel-wise tensor which can be\nfed as input to the CNN model. The proposed model using Vehicle Motion Tensor\n(VMT) achieves an absolute improvement of 5.6% in mIoU over the baseline\narchitecture. We also achieve state-of-the-art results on the public\nKITTI_MoSeg_Extended dataset even compared to methods which make use of LiDAR\nand additional input frames. Our model is also lightweight and runs at 85 fps\non a TitanX GPU. Qualitative results are provided in\nhttps://youtu.be/ezbfjti-kTk.",
          "link": "http://arxiv.org/abs/2104.10985",
          "publishedOn": "2021-07-13T01:59:34.609Z",
          "wordCount": 659,
          "title": "VM-MODNet: Vehicle Motion aware Moving Object Detection for Autonomous Driving. (arXiv:2104.10985v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>",
          "description": "We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n\nHaving all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n\nWe solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article. Just like for linear convolution a\nmorphological convolution is specified by a kernel that we train in our\nPDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling\nand ReLUs as they are already subsumed by morphological convolutions.\n\nWe present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.",
          "link": "http://arxiv.org/abs/2001.09046",
          "publishedOn": "2021-07-13T01:59:34.603Z",
          "wordCount": 751,
          "title": "PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loo_S/0/1/0/all/0/1\">Shing Yan Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mashohor_S/0/1/0/all/0/1\">Syamsiah Mashohor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sai Hong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>",
          "description": "In this paper, we propose a dense monocular SLAM system, named\nDeepRelativeFusion, that is capable to recover a globally consistent 3D\nstructure. To this end, we use a visual SLAM algorithm to reliably recover the\ncamera poses and semi-dense depth maps of the keyframes, and then use relative\ndepth prediction to densify the semi-dense depth maps and refine the keyframe\npose-graph. To improve the semi-dense depth maps, we propose an adaptive\nfiltering scheme, which is a structure-preserving weighted average smoothing\nfilter that takes into account the pixel intensity and depth of the\nneighbouring pixels, yielding substantial reconstruction accuracy gain in\ndensification. To perform densification, we introduce two incremental\nimprovements upon the energy minimization framework proposed by DeepFusion: (1)\nan improved cost function, and (2) the use of single-image relative depth\nprediction. After densification, we update the keyframes with two-view\nconsistent optimized semi-dense and dense depth maps to improve pose-graph\noptimization, providing a feedback loop to refine the keyframe poses for\naccurate scene reconstruction. Our system outperforms the state-of-the-art\ndense SLAM systems quantitatively in dense reconstruction accuracy by a large\nmargin.",
          "link": "http://arxiv.org/abs/2006.04047",
          "publishedOn": "2021-07-13T01:59:34.584Z",
          "wordCount": 686,
          "title": "DeepRelativeFusion: Dense Monocular SLAM using Single-Image Relative Depth Prediction. (arXiv:2006.04047v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>",
          "description": "The task of table structure recognition is to recognize the internal\nstructure of a table, which is a key step to make machines understand tables.\nHowever, tabular data in unstructured digital documents, e.g. Portable Document\nFormat (PDF) and images, are difficult to parse into structured\nmachine-readable format, due to complexity and diversity in their structure and\nstyle, especially for complex tables. In this paper, we introduce Split, Embed\nand Merge (SEM), an accurate table structure recognizer. In the first stage, we\nuse the FCN to predict the potential regions of the table row (column)\nseparators, so as to obtain the bounding boxes of the basic grids in the table.\nIn the second stage, we not only extract the visual features corresponding to\neach grid through RoIAlign, but also use the off-the-shelf recognizer and the\nBERT to extract the semantic features. The fused features of both are used to\ncharacterize each table grid. We find that by adding additional semantic\nfeatures to each grid, the ambiguity problem of the table structure from the\nvisual perspective can be solved to a certain extent and achieve higher\nprecision. Finally, we process the merging of these basic grids in a\nself-regression manner. The correspondent merging results is learned by the\nattention maps in attention mechanism. With the proposed method, we can\nrecognize the structure of tables well, even for complex tables. SEM can\nachieve an average F-Measure of $96.9\\%$ on the SciTSR dataset which\noutperforms other methods by a large margin. Extensive experiments on other\npublicly available table structure recognition datasets show that our model\nachieves state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.05214",
          "publishedOn": "2021-07-13T01:59:34.578Z",
          "wordCount": 698,
          "title": "Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.01751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_K/0/1/0/all/0/1\">Keiller Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mura_M/0/1/0/all/0/1\">Mauro Dalla Mura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "The recent impressive results of deep learning-based methods on computer\nvision applications brought fresh air to the research and industrial community.\nThis success is mainly due to the process that allows those methods to learn\ndata-driven features, generally based upon linear operations. However, in some\nscenarios, such operations do not have a good performance because of their\ninherited process that blurs edges, losing notions of corners, borders, and\ngeometry of objects. Overcoming this, non-linear operations, such as\nmorphological ones, may preserve such properties of the objects, being\npreferable and even state-of-the-art in some applications. Encouraged by this,\nin this work, we propose a novel network, called Deep Morphological Network\n(DeepMorphNet), capable of doing non-linear morphological operations while\nperforming the feature learning process by optimizing the structuring elements.\nThe DeepMorphNets can be trained and optimized end-to-end using traditional\nexisting techniques commonly employed in the training of deep learning\napproaches. A systematic evaluation of the proposed algorithm is conducted\nusing two synthetic and two traditional image classification datasets. Results\nshow that the proposed DeepMorphNets is a promising technique that can learn\ndistinct features when compared to the ones learned by current deep learning\nmethods.",
          "link": "http://arxiv.org/abs/1906.01751",
          "publishedOn": "2021-07-13T01:59:34.539Z",
          "wordCount": 661,
          "title": "An Introduction to Deep Morphological Networks. (arXiv:1906.01751v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.04573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>",
          "description": "We introduce a method for simultaneously classifying, segmenting and tracking\nobject instances in a video sequence. Our method, named MaskProp, adapts the\npopular Mask R-CNN to video by adding a mask propagation branch that propagates\nframe-level object instance masks from each video frame to all the other frames\nin a video clip. This allows our system to predict clip-level instance tracks\nwith respect to the object instances segmented in the middle frame of the clip.\nClip-level instance tracks generated densely for each frame in the sequence are\nfinally aggregated to produce video-level object instance segmentation and\nclassification. Our experiments demonstrate that our clip-level instance\nsegmentation makes our approach robust to motion blur and object occlusions in\nvideo. MaskProp achieves the best reported accuracy on the YouTube-VIS dataset,\noutperforming the ICCV 2019 video instance segmentation challenge winner\ndespite being much simpler and using orders of magnitude less labeled data\n(1.3M vs 1B images and 860K vs 14M bounding boxes).",
          "link": "http://arxiv.org/abs/1912.04573",
          "publishedOn": "2021-07-13T01:59:34.522Z",
          "wordCount": 647,
          "title": "Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation. (arXiv:1912.04573v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nabati_R/0/1/0/all/0/1\">Ramin Nabati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_L/0/1/0/all/0/1\">Landon Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hairong Qi</a>",
          "description": "3D multi-object tracking is a crucial component in the perception system of\nautonomous driving vehicles. Tracking all dynamic objects around the vehicle is\nessential for tasks such as obstacle avoidance and path planning. Autonomous\nvehicles are usually equipped with different sensor modalities to improve\naccuracy and reliability. While sensor fusion has been widely used in object\ndetection networks in recent years, most existing multi-object tracking\nalgorithms either rely on a single input modality, or do not fully exploit the\ninformation provided by multiple sensing modalities. In this work, we propose\nan end-to-end network for joint object detection and tracking based on radar\nand camera sensor fusion. Our proposed method uses a center-based radar-camera\nfusion algorithm for object detection and utilizes a greedy algorithm for\nobject association. The proposed greedy algorithm uses the depth, velocity and\n2D displacement of the detected objects to associate them through time. This\nmakes our tracking algorithm very robust to occluded and overlapping objects,\nas the depth and velocity information can help the network in distinguishing\nthem. We evaluate our method on the challenging nuScenes dataset, where it\nachieves 20.0 AMOTA and outperforms all vision-based 3D tracking methods in the\nbenchmark, as well as the baseline LiDAR-based method. Our method is online\nwith a runtime of 35ms per image, making it very suitable for autonomous\ndriving applications.",
          "link": "http://arxiv.org/abs/2107.05150",
          "publishedOn": "2021-07-13T01:59:34.515Z",
          "wordCount": 671,
          "title": "CFTrack: Center-based Radar and Camera Fusion for 3D Multi-Object Tracking. (arXiv:2107.05150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1\">Or Bar-Shira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubstein_A/0/1/0/all/0/1\">Ahuva Grubstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapson_Y/0/1/0/all/0/1\">Yael Rapson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhami_D/0/1/0/all/0/1\">Dror Suhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atar_E/0/1/0/all/0/1\">Eli Atar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peri_Hanania_K/0/1/0/all/0/1\">Keren Peri-Hanania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_R/0/1/0/all/0/1\">Ronnie Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "Breast cancer is the most common malignancy in women. Mammographic findings\nsuch as microcalcifications and masses, as well as morphologic features of\nmasses in sonographic scans, are the main diagnostic targets for tumor\ndetection. However, improved specificity of these imaging modalities is\nrequired. A leading alternative target is neoangiogenesis. When pathological,\nit contributes to the development of numerous types of tumors, and the\nformation of metastases. Hence, demonstrating neoangiogenesis by visualization\nof the microvasculature may be of great importance. Super resolution ultrasound\nlocalization microscopy enables imaging of the microvasculature at the\ncapillary level. Yet, challenges such as long reconstruction time, dependency\non prior knowledge of the system Point Spread Function (PSF), and separability\nof the Ultrasound Contrast Agents (UCAs), need to be addressed for translation\nof super-resolution US into the clinic. In this work we use a deep neural\nnetwork architecture that makes effective use of signal structure to address\nthese challenges. We present in vivo human results of three different breast\nlesions acquired with a clinical US scanner. By leveraging our trained network,\nthe microvasculature structure is recovered in a short time, without prior PSF\nknowledge, and without requiring separability of the UCAs. Each of the\nrecoveries exhibits a different structure that corresponds with the known\nhistological structure. This study demonstrates the feasibility of in vivo\nhuman super resolution, based on a clinical scanner, to increase US specificity\nfor different breast lesions and promotes the use of US in the diagnosis of\nbreast pathologies.",
          "link": "http://arxiv.org/abs/2107.05270",
          "publishedOn": "2021-07-13T01:59:34.505Z",
          "wordCount": 704,
          "title": "Learned super resolution ultrasound for improved breast lesion characterization. (arXiv:2107.05270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>",
          "description": "The proliferation of remote sensing satellites has resulted in a massive\namount of remote sensing images. However, due to human and material resource\nconstraints, the vast majority of remote sensing images remain unlabeled. As a\nresult, it cannot be applied to currently available deep learning methods. To\nfully utilize the remaining unlabeled images, we propose a Geographical\nKnowledge-driven Representation learning method for remote sensing images\n(GeoKR), improving network performance and reduce the demand for annotated\ndata. The global land cover products and geographical location associated with\neach remote sensing image are regarded as geographical knowledge to provide\nsupervision for representation learning and network pre-training. An efficient\npre-training framework is proposed to eliminate the supervision noises caused\nby imaging times and resolutions difference between remote sensing images and\ngeographical knowledge. A large scale pre-training dataset Levir-KR is proposed\nto support network pre-training. It contains 1,431,950 remote sensing images\nfrom Gaofen series satellites with various resolutions. Experimental results\ndemonstrate that our proposed method outperforms ImageNet pre-training and\nself-supervised representation learning methods and significantly reduces the\nburden of data annotation on downstream tasks such as scene classification,\nsemantic segmentation, object detection, and cloud / snow detection. It\ndemonstrates that our proposed method can be used as a novel paradigm for\npre-training neural networks. Codes will be available on\nhttps://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.",
          "link": "http://arxiv.org/abs/2107.05276",
          "publishedOn": "2021-07-13T01:59:34.497Z",
          "wordCount": 659,
          "title": "Geographical Knowledge-driven Representation Learning for Remote Sensing Images. (arXiv:2107.05276v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05121",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saito_N/0/1/0/all/0/1\">Naoki Saito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1\">Yiqun Shao</a>",
          "description": "Extending computational harmonic analysis tools from the classical setting of\nregular lattices to the more general setting of graphs and networks is very\nimportant and much research has been done recently. The Generalized Haar-Walsh\nTransform (GHWT) developed by Irion and Saito (2014) is a multiscale transform\nfor signals on graphs, which is a generalization of the classical Haar and\nWalsh-Hadamard Transforms. We propose the extended Generalized Haar-Walsh\nTransform (eGHWT), which is a generalization of the adapted time-frequency\ntilings of Thiele and Villemoes (1996). The eGHWT examines not only the\nefficiency of graph-domain partitions but also that of \"sequency-domain\"\npartitions simultaneously. Consequently, the eGHWT and its associated\nbest-basis selection algorithm for graph signals significantly improve the\nperformance of the previous GHWT with the similar computational cost, $O(N \\log\nN)$, where $N$ is the number of nodes of an input graph. While the GHWT\nbest-basis algorithm seeks the most suitable orthonormal basis for a given task\namong more than $(1.5)^N$ possible orthonormal bases in $\\mathbb{R}^N$, the\neGHWT best-basis algorithm can find a better one by searching through more than\n$0.618\\cdot(1.84)^N$ possible orthonormal bases in $\\mathbb{R}^N$. This article\ndescribes the details of the eGHWT best-basis algorithm and demonstrates its\nsuperiority using several examples including genuine graph signals as well as\nconventional digital images viewed as graph signals. Furthermore, we also show\nhow the eGHWT can be extended to 2D signals and matrix-form data by viewing\nthem as a tensor product of graphs generated from their columns and rows and\ndemonstrate its effectiveness on applications such as image approximation.",
          "link": "http://arxiv.org/abs/2107.05121",
          "publishedOn": "2021-07-13T01:59:34.479Z",
          "wordCount": 701,
          "title": "eGHWT: The extended Generalized Haar-Walsh Transform. (arXiv:2107.05121v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chrol_Cannon_J/0/1/0/all/0/1\">Joseph Chrol-Cannon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazic_R/0/1/0/all/0/1\">Ranko Lazic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhusoodanan_A/0/1/0/all/0/1\">Adithya Madhusoodanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>",
          "description": "Video activity recognition by deep neural networks is impressive for many\nclasses. However, it falls short of human performance, especially for\nchallenging to discriminate activities. Humans differentiate these complex\nactivities by recognising critical spatio-temporal relations among explicitly\nrecognised objects and parts, for example, an object entering the aperture of a\ncontainer. Deep neural networks can struggle to learn such critical\nrelationships effectively. Therefore we propose a more human-like approach to\nactivity recognition, which interprets a video in sequential temporal phases\nand extracts specific relationships among objects and hands in those phases.\nRandom forest classifiers are learnt from these extracted relationships. We\napply the method to a challenging subset of the something-something dataset and\nachieve a more robust performance against neural network baselines on\nchallenging activities.",
          "link": "http://arxiv.org/abs/2107.05319",
          "publishedOn": "2021-07-13T01:59:34.472Z",
          "wordCount": 562,
          "title": "Human-like Relational Models for Activity Recognition in Video. (arXiv:2107.05319v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Interpretable brain network models for disease prediction are of great value\nfor the advancement of neuroscience. GNNs are promising to model complicated\nnetwork data, but they are prone to overfitting and suffer from poor\ninterpretability, which prevents their usage in decision-critical scenarios\nlike healthcare. To bridge this gap, we propose BrainNNExplainer, an\ninterpretable GNN framework for brain network analysis. It is mainly composed\nof two jointly learned modules: a backbone prediction model that is\nspecifically designed for brain networks and an explanation generator that\nhighlights disease-specific prominent brain network connections. Extensive\nexperimental results with visualizations on two challenging disease prediction\ndatasets demonstrate the unique interpretability and outstanding performance of\nBrainNNExplainer.",
          "link": "http://arxiv.org/abs/2107.05097",
          "publishedOn": "2021-07-13T01:59:34.466Z",
          "wordCount": 600,
          "title": "BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis. (arXiv:2107.05097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ainetter_S/0/1/0/all/0/1\">Stefan Ainetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>",
          "description": "In this work, we introduce a novel, end-to-end trainable CNN-based\narchitecture to deliver high quality results for grasp detection suitable for a\nparallel-plate gripper, and semantic segmentation. Utilizing this, we propose a\nnovel refinement module that takes advantage of previously calculated grasp\ndetection and semantic segmentation and further increases grasp detection\naccuracy. Our proposed network delivers state-of-the-art accuracy on two\npopular grasp dataset, namely Cornell and Jacquard. As additional contribution,\nwe provide a novel dataset extension for the OCID dataset, making it possible\nto evaluate grasp detection in highly challenging scenes. Using this dataset,\nwe show that semantic segmentation can additionally be used to assign grasp\ncandidates to object classes, which can be used to pick specific objects in the\nscene.",
          "link": "http://arxiv.org/abs/2107.05287",
          "publishedOn": "2021-07-13T01:59:34.459Z",
          "wordCount": 568,
          "title": "End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB. (arXiv:2107.05287v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hira_S/0/1/0/all/0/1\">Sanchit Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Ritwik Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Abhinav Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_D/0/1/0/all/0/1\">Daniil Pakhomov</a>",
          "description": "We present an approach to perform supervised action recognition in the dark.\nIn this work, we present our results on the ARID dataset. Most previous works\nonly evaluate performance on large, well illuminated datasets like Kinetics and\nHMDB51. We demonstrate that our work is able to achieve a very low error rate\nwhile being trained on a much smaller dataset of dark videos. We also explore a\nvariety of training and inference strategies including domain transfer\nmethodologies and also propose a simple but useful frame selection strategy.\nOur empirical results demonstrate that we beat previously published baseline\nmodels by 11%.",
          "link": "http://arxiv.org/abs/2107.05202",
          "publishedOn": "2021-07-13T01:59:34.453Z",
          "wordCount": 539,
          "title": "Delta Sampling R-BERT for limited data and low-light action recognition. (arXiv:2107.05202v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Rongkai Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Jiang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_Z/0/1/0/all/0/1\">Zhiyuan Zha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dauwels_J/0/1/0/all/0/1\">Justin Dauwels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>",
          "description": "State-of-the-art image denoisers exploit various types of deep neural\nnetworks via deterministic training. Alternatively, very recent works utilize\ndeep reinforcement learning for restoring images with diverse or unknown\ncorruptions. Though deep reinforcement learning can generate effective policy\nnetworks for operator selection or architecture search in image restoration,\nhow it is connected to the classic deterministic training in solving inverse\nproblems remains unclear. In this work, we propose a novel image denoising\nscheme via Residual Recovery using Reinforcement Learning, dubbed R3L. We show\nthat R3L is equivalent to a deep recurrent neural network that is trained using\na stochastic reward, in contrast to many popular denoisers using supervised\nlearning with deterministic losses. To benchmark the effectiveness of\nreinforcement learning in R3L, we train a recurrent neural network with the\nsame architecture for residual recovery using the deterministic loss, thus to\nanalyze how the two different training strategies affect the denoising\nperformance. With such a unified benchmarking system, we demonstrate that the\nproposed R3L has better generalizability and robustness in image denoising when\nthe estimated noise level varies, comparing to its counterparts using\ndeterministic training, as well as various state-of-the-art image denoising\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.05318",
          "publishedOn": "2021-07-13T01:59:34.448Z",
          "wordCount": 657,
          "title": "R3L: Connecting Deep Reinforcement Learning to Recurrent Neural Networks for Image Denoising via Residual Recovery. (arXiv:2107.05318v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>",
          "description": "We introduce a novel Recurrent Neural Network-based algorithm for future\nvideo feature generation and action anticipation called feature mapping RNN.\nOur novel RNN architecture builds upon three effective principles of machine\nlearning, namely parameter sharing, Radial Basis Function kernels and\nadversarial training. Using only some of the earliest frames of a video, the\nfeature mapping RNN is able to generate future features with a fraction of the\nparameters needed in traditional RNN. By feeding these future features into a\nsimple multi-layer perceptron facilitated with an RBF kernel layer, we are able\nto accurately predict the action in the video. In our experiments, we obtain\n18% improvement on JHMDB-21 dataset, 6% on UCF101-24 and 13% improvement on\nUT-Interaction datasets over prior state-of-the-art for action anticipation.",
          "link": "http://arxiv.org/abs/1911.07806",
          "publishedOn": "2021-07-13T01:59:34.431Z",
          "wordCount": 600,
          "title": "Action Anticipation with RBF Kernelized Feature Mapping RNN. (arXiv:1911.07806v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>",
          "description": "Early action recognition (action prediction) from limited preliminary\nobservations plays a critical role for streaming vision systems that demand\nreal-time inference, as video actions often possess elongated temporal spans\nwhich cause undesired latency. In this study, we address action prediction by\ninvestigating how action patterns evolve over time in a spatial feature space.\nThere are three key components to our system. First, we work with\nintermediate-layer ConvNet features, which allow for abstraction from raw data,\nwhile retaining spatial layout. Second, instead of propagating features per se,\nwe propagate their residuals across time, which allows for a compact\nrepresentation that reduces redundancy. Third, we employ a Kalman filter to\ncombat error build-up and unify across prediction start times. Extensive\nexperimental results on multiple benchmarks show that our approach leads to\ncompetitive performance in action prediction. Notably, we investigate the\nlearned components of our system to shed light on their otherwise opaque\nnatures in two ways. First, we document that our learned feature propagation\nmodule works as a spatial shifting mechanism under convolution to propagate\ncurrent observations into the future. Thus, it captures flow-based image motion\ninformation. Second, the learned Kalman filter adaptively updates prior\nestimation to aid the sequence learning process.",
          "link": "http://arxiv.org/abs/2107.05122",
          "publishedOn": "2021-07-13T01:59:34.425Z",
          "wordCount": 632,
          "title": "Interpretable Deep Feature Propagation for Early Action Recognition. (arXiv:2107.05122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05274",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Bingzhi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yishu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>",
          "description": "With the development of deep encoder-decoder architectures and large-scale\nannotated medical datasets, great progress has been achieved in the development\nof automatic medical image segmentation. Due to the stacking of convolution\nlayers and the consecutive sampling operations, existing standard models\ninevitably encounter the information recession problem of feature\nrepresentations, which fails to fully model the global contextual feature\ndependencies. To overcome the above challenges, this paper proposes a novel\nTransformer based medical image semantic segmentation framework called\nTransAttUnet, in which the multi-level guided attention and multi-scale skip\nconnection are jointly designed to effectively enhance the functionality and\nflexibility of traditional U-shaped architecture. Inspired by Transformer, a\nnovel self-aware attention (SAA) module with both Transformer Self Attention\n(TSA) and Global Spatial Attention (GSA) is incorporated into TransAttUnet to\neffectively learn the non-local interactions between encoder features. In\nparticular, we also establish additional multi-scale skip connections between\ndecoder blocks to aggregate the different semantic-scale upsampling features.\nIn this way, the representation ability of multi-scale context information is\nstrengthened to generate discriminative features. Benefitting from these\ncomplementary components, the proposed TransAttUnet can effectively alleviate\nthe loss of fine details caused by the information recession problem, improving\nthe diagnostic sensitivity and segmentation quality of medical image analysis.\nExtensive experiments on multiple medical image segmentation datasets of\ndifferent imaging demonstrate that our method consistently outperforms the\nstate-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2107.05274",
          "publishedOn": "2021-07-13T01:59:34.417Z",
          "wordCount": 674,
          "title": "TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation. (arXiv:2107.05274v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menghan_H/0/1/0/all/0/1\">Hu Menghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guangtao_Z/0/1/0/all/0/1\">Zhai Guangtao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Ping_Z/0/1/0/all/0/1\">Zhang Xiao-Ping</a>",
          "description": "In recent years, computer-aided diagnosis has become an increasingly popular\ntopic. Methods based on convolutional neural networks have achieved good\nperformance in medical image segmentation and classification. Due to the\nlimitations of the convolution operation, the long-term spatial features are\noften not accurately obtained. Hence, we propose a TransClaw U-Net network\nstructure, which combines the convolution operation with the transformer\noperation in the encoding part. The convolution part is applied for extracting\nthe shallow spatial features to facilitate the recovery of the image resolution\nafter upsampling. The transformer part is used to encode the patches, and the\nself-attention mechanism is used to obtain global information between\nsequences. The decoding part retains the bottom upsampling structure for better\ndetail segmentation performance. The experimental results on Synapse\nMulti-organ Segmentation Datasets show that the performance of TransClaw U-Net\nis better than other network structures. The ablation experiments also prove\nthe generalization performance of TransClaw U-Net.",
          "link": "http://arxiv.org/abs/2107.05188",
          "publishedOn": "2021-07-13T01:59:34.411Z",
          "wordCount": 611,
          "title": "TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation. (arXiv:2107.05188v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05115",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1\">Basit O. Alawode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1\">Mudassir Masood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1\">Tarig Ballal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1\">Tareq Al-Naffouri</a>",
          "description": "In spite of the improvements achieved by the several denoising algorithms\nover the years, many of them still fail at preserving the fine details of the\nimage after denoising. This is as a result of the smooth-out effect they have\non the images. Most neural network-based algorithms have achieved better\nquantitative performance than the classical denoising algorithms. However, they\nalso suffer from qualitative (visual) performance as a result of the smooth-out\neffect. In this paper, we propose an algorithm to address this shortcoming. We\npropose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image\ndenoising. This algorithm performs collaborative denoising of image patches in\nthe sparse domain using a set of optimized neural network models. This results\nin a fast algorithm that is able to excellently obtain a trade-off between\nnoise removal and details preservation. Extensive experiments show that the\nDeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and\nqualitatively (visually) better than many of the state-of-the-art denoising\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.05115",
          "publishedOn": "2021-07-13T01:59:34.405Z",
          "wordCount": 609,
          "title": "Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising. (arXiv:2107.05115v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_C/0/1/0/all/0/1\">Chun Chet Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazaruddin_A/0/1/0/all/0/1\">Akmalul Khairi Bin Nazaruddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeong Khang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lixin Fan</a>",
          "description": "With hundreds of thousands of electronic chip components are being\nmanufactured every day, chip manufacturers have seen an increasing demand in\nseeking a more efficient and effective way of inspecting the quality of printed\ntexts on chip components. The major problem that deters this area of research\nis the lacking of realistic text on chips datasets to act as a strong\nfoundation. Hence, a text on chips dataset, ICText is used as the main target\nfor the proposed Robust Reading Challenge on Integrated Circuit Text Spotting\nand Aesthetic Assessment (RRC-ICText) 2021 to encourage the research on this\nproblem. Throughout the entire competition, we have received a total of 233\nsubmissions from 10 unique teams/individuals. Details of the competition and\nsubmission results are presented in this report.",
          "link": "http://arxiv.org/abs/2107.05279",
          "publishedOn": "2021-07-13T01:59:34.389Z",
          "wordCount": 609,
          "title": "ICDAR 2021 Competition on Integrated Circuit Text Spotting and Aesthetic Assessment. (arXiv:2107.05279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>",
          "description": "Video predictive understanding encompasses a wide range of efforts that are\nconcerned with the anticipation of the unobserved future from the current as\nwell as historical video observations. Action prediction is a major sub-area of\nvideo predictive understanding and is the focus of this review. This sub-area\nhas two major subdivisions: early action recognition and future action\nprediction. Early action recognition is concerned with recognizing an ongoing\naction as soon as possible. Future action prediction is concerned with the\nanticipation of actions that follow those previously observed. In either case,\nthe \\textbf{\\textit{causal}} relationship between the past, current, and\npotential future information is the main focus. Various mathematical tools such\nas Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian\nrecursive filtering are widely adopted jointly with computer vision techniques\nfor these two tasks. However, these approaches face challenges such as the\ncurse of dimensionality, poor generalization, and constraints from\ndomain-specific knowledge. Recently, structures that rely on deep convolutional\nneural networks and recurrent neural networks have been extensively proposed\nfor improving the performance of existing vision tasks, in general, and action\nprediction tasks, in particular. However, they have their own shortcomings, \\eg\nreliance on massive training data and lack of strong theoretical underpinnings.\nIn this survey, we start by introducing the major sub-areas of the broad area\nof video predictive understanding, which recently have received intensive\nattention and proven to have practical value. Next, a thorough review of\nvarious early action recognition and future action prediction algorithms are\nprovided with suitably organized divisions. Finally, we conclude our discussion\nwith future research directions.",
          "link": "http://arxiv.org/abs/2107.05140",
          "publishedOn": "2021-07-13T01:59:34.383Z",
          "wordCount": 698,
          "title": "Review of Video Predictive Understanding: Early ActionRecognition and Future Action Prediction. (arXiv:2107.05140v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05342",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Celik_N/0/1/0/all/0/1\">Numan Celik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Soumya Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braden_B/0/1/0/all/0/1\">Barbara Braden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1\">Jens Rittscher</a>",
          "description": "Gastrointestinal (GI) cancer precursors require frequent monitoring for risk\nstratification of patients. Automated segmentation methods can help to assess\nrisk areas more accurately, and assist in therapeutic procedures or even\nremoval. In clinical practice, addition to the conventional white-light imaging\n(WLI), complimentary modalities such as narrow-band imaging (NBI) and\nfluorescence imaging are used. While, today most segmentation approaches are\nsupervised and only concentrated on a single modality dataset, this work\nexploits to use a target-independent unsupervised domain adaptation (UDA)\ntechnique that is capable to generalize to an unseen target modality. In this\ncontext, we propose a novel UDA-based segmentation method that couples the\nvariational autoencoder and U-Net with a common EfficientNet-B4 backbone, and\nuses a joint loss for latent-space optimization for target samples. We show\nthat our model can generalize to unseen target NBI (target) modality when\ntrained using only WLI (source) modality. Our experiments on both upper and\nlower GI endoscopy data show the effectiveness of our approach compared to\nnaive supervised approach and state-of-the-art UDA segmentation methods.",
          "link": "http://arxiv.org/abs/2107.05342",
          "publishedOn": "2021-07-13T01:59:34.376Z",
          "wordCount": 632,
          "title": "EndoUDA: A modality independent segmentation approach for endoscopy imaging. (arXiv:2107.05342v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangyue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Y. Chai</a>",
          "description": "In this paper, we study the problem of recognizing compositional\nattribute-object concepts within the zero-shot learning (ZSL) framework. We\npropose an episode-based cross-attention (EpiCA) network which combines merits\nof cross-attention mechanism and episode-based training strategy to recognize\nnovel compositional concepts. Firstly, EpiCA bases on cross-attention to\ncorrelate concept-visual information and utilizes the gated pooling layer to\nbuild contextualized representations for both images and concepts. The updated\nrepresentations are used for a more in-depth multi-modal relevance calculation\nfor concept recognition. Secondly, a two-phase episode training strategy,\nespecially the transductive phase, is adopted to utilize unlabeled test\nexamples to alleviate the low-resource learning problem. Experiments on two\nwidely-used zero-shot compositional learning (ZSCL) benchmarks have\ndemonstrated the effectiveness of the model compared with recent approaches on\nboth conventional and generalized ZSCL settings.",
          "link": "http://arxiv.org/abs/2107.05176",
          "publishedOn": "2021-07-13T01:59:34.369Z",
          "wordCount": 560,
          "title": "Zero-Shot Compositional Concept Learning. (arXiv:2107.05176v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sushobhan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhaoyang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_N/0/1/0/all/0/1\">Nathan Matsuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkovich_A/0/1/0/all/0/1\">Andrew Berkovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1\">Oliver Cossairt</a>",
          "description": "Existing Multi-Plane Image (MPI) based view-synthesis methods generate an MPI\naligned with the input view using a fixed number of planes in one forward pass.\nThese methods produce fast, high-quality rendering of novel views, but rely on\nslow and computationally expensive MPI generation methods unsuitable for\nreal-time applications. In addition, most MPI techniques use fixed\ndepth/disparity planes which cannot be modified once the training is complete,\nhence offering very little flexibility at run-time.\n\nWe propose LiveView - a novel MPI generation and rendering technique that\nproduces high-quality view synthesis in real-time. Our method can also offer\nthe flexibility to select scene-dependent MPI planes (number of planes and\nspacing between them) at run-time. LiveView first warps input images to target\nview (target-centered) and then learns to generate a target view centered MPI,\none depth plane at a time (dynamically). The method generates high-quality\nrenderings, while also enabling fast MPI generation and novel view synthesis.\nAs a result, LiveView enables real-time view synthesis applications where an\nMPI needs to be updated frequently based on a video stream of input views. We\ndemonstrate that LiveView improves the quality of view synthesis while being 70\ntimes faster at run-time compared to state-of-the-art MPI-based methods.",
          "link": "http://arxiv.org/abs/2107.05113",
          "publishedOn": "2021-07-13T01:59:34.362Z",
          "wordCount": 638,
          "title": "LiveView: Dynamic Target-Centered MPI for View Synthesis. (arXiv:2107.05113v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05085",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1\">Gorkem Polat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Serinagaoglu_Y/0/1/0/all/0/1\">Yesim Dogrusoz Serinagaoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halici_U/0/1/0/all/0/1\">Ugur Halici</a>",
          "description": "Recent studies have shown that lung cancer screening using annual low-dose\ncomputed tomography (CT) reduces lung cancer mortality by 20% compared to\ntraditional chest radiography. Therefore, CT lung screening has started to be\nused widely all across the world. However, analyzing these images is a serious\nburden for radiologists. The number of slices in a CT scan can be up to 600.\nTherefore, computer-aided-detection (CAD) systems are very important for faster\nand more accurate assessment of the data. In this study, we proposed a\nframework that analyzes CT lung screenings using convolutional neural networks\n(CNNs) to reduce false positives. We trained our model with different volume\nsizes and showed that volume size plays a critical role in the performance of\nthe system. We also used different fusions in order to show their power and\neffect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D\nconvolutional operations applied to 3D data could result in information loss.\nThe proposed framework has been tested on the dataset provided by the LUNA16\nChallenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.",
          "link": "http://arxiv.org/abs/2107.05085",
          "publishedOn": "2021-07-13T01:59:34.347Z",
          "wordCount": 661,
          "title": "Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks. (arXiv:2107.05085v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dromey_B/0/1/0/all/0/1\">Brian Dromey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napolitano_R/0/1/0/all/0/1\">Raffaele Napolitano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peebles_D/0/1/0/all/0/1\">Donald M. Peebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>",
          "description": "During pregnancy, ultrasound examination in the second trimester can assess\nfetal size according to standardized charts. To achieve a reproducible and\naccurate measurement, a sonographer needs to identify three standard 2D planes\nof the fetal anatomy (head, abdomen, femur) and manually mark the key\nanatomical landmarks on the image for accurate biometry and fetal weight\nestimation. This can be a time-consuming operator-dependent task, especially\nfor a trainee sonographer. Computer-assisted techniques can help in automating\nthe fetal biometry computation process. In this paper, we present a unified\nautomated framework for estimating all measurements needed for the fetal weight\nassessment. The proposed framework semantically segments the key fetal\nanatomies using state-of-the-art segmentation models, followed by region\nfitting and scale recovery for the biometry estimation. We present an ablation\nstudy of segmentation algorithms to show their robustness through 4-fold\ncross-validation on a dataset of 349 ultrasound standard plane images from 42\npregnancies. Moreover, we show that the network with the best segmentation\nperformance tends to be more accurate for biometry estimation. Furthermore, we\ndemonstrate that the error between clinically measured and predicted fetal\nbiometry is lower than the permissible error during routine clinical\nmeasurements.",
          "link": "http://arxiv.org/abs/2107.05255",
          "publishedOn": "2021-07-13T01:59:34.341Z",
          "wordCount": 653,
          "title": "AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2107.05255v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1\">Xuan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Relation prediction among entities in images is an important step in scene\ngraph generation (SGG), which further impacts various visual understanding and\nreasoning tasks. Existing SGG frameworks, however, require heavy training yet\nare incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we\nstress that such incapability is due to the lack of commonsense reasoning,i.e.,\nthe ability to associate similar entities and infer similar relations based on\ngeneral understanding of the world. To fill this gap, we propose\nCommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to\nintegrate commonsense knowledge for SGG, especially for zero-shot relation\nprediction. Specifically, we develop novel graph mining pipelines to model the\nneighborhoods and paths around entities in an external commonsense knowledge\ngraph, and integrate them on top of state-of-the-art SGG frameworks. Extensive\nquantitative evaluations and qualitative case studies on both original and\nmanipulated datasets from Visual Genome demonstrate the effectiveness of our\nproposed approach.",
          "link": "http://arxiv.org/abs/2107.05080",
          "publishedOn": "2021-07-13T01:59:34.335Z",
          "wordCount": 608,
          "title": "Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration. (arXiv:2107.05080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-En Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">En-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_P/0/1/0/all/0/1\">Pei-Yung Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Li-Chen Fu</a>",
          "description": "Recently, there has been a panoptic segmentation task combining semantic and\ninstance segmentation, in which the goal is to classify each pixel with the\ncorresponding instance ID. In this work, we propose a solution to tackle the\npanoptic segmentation task. The overall structure combines the bottom-up method\nand the top-down method. Therefore, not only can there be better performance,\nbut also the execution speed can be maintained. The network mainly pays\nattention to the quality of the mask. In the previous work, we can see that the\nuneven contour of the object is more likely to appear, resulting in low-quality\nprediction. Accordingly, we propose enhancement features and corresponding loss\nfunctions for the silhouette of objects and backgrounds to improve the mask.\nMeanwhile, we use the new proposed confidence score to solve the occlusion\nproblem and make the network tend to use higher quality masks as prediction\nresults. To verify our research, we used the COCO dataset and CityScapes\ndataset to do experiments and obtained competitive results with fast inference\ntime.",
          "link": "http://arxiv.org/abs/2107.05093",
          "publishedOn": "2021-07-13T01:59:34.328Z",
          "wordCount": 614,
          "title": "SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network. (arXiv:2107.05093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Joshua Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Chau-Wai Wong</a>",
          "description": "Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory\nfunctionality and is receiving increasing attention during the COVID-19\npandemic. Clinical findings show that it is possible for COVID-19 patients to\nhave significantly low SpO$_2$ before any obvious symptoms. The prevalence of\ncameras has motivated researchers to investigate methods for monitoring SpO$_2$\nusing videos. Most prior schemes involving smartphones are contact-based: They\nrequire a fingertip to cover the phone's camera and the nearby light source to\ncapture re-emitted light from the illuminated tissue. In this paper, we propose\nthe first convolutional neural network based noncontact SpO$_2$ estimation\nscheme using smartphone cameras. The scheme analyzes the videos of a\nparticipant's hand for physiological sensing, which is convenient and\ncomfortable, and can protect their privacy and allow for keeping face masks on.\nWe design our neural network architectures inspired by the optophysiological\nmodels for SpO$_2$ measurement and demonstrate the explainability by\nvisualizing the weights for channel combination. Our proposed models outperform\nthe state-of-the-art model that is designed for contact-based SpO$_2$\nmeasurement, showing the potential of our proposed method to contribute to\npublic health. We also analyze the impact of skin type and the side of a hand\non SpO$_2$ estimation performance.",
          "link": "http://arxiv.org/abs/2107.05087",
          "publishedOn": "2021-07-13T01:59:34.321Z",
          "wordCount": 689,
          "title": "Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+George_B/0/1/0/all/0/1\">Blessen George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K. Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>",
          "description": "Generative adversarial networks (GANs) are very popular to generate realistic\nimages, but they often suffer from the training instability issues and the\nphenomenon of mode loss. In order to attain greater diversity in GAN\nsynthesized data, it is critical to solving the problem of mode loss. Our work\nexplores probabilistic approaches to GAN modelling that could allow us to\ntackle these issues. We present Prb-GANs, a new variation that uses dropout to\ncreate a distribution over the network parameters with the posterior learnt\nusing variational inference. We describe theoretically and validate\nexperimentally using simple and complex datasets the benefits of such an\napproach. We look into further improvements using the concept of uncertainty\nmeasures. Through a set of further modifications to the loss functions for each\nnetwork of the GAN, we are able to get results that show the improvement of GAN\nperformance. Our methods are extremely simple and require very little\nmodification to existing GAN architecture.",
          "link": "http://arxiv.org/abs/2107.05241",
          "publishedOn": "2021-07-13T01:59:34.305Z",
          "wordCount": 597,
          "title": "Prb-GAN: A Probabilistic Framework for GAN Modelling. (arXiv:2107.05241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05334",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1\">Chih-Chung Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Guan-Lin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Mei-Hsuan Wu</a>",
          "description": "With the massive damage in the world caused by Coronavirus Disease 2019\nSARS-CoV-2 (COVID-19), many related research topics have been proposed in the\npast two years. The Chest Computed Tomography (CT) scans are the most valuable\nmaterials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19\nclassification of Chest CT scan is based on a single-slice level, implying that\nthe most critical CT slice should be selected from the original CT scan volume\nmanually. We simultaneously propose 2-D and 3-D models to predict the COVID-19\nof CT scan to tickle this issue. In our 2-D model, we introduce the Deep\nWilcoxon signed-rank test (DWCC) to determine the importance of each slice of a\nCT scan to overcome the issue mentioned previously. Furthermore, a\nConvolutional CT scan-Aware Transformer (CCAT) is proposed to discover the\ncontext of the slices fully. The frame-level feature is extracted from each CT\nslice based on any backbone network and followed by feeding the features to our\nwithin-slice-Transformer (WST) to discover the context information in the pixel\ndimension. The proposed Between-Slice-Transformer (BST) is used to aggregate\nthe extracted spatial-context features of every CT slice. A simple classifier\nis then used to judge whether the Spatio-temporal features are COVID-19 or\nnon-COVID-19. The extensive experiments demonstrated that the proposed CCAT and\nDWCC significantly outperform the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.05334",
          "publishedOn": "2021-07-13T01:59:34.296Z",
          "wordCount": 713,
          "title": "Visual Transformer with Statistical Test for COVID-19 Classification. (arXiv:2107.05334v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_S/0/1/0/all/0/1\">Sho Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaji_S/0/1/0/all/0/1\">Shizuo Kaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawa_K/0/1/0/all/0/1\">Kanabu Nawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imae_T/0/1/0/all/0/1\">Toshikazu Imae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_A/0/1/0/all/0/1\">Atsushi Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamoto_T/0/1/0/all/0/1\">Takahiro Nakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohta_T/0/1/0/all/0/1\">Takeshi Ohta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozawa_Y/0/1/0/all/0/1\">Yuki Nozawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_H/0/1/0/all/0/1\">Hideomi Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haga_A/0/1/0/all/0/1\">Akihiro Haga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_K/0/1/0/all/0/1\">Keiichi Nakagawa</a>",
          "description": "Deep-learning-based image processing has emerged as a valuable tool in recent\nyears owing to its high performance. However, the quality of\ndeep-learning-based methods relies heavily on the amount of training data, and\nthe cost of acquiring a large amount of data is often prohibitive in medical\nfields. Therefore, we performed CT modality conversion based on deep learning\nrequiring only a small number of unsupervised images. The proposed method is\nbased on generative adversarial networks (GANs) with several extensions\ntailored for CT images. This method emphasizes the preservation of the\nstructure in the processed images and reduction in the amount of training data.\nThis method was applied to realize the conversion of mega-voltage computed\ntomography (MVCT) to kilo-voltage computed tomography (kVCT) images. Training\nwas performed using several datasets acquired from patients with head and neck\ncancer. The size of the datasets ranged from 16 slices (for two patients) to\n2745 slices (for 137 patients) of MVCT and 2824 slices of kVCT for 98 patients.\nThe quality of the processed MVCT images was considerably enhanced, and the\nstructural changes in the images were minimized. With an increase in the size\nof training data, the image quality exhibited a satisfactory convergence from a\nfew hundred slices. In addition to statistical and visual evaluations, these\nresults were clinically evaluated by medical doctors in terms of the accuracy\nof contouring. We developed an MVCT to kVCT conversion model based on deep\nlearning, which can be trained using a few hundred unpaired images. The\nstability of the model against the change in the data size was demonstrated.\nThis research promotes the reliable use of deep learning in clinical medicine\nby partially answering the commonly asked questions: \"Is our data enough? How\nmuch data must we prepare?\"",
          "link": "http://arxiv.org/abs/2107.05238",
          "publishedOn": "2021-07-13T01:59:34.288Z",
          "wordCount": 772,
          "title": "Training deep cross-modality conversion models with a small amount of data and its application to MVCT to kVCT conversion. (arXiv:2107.05238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolf_J/0/1/0/all/0/1\">Joerg Christian Wolf</a>",
          "description": "State-of-the-art motor vehicles are able to break for pedestrians in an\nemergency. We investigate what it would take to issue an early warning to the\ndriver so he/she has time to react. We have identified that predicting the\nintention of a pedestrian reliably by position is a particularly hard\nchallenge. This paper describes an early pedestrian warning demonstration\nsystem.",
          "link": "http://arxiv.org/abs/2107.05186",
          "publishedOn": "2021-07-13T01:59:34.281Z",
          "wordCount": 484,
          "title": "Early warning of pedestrians and cyclists. (arXiv:2107.05186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05190",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xinyu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tianlang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1\">Jinchao Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Y/0/1/0/all/0/1\">Yanqing Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1\">Yanlong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_B/0/1/0/all/0/1\">Banging Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_P/0/1/0/all/0/1\">Pengwei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>",
          "description": "Hyperspectral image (HSI) contains both spatial pattern and spectral\ninformation which has been widely used in food safety, remote sensing, and\nmedical detection. However, the acquisition of hyperspectral images is usually\ncostly due to the complicated apparatus for the acquisition of optical\nspectrum. Recently, it has been reported that HSI can be reconstructed from\nsingle RGB image using convolution neural network (CNN) algorithms. Compared\nwith the traditional hyperspectral cameras, the method based on CNN algorithms\nis simple, portable and low cost. In this study, we focused on the influence of\nthe RGB camera spectral sensitivity (CSS) on the HSI. A Xenon lamp incorporated\nwith a monochromator were used as the standard light source to calibrate the\nCSS. And the experimental results show that the CSS plays a significant role in\nthe reconstruction accuracy of an HSI. In addition, we proposed a new HSI\nreconstruction network where the dimensional structure of the original\nhyperspectral datacube was modified by 3D matrix transpose to improve the\nreconstruction accuracy.",
          "link": "http://arxiv.org/abs/2107.05190",
          "publishedOn": "2021-07-13T01:59:34.243Z",
          "wordCount": 616,
          "title": "Deep-learning-based Hyperspectral imaging through a RGB camera. (arXiv:2107.05190v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shuyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xinqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>",
          "description": "The paper describes our proposed methodology for the seven basic expression\nclassification track of Affective Behavior Analysis in-the-wild (ABAW)\nCompetition 2021. In this task, facial expression recognition (FER) methods aim\nto classify the correct expression category from a diverse background, but\nthere are several challenges. First, to adapt the model to in-the-wild\nscenarios, we use the knowledge from pre-trained large-scale face recognition\ndata. Second, we propose an ensemble model with a convolution neural network\n(CNN), a CNN-recurrent neural network (CNN-RNN), and a CNN-Transformer\n(CNN-Transformer), to incorporate both spatial and temporal information. Our\nensemble model achieved F1 as 0.4133, accuracy as 0.6216 and final metric as\n0.4821 on the validation set.",
          "link": "http://arxiv.org/abs/2107.05160",
          "publishedOn": "2021-07-13T01:59:34.224Z",
          "wordCount": 558,
          "title": "Spatial and Temporal Networks for Facial Expression Recognition in the Wild Videos. (arXiv:2107.05160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05023",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lan_P/0/1/0/all/0/1\">Phan Ngoc Lan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_N/0/1/0/all/0/1\">Nguyen Sy An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_D/0/1/0/all/0/1\">Dao Viet Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_D/0/1/0/all/0/1\">Dao Van Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trung_T/0/1/0/all/0/1\">Tran Quang Trung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thuy_N/0/1/0/all/0/1\">Nguyen Thi Thuy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sang_D/0/1/0/all/0/1\">Dinh Viet Sang</a>",
          "description": "Automatic polyp segmentation has proven to be immensely helpful for endoscopy\nprocedures, reducing the missing rate of adenoma detection for endoscopists\nwhile increasing efficiency. However, classifying a polyp as being neoplasm or\nnot and segmenting it at the pixel level is still a challenging task for\ndoctors to perform in a limited time. In this work, we propose a fine-grained\nformulation for the polyp segmentation problem. Our formulation aims to not\nonly segment polyp regions, but also identify those at high risk of malignancy\nwith high accuracy. In addition, we present a UNet-based neural network\narchitecture called NeoUNet, along with a hybrid loss function to solve this\nproblem. Experiments show highly competitive results for NeoUNet on our\nbenchmark dataset compared to existing polyp segmentation models.",
          "link": "http://arxiv.org/abs/2107.05023",
          "publishedOn": "2021-07-13T01:59:34.204Z",
          "wordCount": 584,
          "title": "NeoUNet: Towards accurate colon polyp segmentation and neoplasm detection. (arXiv:2107.05023v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tianxiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>",
          "description": "This study delves into semi-supervised object detection (SSOD) to improve\ndetector performance with additional unlabeled data. State-of-the-art SSOD\nperformance has been achieved recently by self-training, in which training\nsupervision consists of ground truths and pseudo-labels. In current studies, we\nobserve that class imbalance in SSOD severely impedes the effectiveness of\nself-training. To address the class imbalance, we propose adaptive\nclass-rebalancing self-training (ACRST) with a novel memory module called\nCropBank. ACRST adaptively rebalances the training data with foreground\ninstances extracted from the CropBank, thereby alleviating the class imbalance.\nOwing to the high complexity of detection tasks, we observe that both\nself-training and data-rebalancing suffer from noisy pseudo-labels in SSOD.\nTherefore, we propose a novel two-stage filtering algorithm to generate\naccurate pseudo-labels. Our method achieves satisfactory improvements on\nMS-COCO and VOC benchmarks. When using only 1\\% labeled data in MS-COCO, our\nmethod achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP\nimprovement compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.05031",
          "publishedOn": "2021-07-13T01:59:34.197Z",
          "wordCount": 591,
          "title": "Semi-Supervised Object Detection with Adaptive Class-Rebalancing Self-Training. (arXiv:2107.05031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Catak_F/0/1/0/all/0/1\">Ferhat Ozgur Catak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1\">Tao Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Shaukat Ali</a>",
          "description": "Object detection in autonomous cars is commonly based on camera images and\nLidar inputs, which are often used to train prediction models such as deep\nartificial neural networks for decision making for object recognition,\nadjusting speed, etc. A mistake in such decision making can be damaging; thus,\nit is vital to measure the reliability of decisions made by such prediction\nmodels via uncertainty measurement. Uncertainty, in deep learning models, is\noften measured for classification problems. However, deep learning models in\nautonomous driving are often multi-output regression models. Hence, we propose\na novel method called PURE (Prediction sURface uncErtainty) for measuring\nprediction uncertainty of such regression models. We formulate the object\nrecognition problem as a regression model with more than one outputs for\nfinding object locations in a 2-dimensional camera view. For evaluation, we\nmodified three widely-applied object recognition models (i.e., YoLo, SSD300 and\nSSD512) and used the KITTI, Stanford Cars, Berkeley DeepDrive, and NEXET\ndatasets. Results showed the statistically significant negative correlation\nbetween prediction surface uncertainty and prediction accuracy suggesting that\nuncertainty significantly impacts the decisions made by autonomous driving.",
          "link": "http://arxiv.org/abs/2107.04991",
          "publishedOn": "2021-07-13T01:59:34.191Z",
          "wordCount": 640,
          "title": "Prediction Surface Uncertainty Quantification in Object Detection Models for Autonomous Driving. (arXiv:2107.04991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiangzhu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenzhe Liu</a>",
          "description": "In most practical applications, it's common to utilize multiple features from\ndifferent views to represent one object. Among these works, multi-view\nsubspace-based clustering has gained extensive attention from many researchers,\nwhich aims to provide clustering solutions to multi-view data. However, most\nexisting methods fail to take full use of the locality geometric structure and\nsimilarity relationship among samples under the multi-view scenario. To solve\nthese issues, we propose a novel multi-view learning method with locality\nrelationship constraint to explore the problem of multi-view clustering, called\nLocality Relationship Constrained Multi-view Clustering Framework (LRC-MCF).\nLRC-MCF aims to explore the diversity, geometric, consensus and complementary\ninformation among different views, by capturing the locality relationship\ninformation and the common similarity relationships among multiple views.\nMoreover, LRC-MCF takes sufficient consideration to weights of different views\nin finding the common-view locality structure and straightforwardly produce the\nfinal clusters. To effectually reduce the redundancy of the learned\nrepresentations, the low-rank constraint on the common similarity matrix is\nconsidered additionally. To solve the minimization problem of LRC-MCF, an\nAlternating Direction Minimization (ADM) method is provided to iteratively\ncalculate all variables LRC-MCF. Extensive experimental results on seven\nbenchmark multi-view datasets validate the effectiveness of the LRC-MCF method.",
          "link": "http://arxiv.org/abs/2107.05073",
          "publishedOn": "2021-07-13T01:59:34.175Z",
          "wordCount": 631,
          "title": "Locality Relationship Constrained Multi-view Clustering Framework. (arXiv:2107.05073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zheyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiyong Bu</a>",
          "description": "To prevent the spread of coronavirus disease 2019 (COVID-19), preliminary\ntemperature measurement and mask detection in public areas are conducted.\nHowever, the existing temperature measurement methods face the problems of\nsafety and deployment. In this paper, to realize safe and accurate temperature\nmeasurement even when a person's face is partially obscured, we propose a\ncloud-edge-terminal collaborative system with a lightweight infrared\ntemperature measurement model. A binocular camera with an RGB lens and a\nthermal lens is utilized to simultaneously capture image pairs. Then, a mobile\ndetection model based on a multi-task cascaded convolutional network (MTCNN) is\nproposed to realize face alignment and mask detection on the RGB images. For\naccurate temperature measurement, we transform the facial landmarks on the RGB\nimages to the thermal images by an affine transformation and select a more\naccurate temperature measurement area on the forehead. The collected\ninformation is uploaded to the cloud in real time for COVID-19 prevention.\nExperiments show that the detection model is only 6.1M and the average\ndetection speed is 257ms. At a distance of 1m, the error of indoor temperature\nmeasurement is about 3%. That is, the proposed system can realize real-time\ntemperature measurement in public areas.",
          "link": "http://arxiv.org/abs/2107.05078",
          "publishedOn": "2021-07-13T01:59:34.167Z",
          "wordCount": 703,
          "title": "A Cloud-Edge-Terminal Collaborative System for Temperature Measurement in COVID-19 Prevention. (arXiv:2107.05078v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>",
          "description": "The advancement of convolutional neural networks (CNNs) on various vision\napplications has attracted lots of attention. Yet the majority of CNNs are\nunable to satisfy the strict requirement for real-world deployment. To overcome\nthis, the recent popular network pruning is an effective method to reduce the\nredundancy of the models. However, the ranking of filters according to their\n\"importance\" on different pruning criteria may be inconsistent. One filter\ncould be important according to a certain criterion, while it is unnecessary\naccording to another one, which indicates that each criterion is only a partial\nview of the comprehensive \"importance\". From this motivation, we propose a\nnovel framework to integrate the existing filter pruning criteria by exploring\nthe criteria diversity. The proposed framework contains two stages: Criteria\nClustering and Filters Importance Calibration. First, we condense the pruning\ncriteria via layerwise clustering based on the rank of \"importance\" score.\nSecond, within each cluster, we propose a calibration factor to adjust their\nsignificance for each selected blending candidates and search for the optimal\nblending criterion via Evolutionary Algorithm. Quantitative results on the\nCIFAR-100 and ImageNet benchmarks show that our framework outperforms the\nstate-of-the-art baselines, regrading to the compact model performance after\npruning.",
          "link": "http://arxiv.org/abs/2107.05033",
          "publishedOn": "2021-07-13T01:59:34.161Z",
          "wordCount": 642,
          "title": "Blending Pruning Criteria for Convolutional Neural Networks. (arXiv:2107.05033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi-Geng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hui-Chu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>",
          "description": "Visual object localization is the key step in a series of object detection\ntasks. In the literature, high localization accuracy is achieved with the\nmainstream strongly supervised frameworks. However, such methods require\nobject-level annotations and are unable to detect objects of unknown\ncategories. Weakly supervised methods face similar difficulties. In this paper,\na self-paced learning framework is proposed to achieve accurate object\nlocalization on the rank list returned by instance search. The proposed\nframework mines the target instance gradually from the queries and their\ncorresponding top-ranked search results. Since a common instance is shared\nbetween the query and the images in the rank list, the target visual instance\ncan be accurately localized even without knowing what the object category is.\nIn addition to performing localization on instance search, the issue of\nfew-shot object detection is also addressed under the same framework. Superior\nperformance over state-of-the-art methods is observed on both tasks.",
          "link": "http://arxiv.org/abs/2107.05005",
          "publishedOn": "2021-07-13T01:59:34.154Z",
          "wordCount": 590,
          "title": "Towards Accurate Localization by Instance Search. (arXiv:2107.05005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1\">Kenta Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1\">Daisuke Iwai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1\">Kosuke Sato</a>",
          "description": "We propose a novel projector-camera system (ProCams) in which each pixel has\nboth projection and capturing capabilities. Our proposed ProCams solves the\ndifficulty of obtaining precise pixel correspondence between the projector and\nthe camera. We implemented a proof-of-concept ProCams prototype and\ndemonstrated its applicability to a dynamic projection mapping.",
          "link": "http://arxiv.org/abs/2107.05043",
          "publishedOn": "2021-07-13T01:59:34.147Z",
          "wordCount": 517,
          "title": "A Projector-Camera System Using Hybrid Pixels with Projection and Capturing Capabilities. (arXiv:2107.05043v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Atul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajesh_B/0/1/0/all/0/1\">Bulla Rajesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "Plant leaf diseases pose a significant danger to food security and they cause\ndepletion in quality and volume of production. Therefore accurate and timely\ndetection of leaf disease is very important to check the loss of the crops and\nmeet the growing food demand of the people. Conventional techniques depend on\nlab investigation and human skills which are generally costly and inaccessible.\nRecently, Deep Neural Networks have been exceptionally fruitful in image\nclassification. In this research paper, plant leaf disease detection employing\ntransfer learning is explored in the JPEG compressed domain. Here, the JPEG\ncompressed stream consisting of DCT coefficients is, directly fed into the\nNeural Network to improve the efficiency of classification. The experimental\nresults on JPEG compressed leaf dataset demonstrate the efficacy of the\nproposed model.",
          "link": "http://arxiv.org/abs/2107.04813",
          "publishedOn": "2021-07-13T01:59:34.141Z",
          "wordCount": 601,
          "title": "Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique. (arXiv:2107.04813v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu-Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chun-Chieh Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kuo-Chin Fan</a>",
          "description": "The development of lightweight object detectors is essential due to the\nlimited computation resources. To reduce the computation cost, how to generate\nredundant features plays a significant role. This paper proposes a new\nlightweight Convolution method Cross-Stage Lightweight (CSL) Module, to\ngenerate redundant features from cheap operations. In the intermediate\nexpansion stage, we replaced Pointwise Convolution with Depthwise Convolution\nto produce candidate features. The proposed CSL-Module can reduce the\ncomputation cost significantly. Experiments conducted at MS-COCO show that the\nproposed CSL-Module can approximate the fitting ability of Convolution-3x3.\nFinally, we use the module to construct a lightweight detector CSL-YOLO,\nachieving better detection performance with only 43% FLOPs and 52% parameters\nthan Tiny-YOLOv4.",
          "link": "http://arxiv.org/abs/2107.04829",
          "publishedOn": "2021-07-13T01:59:34.135Z",
          "wordCount": 551,
          "title": "CSL-YOLO: A New Lightweight Object Detection System for Edge Computing. (arXiv:2107.04829v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danny Z. Chen</a>",
          "description": "A large labeled dataset is a key to the success of supervised deep learning,\nbut for medical image segmentation, it is highly challenging to obtain\nsufficient annotated images for model training. In many scenarios, unannotated\nimages are abundant and easy to acquire. Self-supervised learning (SSL) has\nshown great potentials in exploiting raw data information and representation\nlearning. In this paper, we propose Hierarchical Self-Supervised Learning\n(HSSL), a new self-supervised framework that boosts medical image segmentation\nby making good use of unannotated data. Unlike the current literature on\ntask-specific self-supervised pretraining followed by supervised fine-tuning,\nwe utilize SSL to learn task-agnostic knowledge from heterogeneous data for\nvarious medical image segmentation tasks. Specifically, we first aggregate a\ndataset from several medical challenges, then pre-train the network in a\nself-supervised manner, and finally fine-tune on labeled data. We develop a new\nloss function by combining contrastive loss and classification loss and\npretrain an encoder-decoder architecture for segmentation tasks. Our extensive\nexperiments show that multi-domain joint pre-training benefits downstream\nsegmentation tasks and outperforms single-domain pre-training significantly.\nCompared to learning from scratch, our new method yields better performance on\nvarious tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotated\ndata). With limited amounts of training data, our method can substantially\nbridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% of\nannotated data).",
          "link": "http://arxiv.org/abs/2107.04886",
          "publishedOn": "2021-07-13T01:59:34.128Z",
          "wordCount": 676,
          "title": "Hierarchical Self-Supervised Learning for Medical Image Segmentation Based on Multi-Domain Data Aggregation. (arXiv:2107.04886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>",
          "description": "Domain adaptation (DA) approaches address domain shift and enable networks to\nbe applied to different scenarios. Although various image DA approaches have\nbeen proposed in recent years, there is limited research towards video DA. This\nis partly due to the complexity in adapting the different modalities of\nfeatures in videos, which includes the correlation features extracted as\nlong-term dependencies of pixels across spatiotemporal dimensions. The\ncorrelation features are highly associated with action classes and proven their\neffectiveness in accurate video feature extraction through the supervised\naction recognition task. Yet correlation features of the same action would\ndiffer across domains due to domain shift. Therefore we propose a novel\nAdversarial Correlation Adaptation Network (ACAN) to align action videos by\naligning pixel correlations. ACAN aims to minimize the distribution of\ncorrelation information, termed as Pixel Correlation Discrepancy (PCD).\nAdditionally, video DA research is also limited by the lack of cross-domain\nvideo datasets with larger domain shifts. We, therefore, introduce a novel\nHMDB-ARID dataset with a larger domain shift caused by a larger statistical\ndifference between domains. This dataset is built in an effort to leverage\ncurrent datasets for dark video classification. Empirical results demonstrate\nthe state-of-the-art performance of our proposed ACAN for both existing and the\nnew video DA datasets.",
          "link": "http://arxiv.org/abs/2107.04932",
          "publishedOn": "2021-07-13T01:59:34.097Z",
          "wordCount": 663,
          "title": "Aligning Correlation Information for Domain Adaptation in Action Recognition. (arXiv:2107.04932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1\">Euijoon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dagan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>",
          "description": "The segmentation of medical images is a fundamental step in automated\nclinical decision support systems. Existing medical image segmentation methods\nbased on supervised deep learning, however, remain problematic because of their\nreliance on large amounts of labelled training data. Although medical imaging\ndata repositories continue to expand, there has not been a commensurate\nincrease in the amount of annotated data. Hence, we propose a new spatial\nguided self-supervised clustering network (SGSCN) for medical image\nsegmentation, where we introduce multiple loss functions designed to aid in\ngrouping image pixels that are spatially connected and have similar feature\nrepresentations. It iteratively learns feature representations and clustering\nassignment of each pixel in an end-to-end fashion from a single image. We also\npropose a context-based consistency loss that better delineates the shape and\nboundaries of image regions. It enforces all the pixels belonging to a cluster\nto be spatially close to the cluster centre. We evaluated our method on 2\npublic medical image datasets and compared it to existing conventional and\nself-supervised clustering methods. Experimental results show that our method\nwas most accurate for medical image segmentation.",
          "link": "http://arxiv.org/abs/2107.04934",
          "publishedOn": "2021-07-13T01:59:34.089Z",
          "wordCount": 632,
          "title": "A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation. (arXiv:2107.04934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young Kyun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>",
          "description": "Face image retrieval, which searches for images of the same identity from the\nquery input face image, is drawing more attention as the size of the image\ndatabase increases rapidly. In order to conduct fast and accurate retrieval, a\ncompact hash code-based methods have been proposed, and recently, deep face\nimage hashing methods with supervised classification training have shown\noutstanding performance. However, classification-based scheme has a\ndisadvantage in that it cannot reveal complex similarities between face images\ninto the hash code learning. In this paper, we attempt to improve the face\nimage retrieval quality by proposing a Similarity Guided Hashing (SGH) method,\nwhich gently considers self and pairwise-similarity simultaneously. SGH employs\nvarious data augmentations designed to explore elaborate similarities between\nface images, solving both intra and inter identity-wise difficulties. Extensive\nexperimental results on the protocols with existing benchmarks and an\nadditionally proposed large scale higher resolution face image dataset\ndemonstrate that our SGH delivers state-of-the-art retrieval performance.",
          "link": "http://arxiv.org/abs/2107.05025",
          "publishedOn": "2021-07-13T01:59:34.078Z",
          "wordCount": 596,
          "title": "Similarity Guided Deep Face Image Retrieval. (arXiv:2107.05025v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essam_M/0/1/0/all/0/1\">Mariam Essam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Maha Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sallab_A/0/1/0/all/0/1\">Ahmad El Sallab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Detection of moving objects is a very important task in autonomous driving\nsystems. After the perception phase, motion planning is typically performed in\nBird's Eye View (BEV) space. This would require projection of objects detected\non the image plane to top view BEV plane. Such a projection is prone to errors\ndue to lack of depth information and noisy mapping in far away areas. CNNs can\nleverage the global context in the scene to project better. In this work, we\nexplore end-to-end Moving Object Detection (MOD) on the BEV map directly using\nmonocular images as input. To the best of our knowledge, such a dataset does\nnot exist and we create an extended KITTI-raw dataset consisting of 12.9k\nimages with annotations of moving object masks in BEV space for five classes.\nThe dataset is intended to be used for class agnostic motion cue based object\ndetection and classes are provided as meta-data for better tuning. We design\nand implement a two-stream RGB and optical flow fusion architecture which\noutputs motion segmentation directly in BEV space. We compare it with inverse\nperspective mapping of state-of-the-art motion segmentation predictions on the\nimage plane. We observe a significant improvement of 13% in mIoU using the\nsimple baseline implementation. This demonstrates the ability to directly learn\nmotion segmentation output in BEV space. Qualitative results of our baseline\nand the dataset annotations can be found in\nhttps://sites.google.com/view/bev-modnet.",
          "link": "http://arxiv.org/abs/2107.04937",
          "publishedOn": "2021-07-13T01:59:34.067Z",
          "wordCount": 699,
          "title": "BEV-MODNet: Monocular Camera based Bird's Eye View Moving Object Detection for Autonomous Driving. (arXiv:2107.04937v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1\">Gaurav Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandok_S/0/1/0/all/0/1\">Shivam Chandok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "A common problem with most zero and few-shot learning approaches is they\nsuffer from bias towards seen classes resulting in sub-optimal performance.\nExisting efforts aim to utilize unlabeled images from unseen classes (i.e\ntransductive zero-shot) during training to enable generalization. However, this\nlimits their use in practical scenarios where data from target unseen classes\nis unavailable or infeasible to collect. In this work, we present a practical\nsetting of inductive zero and few-shot learning, where unlabeled images from\nother out-of-data classes, that do not belong to seen or unseen categories, can\nbe used to improve generalization in any-shot learning. We leverage a\nformulation based on product-of-experts and introduce a new AUD module that\nenables us to use unlabeled samples from out-of-data classes which are usually\neasily available and practically entail no annotation cost. In addition, we\nalso demonstrate the applicability of our model to address a more practical and\nchallenging, Generalized Zero-shot under a limited supervision setting, where\neven base seen classes do not have sufficient annotated samples.",
          "link": "http://arxiv.org/abs/2107.04952",
          "publishedOn": "2021-07-13T01:59:34.057Z",
          "wordCount": 614,
          "title": "Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision. (arXiv:2107.04952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Rose McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>",
          "description": "It is challenging for humans to enable visual knowledge discovery in data\nwith more than 2-3 dimensions with a naked eye. This chapter explores the\nefficiency of discovering predictive machine learning models interactively\nusing new Elliptic Paired coordinates (EPC) visualizations. It is shown that\nEPC are capable to visualize multidimensional data and support visual machine\nlearning with preservation of multidimensional information in 2-D. Relative to\nparallel and radial coordinates, EPC visualization requires only a half of the\nvisual elements for each n-D point. An interactive software system EllipseVis,\nwhich is developed in this work, processes high-dimensional datasets, creates\nEPC visualizations, and produces predictive classification models by\ndiscovering dominance rules in EPC. By using interactive and automatic\nprocesses it discovers zones in EPC with a high dominance of a single class.\nThe EPC methodology has been successful in discovering non-linear predictive\nmodels with high coverage and precision in the computational experiments. This\ncan benefit multiple domains by producing visually appealing dominance rules.\nThis chapter presents results of successful testing the EPC non-linear\nmethodology in experiments using real and simulated data, EPC generalized to\nthe Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of\ncoordinates to optimize the visual discovery, introduction of an alternative\nEPC design and introduction of the concept of incompact machine learning\nmethodology based on EPC/DEPC.",
          "link": "http://arxiv.org/abs/2107.04974",
          "publishedOn": "2021-07-13T01:59:34.020Z",
          "wordCount": 660,
          "title": "Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates. (arXiv:2107.04974v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hadviger_A/0/1/0/all/0/1\">Antea Hadviger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cvisic_I/0/1/0/all/0/1\">Igor Cvi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovic_I/0/1/0/all/0/1\">Ivan Markovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrazic_S/0/1/0/all/0/1\">Sacha Vra&#x17e;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovic_I/0/1/0/all/0/1\">Ivan Petrovi&#x107;</a>",
          "description": "Event-based cameras are biologically inspired sensors that output events,\ni.e., asynchronous pixel-wise brightness changes in the scene. Their high\ndynamic range and temporal resolution of a microsecond makes them more reliable\nthan standard cameras in environments of challenging illumination and in\nhigh-speed scenarios, thus developing odometry algorithms based solely on event\ncameras offers exciting new possibilities for autonomous systems and robots. In\nthis paper, we propose a novel stereo visual odometry method for event cameras\nbased on feature detection and matching with careful feature management, while\npose estimation is done by reprojection error minimization. We evaluate the\nperformance of the proposed method on two publicly available datasets: MVSEC\nsequences captured by an indoor flying drone and DSEC outdoor driving\nsequences. MVSEC offers accurate ground truth from motion capture, while for\nDSEC, which does not offer ground truth, in order to obtain a reference\ntrajectory on the standard camera frames we used our SOFT visual odometry, one\nof the highest ranking algorithms on the KITTI scoreboards. We compared our\nmethod to the ESVO method, which is the first and still the only stereo event\nodometry method, showing on par performance on the MVSEC sequences, while on\nthe DSEC dataset ESVO, unlike our method, was unable to handle outdoor driving\nscenario with default parameters. Furthermore, two important advantages of our\nmethod over ESVO are that it adapts tracking frequency to the asynchronous\nevent rate and does not require initialization.",
          "link": "http://arxiv.org/abs/2107.04921",
          "publishedOn": "2021-07-13T01:59:34.006Z",
          "wordCount": 679,
          "title": "Feature-based Event Stereo Visual Odometry. (arXiv:2107.04921v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04943",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1\">Xiaohong Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jianping Zhang</a>",
          "description": "Compressed sensing (CS) is an efficient method to reconstruct MR image from\nsmall sampled data in $k$-space and accelerate the acquisition of MRI. In this\nwork, we propose a novel deep geometric distillation network which combines the\nmerits of model-based and deep learning-based CS-MRI methods, it can be\ntheoretically guaranteed to improve geometric texture details of a linear\nreconstruction. Firstly, we unfold the model-based CS-MRI optimization problem\ninto two sub-problems that consist of image linear approximation and image\ngeometric compensation. Secondly, geometric compensation sub-problem for\ndistilling lost texture details in approximation stage can be expanded by\nTaylor expansion to design a geometric distillation module fusing features of\ndifferent geometric characteristic domains. Additionally, we use a learnable\nversion with adaptive initialization of the step-length parameter, which allows\nmodel more flexibility that can lead to convergent smoothly. Numerical\nexperiments verify its superiority over other state-of-the-art CS-MRI\nreconstruction approaches. The source code will be available at\n\\url{https://github.com/fanxiaohong/Deep-Geometric-Distillation-Network-for-CS-MRI}",
          "link": "http://arxiv.org/abs/2107.04943",
          "publishedOn": "2021-07-13T01:59:33.999Z",
          "wordCount": 623,
          "title": "Deep Geometric Distillation Network for Compressive Sensing MRI. (arXiv:2107.04943v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04847",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuangzhuang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gay_H/0/1/0/all/0/1\">Hiram Gay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_B/0/1/0/all/0/1\">Baozhou Sun</a>",
          "description": "In radiotherapy planning, manual contouring is labor-intensive and\ntime-consuming. Accurate and robust automated segmentation models improve the\nefficiency and treatment outcome. We aim to develop a novel hybrid deep\nlearning approach, combining convolutional neural networks (CNNs) and the\nself-attention mechanism, for rapid and accurate multi-organ segmentation on\nhead and neck computed tomography (CT) images. Head and neck CT images with\nmanual contours of 115 patients were retrospectively collected and used. We set\nthe training/validation/testing ratio to 81/9/25 and used the 10-fold\ncross-validation strategy to select the best model parameters. The proposed\nhybrid model segmented ten organs-at-risk (OARs) altogether for each case. The\nperformance of the model was evaluated by three metrics, i.e., the Dice\nSimilarity Coefficient (DSC), Hausdorff distance 95% (HD95), and mean surface\ndistance (MSD). We also tested the performance of the model on the Head and\nNeck 2015 challenge dataset and compared it against several state-of-the-art\nautomated segmentation algorithms. The proposed method generated contours that\nclosely resemble the ground truth for ten OARs. Our results of the new Weaving\nAttention U-net demonstrate superior or similar performance on the segmentation\nof head and neck CT images.",
          "link": "http://arxiv.org/abs/2107.04847",
          "publishedOn": "2021-07-13T01:59:33.993Z",
          "wordCount": 660,
          "title": "Weaving Attention U-net: A Novel Hybrid CNN and Attention-based Method for Organs-at-risk Segmentation in Head and Neck CT Images. (arXiv:2107.04847v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotseruba_I/0/1/0/all/0/1\">Iuliia Kotseruba</a>",
          "description": "This work aims to study the dynamic between research in the industry and\nacademia in computer vision. The results are demonstrated on a set of top-5\nvision conferences that are representative of the field. Since data for such\nanalysis was not readily available, significant effort was spent on gathering\nand processing meta-data from the original publications. First, this study\nquantifies the share of industry-sponsored research. Specifically, it shows\nthat the proportion of papers published by industry-affiliated researchers is\nincreasing and that more academics join companies or collaborate with them.\nNext, the possible impact of industry presence is further explored, namely in\nthe distribution of research topics and citation patterns. The results indicate\nthat the distribution of the research topics is similar in industry and\nacademic papers. However, there is a strong preference towards citing industry\npapers. Finally, possible reasons for citation bias, such as code availability\nand influence, are investigated.",
          "link": "http://arxiv.org/abs/2107.04902",
          "publishedOn": "2021-07-13T01:59:33.987Z",
          "wordCount": 585,
          "title": "Industry and Academic Research in Computer Vision. (arXiv:2107.04902v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Periyasamy_A/0/1/0/all/0/1\">Arul Selvam Periyasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1\">Max Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "We present SynPick, a synthetic dataset for dynamic scene understanding in\nbin-picking scenarios. In contrast to existing datasets, our dataset is both\nsituated in a realistic industrial application domain -- inspired by the\nwell-known Amazon Robotics Challenge (ARC) -- and features dynamic scenes with\nauthentic picking actions as chosen by our picking heuristic developed for the\nARC 2017. The dataset is compatible with the popular BOP dataset format. We\ndescribe the dataset generation process in detail, including object arrangement\ngeneration and manipulation simulation using the NVIDIA PhysX physics engine.\nTo cover a large action space, we perform untargeted and targeted picking\nactions, as well as random moving actions. To establish a baseline for object\nperception, a state-of-the-art pose estimation approach is evaluated on the\ndataset. We demonstrate the usefulness of tracking poses during manipulation\ninstead of single-shot estimation even with a naive filtering approach. The\ngenerator source code and dataset are publicly available.",
          "link": "http://arxiv.org/abs/2107.04852",
          "publishedOn": "2021-07-13T01:59:33.969Z",
          "wordCount": 606,
          "title": "SynPick: A Dataset for Dynamic Bin Picking Scene Understanding. (arXiv:2107.04852v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uwimana1_A/0/1/0/all/0/1\">Anisie Uwimana1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>",
          "description": "Deep learning models have become a popular choice for medical image analysis.\nHowever, the poor generalization performance of deep learning models limits\nthem from being deployed in the real world as robustness is critical for\nmedical applications. For instance, the state-of-the-art Convolutional Neural\nNetworks (CNNs) fail to detect adversarial samples or samples drawn\nstatistically far away from the training distribution. In this work, we\nexperimentally evaluate the robustness of a Mahalanobis distance-based\nconfidence score, a simple yet effective method for detecting abnormal input\nsamples, in classifying malaria parasitized cells and uninfected cells. Results\nindicated that the Mahalanobis confidence score detector exhibits improved\nperformance and robustness of deep learning models, and achieves\nstateof-the-art performance on both out-of-distribution (OOD) and adversarial\nsamples.",
          "link": "http://arxiv.org/abs/2107.04882",
          "publishedOn": "2021-07-13T01:59:33.962Z",
          "wordCount": 578,
          "title": "Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis. (arXiv:2107.04882v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhuheir_M/0/1/0/all/0/1\">Marwan Dhuheir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1\">Emna Baccour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabeeh_S/0/1/0/all/0/1\">Sinan Sabeeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1\">Mounir Hamdi</a>",
          "description": "Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention\ndue to their outstanding ability to be used in different sectors and serve in\ndifficult and dangerous areas. Moreover, the advancements in computer vision\nand artificial intelligence have increased the use of UAVs in various\napplications and solutions, such as forest fires detection and borders\nmonitoring. However, using deep neural networks (DNNs) with UAVs introduces\nseveral challenges of processing deeper networks and complex models, which\nrestricts their on-board computation. In this work, we present a strategy\naiming at distributing inference requests to a swarm of resource-constrained\nUAVs that classifies captured images on-board and finds the minimum\ndecision-making latency. We formulate the model as an optimization problem that\nminimizes the latency between acquiring images and making the final decisions.\nThe formulated optimization solution is an NP-hard problem. Hence it is not\nadequate for online resource allocation. Therefore, we introduce an online\nheuristic solution, namely DistInference, to find the layers placement strategy\nthat gives the best latency among the available UAVs. The proposed approach is\ngeneral enough to be used for different low decision-latency applications as\nwell as for all CNN types organized into the pipeline of layers (e.g., VGG) or\nbased on residual blocks (e.g., ResNet).",
          "link": "http://arxiv.org/abs/2107.04648",
          "publishedOn": "2021-07-13T01:59:33.955Z",
          "wordCount": 677,
          "title": "Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs and Convolutional Networks. (arXiv:2107.04648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>",
          "description": "Partial Domain Adaptation (PDA) is a practical and general domain adaptation\nscenario, which relaxes the fully shared label space assumption such that the\nsource label space subsumes the target one. The key challenge of PDA is the\nissue of negative transfer caused by source-only classes. For videos, such\nnegative transfer could be triggered by both spatial and temporal features,\nwhich leads to a more challenging Partial Video Domain Adaptation (PVDA)\nproblem. In this paper, we propose a novel Partial Adversarial Temporal\nAttentive Network (PATAN) to address the PVDA problem by utilizing both spatial\nand temporal features for filtering source-only classes. Besides, PATAN\nconstructs effective overall temporal features by attending to local temporal\nfeatures that contribute more toward the class filtration process. We further\nintroduce new benchmarks to facilitate research on PVDA problems, covering a\nwide range of PVDA scenarios. Empirical results demonstrate the\nstate-of-the-art performance of our proposed PATAN across the multiple PVDA\nbenchmarks.",
          "link": "http://arxiv.org/abs/2107.04941",
          "publishedOn": "2021-07-13T01:59:33.948Z",
          "wordCount": 617,
          "title": "Partial Video Domain Adaptation with Partial Adversarial Temporal Attentive Network. (arXiv:2107.04941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04808",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Miron_R/0/1/0/all/0/1\">Radu Miron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moisii_C/0/1/0/all/0/1\">Cosmin Moisii</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dinu_S/0/1/0/all/0/1\">Sergiu Dinu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breaban_M/0/1/0/all/0/1\">Mihaela Breaban</a>",
          "description": "The paper presents a comparative analysis of three distinct approaches based\non deep learning for COVID-19 detection in chest CTs. The first approach is a\nvolumetric one, involving 3D convolutions, while the other two approaches\nperform at first slice-wise classification and then aggregate the results at\nthe volume level. The experiments are carried on the COV19-CT-DB dataset, with\nthe aim of addressing the challenge raised by the MIA-COV19D Competition within\nICCV 2021. Our best results on the validation subset reach a macro-F1 score of\n0.92, which improves considerably the baseline score of 0.70 set by the\norganizers.",
          "link": "http://arxiv.org/abs/2107.04808",
          "publishedOn": "2021-07-13T01:59:33.940Z",
          "wordCount": 590,
          "title": "COVID Detection in Chest CTs: Improving the Baseline on COV19-CT-DB. (arXiv:2107.04808v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1\">Nikos Makris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J. O&#x27;Donnell</a>",
          "description": "White matter fiber clustering (WMFC) enables parcellation of white matter\ntractography for applications such as disease classification and anatomical\ntract segmentation. However, the lack of ground truth and the ambiguity of\nfiber data (the points along a fiber can equivalently be represented in forward\nor reverse order) pose challenges to this task. We propose a novel WMFC\nframework based on unsupervised deep learning. We solve the unsupervised\nclustering problem as a self-supervised learning task. Specifically, we use a\nconvolutional neural network to learn embeddings of input fibers, using\npairwise fiber distances as pseudo annotations. This enables WMFC that is\ninsensitive to fiber point ordering. In addition, anatomical coherence of fiber\nclusters is improved by incorporating brain anatomical segmentation data. The\nproposed framework enables outlier removal in a natural way by rejecting fibers\nwith low cluster assignment probability. We train and evaluate our method using\n200 datasets from the Human Connectome Project. Results demonstrate superior\nperformance and efficiency of the proposed approach.",
          "link": "http://arxiv.org/abs/2107.04938",
          "publishedOn": "2021-07-13T01:59:33.914Z",
          "wordCount": 623,
          "title": "Deep Fiber Clustering: Anatomically Informed Unsupervised Deep Learning for Fast and Effective White Matter Parcellation. (arXiv:2107.04938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04930",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Teli_M/0/1/0/all/0/1\">Mohammad Nayeem Teli</a>",
          "description": "Hundreds of millions of cases and millions of deaths have occurred worldwide\ndue to COVID-19. The fight against this pandemic is on-going on multiple\nfronts. While vaccinations are picking up speed, there are still billions of\nunvaccinated people. In this fight diagnosis of the disease and isolation of\nthe patients to prevent any spreads play a huge role. Machine Learning\napproaches have assisted the diagnosis of COVID-19 cases by analyzing chest\nX-ray and CT-scan images of patients. In this research we present a simple and\nshallow Convolutional Neural Network based approach, TeliNet, to classify\nCT-scan images of COVID-19 patients. Our results outperform the F1 score of\nVGGNet and the benchmark approaches. Our proposed solution is also more\nlightweight in comparison to the other methods.",
          "link": "http://arxiv.org/abs/2107.04930",
          "publishedOn": "2021-07-13T01:59:33.905Z",
          "wordCount": 631,
          "title": "TeliNet, a simple and shallow Convolution Neural Network (CNN) to Classify CT Scans of COVID-19 patients. (arXiv:2107.04930v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">An-Chieh Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xueting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>",
          "description": "We propose a canonical point autoencoder (CPAE) that predicts dense\ncorrespondences between 3D shapes of the same category. The autoencoder\nperforms two key functions: (a) encoding an arbitrarily ordered point cloud to\na canonical primitive, e.g., a sphere, and (b) decoding the primitive back to\nthe original input instance shape. As being placed in the bottleneck, this\nprimitive plays a key role to map all the unordered point clouds on the\ncanonical surface and to be reconstructed in an ordered fashion. Once trained,\npoints from different shape instances that are mapped to the same locations on\nthe primitive surface are determined to be a pair of correspondence. Our method\ndoes not require any form of annotation or self-supervised part segmentation\nnetwork and can handle unaligned input point clouds. Experimental results on 3D\nsemantic keypoint transfer and part segmentation transfer show that our model\nperforms favorably against state-of-the-art correspondence learning methods.",
          "link": "http://arxiv.org/abs/2107.04867",
          "publishedOn": "2021-07-13T01:59:33.894Z",
          "wordCount": 587,
          "title": "Learning 3D Dense Correspondence via Canonical Point Autoencoder. (arXiv:2107.04867v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zakazov_I/0/1/0/all/0/1\">Ivan Zakazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirokikh_B/0/1/0/all/0/1\">Boris Shirokikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Alexey Chernyavskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>",
          "description": "Domain Adaptation (DA) methods are widely used in medical image segmentation\ntasks to tackle the problem of differently distributed train (source) and test\n(target) data. We consider the supervised DA task with a limited number of\nannotated samples from the target domain. It corresponds to one of the most\nrelevant clinical setups: building a sufficiently accurate model on the minimum\npossible amount of annotated data. Existing methods mostly fine-tune specific\nlayers of the pretrained Convolutional Neural Network (CNN). However, there is\nno consensus on which layers are better to fine-tune, e.g. the first layers for\nimages with low-level domain shift or the deeper layers for images with\nhigh-level domain shift. To this end, we propose SpotTUnet - a CNN architecture\nthat automatically chooses the layers which should be optimally fine-tuned.\nMore specifically, on the target domain, our method additionally learns the\npolicy that indicates whether a specific layer should be fine-tuned or reused\nfrom the pretrained network. We show that our method performs at the same level\nas the best of the nonflexible fine-tuning methods even under the extreme\nscarcity of annotated data. Secondly, we show that SpotTUnet policy provides a\nlayer-wise visualization of the domain shift impact on the network, which could\nbe further used to develop robust domain generalization methods. In order to\nextensively evaluate SpotTUnet performance, we use a publicly available dataset\nof brain MR images (CC359), characterized by explicit domain shift. We release\na reproducible experimental pipeline.",
          "link": "http://arxiv.org/abs/2107.04914",
          "publishedOn": "2021-07-13T01:59:33.886Z",
          "wordCount": 698,
          "title": "Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation. (arXiv:2107.04914v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04823",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhonghua Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jiewei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yijin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_J/0/1/0/all/0/1\">Junyan Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jiong Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>",
          "description": "Optical coherence tomography angiography (OCTA) is a novel non-invasive\nimaging technique that allows visualizations of vasculature and foveal\navascular zone (FAZ) across retinal layers. Clinical researches suggest that\nthe morphology and contour irregularity of FAZ are important biomarkers of\nvarious ocular pathologies. Therefore, precise segmentation of FAZ has great\nclinical interest. Also, there is no existing research reporting that FAZ\nfeatures can improve the performance of deep diagnostic classification\nnetworks. In this paper, we propose a novel multi-level boundary shape and\ndistance aware joint learning framework, named BSDA-Net, for FAZ segmentation\nand diagnostic classification from OCTA images. Two auxiliary branches, namely\nboundary heatmap regression and signed distance map reconstruction branches,\nare constructed in addition to the segmentation branch to improve the\nsegmentation performance, resulting in more accurate FAZ contours and fewer\noutliers. Moreover, both low-level and high-level features from the\naforementioned three branches, including shape, size, boundary, and signed\ndirectional distance map of FAZ, are fused hierarchically with features from\nthe diagnostic classifier. Through extensive experiments, the proposed BSDA-Net\nis found to yield state-of-the-art segmentation and classification results on\nthe OCTA-500, OCTAGON, and FAZID datasets.",
          "link": "http://arxiv.org/abs/2107.04823",
          "publishedOn": "2021-07-13T01:59:33.878Z",
          "wordCount": 658,
          "title": "BSDA-Net: A Boundary Shape and Distance Aware Joint Learning Framework for Segmenting and Classifying OCTA Images. (arXiv:2107.04823v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_F/0/1/0/all/0/1\">Feng Shuang</a>",
          "description": "Monocular depth estimation (MDE) is a fundamental task in many applications\nsuch as scene understanding and reconstruction. However, most of the existing\nmethods rely on accurately labeled datasets. A weakly-supervised framework\nbased on attention nested U-net (ANU) named as ANUW is introduced in this paper\nfor cases with wrong labels. The ANUW is trained end-to-end to convert an input\nsingle RGB image into a depth image. It consists of a dense residual network\nstructure, an adaptive weight channel attention (AWCA) module, a patch second\nnon-local (PSNL) module and a soft label generation method. The dense residual\nnetwork is the main body of the network to encode and decode the input. The\nAWCA module can adaptively adjust the channel weights to extract important\nfeatures. The PSNL module implements the spatial attention mechanism through a\nsecond-order non-local method. The proposed soft label generation method uses\nthe prior knowledge of the dataset to produce soft labels to replace false\nones. The proposed ANUW is trained on a defective monocular depth dataset and\nthe trained model is tested on three public datasets, and the results\ndemonstrate the superiority of ANUW in comparison with the state-of-the-art MDE\nmethods.",
          "link": "http://arxiv.org/abs/2107.04819",
          "publishedOn": "2021-07-13T01:59:33.855Z",
          "wordCount": 636,
          "title": "A Weakly-Supervised Depth Estimation Network Using Attention Mechanism. (arXiv:2107.04819v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_S/0/1/0/all/0/1\">Shea Hagstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pak_H/0/1/0/all/0/1\">Hee Won Pak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_S/0/1/0/all/0/1\">Stephanie Ku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sean Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Myron Brown</a>",
          "description": "Urban 3D modeling from satellite images requires accurate semantic\nsegmentation to delineate urban features, multiple view stereo for 3D\nreconstruction of surface heights, and 3D model fitting to produce compact\nmodels with accurate surface slopes. In this work, we present a cumulative\nassessment metric that succinctly captures error contributions from each of\nthese components. We demonstrate our approach by providing challenging public\ndatasets and extending two open source projects to provide an end-to-end 3D\nmodeling baseline solution to stimulate further research and evaluation with a\npublic leaderboard.",
          "link": "http://arxiv.org/abs/2107.04622",
          "publishedOn": "2021-07-13T01:59:33.849Z",
          "wordCount": 536,
          "title": "Cumulative Assessment for Urban 3D Modeling. (arXiv:2107.04622v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "In this paper, we propose a new continuously learning generative model,\ncalled the Lifelong Twin Generative Adversarial Networks (LT-GANs). LT-GANs\nlearns a sequence of tasks from several databases and its architecture consists\nof three components: two identical generators, namely the Teacher and\nAssistant, and one Discriminator. In order to allow for the LT-GANs to learn\nnew concepts without forgetting, we introduce a new lifelong training approach,\nnamely Lifelong Adversarial Knowledge Distillation (LAKD), which encourages the\nTeacher and Assistant to alternately teach each other, while learning a new\ndatabase. This training approach favours transferring knowledge from a more\nknowledgeable player to another player which knows less information about a\npreviously given task.",
          "link": "http://arxiv.org/abs/2107.04708",
          "publishedOn": "2021-07-13T01:59:33.843Z",
          "wordCount": 551,
          "title": "Lifelong Twin Generative Adversarial Networks. (arXiv:2107.04708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaohua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_X/0/1/0/all/0/1\">Xiuchao Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yangqin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1\">Daniel Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>",
          "description": "Deep neural networks (DNNs) trained on one set of medical images often\nexperience severe performance drop on unseen test images, due to various domain\ndiscrepancy between the training images (source domain) and the test images\n(target domain), which raises a domain adaptation issue. In clinical settings,\nit is difficult to collect enough annotated target domain data in a short\nperiod. Few-shot domain adaptation, i.e., adapting a trained model with a\nhandful of annotations, is highly practical and useful in this case. In this\npaper, we propose a Polymorphic Transformer (Polyformer), which can be\nincorporated into any DNN backbones for few-shot domain adaptation.\nSpecifically, after the polyformer layer is inserted into a model trained on\nthe source domain, it extracts a set of prototype embeddings, which can be\nviewed as a \"basis\" of the source-domain features. On the target domain, the\npolyformer layer adapts by only updating a projection layer which controls the\ninteractions between image features and the prototype embeddings. All other\nmodel weights (except BatchNorm parameters) are frozen during adaptation. Thus,\nthe chance of overfitting the annotations is greatly reduced, and the model can\nperform robustly on the target domain after being trained on a few annotated\nimages. We demonstrate the effectiveness of Polyformer on two medical\nsegmentation tasks (i.e., optic disc/cup segmentation, and polyp segmentation).\nThe source code of Polyformer is released at\nhttps://github.com/askerlee/segtran.",
          "link": "http://arxiv.org/abs/2107.04805",
          "publishedOn": "2021-07-13T01:59:33.836Z",
          "wordCount": 678,
          "title": "Few-Shot Domain Adaptation with Polymorphic Transformers. (arXiv:2107.04805v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shoaib Ahmed Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>",
          "description": "Common neural network architectures are susceptible to attack by adversarial\nsamples. Neural network architectures are commonly thought of as divided into\nlow-level feature extraction layers and high-level classification layers;\nsusceptibility of networks to adversarial samples is often thought of as a\nproblem related to classification rather than feature extraction. We test this\nidea by selectively retraining different portions of VGG and ResNet\narchitectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and\nadversarial data. Our experimental results show that susceptibility to\nadversarial samples is associated with low-level feature extraction layers.\nTherefore, retraining high-level layers is insufficient for achieving\nrobustness. This phenomenon could have two explanations: either, adversarial\nattacks yield outputs from early layers that are indistinguishable from\nfeatures found in the attack classes, or adversarial attacks yield outputs from\nearly layers that differ statistically from features for non-adversarial\nsamples and do not permit consistent classification by subsequent layers. We\ntest this question by large-scale non-linear dimensionality reduction and\ndensity modeling on distributions of feature vectors in hidden layers and find\nthat the feature distributions between non-adversarial and adversarial samples\ndiffer substantially. Our results provide new insights into the statistical\norigins of adversarial samples and possible defenses.",
          "link": "http://arxiv.org/abs/2107.04827",
          "publishedOn": "2021-07-13T01:59:33.829Z",
          "wordCount": 630,
          "title": "Identifying Layers Susceptible to Adversarial Attacks. (arXiv:2107.04827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1\">Wei Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hailan Huang</a>",
          "description": "The seven basic facial expression classifications are a basic way to express\ncomplex human emotions and are an important part of artificial intelligence\nresearch. Based on the traditional Bayesian neural network framework, the\nResNet-18_BNN network constructed in this paper has been improved in the\nfollowing three aspects: (1) A new objective function is proposed, which is\ncomposed of the KL loss of uncertain parameters and the intersection of\nspecific parameters. Entropy loss composition. (2) Aiming at a special\nobjective function, a training scheme for alternately updating these two\nparameters is proposed. (3) Only model the parameters of the last convolution\ngroup. According to experimental analysis, our method achieves an accuracy of\n98.28% on the evaluation set of the Aff-Wild2 database. Compared with the\ntraditional Bayesian Neural Network, our method brings the highest\nclassification accuracy gain.",
          "link": "http://arxiv.org/abs/2107.04834",
          "publishedOn": "2021-07-13T01:59:33.791Z",
          "wordCount": 570,
          "title": "Bayesian Convolutional Neural Networks for Seven Basic Facial Expression Classifications. (arXiv:2107.04834v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_F/0/1/0/all/0/1\">Fangqiu Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingting Jiang</a>",
          "description": "Surgical phase recognition is of particular interest to computer assisted\nsurgery systems, in which the goal is to predict what phase is occurring at\neach frame for a surgery video. Networks with multi-stage architecture have\nbeen widely applied in many computer vision tasks with rich patterns, where a\npredictor stage first outputs initial predictions and an additional refinement\nstage operates on the initial predictions to perform further refinement.\nExisting works show that surgical video contents are well ordered and contain\nrich temporal patterns, making the multi-stage architecture well suited for the\nsurgical phase recognition task. However, we observe that when simply applying\nthe multi-stage architecture to the surgical phase recognition task, the\nend-to-end training manner will make the refinement ability fall short of its\nwishes. To address the problem, we propose a new non end-to-end training\nstrategy and explore different designs of multi-stage architecture for surgical\nphase recognition task. For the non end-to-end training strategy, the\nrefinement stage is trained separately with proposed two types of disturbed\nsequences. Meanwhile, we evaluate three different choices of refinement models\nto show that our analysis and solution are robust to the choices of specific\nmulti-stage models. We conduct experiments on two public benchmarks, the\nM2CAI16 Workflow Challenge, and the Cholec80 dataset. Results show that\nmulti-stage architecture trained with our strategy largely boosts the\nperformance of the current state-of-the-art single-stage model. Code is\navailable at \\url{https://github.com/ChinaYi/casual_tcn}.",
          "link": "http://arxiv.org/abs/2107.04810",
          "publishedOn": "2021-07-13T01:59:33.780Z",
          "wordCount": 679,
          "title": "Not End-to-End: Explore Multi-Stage Architecture for Online Surgical Phase Recognition. (arXiv:2107.04810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1\">Darshan Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1\">S. Balasubramanian</a>",
          "description": "Presence of noise in the labels of large scale facial expression datasets has\nbeen a key challenge towards Facial Expression Recognition (FER) in the wild.\nDuring early learning stage, deep networks fit on clean data. Then, eventually,\nthey start overfitting on noisy labels due to their memorization ability, which\nlimits FER performance. This work proposes an effective training strategy in\nthe presence of noisy labels, called as Consensual Collaborative Training (CCT)\nframework. CCT co-trains three networks jointly using a convex combination of\nsupervision loss and consistency loss, without making any assumption about the\nnoise distribution. A dynamic transition mechanism is used to move from\nsupervision loss in early learning to consistency loss for consensus of\npredictions among networks in the later stage. Inference is done using a single\nnetwork based on a simple knowledge distillation scheme. Effectiveness of the\nproposed framework is demonstrated on synthetic as well as real noisy FER\ndatasets. In addition, a large test subset of around 5K images is annotated\nfrom the FEC dataset using crowd wisdom of 16 different annotators and reliable\nlabels are inferred. CCT is also validated on it. State-of-the-art performance\nis reported on the benchmark FER datasets RAFDB (90.84%) FERPlus (89.99%) and\nAffectNet (66%). Our codes are available at https://github.com/1980x/CCT.",
          "link": "http://arxiv.org/abs/2107.04746",
          "publishedOn": "2021-07-13T01:59:33.759Z",
          "wordCount": 681,
          "title": "Consensual Collaborative Training And Knowledge Distillation Based Facial Expression Recognition Under Noisy Annotations. (arXiv:2107.04746v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ronghang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>",
          "description": "Owing to the remarkable capability of extracting effective graph embeddings,\ngraph convolutional network (GCN) and its variants have been successfully\napplied to a broad range of tasks, such as node classification, link\nprediction, and graph classification. Traditional GCN models suffer from the\nissues of overfitting and oversmoothing, while some recent techniques like\nDropEdge could alleviate these issues and thus enable the development of deep\nGCN. However, training GCN models is non-trivial, as it is sensitive to the\nchoice of hyperparameters such as dropout rate and learning weight decay,\nespecially for deep GCN models. In this paper, we aim to automate the training\nof GCN models through hyperparameter optimization. To be specific, we propose a\nself-tuning GCN approach with an alternate training algorithm, and further\nextend our approach by incorporating the population based training scheme.\nExperimental results on three benchmark datasets demonstrate the effectiveness\nof our approaches on optimizing multi-layer GCN, compared with several\nrepresentative baselines.",
          "link": "http://arxiv.org/abs/2107.04713",
          "publishedOn": "2021-07-13T01:59:33.731Z",
          "wordCount": 600,
          "title": "Automated Graph Learning via Population Based Self-Tuning GCN. (arXiv:2107.04713v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "A unique cognitive capability of humans consists in their ability to acquire\nnew knowledge and skills from a sequence of experiences. Meanwhile, artificial\nintelligence systems are good at learning only the last given task without\nbeing able to remember the databases learnt in the past. We propose a novel\nlifelong learning methodology by employing a Teacher-Student network framework.\nWhile the Student module is trained with a new given database, the Teacher\nmodule would remind the Student about the information learnt in the past. The\nTeacher, implemented by a Generative Adversarial Network (GAN), is trained to\npreserve and replay past knowledge corresponding to the probabilistic\nrepresentations of previously learn databases. Meanwhile, the Student module is\nimplemented by a Variational Autoencoder (VAE) which infers its latent variable\nrepresentation from both the output of the Teacher module as well as from the\nnewly available database. Moreover, the Student module is trained to capture\nboth continuous and discrete underlying data representations across different\ndomains. The proposed lifelong learning framework is applied in supervised,\nsemi-supervised and unsupervised training. The code is available~:\n\\url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}",
          "link": "http://arxiv.org/abs/2107.04689",
          "publishedOn": "2021-07-13T01:59:33.711Z",
          "wordCount": 625,
          "title": "Lifelong Teacher-Student Network Learning. (arXiv:2107.04689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teli_M/0/1/0/all/0/1\">Mohammad Nayeem Teli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seungwon Oh</a>",
          "description": "Due to the vulnerability of deep neural networks to adversarial examples,\nnumerous works on adversarial attacks and defenses have been burgeoning over\nthe past several years. However, there seem to be some conventional views\nregarding adversarial attacks and object detection approaches that most\nresearchers take for granted. In this work, we bring a fresh perspective on\nthose procedures by evaluating the impact of universal perturbations on object\ndetection at a class-level. We apply it to a carefully curated data set related\nto autonomous driving. We use Faster-RCNN object detector on images of five\ndifferent categories: person, car, truck, stop sign and traffic light from the\nCOCO data set, while carefully perturbing the images using Universal Dense\nObject Suppression algorithm. Our results indicate that person, car, traffic\nlight, truck and stop sign are resilient in that order (most to least) to\nuniversal perturbations. To the best of our knowledge, this is the first time\nsuch a ranking has been established which is significant for the security of\nthe data sets pertaining to autonomous vehicles and object detection in\ngeneral.",
          "link": "http://arxiv.org/abs/2107.04749",
          "publishedOn": "2021-07-13T01:59:33.685Z",
          "wordCount": 633,
          "title": "Resilience of Autonomous Vehicle Object Category Detection to Universal Adversarial Perturbations. (arXiv:2107.04749v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Richard Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lihan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huster_T/0/1/0/all/0/1\">Todd Huster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_W/0/1/0/all/0/1\">William Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arleth_S/0/1/0/all/0/1\">Stephen Arleth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Justin Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridge_D/0/1/0/all/0/1\">Devin Ridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_M/0/1/0/all/0/1\">Michael Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Headley_W/0/1/0/all/0/1\">William C. Headley</a>",
          "description": "This paper describes a systematic approach towards building a new family of\nneural networks based on a delay-loop version of a reservoir neural network.\nThe resulting architecture, called Scaled-Time-Attention Robust Edge (STARE)\nnetwork, exploits hyper dimensional space and non-multiply-and-add computation\nto achieve a simpler architecture, which has shallow layers, is simple to\ntrain, and is better suited for Edge applications, such as Internet of Things\n(IoT), over traditional deep neural networks. STARE incorporates new AI\nconcepts such as Attention and Context, and is best suited for temporal feature\nextraction and classification. We demonstrate that STARE is applicable to a\nvariety of applications with improved performance and lower implementation\ncomplexity. In particular, we showed a novel way of applying a dual-loop\nconfiguration to detection and identification of drone vs bird in a counter\nUnmanned Air Systems (UAS) detection application by exploiting both spatial\n(video frame) and temporal (trajectory) information. We also demonstrated that\nthe STARE performance approaches that of a State-of-the-Art deep neural network\nin classifying RF modulations, and outperforms Long Short-term Memory (LSTM) in\na special case of Mackey Glass time series prediction. To demonstrate hardware\nefficiency, we designed and developed an FPGA implementation of the STARE\nalgorithm to demonstrate its low-power and high-throughput operations. In\naddition, we illustrate an efficient structure for integrating a massively\nparallel implementation of the STARE algorithm for ASIC implementation.",
          "link": "http://arxiv.org/abs/2107.04688",
          "publishedOn": "2021-07-13T01:59:33.662Z",
          "wordCount": 683,
          "title": "Scaled-Time-Attention Robust Edge Network. (arXiv:2107.04688v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shuwei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Co-training, extended from self-training, is one of the frameworks for\nsemi-supervised learning. It works at the cost of training extra classifiers,\nwhere the algorithm should be delicately designed to prevent individual\nclassifiers from collapsing into each other. In this paper, we present a simple\nand efficient co-training algorithm, named Multi-Head Co-Training, for\nsemi-supervised image classification. By integrating base learners into a\nmulti-head structure, the model is in a minimal amount of extra parameters.\nEvery classification head in the unified model interacts with its peers through\na \"Weak and Strong Augmentation\" strategy, achieving single-view co-training\nwithout promoting diversity explicitly. The effectiveness of Multi-Head\nCo-Training is demonstrated in an empirical study on standard semi-supervised\nlearning benchmarks.",
          "link": "http://arxiv.org/abs/2107.04795",
          "publishedOn": "2021-07-13T01:59:33.650Z",
          "wordCount": 547,
          "title": "Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huabin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1\">Mengjuan Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>",
          "description": "Few-shot action recognition aims to recognize novel action classes (query)\nusing just a few samples (support). The majority of current approaches follow\nthe metric learning paradigm, which learns to compare the similarity between\nvideos. Recently, it has been observed that directly measuring this similarity\nis not ideal since different action instances may show distinctive temporal\ndistribution, resulting in severe misalignment issues across query and support\nvideos. In this paper, we arrest this problem from two distinct aspects --\naction duration misalignment and motion evolution misalignment. We address them\nsequentially through a Two-stage Temporal Alignment Network (TTAN). The first\nstage performs temporal transformation with the predicted affine warp\nparameters, while the second stage utilizes a cross-attention mechanism to\ncoordinate the features of the support and query to a consistent evolution.\nBesides, we devise a novel multi-shot fusion strategy, which takes the\nmisalignment among support samples into consideration. Ablation studies and\nvisualizations demonstrate the role played by both stages in addressing the\nmisalignment. Extensive experiments on benchmark datasets show the potential of\nthe proposed method in achieving state-of-the-art performance for few-shot\naction recognition.",
          "link": "http://arxiv.org/abs/2107.04782",
          "publishedOn": "2021-07-13T01:59:33.636Z",
          "wordCount": 626,
          "title": "TTAN: Two-Stage Temporal Alignment Network for Few-shot Action Recognition. (arXiv:2107.04782v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "In this paper, we propose an end-to-end lifelong learning mixture of experts.\nEach expert is implemented by a Variational Autoencoder (VAE). The experts in\nthe mixture system are jointly trained by maximizing a mixture of individual\ncomponent evidence lower bounds (MELBO) on the log-likelihood of the given\ntraining samples. The mixing coefficients in the mixture, control the\ncontributions of each expert in the goal representation. These are sampled from\na Dirichlet distribution whose parameters are determined through non-parametric\nestimation during lifelong learning. The model can learn new tasks fast when\nthese are similar to those previously learnt. The proposed Lifelong mixture of\nVAE (L-MVAE) expands its architecture with new components when learning a\ncompletely new task. After the training, our model can automatically determine\nthe relevant expert to be used when fed with new data samples. This mechanism\nbenefits both the memory efficiency and the required computational cost as only\none expert is used during the inference. The L-MVAE inference model is able to\nperform interpolation in the joint latent space across the data domains\nassociated with different tasks and is shown to be efficient for disentangled\nlearning representation.",
          "link": "http://arxiv.org/abs/2107.04694",
          "publishedOn": "2021-07-13T01:59:33.629Z",
          "wordCount": 631,
          "title": "Lifelong Mixture of Variational Autoencoders. (arXiv:2107.04694v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Transformers have demonstrated great potential in computer vision tasks. To\navoid dense computations of self-attentions in high-resolution visual data,\nsome recent Transformer models adopt a hierarchical design, where\nself-attentions are only computed within local windows. This design\nsignificantly improves the efficiency but lacks global feature reasoning in\nearly stages. In this work, we design a multi-path structure of the\nTransformer, which enables local-to-global reasoning at multiple granularities\nin each stage. The proposed framework is computationally efficient and highly\neffective. With a marginal increasement in computational overhead, our model\nachieves notable improvements in both image classification and semantic\nsegmentation. Code is available at https://github.com/ljpadam/LG-Transformer",
          "link": "http://arxiv.org/abs/2107.04735",
          "publishedOn": "2021-07-13T01:59:33.605Z",
          "wordCount": 537,
          "title": "Local-to-Global Self-Attention in Vision Transformers. (arXiv:2107.04735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1\">Bing-Kun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>",
          "description": "Video question answering is a challenging task, which requires agents to be\nable to understand rich video contents and perform spatial-temporal reasoning.\nHowever, existing graph-based methods fail to perform multi-step reasoning\nwell, neglecting two properties of VideoQA: (1) Even for the same video,\ndifferent questions may require different amount of video clips or objects to\ninfer the answer with relational reasoning; (2) During reasoning, appearance\nand motion features have complicated interdependence which are correlated and\ncomplementary to each other. Based on these observations, we propose a\nDual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an\nend-to-end fashion. The first contribution of our DualVGR is the design of an\nexplainable Query Punishment Module, which can filter out irrelevant visual\nfeatures through multiple cycles of reasoning. The second contribution is the\nproposed Video-based Multi-view Graph Attention Network, which captures the\nrelations between appearance and motion features. Our DualVGR network achieves\nstate-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and\ndemonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is\navailable at https://github.com/MMIR/DualVGR-VideoQA.",
          "link": "http://arxiv.org/abs/2107.04768",
          "publishedOn": "2021-07-13T01:59:33.598Z",
          "wordCount": 628,
          "title": "DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering. (arXiv:2107.04768v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "Learning disentangled and interpretable representations is an important step\ntowards accomplishing comprehensive data representations on the manifold. In\nthis paper, we propose a novel representation learning algorithm which combines\nthe inference abilities of Variational Autoencoders (VAE) with the\ngeneralization capability of Generative Adversarial Networks (GAN). The\nproposed model, called InfoVAEGAN, consists of three networks~: Encoder,\nGenerator and Discriminator. InfoVAEGAN aims to jointly learn discrete and\ncontinuous interpretable representations in an unsupervised manner by using two\ndifferent data-free log-likelihood functions onto the variables sampled from\nthe generator's distribution. We propose a two-stage algorithm for optimizing\nthe inference network separately from the generator training. Moreover, we\nenforce the learning of interpretable representations through the maximization\nof the mutual information between the existing latent variables and those\ncreated through generative and inference processes.",
          "link": "http://arxiv.org/abs/2107.04705",
          "publishedOn": "2021-07-13T01:59:33.590Z",
          "wordCount": 585,
          "title": "InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood. (arXiv:2107.04705v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1\">Mayur R. Parate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhurchandi_K/0/1/0/all/0/1\">Kishor M. Bhurchandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Ashwin G. Kothari</a>",
          "description": "Intelligent resident surveillance is one of the most essential smart\ncommunity services. The increasing demand for security needs surveillance\nsystems to be able to detect anomalies in surveillance scenes. Employing\nhigh-capacity computational devices for intelligent surveillance in residential\nsocieties is costly and not feasible. Therefore, we propose anomaly detection\nfor intelligent surveillance using CPU-only edge devices. A modular framework\nto capture object-level inferences and tracking is developed. To cope with\npartial occlusions, posture deformations, and complex scenes we employed\nfeature encoding and trajectory associations. Elements of the anomaly detection\nframework are optimized to run on CPU-only edge devices with sufficient FPS.\nThe experimental results indicate the proposed method is feasible and achieves\nsatisfactory results in real-life scenarios.",
          "link": "http://arxiv.org/abs/2107.04767",
          "publishedOn": "2021-07-13T01:59:33.576Z",
          "wordCount": 573,
          "title": "Anomaly Detection in Residential Video Surveillance on Edge Devices in IoT Framework. (arXiv:2107.04767v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wight_C/0/1/0/all/0/1\">Colby Wight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akers_S/0/1/0/all/0/1\">Sarah Akers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1\">Scott Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Woongjo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosink_L/0/1/0/all/0/1\">Luke Gosink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurrus_E/0/1/0/all/0/1\">Elizabeth Jurrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kappagantula_K/0/1/0/all/0/1\">Keerti Kappagantula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1\">Tegan H. Emerson</a>",
          "description": "As both machine learning models and the datasets on which they are evaluated\nhave grown in size and complexity, the practice of using a few summary\nstatistics to understand model performance has become increasingly problematic.\nThis is particularly true in real-world scenarios where understanding model\nfailure on certain subpopulations of the data is of critical importance. In\nthis paper we propose a topological framework for evaluating machine learning\nmodels in which a dataset is treated as a \"space\" on which a model operates.\nThis provides us with a principled way to organize information about model\nperformance at both the global level (over the entire test set) and also the\nlocal level (on specific subpopulations). Finally, we describe a topological\ndata structure, presheaves, which offer a convenient way to store and analyze\nmodel performance between different subpopulations.",
          "link": "http://arxiv.org/abs/2107.04714",
          "publishedOn": "2021-07-13T01:59:33.558Z",
          "wordCount": 598,
          "title": "A Topological-Framework to Improve Analysis of Machine Learning Model Performance. (arXiv:2107.04714v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1\">Ali Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1\">Madhusudhanan Balasubramanian</a>",
          "description": "Dense pixel matching problems such as optical flow and disparity estimation\nare among the most challenging tasks in computer vision. Recently, several deep\nlearning methods designed for these problems have been successful. A\nsufficiently larger effective receptive field (ERF) and a higher resolution of\nspatial features within a network are essential for providing higher-resolution\ndense estimates. In this work, we present a systemic approach to design network\narchitectures that can provide a larger receptive field while maintaining a\nhigher spatial feature resolution. To achieve a larger ERF, we utilized dilated\nconvolutional layers. By aggressively increasing dilation rates in the deeper\nlayers, we were able to achieve a sufficiently larger ERF with a significantly\nfewer number of trainable parameters. We used optical flow estimation problem\nas the primary benchmark to illustrate our network design strategy. The\nbenchmark results (Sintel, KITTI, and Middlebury) indicate that our compact\nnetworks can achieve comparable performance in the class of lightweight\nnetworks.",
          "link": "http://arxiv.org/abs/2107.04715",
          "publishedOn": "2021-07-13T01:59:33.551Z",
          "wordCount": 597,
          "title": "DDCNet: Deep Dilated Convolutional Neural Network for Dense Prediction. (arXiv:2107.04715v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baoru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuch_D/0/1/0/all/0/1\">David Tuch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_K/0/1/0/all/0/1\">Kunal Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1\">Stamatia Giannarou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elson_D/0/1/0/all/0/1\">Daniel S. Elson</a>",
          "description": "Dense depth estimation and 3D reconstruction of a surgical scene are crucial\nsteps in computer assisted surgery. Recent work has shown that depth estimation\nfrom a stereo images pair could be solved with convolutional neural networks.\nHowever, most recent depth estimation models were trained on datasets with\nper-pixel ground truth. Such data is especially rare for laparoscopic imaging,\nmaking it hard to apply supervised depth estimation to real surgical\napplications. To overcome this limitation, we propose SADepth, a new\nself-supervised depth estimation method based on Generative Adversarial\nNetworks. It consists of an encoder-decoder generator and a discriminator to\nincorporate geometry constraints during training. Multi-scale outputs from the\ngenerator help to solve the local minima caused by the photometric reprojection\nloss, while the adversarial learning improves the framework generation quality.\nExtensive experiments on two public datasets show that SADepth outperforms\nrecent state-of-the-art unsupervised methods by a large margin, and reduces the\ngap between supervised and unsupervised depth estimation in laparoscopic\nimages.",
          "link": "http://arxiv.org/abs/2107.04644",
          "publishedOn": "2021-07-13T01:59:33.541Z",
          "wordCount": 607,
          "title": "Self-Supervised Generative Adversarial Network for Depth Estimation in Laparoscopic Images. (arXiv:2107.04644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04721",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1\">Shuyun Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_Z/0/1/0/all/0/1\">Ziming Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granley_J/0/1/0/all/0/1\">Jacob Granley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beyeler_M/0/1/0/all/0/1\">Michael Beyeler</a>",
          "description": "Fundus photography has routinely been used to document the presence and\nseverity of retinal degenerative diseases such as age-related macular\ndegeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical\npractice, for which the fovea and optic disc (OD) are important retinal\nlandmarks. However, the occurrence of lesions, drusen, and other retinal\nabnormalities during retinal degeneration severely complicates automatic\nlandmark detection and segmentation. Here we propose HBA-U-Net: a U-Net\nbackbone enriched with hierarchical bottleneck attention. The network consists\nof a novel bottleneck attention block that combines and refines self-attention,\nchannel attention, and relative-position attention to highlight retinal\nabnormalities that may be important for fovea and OD segmentation in the\ndegenerated retina. HBA-U-Net achieved state-of-the-art results on fovea\ndetection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of\n25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for\nAMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:\nED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for\nlandmark detection in the presence of a variety of retinal degenerative\ndiseases.",
          "link": "http://arxiv.org/abs/2107.04721",
          "publishedOn": "2021-07-13T01:59:33.534Z",
          "wordCount": 641,
          "title": "U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina. (arXiv:2107.04721v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_G/0/1/0/all/0/1\">Gaurav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>",
          "description": "Generating future frames given a few context (or past) frames is a\nchallenging task. It requires modeling the temporal coherence of videos and\nmulti-modality in terms of diversity in the potential future states. Current\nvariational approaches for video generation tend to marginalize over\nmulti-modal future outcomes. Instead, we propose to explicitly model the\nmulti-modality in the future outcomes and leverage it to sample diverse\nfutures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to\nlearn priors on future states given the past and maintains a probability\ndistribution over possible futures given a particular sample. In addition, we\nleverage the changes in this distribution over time to control the sampling of\ndiverse future states by estimating the end of ongoing sequences. That is, we\nuse the variance of GP over the output function space to trigger a change in an\naction sequence. We achieve state-of-the-art results on diverse future frame\ngeneration in terms of reconstruction quality and diversity of the generated\nsequences.",
          "link": "http://arxiv.org/abs/2107.04619",
          "publishedOn": "2021-07-13T01:59:33.493Z",
          "wordCount": 612,
          "title": "Diverse Video Generation using a Gaussian Process Trigger. (arXiv:2107.04619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasiri_S/0/1/0/all/0/1\">Seyed-Mahdi Nasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>",
          "description": "Triangulation refers to the problem of finding a 3D point from its 2D\nprojections on multiple camera images. For solving this problem, it is the\ncommon practice to use so-called optimal triangulation method, which we call\nthe L2 method in this paper. But, the method can be optimal only if we assume\nno uncertainty in the camera parameters. Through extensive comparison on\nsynthetic and real data, we observed that the L2 method is actually not the\nbest choice when there is uncertainty in the camera parameters. Interestingly,\nit can be observed that the simple mid-point method outperforms other methods.\nApart from its high performance, the mid-point method has a simple closed\nformed solution for multiple camera images while the L2 method is hard to be\nused for more than two camera images. Therefore, in contrast to the common\npractice, we argue that the simple mid-point method should be used in\nstructure-from-motion applications where there is uncertainty in camera\nparameters.",
          "link": "http://arxiv.org/abs/2107.04618",
          "publishedOn": "2021-07-13T01:59:33.475Z",
          "wordCount": 595,
          "title": "Optimal Triangulation Method is Not Really Optimal. (arXiv:2107.04618v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2102.00815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qinghua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1\">Sobhan Miryoosefi</a>",
          "description": "Finding the minimal structural assumptions that empower sample-efficient\nlearning is one of the most important research directions in Reinforcement\nLearning (RL). This paper advances our understanding of this fundamental\nquestion by introducing a new complexity measure -- Bellman Eluder (BE)\ndimension. We show that the family of RL problems of low BE dimension is\nremarkably rich, which subsumes a vast majority of existing tractable RL\nproblems including but not limited to tabular MDPs, linear MDPs, reactive\nPOMDPs, low Bellman rank problems as well as low Eluder dimension problems.\nThis paper further designs a new optimization-based algorithm -- GOLF, and\nreanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang\net al., 2017). We prove that both algorithms learn the near-optimal policies of\nlow BE dimension problems in a number of samples that is polynomial in all\nrelevant parameters, but independent of the size of state-action space. Our\nregret and sample complexity results match or improve the best existing results\nfor several well-known subclasses of low BE dimension problems.",
          "link": "http://arxiv.org/abs/2102.00815",
          "publishedOn": "2021-07-19T00:49:08.148Z",
          "wordCount": 659,
          "title": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms. (arXiv:2102.00815v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03686",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1\">Udaya S.K.P. Miriya Thanthrige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1\">Aydin Sezgin</a>",
          "description": "We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-input and\nmultiple-output (MIMO) wireless radar. Here, the strong clutter due to the\nreflection of the layered structure's surface often makes the detection of the\ndefects challenging. Thus, sophisticated signal separation methods are required\nfor improved defect detection. In many scenarios, the number of defects that we\nare interested in is limited and the signaling response of the layered\nstructure can be modeled as a low-rank structure. Therefore, we propose joint\nrank and sparsity minimization for defect detection. In particular, we propose\na non-convex approach based on the iteratively reweighted nuclear and\n$\\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy\ncompared to the conventional nuclear norm and $\\ell_1-$norm minimization. To\nthis end, an iterative algorithm is designed to estimate the low-rank and\nsparse contributions. Further, we propose deep learning to learn the parameters\nof the algorithm (i.e., algorithm unfolding) to improve the accuracy and the\nspeed of convergence of the algorithm. Our numerical results show that the\nproposed approach outperforms the conventional approaches in terms of mean\nsquare errors of the recovered low-rank and sparse components and the speed of\nconvergence.",
          "link": "http://arxiv.org/abs/2106.03686",
          "publishedOn": "2021-07-19T00:49:08.127Z",
          "wordCount": 681,
          "title": "Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">David J. Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedantam_R/0/1/0/all/0/1\">Ramakrishna Vedantam</a>",
          "description": "We address the question of characterizing and finding optimal representations\nfor supervised learning. Traditionally, this question has been tackled using\nthe Information Bottleneck, which compresses the inputs while retaining\ninformation about the targets, in a decoder-agnostic fashion. In machine\nlearning, however, our goal is not compression but rather generalization, which\nis intimately linked to the predictive family or decoder of interest (e.g.\nlinear classifier). We propose the Decodable Information Bottleneck (DIB) that\nconsiders information retention and compression from the perspective of the\ndesired predictive family. As a result, DIB gives rise to representations that\nare optimal in terms of expected test performance and can be estimated with\nguarantees. Empirically, we show that the framework can be used to enforce a\nsmall generalization gap on downstream classifiers and to predict the\ngeneralization ability of neural networks.",
          "link": "http://arxiv.org/abs/2009.12789",
          "publishedOn": "2021-07-19T00:49:08.103Z",
          "wordCount": 611,
          "title": "Learning Optimal Representations with the Decodable Information Bottleneck. (arXiv:2009.12789v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yujiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "Catastrophic forgetting means that a trained neural network model gradually\nforgets the previously learned tasks when being retrained on new tasks.\nOvercoming the forgetting problem is a major problem in machine learning.\nNumerous continual learning algorithms are very successful in incremental\nlearning of classification tasks, where new samples with their labels appear\nfrequently. However, there is currently no research that addresses the\ncatastrophic forgetting problem in regression tasks as far as we know. This\nproblem has emerged as one of the primary constraints in some applications,\nsuch as renewable energy forecasts. This article clarifies problem-related\ndefinitions and proposes a new methodological framework that can forecast\ntargets and update itself by means of continual learning. The framework\nconsists of forecasting neural networks and buffers, which store newly\ncollected data from a non-stationary data stream in an application. The changed\nprobability distribution of the data stream, which the framework has\nidentified, will be learned sequentially. The framework is called CLeaR\n(Continual Learning for Regression Tasks), where components can be flexibly\ncustomized for a specific application scenario. We design two sets of\nexperiments to evaluate the CLeaR framework concerning fitting error\n(training), prediction error (test), and forgetting ratio. The first one is\nbased on an artificial time series to explore how hyperparameters affect the\nCLeaR framework. The second one is designed with data collected from European\nwind farms to evaluate the CLeaR framework's performance in a real-world\napplication. The experimental results demonstrate that the CLeaR framework can\ncontinually acquire knowledge in the data stream and improve the prediction\naccuracy. The article concludes with further research issues arising from\nrequirements to extend the framework.",
          "link": "http://arxiv.org/abs/2101.00926",
          "publishedOn": "2021-07-19T00:49:08.077Z",
          "wordCount": 751,
          "title": "CLeaR: An Adaptive Continual Learning Framework for Regression Tasks. (arXiv:2101.00926v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shulman_Y/0/1/0/all/0/1\">Yaniv Shulman</a>",
          "description": "Quantization based model compression serves as high performing and fast\napproach for inference that yields models which are highly compressed when\ncompared to their full-precision floating point counterparts. The most extreme\nquantization is a 1-bit representation of parameters such that they have only\ntwo possible values, typically -1(0) or +1, enabling efficient implementation\nof the ubiquitous dot product using only additions. The main contribution of\nthis work is the introduction of a method to smooth the combinatorial problem\nof determining a binary vector of weights to minimize the expected loss for a\ngiven objective by means of empirical risk minimization with backpropagation.\nThis is achieved by approximating a multivariate binary state over the weights\nutilizing a deterministic and differentiable transformation of real-valued,\ncontinuous parameters. The proposed method adds little overhead in training,\ncan be readily applied without any substantial modifications to the original\narchitecture, does not introduce additional saturating nonlinearities or\nauxiliary losses, and does not prohibit applying other methods for binarizing\nthe activations. Contrary to common assertions made in the literature, it is\ndemonstrated that binary weighted networks can train well with the same\nstandard optimization techniques and similar hyperparameter settings as their\nfull-precision counterparts, specifically momentum SGD with large learning\nrates and $L_2$ regularization. To conclude experiments demonstrate the method\nperforms remarkably well across a number of inductive image classification\ntasks with various architectures compared to their full-precision counterparts.\nThe source code is publicly available at\nhttps://bitbucket.org/YanivShu/binary_weighted_networks_public.",
          "link": "http://arxiv.org/abs/2107.01400",
          "publishedOn": "2021-07-19T00:49:08.072Z",
          "wordCount": 691,
          "title": "Exact Backpropagation in Binary Weighted Networks with Group Weight Transformations. (arXiv:2107.01400v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chari_S/0/1/0/all/0/1\">Shruthi Chari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Prithwish Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghalwash_M/0/1/0/all/0/1\">Mohamed Ghalwash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_O/0/1/0/all/0/1\">Oshani Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyigoz_E/0/1/0/all/0/1\">Elif K. Eyigoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruen_D/0/1/0/all/0/1\">Daniel M. Gruen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saiz_F/0/1/0/all/0/1\">Fernando Suarez Saiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Ching-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_P/0/1/0/all/0/1\">Pablo Meyer Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_D/0/1/0/all/0/1\">Deborah L. McGuinness</a>",
          "description": "Academic advances of AI models in high-precision domains, like healthcare,\nneed to be made explainable in order to enhance real-world adoption. Our past\nstudies and ongoing interactions indicate that medical experts can use AI\nsystems with greater trust if there are ways to connect the model inferences\nabout patients to explanations that are tied back to the context of use.\nSpecifically, risk prediction is a complex problem of diagnostic and\ninterventional importance to clinicians wherein they consult different sources\nto make decisions. To enable the adoption of the ever improving AI risk\nprediction models in practice, we have begun to explore techniques to\ncontextualize such models along three dimensions of interest: the patients'\nclinical state, AI predictions about their risk of complications, and\nalgorithmic explanations supporting the predictions. We validate the importance\nof these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes\n(T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a\ncommon T2DM comorbidity. Within the POC, we include risk prediction models for\nCKD, post-hoc explainers of the predictions, and other natural-language modules\nwhich operationalize domain knowledge and CPGs to provide context. With primary\ncare physicians (PCP) as our end-users, we present our initial results and\nclinician feedback in this paper. Our POC approach covers multiple knowledge\nsources and clinical scenarios, blends knowledge to explain data and\npredictions to PCPs, and received an enthusiastic response from our medical\nexpert.",
          "link": "http://arxiv.org/abs/2107.02359",
          "publishedOn": "2021-07-19T00:49:07.963Z",
          "wordCount": 746,
          "title": "Leveraging Clinical Context for User-Centered Explainability: A Diabetes Use Case. (arXiv:2107.02359v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14172",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kileel_J/0/1/0/all/0/1\">Joe Kileel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moscovich_A/0/1/0/all/0/1\">Amit Moscovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelesko_N/0/1/0/all/0/1\">Nathan Zelesko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_A/0/1/0/all/0/1\">Amit Singer</a>",
          "description": "Manifold learning methods play a prominent role in nonlinear dimensionality\nreduction and other tasks involving high-dimensional data sets with low\nintrinsic dimensionality. Many of these methods are graph-based: they associate\na vertex with each data point and a weighted edge with each pair. Existing\ntheory shows that the Laplacian matrix of the graph converges to the\nLaplace-Beltrami operator of the data manifold, under the assumption that the\npairwise affinities are based on the Euclidean norm. In this paper, we\ndetermine the limiting differential operator for graph Laplacians constructed\nusing $\\textit{any}$ norm. Our proof involves an interplay between the second\nfundamental form of the manifold and the convex geometry of the given norm's\nunit ball. To demonstrate the potential benefits of non-Euclidean norms in\nmanifold learning, we consider the task of mapping the motion of large\nmolecules with continuous variability. In a numerical simulation we show that a\nmodified Laplacian eigenmaps algorithm, based on the Earthmover's distance,\noutperforms the classic Euclidean Laplacian eigenmaps, both in terms of\ncomputational cost and the sample size needed to recover the intrinsic\ngeometry.",
          "link": "http://arxiv.org/abs/2012.14172",
          "publishedOn": "2021-07-19T00:49:07.957Z",
          "wordCount": 651,
          "title": "Manifold learning with arbitrary norms. (arXiv:2012.14172v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Traditionally, distillation has been used to train a student model to emulate\nthe input/output functionality of a teacher. A more useful goal than emulation,\nyet under-explored, is for the student to learn feature representations that\ntransfer well to future tasks. However, we observe that standard distillation\nof task-specific teachers actually *reduces* the transferability of student\nrepresentations to downstream tasks. We show that a multi-head, multi-task\ndistillation method using an unlabeled proxy dataset and a generalist teacher\nis sufficient to consolidate representations from task-specific teacher(s) and\nimprove downstream performance, outperforming the teacher(s) and the strong\nbaseline of ImageNet pretrained features. Our method can also combine the\nrepresentational knowledge of multiple teachers trained on one or multiple\ndomains into a single model, whose representation is improved on all teachers'\ndomain(s).",
          "link": "http://arxiv.org/abs/2107.08039",
          "publishedOn": "2021-07-19T00:49:07.951Z",
          "wordCount": 568,
          "title": "Representation Consolidation for Training Expert Students. (arXiv:2107.08039v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zombori_Z/0/1/0/all/0/1\">Zsolt Zombori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">Josef Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">Miroslav Ol&#x161;&#xe1;k</a>",
          "description": "In this work we study how to learn good algorithms for selecting reasoning\nsteps in theorem proving. We explore this in the connection tableau calculus\nimplemented by leanCoP where the partial tableau provides a clean and compact\nnotion of a state to which a limited number of inferences can be applied. We\nstart by incorporating a state-of-the-art learning algorithm -- a graph neural\nnetwork (GNN) -- into the plCoP theorem prover. Then we use it to observe the\nsystem's behaviour in a reinforcement learning setting, i.e., when learning\ninference guidance from successful Monte-Carlo tree searches on many problems.\nDespite its better pattern matching capability, the GNN initially performs\nworse than a simpler previously used learning algorithm. We observe that the\nsimpler algorithm is less confident, i.e., its recommendations have higher\nentropy. This leads us to explore how the entropy of the inference selection\nimplemented via the neural network influences the proof search. This is related\nto research in human decision-making under uncertainty, and in particular the\nprobability matching theory. Our main result shows that a proper entropy\nregularisation, i.e., training the GNN not to be overconfident, greatly\nimproves plCoP's performance on a large mathematical corpus.",
          "link": "http://arxiv.org/abs/2105.14706",
          "publishedOn": "2021-07-19T00:49:07.934Z",
          "wordCount": 664,
          "title": "The Role of Entropy in Guiding a Connection Prover. (arXiv:2105.14706v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ablett_T/0/1/0/all/0/1\">Trevor Ablett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yifan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>",
          "description": "Learned visuomotor policies have shown considerable success as an alternative\nto traditional, hand-crafted frameworks for robotic manipulation. Surprisingly,\nan extension of these methods to the multiview domain is relatively unexplored.\nA successful multiview policy could be deployed on a mobile manipulation\nplatform, allowing the robot to complete a task regardless of its view of the\nscene. In this work, we demonstrate that a multiview policy can be found\nthrough imitation learning by collecting data from a variety of viewpoints. We\nillustrate the general applicability of the method by learning to complete\nseveral challenging multi-stage and contact-rich tasks, from numerous\nviewpoints, both in a simulated environment and on a real mobile manipulation\nplatform. Furthermore, we analyze our policies to determine the benefits of\nlearning from multiview data compared to learning with data collected from a\nfixed perspective. We show that learning from multiview data results in little,\nif any, penalty to performance for a fixed-view task compared to learning with\nan equivalent amount of fixed-view data. Finally, we examine the visual\nfeatures learned by the multiview and fixed-view policies. Our results indicate\nthat multiview policies implicitly learn to identify spatially correlated\nfeatures.",
          "link": "http://arxiv.org/abs/2104.13907",
          "publishedOn": "2021-07-19T00:49:07.928Z",
          "wordCount": 673,
          "title": "Seeing All the Angles: Learning Multiview Manipulation Policies for Contact-Rich Tasks from Demonstrations. (arXiv:2104.13907v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Charles Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>",
          "description": "As machine learning (ML) continue to be integrated into healthcare systems\nthat affect clinical decision making, new strategies will need to be\nincorporated in order to effectively detect and evaluate subgroup disparities\nto ensure accountability and generalizability in clinical workflows. In this\npaper, we explore how epistemic uncertainty can be used to evaluate disparity\nin patient demographics (race) and data acquisition (scanner) subgroups for\nbreast density assessment on a dataset of 108,190 mammograms collected from 33\nclinical sites. Our results show that even if aggregate performance is\ncomparable, the choice of uncertainty quantification metric can significantly\nthe subgroup level. We hope this analysis can promote further work on how\nuncertainty can be leveraged to increase transparency of machine learning\napplications for clinical deployment.",
          "link": "http://arxiv.org/abs/2107.02716",
          "publishedOn": "2021-07-19T00:49:07.921Z",
          "wordCount": 591,
          "title": "Evaluating subgroup disparity using epistemic uncertainty in mammography. (arXiv:2107.02716v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09048",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Rolle_A/0/1/0/all/0/1\">Alexander Rolle</a>, <a href=\"http://arxiv.org/find/math/1/au:+Scoccola_L/0/1/0/all/0/1\">Luis Scoccola</a>",
          "description": "We present a multiscale, consistent approach to density-based clustering that\nsatisfies stability theorems -- in both the input data and in the parameters --\nwhich hold without distributional assumptions. The stability in the input data\nis with respect to the Gromov--Hausdorff--Prokhorov distance on metric\nprobability spaces and interleaving distances between (multi-parameter)\nhierarchical clusterings we introduce. We prove stability results for standard\nsimplification procedures for hierarchical clusterings, which can be combined\nwith our approach to yield a stable flat clustering algorithm. We illustrate\nthe stability of the approach with computational examples. Our framework is\nbased on the concepts of persistence and interleaving distance from Topological\nData Analysis.",
          "link": "http://arxiv.org/abs/2005.09048",
          "publishedOn": "2021-07-19T00:49:07.914Z",
          "wordCount": 564,
          "title": "Stable and consistent density-based clustering. (arXiv:2005.09048v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02522",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Wu_J/0/1/0/all/0/1\">Junran Wu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1\">Xueyuan Chen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Li_S/0/1/0/all/0/1\">Shangzhe Li</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhao_J/0/1/0/all/0/1\">Jichang Zhao</a>",
          "description": "Great research efforts have been devoted to exploiting deep neural networks\nin stock prediction. While long-range dependencies and chaotic property are\nstill two major issues that lower the performance of state-of-the-art deep\nlearning models in forecasting future price trends. In this study, we propose a\nnovel framework to address both issues. Specifically, in terms of transforming\ntime series into complex networks, we convert market price series into graphs.\nThen, structural information, referring to associations among temporal points\nand the node weights, is extracted from the mapped graphs to resolve the\nproblems regarding long-range dependencies and the chaotic property. We take\ngraph embeddings to represent the associations among temporal points as the\nprediction model inputs. Node weights are used as a priori knowledge to enhance\nthe learning of temporal attention. The effectiveness of our proposed framework\nis validated using real-world stock data, and our approach obtains the best\nperformance among several state-of-the-art benchmarks. Moreover, in the\nconducted trading simulations, our framework further obtains the highest\ncumulative profits. Our results supplement the existing applications of complex\nnetwork methods in the financial realm and provide insightful implications for\ninvestment applications regarding decision support in financial markets.",
          "link": "http://arxiv.org/abs/2106.02522",
          "publishedOn": "2021-07-19T00:49:07.896Z",
          "wordCount": 671,
          "title": "Price graphs: Utilizing the structural information of financial time series for stock prediction. (arXiv:2106.02522v3 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "Humans are incredibly good at transferring knowledge from one domain to\nanother, enabling rapid learning of new tasks. Likewise, transfer learning has\nenabled enormous success in many computer vision problems using pretraining.\nHowever, the benefits of transfer in multi-domain learning, where a network\nlearns multiple tasks defined by different datasets, has not been adequately\nstudied. Learning multiple domains could be beneficial or these domains could\ninterfere with each other given limited network capacity. In this work, we\ndecipher the conditions where interference and knowledge transfer occur in\nmulti-domain learning. We propose new metrics disentangling interference and\ntransfer and set up experimental protocols. We further examine the roles of\nnetwork capacity, task grouping, and dynamic loss weighting in reducing\ninterference and facilitating transfer. We demonstrate our findings on the\nCIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.05445",
          "publishedOn": "2021-07-19T00:49:07.890Z",
          "wordCount": 591,
          "title": "Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We consider a binary classification problem when the data comes from a\nmixture of two isotropic distributions satisfying concentration and\nanti-concentration properties enjoyed by log-concave distributions among\nothers. We show that there exists a universal constant $C_{\\mathrm{err}}>0$\nsuch that if a pseudolabeler $\\boldsymbol{\\beta}_{\\mathrm{pl}}$ can achieve\nclassification error at most $C_{\\mathrm{err}}$, then for any $\\varepsilon>0$,\nan iterative self-training algorithm initialized at $\\boldsymbol{\\beta}_0 :=\n\\boldsymbol{\\beta}_{\\mathrm{pl}}$ using pseudolabels $\\hat y =\n\\mathrm{sgn}(\\langle \\boldsymbol{\\beta}_t, \\mathbf{x}\\rangle)$ and using at\nmost $\\tilde O(d/\\varepsilon^2)$ unlabeled examples suffices to learn the\nBayes-optimal classifier up to $\\varepsilon$ error, where $d$ is the ambient\ndimension. That is, self-training converts weak learners to strong learners\nusing only unlabeled examples. We additionally show that by running gradient\ndescent on the logistic loss one can obtain a pseudolabeler\n$\\boldsymbol{\\beta}_{\\mathrm{pl}}$ with classification error $C_{\\mathrm{err}}$\nusing only $O(d)$ labeled examples (i.e., independent of $\\varepsilon$).\nTogether our results imply that mixture models can be learned to within\n$\\varepsilon$ of the Bayes-optimal accuracy using at most $O(d)$ labeled\nexamples and $\\tilde O(d/\\varepsilon^2)$ unlabeled examples by way of a\nsemi-supervised self-training algorithm.",
          "link": "http://arxiv.org/abs/2106.13805",
          "publishedOn": "2021-07-19T00:49:07.884Z",
          "wordCount": 650,
          "title": "Self-training Converts Weak Learners to Strong Learners in Mixture Models. (arXiv:2106.13805v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guanya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honig_W/0/1/0/all/0/1\">Wolfgang H&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xichen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Soon-Jo Chung</a>",
          "description": "We present Neural-Swarm2, a learning-based method for motion planning and\ncontrol that allows heterogeneous multirotors in a swarm to safely fly in close\nproximity. Such operation for drones is challenging due to complex aerodynamic\ninteraction forces, such as downwash generated by nearby drones and ground\neffect. Conventional planning and control methods neglect capturing these\ninteraction forces, resulting in sparse swarm configuration during flight. Our\napproach combines a physics-based nominal dynamics model with learned Deep\nNeural Networks (DNNs) with strong Lipschitz properties. We make use of two\ntechniques to accurately predict the aerodynamic interactions between\nheterogeneous multirotors: i) spectral normalization for stability and\ngeneralization guarantees of unseen data and ii) heterogeneous deep sets for\nsupporting any number of heterogeneous neighbors in a permutation-invariant\nmanner without reducing expressiveness. The learned residual dynamics benefit\nboth the proposed interaction-aware multi-robot motion planning and the\nnonlinear tracking control design because the learned interaction forces reduce\nthe modelling errors. Experimental results demonstrate that Neural-Swarm2 is\nable to generalize to larger swarms beyond training cases and significantly\noutperforms a baseline nonlinear tracking controller with up to three times\nreduction in worst-case tracking errors.",
          "link": "http://arxiv.org/abs/2012.05457",
          "publishedOn": "2021-07-19T00:49:07.877Z",
          "wordCount": 683,
          "title": "Neural-Swarm2: Planning and Control of Heterogeneous Multirotor Swarms using Learned Interactions. (arXiv:2012.05457v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yizhen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>",
          "description": "Graph representation learning plays a vital role in processing\ngraph-structured data. However, prior arts on graph representation learning\nheavily rely on labeling information. To overcome this problem, inspired by the\nrecent success of graph contrastive learning and Siamese networks in visual\nrepresentation learning, we propose a novel self-supervised approach in this\npaper to learn node representations by enhancing Siamese self-distillation with\nmulti-scale contrastive learning. Specifically, we first generate two augmented\nviews from the input graph based on local and global perspectives. Then, we\nemploy two objectives called cross-view and cross-network contrastiveness to\nmaximize the agreement between node representations across different views and\nnetworks. To demonstrate the effectiveness of our approach, we perform\nempirical experiments on five real-world datasets. Our method not only achieves\nnew state-of-the-art results but also surpasses some semi-supervised\ncounterparts by large margins. Code is made available at\nhttps://github.com/GRAND-Lab/MERIT",
          "link": "http://arxiv.org/abs/2105.05682",
          "publishedOn": "2021-07-19T00:49:07.866Z",
          "wordCount": 632,
          "title": "Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning. (arXiv:2105.05682v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muniz_I/0/1/0/all/0/1\">I. Muniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camargo_F/0/1/0/all/0/1\">F. H. F. Camargo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1\">A. Marques</a>",
          "description": "With the constant advancements of genetic engineering, a common concern is to\nbe able to identify the lab-of-origin of genetically engineered DNA sequences.\nFor that reason, AltLabs has hosted the genetic Engineering Attribution\nChallenge to gather many teams to propose new tools to solve this problem. Here\nwe show our proposed method to rank the most likely labs-of-origin and generate\nembeddings for DNA sequences and labs. These embeddings can also perform\nvarious other tasks, like clustering both DNA sequences and labs and using them\nas features for Machine Learning models applied to solve other problems. This\nwork demonstrates that our method outperforms the classic training method for\nthis task while generating other helpful information.",
          "link": "http://arxiv.org/abs/2107.07878",
          "publishedOn": "2021-07-19T00:49:07.860Z",
          "wordCount": 567,
          "title": "Ranking labs-of-origin for genetically engineered DNA using Metric Learning. (arXiv:2107.07878v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10785",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1\">Reza Godaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1\">Faezeh Toutounian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>",
          "description": "In this paper we tackle two important challenges related to the accurate\npartial singular value decomposition (SVD) and numerical rank estimation of a\nhuge matrix to use in low-rank learning problems in a fast way. We use the\nconcepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process\nas well as Ritz vectors to achieve these goals. Our experiments identify\nvarious advantages of the proposed methods compared to traditional and\nrandomized SVD (R-SVD) methods with respect to the accuracy of the singular\nvalues and corresponding singular vectors computed in a similar execution time.\nThe proposed methods are appropriate for applications involving huge matrices\nwhere accuracy in all spectrum of the desired singular values, and also all of\ncorresponding singular vectors is essential. We evaluate our method in the real\napplication of Riemannian similarity learning (RSL) between two various image\ndatasets of MNIST and USPS.",
          "link": "http://arxiv.org/abs/2104.10785",
          "publishedOn": "2021-07-19T00:49:07.855Z",
          "wordCount": 611,
          "title": "Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Neural architecture search (NAS) with an accuracy predictor that predicts the\naccuracy of candidate architectures has drawn increasing attention due to its\nsimplicity and effectiveness. Previous works usually employ neural\nnetwork-based predictors which require more delicate design and are easy to\noverfit. Considering that most architectures are represented as sequences of\ndiscrete symbols which are more like tabular data and preferred by non-neural\npredictors, in this paper, we study an alternative approach which uses\nnon-neural model for accuracy prediction. Specifically, as decision tree based\nmodels can better handle tabular data, we leverage gradient boosting decision\ntree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor\ncan achieve comparable (if not better) prediction accuracy than neural network\nbased predictors. Moreover, considering that a compact search space can ease\nthe search process, we propose to prune the search space gradually according to\nimportant features derived from GBDT. In this way, NAS can be performed by\nfirst pruning the search space and then searching a neural architecture, which\nis more efficient and effective. Experiments on NASBench-101 and ImageNet\ndemonstrate the effectiveness of using GBDT as predictor for NAS: (1) On\nNASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,\nregularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global\noptimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further\nachieves 23.4% top-1 error rate on ImageNet when enhanced with search space\npruning. Code is provided in the supplementary materials.",
          "link": "http://arxiv.org/abs/2007.04785",
          "publishedOn": "2021-07-19T00:49:07.849Z",
          "wordCount": 738,
          "title": "Accuracy Prediction with Non-neural Model for Neural Architecture Search. (arXiv:2007.04785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_P/0/1/0/all/0/1\">Parikshit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Prathamesh Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>",
          "description": "We present DeepMVI, a deep learning method for missing value imputation in\nmultidimensional time-series datasets. Missing values are commonplace in\ndecision support platforms that aggregate data over long time stretches from\ndisparate sources, and reliable data analytics calls for careful handling of\nmissing data. One strategy is imputing the missing values, and a wide variety\nof algorithms exist spanning simple interpolation, matrix factorization methods\nlike SVD, statistical models like Kalman filters, and recent deep learning\nmethods. We show that often these provide worse results on aggregate analytics\ncompared to just excluding the missing data. DeepMVI uses a neural network to\ncombine fine-grained and coarse-grained patterns along a time series, and\ntrends from related series across categorical dimensions. After failing with\noff-the-shelf neural architectures, we design our own network that includes a\ntemporal transformer with a novel convolutional window feature, and kernel\nregression with learned embeddings. The parameters and their training are\ndesigned carefully to generalize across different placements of missing blocks\nand data characteristics. Experiments across nine real datasets, four different\nmissing scenarios, comparing seven existing methods show that DeepMVI is\nsignificantly more accurate, reducing error by more than 50% in more than half\nthe cases, compared to the best existing method. Although slower than simpler\nmatrix factorization methods, we justify the increased time overheads by\nshowing that DeepMVI is the only option that provided overall more accurate\nanalytics than dropping missing values.",
          "link": "http://arxiv.org/abs/2103.01600",
          "publishedOn": "2021-07-19T00:49:07.816Z",
          "wordCount": 694,
          "title": "Missing Value Imputation on Multidimensional Time Series. (arXiv:2103.01600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schoeffer_J/0/1/0/all/0/1\">Jakob Schoeffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_N/0/1/0/all/0/1\">Niklas Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1\">Isabel Valera</a>",
          "description": "Algorithmic decision systems are increasingly used in areas such as hiring,\nschool admission, or loan approval. Typically, these systems rely on labeled\ndata for training a classification model. However, in many scenarios,\nground-truth labels are unavailable, and instead we have only access to\nimperfect labels as the result of (potentially biased) human-made decisions.\nDespite being imperfect, historical decisions often contain some useful\ninformation on the unobserved true labels. In this paper, we focus on scenarios\nwhere only imperfect labels are available and propose a new fair ranking-based\ndecision system based on monotonic relationships between legitimate features\nand the outcome. Our approach is both intuitive and easy to implement, and thus\nparticularly suitable for adoption in real-world settings. More in detail, we\nintroduce a distance-based decision criterion, which incorporates useful\ninformation from historical decisions and accounts for unwanted correlation\nbetween protected and legitimate features. Through extensive experiments on\nsynthetic and real-world data, we show that our method is fair in the sense\nthat a) it assigns the desirable outcome to the most qualified individuals, and\nb) it removes the effect of stereotypes in decision-making, thereby\noutperforming traditional classification algorithms. Additionally, we are able\nto show theoretically that our method is consistent with a prominent concept of\nindividual fairness which states that \"similar individuals should be treated\nsimilarly.\"",
          "link": "http://arxiv.org/abs/2102.04565",
          "publishedOn": "2021-07-19T00:49:07.808Z",
          "wordCount": 688,
          "title": "A Ranking Approach to Fair Classification. (arXiv:2102.04565v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jia-Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shuguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1\">Tao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_B/0/1/0/all/0/1\">Bin Tong</a>",
          "description": "Conversion rate (CVR) prediction is one of the most critical tasks for\ndigital display advertising. Commercial systems often require to update models\nin an online learning manner to catch up with the evolving data distribution.\nHowever, conversions usually do not happen immediately after a user click. This\nmay result in inaccurate labeling, which is called delayed feedback problem. In\nprevious studies, delayed feedback problem is handled either by waiting\npositive label for a long period of time, or by consuming the negative sample\non its arrival and then insert a positive duplicate when a conversion happens\nlater. Indeed, there is a trade-off between waiting for more accurate labels\nand utilizing fresh data, which is not considered in existing works. To strike\na balance in this trade-off, we propose Elapsed-Time Sampling Delayed Feedback\nModel (ES-DFM), which models the relationship between the observed conversion\ndistribution and the true conversion distribution. Then we optimize the\nexpectation of true conversion distribution via importance sampling under the\nelapsed-time sampling distribution. We further estimate the importance weight\nfor each instance, which is used as the weight of loss function in CVR\nprediction. To demonstrate the effectiveness of ES-DFM, we conduct extensive\nexperiments on a public data and a private industrial dataset. Experimental\nresults confirm that our method consistently outperforms the previous\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2012.03245",
          "publishedOn": "2021-07-19T00:49:07.797Z",
          "wordCount": 713,
          "title": "Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling. (arXiv:2012.03245v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galindo_Torres_S/0/1/0/all/0/1\">S.A. Galindo-Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "A better understanding of dispersion in natural streams requires knowledge of\nlongitudinal dispersion coefficient(LDC). Various methods have been proposed\nfor predictions of LDC. Those studies can be grouped into three types:\nanalytical, statistical and ML-driven researches(Implicit and explicit).\nHowever, a comprehensive evaluation of them is still lacking. In this paper, we\nfirst present an in-depth analysis of those methods and find out their defects.\nThis is carried out on an extensive database composed of 660 samples of\nhydraulic and channel properties worldwide. The reliability and\nrepresentativeness of utilized data are enhanced through the deployment of the\nSubset Selection of Maximum Dissimilarity(SSMD) for testing set selection and\nthe Inter Quartile Range(IQR) for removal of the outlier. The evaluation\nreveals the rank of those methods as: ML-driven method > the statistical method\n> the analytical method. Whereas implicit ML-driven methods are black-boxes in\nnature, explicit ML-driven methods have more potential in prediction of LDC.\nBesides, overfitting is a universal problem in existing models. Those models\nalso suffer from a fixed parameter combination. To establish an interpretable\nmodel for LDC prediction with higher performance, we then design a novel\nsymbolic regression method called evolutionary symbolic regression\nnetwork(ESRN). It is a combination of genetic algorithms and neural networks.\nStrategies are introduced to avoid overfitting and explore more parameter\ncombinations. Results show that the ESRN model has superiorities over other\nexisting symbolic models in performance. The proposed model is suitable for\npractical engineering problems due to its advantage in low requirement of\nparameters (only w and U* are required). It can provide convincing solutions\nfor situations where the field test cannot be carried out or limited field\ninformation can be obtained.",
          "link": "http://arxiv.org/abs/2106.11026",
          "publishedOn": "2021-07-19T00:49:07.788Z",
          "wordCount": 788,
          "title": "A data-based comparative review and AI-driven symbolic model for longitudinal dispersion coefficient in natural streams. (arXiv:2106.11026v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07863",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1\">Ushnish Sengupta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kontogiannis_A/0/1/0/all/0/1\">Alexandros Kontogiannis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1\">Matthew P. Juniper</a>",
          "description": "Magnetic resonance velocimetry (MRV) is a non-invasive experimental technique\nwidely used in medicine and engineering to measure the velocity field of a\nfluid. These measurements are dense but have a low signal-to-noise ratio (SNR).\nThe measurements can be de-noised by imposing physical constraints on the flow,\nwhich are encapsulated in governing equations for mass and momentum. Previous\nstudies have required the shape of the boundary (for example, a blood vessel)\nto be known a priori. This, however, requires a set of additional measurements,\nwhich can be expensive to obtain. In this paper, we present a physics-informed\nneural network that instead uses the noisy MRV data alone to simultaneously\ninfer the most likely boundary shape and de-noised velocity field. We achieve\nthis by training an auxiliary neural network that takes the value 1.0 within\nthe inferred domain of the governing PDE and 0.0 outside. This network is used\nto weight the PDE residual term in the loss function accordingly and implicitly\nlearns the geometry of the system. We test our algorithm by assimilating both\nsynthetic and real MRV measurements for flows that can be well modeled by the\nPoisson and Stokes equations. We find that we are able to reconstruct very\nnoisy (SNR = 2.5) MRV signals and recover the ground truth with low\nreconstruction errors of 3.7 - 7.5%. The simplicity and flexibility of our\nphysics-informed neural network approach can readily scale to assimilating MRV\ndata with complex 3D geometries, time-varying 4D data, or unknown parameters in\nthe physical model.",
          "link": "http://arxiv.org/abs/2107.07863",
          "publishedOn": "2021-07-19T00:49:07.736Z",
          "wordCount": 698,
          "title": "Simultaneous boundary shape estimation and velocity field de-noising in Magnetic Resonance Velocimetry using Physics-informed Neural Networks. (arXiv:2107.07863v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chaochao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1\">Jo&#x15b;e Miguel Hern&#xe1;ndez-Lobato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Due to spurious correlations, machine learning systems often fail to\ngeneralize to environments whose distributions differ from the ones used at\ntraining time. Prior work addressing this, either explicitly or implicitly,\nattempted to find a data representation that has an invariant relationship with\nthe target. This is done by leveraging a diverse set of training environments\nto reduce the effect of spurious features and build an invariant predictor.\nHowever, these methods have generalization guarantees only when both data\nrepresentation and classifiers come from a linear model class. We propose\ninvariant Causal Representation Learning (iCaRL), an approach that enables\nout-of-distribution (OOD) generalization in the nonlinear setting (i.e.,\nnonlinear representations and nonlinear classifiers). It builds upon a\npractical and general assumption: the prior over the data representation (i.e.,\na set of latent variables encoding the data) given the target and the\nenvironment belongs to general exponential family distributions. Based on this,\nwe show that it is possible to identify the data representation up to simple\ntransformations. We also prove that all direct causes of the target can be\nfully discovered, which further enables us to obtain generalization guarantees\nin the nonlinear setting. Extensive experiments on both synthetic and\nreal-world datasets show that our approach outperforms a variety of baseline\nmethods. Finally, in the discussion, we further explore the aforementioned\nassumption and propose a more general hypothesis, called the Agnostic\nHypothesis: there exist a set of hidden causal factors affecting both inputs\nand outcomes. The Agnostic Hypothesis can provide a unifying view of machine\nlearning. More importantly, it can inspire a new direction to explore a general\ntheory for identifying hidden causal factors, which is key to enabling the OOD\ngeneralization guarantees.",
          "link": "http://arxiv.org/abs/2102.12353",
          "publishedOn": "2021-07-19T00:49:07.724Z",
          "wordCount": 752,
          "title": "Nonlinear Invariant Risk Minimization: A Causal Approach. (arXiv:2102.12353v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Lifeng Lai</a>",
          "description": "Collecting and aggregating information from several probability measures or\nhistograms is a fundamental task in machine learning. One of the popular\nsolution methods for this task is to compute the barycenter of the probability\nmeasures under the Wasserstein metric. However, approximating the Wasserstein\nbarycenter is numerically challenging because of the curse of dimensionality.\nThis paper proposes the projection robust Wasserstein barycenter (PRWB) that\nhas the potential to mitigate the curse of dimensionality. Since PRWB is\nnumerically very challenging to solve, we further propose a relaxed PRWB\n(RPRWB) model, which is more tractable. The RPRWB projects the probability\nmeasures onto a lower-dimensional subspace that maximizes the Wasserstein\nbarycenter objective. The resulting problem is a max-min problem over the\nStiefel manifold. By combining the iterative Bregman projection algorithm and\nRiemannian optimization, we propose two new algorithms for computing the RPRWB.\nThe complexity of arithmetic operations of the proposed algorithms for\nobtaining an $\\epsilon$-stationary solution is analyzed. We incorporate the\nRPRWB into a discrete distribution clustering algorithm, and the numerical\nresults on real text datasets confirm that our RPRWB model helps improve the\nclustering performance significantly.",
          "link": "http://arxiv.org/abs/2102.03390",
          "publishedOn": "2021-07-19T00:49:07.718Z",
          "wordCount": 644,
          "title": "Projection Robust Wasserstein Barycenters. (arXiv:2102.03390v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Lifeng Lai</a>",
          "description": "The Wasserstein distance has become increasingly important in machine\nlearning and deep learning. Despite its popularity, the Wasserstein distance is\nhard to approximate because of the curse of dimensionality. A recently proposed\napproach to alleviate the curse of dimensionality is to project the sampled\ndata from the high dimensional probability distribution onto a\nlower-dimensional subspace, and then compute the Wasserstein distance between\nthe projected data. However, this approach requires to solve a max-min problem\nover the Stiefel manifold, which is very challenging in practice. The only\nexisting work that solves this problem directly is the RGAS (Riemannian\nGradient Ascent with Sinkhorn Iteration) algorithm, which requires to solve an\nentropy-regularized optimal transport problem in each iteration, and thus can\nbe costly for large-scale problems. In this paper, we propose a Riemannian\nblock coordinate descent (RBCD) method to solve this problem, which is based on\na novel reformulation of the regularized max-min problem over the Stiefel\nmanifold. We show that the complexity of arithmetic operations for RBCD to\nobtain an $\\epsilon$-stationary point is $O(\\epsilon^{-3})$. This significantly\nimproves the corresponding complexity of RGAS, which is $O(\\epsilon^{-12})$.\nMoreover, our RBCD has very low per-iteration complexity, and hence is suitable\nfor large-scale problems. Numerical results on both synthetic and real datasets\ndemonstrate that our method is more efficient than existing methods, especially\nwhen the number of sampled data is very large.",
          "link": "http://arxiv.org/abs/2012.05199",
          "publishedOn": "2021-07-19T00:49:07.702Z",
          "wordCount": 720,
          "title": "A Riemannian Block Coordinate Descent Method for Computing the Projection Robust Wasserstein Distance. (arXiv:2012.05199v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Traganitis_P/0/1/0/all/0/1\">Panagiotis A. Traganitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1\">Georgios B. Giannakis</a>",
          "description": "Crowdsourcing has emerged as a powerful paradigm for efficiently labeling\nlarge datasets and performing various learning tasks, by leveraging crowds of\nhuman annotators. When additional information is available about the data,\nsemi-supervised crowdsourcing approaches that enhance the aggregation of labels\nfrom human annotators are well motivated. This work deals with semi-supervised\ncrowdsourced classification, under two regimes of semi-supervision: a) label\nconstraints, that provide ground-truth labels for a subset of data; and b)\npotentially easier to obtain instance-level constraints, that indicate\nrelationships between pairs of data. Bayesian algorithms based on variational\ninference are developed for each regime, and their quantifiably improved\nperformance, compared to unsupervised crowdsourcing, is analytically and\nempirically validated on several crowdsourcing datasets.",
          "link": "http://arxiv.org/abs/2012.11048",
          "publishedOn": "2021-07-19T00:49:07.695Z",
          "wordCount": 574,
          "title": "Bayesian Crowdsourcing with Constraints. (arXiv:2012.11048v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1\">Shiori Sagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marklund_H/0/1/0/all/0/1\">Henrik Marklund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Marvin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balsubramani_A/0/1/0/all/0/1\">Akshay Balsubramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weihua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_R/0/1/0/all/0/1\">Richard Lanas Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1\">Irena Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1\">Etienne David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavness_I/0/1/0/all/0/1\">Ian Stavness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earnshaw_B/0/1/0/all/0/1\">Berton A. Earnshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_I/0/1/0/all/0/1\">Imran S. Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundaje_A/0/1/0/all/0/1\">Anshul Kundaje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1\">Emma Pierson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "Distribution shifts -- where the training distribution differs from the test\ndistribution -- can substantially degrade the accuracy of machine learning (ML)\nsystems deployed in the wild. Despite their ubiquity in the real-world\ndeployments, these distribution shifts are under-represented in the datasets\nwidely used in the ML community today. To address this gap, we present WILDS, a\ncurated benchmark of 10 datasets reflecting a diverse range of distribution\nshifts that naturally arise in real-world applications, such as shifts across\nhospitals for tumor identification; across camera traps for wildlife\nmonitoring; and across time and location in satellite imaging and poverty\nmapping. On each dataset, we show that standard training yields substantially\nlower out-of-distribution than in-distribution performance. This gap remains\neven with models trained by existing methods for tackling distribution shifts,\nunderscoring the need for new methods for training models that are more robust\nto the types of distribution shifts that arise in practice. To facilitate\nmethod development, we provide an open-source package that automates dataset\nloading, contains default model architectures and hyperparameters, and\nstandardizes evaluations. Code and leaderboards are available at\nhttps://wilds.stanford.edu.",
          "link": "http://arxiv.org/abs/2012.07421",
          "publishedOn": "2021-07-19T00:49:07.662Z",
          "wordCount": 696,
          "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts. (arXiv:2012.07421v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>",
          "description": "Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.",
          "link": "http://arxiv.org/abs/2010.11524",
          "publishedOn": "2021-07-19T00:49:07.642Z",
          "wordCount": 654,
          "title": "SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arpita Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_D/0/1/0/all/0/1\">Debasis Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarma_M/0/1/0/all/0/1\">Monalisa Sarma</a>",
          "description": "User intention which often changes dynamically is considered to be an\nimportant factor for modeling users in the design of recommendation systems.\nRecent studies are starting to focus on predicting user intention (what users\nwant) beyond user preference (what users like). In this work, a user intention\nmodel is proposed based on deep sequential topic analysis. The model predicts a\nuser's intention in terms of the topic of interest. The Hybrid Topic Model\n(HTM) comprising Latent Dirichlet Allocation (LDA) and Word2Vec is proposed to\nderive the topic of interest of users and the history of preferences. HTM finds\nthe true topics of papers estimating word-topic distribution which includes\nsyntactic and semantic correlations among words. Next, to model user intention,\na Long Short Term Memory (LSTM) based sequential deep learning model is\nproposed. This model takes into account temporal context, namely the time\ndifference between clicks of two consecutive papers seen by a user. Extensive\nexperiments with the real-world research paper dataset indicate that the\nproposed approach significantly outperforms the state-of-the-art methods.\nFurther, the proposed approach introduces a new road map to model a user\nactivity suitable for the design of a research paper recommendation system.",
          "link": "http://arxiv.org/abs/2107.07831",
          "publishedOn": "2021-07-19T00:49:07.625Z",
          "wordCount": 630,
          "title": "Modeling User Behaviour in Research Paper Recommendation System. (arXiv:2107.07831v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1\">Gilad Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meirom_E/0/1/0/all/0/1\">Eli Meirom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>",
          "description": "Graph neural networks (GNNs) can process graphs of different sizes, but their\nability to generalize across sizes, specifically from small to large graphs, is\nstill not well understood. In this paper, we identify an important type of data\nwhere generalization from small to large graphs is challenging: graph\ndistributions for which the local structure depends on the graph size. This\neffect occurs in multiple important graph learning domains, including social\nand biological networks. We first prove that when there is a difference between\nthe local structures, GNNs are not guaranteed to generalize across sizes: there\nare \"bad\" global minima that do well on small graphs but fail on large graphs.\nWe then study the size-generalization problem empirically and demonstrate that\nwhen there is a discrepancy in local structure, GNNs tend to converge to\nnon-generalizing solutions. Finally, we suggest two approaches for improving\nsize generalization, motivated by our findings. Notably, we propose a novel\nSelf-Supervised Learning (SSL) task aimed at learning meaningful\nrepresentations of local structures that appear in large graphs. Our SSL task\nimproves classification accuracy on several popular datasets.",
          "link": "http://arxiv.org/abs/2010.08853",
          "publishedOn": "2021-07-19T00:49:07.610Z",
          "wordCount": 674,
          "title": "From Local Structures to Size Generalization in Graph Neural Networks. (arXiv:2010.08853v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1\">Marios Mattheakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Recent advances show that neural networks embedded with physics-informed\npriors significantly outperform vanilla neural networks in learning and\npredicting the long term dynamics of complex physical systems from noisy data.\nDespite this success, there has only been a limited study on how to optimally\ncombine physics priors to improve predictive performance. To tackle this\nproblem we unpack and generalize recent innovations into individual inductive\nbias segments. As such, we are able to systematically investigate all possible\ncombinations of inductive biases of which existing methods are a natural\nsubset. Using this framework we introduce Variational Integrator Graph Networks\n- a novel method that unifies the strengths of existing approaches by combining\nan energy constraint, high-order symplectic variational integrators, and graph\nneural networks. We demonstrate, across an extensive ablation, that the\nproposed unifying framework outperforms existing methods, for data-efficient\nlearning and in predictive accuracy, across both single and many-body problems\nstudied in recent literature. We empirically show that the improvements arise\nbecause high order variational integrators combined with a potential energy\nconstraint induce coupled learning of generalized position and momentum updates\nwhich can be formalized via the Partitioned Runge-Kutta method.",
          "link": "http://arxiv.org/abs/2004.13688",
          "publishedOn": "2021-07-19T00:49:07.603Z",
          "wordCount": 669,
          "title": "Variational Integrator Graph Networks for Learning Energy Conserving Dynamical Systems. (arXiv:2004.13688v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07757",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Musso_D/0/1/0/all/0/1\">Daniele Musso</a>",
          "description": "Local entropic loss functions provide a versatile framework to define\narchitecture-aware regularization procedures. Besides the possibility of being\nanisotropic in the synaptic space, the local entropic smoothening of the loss\nfunction can vary during training, thus yielding a tunable model complexity. A\nscoping protocol where the regularization is strong in the early-stage of the\ntraining and then fades progressively away constitutes an alternative to\nstandard initialization procedures for deep convolutional neural networks,\nnonetheless, it has wider applicability. We analyze anisotropic, local entropic\nsmoothenings in the language of statistical physics and information theory,\nproviding insight into both their interpretation and workings. We comment some\naspects related to the physics of renormalization and the spacetime structure\nof convolutional networks.",
          "link": "http://arxiv.org/abs/2107.07757",
          "publishedOn": "2021-07-19T00:49:07.595Z",
          "wordCount": 563,
          "title": "Entropic alternatives to initialization. (arXiv:2107.07757v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1\">Yasunori Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1\">Takayoshi Yamashita</a>",
          "description": "It is difficult to collect data on a large scale in a monocular depth\nestimation because the task requires the simultaneous acquisition of RGB images\nand depths. Data augmentation is thus important to this task. However, there\nhas been little research on data augmentation for tasks such as monocular depth\nestimation, where the transformation is performed pixel by pixel. In this\npaper, we propose a data augmentation method, called CutDepth. In CutDepth,\npart of the depth is pasted onto an input image during training. The method\nextends variations data without destroying edge features. Experiments\nobjectively and subjectively show that the proposed method outperforms\nconventional methods of data augmentation. The estimation accuracy is improved\nwith CutDepth even though there are few training data at long distances.",
          "link": "http://arxiv.org/abs/2107.07684",
          "publishedOn": "2021-07-19T00:49:07.581Z",
          "wordCount": 561,
          "title": "CutDepth:Edge-aware Data Augmentation in Depth Estimation. (arXiv:2107.07684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_L/0/1/0/all/0/1\">Long Kiu Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Adam Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knowles_D/0/1/0/all/0/1\">Derek Knowles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kousik_S/0/1/0/all/0/1\">Shreyas Kousik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Grace X. Gao</a>",
          "description": "Neural networks have recently become popular for a wide variety of uses, but\nhave seen limited application in safety-critical domains such as robotics near\nand around humans. This is because it remains an open challenge to train a\nneural network to obey safety constraints. Most existing safety-related methods\nonly seek to verify that already-trained networks obey constraints, requiring\nalternating training and verification. Instead, this work proposes a\nconstrained method to simultaneously train and verify a feedforward neural\nnetwork with rectified linear unit (ReLU) nonlinearities. Constraints are\nenforced by computing the network's output-space reachable set and ensuring\nthat it does not intersect with unsafe sets; training is achieved by\nformulating a novel collision-check loss function between the reachable set and\nunsafe portions of the output space. The reachable and unsafe sets are\nrepresented by constrained zonotopes, a convex polytope representation that\nenables differentiable collision checking. The proposed method is demonstrated\nsuccessfully on a network with one nonlinearity layer and approximately 50\nparameters.",
          "link": "http://arxiv.org/abs/2107.07696",
          "publishedOn": "2021-07-19T00:49:07.574Z",
          "wordCount": 608,
          "title": "Constrained Feedforward Neural Network Training via Reachability Analysis. (arXiv:2107.07696v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00628",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Frasch_M/0/1/0/all/0/1\">Martin G. Frasch</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Strong_S/0/1/0/all/0/1\">Shadrian B. Strong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nilosek_D/0/1/0/all/0/1\">David Nilosek</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Leaverton_J/0/1/0/all/0/1\">Joshua Leaverton</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Schifrin_B/0/1/0/all/0/1\">Barry S. Schifrin</a>",
          "description": "Despite broad application during labor and delivery, there remains\nconsiderable debate about the value of electronic fetal monitoring (EFM). EFM\nincludes the surveillance of the fetal heart rate (FHR) patterns in conjunction\nwith the maternal uterine contractions providing a wealth of data about fetal\nbehavior and the threat of diminished oxygenation and perfusion. Adverse\noutcomes universally associate a fetal injury with the failure to timely\nrespond to FHR pattern information. Historically, the EFM data, stored\ndigitally, are available only as rasterized pdf images for contemporary or\nhistorical discussion and examination. In reality, however, they are rarely\nreviewed systematically. Using a unique archive of EFM collected over 50 years\nof practice in conjunction with adverse outcomes, we present a deep learning\nframework for training and detection of incipient or past fetal injury. We\nreport 94% accuracy in identifying early, preventable fetal injury intrapartum.\nThis framework is suited for automating an early warning and decision support\nsystem for maintaining fetal well-being during the stresses of labor.\nUltimately, such a system could enable a physician to timely respond during\nlabor and prevent adverse outcomes. When adverse outcomes cannot be avoided,\nthey can provide guidance to the early neuroprotective treatment of the\nnewborn.",
          "link": "http://arxiv.org/abs/2106.00628",
          "publishedOn": "2021-07-19T00:49:07.517Z",
          "wordCount": 666,
          "title": "Detection of preventable fetal distress during labor from scanned cardiotocogram tracings using deep learning. (arXiv:2106.00628v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silver_T/0/1/0/all/0/1\">Tom Silver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1\">Rohan Chitnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>",
          "description": "Robotic planning problems in hybrid state and action spaces can be solved by\nintegrated task and motion planners (TAMP) that handle the complex interaction\nbetween motion-level decisions and task-level plan feasibility. TAMP approaches\nrely on domain-specific symbolic operators to guide the task-level search,\nmaking planning efficient. In this work, we formalize and study the problem of\noperator learning for TAMP. Central to this study is the view that operators\ndefine a lossy abstraction of the transition model of a domain. We then propose\na bottom-up relational learning method for operator learning and show how the\nlearned operators can be used for planning in a TAMP system. Experimentally, we\nprovide results in three domains, including long-horizon robotic planning\ntasks. We find our approach to substantially outperform several baselines,\nincluding three graph neural network-based model-free approaches from the\nrecent literature. Video: https://youtu.be/iVfpX9BpBRo Code:\nhttps://git.io/JCT0g",
          "link": "http://arxiv.org/abs/2103.00589",
          "publishedOn": "2021-07-19T00:49:07.500Z",
          "wordCount": 619,
          "title": "Learning Symbolic Operators for Task and Motion Planning. (arXiv:2103.00589v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1\">Elliot Creager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1\">Niki Kilbertus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Anirudh Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>",
          "description": "The focus of disentanglement approaches has been on identifying independent\nfactors of variation in data. However, the causal variables underlying\nreal-world observations are often not statistically independent. In this work,\nwe bridge the gap to real-world scenarios by analyzing the behavior of the most\nprominent disentanglement approaches on correlated data in a large-scale\nempirical study (including 4260 models). We show and quantify that\nsystematically induced correlations in the dataset are being learned and\nreflected in the latent representations, which has implications for downstream\napplications of disentanglement such as fairness. We also demonstrate how to\nresolve these latent correlations, either using weak supervision during\ntraining or by post-hoc correcting a pre-trained model with a small number of\nlabels.",
          "link": "http://arxiv.org/abs/2006.07886",
          "publishedOn": "2021-07-19T00:49:07.484Z",
          "wordCount": 610,
          "title": "On Disentangled Representations Learned From Correlated Data. (arXiv:2006.07886v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07634",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1\">Takuya Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Anmol Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1\">Chandra Dhir</a>",
          "description": "Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\noutperformed the conventional multi-task learning with split branches and a\nbi-directional long short-team memory decoder by 12% on average.",
          "link": "http://arxiv.org/abs/2107.07634",
          "publishedOn": "2021-07-19T00:49:07.478Z",
          "wordCount": 652,
          "title": "Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05073",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Mikuni_V/0/1/0/all/0/1\">Vinicius Mikuni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Canelli_F/0/1/0/all/0/1\">Florencia Canelli</a>",
          "description": "Methods for processing point cloud information have seen a great success in\ncollider physics applications. One recent breakthrough in machine learning is\nthe usage of Transformer networks to learn semantic relationships between\nsequences in language processing. In this work, we apply a modified Transformer\nnetwork called Point Cloud Transformer as a method to incorporate the\nadvantages of the Transformer architecture to an unordered set of particles\nresulting from collision events. To compare the performance with other\nstrategies, we study jet-tagging applications for highly-boosted particles.",
          "link": "http://arxiv.org/abs/2102.05073",
          "publishedOn": "2021-07-19T00:49:07.471Z",
          "wordCount": 559,
          "title": "Point Cloud Transformers applied to Collider Physics. (arXiv:2102.05073v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Junghyo Jo</a>",
          "description": "The outstanding performance of deep learning in various fields has been a\nfundamental query, which can be potentially examined using information theory\nthat interprets the learning process as the transmission and compression of\ninformation. Information plane analyses of the mutual information between the\ninput-hidden-output layers demonstrated two distinct learning phases of fitting\nand compression. It is debatable if the compression phase is necessary to\ngeneralize the input-output relations extracted from training data. In this\nstudy, we investigated this through experiments with various species of\nautoencoders and evaluated their information processing phase with an accurate\nkernel-based estimator of mutual information. Given sufficient training data,\nvanilla autoencoders demonstrated the compression phase, which was amplified\nafter imposing sparsity regularization for hidden activities. However, we found\nthat the compression phase is not universally observed in different species of\nautoencoders, including variational autoencoders, that have special constraints\non network weights or manifold of hidden space. These types of autoencoders\nexhibited perfect generalization ability for test data without requiring the\ncompression phase. Thus, we conclude that the compression phase is not\nnecessary for generalization in representation learning.",
          "link": "http://arxiv.org/abs/2102.07402",
          "publishedOn": "2021-07-19T00:49:07.466Z",
          "wordCount": 633,
          "title": "Information flows of diverse autoencoders. (arXiv:2102.07402v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaojian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuchen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>",
          "description": "Semantic segmentation for scene understanding is nowadays widely demanded,\nraising significant challenges for the algorithm efficiency, especially its\napplications on resource-limited platforms. Current segmentation models are\ntrained and evaluated on massive high-resolution scene images (\"data level\")\nand suffer from the expensive computation arising from the required multi-scale\naggregation(\"network level\"). In both folds, the computational and energy costs\nin training and inference are notable due to the often desired large input\nresolutions and heavy computational burden of segmentation models. To this end,\nwe propose DANCE, general automated DAta-Network Co-optimization for Efficient\nsegmentation model training and inference. Distinct from existing efficient\nsegmentation approaches that focus merely on light-weight network design, DANCE\ndistinguishes itself as an automated simultaneous data-network co-optimization\nvia both input data manipulation and network architecture slimming.\nSpecifically, DANCE integrates automated data slimming which adaptively\ndownsamples/drops input images and controls their corresponding contribution to\nthe training loss guided by the images' spatial complexity. Such a downsampling\noperation, in addition to slimming down the cost associated with the input size\ndirectly, also shrinks the dynamic range of input object and context scales,\ntherefore motivating us to also adaptively slim the network to match the\ndownsampled data. Extensive experiments and ablating studies (on four SOTA\nsegmentation models with three popular segmentation datasets under two training\nsettings) demonstrate that DANCE can achieve \"all-win\" towards efficient\nsegmentation(reduced training cost, less expensive inference, and better mean\nIntersection-over-Union (mIoU)).",
          "link": "http://arxiv.org/abs/2107.07706",
          "publishedOn": "2021-07-19T00:49:07.450Z",
          "wordCount": 687,
          "title": "DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference. (arXiv:2107.07706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berg_J/0/1/0/all/0/1\">Jan Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drossos_K/0/1/0/all/0/1\">Konstantinos Drossos</a>",
          "description": "Automated audio captioning (AAC) is the task of automatically creating\ntextual descriptions (i.e. captions) for the contents of a general audio\nsignal. Most AAC methods are using existing datasets to optimize and/or\nevaluate upon. Given the limited information held by the AAC datasets, it is\nvery likely that AAC methods learn only the information contained in the\nutilized datasets. In this paper we present a first approach for continuously\nadapting an AAC method to new information, using a continual learning method.\nIn our scenario, a pre-optimized AAC method is used for some unseen general\naudio signals and can update its parameters in order to adapt to the new\ninformation, given a new reference caption. We evaluate our method using a\nfreely available, pre-optimized AAC method and two freely available AAC\ndatasets. We compare our proposed method with three scenarios, two of training\non one of the datasets and evaluating on the other and a third of training on\none dataset and fine-tuning on the other. Obtained results show that our method\nachieves a good balance between distilling new knowledge and not forgetting the\nprevious one.",
          "link": "http://arxiv.org/abs/2107.08028",
          "publishedOn": "2021-07-19T00:49:07.440Z",
          "wordCount": 628,
          "title": "Continual Learning for Automated Audio Captioning Using The Learning Without Forgetting Approach. (arXiv:2107.08028v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_M/0/1/0/all/0/1\">Meng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xhonneux_L/0/1/0/all/0/1\">Louis-Pascal Xhonneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>",
          "description": "This paper studies learning logic rules for reasoning on knowledge graphs.\nLogic rules provide interpretable explanations when used for prediction as well\nas being able to generalize to other tasks, and hence are critical to learn.\nExisting methods either suffer from the problem of searching in a large search\nspace (e.g., neural logic programming) or ineffective optimization due to\nsparse rewards (e.g., techniques based on reinforcement learning). To address\nthese limitations, this paper proposes a probabilistic model called RNNLogic.\nRNNLogic treats logic rules as a latent variable, and simultaneously trains a\nrule generator as well as a reasoning predictor with logic rules. We develop an\nEM-based algorithm for optimization. In each iteration, the reasoning predictor\nis first updated to explore some generated logic rules for reasoning. Then in\nthe E-step, we select a set of high-quality rules from all generated rules with\nboth the rule generator and reasoning predictor via posterior inference; and in\nthe M-step, the rule generator is updated with the rules selected in the\nE-step. Experiments on four datasets prove the effectiveness of RNNLogic.",
          "link": "http://arxiv.org/abs/2010.04029",
          "publishedOn": "2021-07-19T00:49:07.391Z",
          "wordCount": 647,
          "title": "RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs. (arXiv:2010.04029v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.09910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1\">Johan A.K. Suykens</a>",
          "description": "This paper generalizes regularized regression problems in a hyper-reproducing\nkernel Hilbert space (hyper-RKHS), illustrates its utility for kernel learning\nand out-of-sample extensions, and proves asymptotic convergence results for the\nintroduced regression models in an approximation theory view. Algorithmically,\nwe consider two regularized regression models with bivariate forms in this\nspace, including kernel ridge regression (KRR) and support vector regression\n(SVR) endowed with hyper-RKHS, and further combine divide-and-conquer with\nNystr\\\"{o}m approximation for scalability in large sample cases. This framework\nis general: the underlying kernel is learned from a broad class, and can be\npositive definite or not, which adapts to various requirements in kernel\nlearning. Theoretically, we study the convergence behavior of regularized\nregression algorithms in hyper-RKHS and derive the learning rates, which goes\nbeyond the classical analysis on RKHS due to the non-trivial independence of\npairwise samples and the characterisation of hyper-RKHS. Experimentally,\nresults on several benchmarks suggest that the employed framework is able to\nlearn a general kernel function form an arbitrary similarity matrix, and thus\nachieves a satisfactory performance on classification tasks.",
          "link": "http://arxiv.org/abs/1809.09910",
          "publishedOn": "2021-07-19T00:49:07.386Z",
          "wordCount": 656,
          "title": "Generalization Properties of hyper-RKHS and its Applications. (arXiv:1809.09910v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1\">Lina Achaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1\">Thibault Fouqueray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1\">Francois Aioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1\">Francois Charpillet</a>",
          "description": "The human driver is no longer the only one concerned with the complexity of\nthe driving scenarios. Autonomous vehicles (AV) are similarly becoming involved\nin the process. Nowadays, the development of AV in urban places underpins\nessential safety concerns for vulnerable road users (VRUs) such as pedestrians.\nTherefore, to make the roads safer, it is critical to classify and predict\ntheir future behavior. In this paper, we present a framework based on multiple\nvariations of the Transformer models to reason attentively about the dynamic\nevolution of the pedestrians' past trajectory and predict its future actions of\ncrossing or not crossing the street. We proved that using only bounding boxes\nas input to our model can outperform the previous state-of-the-art models and\nreach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset\nup to two seconds ahead in the future. In addition, we introduced a large-size\nsimulated dataset (CP2A) using CARLA for action prediction. Our model has\nsimilarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.\nInterestingly, we showed that pre-training our Transformer model on the\nsimulated dataset and then fine-tuning it on the real dataset can be very\neffective for the action prediction task.",
          "link": "http://arxiv.org/abs/2107.08031",
          "publishedOn": "2021-07-19T00:49:07.373Z",
          "wordCount": 656,
          "title": "Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoxian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>",
          "description": "We introduce a new class of graph neural networks (GNNs), by combining\nseveral concepts that were so far studied independently - graph kernels,\nattention-based networks with structural priors and more recently, efficient\nTransformers architectures applying small memory footprint implicit attention\nmethods via low rank decomposition techniques. The goal of the paper is\ntwofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much\nmore expressive than SOTA GNNs as capable of modeling longer-range dependencies\nwithin a single layer. Consequently, they can use more shallow architecture\ndesign. Furthermore, GKAT attention layers scale linearly rather than\nquadratically in the number of nodes of the input graphs, even when those\ngraphs are dense, requiring less compute than their regular graph attention\ncounterparts. They achieve it by applying new classes of graph kernels\nadmitting random feature map decomposition via random walks on graphs. As a\nbyproduct of the introduced techniques, we obtain a new class of learnable\ngraph sketches, called graphots, compactly encoding topological graph\nproperties as well as nodes' features. We conducted exhaustive empirical\ncomparison of our method with nine different GNN classes on tasks ranging from\nmotif detection through social network classification to bioinformatics\nchallenges, showing consistent gains coming from GKATs.",
          "link": "http://arxiv.org/abs/2107.07999",
          "publishedOn": "2021-07-19T00:49:07.330Z",
          "wordCount": 630,
          "title": "Graph Kernel Attention Transformers. (arXiv:2107.07999v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tanveer Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalas_A/0/1/0/all/0/1\">Antonis Michalas</a>",
          "description": "Social networking and micro-blogging services, such as Twitter, play an\nimportant role in sharing digital information. Despite the popularity and\nusefulness of social media, there have been many instances where corrupted\nusers found ways to abuse it, as for instance, through raising or lowering\nuser's credibility. As a result, while social media facilitates an\nunprecedented ease of access to information, it also introduces a new challenge\n- that of ascertaining the credibility of shared information. Currently, there\nis no automated way of determining which news or users are credible and which\nare not. Hence, establishing a system that can measure the social media user's\ncredibility has become an issue of great importance. Assigning a credibility\nscore to a user has piqued the interest of not only the research community but\nalso most of the big players on both sides - such as Facebook, on the side of\nindustry, and political parties on the societal one. In this work, we created a\nmodel which, we hope, will ultimately facilitate and support the increase of\ntrust in the social network communities. Our model collected data and analysed\nthe behaviour of~50,000 politicians on Twitter. Influence score, based on\nseveral chosen features, was assigned to each evaluated user. Further, we\nclassified the political Twitter users as either trusted or untrusted using\nrandom forest, multilayer perceptron, and support vector machine. An active\nlearning model was used to classify any unlabelled ambiguous records from our\ndataset. Finally, to measure the performance of the proposed model, we used\nprecision, recall, F1 score, and accuracy as the main evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.08027",
          "publishedOn": "2021-07-19T00:49:07.323Z",
          "wordCount": 704,
          "title": "SOK: Seeing and Believing: Evaluating the Trustworthiness of Twitter Users. (arXiv:2107.08027v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi-Gang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul N. Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1\">Matthew Mattina</a>",
          "description": "Exploiting sparsity is a key technique in accelerating quantized\nconvolutional neural network (CNN) inference on mobile devices. Prior sparse\nCNN accelerators largely exploit un-structured sparsity and achieve significant\nspeedups. Due to the unbounded, largely unpredictable sparsity patterns,\nhowever, exploiting unstructured sparsity requires complicated hardware design\nwith significant energy and area overhead, which is particularly detrimental to\nmobile/IoT inference scenarios where energy and area efficiency are crucial. We\npropose to exploit structured sparsity, more specifically, Density Bound Block\n(DBB) sparsity for both weights and activations. DBB block tensors bound the\nmaximum number of non-zeros per block. DBB thus exposes statically predictable\nsparsity patterns that enable lean sparsity-exploiting hardware. We propose new\nhardware primitives to implement DBB sparsity for (static) weights and\n(dynamic) activations, respectively, with very low overheads. Building on top\nof the primitives, we describe S2TA, a systolic array-based CNN accelerator\nthat exploits joint weight and activation DBB sparsity and new dimensions of\ndata reuse unavailable on the traditional systolic array. S2TA in 16nm achieves\nmore than 2x speedup and energy reduction compared to a strong baseline of a\nsystolic array with zero-value clock gating, over five popular CNN benchmarks.\nCompared to two recent non-systolic sparse accelerators, Eyeriss v2 (65nm) and\nSparTen (45nm), S2TA in 65nm uses about 2.2x and 3.1x less energy per\ninference, respectively.",
          "link": "http://arxiv.org/abs/2107.07983",
          "publishedOn": "2021-07-19T00:49:07.318Z",
          "wordCount": 652,
          "title": "S2TA: Exploiting Structured Sparsity for Energy-Efficient Mobile CNN Acceleration. (arXiv:2107.07983v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2004.02653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sigrist_F/0/1/0/all/0/1\">Fabio Sigrist</a>",
          "description": "We introduce a novel way to combine boosting with Gaussian process and mixed\neffects models. This allows for relaxing, first, the linearity assumption for\nthe mean function in Gaussian process and grouped random effects models in a\nflexible non-parametric way and, second, the independence assumption made in\nmost boosting algorithms. The former is advantageous for predictive accuracy\nand for avoiding model misspecifications. The latter is important for more\nefficient learning of the mean function and for obtaining probabilistic\npredictions. In addition, we present an extension that scales to large data\nusing a Vecchia approximation for the Gaussian process model relying on novel\nresults for covariance parameter inference. We obtain increased predictive\naccuracy compared to existing approaches on several simulated and real-world\ndata sets.",
          "link": "http://arxiv.org/abs/2004.02653",
          "publishedOn": "2021-07-19T00:49:07.312Z",
          "wordCount": 583,
          "title": "Gaussian Process Boosting. (arXiv:2004.02653v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08013",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Miles_C/0/1/0/all/0/1\">Cole Miles</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Carbone_M/0/1/0/all/0/1\">Matthew R. Carbone</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Sturm_E/0/1/0/all/0/1\">Erica J. Sturm</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lu_D/0/1/0/all/0/1\">Deyu Lu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Weichselbaum_A/0/1/0/all/0/1\">Andreas Weichselbaum</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Barros_K/0/1/0/all/0/1\">Kipton Barros</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Konik_R/0/1/0/all/0/1\">Robert M. Konik</a>",
          "description": "We employ variational autoencoders to extract physical insight from a dataset\nof one-particle Anderson impurity model spectral functions. Autoencoders are\ntrained to find a low-dimensional, latent space representation that faithfully\ncharacterizes each element of the training set, as measured by a reconstruction\nerror. Variational autoencoders, a probabilistic generalization of standard\nautoencoders, further condition the learned latent space to promote highly\ninterpretable features. In our study, we find that the learned latent space\ncomponents strongly correlate with well known, but nontrivial, parameters that\ncharacterize emergent behaviors in the Anderson impurity model. In particular,\none latent space component correlates with particle-hole asymmetry, while\nanother is in near one-to-one correspondence with the Kondo temperature, a\ndynamically generated low-energy scale in the impurity model. With symbolic\nregression, we model this component as a function of bare physical input\nparameters and \"rediscover\" the non-perturbative formula for the Kondo\ntemperature. The machine learning pipeline we develop opens opportunities to\ndiscover new domain knowledge in other physical systems.",
          "link": "http://arxiv.org/abs/2107.08013",
          "publishedOn": "2021-07-19T00:49:07.294Z",
          "wordCount": 623,
          "title": "Machine-learning Kondo physics using variational autoencoders. (arXiv:2107.08013v1 [cond-mat.str-el])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lulan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guikang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "Multiple studies in the past have shown that there is a strong correlation\nbetween human vocal characteristics and facial features. However, existing\napproaches generate faces simply from voice, without exploring the set of\nfeatures that contribute to these observed correlations. A computational\nmethodology to explore this can be devised by rephrasing the question to: \"how\nmuch would a target face have to change in order to be perceived as the\noriginator of a source voice?\" With this in perspective, we propose a framework\nto morph a target face in response to a given voice in a way that facial\nfeatures are implicitly guided by learned voice-face correlation in this paper.\nOur framework includes a guided autoencoder that converts one face to another,\ncontrolled by a unique model-conditioning component called a gating controller\nwhich modifies the reconstructed face based on input voice recordings. We\nevaluate the framework on VoxCelab and VGGFace datasets through human subjects\nand face retrieval. Various experiments demonstrate the effectiveness of our\nproposed model.",
          "link": "http://arxiv.org/abs/2107.07988",
          "publishedOn": "2021-07-19T00:49:07.288Z",
          "wordCount": 621,
          "title": "Controlled AutoEncoders to Generate Faces from Voices. (arXiv:2107.07988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08001",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gabrie_M/0/1/0/all/0/1\">Marylou Gabri&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rotskoff_G/0/1/0/all/0/1\">Grant M. Rotskoff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1\">Eric Vanden-Eijnden</a>",
          "description": "Normalizing flows can generate complex target distributions and thus show\npromise in many applications in Bayesian statistics as an alternative or\ncomplement to MCMC for sampling posteriors. Since no data set from the target\nposterior distribution is available beforehand, the flow is typically trained\nusing the reverse Kullback-Leibler (KL) divergence that only requires samples\nfrom a base distribution. This strategy may perform poorly when the posterior\nis complicated and hard to sample with an untrained normalizing flow. Here we\nexplore a distinct training strategy, using the direct KL divergence as loss,\nin which samples from the posterior are generated by (i) assisting a local MCMC\nalgorithm on the posterior with a normalizing flow to accelerate its mixing\nrate and (ii) using the data generated this way to train the flow. The method\nonly requires a limited amount of \\textit{a~priori} input about the posterior,\nand can be used to estimate the evidence required for model validation, as we\nillustrate on examples.",
          "link": "http://arxiv.org/abs/2107.08001",
          "publishedOn": "2021-07-19T00:49:07.282Z",
          "wordCount": 613,
          "title": "Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods. (arXiv:2107.08001v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07886",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Jiang_H/0/1/0/all/0/1\">Haodi Jiang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jing_J/0/1/0/all/0/1\">Ju Jing</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1\">Jiasheng Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1\">Jason T. L. Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_H/0/1/0/all/0/1\">Haimin Wang</a>",
          "description": "We present a new deep learning method, dubbed FibrilNet, for tracing\nchromospheric fibrils in Halpha images of solar observations. Our method\nconsists of a data pre-processing component that prepares training data from a\nthreshold-based tool, a deep learning model implemented as a Bayesian\nconvolutional neural network for probabilistic image segmentation with\nuncertainty quantification to predict fibrils, and a post-processing component\ncontaining a fibril-fitting algorithm to determine fibril orientations. The\nFibrilNet tool is applied to high-resolution Halpha images from an active\nregion (AR 12665) collected by the 1.6 m Goode Solar Telescope (GST) equipped\nwith high-order adaptive optics at the Big Bear Solar Observatory (BBSO). We\nquantitatively assess the FibrilNet tool, comparing its image segmentation\nalgorithm and fibril-fitting algorithm with those employed by the\nthreshold-based tool. Our experimental results and major findings are\nsummarized as follows. First, the image segmentation results (i.e., detected\nfibrils) of the two tools are quite similar, demonstrating the good learning\ncapability of FibrilNet. Second, FibrilNet finds more accurate and smoother\nfibril orientation angles than the threshold-based tool. Third, FibrilNet is\nfaster than the threshold-based tool and the uncertainty maps produced by\nFibrilNet not only provide a quantitative way to measure the confidence on each\ndetected fibril, but also help identify fibril structures that are not detected\nby the threshold-based tool but are inferred through machine learning. Finally,\nwe apply FibrilNet to full-disk Halpha images from other solar observatories\nand additional high-resolution Halpha images collected by BBSO/GST,\ndemonstrating the tool's usability in diverse datasets.",
          "link": "http://arxiv.org/abs/2107.07886",
          "publishedOn": "2021-07-19T00:49:07.276Z",
          "wordCount": 700,
          "title": "Tracing Halpha Fibrils through Bayesian Deep Learning. (arXiv:2107.07886v1 [astro-ph.SR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyeon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyung-Kwon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaemin Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngtaek Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jinwook Seo</a>",
          "description": "We propose Steadiness and Cohesiveness, two novel metrics to measure the\ninter-cluster reliability of multidimensional projection (MDP), specifically\nhow well the inter-cluster structures are preserved between the original\nhigh-dimensional space and the low-dimensional projection space. Measuring\ninter-cluster reliability is crucial as it directly affects how well\ninter-cluster tasks (e.g., identifying cluster relationships in the original\nspace from a projected view) can be conducted; however, despite the importance\nof inter-cluster tasks, we found that previous metrics, such as Trustworthiness\nand Continuity, fail to measure inter-cluster reliability. Our metrics consider\ntwo aspects of the inter-cluster reliability: Steadiness measures the extent to\nwhich clusters in the projected space form clusters in the original space, and\nCohesiveness measures the opposite. They extract random clusters with arbitrary\nshapes and positions in one space and evaluate how much the clusters are\nstretched or dispersed in the other space. Furthermore, our metrics can\nquantify pointwise distortions, allowing for the visualization of inter-cluster\nreliability in a projection, which we call a reliability map. Through\nquantitative experiments, we verify that our metrics precisely capture the\ndistortions that harm inter-cluster reliability while previous metrics have\ndifficulty capturing the distortions. A case study also demonstrates that our\nmetrics and the reliability map 1) support users in selecting the proper\nprojection techniques or hyperparameters and 2) prevent misinterpretation while\nperforming inter-cluster tasks, thus allow an adequate identification of\ninter-cluster structure.",
          "link": "http://arxiv.org/abs/2107.07859",
          "publishedOn": "2021-07-19T00:49:07.270Z",
          "wordCount": 686,
          "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections. (arXiv:2107.07859v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "Molecular property prediction plays a fundamental role in drug discovery to\ndiscover candidate molecules with target properties. However, molecular\nproperty prediction is essentially a few-shot problem which makes it hard to\nobtain regular models. In this paper, we propose a property-aware adaptive\nrelation networks (PAR) for the few-shot molecular property prediction problem.\nIn comparison to existing works, we leverage the facts that both substructures\nand relationships among molecules are different considering various molecular\nproperties. Our PAR is compatible with existing graph-based molecular encoders,\nand are further equipped with the ability to obtain property-aware molecular\nembedding and model molecular relation graph adaptively. The resultant relation\ngraph also facilitates effective label propagation within each task. Extensive\nexperiments on benchmark molecular property prediction datasets show that our\nmethod consistently outperforms state-of-the-art methods and is able to obtain\nproperty-aware molecular embedding and model molecular relation graph properly.",
          "link": "http://arxiv.org/abs/2107.07994",
          "publishedOn": "2021-07-19T00:49:07.265Z",
          "wordCount": 579,
          "title": "Property-aware Adaptive Relation Networks for Molecular Property Prediction. (arXiv:2107.07994v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_T/0/1/0/all/0/1\">Tim Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernsting_J/0/1/0/all/0/1\">Jan Ernsting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_N/0/1/0/all/0/1\">Nils R. Winter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holstein_V/0/1/0/all/0/1\">Vincent Holstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leenings_R/0/1/0/all/0/1\">Ramona Leenings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beisemann_M/0/1/0/all/0/1\">Marie Beisemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_L/0/1/0/all/0/1\">Lukas Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarink_K/0/1/0/all/0/1\">Kelvin Sarink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emden_D/0/1/0/all/0/1\">Daniel Emden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opel_N/0/1/0/all/0/1\">Nils Opel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redlich_R/0/1/0/all/0/1\">Ronny Redlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repple_J/0/1/0/all/0/1\">Jonathan Repple</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grotegerd_D/0/1/0/all/0/1\">Dominik Grotegerd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinert_S/0/1/0/all/0/1\">Susanne Meinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_J/0/1/0/all/0/1\">Jochen G. Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niendorf_T/0/1/0/all/0/1\">Thoralf Niendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Endemann_B/0/1/0/all/0/1\">Beate Endemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamberg_F/0/1/0/all/0/1\">Fabian Bamberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroncke_T/0/1/0/all/0/1\">Thomas Kr&#xf6;ncke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulow_R/0/1/0/all/0/1\">Robin B&#xfc;low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volzke_H/0/1/0/all/0/1\">Henry V&#xf6;lzke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stackelberg_O/0/1/0/all/0/1\">Oyunbileg von Stackelberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sowade_R/0/1/0/all/0/1\">Ramona Felizitas Sowade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umutlu_L/0/1/0/all/0/1\">Lale Umutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_B/0/1/0/all/0/1\">B&#xf6;rge Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caspers_S/0/1/0/all/0/1\">Svenja Caspers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Consortium_G/0/1/0/all/0/1\">German National Cohort Study Center Consortium</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugel_H/0/1/0/all/0/1\">Harald Kugel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kircher_T/0/1/0/all/0/1\">Tilo Kircher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risse_B/0/1/0/all/0/1\">Benjamin Risse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaser_C/0/1/0/all/0/1\">Christian Gaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">James H. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannlowski_U/0/1/0/all/0/1\">Udo Dannlowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_K/0/1/0/all/0/1\">Klaus Berger</a>",
          "description": "The deviation between chronological age and age predicted from neuroimaging\ndata has been identified as a sensitive risk-marker of cross-disorder brain\nchanges, growing into a cornerstone of biological age-research. However,\nMachine Learning models underlying the field do not consider uncertainty,\nthereby confounding results with training data density and variability. Also,\nexisting models are commonly based on homogeneous training sets, often not\nindependently validated, and cannot be shared due to data protection issues.\nHere, we introduce an uncertainty-aware, shareable, and transparent Monte-Carlo\nDropout Composite-Quantile-Regression (MCCQR) Neural Network trained on\nN=10,691 datasets from the German National Cohort. The MCCQR model provides\nrobust, distribution-free uncertainty quantification in high-dimensional\nneuroimaging data, achieving lower error rates compared to existing models\nacross ten recruitment centers and in three independent validation samples\n(N=4,004). In two examples, we demonstrate that it prevents spurious\nassociations and increases power to detect accelerated brain-aging. We make the\npre-trained model publicly available.",
          "link": "http://arxiv.org/abs/2107.07977",
          "publishedOn": "2021-07-19T00:49:07.248Z",
          "wordCount": 661,
          "title": "An Uncertainty-Aware, Shareable and Transparent Neural Network Architecture for Brain-Age Modeling. (arXiv:2107.07977v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08020",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiye Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bigot_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;mie Bigot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maabout_S/0/1/0/all/0/1\">Sofian Maabout</a>",
          "description": "This paper is concerned with the statistical analysis of matrix-valued time\nseries. These are data collected over a network of sensors (typically a set of\nspatial locations), recording, over time, observations of multiple\nmeasurements. From such data, we propose to learn, in an online fashion, a\ngraph that captures two aspects of dependency: one describing the sparse\nspatial relationship between sensors, and the other characterizing the\nmeasurement relationship. To this purpose, we introduce a novel multivariate\nautoregressive model to infer the graph topology encoded in the coefficient\nmatrix which captures the sparse Granger causality dependency structure present\nin such matrix-valued time series. We decompose the graph by imposing a\nKronecker sum structure on the coefficient matrix. We develop two online\napproaches to learn the graph in a recursive way. The first one uses Wald test\nfor the projected OLS estimation, where we derive the asymptotic distribution\nfor the estimator. For the second one, we formalize a Lasso-type optimization\nproblem. We rely on homotopy algorithms to derive updating rules for estimating\nthe coefficient matrix. Furthermore, we provide an adaptive tuning procedure\nfor the regularization parameter. Numerical experiments using both synthetic\nand real data, are performed to support the effectiveness of the proposed\nlearning approaches.",
          "link": "http://arxiv.org/abs/2107.08020",
          "publishedOn": "2021-07-19T00:49:07.242Z",
          "wordCount": 635,
          "title": "Online Graph Topology Learning from Matrix-valued Time Series. (arXiv:2107.08020v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavazza_F/0/1/0/all/0/1\">Francesca Tavazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cost_B/0/1/0/all/0/1\">Brian De Cost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_K/0/1/0/all/0/1\">Kamal Choudhary</a>",
          "description": "Uncertainty quantification in Artificial Intelligence (AI)-based predictions\nof material properties is of immense importance for the success and reliability\nof AI applications in material science. While confidence intervals are commonly\nreported for machine learning (ML) models, prediction intervals, i.e., the\nevaluation of the uncertainty on each prediction, are seldomly available. In\nthis work we compare 3 different approaches to obtain such individual\nuncertainty, testing them on 12 ML-physical properties. Specifically, we\ninvestigated using the Quantile loss function, machine learning the prediction\nintervals directly and using Gaussian Processes. We identify each approachs\nadvantages and disadvantages and end up slightly favoring the modeling of the\nindividual uncertainties directly, as it is the easiest to fit and, in most\ncases, minimizes over-and under-estimation of the predicted errors. All data\nfor training and testing were taken from the publicly available JARVIS-DFT\ndatabase, and the codes developed for computing the prediction intervals are\navailable through JARVIS-Tools.",
          "link": "http://arxiv.org/abs/2107.07997",
          "publishedOn": "2021-07-19T00:49:07.236Z",
          "wordCount": 585,
          "title": "Uncertainty Prediction for Machine Learning Models of Material Properties. (arXiv:2107.07997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1\">Marios Mattheakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sondak_D/0/1/0/all/0/1\">David Sondak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1\">Pavlos Protopapas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Accurately learning the temporal behavior of dynamical systems requires\nmodels with well-chosen learning biases. Recent innovations embed the\nHamiltonian and Lagrangian formalisms into neural networks and demonstrate a\nsignificant improvement over other approaches in predicting trajectories of\nphysical systems. These methods generally tackle autonomous systems that depend\nimplicitly on time or systems for which a control signal is known apriori.\nDespite this success, many real world dynamical systems are non-autonomous,\ndriven by time-dependent forces and experience energy dissipation. In this\nstudy, we address the challenge of learning from such non-autonomous systems by\nembedding the port-Hamiltonian formalism into neural networks, a versatile\nframework that can capture energy dissipation and time-dependent control\nforces. We show that the proposed \\emph{port-Hamiltonian neural network} can\nefficiently learn the dynamics of nonlinear physical systems of practical\ninterest and accurately recover the underlying stationary Hamiltonian,\ntime-dependent force, and dissipative coefficient. A promising outcome of our\nnetwork is its ability to learn and predict chaotic systems such as the Duffing\nequation, for which the trajectories are typically hard to learn.",
          "link": "http://arxiv.org/abs/2107.08024",
          "publishedOn": "2021-07-19T00:49:07.219Z",
          "wordCount": 619,
          "title": "Port-Hamiltonian Neural Networks for Learning Explicit Time-Dependent Dynamical Systems. (arXiv:2107.08024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Syed Ali Raza Zaidi</a>",
          "description": "In this paper, we present an overview of Nearest neighbor (NN) methods, which\nare frequently employed for solving classification problems using supervised\nlearning. The article concisely introduces the theoretical background,\nalgorithmic, and implementation aspects along with the key applications. From\nan application standpoint, this article explores the challenges related to the\n5G and beyond wireless networks which can be solved using NN classification\ntechniques.",
          "link": "http://arxiv.org/abs/2107.07869",
          "publishedOn": "2021-07-19T00:49:07.212Z",
          "wordCount": 516,
          "title": "Nearest neighbor Methods and their Applications in Design of 5G & Beyond Wireless Networks. (arXiv:2107.07869v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07871",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Moseley_B/0/1/0/all/0/1\">Ben Moseley</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nissen_Meyer_T/0/1/0/all/0/1\">Tarje Nissen-Meyer</a>",
          "description": "Recently, physics-informed neural networks (PINNs) have offered a powerful\nnew paradigm for solving problems relating to differential equations. Compared\nto classical numerical methods PINNs have several advantages, for example their\nability to provide mesh-free solutions of differential equations and their\nability to carry out forward and inverse modelling within the same optimisation\nproblem. Whilst promising, a key limitation to date is that PINNs have\nstruggled to accurately and efficiently solve problems with large domains\nand/or multi-scale solutions, which is crucial for their real-world\napplication. Multiple significant and related factors contribute to this issue,\nincluding the increasing complexity of the underlying PINN optimisation problem\nas the problem size grows and the spectral bias of neural networks. In this\nwork we propose a new, scalable approach for solving large problems relating to\ndifferential equations called Finite Basis PINNs (FBPINNs). FBPINNs are\ninspired by classical finite element methods, where the solution of the\ndifferential equation is expressed as the sum of a finite set of basis\nfunctions with compact support. In FBPINNs neural networks are used to learn\nthese basis functions, which are defined over small, overlapping subdomains.\nFBINNs are designed to address the spectral bias of neural networks by using\nseparate input normalisation over each subdomain, and reduce the complexity of\nthe underlying optimisation problem by using many smaller neural networks in a\nparallel divide-and-conquer approach. Our numerical experiments show that\nFBPINNs are effective in solving both small and larger, multi-scale problems,\noutperforming standard PINNs in both accuracy and computational resources\nrequired, potentially paving the way to the application of PINNs on large,\nreal-world problems.",
          "link": "http://arxiv.org/abs/2107.07871",
          "publishedOn": "2021-07-19T00:49:07.205Z",
          "wordCount": 721,
          "title": "Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations. (arXiv:2107.07871v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08011",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Antonakopoulos_K/0/1/0/all/0/1\">Kimon Antonakopoulos</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>",
          "description": "We propose a new family of adaptive first-order methods for a class of convex\nminimization problems that may fail to be Lipschitz continuous or smooth in the\nstandard sense. Specifically, motivated by a recent flurry of activity on\nnon-Lipschitz (NoLips) optimization, we consider problems that are continuous\nor smooth relative to a reference Bregman function - as opposed to a global,\nambient norm (Euclidean or otherwise). These conditions encompass a wide range\nof problems with singular objectives, such as Fisher markets, Poisson\ntomography, D-design, and the like. In this setting, the application of\nexisting order-optimal adaptive methods - like UnixGrad or AcceleGrad - is not\npossible, especially in the presence of randomness and uncertainty. The\nproposed method - which we call adaptive mirror descent (AdaMir) - aims to\nclose this gap by concurrently achieving min-max optimal rates in problems that\nare relatively continuous or smooth, including stochastic ones.",
          "link": "http://arxiv.org/abs/2107.08011",
          "publishedOn": "2021-07-19T00:49:07.198Z",
          "wordCount": 596,
          "title": "Adaptive first-order methods revisited: Convex optimization without Lipschitz requirements. (arXiv:2107.08011v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "In applications such as natural language processing or computer vision, one\nis given a large $n \\times d$ matrix $A = (a_{i,j})$ and would like to compute\na matrix decomposition, e.g., a low rank approximation, of a function $f(A) =\n(f(a_{i,j}))$ applied entrywise to $A$. A very important special case is the\nlikelihood function $f\\left( A \\right ) = \\log{\\left( \\left| a_{ij}\\right|\n+1\\right)}$. A natural way to do this would be to simply apply $f$ to each\nentry of $A$, and then compute the matrix decomposition, but this requires\nstoring all of $A$ as well as multiple passes over its entries. Recent work of\nLiang et al.\\ shows how to find a rank-$k$ factorization to $f(A)$ for an $n\n\\times n$ matrix $A$ using only $n \\cdot \\operatorname{poly}(\\epsilon^{-1}k\\log\nn)$ words of memory, with overall error $10\\|f(A)-[f(A)]_k\\|_F^2 +\n\\operatorname{poly}(\\epsilon/k) \\|f(A)\\|_{1,2}^2$, where $[f(A)]_k$ is the best\nrank-$k$ approximation to $f(A)$ and $\\|f(A)\\|_{1,2}^2$ is the square of the\nsum of Euclidean lengths of rows of $f(A)$. Their algorithm uses three passes\nover the entries of $A$. The authors pose the open question of obtaining an\nalgorithm with $n \\cdot \\operatorname{poly}(\\epsilon^{-1}k\\log n)$ words of\nmemory using only a single pass over the entries of $A$. In this paper we\nresolve this open question, obtaining the first single-pass algorithm for this\nproblem and for the same class of functions $f$ studied by Liang et al.\nMoreover, our error is $\\|f(A)-[f(A)]_k\\|_F^2 + \\operatorname{poly}(\\epsilon/k)\n\\|f(A)\\|_F^2$, where $\\|f(A)\\|_F^2$ is the sum of squares of Euclidean lengths\nof rows of $f(A)$. Thus our error is significantly smaller, as it removes the\nfactor of $10$ and also $\\|f(A)\\|_F^2 \\leq \\|f(A)\\|_{1,2}^2$. We also give an\nalgorithm for regression, pointing out an error in previous work, and\nempirically validate our results.",
          "link": "http://arxiv.org/abs/2107.07889",
          "publishedOn": "2021-07-19T00:49:07.186Z",
          "wordCount": 731,
          "title": "Single Pass Entrywise-Transformed Low Rank Approximation. (arXiv:2107.07889v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolcun_R/0/1/0/all/0/1\">Roman Kolcun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_D/0/1/0/all/0/1\">Diana Andreea Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safronov_V/0/1/0/all/0/1\">Vadim Safronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Poonam Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandalari_A/0/1/0/all/0/1\">Anna Maria Mandalari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortier_R/0/1/0/all/0/1\">Richard Mortier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>",
          "description": "Internet-of-Things (IoT) devices are known to be the source of many security\nproblems, and as such, they would greatly benefit from automated management.\nThis requires robustly identifying devices so that appropriate network security\npolicies can be applied. We address this challenge by exploring how to\naccurately identify IoT devices based on their network behavior, while\nleveraging approaches previously proposed by other researchers.\n\nWe compare the accuracy of four different previously proposed machine\nlearning models (tree-based and neural network-based) for identifying IoT\ndevices. We use packet trace data collected over a period of six months from a\nlarge IoT test-bed. We show that, while all models achieve high accuracy when\nevaluated on the same dataset as they were trained on, their accuracy degrades\nover time, when evaluated on data collected outside the training set. We show\nthat on average the models' accuracy degrades after a couple of weeks by up to\n40 percentage points (on average between 12 and 21 percentage points). We argue\nthat, in order to keep the models' accuracy at a high level, these need to be\ncontinuously updated.",
          "link": "http://arxiv.org/abs/2107.07818",
          "publishedOn": "2021-07-19T00:49:07.140Z",
          "wordCount": 634,
          "title": "Revisiting IoT Device Identification. (arXiv:2107.07818v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thor_M/0/1/0/all/0/1\">Mathias Thor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoonpong_P/0/1/0/all/0/1\">Poramate Manoonpong</a>",
          "description": "Legged robots have significant potential to operate in highly unstructured\nenvironments. The design of locomotion control is, however, still challenging.\nCurrently, controllers must be either manually designed for specific robots and\ntasks, or automatically designed via machine learning methods that require long\ntraining times and yield large opaque controllers. Drawing inspiration from\nanimal locomotion, we propose a simple yet versatile modular neural control\nstructure with fast learning. The key advantages of our approach are that\nbehavior-specific control modules can be added incrementally to obtain\nincreasingly complex emergent locomotion behaviors, and that neural connections\ninterfacing with existing modules can be quickly and automatically learned. In\na series of experiments, we show how eight modules can be quickly learned and\nadded to a base control module to obtain emergent adaptive behaviors allowing a\nhexapod robot to navigate in complex environments. We also show that modules\ncan be added and removed during operation without affecting the functionality\nof the remaining controller. Finally, the control approach was successfully\ndemonstrated on a physical hexapod robot. Taken together, our study reveals a\nsignificant step towards fast automatic design of versatile neural locomotion\ncontrol for complex robotic systems.",
          "link": "http://arxiv.org/abs/2107.07844",
          "publishedOn": "2021-07-19T00:49:07.134Z",
          "wordCount": 641,
          "title": "Versatile modular neural locomotion control with fast learning. (arXiv:2107.07844v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07875",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nalamada_T/0/1/0/all/0/1\">Trikay Nalamada</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Agarwal_S/0/1/0/all/0/1\">Shruti Agarwal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jahja_M/0/1/0/all/0/1\">Maria Jahja</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chakraborty_B/0/1/0/all/0/1\">Bibhas Chakraborty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_P/0/1/0/all/0/1\">Palash Ghosh</a>",
          "description": "A dynamic treatment regimen (DTR) is a set of decision rules to personalize\ntreatments for an individual using their medical history. The Q-learning based\nQ-shared algorithm has been used to develop DTRs that involve decision rules\nshared across multiple stages of intervention. We show that the existing\nQ-shared algorithm can suffer from non-convergence due to the use of linear\nmodels in the Q-learning setup, and identify the condition in which Q-shared\nfails. Leveraging properties from expansion-constrained ordinary least-squares,\nwe give a penalized Q-shared algorithm that not only converges in settings that\nviolate the condition, but can outperform the original Q-shared algorithm even\nwhen the condition is satisfied. We give evidence for the proposed method in a\nreal-world application and several synthetic simulations.",
          "link": "http://arxiv.org/abs/2107.07875",
          "publishedOn": "2021-07-19T00:49:07.126Z",
          "wordCount": 561,
          "title": "A Penalized Shared-parameter Algorithm for Estimating Optimal Dynamic Treatment Regimens. (arXiv:2107.07875v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07853",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1\">Gunnar K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1\">Timo Freiesleben</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1\">Moritz Grosse-Wentrup</a>",
          "description": "Algorithmic recourse explanations inform stakeholders on how to act to revert\nunfavorable predictions. However, in general ML models do not predict well in\ninterventional distributions. Thus, an action that changes the prediction in\nthe desired way may not lead to an improvement of the underlying target. Such\nrecourse is neither meaningful nor robust to model refits. Extending the work\nof Karimi et al. (2021), we propose meaningful algorithmic recourse (MAR) that\nonly recommends actions that improve both prediction and target. We justify\nthis selection constraint by highlighting the differences between model audit\nand meaningful, actionable recourse explanations. Additionally, we introduce a\nrelaxation of MAR called effective algorithmic recourse (EAR), which, under\ncertain assumptions, yields meaningful recourse by only allowing interventions\non causes of the target.",
          "link": "http://arxiv.org/abs/2107.07853",
          "publishedOn": "2021-07-19T00:49:07.073Z",
          "wordCount": 570,
          "title": "A Causal Perspective on Meaningful and Robust Algorithmic Recourse. (arXiv:2107.07853v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Puck de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Sindy L&#xf6;we</a>",
          "description": "Reliable detection of anomalies is crucial when deploying machine learning\nmodels in practice, but remains challenging due to the lack of labeled data. To\ntackle this challenge, contrastive learning approaches are becoming\nincreasingly popular, given the impressive results they have achieved in\nself-supervised representation learning settings. However, while most existing\ncontrastive anomaly detection and segmentation approaches have been applied to\nimages, none of them can use the contrastive losses directly for both anomaly\ndetection and segmentation. In this paper, we close this gap by making use of\nthe Contrastive Predictive Coding model (arXiv:1807.03748). We show that its\npatch-wise contrastive loss can directly be interpreted as an anomaly score,\nand how this allows for the creation of anomaly segmentation masks. The\nresulting model achieves promising results for both anomaly detection and\nsegmentation on the challenging MVTec-AD dataset.",
          "link": "http://arxiv.org/abs/2107.07820",
          "publishedOn": "2021-07-19T00:49:07.065Z",
          "wordCount": 583,
          "title": "Contrastive Predictive Coding for Anomaly Detection. (arXiv:2107.07820v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-07-19T00:49:07.046Z",
          "wordCount": 587,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dunjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zhaoyan Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Mingwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>",
          "description": "Graph classification plays a significant role in network analysis. It also\nfaces potential security threat like adversarial attacks. Some defense methods\nmay sacrifice algorithm complexity for robustness like adversarial training,\nwhile others may sacrifice the clean example performance such as\nsmoothing-based defense. Most of them are suffered from high-complexity or less\ntransferability. To address this problem, we proposed EGC$^2$, an enhanced\ngraph classification model with easy graph compression. EGC$^2$ captures the\nrelationship between features of different nodes by constructing feature graphs\nand improving aggregate node-level representation. To achieve lower complexity\ndefense applied to various graph classification models, EGC$^2$ utilizes a\ncentrality-based edge importance index to compress graphs, filtering out\ntrivial structures and even adversarial perturbations of the input graphs, thus\nimproves its robustness. Experiments on seven benchmark datasets demonstrate\nthat the proposed feature read-out and graph compression mechanisms enhance the\nrobustness of various basic models, thus achieving the state-of-the-art\nperformance of accuracy and robustness in the threat of different adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2107.07737",
          "publishedOn": "2021-07-19T00:49:07.040Z",
          "wordCount": 600,
          "title": "EGC2: Enhanced Graph Classification with Easy Graph Compression. (arXiv:2107.07737v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Shivshankar Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1\">Anand Vir Singh Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Karamjit Singh</a>",
          "description": "Temporal Point Processes (TPPs) are often used to represent the sequence of\nevents ordered as per the time of occurrence. Owing to their flexible nature,\nTPPs have been used to model different scenarios and have shown applicability\nin various real-world applications. While TPPs focus on modeling the event\noccurrence, Marked Temporal Point Process (MTPP) focuses on modeling the\ncategory/class of the event as well (termed as the marker). Research in MTPP\nhas garnered substantial attention over the past few years, with an extensive\nfocus on supervised algorithms. Despite the research focus, limited attention\nhas been given to the challenging problem of developing solutions in\nsemi-supervised settings, where algorithms have access to a mix of labeled and\nunlabeled data. This research proposes a novel algorithm for Semi-supervised\nLearning for Marked Temporal Point Processes (SSL-MTPP) applicable in such\nscenarios. The proposed SSL-MTPP algorithm utilizes a combination of labeled\nand unlabeled data for learning a robust marker prediction model. The proposed\nalgorithm utilizes an RNN-based Encoder-Decoder module for learning effective\nrepresentations of the time sequence. The efficacy of the proposed algorithm\nhas been demonstrated via multiple protocols on the Retweet dataset, where the\nproposed SSL-MTPP demonstrates improved performance in comparison to the\ntraditional supervised learning approach.",
          "link": "http://arxiv.org/abs/2107.07729",
          "publishedOn": "2021-07-19T00:49:07.033Z",
          "wordCount": 637,
          "title": "Semi-supervised Learning for Marked Temporal Point Processes. (arXiv:2107.07729v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henkel_C/0/1/0/all/0/1\">Christof Henkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1\">Pascal Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_P/0/1/0/all/0/1\">Philipp Singer</a>",
          "description": "We present a robust classification approach for avian vocalization in complex\nand diverse soundscapes, achieving second place in the BirdCLEF2021 challenge.\nWe illustrate how to make full use of pre-trained convolutional neural\nnetworks, by using an efficient modeling and training routine supplemented by\nnovel augmentation methods. Thereby, we improve the generalization of weakly\nlabeled crowd-sourced data to productive data collected by autonomous recording\nunits. As such, we illustrate how to progress towards an accurate automated\nassessment of avian population which would enable global biodiversity\nmonitoring at scale, impossible by manual annotation.",
          "link": "http://arxiv.org/abs/2107.07728",
          "publishedOn": "2021-07-19T00:49:07.026Z",
          "wordCount": 541,
          "title": "Recognizing bird species in diverse soundscapes under weak supervision. (arXiv:2107.07728v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qin Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
          "link": "http://arxiv.org/abs/2107.07705",
          "publishedOn": "2021-07-19T00:49:07.019Z",
          "wordCount": 491,
          "title": "Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Arnab Kumar Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asnani_H/0/1/0/all/0/1\">Himanshu Asnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>",
          "description": "Clustering single-cell RNA sequence (scRNA-seq) data poses statistical and\ncomputational challenges due to their high-dimensionality and data-sparsity,\nalso known as `dropout' events. Recently, Regularized Auto-Encoder (RAE) based\ndeep neural network models have achieved remarkable success in learning robust\nlow-dimensional representations. The basic idea in RAEs is to learn a\nnon-linear mapping from the high-dimensional data space to a low-dimensional\nlatent space and vice-versa, simultaneously imposing a distributional prior on\nthe latent space, which brings in a regularization effect. This paper argues\nthat RAEs suffer from the infamous problem of bias-variance trade-off in their\nnaive formulation. While a simple AE without a latent regularization results in\ndata over-fitting, a very strong prior leads to under-representation and thus\nbad clustering. To address the above issues, we propose a modified RAE\nframework (called the scRAE) for effective clustering of the single-cell RNA\nsequencing data. scRAE consists of deterministic AE with a flexibly learnable\nprior generator network, which is jointly trained with the AE. This facilitates\nscRAE to trade-off better between the bias and variance in the latent space. We\ndemonstrate the efficacy of the proposed method through extensive\nexperimentation on several real-world single-cell Gene expression datasets.",
          "link": "http://arxiv.org/abs/2107.07709",
          "publishedOn": "2021-07-19T00:49:07.002Z",
          "wordCount": 642,
          "title": "ScRAE: Deterministic Regularized Autoencoders with Flexible Priors for Clustering Single-cell Gene Expression Data. (arXiv:2107.07709v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rushil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Vishal Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_Y/0/1/0/all/0/1\">Yash Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yitao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>",
          "description": "We focus on the task of future frame prediction in video governed by\nunderlying physical dynamics. We work with models which are object-centric,\ni.e., explicitly work with object representations, and propagate a loss in the\nlatent space. Specifically, our research builds on recent work by Kipf et al.\n\\cite{kipf&al20}, which predicts the next state via contrastive learning of\nobject interactions in a latent space using a Graph Neural Network. We argue\nthat injecting explicit inductive bias in the model, in form of general\nphysical laws, can help not only make the model more interpretable, but also\nimprove the overall prediction of model. As a natural by-product, our model can\nlearn feature maps which closely resemble actual object positions in the image,\nwithout having any explicit supervision about the object positions at the\ntraining time. In comparison with earlier works \\cite{jaques&al20}, which\nassume a complete knowledge of the dynamics governing the motion in the form of\na physics engine, we rely only on the knowledge of general physical laws, such\nas, world consists of objects, which have position and velocity. We propose an\nadditional decoder based loss in the pixel space, imposed in a curriculum\nmanner, to further refine the latent space predictions. Experiments in multiple\ndifferent settings demonstrate that while Kipf et al. model is effective at\ncapturing object interactions, our model can be significantly more effective at\nlocalising objects, resulting in improved performance in 3 out of 4 domains\nthat we experiment with. Additionally, our model can learn highly intrepretable\nfeature maps, resembling actual object positions.",
          "link": "http://arxiv.org/abs/2107.07713",
          "publishedOn": "2021-07-19T00:49:06.996Z",
          "wordCount": 709,
          "title": "Towards an Interpretable Latent Space in Structured Models for Video Prediction. (arXiv:2107.07713v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07732",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1\">Xinyi Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ghai_U/0/1/0/all/0/1\">Udaya Ghai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hazan_E/0/1/0/all/0/1\">Elad Hazan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Megretski_A/0/1/0/all/0/1\">Alexandre Megretski</a>",
          "description": "We study online control of an unknown nonlinear dynamical system that is\napproximated by a time-invariant linear system with model misspecification. Our\nstudy focuses on robustness, which measures how much deviation from the assumed\nlinear approximation can be tolerated while maintaining a bounded $\\ell_2$-gain\ncompared to the optimal control in hindsight. Some models cannot be stabilized\neven with perfect knowledge of their coefficients: the robustness is limited by\nthe minimal distance between the assumed dynamics and the set of unstabilizable\ndynamics. Therefore it is necessary to assume a lower bound on this distance.\nUnder this assumption, and with full observation of the $d$ dimensional state,\nwe describe an efficient controller that attains $\\Omega(\\frac{1}{\\sqrt{d}})$\nrobustness together with an $\\ell_2$-gain whose dimension dependence is near\noptimal. We also give an inefficient algorithm that attains constant robustness\nindependent of the dimension, with a finite but sub-optimal $\\ell_2$-gain.",
          "link": "http://arxiv.org/abs/2107.07732",
          "publishedOn": "2021-07-19T00:49:06.989Z",
          "wordCount": 576,
          "title": "Robust Online Control with Model Misspecification. (arXiv:2107.07732v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07752",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cognolato_F/0/1/0/all/0/1\">Francesco Cognolato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OBrien_K/0/1/0/all/0/1\">Kieran O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jin Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_S/0/1/0/all/0/1\">Simon Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laun_F/0/1/0/all/0/1\">Frederik B. Laun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barth_M/0/1/0/all/0/1\">Markus Barth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollmann_S/0/1/0/all/0/1\">Steffen Bollmann</a>",
          "description": "Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great\npotential in recent years, outperforming traditional non-learning approaches in\nspeed and accuracy. However, many of the current deep learning approaches are\nnot data consistent, require in vivo training data or do not solve all steps of\nthe QSM processing pipeline. Here we aim to overcome these limitations and\ndeveloped a framework to solve the QSM processing steps jointly. We developed a\nnew hybrid training data generation method that enables the end-to-end training\nfor solving background field correction and dipole inversion in a\ndata-consistent fashion using a variational network that combines the QSM model\nterm and a learned regularizer. We demonstrate that NeXtQSM overcomes the\nlimitations of previous model-agnostic deep learning methods and show that\nNeXtQSM offers a complete deep learning based pipeline for computing robust,\nfast and accurate quantitative susceptibility maps.",
          "link": "http://arxiv.org/abs/2107.07752",
          "publishedOn": "2021-07-19T00:49:06.983Z",
          "wordCount": 613,
          "title": "NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07788",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhong-Ping Jiang</a>",
          "description": "This paper studies the optimal stationary control of continuous-time linear\nstochastic systems with both additive and multiplicative noises, using\nreinforcement learning techniques. Based on policy iteration, a novel\noff-policy reinforcement learning algorithm, named optimistic\nleast-squares-based policy iteration, is proposed which is able to iteratively\nfind near-optimal policies of the optimal stationary control problem directly\nfrom input/state data without explicitly identifying any system matrices,\nstarting from an initial admissible control policy. The solutions given by the\nproposed optimistic least-squares-based policy iteration are proved to converge\nto a small neighborhood of the optimal solution with probability one, under\nmild conditions. The application of the proposed algorithm to a triple inverted\npendulum example validates its feasibility and effectiveness.",
          "link": "http://arxiv.org/abs/2107.07788",
          "publishedOn": "2021-07-19T00:49:06.976Z",
          "wordCount": 567,
          "title": "Reinforcement Learning for Optimal Stationary Control of Linear Stochastic Systems. (arXiv:2107.07788v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07582",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Mamandipoor_B/0/1/0/all/0/1\">Behrooz Mamandipoor</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yeung_W/0/1/0/all/0/1\">Wesley Yeung</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Agha_Mir_Salim_L/0/1/0/all/0/1\">Louis Agha-Mir-Salim</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stone_D/0/1/0/all/0/1\">David J. Stone</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Osmani_V/0/1/0/all/0/1\">Venet Osmani</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Celi_L/0/1/0/all/0/1\">Leo Anthony Celi</a>",
          "description": "Purpose. Elevations in initially obtained serum lactate levels are strong\npredictors of mortality in critically ill patients. Identifying patients whose\nserum lactate levels are more likely to increase can alert physicians to\nintensify care and guide them in the frequency of tending the blood test. We\ninvestigate whether machine learning models can predict subsequent serum\nlactate changes.\n\nMethods. We investigated serum lactate change prediction using the MIMIC-III\nand eICU-CRD datasets in internal as well as external validation of the eICU\ncohort on the MIMIC-III cohort. Three subgroups were defined based on the\ninitial lactate levels: i) normal group (<2 mmol/L), ii) mild group (2-4\nmmol/L), and iii) severe group (>4 mmol/L). Outcomes were defined based on\nincrease or decrease of serum lactate levels between the groups. We also\nperformed sensitivity analysis by defining the outcome as lactate change of\n>10% and furthermore investigated the influence of the time interval between\nsubsequent lactate measurements on predictive performance.\n\nResults. The LSTM models were able to predict deterioration of serum lactate\nvalues of MIMIC-III patients with an AUC of 0.77 (95% CI 0.762-0.771) for the\nnormal group, 0.77 (95% CI 0.768-0.772) for the mild group, and 0.85 (95% CI\n0.840-0.851) for the severe group, with a slightly lower performance in the\nexternal validation.\n\nConclusion. The LSTM demonstrated good discrimination of patients who had\ndeterioration in serum lactate levels. Clinical studies are needed to evaluate\nwhether utilization of a clinical decision support tool based on these results\ncould positively impact decision-making and patient outcomes.",
          "link": "http://arxiv.org/abs/2107.07582",
          "publishedOn": "2021-07-19T00:49:06.959Z",
          "wordCount": 727,
          "title": "Prediction of Blood Lactate Values in Critically Ill Patients: A Retrospective Multi-center Cohort Study. (arXiv:2107.07582v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1\">Christopher T.H Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1\">Ngai-Man Cheung</a>",
          "description": "Deep generative models have made much progress in improving training\nstability and quality of generated data. Recently there has been increased\ninterest in the fairness of deep-generated data. Fairness is important in many\napplications, e.g. law enforcement, as biases will affect efficacy. Central to\nfair data generation are the fairness metrics for the assessment and evaluation\nof different generative models. In this paper, we first review fairness metrics\nproposed in previous works and highlight potential weaknesses. We then discuss\na performance benchmark framework along with the assessment of alternative\nmetrics.",
          "link": "http://arxiv.org/abs/2107.07754",
          "publishedOn": "2021-07-19T00:49:06.952Z",
          "wordCount": 525,
          "title": "Measuring Fairness in Generative Models. (arXiv:2107.07754v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1\">Toshinori Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takamitsu Matsubara</a>",
          "description": "The recent booming of entropy-regularized literature reveals that\nKullback-Leibler (KL) regularization brings advantages to Reinforcement\nLearning (RL) algorithms by canceling out errors under mild assumptions.\nHowever, existing analyses focus on fixed regularization with a constant\nweighting coefficient and have not considered the case where the coefficient is\nallowed to change dynamically. In this paper, we study the dynamic coefficient\nscheme and present the first asymptotic error bound. Based on the dynamic\ncoefficient error bound, we propose an effective scheme to tune the coefficient\naccording to the magnitude of error in favor of more robust learning. On top of\nthis development, we propose a novel algorithm: Geometric Value Iteration (GVI)\nthat features a dynamic error-aware KL coefficient design aiming to mitigate\nthe impact of errors on the performance. Our experiments demonstrate that GVI\ncan effectively exploit the trade-off between learning speed and robustness\nover uniform averaging of constant KL coefficient. The combination of GVI and\ndeep networks shows stable learning behavior even in the absence of a target\nnetwork where algorithms with a constant KL coefficient would greatly oscillate\nor even fail to converge.",
          "link": "http://arxiv.org/abs/2107.07659",
          "publishedOn": "2021-07-19T00:49:06.946Z",
          "wordCount": 616,
          "title": "Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning. (arXiv:2107.07659v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barata_R/0/1/0/all/0/1\">Ricardo Barata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leite_M/0/1/0/all/0/1\">Miguel Leite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_R/0/1/0/all/0/1\">Ricardo Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampaio_M/0/1/0/all/0/1\">Marco O. P. Sampaio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ascensao_J/0/1/0/all/0/1\">Jo&#xe3;o Tiago Ascens&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1\">Pedro Bizarro</a>",
          "description": "Labeled data is essential in modern systems that rely on Machine Learning\n(ML) for predictive modelling. Such systems may suffer from the cold-start\nproblem: supervised models work well but, initially, there are no labels, which\nare costly or slow to obtain. This problem is even worse in imbalanced data\nscenarios. Online financial fraud detection is an example where labeling is: i)\nexpensive, or ii) it suffers from long delays, if relying on victims filing\ncomplaints. The latter may not be viable if a model has to be in place\nimmediately, so an option is to ask analysts to label events while minimizing\nthe number of annotations to control costs. We propose an Active Learning (AL)\nannotation system for datasets with orders of magnitude of class imbalance, in\na cold start streaming scenario. We present a computationally efficient\nOutlier-based Discriminative AL approach (ODAL) and design a novel 3-stage\nsequence of AL labeling policies where it is used as warm-up. Then, we perform\nempirical studies in four real world datasets, with various magnitudes of class\nimbalance. The results show that our method can more quickly reach a high\nperformance model than standard AL policies. Its observed gains over random\nsampling can reach 80% and be competitive with policies with an unlimited\nannotation budget or additional historical data (with 1/10 to 1/50 of the\nlabels).",
          "link": "http://arxiv.org/abs/2107.07724",
          "publishedOn": "2021-07-19T00:49:06.939Z",
          "wordCount": 678,
          "title": "Active learning for online training in imbalanced data streams under cold start. (arXiv:2107.07724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhunan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Cunhang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huiguang He</a>",
          "description": "As an essential element for the diagnosis and rehabilitation of psychiatric\ndisorders, the electroencephalogram (EEG) based emotion recognition has\nachieved significant progress due to its high precision and reliability.\nHowever, one obstacle to practicality lies in the variability between subjects\nand sessions. Although several studies have adopted domain adaptation (DA)\napproaches to tackle this problem, most of them treat multiple EEG data from\ndifferent subjects and sessions together as a single source domain for\ntransfer, which either fails to satisfy the assumption of domain adaptation\nthat the source has a certain marginal distribution, or increases the\ndifficulty of adaptation. We therefore propose the multi-source marginal\ndistribution adaptation (MS-MDA) for EEG emotion recognition, which takes both\ndomain-invariant and domain-specific features into consideration. First, we\nassume that different EEG data share the same low-level features, then we\nconstruct independent branches for multiple EEG data source domains to adopt\none-to-one domain adaptation and extract domain-specific features. Finally, the\ninference is made by multiple branches. We evaluate our method on SEED and\nSEED-IV for recognizing three and four emotions, respectively. Experimental\nresults show that the MS-MDA outperforms the comparison methods and\nstate-of-the-art models in cross-session and cross-subject transfer scenarios\nin our settings. Codes at https://github.com/VoiceBeer/MS-MDA.",
          "link": "http://arxiv.org/abs/2107.07740",
          "publishedOn": "2021-07-19T00:49:06.933Z",
          "wordCount": 657,
          "title": "MS-MDA: Multisource Marginal Distribution Adaptation for Cross-subject and Cross-session EEG Emotion Recognition. (arXiv:2107.07740v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Niel Teng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yosinski_J/0/1/0/all/0/1\">Jason Yosinski</a>",
          "description": "Not all examples are created equal, but standard deep neural network training\nprotocols treat each training point uniformly. Each example is propagated\nforward and backward through the network the same amount of times, independent\nof how much the example contributes to the learning protocol. Recent work has\nproposed ways to accelerate training by deviating from this uniform treatment.\nPopular methods entail up-weighting examples that contribute more to the loss\nwith the intuition that examples with low loss have already been learned by the\nmodel, so their marginal value to the training procedure should be lower. This\nview assumes that updating the model with high loss examples will be beneficial\nto the model. However, this may not hold for noisy, real world data. In this\npaper, we theorize and then empirically demonstrate that loss-based\nacceleration methods degrade in scenarios with noisy and corrupted data. Our\nwork suggests measures of example difficulty need to correctly separate out\nnoise from other types of challenging examples.",
          "link": "http://arxiv.org/abs/2107.07741",
          "publishedOn": "2021-07-19T00:49:06.895Z",
          "wordCount": 589,
          "title": "When does loss-based prioritization fail?. (arXiv:2107.07741v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>",
          "description": "Scenario generation is a fundamental and crucial tool for decision-making in\npower systems with high-penetration renewables. Based on big historical data, a\nnovel federated deep generative learning framework, called Fed-LSGAN, is\nproposed by integrating federated learning and least square generative\nadversarial networks (LSGANs) for renewable scenario generation. Specifically,\nfederated learning learns a shared global model in a central server from\nrenewable sites at network edges, which enables the Fed-LSGAN to generate\nscenarios in a privacy-preserving manner without sacrificing the generation\nquality by transferring model parameters, rather than all data. Meanwhile, the\nLSGANs-based deep generative model generates scenarios that conform to the\ndistribution of historical data through fully capturing the spatial-temporal\ncharacteristics of renewable powers, which leverages the least squares loss\nfunction to improve the training stability and generation quality. The\nsimulation results demonstrate that the proposal manages to generate\nhigh-quality renewable scenarios and outperforms the state-of-the-art\ncentralized methods. Besides, an experiment with different federated learning\nsettings is designed and conducted to verify the robustness of our method.",
          "link": "http://arxiv.org/abs/2107.07738",
          "publishedOn": "2021-07-19T00:49:06.886Z",
          "wordCount": 626,
          "title": "Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach. (arXiv:2107.07738v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07687",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yuming Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sanz_Alonso_D/0/1/0/all/0/1\">Daniel Sanz-Alonso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1\">Rebecca Willett</a>",
          "description": "Data assimilation is concerned with sequentially estimating a\ntemporally-evolving state. This task, which arises in a wide range of\nscientific and engineering applications, is particularly challenging when the\nstate is high-dimensional and the state-space dynamics are unknown. This paper\nintroduces a machine learning framework for learning dynamical systems in data\nassimilation. Our auto-differentiable ensemble Kalman filters (AD-EnKFs) blend\nensemble Kalman filters for state recovery with machine learning tools for\nlearning the dynamics. In doing so, AD-EnKFs leverage the ability of ensemble\nKalman filters to scale to high-dimensional states and the power of automatic\ndifferentiation to train high-dimensional surrogate models for the dynamics.\nNumerical results using the Lorenz-96 model show that AD-EnKFs outperform\nexisting methods that use expectation-maximization or particle filters to merge\ndata assimilation and machine learning. In addition, AD-EnKFs are easy to\nimplement and require minimal tuning.",
          "link": "http://arxiv.org/abs/2107.07687",
          "publishedOn": "2021-07-19T00:49:06.870Z",
          "wordCount": 564,
          "title": "Auto-differentiable Ensemble Kalman Filters. (arXiv:2107.07687v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sanjoy Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navlakha_S/0/1/0/all/0/1\">Saket Navlakha</a>",
          "description": "Continual learning in computational systems is challenging due to\ncatastrophic forgetting. We discovered a two layer neural circuit in the fruit\nfly olfactory system that addresses this challenge by uniquely combining sparse\ncoding and associative learning. In the first layer, odors are encoded using\nsparse, high dimensional representations, which reduces memory interference by\nactivating non overlapping populations of neurons for different odors. In the\nsecond layer, only the synapses between odor activated neurons and the output\nneuron associated with the odor are modified during learning; the rest of the\nweights are frozen to prevent unrelated memories from being overwritten. We\nshow empirically and analytically that this simple and lightweight algorithm\nsignificantly boosts continual learning performance. The fly associative\nlearning algorithm is strikingly similar to the classic perceptron learning\nalgorithm, albeit two modifications, which we show are critical for reducing\ncatastrophic forgetting. Overall, fruit flies evolved an efficient lifelong\nlearning algorithm, and circuit mechanisms from neuroscience can be translated\nto improve machine computation.",
          "link": "http://arxiv.org/abs/2107.07617",
          "publishedOn": "2021-07-19T00:49:06.824Z",
          "wordCount": 595,
          "title": "Algorithmic insights on continual learning from fruit flies. (arXiv:2107.07617v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Khondker Fariha Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sharif Amit Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavakkoli_A/0/1/0/all/0/1\">Alireza Tavakkoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Daniel Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasegarar_S/0/1/0/all/0/1\">Sutharshan Rajasegarar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmaker_C/0/1/0/all/0/1\">Chandan Karmaker</a>",
          "description": "Electrocardiogram (ECG) acquisition requires an automated system and analysis\npipeline for understanding specific rhythm irregularities. Deep neural networks\nhave become a popular technique for tracing ECG signals, outperforming human\nexperts. Despite this, convolutional neural networks are susceptible to\nadversarial examples that can misclassify ECG signals and decrease the model's\nprecision. Moreover, they do not generalize well on the out-of-distribution\ndataset. The GAN architecture has been employed in recent works to synthesize\nadversarial ECG signals to increase existing training data. However, they use a\ndisjointed CNN-based classification architecture to detect arrhythmia. Till\nnow, no versatile architecture has been proposed that can detect adversarial\nexamples and classify arrhythmia simultaneously. To alleviate this, we propose\na novel Conditional Generative Adversarial Network to simultaneously generate\nECG signals for different categories and detect cardiac abnormalities.\nMoreover, the model is conditioned on class-specific ECG signals to synthesize\nrealistic adversarial examples. Consequently, we compare our architecture and\nshow how it outperforms other classification models in normal/abnormal ECG\nsignal detection by benchmarking real world and adversarial signals.",
          "link": "http://arxiv.org/abs/2107.07677",
          "publishedOn": "2021-07-19T00:49:06.794Z",
          "wordCount": 618,
          "title": "ECG-Adv-GAN: Detecting ECG Adversarial Examples with Conditional Generative Adversarial Networks. (arXiv:2107.07677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdous_R/0/1/0/all/0/1\">Refat E Ferdous</a>",
          "description": "During the COVID-19 pandemic, most of the human-to-human interactions have\nbeen stopped. To mitigate the spread of deadly coronavirus, many offices took\nthe initiative so that the employees can work from home. But, tracking the\nemployees and finding out if they are really performing what they were supposed\nto turn out to be a serious challenge for all the companies and organizations\nwho are facilitating \"Work From Home\". To deal with the challenge effectively,\nwe came up with a solution to track the employees with face recognition. We\nhave been testing this system experimentally for our office. To train the face\nrecognition module, we used FaceNet with KNN using the Labeled Faces in the\nWild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model\ninto our central system, where the employees log their time. In this paper, we\ndiscuss in brief the system we have been experimenting with and the pros and\ncons of the system.",
          "link": "http://arxiv.org/abs/2107.07576",
          "publishedOn": "2021-07-19T00:49:06.772Z",
          "wordCount": 666,
          "title": "Real-Time Face Recognition System for Remote Employee Tracking. (arXiv:2107.07576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carmona_C/0/1/0/all/0/1\">Chris U. Carmona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubet_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Aubet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flunkert_V/0/1/0/all/0/1\">Valentin Flunkert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasthaus_J/0/1/0/all/0/1\">Jan Gasthaus</a>",
          "description": "We introduce Neural Contextual Anomaly Detection (NCAD), a framework for\nanomaly detection on time series that scales seamlessly from the unsupervised\nto supervised setting, and is applicable to both univariate and multivariate\ntime series. This is achieved by effectively combining recent developments in\nrepresentation learning for multivariate time series, with techniques for deep\nanomaly detection originally developed for computer vision that we tailor to\nthe time series setting. Our window-based approach facilitates learning the\nboundary between normal and anomalous classes by injecting generic synthetic\nanomalies into the available data. Moreover, our method can effectively take\nadvantage of all the available information, be it as domain knowledge, or as\ntraining labels in the semi-supervised setting. We demonstrate empirically on\nstandard benchmark datasets that our approach obtains a state-of-the-art\nperformance in these settings.",
          "link": "http://arxiv.org/abs/2107.07702",
          "publishedOn": "2021-07-19T00:49:06.765Z",
          "wordCount": 570,
          "title": "Neural Contextual Anomaly Detection for Time Series. (arXiv:2107.07702v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07642",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Lohani_S/0/1/0/all/0/1\">Sanjaya Lohani</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lukens_J/0/1/0/all/0/1\">Joseph M. Lukens</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jones_D/0/1/0/all/0/1\">Daniel E. Jones</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Searles_T/0/1/0/all/0/1\">Thomas A. Searles</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Glasser_R/0/1/0/all/0/1\">Ryan T. Glasser</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kirby_B/0/1/0/all/0/1\">Brian T. Kirby</a>",
          "description": "We consider the properties of a specific distribution of mixed quantum states\nof arbitrary dimension that can be biased towards a specific mean purity. In\nparticular, we analyze mixtures of Haar-random pure states with\nDirichlet-distributed coefficients. We analytically derive the concentration\nparameters required to match the mean purity of the Bures and Hilbert--Schmidt\ndistributions in any dimension. Numerical simulations suggest that this value\nrecovers the Hilbert--Schmidt distribution exactly, offering an alternative and\nintuitive physical interpretation for ensembles of Hilbert--Schmidt-distributed\nrandom quantum states. We then demonstrate how substituting these\nDirichlet-weighted Haar mixtures in place of the Bures and Hilbert--Schmidt\ndistributions results in measurable performance advantages in\nmachine-learning-based quantum state tomography systems and Bayesian quantum\nstate reconstruction. Finally, we experimentally characterize the distribution\nof quantum states generated by both a cloud-accessed IBM quantum computer and\nan in-house source of polarization-entangled photons. In each case, our method\ncan more closely match the underlying distribution than either Bures or\nHilbert--Schmidt distributed states for various experimental conditions.",
          "link": "http://arxiv.org/abs/2107.07642",
          "publishedOn": "2021-07-19T00:49:06.759Z",
          "wordCount": 611,
          "title": "Improving application performance with biased distributions of quantum states. (arXiv:2107.07642v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magee_L/0/1/0/all/0/1\">Liam Magee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahremanlou_L/0/1/0/all/0/1\">Lida Ghahremanlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldatic_K/0/1/0/all/0/1\">Karen Soldatic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1\">Shanthi Robertson</a>",
          "description": "To examine whether intersectional bias can be observed in language\ngeneration, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size\nfrom 124 million to ~2.7 billion parameters. We conduct an experiment combining\nup to three social categories - gender, religion and disability - into\nunconditional or zero-shot prompts used to generate sentences that are then\nanalysed for sentiment. Our results confirm earlier tests conducted with\nauto-regressive causal models, including the \\emph{GPT} family of models. We\nalso illustrate why bias may be resistant to techniques that target single\ncategories (e.g. gender, religion and race), as it can also manifest, in often\nsubtle ways, in texts prompted by concatenated social categories. To address\nthese difficulties, we suggest technical and community-based approaches need to\ncombine to acknowledge and address complex and intersectional language model\nbias.",
          "link": "http://arxiv.org/abs/2107.07691",
          "publishedOn": "2021-07-19T00:49:06.746Z",
          "wordCount": 568,
          "title": "Intersectional Bias in Causal Language Models. (arXiv:2107.07691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07603",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Scurll_J/0/1/0/all/0/1\">Joshua M. Scurll</a>",
          "description": "Clustering and visualizing high-dimensional (HD) data are important tasks in\na variety of fields. For example, in bioinformatics, they are crucial for\nanalyses of single-cell data such as mass cytometry (CyTOF) data. Some of the\nmost effective algorithms for clustering HD data are based on representing the\ndata by nodes in a graph, with edges connecting neighbouring nodes according to\nsome measure of similarity or distance. However, users of graph-based\nalgorithms are typically faced with the critical but challenging task of\nchoosing the value of an input parameter that sets the size of neighbourhoods\nin the graph, e.g. the number of nearest neighbours to which to connect each\nnode or a threshold distance for connecting nodes. The burden on the user could\nbe alleviated by a measure of inter-node similarity that can have value 0 for\ndissimilar nodes without requiring any user-defined parameters or thresholds.\nThis would determine the neighbourhoods automatically while still yielding a\nsparse graph. To this end, I propose a new method called ASTRICS to measure\nsimilarity between clusters of HD data points based on local dimensionality\nreduction and triangulation of critical alpha shapes. I show that my ASTRICS\nsimilarity measure can facilitate both clustering and visualization of HD data\nby using it in Stage 2 of a three-stage pipeline: Stage 1 = perform an initial\nclustering of the data by any method; Stage 2 = let graph nodes represent\ninitial clusters instead of individual data points and use ASTRICS to\nautomatically define edges between nodes; Stage 3 = use the graph for further\nclustering and visualization. This trades the critical task of choosing a graph\nneighbourhood size for the easier task of essentially choosing a resolution at\nwhich to view the data. The graph and consequently downstream clustering and\nvisualization are then automatically adapted to the chosen resolution.",
          "link": "http://arxiv.org/abs/2107.07603",
          "publishedOn": "2021-07-19T00:49:06.719Z",
          "wordCount": 764,
          "title": "Measuring inter-cluster similarities with Alpha Shape TRIangulation in loCal Subspaces (ASTRICS) facilitates visualization and clustering of high-dimensional data. (arXiv:2107.07603v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1\">Daniel D. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1\">Rianne van den Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1\">Daniel Tarlow</a>",
          "description": "Denoising diffusion probabilistic models (DDPMs) have shown impressive\nresults on sequence generation by iteratively corrupting each example and then\nlearning to map corrupted versions back to the original. However, previous work\nhas largely focused on in-place corruption, adding noise to each pixel or token\nindividually while keeping their locations the same. In this work, we consider\na broader class of corruption processes and denoising models over sequence data\nthat can insert and delete elements, while still being efficient to train and\nsample from. We demonstrate that these models outperform standard in-place\nmodels on an arithmetic sequence task, and that when trained on the text8\ndataset they can be used to fix spelling errors without any fine-tuning.",
          "link": "http://arxiv.org/abs/2107.07675",
          "publishedOn": "2021-07-19T00:49:06.696Z",
          "wordCount": 572,
          "title": "Beyond In-Place Corruption: Insertion and Deletion In Denoising Probabilistic Models. (arXiv:2107.07675v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yukun Jiang</a>",
          "description": "Active learning is a state-of-art machine learning approach to deal with an\nabundance of unlabeled data. In the field of Natural Language Processing,\ntypically it is costly and time-consuming to have all the data annotated. This\ninefficiency inspires out our application of active learning in text\nclassification. Traditional unsupervised k-means clustering is first modified\ninto a semi-supervised version in this research. Then, a novel attempt is\napplied to further extend the algorithm into active learning scenario with\nPenalized Min-Max-selection, so as to make limited queries that yield more\nstable initial centroids. This method utilizes both the interactive query\nresults from users and the underlying distance representation. After tested on\na Chinese news dataset, it shows a consistent increase in accuracy while\nlowering the cost in training.",
          "link": "http://arxiv.org/abs/2107.07682",
          "publishedOn": "2021-07-19T00:49:06.689Z",
          "wordCount": 600,
          "title": "The Application of Active Query K-Means in Text Classification. (arXiv:2107.07682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alarab_I/0/1/0/all/0/1\">Ismail Alarab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakoonwit_S/0/1/0/all/0/1\">Simant Prakoonwit</a>",
          "description": "We propose a novel method to capture data points near decision boundary in\nneural network that are often referred to a specific type of uncertainty. In\nour approach, we sought to perform uncertainty estimation based on the idea of\nadversarial attack method. In this paper, uncertainty estimates are derived\nfrom the input perturbations, unlike previous studies that provide\nperturbations on the model's parameters as in Bayesian approach. We are able to\nproduce uncertainty with couple of perturbations on the inputs. Interestingly,\nwe apply the proposed method to datasets derived from blockchain. We compare\nthe performance of model uncertainty with the most recent uncertainty methods.\nWe show that the proposed method has revealed a significant outperformance over\nother methods and provided less risk to capture model uncertainty in machine\nlearning.",
          "link": "http://arxiv.org/abs/2107.07618",
          "publishedOn": "2021-07-19T00:49:06.684Z",
          "wordCount": 576,
          "title": "Adversarial Attack for Uncertainty Estimation: Identifying Critical Regions in Neural Networks. (arXiv:2107.07618v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganassali_L/0/1/0/all/0/1\">Luca Ganassali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lelarge_M/0/1/0/all/0/1\">Marc Lelarge</a>",
          "description": "We consider alignment of sparse graphs, which consists in finding a mapping\nbetween the nodes of two graphs which preserves most of the edges. Our approach\nis to compare local structures in the two graphs, matching two nodes if their\nneighborhoods are 'close enough': for correlated Erd\\H{o}s-R\\'enyi random\ngraphs, this problem can be locally rephrased in terms of testing whether a\npair of branching trees is drawn from either a product distribution, or a\ncorrelated distribution. We design an optimal test for this problem which gives\nrise to a message-passing algorithm for graph alignment, which provably returns\nin polynomial time a positive fraction of correctly matched vertices, and a\nvanishing fraction of mismatches. With an average degree $\\lambda = O(1)$ in\nthe graphs, and a correlation parameter $s \\in [0,1]$, this result holds with\n$\\lambda s$ large enough, and $1-s$ small enough, completing the recent\nstate-of-the-art diagram. Tighter conditions for determining whether partial\ngraph alignment (or correlation detection in trees) is feasible in polynomial\ntime are given in terms of Kullback-Leibler divergences.",
          "link": "http://arxiv.org/abs/2107.07623",
          "publishedOn": "2021-07-19T00:49:06.669Z",
          "wordCount": 629,
          "title": "Correlation detection in trees for partial graph alignment. (arXiv:2107.07623v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopanicakova_A/0/1/0/all/0/1\">Alena Kopani&#x10d;&#xe1;kov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_R/0/1/0/all/0/1\">Rolf Krause</a>",
          "description": "We propose a globally convergent multilevel training method for deep residual\nnetworks (ResNets). The devised method can be seen as a novel variant of the\nrecursive multilevel trust-region (RMTR) method, which operates in hybrid\n(stochastic-deterministic) settings by adaptively adjusting mini-batch sizes\nduring the training. The multilevel hierarchy and the transfer operators are\nconstructed by exploiting a dynamical system's viewpoint, which interprets\nforward propagation through the ResNet as a forward Euler discretization of an\ninitial value problem. In contrast to traditional training approaches, our\nnovel RMTR method also incorporates curvature information on all levels of the\nmultilevel hierarchy by means of the limited-memory SR1 method. The overall\nperformance and the convergence properties of our multilevel training method\nare numerically investigated using examples from the field of classification\nand regression.",
          "link": "http://arxiv.org/abs/2107.07572",
          "publishedOn": "2021-07-19T00:49:06.659Z",
          "wordCount": 564,
          "title": "Globally Convergent Multilevel Training of Deep Residual Networks. (arXiv:2107.07572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitros_J/0/1/0/all/0/1\">John Mitros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>",
          "description": "Neural networks are often utilised in critical domain applications\n(e.g.~self-driving cars, financial markets, and aerospace engineering), even\nthough they exhibit overconfident predictions for ambiguous inputs. This\ndeficiency demonstrates a fundamental flaw indicating that neural networks\noften overfit on spurious correlations. To address this problem in this work we\npresent two novel objectives that improve the ability of a network to detect\nout-of-distribution samples and therefore avoid overconfident predictions for\nambiguous inputs. We empirically demonstrate that our methods outperform the\nbaseline and perform better than the majority of existing approaches, while\nperforming competitively those that they don't outperform. Additionally, we\nempirically demonstrate the robustness of our approach against common\ncorruptions and demonstrate the importance of regularisation and auxiliary\ninformation in out-of-distribution detection.",
          "link": "http://arxiv.org/abs/2107.07564",
          "publishedOn": "2021-07-19T00:49:06.653Z",
          "wordCount": 554,
          "title": "On the Importance of Regularisation & Auxiliary Information in OOD Detection. (arXiv:2107.07564v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1\">Ondrej Bohdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1\">Rajesh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyeji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>",
          "description": "Meta-learning provides a popular and effective family of methods for\ndata-efficient learning of new tasks. However, several important issues in\nmeta-learning have proven hard to study thus far. For example, performance\ndegrades in real-world settings where meta-learners must learn from a wide and\npotentially multi-modal distribution of training tasks; and when distribution\nshift exists between meta-train and meta-test task distributions. These issues\nare typically hard to study since the shape of task distributions, and shift\nbetween them are not straightforward to measure or control in standard\nbenchmarks. We propose the channel coding problem as a benchmark for\nmeta-learning. Channel coding is an important practical application where task\ndistributions naturally arise, and fast adaptation to new tasks is practically\nvaluable. We use this benchmark to study several aspects of meta-learning,\nincluding the impact of task distribution breadth and shift, which can be\ncontrolled in the coding problem. Going forward, this benchmark provides a tool\nfor the community to study the capabilities and limitations of meta-learning,\nand to drive research on practically robust and effective meta-learners.",
          "link": "http://arxiv.org/abs/2107.07579",
          "publishedOn": "2021-07-19T00:49:06.648Z",
          "wordCount": 616,
          "title": "A Channel Coding Benchmark for Meta-Learning. (arXiv:2107.07579v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-19T00:49:06.633Z",
          "wordCount": 664,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>",
          "description": "Gradient descent algorithm is the most utilized method when optimizing\nmachine learning issues. However, there exists many local minimums and saddle\npoints in the loss function, especially for high dimensional non-convex\noptimization problems like deep learning. Gradient descent may make loss\nfunction trapped in these local intervals which impedes further optimization,\nresulting in poor generalization ability. This paper proposes the SA-GD\nalgorithm which introduces the thought of simulated annealing algorithm to\ngradient descent. SA-GD method offers model the ability of mounting hills in\nprobability, tending to enable the model to jump out of these local areas and\nconverge to a optimal state finally. We took CNN models as an example and\ntested the basic CNN models on various benchmark datasets. Compared to the\nbaseline models with traditional gradient descent algorithm, models with SA-GD\nalgorithm possess better generalization ability without sacrificing the\nefficiency and stability of model convergence. In addition, SA-GD can be\nutilized as an effective ensemble learning approach which improves the final\nperformance significantly.",
          "link": "http://arxiv.org/abs/2107.07558",
          "publishedOn": "2021-07-19T00:49:06.611Z",
          "wordCount": 596,
          "title": "SA-GD: Improved Gradient Descent Learning Strategy with Simulated Annealing. (arXiv:2107.07558v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kevin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kai-Zhan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1\">Elias Bareinboim</a>",
          "description": "One of the central elements of any causal inference is an object called\nstructural causal model (SCM), which represents a collection of mechanisms and\nexogenous sources of random variation of the system under investigation (Pearl,\n2000). An important property of many kinds of neural networks is universal\napproximability: the ability to approximate any function to arbitrary\nprecision. Given this property, one may be tempted to surmise that a collection\nof neural nets is capable of learning any SCM by training on data generated by\nthat SCM. In this paper, we show this is not the case by disentangling the\nnotions of expressivity and learnability. Specifically, we show that the causal\nhierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits\nof what can be learned from data, still holds for neural models. For instance,\nan arbitrarily complex and expressive neural net is unable to predict the\neffects of interventions given observational data alone. Given this result, we\nintroduce a special type of SCM called a neural causal model (NCM), and\nformalize a new type of inductive bias to encode structural constraints\nnecessary for performing causal inferences. Building on this new class of\nmodels, we focus on solving two canonical tasks found in the literature known\nas causal identification and estimation. Leveraging the neural toolbox, we\ndevelop an algorithm that is both sufficient and necessary to determine whether\na causal effect can be learned from data (i.e., causal identifiability); it\nthen estimates the effect whenever identifiability holds (causal estimation).\nSimulations corroborate the proposed approach.",
          "link": "http://arxiv.org/abs/2107.00793",
          "publishedOn": "2021-07-16T00:48:26.348Z",
          "wordCount": 725,
          "title": "The Causal-Neural Connection: Expressiveness, Learnability, and Inference. (arXiv:2107.00793v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-07-16T00:48:26.334Z",
          "wordCount": 614,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1\">Mohammad Hossein Samavatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Saikat Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1\">Kristin Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1\">Radu Teodorescu</a>",
          "description": "Deep Neural Networks (DNNs) are employed in an increasing number of\napplications, some of which are safety critical. Unfortunately, DNNs are known\nto be vulnerable to so-called adversarial attacks that manipulate inputs to\ncause incorrect results that can be beneficial to an attacker or damaging to\nthe victim. Multiple defenses have been proposed to increase the robustness of\nDNNs. In general, these defenses have high overhead, some require\nattack-specific re-training of the model or careful tuning to adapt to\ndifferent attacks.\n\nThis paper presents HASI, a hardware-accelerated defense that uses a process\nwe call stochastic inference to detect adversarial inputs. We show that by\ncarefully injecting noise into the model at inference time, we can\ndifferentiate adversarial inputs from benign ones. HASI uses the output\ndistribution characteristics of noisy inference compared to a non-noisy\nreference to detect adversarial inputs. We show an adversarial detection rate\nof 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds\nthe detection rate of the state of the art approaches, with a much lower\noverhead. We demonstrate two software/hardware-accelerated co-designs, which\nreduces the performance impact of stochastic inference to 1.58X-2X relative to\nthe unprotected baseline, compared to 15X-20X overhead for a software-only GPU\nimplementation.",
          "link": "http://arxiv.org/abs/2106.05825",
          "publishedOn": "2021-07-16T00:48:26.309Z",
          "wordCount": 677,
          "title": "HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casado_F/0/1/0/all/0/1\">Fernando E. Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lema_D/0/1/0/all/0/1\">Dylan Lema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Criado_M/0/1/0/all/0/1\">Marcos F. Criado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_R/0/1/0/all/0/1\">Roberto Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regueiro_C/0/1/0/all/0/1\">Carlos V. Regueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barro_S/0/1/0/all/0/1\">Sen&#xe9;n Barro</a>",
          "description": "Smart devices, such as smartphones, wearables, robots, and others, can\ncollect vast amounts of data from their environment. This data is suitable for\ntraining machine learning models, which can significantly improve their\nbehavior, and therefore, the user experience. Federated learning is a young and\npopular framework that allows multiple distributed devices to train deep\nlearning models collaboratively while preserving data privacy. Nevertheless,\nthis approach may not be optimal for scenarios where data distribution is\nnon-identical among the participants or changes over time, causing what is\nknown as concept drift. Little research has yet been done in this field, but\nthis kind of situation is quite frequent in real life and poses new challenges\nto both continual and federated learning. Therefore, in this work, we present a\nnew method, called Concept-Drift-Aware Federated Averaging (CDA-FedAvg). Our\nproposal is an extension of the most popular federated algorithm, Federated\nAveraging (FedAvg), enhancing it for continual adaptation under concept drift.\nWe empirically demonstrate the weaknesses of regular FedAvg and prove that\nCDA-FedAvg outperforms it in this type of scenario.",
          "link": "http://arxiv.org/abs/2105.13309",
          "publishedOn": "2021-07-16T00:48:26.297Z",
          "wordCount": 643,
          "title": "Concept drift detection and adaptation for federated and continual learning. (arXiv:2105.13309v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1\">Vivien Cabannes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "Machine learning approached through supervised learning requires expensive\nannotation of data. This motivates weakly supervised learning, where data are\nannotated with incomplete yet discriminative information. In this paper, we\nfocus on partial labelling, an instance of weak supervision where, from a given\ninput, we are given a set of potential targets. We review a disambiguation\nprinciple to recover full supervision from weak supervision, and propose an\nempirical disambiguation algorithm. We prove exponential convergence rates of\nour algorithm under classical learnability assumptions, and we illustrate the\nusefulness of our method on practical examples.",
          "link": "http://arxiv.org/abs/2102.02789",
          "publishedOn": "2021-07-16T00:48:26.220Z",
          "wordCount": 592,
          "title": "Disambiguation of weak supervision with exponential convergence rates. (arXiv:2102.02789v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuoran Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chen Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lei Lyu</a>",
          "description": "Source code can be parsed into the abstract syntax tree (AST) based on\ndefined syntax rules. However, in pre-training, little work has considered the\nincorporation of tree structure into the learning process. In this paper, we\npresent TreeBERT, a tree-based pre-trained model for improving programming\nlanguage-oriented generation tasks. To utilize tree structure, TreeBERT\nrepresents the AST corresponding to the code as a set of composition paths and\nintroduces node position embedding. The model is trained by tree masked\nlanguage modeling (TMLM) and node order prediction (NOP) with a hybrid\nobjective. TMLM uses a novel masking strategy designed according to the tree's\ncharacteristics to help the model understand the AST and infer the missing\nsemantics of the AST. With NOP, TreeBERT extracts the syntactical structure by\nlearning the order constraints of nodes in AST. We pre-trained TreeBERT on\ndatasets covering multiple programming languages. On code summarization and\ncode documentation tasks, TreeBERT outperforms other pre-trained models and\nstate-of-the-art models designed for these tasks. Furthermore, TreeBERT\nperforms well when transferred to the pre-trained unseen programming language.",
          "link": "http://arxiv.org/abs/2105.12485",
          "publishedOn": "2021-07-16T00:48:26.207Z",
          "wordCount": 640,
          "title": "TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. (arXiv:2105.12485v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12684",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Overes_B/0/1/0/all/0/1\">Bart H.L. Overes</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wel_M/0/1/0/all/0/1\">Michel van der Wel</a>",
          "description": "Sovereign credit ratings summarize the creditworthiness of countries. These\nratings have a large influence on the economy and the yields at which\ngovernments can issue new debt. This paper investigates the use of a Multilayer\nPerceptron (MLP), Classification and Regression Trees (CART), Support Vector\nMachines (SVM), Na\\\"ive Bayes (NB), and an Ordered Logit (OL) model for the\nprediction of sovereign credit ratings. We show that MLP is best suited for\npredicting sovereign credit ratings, with a random cross-validated accuracy of\n68%, followed by CART (59%), SVM (41%), NB (38%), and OL (33%). Investigation\nof the determining factors shows that there is some heterogeneity in the\nimportant variables across the models. However, the two models with the highest\nout-of-sample predictive accuracy, MLP and CART, show a lot of similarities in\nthe influential variables, with regulatory quality, and GDP per capita as\ncommon important variables. Consistent with economic theory, a higher\nregulatory quality and/or GDP per capita are associated with a higher credit\nrating.",
          "link": "http://arxiv.org/abs/2101.12684",
          "publishedOn": "2021-07-16T00:48:26.183Z",
          "wordCount": 627,
          "title": "Modelling Sovereign Credit Ratings: Evaluating the Accuracy and Driving Factors using Machine Learning Techniques. (arXiv:2101.12684v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data. Towards this research\ngap, we first analyze the origin of biases from the perspective of \\textit{risk\ndiscrepancy} that represents the difference between the expectation empirical\nrisk and the true risk. Remarkably, we derive a general learning framework that\nwell summarizes most existing debiasing strategies by specifying some\nparameters of the general framework. This provides a valuable opportunity to\ndevelop a universal solution for debiasing, e.g., by learning the debiasing\nparameters from data. However, the training data lacks important signal of how\nthe data is biased and what the unbiased data looks like. To move this idea\nforward, we propose \\textit{AotoDebias} that leverages another (small) set of\nuniform data to optimize the debiasing parameters by solving the bi-level\noptimization problem with meta-learning. Through theoretical analyses, we\nderive the generalization bound for AutoDebias and prove its ability to acquire\nthe appropriate debiasing strategy. Extensive experiments on two real datasets\nand a simulated dataset demonstrated effectiveness of AutoDebias. The code is\navailable at \\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-07-16T00:48:26.178Z",
          "wordCount": 722,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zehao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiayi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "Domain generalization is challenging due to the domain shift and the\nuncertainty caused by the inaccessibility of target domain data. In this paper,\nwe address both challenges with a probabilistic framework based on variational\nBayesian inference, by incorporating uncertainty into neural network weights.\nWe couple domain invariance in a probabilistic formula with the variational\nBayesian inference. This enables us to explore domain-invariant learning in a\nprincipled way. Specifically, we derive domain-invariant representations and\nclassifiers, which are jointly established in a two-layer Bayesian neural\nnetwork. We empirically demonstrate the effectiveness of our proposal on four\nwidely used cross-domain visual recognition benchmarks. Ablation studies\nvalidate the synergistic benefits of our Bayesian treatment when jointly\nlearning domain-invariant representations and classifiers for domain\ngeneralization. Further, our method consistently delivers state-of-the-art mean\naccuracy on all benchmarks.",
          "link": "http://arxiv.org/abs/2105.04030",
          "publishedOn": "2021-07-16T00:48:26.165Z",
          "wordCount": 609,
          "title": "A Bit More Bayesian: Domain-Invariant Learning with Uncertainty. (arXiv:2105.04030v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02579",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Llorente_F/0/1/0/all/0/1\">F. Llorente</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Curbelo_E/0/1/0/all/0/1\">E. Curbelo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1\">L. Martino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Elvira_V/0/1/0/all/0/1\">V. Elvira</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delgado_D/0/1/0/all/0/1\">D. Delgado</a>",
          "description": "Monte Carlo methods are the standard procedure for estimating complicated\nintegrals of multidimensional Bayesian posterior distributions. In this work,\nwe focus on LAIS, a class of adaptive importance samplers where Markov chain\nMonte Carlo (MCMC) algorithms are employed to drive an underlying multiple\nimportance sampling (IS) scheme. Its power lies in the simplicity of the\nlayered framework: the upper layer locates proposal densities by means of MCMC\nalgorithms; while the lower layer handles the multiple IS scheme, in order to\ncompute the final estimators. The modular nature of LAIS allows for different\npossible choices in the upper and lower layers, that will have different\nperformance and computational costs. In this work, we propose different\nenhancements in order to increase the efficiency and reduce the computational\ncost, of both upper and lower layers. The different variants are essential if\nwe aim to address computational challenges arising in real-world applications,\nsuch as highly concentrated posterior distributions (due to large amounts of\ndata, etc.). Hamiltonian-driven importance samplers are presented and tested.\nFurthermore, we introduce different strategies for designing cheaper schemes,\nfor instance, recycling samples generated in the upper layer and using them in\nthe final estimators in the lower layer. Numerical experiments show the\nbenefits of the proposed schemes as compared to the vanilla version of LAIS and\nother benchmark methods.",
          "link": "http://arxiv.org/abs/2105.02579",
          "publishedOn": "2021-07-16T00:48:26.151Z",
          "wordCount": 669,
          "title": "MCMC-driven importance samplers. (arXiv:2105.02579v3 [stat.CO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-16T00:48:26.119Z",
          "wordCount": 744,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1\">Juan S. Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio A. Gonz&#xe1;lez</a>",
          "description": "The dissimilarity mixture autoencoder (DMAE) is a neural network model for\nfeature-based clustering that incorporates a flexible dissimilarity function\nand can be integrated into any kind of deep learning architecture. It\ninternally represents a dissimilarity mixture model (DMM) that extends\nclassical methods like K-Means, Gaussian mixture models, or Bregman clustering\nto any convex and differentiable dissimilarity function through the\nreinterpretation of probabilities as neural network representations. DMAE can\nbe integrated with deep learning architectures into end-to-end models, allowing\nthe simultaneous estimation of the clustering and neural network's parameters.\nExperimental evaluation was performed on image and text clustering benchmark\ndatasets showing that DMAE is competitive in terms of unsupervised\nclassification accuracy and normalized mutual information. The source code with\nthe implementation of DMAE is publicly available at:\nhttps://github.com/juselara1/dmae",
          "link": "http://arxiv.org/abs/2006.08177",
          "publishedOn": "2021-07-16T00:48:26.114Z",
          "wordCount": 622,
          "title": "Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10620",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Perdomo_J/0/1/0/all/0/1\">Juan C. Perdomo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter Bartlett</a>",
          "description": "We study the problem of adaptive control of the linear quadratic regulator\nfor systems in very high, or even infinite dimension. We demonstrate that while\nsublinear regret requires finite dimensional inputs, the ambient state\ndimension of the system need not be bounded in order to perform online control.\nWe provide the first regret bounds for LQR which hold for infinite dimensional\nsystems, replacing dependence on ambient dimension with more natural notions of\nproblem complexity. Our guarantees arise from a novel perturbation bound for\ncertainty equivalence which scales with the prediction error in estimating the\nsystem parameters, without requiring consistent parameter recovery in more\nstringent measures like the operator norm. When specialized to finite\ndimensional settings, our bounds recover near optimal dimension and time\nhorizon dependence.",
          "link": "http://arxiv.org/abs/2103.10620",
          "publishedOn": "2021-07-16T00:48:26.106Z",
          "wordCount": 589,
          "title": "Towards a Dimension-Free Understanding of Adaptive Linear Control. (arXiv:2103.10620v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplanis_C/0/1/0/all/0/1\">Christos Kaplanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1\">Jovana Mitrovi&#x107;</a>",
          "description": "We present an architecture that is effective for continual learning in an\nespecially demanding setting, where task boundaries do not exist or are\nunknown. Our architecture comprises an encoder, pre-trained on a separate\ndataset, and an ensemble of simple one-layer classifiers. Two main innovations\nare required to make this combination work. First, the provision of suitably\ngeneric pre-trained encoders has been made possible thanks to recent progress\nin self-supervised training methods. Second, pairing each classifier in the\nensemble with a key, where the key-space is identical to the latent space of\nthe encoder, allows them to be used collectively, yet selectively, via\nk-nearest neighbour lookup. We show that models trained with the\nencoders-and-ensembles architecture are state-of-the-art for the task-free\nsetting on standard image classification continual learning benchmarks, and\nimprove on prior state-of-the-art by a large margin in the most challenging\ncases. We also show that the architecture learns well in a fully incremental\nsetting, where one class is learned at a time, and we demonstrate its\neffectiveness in this setting with up to 100 classes. Finally, we show that the\narchitecture works in a task-free continual learning context where the data\ndistribution changes gradually, and existing approaches requiring knowledge of\ntask boundaries cannot be applied.",
          "link": "http://arxiv.org/abs/2105.13327",
          "publishedOn": "2021-07-16T00:48:26.099Z",
          "wordCount": 658,
          "title": "Encoders and Ensembles for Task-Free Continual Learning. (arXiv:2105.13327v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1\">Hossein Esfandiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1\">Vahab Mirrokni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>",
          "description": "Recently, due to an increasing interest for transparency in artificial\nintelligence, several methods of explainable machine learning have been\ndeveloped with the simultaneous goal of accuracy and interpretability by\nhumans. In this paper, we study a recent framework of explainable clustering\nfirst suggested by Dasgupta et al.~\\cite{dasgupta2020explainable}.\nSpecifically, we focus on the $k$-means and $k$-medians problems and provide\nnearly tight upper and lower bounds.\n\nFirst, we provide an $O(\\log k \\log \\log k)$-approximation algorithm for\nexplainable $k$-medians, improving on the best known algorithm of\n$O(k)$~\\cite{dasgupta2020explainable} and nearly matching the known\n$\\Omega(\\log k)$ lower bound~\\cite{dasgupta2020explainable}. In addition, in\nlow-dimensional spaces $d \\ll \\log k$, we show that our algorithm also provides\nan $O(d \\log^2 d)$-approximate solution for explainable $k$-medians. This\nimproves over the best known bound of $O(d \\log k)$ for low\ndimensions~\\cite{laber2021explainable}, and is a constant for constant\ndimensional spaces. To complement this, we show a nearly matching $\\Omega(d)$\nlower bound. Next, we study the $k$-means problem in this context and provide\nan $O(k \\log k)$-approximation algorithm for explainable $k$-means, improving\nover the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \\log k)$ bound of\n\\cite{laber2021explainable}. To complement this we provide an almost tight\n$\\Omega(k)$ lower bound, improving over the $\\Omega(\\log k)$ lower bound of\nDasgupta et al. Given an approximate solution to the classic $k$-means and\n$k$-medians, our algorithm for $k$-medians runs in time $O(kd \\log^2 k )$ and\nour algorithm for $k$-means runs in time $ O(k^2 d)$.",
          "link": "http://arxiv.org/abs/2107.00774",
          "publishedOn": "2021-07-16T00:48:26.079Z",
          "wordCount": 720,
          "title": "Almost Tight Approximation Algorithms for Explainable Clustering. (arXiv:2107.00774v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00760",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1\">Vivien Cabannes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>",
          "description": "Discrete supervised learning problems such as classification are often\ntackled by introducing a continuous surrogate problem akin to regression.\nBounding the original error, between estimate and solution, by the surrogate\nerror endows discrete problems with convergence rates already shown for\ncontinuous instances. Yet, current approaches do not leverage the fact that\ndiscrete problems are essentially predicting a discrete output when continuous\nproblems are predicting a continuous value. In this paper, we tackle this issue\nfor general structured prediction problems, opening the way to \"super fast\"\nrates, that is, convergence rates for the excess risk faster than $n^{-1}$,\nwhere $n$ is the number of observations, with even exponential rates with the\nstrongest assumptions. We first illustrate it for predictors based on nearest\nneighbors, generalizing rates known for binary classification to any discrete\nproblem within the framework of structured prediction. We then consider kernel\nridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast\nrates, depending on a parameter characterizing the hardness of the problem,\nthus allowing, under smoothness assumptions, to bypass the curse of\ndimensionality.",
          "link": "http://arxiv.org/abs/2102.00760",
          "publishedOn": "2021-07-16T00:48:26.072Z",
          "wordCount": 668,
          "title": "Fast rates in structured prediction. (arXiv:2102.00760v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Faruqui_S/0/1/0/all/0/1\">Syed Hasib Akhter Faruqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaeddini_A/0/1/0/all/0/1\">Adel Alaeddini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaramillo_C/0/1/0/all/0/1\">Carlos A. Jaramillo</a>",
          "description": "Bayesian networks are powerful statistical models to study the probabilistic\nrelationships among set random variables with major applications in disease\nmodeling and prediction. Here, we propose a continuous time Bayesian network\nwith conditional dependencies, represented as Poisson regression, to model the\nimpact of exogenous variables on the conditional dependencies of the network.\nWe also propose an adaptive regularization method with an intuitive early\nstopping feature based on density based clustering for efficient learning of\nthe structure and parameters of the proposed network. Using a dataset of\npatients with multiple chronic conditions extracted from electronic health\nrecords of the Department of Veterans Affairs we compare the performance of the\nproposed approach with some of the existing methods in the literature for both\nshort-term (one-year ahead) and long-term (multi-year ahead) predictions. The\nproposed approach provides a sparse intuitive representation of the complex\nfunctional relationships between multiple chronic conditions. It also provides\nthe capability of analyzing multiple disease trajectories over time given any\ncombination of prior conditions.",
          "link": "http://arxiv.org/abs/2007.15847",
          "publishedOn": "2021-07-16T00:48:26.063Z",
          "wordCount": 671,
          "title": "A Functional Model for Structure Learning and Parameter Estimation in Continuous Time Bayesian Network: An Application in Identifying Patterns of Multiple Chronic Conditions. (arXiv:2007.15847v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1\">Chris Cummins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1\">Benoit Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>",
          "description": "Path planning, the problem of efficiently discovering high-reward\ntrajectories, often requires optimizing a high-dimensional and multimodal\nreward function. Popular approaches like CEM and CMA-ES greedily focus on\npromising regions of the search space and may get trapped in local maxima. DOO\nand VOOT balance exploration and exploitation, but use space partitioning\nstrategies independent of the reward function to be optimized. Recently, LaMCTS\nempirically learns to partition the search space in a reward-sensitive manner\nfor black-box optimization. In this paper, we develop a novel formal regret\nanalysis for when and why such an adaptive region partitioning scheme works. We\nalso propose a new path planning method PlaLaM which improves the function\nvalue estimation within each sub-region, and uses a latent representation of\nthe search space. Empirically, PlaLaM outperforms existing path planning\nmethods in 2D navigation tasks, especially in the presence of\ndifficult-to-escape local optima, and shows benefits when plugged into\nmodel-based RL with planning components such as PETS. These gains transfer to\nhighly multimodal real-world tasks, where we outperform strong baselines in\ncompiler phase ordering by up to 245% and in molecular design by up to 0.4 on\nproperties on a 0-1 scale. Code is available at\nhttps://github.com/yangkevin2/plalam.",
          "link": "http://arxiv.org/abs/2106.10544",
          "publishedOn": "2021-07-16T00:48:26.048Z",
          "wordCount": 674,
          "title": "Learning Space Partitions for Path Planning. (arXiv:2106.10544v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xilin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "Recent progress in audio source separation lead by deep learning has enabled\nmany neural network models to provide robust solutions to this fundamental\nestimation problem. In this study, we provide a family of efficient neural\nnetwork architectures for general purpose audio source separation while\nfocusing on multiple computational aspects that hinder the application of\nneural networks in real-world scenarios. The backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRM-RF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. This mechanism enables\nour models to obtain high fidelity signal separation in a wide variety of\nsettings where variable number of sources are present and with limited\ncomputational resources (e.g. floating point operations, memory footprint,\nnumber of parameters and latency). Our experiments show that SuDoRM-RF models\nperform comparably and even surpass several state-of-the-art benchmarks with\nsignificantly higher computational resource requirements. The causal variation\nof SuDoRM-RF is able to obtain competitive performance in real-time speech\nseparation of around 10dB scale-invariant signal-to-distortion ratio\nimprovement (SI-SDRi) while remaining up to 20 times faster than real-time on a\nlaptop device.",
          "link": "http://arxiv.org/abs/2103.02644",
          "publishedOn": "2021-07-16T00:48:26.042Z",
          "wordCount": 674,
          "title": "Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yingjun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Huan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1\">Qiang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "In this paper, we introduce variational semantic memory into meta-learning to\nacquire long-term knowledge for few-shot learning. The variational semantic\nmemory accrues and stores semantic information for the probabilistic inference\nof class prototypes in a hierarchical Bayesian framework. The semantic memory\nis grown from scratch and gradually consolidated by absorbing information from\ntasks it experiences. By doing so, it is able to accumulate long-term, general\nknowledge that enables it to learn new concepts of objects. We formulate memory\nrecall as the variational inference of a latent memory variable from addressed\ncontents, which offers a principled way to adapt the knowledge to individual\ntasks. Our variational semantic memory, as a new long-term memory module,\nconfers principled recall and update mechanisms that enable semantic\ninformation to be efficiently accrued and adapted for few-shot learning.\nExperiments demonstrate that the probabilistic modelling of prototypes achieves\na more informative representation of object classes compared to deterministic\nvectors. The consistent new state-of-the-art performance on four benchmarks\nshows the benefit of variational semantic memory in boosting few-shot\nrecognition.",
          "link": "http://arxiv.org/abs/2010.10341",
          "publishedOn": "2021-07-16T00:48:26.030Z",
          "wordCount": 654,
          "title": "Learning to Learn Variational Semantic Memory. (arXiv:2010.10341v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daniel Wontae Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Y. Park</a>",
          "description": "In this paper, we devise a distributional framework on actor-critic as a\nsolution to distributional instability, action type restriction, and conflation\nbetween samples and statistics. We propose a new method that minimizes the\nCram\\'er distance with the multi-step Bellman target distribution generated\nfrom a novel Sample-Replacement algorithm denoted SR($\\lambda$), which learns\nthe correct value distribution under multiple Bellman operations.\nParameterizing a value distribution with Gaussian Mixture Model further\nimproves the efficiency and the performance of the method, which we name GMAC.\nWe empirically show that GMAC captures the correct representation of value\ndistributions and improves the performance of a conventional actor-critic\nmethod with low computational cost, in both discrete and continuous action\nspaces using Arcade Learning Environment (ALE) and PyBullet environment.",
          "link": "http://arxiv.org/abs/2105.11366",
          "publishedOn": "2021-07-16T00:48:26.025Z",
          "wordCount": 588,
          "title": "GMAC: A Distributional Perspective on Actor-Critic Framework. (arXiv:2105.11366v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_Y/0/1/0/all/0/1\">Yury Makarychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1\">Ali Vakilian</a>",
          "description": "We present an $(e^{O(p)} \\frac{\\log \\ell}{\\log\\log\\ell})$-approximation\nalgorithm for socially fair clustering with the $\\ell_p$-objective. In this\nproblem, we are given a set of points in a metric space. Each point belongs to\none (or several) of $\\ell$ groups. The goal is to find a $k$-medians,\n$k$-means, or, more generally, $\\ell_p$-clustering that is simultaneously good\nfor all of the groups. More precisely, we need to find a set of $k$ centers $C$\nso as to minimize the maximum over all groups $j$ of $\\sum_{u \\text{ in group\n}j} d(u,C)^p$. The socially fair clustering problem was independently proposed\nby Ghadiri, Samadi, and Vempala [2021] and Abbasi, Bhaskara, and\nVenkatasubramanian [2021]. Our algorithm improves and generalizes their\n$O(\\ell)$-approximation algorithms for the problem. The natural LP relaxation\nfor the problem has an integrality gap of $\\Omega(\\ell)$. In order to obtain\nour result, we introduce a strengthened LP relaxation and show that it has an\nintegrality gap of $\\Theta(\\frac{\\log \\ell}{\\log\\log\\ell})$ for a fixed $p$.\nAdditionally, we present a bicriteria approximation algorithm, which\ngeneralizes the bicriteria approximation of Abbasi et al. [2021].",
          "link": "http://arxiv.org/abs/2103.02512",
          "publishedOn": "2021-07-16T00:48:26.007Z",
          "wordCount": 642,
          "title": "Approximation Algorithms for Socially Fair Clustering. (arXiv:2103.02512v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios N. Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>",
          "description": "Black-box machine learning learning methods are now routinely used in\nhigh-risk settings, like medical diagnostics, which demand uncertainty\nquantification to avoid consequential model failures. Distribution-free\nuncertainty quantification (distribution-free UQ) is a user-friendly paradigm\nfor creating statistically rigorous confidence intervals/sets for such\npredictions. Critically, the intervals/sets are valid without distributional\nassumptions or model assumptions, with explicit guarantees with finitely many\ndatapoints. Moreover, they adapt to the difficulty of the input; when the input\nexample is difficult, the uncertainty intervals/sets are large, signaling that\nthe model might be wrong. Without much work, one can use distribution-free\nmethods on any underlying algorithm, such as a neural network, to produce\nconfidence sets guaranteed to contain the ground truth with a user-specified\nprobability, such as 90%. Indeed, the methods are easy-to-understand and\ngeneral, applying to many modern prediction problems arising in the fields of\ncomputer vision, natural language processing, deep reinforcement learning, and\nso on. This hands-on introduction is aimed at a reader interested in the\npractical implementation of distribution-free UQ, including conformal\nprediction and related methods, who is not necessarily a statistician. We will\ninclude many explanatory illustrations, examples, and code samples in Python,\nwith PyTorch syntax. The goal is to provide the reader a working understanding\nof distribution-free UQ, allowing them to put confidence intervals on their\nalgorithms, with one self-contained document.",
          "link": "http://arxiv.org/abs/2107.07511",
          "publishedOn": "2021-07-16T00:48:26.000Z",
          "wordCount": 676,
          "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification. (arXiv:2107.07511v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tino_P/0/1/0/all/0/1\">Peter Ti&#x148;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "Along with the great success of deep neural networks, there is also growing\nconcern about their black-box nature. The interpretability issue affects\npeople's trust on deep learning systems. It is also related to many ethical\nproblems, e.g., algorithmic discrimination. Moreover, interpretability is a\ndesired property for deep networks to become powerful tools in other research\nfields, e.g., drug discovery and genomics. In this survey, we conduct a\ncomprehensive review of the neural network interpretability research. We first\nclarify the definition of interpretability as it has been used in many\ndifferent contexts. Then we elaborate on the importance of interpretability and\npropose a novel taxonomy organized along three dimensions: type of engagement\n(passive vs. active interpretation approaches), the type of explanation, and\nthe focus (from local to global interpretability). This taxonomy provides a\nmeaningful 3D view of distribution of papers from the relevant literature as\ntwo of the dimensions are not simply categorical but allow ordinal\nsubcategories. Finally, we summarize the existing interpretability evaluation\nmethods and suggest possible research directions inspired by our new taxonomy.",
          "link": "http://arxiv.org/abs/2012.14261",
          "publishedOn": "2021-07-16T00:48:25.993Z",
          "wordCount": 650,
          "title": "A Survey on Neural Network Interpretability. (arXiv:2012.14261v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Derek_K/0/1/0/all/0/1\">Kenneth Derek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>",
          "description": "In the natural world, life has found innumerable ways to survive and often\nthrive. Between and even within species, each individual is in some manner\nunique, and this diversity lends adaptability and robustness to life. In this\nwork, we aim to learn a space of diverse and high-reward policies on any given\nenvironment. To this end, we introduce a generative model of policies, which\nmaps a low-dimensional latent space to an agent policy space. Our method\nenables learning an entire population of agent policies, without requiring the\nuse of separate policy parameters. Just as real world populations can adapt and\nevolve via natural selection, our method is able to adapt to changes in our\nenvironment solely by selecting for policies in latent space. We test our\ngenerative model's capabilities in a variety of environments, including an\nopen-ended grid-world and a two-player soccer environment. Code,\nvisualizations, and additional experiments can be found at\nhttps://kennyderek.github.io/adap/.",
          "link": "http://arxiv.org/abs/2107.07506",
          "publishedOn": "2021-07-16T00:48:25.958Z",
          "wordCount": 594,
          "title": "Adaptable Agent Populations via a Generative Model of Policies. (arXiv:2107.07506v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_C/0/1/0/all/0/1\">Chuyang Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1\">Jean Honorio</a>",
          "description": "In this paper we propose an algorithm for exact partitioning of high-order\nmodels. We define a general class of $m$-degree Homogeneous Polynomial Models,\nwhich subsumes several examples motivated from prior literature. Exact\npartitioning can be formulated as a tensor optimization problem. We relax this\nhigh-order combinatorial problem to a convex conic form problem. To this end,\nwe carefully define the Carath\\'eodory symmetric tensor cone, and show its\nconvexity, and the convexity of its dual cone. This allows us to construct a\nprimal-dual certificate to show that the solution of the convex relaxation is\ncorrect (equal to the unobserved true group assignment) and to analyze the\nstatistical upper bound of exact partitioning.",
          "link": "http://arxiv.org/abs/1911.02161",
          "publishedOn": "2021-07-16T00:48:25.941Z",
          "wordCount": 577,
          "title": "Exact Partitioning of High-order Models with a Novel Convex Tensor Cone Relaxation. (arXiv:1911.02161v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1\">Tianyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Biyi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yixin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Sheng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xiao Tu</a>",
          "description": "Structured pruning is a commonly used technique in deploying deep neural\nnetworks (DNNs) onto resource-constrained devices. However, the existing\npruning methods are usually heuristic, task-specified, and require an extra\nfine-tuning procedure. To overcome these limitations, we propose a framework\nthat compresses DNNs into slimmer architectures with competitive performances\nand significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two\nkeys: (i) we partition the parameters of DNNs into zero-invariant groups,\nenabling us to prune zero groups without affecting the output; and (ii) to\npromote zero groups, we then formulate a structured-sparsity optimization\nproblem and propose a novel optimization algorithm, Half-Space Stochastic\nProjected Gradient (HSPG), to solve it, which outperforms the standard proximal\nmethods on group sparsity exploration and maintains comparable convergence. To\ndemonstrate the effectiveness of OTO, we train and compress full models\nsimultaneously from scratch without fine-tuning for inference speedup and\nparameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10,\nResNet50 for CIFAR10/ImageNet and Bert for SQuAD.",
          "link": "http://arxiv.org/abs/2107.07467",
          "publishedOn": "2021-07-16T00:48:25.932Z",
          "wordCount": 608,
          "title": "Only Train Once: A One-Shot Neural Network Training And Pruning Framework. (arXiv:2107.07467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">Jay Roberts</a>",
          "description": "Deep neural networks are easily fooled by small perturbations known as\nadversarial attacks. Adversarial Training (AT) is a technique that\napproximately solves a robust optimization problem to minimize the worst-case\nloss and is widely regarded as the most effective defense against such attacks.\nWe develop a theoretical framework for adversarial training with FW\noptimization (FW-AT) that reveals a geometric connection between the loss\nlandscape and the distortion of $\\ell_\\infty$ FW attacks (the attack's $\\ell_2$\nnorm). Specifically, we show that high distortion of FW attacks is equivalent\nto low variation along the attack path. It is then experimentally demonstrated\non various deep neural network architectures that $\\ell_\\infty$ attacks against\nrobust models achieve near maximal $\\ell_2$ distortion. This mathematical\ntransparency differentiates FW from the more popular Projected Gradient Descent\n(PGD) optimization. To demonstrate the utility of our theoretical framework we\ndevelop FW-Adapt, a novel adversarial training algorithm which uses simple\ndistortion measure to adaptively change number of attack steps during training.\nFW-Adapt provides strong robustness at lower training times in comparison to\nPGD-AT for a variety of white-box and black-box attacks.",
          "link": "http://arxiv.org/abs/2012.12368",
          "publishedOn": "2021-07-16T00:48:25.925Z",
          "wordCount": 649,
          "title": "Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In view of the difficulty in reconstructing object details in point cloud\ncompletion, we propose a shape prior learning method for object completion. The\nshape priors include geometric information in both complete and the partial\npoint clouds. We design a feature alignment strategy to learn the shape prior\nfrom complete points, and a coarse to fine strategy to incorporate partial\nprior in the fine stage. To learn the complete objects prior, we first train a\npoint cloud auto-encoder to extract the latent embeddings from complete points.\nThen we learn a mapping to transfer the point features from partial points to\nthat of the complete points by optimizing feature alignment losses. The feature\nalignment losses consist of a L2 distance and an adversarial loss obtained by\nMaximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2\ndistance optimizes the partial features towards the complete ones in the\nfeature space, and MMD-GAN decreases the statistical distance of two point\nfeatures in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art\nperformances on the point cloud completion task. Our code is available at\nhttps://github.com/xiaogangw/point-cloud-completion-shape-prior.",
          "link": "http://arxiv.org/abs/2008.00394",
          "publishedOn": "2021-07-16T00:48:25.918Z",
          "wordCount": 661,
          "title": "Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1\">Mikolaj Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>",
          "description": "We study the image retrieval problem at the wireless edge, where an edge\ndevice captures an image, which is then used to retrieve similar images from an\nedge server. These can be images of the same person or a vehicle taken from\nother cameras at different times and locations. Our goal is to maximize the\naccuracy of the retrieval task under power and bandwidth constraints over the\nwireless link. Due to the stringent delay constraint of the underlying\napplication, sending the whole image at a sufficient quality is not possible.\nWe propose two alternative schemes based on digital and analog communications,\nrespectively. In the digital approach, we first propose a deep neural network\n(DNN) aided retrieval-oriented image compression scheme, whose output bit\nsequence is transmitted over the channel using conventional channel codes. In\nthe analog joint source and channel coding (JSCC) approach, the feature vectors\nare directly mapped into channel symbols. We evaluate both schemes on image\nbased re-identification (re-ID) tasks under different channel conditions,\nincluding both static and fading channels. We show that the JSCC scheme\nsignificantly increases the end-to-end accuracy, speeds up the encoding\nprocess, and provides graceful degradation with channel conditions. The\nproposed architecture is evaluated through extensive simulations on different\ndatasets and channel conditions, as well as through ablation studies.",
          "link": "http://arxiv.org/abs/2007.10915",
          "publishedOn": "2021-07-16T00:48:25.911Z",
          "wordCount": 671,
          "title": "Wireless Image Retrieval at the Edge. (arXiv:2007.10915v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sah_R/0/1/0/all/0/1\">Ramesh Kumar Sah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1\">Hassan Ghasemzadeh</a>",
          "description": "Machine learning is used for inference and decision making in wearable sensor\nsystems. However, recent studies have found that machine learning algorithms\nare easily fooled by the addition of adversarial perturbations to their inputs.\nWhat is more interesting is that adversarial examples generated for one machine\nlearning system is also effective against other systems. This property of\nadversarial examples is called transferability. In this work, we take the first\nstride in studying adversarial transferability in wearable sensor systems from\nthe following perspectives: 1) transferability between machine learning\nsystems, 2) transferability across subjects, 3) transferability across sensor\nbody locations, and 4) transferability across datasets. We found strong\nuntargeted transferability in most cases. Targeted attacks were less successful\nwith success scores from $0\\%$ to $80\\%$. The transferability of adversarial\nexamples depends on many factors such as the inclusion of data from all\nsubjects, sensor body position, number of samples in the dataset, type of\nlearning algorithm, and the distribution of source and target system dataset.\nThe transferability of adversarial examples decreases sharply when the data\ndistribution of the source and target system becomes more distinct. We also\nprovide guidelines for the community for designing robust sensor systems.",
          "link": "http://arxiv.org/abs/2003.07982",
          "publishedOn": "2021-07-16T00:48:25.897Z",
          "wordCount": 661,
          "title": "Adversarial Transferability in Wearable Sensor Systems. (arXiv:2003.07982v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.11094",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pradhan_H/0/1/0/all/0/1\">Hrusikesha Pradhan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rajawat_K/0/1/0/all/0/1\">Ketan Rajawat</a>",
          "description": "Gaussian processes provide a framework for nonlinear nonparametric Bayesian\ninference widely applicable across science and engineering. Unfortunately,\ntheir computational burden scales cubically with the training sample size,\nwhich in the case that samples arrive in perpetuity, approaches infinity. This\nissue necessitates approximations for use with streaming data, which to date\nmostly lack convergence guarantees. Thus, we develop the first online Gaussian\nprocess approximation that preserves convergence to the population posterior,\ni.e., asymptotic posterior consistency, while ameliorating its intractable\ncomplexity growth with the sample size. We propose an online compression scheme\nthat, following each a posteriori update, fixes an error neighborhood with\nrespect to the Hellinger metric centered at the current posterior, and greedily\ntosses out past kernel dictionary elements until its boundary is hit. We call\nthe resulting method Parsimonious Online Gaussian Processes (POG). For\ndiminishing error radius, exact asymptotic consistency is preserved (Theorem\n1(i)) at the cost of unbounded memory in the limit. On the other hand, for\nconstant error radius, POG converges to a neighborhood of the population\nposterior (Theorem 1(ii))but with finite memory at-worst determined by the\nmetric entropy of the feature space (Theorem 2). Experimental results are\npresented on several nonlinear regression problems which illuminates the merits\nof this approach as compared with alternatives that fix the subspace dimension\ndefining the history of past points.",
          "link": "http://arxiv.org/abs/2004.11094",
          "publishedOn": "2021-07-16T00:48:25.890Z",
          "wordCount": 678,
          "title": "Consistent Online Gaussian Process Regression Without the Sample Complexity Bottleneck. (arXiv:2004.11094v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11658",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1\">Chihiro Watanabe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>",
          "description": "Biclustering is a method for detecting homogeneous submatrices in a given\nobserved matrix, and it is an effective tool for relational data analysis.\nAlthough there are many studies that estimate the underlying bicluster\nstructure of a matrix, few have enabled us to determine the appropriate number\nof biclusters in an observed matrix. Recently, a statistical test on the number\nof biclusters has been proposed for a regular-grid bicluster structure, where\nwe assume that the latent bicluster structure can be represented by row-column\nclustering. However, when the latent bicluster structure does not satisfy such\nregular-grid assumption, the previous test requires a larger number of\nbiclusters than necessary (i.e., a finer bicluster structure than necessary)\nfor the null hypothesis to be accepted, which is not desirable in terms of\ninterpreting the accepted bicluster structure. In this study, we propose a new\nstatistical test on the number of biclusters that does not require the\nregular-grid assumption and derive the asymptotic behavior of the proposed test\nstatistic in both null and alternative cases. We illustrate the effectiveness\nof the proposed method by applying it to both synthetic and practical\nrelational data matrices.",
          "link": "http://arxiv.org/abs/2102.11658",
          "publishedOn": "2021-07-16T00:48:25.882Z",
          "wordCount": 656,
          "title": "A Goodness-of-fit Test on the Number of Biclusters in a Relational Data Matrix. (arXiv:2102.11658v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shuyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Natural language often exhibits inherent hierarchical structure ingrained\nwith complex syntax and semantics. However, most state-of-the-art deep\ngenerative models learn embeddings only in Euclidean vector space, without\naccounting for this structural property of language. In this paper, we\ninvestigate text generation in a hyperbolic latent space to learn continuous\nhierarchical representations. An Adversarial Poincare Variational Autoencoder\n(APo-VAE) is presented, where both the prior and variational posterior of\nlatent variables are defined over a Poincare ball via wrapped normal\ndistributions. By adopting the primal-dual formulation of KL divergence, an\nadversarial learning procedure is introduced to empower robust model training.\nExtensive experiments in language modeling and dialog-response generation tasks\ndemonstrate the winning effectiveness of the proposed APo-VAE model over VAEs\nin Euclidean latent space, thanks to its superb capabilities in capturing\nlatent language hierarchies in hyperbolic space.",
          "link": "http://arxiv.org/abs/2005.00054",
          "publishedOn": "2021-07-16T00:48:25.876Z",
          "wordCount": 610,
          "title": "APo-VAE: Text Generation in Hyperbolic Space. (arXiv:2005.00054v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gural_A/0/1/0/all/0/1\">Albert Gural</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_P/0/1/0/all/0/1\">Phillip Nadeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikekar_M/0/1/0/all/0/1\">Mehul Tikekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murmann_B/0/1/0/all/0/1\">Boris Murmann</a>",
          "description": "The recent success of neural networks for solving difficult decision tasks\nhas incentivized incorporating smart decision making \"at the edge.\" However,\nthis work has traditionally focused on neural network inference, rather than\ntraining, due to memory and compute limitations, especially in emerging\nnon-volatile memory systems, where writes are energetically costly and reduce\nlifespan. Yet, the ability to train at the edge is becoming increasingly\nimportant as it enables real-time adaptability to device drift and\nenvironmental variation, user customization, and federated learning across\ndevices. In this work, we address two key challenges for training on edge\ndevices with non-volatile memory: low write density and low auxiliary memory.\nWe present a low-rank training scheme that addresses these challenges while\nmaintaining computational efficiency. We then demonstrate the technique on a\nrepresentative convolutional neural network across several adaptation problems,\nwhere it out-performs standard SGD both in accuracy and in number of weight\nwrites.",
          "link": "http://arxiv.org/abs/2009.03887",
          "publishedOn": "2021-07-16T00:48:25.859Z",
          "wordCount": 615,
          "title": "Low-Rank Training of Deep Neural Networks for Emerging Memory Technology. (arXiv:2009.03887v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tizikara_D/0/1/0/all/0/1\">Dativa K. Tizikara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serugunda_J/0/1/0/all/0/1\">Jonathan Serugunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katumba_A/0/1/0/all/0/1\">Andrew Katumba</a>",
          "description": "Future communication systems are faced with increased demand for high\ncapacity, dynamic bandwidth, reliability and heterogeneous traffic. To meet\nthese requirements, networks have become more complex and thus require new\ndesign methods and monitoring techniques, as they evolve towards becoming\nautonomous. Machine learning has come to the forefront in recent years as a\npromising technology to aid in this evolution. Optical fiber communications can\nalready provide the high capacity required for most applications, however,\nthere is a need for increased scalability and adaptability to changing user\ndemands and link conditions. Accurate performance monitoring is an integral\npart of this transformation. In this paper we review optical performance\nmonitoring techniques where machine learning algorithms have been applied.\nMoreover, since alot of OPM depends on knowledge of the signal type, we also\nreview work for modulation format recognition and bitrate identification. We\nadditionally briefly introduce a neuromorphic approach to OPM as an emerging\ntechnique that has only recently been applied to this domain.",
          "link": "http://arxiv.org/abs/2107.07338",
          "publishedOn": "2021-07-16T00:48:25.825Z",
          "wordCount": 596,
          "title": "An Overview of Machine Learning-aided Optical Performance Monitoring Techniques. (arXiv:2107.07338v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palermo_J/0/1/0/all/0/1\">Joseph Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Johnny Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Alok Singh</a>",
          "description": "We convert the DeepMind Mathematics Dataset into a reinforcement learning\nenvironment by interpreting it as a program synthesis problem. Each action\ntaken in the environment adds an operator or an input into a discrete compute\ngraph. Graphs which compute correct answers yield positive reward, enabling the\noptimization of a policy to construct compute graphs conditioned on problem\nstatements. Baseline models are trained using Double DQN on various subsets of\nproblem types, demonstrating the capability to learn to correctly construct\ngraphs despite the challenges of combinatorial explosion and noisy rewards.",
          "link": "http://arxiv.org/abs/2107.07373",
          "publishedOn": "2021-07-16T00:48:25.816Z",
          "wordCount": 524,
          "title": "A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis. (arXiv:2107.07373v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_R/0/1/0/all/0/1\">Rahul Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Ramon_M/0/1/0/all/0/1\">Manel Mart&#xed;nez-Ram&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busani_T/0/1/0/all/0/1\">Tito Busani</a>",
          "description": "This work investigates application of different machine learning based\nprediction methodologies to estimate the performance of silicon based textured\ncells. Concept of confidence bound regions is introduced and advantages of this\nconcept are discussed in detail. Results show that reflection profiles and\ndepth dependent optical generation profiles can be accurately estimated using\nGaussian processes with exact knowledge of uncertainty in the prediction\nvalues.It is also shown that cell design parameters can be estimated for a\ndesired performance metric.",
          "link": "http://arxiv.org/abs/2107.07342",
          "publishedOn": "2021-07-16T00:48:25.802Z",
          "wordCount": 522,
          "title": "Probabilistic analysis of solar cell optical performance using Gaussian processes. (arXiv:2107.07342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>",
          "description": "We argue that immature data pipelines are preventing a large portion of\nindustry practitioners from leveraging the latest research on recommender\nsystems. We propose our template data stack for machine learning at \"reasonable\nscale\", and show how many challenges are solved by embracing a serverless\nparadigm. Leveraging our experience, we detail how modern open source can\nprovide a pipeline processing terabytes of data with limited infrastructure\nwork.",
          "link": "http://arxiv.org/abs/2107.07346",
          "publishedOn": "2021-07-16T00:48:25.796Z",
          "wordCount": 525,
          "title": "You Do Not Need a Bigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack. (arXiv:2107.07346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07480",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Derezinski_M/0/1/0/all/0/1\">Micha&#x142; Derezi&#x144;ski</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lacotte_J/0/1/0/all/0/1\">Jonathan Lacotte</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "In second-order optimization, a potential bottleneck can be computing the\nHessian matrix of the optimized function at every iteration. Randomized\nsketching has emerged as a powerful technique for constructing estimates of the\nHessian which can be used to perform approximate Newton steps. This involves\nmultiplication by a random sketching matrix, which introduces a trade-off\nbetween the computational cost of sketching and the convergence rate of the\noptimization algorithm. A theoretically desirable but practically much too\nexpensive choice is to use a dense Gaussian sketching matrix, which produces\nunbiased estimates of the exact Newton step and which offers strong\nproblem-independent convergence guarantees. We show that the Gaussian sketching\nmatrix can be drastically sparsified, significantly reducing the computational\ncost of sketching, without substantially affecting its convergence properties.\nThis approach, called Newton-LESS, is based on a recently introduced sketching\ntechnique: LEverage Score Sparsified (LESS) embeddings. We prove that\nNewton-LESS enjoys nearly the same problem-independent local convergence rate\nas Gaussian embeddings, not just up to constant factors but even down to lower\norder terms, for a large class of optimization tasks. In particular, this leads\nto a new state-of-the-art convergence result for an iterative least squares\nsolver. Finally, we extend LESS embeddings to include uniformly sparsified\nrandom sign matrices which can be implemented efficiently and which perform\nwell in numerical experiments.",
          "link": "http://arxiv.org/abs/2107.07480",
          "publishedOn": "2021-07-16T00:48:25.778Z",
          "wordCount": 666,
          "title": "Newton-LESS: Sparsification without Trade-offs for the Sketched Newton Update. (arXiv:2107.07480v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1\">Neil Band</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesnokov_G/0/1/0/all/0/1\">German Chesnokov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noskov_A/0/1/0/all/0/1\">Alexey Noskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploskonosov_A/0/1/0/all/0/1\">Andrey Ploskonosov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1\">Liudmila Prokhorenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1\">Ivan Provilkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatova_M/0/1/0/all/0/1\">Mariya Shmatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1\">Panos Tigas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1\">Boris Yangel</a>",
          "description": "There has been significant research done on developing methods for improving\nrobustness to distributional shift and uncertainty estimation. In contrast,\nonly limited work has examined developing standard datasets and benchmarks for\nassessing these approaches. Additionally, most work on uncertainty estimation\nand robustness has developed new techniques based on small-scale regression or\nimage classification tasks. However, many tasks of practical interest have\ndifferent modalities, such as tabular data, audio, text, or sensor data, which\noffer significant challenges involving regression and discrete or continuous\nstructured prediction. Thus, given the current state of the field, a\nstandardized large-scale dataset of tasks across a range of modalities affected\nby distributional shifts is necessary. This will enable researchers to\nmeaningfully evaluate the plethora of recently developed uncertainty\nquantification methods, as well as assessment criteria and state-of-the-art\nbaselines. In this work, we propose the \\emph{Shifts Dataset} for evaluation of\nuncertainty estimates and robustness to distributional shift. The dataset,\nwhich has been collected from industrial sources and services, is composed of\nthree tasks, with each corresponding to a particular data modality: tabular\nweather prediction, machine translation, and self-driving car (SDC) vehicle\nmotion prediction. All of these data modalities and tasks are affected by real,\n`in-the-wild' distributional shifts and pose interesting challenges with\nrespect to uncertainty estimation. In this work we provide a description of the\ndataset and baseline results for all tasks.",
          "link": "http://arxiv.org/abs/2107.07455",
          "publishedOn": "2021-07-16T00:48:25.772Z",
          "wordCount": 693,
          "title": "Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. (arXiv:2107.07455v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fang-Yi Yu</a>",
          "description": "This paper introduces an optimization problem for proper scoring rule design.\nConsider a principal who wants to collect an agent's prediction about an\nunknown state. The agent can either report his prior prediction or access a\ncostly signal and report the posterior prediction. Given a collection of\npossible distributions containing the agent's posterior prediction\ndistribution, the principal's objective is to design a bounded scoring rule to\nmaximize the agent's worst-case payoff increment between reporting his\nposterior prediction and reporting his prior prediction.\n\nWe study two settings of such optimization for proper scoring rules: static\nand asymptotic settings. In the static setting, where the agent can access one\nsignal, we propose an efficient algorithm to compute an optimal scoring rule\nwhen the collection of distributions is finite. The agent can adaptively and\nindefinitely refine his prediction in the asymptotic setting. We first consider\na sequence of collections of posterior distributions with vanishing covariance,\nwhich emulates general estimators with large samples, and show the optimality\nof the quadratic scoring rule. Then, when the agent's posterior distribution is\na Beta-Bernoulli process, we find that the log scoring rule is optimal. We also\nprove the optimality of the log scoring rule over a smaller set of functions\nfor categorical distributions with Dirichlet priors.",
          "link": "http://arxiv.org/abs/2107.07420",
          "publishedOn": "2021-07-16T00:48:25.765Z",
          "wordCount": 643,
          "title": "Optimal Scoring Rule Design. (arXiv:2107.07420v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07423",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ginige_N/0/1/0/all/0/1\">Nipuni Ginige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manosha_K/0/1/0/all/0/1\">K. B. Shashika Manosha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajatheva_N/0/1/0/all/0/1\">Nandana Rajatheva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Latva_aho_M/0/1/0/all/0/1\">Matti Latva-aho</a>",
          "description": "Reconfigurable intelligent surface (RIS) is an emerging technology for\nimproving performance in fifth-generation (5G) and beyond networks. Practically\nchannel estimation of RIS-assisted systems is challenging due to the passive\nnature of the RIS. The purpose of this paper is to introduce a deep\nlearning-based, low complexity channel estimator for the RIS-assisted\nmulti-user single-input-multiple-output (SIMO) orthogonal frequency division\nmultiplexing (OFDM) system with hardware impairments. We propose an untrained\ndeep neural network (DNN) based on the deep image prior (DIP) network to\ndenoise the effective channel of the system obtained from the conventional\npilot-based least-square (LS) estimation and acquire a more accurate\nestimation. We have shown that our proposed method has high performance in\nterms of accuracy and low complexity compared to conventional methods. Further,\nwe have shown that the proposed estimator is robust to interference caused by\nthe hardware impairments at the transceiver and RIS.",
          "link": "http://arxiv.org/abs/2107.07423",
          "publishedOn": "2021-07-16T00:48:25.725Z",
          "wordCount": 595,
          "title": "Untrained DNN for Channel Estimation of RIS-Assisted Multi-User OFDM System with Hardware Impairments. (arXiv:2107.07423v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weilbach_J/0/1/0/all/0/1\">Juliane Weilbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerwinn_S/0/1/0/all/0/1\">Sebastian Gerwinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weilbach_C/0/1/0/all/0/1\">Christian Weilbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1\">Melih Kandemir</a>",
          "description": "Understanding physical phenomena oftentimes means understanding the\nunderlying dynamical system that governs observational measurements. While\naccurate prediction can be achieved with black box systems, they often lack\ninterpretability and are less amenable for further expert investigation.\nAlternatively, the dynamics can be analysed via symbolic regression. In this\npaper, we extend the approach by (Udrescu et al., 2020) called AIFeynman to the\ndynamic setting to perform symbolic regression on ODE systems based on\nobservations from the resulting trajectories. We compare this extension to\nstate-of-the-art approaches for symbolic regression empirically on several\ndynamical systems for which the ground truth equations of increasing complexity\nare available. Although the proposed approach performs best on this benchmark,\nwe observed difficulties of all the compared symbolic regression approaches on\nmore complex systems, such as Cart-Pole.",
          "link": "http://arxiv.org/abs/2107.07345",
          "publishedOn": "2021-07-16T00:48:25.703Z",
          "wordCount": 557,
          "title": "Inferring the Structure of Ordinary Differential Equations. (arXiv:2107.07345v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07436",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1\">Neil Jethani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1\">Mukund Sudarshan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Shapley values are widely used to explain black-box models, but they are\ncostly to calculate because they require many model evaluations. We introduce\nFastSHAP, a method for estimating Shapley values in a single forward pass using\na learned explainer model. FastSHAP amortizes the cost of explaining many\ninputs via a learning approach inspired by the Shapley value's weighted least\nsquares characterization, and it can be trained using standard stochastic\ngradient optimization. We compare FastSHAP to existing estimation approaches,\nrevealing that it generates high-quality explanations with orders of magnitude\nspeedup.",
          "link": "http://arxiv.org/abs/2107.07436",
          "publishedOn": "2021-07-16T00:48:25.693Z",
          "wordCount": 533,
          "title": "FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07483",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Valente_F/0/1/0/all/0/1\">Francisco Valente</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Paredes_S/0/1/0/all/0/1\">Sim&#xe3;o Paredes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Henriques_J/0/1/0/all/0/1\">Jorge Henriques</a>",
          "description": "In this study, we present a novel clinical decision support system and\ndiscuss its interpretability-related properties. It combines a decision set of\nrules with a machine learning scheme to offer global and local\ninterpretability. More specifically, machine learning is used to predict the\nlikelihood of each of those rules to be correct for a particular patient, which\nmay also contribute to better predictive performances. Moreover, the\nreliability analysis of individual predictions is also addressed, contributing\nto further personalized interpretability. The combination of these several\nelements may be crucial to obtain the clinical stakeholders' trust, leading to\na better assessment of patients' conditions and improvement of the physicians'\ndecision-making.",
          "link": "http://arxiv.org/abs/2107.07483",
          "publishedOn": "2021-07-16T00:48:25.687Z",
          "wordCount": 559,
          "title": "Personalized and Reliable Decision Sets: Enhancing Interpretability in Clinical Decision Support Systems. (arXiv:2107.07483v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Nico Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>",
          "description": "The increasing demand for commodities is leading to changes in land use\nworldwide. In the tropics, deforestation, which causes high carbon emissions\nand threatens biodiversity, is often linked to agricultural expansion. While\nthe need for deforestation-free global supply chains is widely recognized,\nmaking progress in practice remains a challenge. Here, we propose an automated\napproach that aims to support conservation and sustainable land use planning\ndecisions by mapping tropical landscapes at large scale and high spatial\nresolution following the High Carbon Stock (HCS) approach. A deep learning\napproach is developed that estimates canopy height for each 10 m Sentinel-2\npixel by learning from sparse GEDI LIDAR reference data, achieving an overall\nRMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are\npredictive for classifying HCS forests and degraded areas with an overall\naccuracy of 86 % and produce a first high carbon stock map for Indonesia,\nMalaysia, and the Philippines.",
          "link": "http://arxiv.org/abs/2107.07431",
          "publishedOn": "2021-07-16T00:48:25.677Z",
          "wordCount": 606,
          "title": "High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1\">Mike Laszkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1\">Johannes Lederer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1\">Asja Fischer</a>",
          "description": "Normalizing flows, which learn a distribution by transforming the data to\nsamples from a Gaussian base distribution, have proven powerful density\napproximations. But their expressive power is limited by this choice of the\nbase distribution. We, therefore, propose to generalize the base distribution\nto a more elaborate copula distribution to capture the properties of the target\ndistribution more accurately. In a first empirical analysis, we demonstrate\nthat this replacement can dramatically improve the vanilla normalizing flows in\nterms of flexibility, stability, and effectivity for heavy-tailed data. Our\nresults suggest that the improvements are related to an increased local\nLipschitz-stability of the learned flow.",
          "link": "http://arxiv.org/abs/2107.07352",
          "publishedOn": "2021-07-16T00:48:25.658Z",
          "wordCount": 550,
          "title": "Copula-Based Normalizing Flows. (arXiv:2107.07352v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>",
          "description": "Gradient Episodic Memory is indeed a novel method for continual learning,\nwhich solves new problems quickly without forgetting previously acquired\nknowledge. However, in the process of studying the paper, we found there were\nsome problems in the proof of the dual problem of Quadratic Program, so here we\ngive our fixed version for this problem.",
          "link": "http://arxiv.org/abs/2107.07384",
          "publishedOn": "2021-07-16T00:48:25.640Z",
          "wordCount": 485,
          "title": "A Fixed Version of Quadratic Program in Gradient Episodic Memory. (arXiv:2107.07384v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fickinger_A/0/1/0/all/0/1\">Arnaud Fickinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1\">Natasha Jaques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1\">Samyak Parajuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Michael Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1\">Nicholas Rhinehart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1\">Glen Berseth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Reinforcement learning (RL) provides a framework for learning goal-directed\npolicies given user-specified rewards. However, since designing rewards often\nrequires substantial engineering effort, we are interested in the problem of\nlearning without rewards, where agents must discover useful behaviors in the\nabsence of task-specific incentives. Intrinsic motivation is a family of\nunsupervised RL techniques which develop general objectives for an RL agent to\noptimize that lead to better exploration or the discovery of skills. In this\npaper, we propose a new unsupervised RL technique based on an adversarial game\nwhich pits two policies against each other to compete over the amount of\nsurprise an RL agent experiences. The policies each take turns controlling the\nagent. The Explore policy maximizes entropy, putting the agent into surprising\nor unfamiliar situations. Then, the Control policy takes over and seeks to\nrecover from those situations by minimizing entropy. The game harnesses the\npower of multi-agent competition to drive the agent to seek out increasingly\nsurprising parts of the environment while learning to gain mastery over them.\nWe show empirically that our method leads to the emergence of complex skills by\nexhibiting clear phase transitions. Furthermore, we show both theoretically\n(via a latent state space coverage argument) and empirically that our method\nhas the potential to be applied to the exploration of stochastic,\npartially-observed environments. We show that Adversarial Surprise learns more\ncomplex behaviors, and explores more effectively than competitive baselines,\noutperforming intrinsic motivation methods based on active inference,\nnovelty-seeking (Random Network Distillation (RND)), and multi-agent\nunsupervised RL (Asymmetric Self-Play (ASP)) in MiniGrid, Atari and VizDoom\nenvironments.",
          "link": "http://arxiv.org/abs/2107.07394",
          "publishedOn": "2021-07-16T00:48:25.631Z",
          "wordCount": 698,
          "title": "Explore and Control with Adversarial Surprise. (arXiv:2107.07394v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to\nits potential sample efficiency compared to model-free RL. However, existing\nempirical model-based RL approaches lack the ability to explore. This work\nstudies a computationally and statistically efficient model-based algorithm for\nboth Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes\n(MDPs). For both models, our algorithm guarantees polynomial sample complexity\nand only uses access to a planning oracle. Experimentally, we first demonstrate\nthe flexibility and efficacy of our algorithm on a set of exploration\nchallenging control tasks where existing empirical model-based RL approaches\ncompletely fail. We then show that our approach retains excellent performance\neven in common dense reward control benchmarks that do not require heavy\nexploration. Finally, we demonstrate that our method can also perform\nreward-free exploration efficiently. Our code can be found at\nhttps://github.com/yudasong/PCMLP.",
          "link": "http://arxiv.org/abs/2107.07410",
          "publishedOn": "2021-07-16T00:48:25.625Z",
          "wordCount": 567,
          "title": "PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration. (arXiv:2107.07410v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_E/0/1/0/all/0/1\">Elaine Pimentel</a> (UFRN), <a href=\"http://arxiv.org/find/cs/1/au:+Tassi_E/0/1/0/all/0/1\">Enrico Tassi</a> (Inria)",
          "description": "Logical frameworks and meta-languages form a common substrate for\nrepresenting, implementing and reasoning about a wide variety of deductive\nsystems of interest in logic and computer science. Their design, implementation\nand their use in reasoning tasks, ranging from the correctness of software to\nthe properties of formal systems, have been the focus of considerable research\nover the last two decades. This workshop brings together designers,\nimplementors and practitioners to discuss various aspects impinging on the\nstructure and utility of logical frameworks, including the treatment of\nvariable binding, inductive and co-inductive reasoning techniques and the\nexpressiveness and lucidity of the reasoning process.",
          "link": "http://arxiv.org/abs/2107.07376",
          "publishedOn": "2021-07-16T00:48:25.611Z",
          "wordCount": 567,
          "title": "Proceedings of the Sixteenth Workshop on Logical Frameworks and Meta-Languages: Theory and Practice. (arXiv:2107.07376v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1\">Alon Talmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "Models pre-trained with a language modeling objective possess ample world\nknowledge and language skills, but are known to struggle in tasks that require\nreasoning. In this work, we propose to leverage semi-structured tables, and\nautomatically generate at scale question-paragraph pairs, where answering the\nquestion requires reasoning over multiple facts in the paragraph. We add a\npre-training step over this synthetic data, which includes examples that\nrequire 16 different reasoning skills such as number comparison, conjunction,\nand fact composition. To improve data efficiency, we propose sampling\nstrategies that focus training on reasoning skills the model is currently\nlacking. We evaluate our approach on three reading comprehension datasets that\nare focused on reasoning, and show that our model, PReasM, substantially\noutperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling\nexamples based on current model errors leads to faster training and higher\noverall performance.",
          "link": "http://arxiv.org/abs/2107.07261",
          "publishedOn": "2021-07-16T00:48:25.604Z",
          "wordCount": 587,
          "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rutwik Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astuto_B/0/1/0/all/0/1\">Bruno Astuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_T/0/1/0/all/0/1\">Tyler Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_W/0/1/0/all/0/1\">Will Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaga_J/0/1/0/all/0/1\">Justin Banaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweetwood_K/0/1/0/all/0/1\">Kevin Sweetwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1\">Allen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Rina Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGill_K/0/1/0/all/0/1\">Kevin McGill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Link_T/0/1/0/all/0/1\">Thomas Link</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crane_J/0/1/0/all/0/1\">Jason Crane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedoia_V/0/1/0/all/0/1\">Valentina Pedoia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Sharmila Majumdar</a>",
          "description": "Radiologists today play a key role in making diagnostic decisions and\nlabeling images for training A.I. algorithms. Low inter-reader reliability\n(IRR) can be seen between experts when interpreting challenging cases. While\nteams-based decisions are known to outperform individual decisions,\ninter-personal biases often creep up in group interactions which limit\nnon-dominant participants from expressing true opinions. To overcome the dual\nproblems of low consensus and inter-personal bias, we explored a solution\nmodeled on biological swarms of bees. Two separate cohorts; three radiologists\nand five radiology residents collaborated on a digital swarm platform in real\ntime and in a blinded fashion, grading meniscal lesions on knee MR exams. These\nconsensus votes were benchmarked against clinical (arthroscopy) and\nradiological (senior-most radiologist) observations. The IRR of the consensus\nvotes was compared to the IRR of the majority and most confident votes of the\ntwo cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm\nvotes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm\nvotes over majority vote, was observed. The 5-resident swarm had an even higher\nimprovement of 32% in IRR over majority vote. Swarm consensus votes also\nimproved specificity by up to 50%. The swarm consensus votes outperformed\nindividual and majority vote decisions in both the radiologists and resident\ncohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating\npositive effect of increased swarm size. The attending and resident swarms also\noutperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a\ndigital swarm platform improved agreement and allows participants to express\njudgement free intent, resulting in superior clinical performance and robust\nA.I. training labels.",
          "link": "http://arxiv.org/abs/2107.07341",
          "publishedOn": "2021-07-16T00:48:25.589Z",
          "wordCount": 771,
          "title": "Leveraging wisdom of the crowds to improve consensus among radiologists by real time, blinded collaborations on a digital swarm platform. (arXiv:2107.07341v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bej_S/0/1/0/all/0/1\">Saptarshi Bej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schultz_K/0/1/0/all/0/1\">Kristian Schultz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prashant Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfien_M/0/1/0/all/0/1\">Markus Wolfien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolkenhauer_O/0/1/0/all/0/1\">Olaf Wolkenhauer</a>",
          "description": "Over 85 oversampling algorithms, mostly extensions of the SMOTE algorithm,\nhave been built over the past two decades, to solve the problem of imbalanced\ndatasets. However, it has been evident from previous studies that different\noversampling algorithms have different degrees of efficiency with different\nclassifiers. With numerous algorithms available, it is difficult to decide on\nan oversampling algorithm for a chosen classifier. Here, we overcome this\nproblem with a multi-schematic and classifier-independent oversampling\napproach: ProWRAS(Proximity Weighted Random Affine Shadowsampling). ProWRAS\nintegrates the Localized Random Affine Shadowsampling (LoRAS)algorithm and the\nProximity Weighted Synthetic oversampling (ProWSyn) algorithm. By controlling\nthe variance of the synthetic samples, as well as a proximity-weighted\nclustering system of the minority classdata, the ProWRAS algorithm improves\nperformance, compared to algorithms that generate synthetic samples through\nmodelling high dimensional convex spaces of the minority class. ProWRAS has\nfour oversampling schemes, each of which has its unique way to model the\nvariance of the generated data. Most importantly, the performance of ProWRAS\nwith proper choice of oversampling schemes, is independent of the classifier\nused. We have benchmarked our newly developed ProWRAS algorithm against five\nsate-of-the-art oversampling models and four different classifiers on 20\npublicly available datasets. ProWRAS outperforms other oversampling algorithms\nin a statistically significant way, in terms of both F1-score and Kappa-score.\nMoreover, we have introduced a novel measure for classifier independence\nI-score, and showed quantitatively that ProWRAS performs better, independent of\nthe classifier used. In practice, ProWRAS customizes synthetic sample\ngeneration according to a classifier of choice and thereby reduces benchmarking\nefforts.",
          "link": "http://arxiv.org/abs/2107.07349",
          "publishedOn": "2021-07-16T00:48:25.573Z",
          "wordCount": 695,
          "title": "A multi-schematic classifier-independent oversampling approach for imbalanced datasets. (arXiv:2107.07349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azam_M/0/1/0/all/0/1\">Md Ali Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossen_A/0/1/0/all/0/1\">Abir Hossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Hafizur Rahman</a>",
          "description": "Biologically inspired computing techniques are very effective and useful in\nmany areas of research including data clustering. Ant clustering algorithm is a\nnature-inspired clustering technique which is extensively studied for over two\ndecades. In this study, we extend the ant clustering algorithm (ACA) to a\nhybrid ant clustering algorithm (hACA). Specifically, we include a genetic\nalgorithm in standard ACA to extend the hybrid algorithm for better\nperformance. We also introduced novel pick up and drop off rules to speed up\nthe clustering performance. We study the performance of the hACA algorithm and\ncompare with standard ACA as a benchmark.",
          "link": "http://arxiv.org/abs/2107.07382",
          "publishedOn": "2021-07-16T00:48:25.558Z",
          "wordCount": 545,
          "title": "Hybrid Ant Swarm-Based Data Clustering. (arXiv:2107.07382v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiangyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chulin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>",
          "description": "We study the realistic potential of conducting backdoor attack against deep\nneural networks (DNNs) during deployment stage. Specifically, our goal is to\ndesign a deployment-stage backdoor attack algorithm that is both threatening\nand realistically implementable. To this end, we propose Subnet Replacement\nAttack (SRA), which is capable of embedding backdoor into DNNs by directly\nmodifying a limited number of model parameters. Considering the realistic\npracticability, we abandon the strong white-box assumption widely adopted in\nexisting studies, instead, our algorithm works in a gray-box setting, where\narchitecture information of the victim model is available but the adversaries\ndo not have any knowledge of parameter values. The key philosophy underlying\nour approach is -- given any neural network instance (regardless of its\nspecific parameter values) of a certain architecture, we can always embed a\nbackdoor into that model instance, by replacing a very narrow subnet of a\nbenign model (without backdoor) with a malicious backdoor subnet, which is\ndesigned to be sensitive (fire large activation value) to a particular backdoor\ntrigger pattern.",
          "link": "http://arxiv.org/abs/2107.07240",
          "publishedOn": "2021-07-16T00:48:25.541Z",
          "wordCount": 628,
          "title": "Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting. (arXiv:2107.07240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>",
          "description": "We present a generic method for recurrently using the same parameters for\nmany different convolution layers to build a deep network. Specifically, for a\nnetwork, we create a recurrent parameter generator (RPG), from which the\nparameters of each convolution layer are generated. Though using recurrent\nmodels to build a deep convolutional neural network (CNN) is not entirely new,\nour method achieves significant performance gain compared to the existing\nworks. We demonstrate how to build a one-layer neural network to achieve\nsimilar performance compared to other traditional CNN models on various\napplications and datasets. Such a method allows us to build an arbitrarily\ncomplex neural network with any amount of parameters. For example, we build a\nResNet34 with model parameters reduced by more than $400$ times, which still\nachieves $41.6\\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG\ncan be applied at different scales, such as layers, blocks, or even\nsub-networks. Specifically, we use the RPG to build a ResNet18 network with the\nnumber of weights equivalent to one convolutional layer of a conventional\nResNet and show this model can achieve $67.2\\%$ ImageNet top-1 accuracy. The\nproposed method can be viewed as an inverse approach to model compression.\nRather than removing the unused parameters from a large model, it aims to\nsqueeze more information into a small number of parameters. Extensive\nexperiment results are provided to demonstrate the power of the proposed\nrecurrent parameter generator.",
          "link": "http://arxiv.org/abs/2107.07110",
          "publishedOn": "2021-07-16T00:48:25.503Z",
          "wordCount": 666,
          "title": "Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafrasteh_B/0/1/0/all/0/1\">Bahram Jafrasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villacampa_Calvo_C/0/1/0/all/0/1\">Carlos Villacampa-Calvo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_D/0/1/0/all/0/1\">Daniel Hern&#xe1;ndez-Lobato</a>",
          "description": "Gaussian Processes (GPs) are Bayesian models that provide uncertainty\nestimates associated to the predictions made. They are also very flexible due\nto their non-parametric nature. Nevertheless, GPs suffer from poor scalability\nas the number of training instances N increases. More precisely, they have a\ncubic cost with respect to $N$. To overcome this problem, sparse GP\napproximations are often used, where a set of $M \\ll N$ inducing points is\nintroduced during training. The location of the inducing points is learned by\nconsidering them as parameters of an approximate posterior distribution $q$.\nSparse GPs, combined with variational inference for inferring $q$, reduce the\ntraining cost of GPs to $\\mathcal{O}(M^3)$. Critically, the inducing points\ndetermine the flexibility of the model and they are often located in regions of\nthe input space where the latent function changes. A limitation is, however,\nthat for some learning tasks a large number of inducing points may be required\nto obtain a good prediction performance. To address this limitation, we propose\nhere to amortize the computation of the inducing points locations, as well as\nthe parameters of the variational posterior approximation q. For this, we use a\nneural network that receives the observed data as an input and outputs the\ninducing points locations and the parameters of $q$. We evaluate our method in\nseveral experiments, showing that it performs similar or better than other\nstate-of-the-art sparse variational GP approaches. However, with our method the\nnumber of inducing points is reduced drastically due to their dependency on the\ninput data. This makes our method scale to larger datasets and have faster\ntraining and prediction times.",
          "link": "http://arxiv.org/abs/2107.07281",
          "publishedOn": "2021-07-16T00:48:25.487Z",
          "wordCount": 695,
          "title": "Input Dependent Sparse Gaussian Processes. (arXiv:2107.07281v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kungurtsev_V/0/1/0/all/0/1\">Vyacheslav Kungurtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobb_A/0/1/0/all/0/1\">Adam Cobb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1\">Tara Javidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalaian_B/0/1/0/all/0/1\">Brian Jalaian</a>",
          "description": "Federated learning performed by a decentralized networks of agents is\nbecoming increasingly important with the prevalence of embedded software on\nautonomous devices. Bayesian approaches to learning benefit from offering more\ninformation as to the uncertainty of a random quantity, and Langevin and\nHamiltonian methods are effective at realizing sampling from an uncertain\ndistribution with large parameter dimensions. Such methods have only recently\nappeared in the decentralized setting, and either exclusively use stochastic\ngradient Langevin and Hamiltonian Monte Carlo approaches that require a\ndiminishing stepsize to asymptotically sample from the posterior and are known\nin practice to characterize uncertainty less faithfully than constant step-size\nmethods with a Metropolis adjustment, or assume strong convexity properties of\nthe potential function. We present the first approach to incorporating constant\nstepsize Metropolis-adjusted HMC in the decentralized sampling framework, show\ntheoretical guarantees for consensus and probability distance to the posterior\nstationary distribution, and demonstrate their effectiveness numerically on\nstandard real world problems, including decentralized learning of neural\nnetworks which is known to be highly non-convex.",
          "link": "http://arxiv.org/abs/2107.07211",
          "publishedOn": "2021-07-16T00:48:25.480Z",
          "wordCount": 609,
          "title": "Decentralized Bayesian Learning with Metropolis-Adjusted Hamiltonian Monte Carlo. (arXiv:2107.07211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1\">Ivona Najdenkoska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Automating report generation for medical imaging promises to reduce workload\nand assist diagnosis in clinical practice. Recent work has shown that deep\nlearning models can successfully caption natural images. However, learning from\nmedical data is challenging due to the diversity and uncertainty inherent in\nthe reports written by different radiologists with discrepant expertise and\nexperience. To tackle these challenges, we propose variational topic inference\nfor automatic report generation. Specifically, we introduce a set of topics as\nlatent variables to guide sentence generation by aligning image and language\nmodalities in a latent space. The topics are inferred in a conditional\nvariational inference framework, with each topic governing the generation of a\nsentence in the report. Further, we adopt a visual attention module that\nenables the model to attend to different locations in the image and generate\nmore informative descriptions. We conduct extensive experiments on two\nbenchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results\ndemonstrate that our proposed variational topic inference method can generate\nnovel reports rather than mere copies of reports used in training, while still\nachieving comparable performance to state-of-the-art methods in terms of\nstandard language generation criteria.",
          "link": "http://arxiv.org/abs/2107.07314",
          "publishedOn": "2021-07-16T00:48:25.471Z",
          "wordCount": 653,
          "title": "Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Mansheej Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1\">Gintare Karolina Dziugaite</a>",
          "description": "The recent success of deep learning has partially been driven by training\nincreasingly overparametrized networks on ever larger datasets. It is therefore\nnatural to ask: how much of the data is superfluous, which examples are\nimportant for generalization, and how do we find them? In this work, we make\nthe striking observation that, on standard vision benchmarks, the initial loss\ngradient norm of individual training examples, averaged over several weight\ninitializations, can be used to identify a smaller set of training data that is\nimportant for generalization. Furthermore, after only a few epochs of training,\nthe information in gradient norms is reflected in the normed error--L2 distance\nbetween the predicted probabilities and one hot labels--which can be used to\nprune a significant fraction of the dataset without sacrificing test accuracy.\nBased on this, we propose data pruning methods which use only local information\nearly in training, and connect them to recent work that prunes data by\ndiscarding examples that are rarely forgotten over the course of training. Our\nmethods also shed light on how the underlying data distribution shapes the\ntraining dynamics: they rank examples based on their importance for\ngeneralization, detect noisy examples and identify subspaces of the model's\ndata representation that are relatively stable over training.",
          "link": "http://arxiv.org/abs/2107.07075",
          "publishedOn": "2021-07-16T00:48:25.464Z",
          "wordCount": 650,
          "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training. (arXiv:2107.07075v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>",
          "description": "Robust physics discovery is of great interest for many scientific and\nengineering fields. Inspired by the principle that a representative model is\nthe one simplest possible, a new model selection criteria considering both\nmodel's Parsimony and Sparsity is proposed. A Parsimony Enhanced Sparse\nBayesian Learning (PeSBL) method is developed for discovering the governing\nPartial Differential Equations (PDEs) of nonlinear dynamical systems. Compared\nwith the conventional Sparse Bayesian Learning (SBL) method, the PeSBL method\npromotes parsimony of the learned model in addition to its sparsity. In this\nmethod, the parsimony of model terms is evaluated using their locations in the\nprescribed candidate library, for the first time, considering the increased\ncomplexity with the power of polynomials and the order of spatial derivatives.\nSubsequently, the model parameters are updated through Bayesian inference with\nthe raw data. This procedure aims to reduce the error associated with the\npossible loss of information in data preprocessing and numerical\ndifferentiation prior to sparse regression. Results of numerical case studies\nindicate that the governing PDEs of many canonical dynamical systems can be\ncorrectly identified using the proposed PeSBL method from highly noisy data (up\nto 50% in the current study). Next, the proposed methodology is extended for\nstochastic PDE learning where all parameters and modeling error are considered\nas random variables. Hierarchical Bayesian Inference (HBI) is integrated with\nthe proposed framework for stochastic PDE learning from a population of\nobservations. Finally, the proposed PeSBL is demonstrated for system response\nprediction with uncertainties and anomaly diagnosis. Codes of all demonstrated\nexamples in this study are available on the website: https://github.com/ymlasu.",
          "link": "http://arxiv.org/abs/2107.07040",
          "publishedOn": "2021-07-16T00:48:25.449Z",
          "wordCount": 704,
          "title": "Parsimony-Enhanced Sparse Bayesian Learning for Robust Discovery of Partial Differential Equations. (arXiv:2107.07040v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zohdinasab_T/0/1/0/all/0/1\">Tahereh Zohdinasab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccio_V/0/1/0/all/0/1\">Vincenzo Riccio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gambi_A/0/1/0/all/0/1\">Alessio Gambi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonella_P/0/1/0/all/0/1\">Paolo Tonella</a>",
          "description": "Deep Learning (DL) has been successfully applied to a wide range of\napplication domains, including safety-critical ones. Several DL testing\napproaches have been recently proposed in the literature but none of them aims\nto assess how different interpretable features of the generated inputs affect\nthe system's behaviour. In this paper, we resort to Illumination Search to find\nthe highest-performing test cases (i.e., misbehaving and closest to\nmisbehaving), spread across the cells of a map representing the feature space\nof the system. We introduce a methodology that guides the users of our approach\nin the tasks of identifying and quantifying the dimensions of the feature space\nfor a given domain. We developed DeepHyperion, a search-based tool for DL\nsystems that illuminates, i.e., explores at large, the feature space, by\nproviding developers with an interpretable feature map where automatically\ngenerated inputs are placed along with information about the exposed\nbehaviours.",
          "link": "http://arxiv.org/abs/2107.06997",
          "publishedOn": "2021-07-16T00:48:25.429Z",
          "wordCount": 629,
          "title": "DeepHyperion: Exploring the Feature Space of Deep Learning-Based Systems through Illumination Search. (arXiv:2107.06997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07271",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1\">Andrew Moyes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1\">Richard Gault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1\">Ji Ming</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1\">Danny Crookes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>",
          "description": "Domain shift is a problem commonly encountered when developing automated\nhistopathology pipelines. The performance of machine learning models such as\nconvolutional neural networks within automated histopathology pipelines is\noften diminished when applying them to novel data domains due to factors\narising from differing staining and scanning protocols. The Dual-Channel\nAuto-Encoder (DCAE) model was previously shown to produce feature\nrepresentations that are less sensitive to appearance variation introduced by\ndifferent digital slide scanners. In this work, the Multi-Channel Auto-Encoder\n(MCAE) model is presented as an extension to DCAE which learns from more than\ntwo domains of data. Additionally, a synthetic dataset is generated using\nCycleGANs that contains aligned tissue images that have had their appearance\nsynthetically modified. Experimental results show that the MCAE model produces\nfeature representations that are less sensitive to inter-domain variations than\nthe comparative StaNoSA method when tested on the novel synthetic data.\nAdditionally, the MCAE and StaNoSA models are tested on a novel tissue\nclassification task. The results of this experiment show the MCAE model out\nperforms the StaNoSA model by 5 percentage-points in the f1-score. These\nresults show that the MCAE model is able to generalise better to novel data and\ntasks than existing approaches by actively learning normalised feature\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.07271",
          "publishedOn": "2021-07-16T00:48:25.382Z",
          "wordCount": 669,
          "title": "Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.06572",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cohen_M/0/1/0/all/0/1\">Michael B. Cohen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sidford_A/0/1/0/all/0/1\">Aaron Sidford</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tian_K/0/1/0/all/0/1\">Kevin Tian</a>",
          "description": "We show that standard extragradient methods (i.e. mirror prox and dual\nextrapolation) recover optimal accelerated rates for first-order minimization\nof smooth convex functions. To obtain this result we provide a fine-grained\ncharacterization of the convergence rates of extragradient methods for solving\nmonotone variational inequalities in terms of a natural condition we call\nrelative Lipschitzness. We further generalize this framework to handle local\nand randomized notions of relative Lipschitzness and thereby recover rates for\nbox-constrained $\\ell_\\infty$ regression based on area convexity and complexity\nbounds achieved by accelerated (randomized) coordinate descent for smooth\nconvex function minimization.",
          "link": "http://arxiv.org/abs/2011.06572",
          "publishedOn": "2021-07-16T00:48:25.365Z",
          "wordCount": 581,
          "title": "Relative Lipschitzness in Extragradient Methods and a Direct Recipe for Acceleration. (arXiv:2011.06572v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neri_F/0/1/0/all/0/1\">Filippo Neri</a>",
          "description": "In the paper, we propose a novel methodology to map learning algorithms on\ndata (performance map) in order to gain more insights in the distribution of\ntheir performances across their parameter space. This methodology provides\nuseful information when selecting a learner's best configuration for the data\nat hand, and it also enhances the comparison of learners across learning\ncontexts. In order to explain the proposed methodology, the study introduces\nthe notions of learning context, performance map, and high performance\nfunction. It then applies these concepts to a variety of learning contexts to\nshow how their use can provide more insights in a learner's behavior, and can\nenhance the comparison of learners across learning contexts. The study is\ncompleted by an extensive experimental study describing how the proposed\nmethodology can be applied.",
          "link": "http://arxiv.org/abs/2107.06981",
          "publishedOn": "2021-07-16T00:48:25.343Z",
          "wordCount": 577,
          "title": "Mapping Learning Algorithms on Data, a useful step for optimizing performances and their comparison. (arXiv:2107.06981v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:25.336Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07412",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sahmoud_T/0/1/0/all/0/1\">Thaer Sahmoud</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashor_W/0/1/0/all/0/1\">Wesam Ashor</a>",
          "description": "Gaza Strip suffers from a chronic electricity deficit that affects all\nindustries including the telecommunication field, so there is a need to\noptimize and reduce power consumption of the telecommunication equipment. In\nthis paper we propose a new model that helps GSM radio frequency engineers to\nchoose the optimal value of hysteresis parameter for Ericsson BTS power saving\nalgorithm which aims to switch OFF unused frequency channels, our model is\nbased on unsupervised machine learning clustering K-means algorithm. By using\nour model with BTS power saving algorithm we reduce number of active TRX by\n20.9%.",
          "link": "http://arxiv.org/abs/2107.07412",
          "publishedOn": "2021-07-16T00:48:25.316Z",
          "wordCount": 543,
          "title": "Assign Hysteresis Parameter For Ericsson BTS Power Saving Algorithm Using Unsupervised Learning. (arXiv:2107.07412v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basioti_K/0/1/0/all/0/1\">Kalliopi Basioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustakides_G/0/1/0/all/0/1\">George V. Moustakides</a>",
          "description": "We are interested in the design of generative networks. The training of these\nmathematical structures is mostly performed with the help of adversarial\n(min-max) optimization problems. We propose a simple methodology for\nconstructing such problems assuring, at the same time, consistency of the\ncorresponding solution. We give characteristic examples developed by our\nmethod, some of which can be recognized from other applications, and some are\nintroduced here for the first time. We present a new metric, the likelihood\nratio, that can be employed online to examine the convergence and stability\nduring the training of different Generative Adversarial Networks (GANs).\nFinally, we compare various possibilities by applying them to well-known\ndatasets using neural networks of different configurations and sizes.",
          "link": "http://arxiv.org/abs/2002.00865",
          "publishedOn": "2021-07-16T00:48:25.301Z",
          "wordCount": 595,
          "title": "Designing GANs: A Likelihood Ratio Approach. (arXiv:2002.00865v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "We propose a generative adversarial network with multiple discriminators,\nwhere each discriminator is specialized to distinguish the subset of a real\ndataset. This approach facilitates learning a generator coinciding with the\nunderlying data distribution and thus mitigates the chronic mode collapse\nproblem. From the inspiration of multiple choice learning, we guide each\ndiscriminator to have expertise in the subset of the entire data and allow the\ngenerator to find reasonable correspondences between the latent and real data\nspaces automatically without supervision for training examples and the number\nof discriminators. Despite the use of multiple discriminators, the backbone\nnetworks are shared across the discriminators and the increase of training cost\nis minimized. We demonstrate the effectiveness of our algorithm in the standard\ndatasets using multiple evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.07260",
          "publishedOn": "2021-07-16T00:48:25.283Z",
          "wordCount": 551,
          "title": "MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators. (arXiv:2107.07260v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07312",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1\">Chong Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenda Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vishwakarma_S/0/1/0/all/0/1\">Shelly Vishwakarma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_F/0/1/0/all/0/1\">Fangzhan Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Julier_S/0/1/0/all/0/1\">Simon Julier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chetty_K/0/1/0/all/0/1\">Kevin Chetty</a>",
          "description": "Micro-Doppler signatures contain considerable information about target\ndynamics. However, the radar sensing systems are easily affected by noisy\nsurroundings, resulting in uninterpretable motion patterns on the micro-Doppler\nspectrogram. Meanwhile, radar returns often suffer from multipath, clutter and\ninterference. These issues lead to difficulty in, for example motion feature\nextraction, activity classification using micro Doppler signatures ($\\mu$-DS),\netc. In this paper, we propose a latent feature-wise mapping strategy, called\nFeature Mapping Network (FMNet), to transform measured spectrograms so that\nthey more closely resemble the output from a simulation under the same\nconditions. Based on measured spectrogram and the matched simulated data, our\nframework contains three parts: an Encoder which is used to extract latent\nrepresentations/features, a Decoder outputs reconstructed spectrogram according\nto the latent features, and a Discriminator minimizes the distance of latent\nfeatures of measured and simulated data. We demonstrate the FMNet with six\nactivities data and two experimental scenarios, and final results show strong\nenhanced patterns and can keep actual motion information to the greatest\nextent. On the other hand, we also propose a novel idea which trains a\nclassifier with only simulated data and predicts new measured samples after\ncleaning them up with the FMNet. From final classification results, we can see\nsignificant improvements.",
          "link": "http://arxiv.org/abs/2107.07312",
          "publishedOn": "2021-07-16T00:48:25.277Z",
          "wordCount": 654,
          "title": "FMNet: Latent Feature-wise Mapping Network for Cleaning up Noisy Micro-Doppler Spectrogram. (arXiv:2107.07312v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haiyong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuechun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yida Zhu</a>",
          "description": "Human activity recognition (HAR) based on IMU sensors is an essential domain\nin ubiquitous computing. Because of the improving trend to deploy artificial\nintelligence into IoT devices or smartphones, more researchers design the HAR\nmodels for embedded devices. We propose a plug-and-play HAR modeling pipeline\nwith multi-level distillation to build deep convolutional HAR models with\nnative support of embedded devices. SMLDist consists of stage distillation,\nmemory distillation, and logits distillation, which covers all the information\nflow of the deep models. Stage distillation constrains the learning direction\nof the intermediate features. Memory distillation teaches the student models\nhow to explain and store the inner relationship between high-dimensional\nfeatures based on Hopfield networks. Logits distillation constructs distilled\nlogits by a smoothed conditional rule to keep the probable distribution and\nimprove the correctness of the soft target. We compare the performance of\naccuracy, F1 macro score, and energy cost on the embedded platform of various\nstate-of-the-art HAR frameworks with a MobileNet V3 model built by SMLDist. The\nproduced model has well balance with robustness, efficiency, and accuracy.\nSMLDist can also compress the models with minor performance loss in an equal\ncompression rate than other state-of-the-art knowledge distillation methods on\nseven public datasets.",
          "link": "http://arxiv.org/abs/2107.07331",
          "publishedOn": "2021-07-16T00:48:25.265Z",
          "wordCount": 674,
          "title": "Modeling Accurate Human Activity Recognition for Embedded Devices Using Multi-level Distillation. (arXiv:2107.07331v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1\">Amirreza Yousefzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1\">Manolis Sifalakis</a>",
          "description": "Activation sparsity improves compute efficiency and resource utilization in\nsparsity-aware neural network accelerators. As the predominant operation in\nDNNs is multiply-accumulate (MAC) of activations with weights to compute inner\nproducts, skipping operations where (at least) one of the two operands is zero\ncan make inference more efficient in terms of latency and power. Spatial\nsparsification of activations is a popular topic in DNN literature and several\nmethods have already been established to bias a DNN for it. On the other hand,\ntemporal sparsity is an inherent feature of bio-inspired spiking neural\nnetworks (SNNs), which neuromorphic processing exploits for hardware\nefficiency. Introducing and exploiting spatio-temporal sparsity, is a topic\nmuch less explored in DNN literature, but in perfect resonance with the trend\nin DNN, to shift from static signal processing to more streaming signal\nprocessing. Towards this goal, in this paper we introduce a new DNN layer\n(called Delta Activation Layer), whose sole purpose is to promote temporal\nsparsity of activations during training. A Delta Activation Layer casts\ntemporal sparsity into spatial activation sparsity to be exploited when\nperforming sparse tensor multiplications in hardware. By employing delta\ninference and ``the usual'' spatial sparsification heuristics during training,\nthe resulting model learns to exploit not only spatial but also temporal\nactivation sparsity (for a given input data distribution). One may use the\nDelta Activation Layer either during vanilla training or during a refinement\nphase. We have implemented Delta Activation Layer as an extension of the\nstandard Tensoflow-Keras library, and applied it to train deep neural networks\non the Human Action Recognition (UCF101) dataset. We report an almost 3x\nimprovement of activation sparsity, with recoverable loss of model accuracy\nafter longer training.",
          "link": "http://arxiv.org/abs/2107.07305",
          "publishedOn": "2021-07-16T00:48:25.258Z",
          "wordCount": 728,
          "title": "Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1\">L&#xea;-Nguy&#xea;n Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faucon_L/0/1/0/all/0/1\">Louis Faucon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jungo_A/0/1/0/all/0/1\">Aidan Jungo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volodin_S/0/1/0/all/0/1\">Sergei Volodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papuc_D/0/1/0/all/0/1\">Dalia Papuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liossatos_O/0/1/0/all/0/1\">Orfeas Liossatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crulis_B/0/1/0/all/0/1\">Ben Crulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighanimine_M/0/1/0/all/0/1\">Mariame Tighanimine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantin_I/0/1/0/all/0/1\">Isabela Constantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucherenko_A/0/1/0/all/0/1\">Anastasiia Kucherenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1\">Alexandre Maurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimberg_F/0/1/0/all/0/1\">Felix Grimberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitu_V/0/1/0/all/0/1\">Vlad Nitu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vossen_C/0/1/0/all/0/1\">Chris Vossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1\">S&#xe9;bastien Rouault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Mhamdi_E/0/1/0/all/0/1\">El-Mahdi El-Mhamdi</a>",
          "description": "Today's large-scale algorithms have become immensely influential, as they\nrecommend and moderate the content that billions of humans are exposed to on a\ndaily basis. They are the de-facto regulators of our societies' information\ndiet, from shaping opinions on public health to organizing groups for social\nmovements. This creates serious concerns, but also great opportunities to\npromote quality information. Addressing the concerns and seizing the\nopportunities is a challenging, enormous and fabulous endeavor, as intuitively\nappealing ideas often come with unwanted {\\it side effects}, and as it requires\nus to think about what we deeply prefer.\n\nUnderstanding how today's large-scale algorithms are built is critical to\ndetermine what interventions will be most effective. Given that these\nalgorithms rely heavily on {\\it machine learning}, we make the following key\nobservation: \\emph{any algorithm trained on uncontrolled data must not be\ntrusted}. Indeed, a malicious entity could take control over the data, poison\nit with dangerously manipulative fabricated inputs, and thereby make the\ntrained algorithm extremely unsafe. We thus argue that the first step towards\nsafe and ethical large-scale algorithms must be the collection of a large,\nsecure and trustworthy dataset of reliable human judgments.\n\nTo achieve this, we introduce \\emph{Tournesol}, an open source platform\navailable at \\url{https://tournesol.app}. Tournesol aims to collect a large\ndatabase of human judgments on what algorithms ought to widely recommend (and\nwhat they ought to stop widely recommending). We outline the structure of the\nTournesol database, the key features of the Tournesol platform and the main\nhurdles that must be overcome to make it a successful project. Most\nimportantly, we argue that, if successful, Tournesol may then serve as the\nessential foundation for any safe and ethical large-scale algorithm.",
          "link": "http://arxiv.org/abs/2107.07334",
          "publishedOn": "2021-07-16T00:48:25.252Z",
          "wordCount": 766,
          "title": "Tournesol: A quest for a large, secure and trustworthy database of reliable human judgments. (arXiv:2107.07334v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07322",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyu Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1\">Ruodu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "In bandit multiple hypothesis testing, each arm corresponds to a different\nnull hypothesis that we wish to test, and the goal is to design adaptive\nalgorithms that correctly identify large set of interesting arms (true\ndiscoveries), while only mistakenly identifying a few uninteresting ones (false\ndiscoveries). One common metric in non-bandit multiple testing is the false\ndiscovery rate (FDR). We propose a unified, modular framework for bandit FDR\ncontrol that emphasizes the decoupling of exploration and summarization of\nevidence. We utilize the powerful martingale-based concept of ``e-processes''\nto ensure FDR control for arbitrary composite nulls, exploration rules and\nstopping times in generic problem settings. In particular, valid FDR control\nholds even if the reward distributions of the arms could be dependent, multiple\narms may be queried simultaneously, and multiple (cooperating or competing)\nagents may be querying arms, covering combinatorial semi-bandit type settings\nas well. Prior work has considered in great detail the setting where each arm's\nreward distribution is independent and sub-Gaussian, and a single arm is\nqueried at each step. Our framework recovers matching sample complexity\nguarantees in this special case, and performs comparably or better in practice.\nFor other settings, sample complexities will depend on the finer details of the\nproblem (composite nulls being tested, exploration algorithm, data dependence\nstructure, stopping rule) and we do not explore these; our contribution is to\nshow that the FDR guarantee is clean and entirely agnostic to these details.",
          "link": "http://arxiv.org/abs/2107.07322",
          "publishedOn": "2021-07-16T00:48:25.245Z",
          "wordCount": 675,
          "title": "A unified framework for bandit multiple testing. (arXiv:2107.07322v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1\">Lennart Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfisterer_F/0/1/0/all/0/1\">Florian Pfisterer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_M/0/1/0/all/0/1\">Martin Binder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>",
          "description": "Neural architecture search (NAS) promises to make deep learning accessible to\nnon-experts by automating architecture engineering of deep neural networks.\nBANANAS is one state-of-the-art NAS method that is embedded within the Bayesian\noptimization framework. Recent experimental findings have demonstrated the\nstrong performance of BANANAS on the NAS-Bench-101 benchmark being determined\nby its path encoding and not its choice of surrogate model. We present\nexperimental results suggesting that the performance of BANANAS on the\nNAS-Bench-301 benchmark is determined by its acquisition function optimizer,\nwhich minimally mutates the incumbent.",
          "link": "http://arxiv.org/abs/2107.07343",
          "publishedOn": "2021-07-16T00:48:25.203Z",
          "wordCount": 535,
          "title": "Mutation is all you need. (arXiv:2107.07343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The increasing population of elderly people is associated with the need to\nmeet their increasing requirements and to provide solutions that can improve\ntheir quality of life in a smart home. In addition to fear and anxiety towards\ninterfacing with systems; cognitive disabilities, weakened memory, disorganized\nbehavior and even physical limitations are some of the problems that elderly\npeople tend to face with increasing age. The essence of providing\ntechnology-based solutions to address these needs of elderly people and to\ncreate smart and assisted living spaces for the elderly; lies in developing\nsystems that can adapt by addressing their diversity and can augment their\nperformances in the context of their day to day goals. Therefore, this work\nproposes a framework for development of a Personalized Intelligent Assistant to\nhelp elderly people perform Activities of Daily Living (ADLs) in a smart and\nconnected Internet of Things (IoT) based environment. This Personalized\nIntelligent Assistant can analyze different tasks performed by the user and\nrecommend activities by considering their daily routine, current affective\nstate and the underlining user experience. To uphold the efficacy of this\nproposed framework, it has been tested on a couple of datasets for modelling an\naverage user and a specific user respectively. The results presented show that\nthe model achieves a performance accuracy of 73.12% when modelling a specific\nuser, which is considerably higher than its performance while modelling an\naverage user, this upholds the relevance for development and implementation of\nthis proposed framework.",
          "link": "http://arxiv.org/abs/2107.07344",
          "publishedOn": "2021-07-16T00:48:25.194Z",
          "wordCount": 731,
          "title": "Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yufeng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingsong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Deep neural networks (DNNs) have successfully learned useful data\nrepresentations in various tasks, however, assessing the reliability of these\nrepresentations remains a challenge. Deep Ensemble is widely considered the\nstate-of-the-art method for uncertainty estimation, but it is very expensive to\ntrain and test. MC-Dropout is another alternative method, which is less\nexpensive but lacks the diversity of predictions. To get more diverse\npredictions in less time, we introduce Randomized ReLU Activation (RRA)\nframework. Under the framework, we propose two strategies, MC-DropReLU and\nMC-RReLU, to estimate uncertainty. Instead of randomly dropping some neurons of\nthe network as in MC-Dropout, the RRA framework adds randomness to the\nactivation function module, making the outputs diverse. As far as we know, this\nis the first attempt to add randomness to the activation function module to\ngenerate predictive uncertainty. We analyze and compare the output diversity of\nMC-Dropout and our method from the variance perspective and obtain the\nrelationship between the hyperparameters and output diversity in the two\nmethods. Moreover, our method is simple to implement and does not need to\nmodify the existing model. We experimentally validate the RRA framework on\nthree widely used datasets, CIFAR10, CIFAR100, and TinyImageNet. The\nexperiments demonstrate that our method has competitive performance but is more\nfavorable in training time and memory requirements.",
          "link": "http://arxiv.org/abs/2107.07197",
          "publishedOn": "2021-07-16T00:48:25.172Z",
          "wordCount": 649,
          "title": "Randomized ReLU Activation for Uncertainty Estimation of Deep Neural Networks. (arXiv:2107.07197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zona Kostic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1\">Aleksandar Jevremovic</a>",
          "description": "The attractiveness of a property is one of the most interesting, yet\nchallenging, categories to model. Image characteristics are used to describe\ncertain attributes, and to examine the influence of visual factors on the price\nor timeframe of the listing. In this paper, we propose a set of techniques for\nthe extraction of visual features for efficient numerical inclusion in\nmodern-day predictive algorithms. We discuss techniques such as Shannon's\nentropy, calculating the center of gravity, employing image segmentation, and\nusing Convolutional Neural Networks. After comparing these techniques as\napplied to a set of property-related images (indoor, outdoor, and satellite),\nwe conclude the following: (i) the entropy is the most efficient single-digit\nvisual measure for housing price prediction; (ii) image segmentation is the\nmost important visual feature for the prediction of housing lifespan; and (iii)\ndeep image features can be used to quantify interior characteristics and\ncontribute to captivation modeling. The set of 40 image features selected here\ncarries a significant amount of predictive power and outperforms some of the\nstrongest metadata predictors. Without any need to replace a human expert in a\nreal-estate appraisal process, we conclude that the techniques presented in\nthis paper can efficiently describe visible characteristics, thus introducing\nperceived attractiveness as a quantitative measure into the predictive modeling\nof housing.",
          "link": "http://arxiv.org/abs/2107.07148",
          "publishedOn": "2021-07-16T00:48:25.154Z",
          "wordCount": 649,
          "title": "What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1\">Alex Egg</a>",
          "description": "We propose a method to easily modify existing offline Recommender Systems to\nrun online using Transfer Learning. Online Learning for Recommender Systems has\ntwo main advantages: quality and scale. Like many Machine Learning algorithms\nin production if not regularly retrained will suffer from Concept Drift. A\npolicy that is updated frequently online can adapt to drift faster than a batch\nsystem. This is especially true for user-interaction systems like recommenders\nwhere the underlying distribution can shift drastically to follow user\nbehaviour. As a platform grows rapidly like Grubhub, the cost of running batch\ntraining jobs becomes material. A shift from stateless batch learning offline\nto stateful incremental learning online can recover, for example, at Grubhub,\nup to a 45x cost savings and a +20% metrics increase. There are a few\nchallenges to overcome with the transition to online stateful learning, namely\nconvergence, non-stationary embeddings and off-policy evaluation, which we\nexplore from our experiences running this system in production.",
          "link": "http://arxiv.org/abs/2107.07106",
          "publishedOn": "2021-07-16T00:48:25.137Z",
          "wordCount": 583,
          "title": "Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruijuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_F/0/1/0/all/0/1\">Feng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xinlei Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>",
          "description": "Federated learning enables a large number of clients to participate in\nlearning a shared model while maintaining the training data stored in each\nclient, which protects data privacy and security. Till now, federated learning\nframeworks are built in a centralized way, in which a central client is needed\nfor collecting and distributing information from every other client. This not\nonly leads to high communication pressure at the central client, but also\nrenders the central client highly vulnerable to failure and attack. Here we\npropose a principled decentralized federated learning algorithm (DeFed), which\nremoves the central client in the classical Federated Averaging (FedAvg)\nsetting and only relies information transmission between clients and their\nlocal neighbors. The proposed DeFed algorithm is proven to reach the global\nminimum with a convergence rate of $O(1/T)$ when the loss function is smooth\nand strongly convex, where $T$ is the number of iterations in gradient descent.\nFinally, the proposed algorithm has been applied to a number of toy examples to\ndemonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2107.07171",
          "publishedOn": "2021-07-16T00:48:25.121Z",
          "wordCount": 621,
          "title": "DeFed: A Principled Decentralized and Privacy-Preserving Federated Learning Algorithm. (arXiv:2107.07171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kevin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1\">Ashwin Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1\">Vitchyr Pong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aurick Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Justin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Exploration in reinforcement learning is a challenging problem: in the worst\ncase, the agent must search for reward states that could be hidden anywhere in\nthe state space. Can we define a more tractable class of RL problems, where the\nagent is provided with examples of successful outcomes? In this problem\nsetting, the reward function can be obtained automatically by training a\nclassifier to categorize states as successful or not. If trained properly, such\na classifier can not only afford a reward function, but actually provide a\nwell-shaped objective landscape that both promotes progress toward good states\nand provides a calibrated exploration bonus. In this work, we we show that an\nuncertainty aware classifier can solve challenging reinforcement learning\nproblems by both encouraging exploration and provided directed guidance towards\npositive outcomes. We propose a novel mechanism for obtaining these calibrated,\nuncertainty-aware classifiers based on an amortized technique for computing the\nnormalized maximum likelihood (NML) distribution, also showing how these\ntechniques can be made computationally tractable by leveraging tools from\nmeta-learning. We show that the resulting algorithm has a number of intriguing\nconnections to both count-based exploration methods and prior algorithms for\nlearning reward functions, while also providing more effective guidance towards\nthe goal. We demonstrate that our algorithm solves a number of challenging\nnavigation and robotic manipulation tasks which prove difficult or impossible\nfor prior methods.",
          "link": "http://arxiv.org/abs/2107.07184",
          "publishedOn": "2021-07-16T00:48:25.115Z",
          "wordCount": 675,
          "title": "MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning. (arXiv:2107.07184v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1\">Ankur Mali</a>",
          "description": "In humans, perceptual awareness facilitates the fast recognition and\nextraction of information from sensory input. This awareness largely depends on\nhow the human agent interacts with the environment. In this work, we propose\nactive neural generative coding, a computational framework for learning\naction-driven generative models without backpropagation of errors (backprop) in\ndynamic environments. Specifically, we develop an intelligent agent that\noperates even with sparse rewards, drawing inspiration from the cognitive\ntheory of planning as inference. We demonstrate on several control problems, in\nthe online learning setting, that our proposed modeling framework performs\ncompetitively with deep Q-learning models. The robust performance of our agent\noffers promising evidence that a backprop-free approach for neural inference\nand learning can drive goal-directed behavior.",
          "link": "http://arxiv.org/abs/2107.07046",
          "publishedOn": "2021-07-16T00:48:25.095Z",
          "wordCount": 545,
          "title": "Backprop-Free Reinforcement Learning with Active Neural Generative Coding. (arXiv:2107.07046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bailian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harp_D/0/1/0/all/0/1\">Dylan Robert Harp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawar_R/0/1/0/all/0/1\">Rajesh J. Pawar</a>",
          "description": "This paper contributes to the development and evaluation of a deep learning\nworkflow that accurately and efficiently predicts the temporal-spatial\nevolution of pressure and CO2 plumes during injection and post-injection\nperiods of geologic CO2 sequestration (GCS) operations. Based on a Fourier\nNeuron Operator, the deep learning workflow takes input variables or features\nincluding rock properties, well operational controls and time steps, and\npredicts the state variables of pressure and CO2 saturation. To further improve\nthe predictive fidelity, separate deep learning models are trained for CO2\ninjection and post-injection periods due the difference in primary driving\nforce of fluid flow and transport during these two phases. We also explore\ndifferent combinations of features to predict the state variables. We use a\nrealistic example of CO2 injection and storage in a 3D heterogeneous saline\naquifer, and apply the deep learning workflow that is trained from\nphysics-based simulation data and emulate the physics process. Through this\nnumerical experiment, we demonstrate that using two separate deep learning\nmodels to distinguish post-injection from injection period generates the most\naccurate prediction of pressure, and a single deep learning model of the whole\nGCS process including the cumulative injection volume of CO2 as a deep learning\nfeature, leads to the most accurate prediction of CO2 saturation. For the\npost-injection period, it is key to use cumulative CO2 injection volume to\ninform the deep learning models about the total carbon storage when predicting\neither pressure or saturation. The deep learning workflow not only provides\nhigh predictive fidelity across temporal and spatial scales, but also offers a\nspeedup of 250 times compared to full physics reservoir simulation, and thus\nwill be a significant predictive tool for engineers to manage the long term\nprocess of GCS.",
          "link": "http://arxiv.org/abs/2107.07274",
          "publishedOn": "2021-07-16T00:48:25.087Z",
          "wordCount": 747,
          "title": "A Robust Deep Learning Workflow to Predict Multiphase Flow Behavior during Geological CO2 Sequestration Injection and Post-Injection Periods. (arXiv:2107.07274v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1\">Jonah Casebeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.",
          "link": "http://arxiv.org/abs/2105.04727",
          "publishedOn": "2021-07-16T00:48:25.080Z",
          "wordCount": 627,
          "title": "Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1\">Sergio Rojas-Galeano</a>",
          "description": "One of the stratagems used to deceive spam filters is to substitute vocables\nwith synonyms or similar words that turn the message unrecognisable by the\ndetection algorithms. In this paper we investigate whether the recent\ndevelopment of language models sensitive to the semantics and context of words,\nsuch as Google's BERT, may be useful to overcome this adversarial attack\n(called \"Mad-lib\" as per the word substitution game). Using a dataset of 5572\nSMS spam messages, we first established a baseline of detection performance\nusing widely known document representation models (BoW and TFIDF) and the novel\nBERT model, coupled with a variety of classification algorithms (Decision Tree,\nkNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we\nbuilt a thesaurus of the vocabulary contained in these messages, and set up a\nMad-lib attack experiment in which we modified each message of a held out\nsubset of data (not used in the baseline experiment) with different rates of\nsubstitution of original words with synonyms from the thesaurus. Lastly, we\nevaluated the detection performance of the three representation models (BoW,\nTFIDF and BERT) coupled with the best classifier from the baseline experiment\n(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)\nin the original dataset, whereas the BERT model obtained 96%. On the other\nhand, the Mad-lib attack experiment showed that BERT encodings manage to\nmaintain a similar BA performance of 96% with an average substitution rate of\n1.82 words per message, and 95% with 3.34 words substituted per message. In\ncontrast, the BA performance of the BoW and TFIDF encoders dropped to chance.\nThese results hint at the potential advantage of BERT models to combat these\ntype of ingenious attacks, offsetting to some extent for the inappropriate use\nof semantic relationships in language.",
          "link": "http://arxiv.org/abs/2107.06400",
          "publishedOn": "2021-07-16T00:48:25.059Z",
          "wordCount": 736,
          "title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verine_A/0/1/0/all/0/1\">Alexandre Verine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negrevergne_B/0/1/0/all/0/1\">Benjamin Negrevergne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1\">Yann Chevaleyre</a>",
          "description": "An invertible function is bi-Lipschitz if both the function and its inverse\nhave bounded Lipschitz constants. Nowadays, most Normalizing Flows are\nbi-Lipschitz by design or by training to limit numerical errors (among other\nthings). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing\nFlows and identify several target distributions that are difficult to\napproximate using such models. Then, we characterize the expressivity of\nbi-Lipschitz Normalizing Flows by giving several lower bounds on the Total\nVariation distance between these particularly unfavorable distributions and\ntheir best possible approximation. Finally, we discuss potential remedies which\ninclude using more complex latent distributions.",
          "link": "http://arxiv.org/abs/2107.07232",
          "publishedOn": "2021-07-16T00:48:25.051Z",
          "wordCount": 532,
          "title": "On the expressivity of bi-Lipschitz normalizing flows. (arXiv:2107.07232v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1\">Elliot Creager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1\">J&#xf6;rn-Henrik Jacobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>",
          "description": "Learning models that gracefully handle distribution shifts is central to\nresearch on domain generalization, robust optimization, and fairness. A\npromising formulation is domain-invariant learning, which identifies the key\nissue of learning which features are domain-specific versus domain-invariant.\nAn important assumption in this area is that the training examples are\npartitioned into \"domains\" or \"environments\". Our focus is on the more common\nsetting where such partitions are not provided. We propose EIIL, a general\nframework for domain-invariant learning that incorporates Environment Inference\nto directly infer partitions that are maximally informative for downstream\nInvariant Learning. We show that EIIL outperforms invariant learning methods on\nthe CMNIST benchmark without using environment labels, and significantly\noutperforms ERM on worst-group performance in the Waterbirds and CivilComments\ndatasets. Finally, we establish connections between EIIL and algorithmic\nfairness, which enables EIIL to improve accuracy and calibration in a fair\nprediction problem.",
          "link": "http://arxiv.org/abs/2010.07249",
          "publishedOn": "2021-07-16T00:48:25.044Z",
          "wordCount": 626,
          "title": "Environment Inference for Invariant Learning. (arXiv:2010.07249v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1\">Charlotte Bunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Papaxanthos_L/0/1/0/all/0/1\">Laetitia Meng-Papaxanthos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>",
          "description": "Consider a heterogeneous population of points evolving with time. While the\npopulation evolves, both in size and nature, we can observe it periodically,\nthrough snapshots taken at different timestamps. Each of these snapshots is\nformed by sampling points from the population at that time, and then creating\nfeatures to recover point clouds. While these snapshots describe the\npopulation's evolution on aggregate, they do not provide directly insights on\nindividual trajectories. This scenario is encountered in several applications,\nnotably single-cell genomics experiments, tracking of particles, or when\nstudying crowd motion. In this paper, we propose to model that dynamic as\nresulting from the celebrated Jordan-Kinderlehrer-Otto (JKO) proximal scheme.\nThe JKO scheme posits that the configuration taken by a population at time $t$\nis one that trades off a decrease w.r.t. an energy (the model we seek to learn)\npenalized by an optimal transport distance w.r.t. the previous configuration.\nTo that end, we propose JKOnet, a neural architecture that combines an energy\nmodel on measures, with (small) optimal displacements solved with input convex\nneural networks (ICNN). We demonstrate the applicability of our model to\nexplain and predict population dynamics.",
          "link": "http://arxiv.org/abs/2106.06345",
          "publishedOn": "2021-07-16T00:48:25.020Z",
          "wordCount": 644,
          "title": "JKOnet: Proximal Optimal Transport Modeling of Population Dynamics. (arXiv:2106.06345v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04150",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yilin Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+DeJong_J/0/1/0/all/0/1\">Jennifer DeJong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Halverson_T/0/1/0/all/0/1\">Tom Halverson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shuman_D/0/1/0/all/0/1\">David I Shuman</a>",
          "description": "Ranked data sets, where m judges/voters specify a preference ranking of n\nobjects/candidates, are increasingly prevalent in contexts such as political\nelections, computer vision, recommender systems, and bioinformatics. The vote\ncounts for each ranking can be viewed as an n! data vector lying on the\npermutahedron, which is a Cayley graph of the symmetric group with vertices\nlabeled by permutations and an edge when two permutations differ by an adjacent\ntransposition. Leveraging combinatorial representation theory and recent\nprogress in signal processing on graphs, we investigate a novel, scalable\ntransform method to interpret and exploit structure in ranked data. We\nrepresent data on the permutahedron using an overcomplete dictionary of atoms,\neach of which captures both smoothness information about the data (typically\nthe focus of spectral graph decomposition methods in graph signal processing)\nand structural information about the data (typically the focus of symmetry\ndecomposition methods from representation theory). These atoms have a more\nnaturally interpretable structure than any known basis for signals on the\npermutahedron, and they form a Parseval frame, ensuring beneficial numerical\nproperties such as energy preservation. We develop specialized algorithms and\nopen software that take advantage of the symmetry and structure of the\npermutahedron to improve the scalability of the proposed method, making it more\napplicable to the high-dimensional ranked data found in applications.",
          "link": "http://arxiv.org/abs/2103.04150",
          "publishedOn": "2021-07-16T00:48:25.014Z",
          "wordCount": 687,
          "title": "Signal Processing on the Permutahedron: Tight Spectral Frames for Ranked Data Analysis. (arXiv:2103.04150v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01708",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1\">Stephen Y. Zhang</a>",
          "description": "Non-negative matrix and tensor factorisations are a classical tool for\nfinding low-dimensional representations of high-dimensional datasets. In\napplications such as imaging, datasets can be regarded as distributions\nsupported on a space with metric structure. In such a setting, a loss function\nbased on the Wasserstein distance of optimal transportation theory is a natural\nchoice since it incorporates the underlying geometry of the data. We introduce\na general mathematical framework for computing non-negative factorisations of\nboth matrices and tensors with respect to an optimal transport loss. We derive\nan efficient computational method for its solution using a convex dual\nformulation, and demonstrate the applicability of this approach with several\nnumerical illustrations with both matrix and tensor-valued data.",
          "link": "http://arxiv.org/abs/2104.01708",
          "publishedOn": "2021-07-16T00:48:24.998Z",
          "wordCount": 584,
          "title": "A unified framework for non-negative matrix and tensor factorisations with a smoothed Wasserstein loss. (arXiv:2104.01708v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_L/0/1/0/all/0/1\">Lucas F. F. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1\">Vitor C. A. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frances_R/0/1/0/all/0/1\">Regiane S. Kawasaki Franc&#xea;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudencio_R/0/1/0/all/0/1\">Ricardo B. C. Prud&#xea;ncio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1\">Ronnie C. O. Alves</a>",
          "description": "The classification experiments covered by machine learning (ML) are composed\nby two important parts: the data and the algorithm. As they are a fundamental\npart of the problem, both must be considered when evaluating a model's\nperformance against a benchmark. The best classifiers need robust benchmarks to\nbe properly evaluated. For this, gold standard benchmarks such as OpenML-CC18\nare used. However, data complexity is commonly not considered along with the\nmodel during a performance evaluation. Recent studies employ Item Response\nTheory (IRT) as a new approach to evaluating datasets and algorithms, capable\nof evaluating both simultaneously. This work presents a new evaluation\nmethodology based on IRT and Glicko-2, jointly with the decodIRT tool developed\nto guide the estimation of IRT in ML. It explores the IRT as a tool to evaluate\nthe OpenML-CC18 benchmark for its algorithmic evaluation capability and checks\nif there is a subset of datasets more efficient than the original benchmark.\nSeveral classifiers, from classics to ensemble, are also evaluated using the\nIRT models. The Glicko-2 rating system was applied together with IRT to\nsummarize the innate ability and classifiers performance. It was noted that not\nall OpenML-CC18 datasets are really useful for evaluating algorithms, where\nonly 10% were rated as being really difficult. Furthermore, it was verified the\nexistence of a more efficient subset containing only 50% of the original size.\nWhile Randon Forest was singled out as the algorithm with the best innate\nability.",
          "link": "http://arxiv.org/abs/2107.07451",
          "publishedOn": "2021-07-16T00:48:24.981Z",
          "wordCount": 685,
          "title": "Data vs classifiers, who wins?. (arXiv:2107.07451v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.05383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-An Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Siang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Ya-Liang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "We study the XAI (explainable AI) on the face recognition task, particularly\nthe face verification here. Face verification is a crucial task in recent days\nand it has been deployed to plenty of applications, such as access control,\nsurveillance, and automatic personal log-on for mobile devices. With the\nincreasing amount of data, deep convolutional neural networks can achieve very\nhigh accuracy for the face verification task. Beyond exceptional performances,\ndeep face verification models need more interpretability so that we can trust\nthe results they generate. In this paper, we propose a novel similarity metric,\ncalled explainable cosine ($xCos$), that comes with a learnable module that can\nbe plugged into most of the verification models to provide meaningful\nexplanations. With the help of $xCos$, we can see which parts of the two input\nfaces are similar, where the model pays its attention to, and how the local\nsimilarities are weighted to form the output $xCos$ score. We demonstrate the\neffectiveness of our proposed method on LFW and various competitive benchmarks,\nresulting in not only providing novel and desiring model interpretability for\nface verification but also ensuring the accuracy as plugging into existing face\nrecognition models.",
          "link": "http://arxiv.org/abs/2003.05383",
          "publishedOn": "2021-07-16T00:48:24.971Z",
          "wordCount": 699,
          "title": "xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tong_G/0/1/0/all/0/1\">Guangmo Tong</a>",
          "description": "Real-world decision-making systems are often subject to uncertainties that\nhave to be resolved through observational data. Therefore, we are frequently\nconfronted with combinatorial optimization problems of which the objective\nfunction is unknown and thus has to be debunked using empirical evidence. In\ncontrast to the common practice that relies on a learning-and-optimization\nstrategy, we consider the regression between combinatorial spaces, aiming to\ninfer high-quality optimization solutions from samples of input-solution pairs\n-- without the need to learn the objective function. Our main deliverable is a\nuniversal solver that is able to handle abstract undetermined stochastic\ncombinatorial optimization problems. For learning foundations, we present\nlearning-error analysis under the PAC-Bayesian framework using a new\nmargin-based analysis. In empirical studies, we demonstrate our design using\nproof-of-concept experiments, and compare it with other methods that are\npotentially applicable. Overall, we obtain highly encouraging experimental\nresults for several classic combinatorial problems on both synthetic and\nreal-world datasets.",
          "link": "http://arxiv.org/abs/2107.07508",
          "publishedOn": "2021-07-16T00:48:24.957Z",
          "wordCount": 574,
          "title": "USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization Problems. (arXiv:2107.07508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1\">Neeraj Chimmwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>",
          "description": "We present a CLSRIL-23, a self supervised learning based audio pre-trained\nmodel which learns cross lingual speech representations from raw audio across\n23 Indic languages. It is built on top of wav2vec 2.0 which is solved by\ntraining a contrastive task over masked latent speech representations and\njointly learns the quantization of latents shared across all languages. We\ncompare the language wise loss during pretraining to compare effects of\nmonolingual and multilingual pretraining. Performance on some downstream\nfine-tuning tasks for speech recognition is also compared and our experiments\nshow that multilingual pretraining outperforms monolingual training, in terms\nof learning speech representations which encodes phonetic similarity of\nlanguages and also in terms of performance on down stream tasks. A decrease of\n5% is observed in WER and 9.5% in CER when a multilingual pretrained model is\nused for finetuning in Hindi. All the code models are also open sourced.\nCLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio\ndata to facilitate research in speech recognition for Indic languages. We hope\nthat new state of the art systems will be created using the self supervised\napproach, especially for low resources Indic languages.",
          "link": "http://arxiv.org/abs/2107.07402",
          "publishedOn": "2021-07-16T00:48:24.904Z",
          "wordCount": 652,
          "title": "CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1\">Ladislav Ramp&#xe1;&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>",
          "description": "Graph neural networks (GNNs) based on message passing between neighboring\nnodes are known to be insufficient for capturing long-range interactions in\ngraphs. In this project we study hierarchical message passing models that\nleverage a multi-resolution representation of a given graph. This facilitates\nlearning of features that span large receptive fields without loss of local\ninformation, an aspect not studied in preceding work on hierarchical GNNs. We\nintroduce Hierarchical Graph Net (HGNet), which for any two connected nodes\nguarantees existence of message-passing paths of at most logarithmic length\nw.r.t. the input graph size. Yet, under mild assumptions, its internal\nhierarchy maintains asymptotic size equivalent to that of the input graph. We\nobserve that our HGNet outperforms conventional stacking of GCN layers\nparticularly in molecular property prediction benchmarks. Finally, we propose\ntwo benchmarking tasks designed to elucidate capability of GNNs to leverage\nlong-range interactions in graphs.",
          "link": "http://arxiv.org/abs/2107.07432",
          "publishedOn": "2021-07-16T00:48:24.898Z",
          "wordCount": 575,
          "title": "Hierarchical graph neural nets can capture long-range interactions. (arXiv:2107.07432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Han-Chih Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "The development of active and passive biometric authentication and\nidentification technology plays an increasingly important role in\ncybersecurity. Keystroke dynamics can be used to analyze the way that a user\ntypes based on various keyboard input. Previous work has shown that user\nauthentication and classification can be achieved based on keystroke dynamics.\nIn this research, we consider the problem of user classification based on\nkeystroke dynamics features collected from free-text. We implement and analyze\na novel a deep learning model that combines a convolutional neural network\n(CNN) and a gated recurrent unit (GRU). We optimize the resulting model and\nconsider several relevant related problems. Our model is competitive with the\nbest results obtained in previous comparable research.",
          "link": "http://arxiv.org/abs/2107.07409",
          "publishedOn": "2021-07-16T00:48:24.892Z",
          "wordCount": 542,
          "title": "Machine Learning-Based Analysis of Free-Text Keystroke Dynamics. (arXiv:2107.07409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_D/0/1/0/all/0/1\">Dhasarathy Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_A/0/1/0/all/0/1\">Anton Johansson</a>",
          "description": "Automotive software testing continues to rely largely upon expensive field\ntests to ensure quality because alternatives like simulation-based testing are\nrelatively immature. As a step towards lowering reliance on field tests, we\npresent SilGAN, a deep generative model that eases specification, stimulus\ngeneration, and automation of automotive software-in-the-loop testing. The\nmodel is trained using data recorded from vehicles in the field. Upon training,\nthe model uses a concise specification for a driving scenario to generate\nrealistic vehicle state transitions that can occur during such a scenario. Such\nauthentic emulation of internal vehicle behavior can be used for rapid,\nsystematic and inexpensive testing of vehicle control software. In addition, by\npresenting a targeted method for searching through the information learned by\nthe model, we show how a test objective like code coverage can be automated.\nThe data driven end-to-end testing pipeline that we present vastly expands the\nscope and credibility of automotive simulation-based testing. This reduces time\nto market while helping maintain required standards of quality.",
          "link": "http://arxiv.org/abs/2107.07364",
          "publishedOn": "2021-07-16T00:48:24.885Z",
          "wordCount": 613,
          "title": "SilGAN: Generating driving maneuvers for scenario-based software-in-the-loop testing. (arXiv:2107.07364v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_B/0/1/0/all/0/1\">Bijan Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddharth Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruck_J/0/1/0/all/0/1\">Jehoshua Bruck</a>",
          "description": "Consider multiple experts with overlapping expertise working on a\nclassification problem under uncertain input. What constitutes a consistent set\nof opinions? How can we predict the opinions of experts on missing sub-domains?\nIn this paper, we define a framework of to analyze this problem, termed \"expert\ngraphs.\" In an expert graph, vertices represent classes and edges represent\nbinary opinions on the topics of their vertices. We derive necessary conditions\nfor expert graph validity and use them to create \"synthetic experts\" which\ndescribe opinions consistent with the observed opinions of other experts. We\nshow this framework to be equivalent to the well-studied linear ordering\npolytope. We show our conditions are not sufficient for describing all expert\ngraphs on cliques, but are sufficient for cycles.",
          "link": "http://arxiv.org/abs/2107.07054",
          "publishedOn": "2021-07-16T00:48:24.879Z",
          "wordCount": 573,
          "title": "Expert Graphs: Synthesizing New Expertise via Collaboration. (arXiv:2107.07054v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07494",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_T/0/1/0/all/0/1\">Tian Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ren_L/0/1/0/all/0/1\">Lihua Ren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karlsson_N/0/1/0/all/0/1\">Niklas Karlsson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Flores_A/0/1/0/all/0/1\">Aaron Flores</a>",
          "description": "For Verizon MediaDemand Side Platform(DSP), forecasting of ad campaign\nperformance not only feeds key information to the optimization server to allow\nthe system to operate on a high-performance mode, but also produces actionable\ninsights to the advertisers. In this paper, the forecasting problem for CPA\nlines in the middle of the flight is investigated by taking the bidding\nmechanism into account. The proposed methodology generates relationships\nbetween various key performance metrics and optimization signals. It can also\nbe used to estimate the sensitivity of ad campaign performance metrics to the\nadjustments of optimization signal, which is important to the design of a\ncampaign management system. The relationship between advertiser spends and\neffective Cost Per Action(eCPA) is also characterized, which serves as a\nguidance for mid-flight line adjustment to the advertisers. Several practical\nissues in implementation, such as downsampling of the dataset, are also\ndiscussed in the paper. At last, the forecasting results are validated against\nactual deliveries and demonstrates promising accuracy.",
          "link": "http://arxiv.org/abs/2107.07494",
          "publishedOn": "2021-07-16T00:48:24.872Z",
          "wordCount": 605,
          "title": "Mid-flight Forecasting for CPA Lines in Online Advertising. (arXiv:2107.07494v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glynn_P/0/1/0/all/0/1\">Peter W. Glynn</a>",
          "description": "We study the behavior of Thompson sampling from the perspective of weak\nconvergence. In the regime where the gaps between arm means scale as\n$1/\\sqrt{n}$ with the time horizon $n$, we show that the dynamics of Thompson\nsampling evolve according to discrete versions of SDEs and random ODEs. As $n\n\\to \\infty$, we show that the dynamics converge weakly to solutions of the\ncorresponding SDEs and random ODEs. (Recently, Wager and Xu (arXiv:2101.09855)\nindependently proposed this regime and developed similar SDE and random ODE\napproximations for Thompson sampling in the multi-armed bandit setting.) Our\nweak convergence theory, which covers both multi-armed and linear bandit\nsettings, is developed from first principles using the Continuous Mapping\nTheorem and can be directly adapted to analyze other sampling-based bandit\nalgorithms, for example, algorithms using the bootstrap for exploration. We\nalso establish an invariance principle for multi-armed bandits with gaps\nscaling as $1/\\sqrt{n}$ -- for Thompson sampling and related algorithms\ninvolving posterior approximation or the bootstrap, the weak diffusion limits\nare in general the same regardless of the specifics of the reward distributions\nor the choice of prior. In particular, as suggested by the classical\nBernstein-von Mises normal approximation for posterior distributions, the weak\ndiffusion limits generally coincide with the limit for normally-distributed\nrewards and priors.",
          "link": "http://arxiv.org/abs/2105.09232",
          "publishedOn": "2021-07-16T00:48:24.856Z",
          "wordCount": 671,
          "title": "Diffusion Approximations for Thompson Sampling. (arXiv:2105.09232v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12382",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1\">Lang Tong</a>",
          "description": "An innovations sequence of a time series is a sequence of independent and\nidentically distributed random variables with which the original time series\nhas a causal representation. The innovation at a time is statistically\nindependent of the history of the time series. As such, it represents the new\ninformation contained at present but not in the past. Because of its simple\nprobability structure, an innovations sequence is the most efficient signature\nof the original. Unlike the principle or independent component analysis\nrepresentations, an innovations sequence preserves not only the complete\nstatistical properties but also the temporal order of the original time series.\nAn long-standing open problem is to find a computationally tractable way to\nextract an innovations sequence of non-Gaussian processes. This paper presents\na deep learning approach, referred to as Innovations Autoencoder (IAE), that\nextracts innovations sequences using a causal convolutional neural network. An\napplication of IAE to the one-class anomalous sequence detection problem with\nunknown anomaly and anomaly-free models is also presented.",
          "link": "http://arxiv.org/abs/2106.12382",
          "publishedOn": "2021-07-16T00:48:24.848Z",
          "wordCount": 616,
          "title": "Innovations Autoencoder and its Application in One-class Anomalous Sequence Detection. (arXiv:2106.12382v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1\">Mauricio Mendez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1\">Ivan Garcia Jorge Gonzalez-Zapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1\">Andres Mendez-Vazquez</a>",
          "description": "Few-shot learning is a relatively new technique that specializes in problems\nwhere we have little amounts of data. The goal of these methods is to classify\ncategories that have not been seen before with just a handful of samples.\nRecent approaches, such as metric learning, adopt the meta-learning strategy in\nwhich we have episodic tasks conformed by support (training) data and query\n(test) data. Metric learning methods have demonstrated that simple models can\nachieve good performance by learning a similarity function to compare the\nsupport and the query data. However, the feature space learned by a given\nmetric learning approach may not exploit the information given by a specific\nfew-shot task. In this work, we explore the use of dimension reduction\ntechniques as a way to find task-significant features helping to make better\npredictions. We measure the performance of the reduced features by assigning a\nscore based on the intra-class and inter-class distance, and selecting a\nfeature reduction method in which instances of different classes are far away\nand instances of the same class are close. This module helps to improve the\naccuracy performance by allowing the similarity function, given by the metric\nlearning method, to have more discriminative features for the classification.\nOur method outperforms the metric learning baselines in the miniImageNet\ndataset by around 2% in accuracy performance.",
          "link": "http://arxiv.org/abs/2107.06992",
          "publishedOn": "2021-07-16T00:48:24.841Z",
          "wordCount": 677,
          "title": "Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Alarcon_Y/0/1/0/all/0/1\">Yonatan Carlos Carranza Alarc&#xf3;n</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Destercke_S/0/1/0/all/0/1\">S&#xe9;bastien Destercke</a>",
          "description": "We present two different strategies to extend the classical multi-label\nchaining approach to handle imprecise probability estimates. These estimates\nuse convex sets of distributions (or credal sets) in order to describe our\nuncertainty rather than a precise one. The main reasons one could have for\nusing such estimations are (1) to make cautious predictions (or no decision at\nall) when a high uncertainty is detected in the chaining and (2) to make better\nprecise predictions by avoiding biases caused in early decisions in the\nchaining. Through the use of the naive credal classifier, we propose efficient\nprocedures with theoretical justifications to solve both strategies. Our\nexperimental results on missing labels, which investigate how reliable these\npredictions are in both approaches, indicate that our approaches produce\nrelevant cautiousness on those hard-to-predict instances where the precise\nmodels fail.",
          "link": "http://arxiv.org/abs/2107.07443",
          "publishedOn": "2021-07-16T00:48:24.835Z",
          "wordCount": 566,
          "title": "Multi-label Chaining with Imprecise Probabilities. (arXiv:2107.07443v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulinski_S/0/1/0/all/0/1\">Sean Kulinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inouye_D/0/1/0/all/0/1\">David I. Inouye</a>",
          "description": "While previous distribution shift detection approaches can identify if a\nshift has occurred, these approaches cannot localize which specific features\nhave caused a distribution shift -- a critical step in diagnosing or fixing any\nunderlying issue. For example, in military sensor networks, users will want to\ndetect when one or more of the sensors has been compromised, and critically,\nthey will want to know which specific sensors might be compromised. Thus, we\nfirst define a formalization of this problem as multiple conditional\ndistribution hypothesis tests and propose both non-parametric and parametric\nstatistical tests. For both efficiency and flexibility, we then propose to use\na test statistic based on the density model score function (i.e. gradient with\nrespect to the input) -- which can easily compute test statistics for all\ndimensions in a single forward and backward pass. Any density model could be\nused for computing the necessary statistics including deep density models such\nas normalizing flows or autoregressive models. We additionally develop methods\nfor identifying when and where a shift occurs in multivariate time-series data\nand show results for multiple scenarios using realistic attack models on both\nsimulated and real world data.",
          "link": "http://arxiv.org/abs/2107.06929",
          "publishedOn": "2021-07-16T00:48:24.821Z",
          "wordCount": 645,
          "title": "Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests. (arXiv:2107.06929v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yikun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>",
          "description": "Online advertising is ubiquitous in web business. Image displaying is\nconsidered as one of the most commonly used formats to interact with customers.\nContextual multi-armed bandit has shown success in the application of\nadvertising to solve the exploration-exploitation dilemma existed in the\nrecommendation procedure. Inspired by the visual-aware advertising, in this\npaper, we propose a contextual bandit algorithm, where the convolutional neural\nnetwork (CNN) is utilized to learn the reward function along with an upper\nconfidence bound (UCB) for exploration. We also prove a near-optimal regret\nbound $\\tilde{\\mathcal{O}}(\\sqrt{T})$ when the network is over-parameterized\nand establish strong connections with convolutional neural tangent kernel\n(CNTK). Finally, we evaluate the empirical performance of the proposed\nalgorithm and show that it outperforms other state-of-the-art UCB-based bandit\nalgorithms on real-world image data sets.",
          "link": "http://arxiv.org/abs/2107.07438",
          "publishedOn": "2021-07-16T00:48:24.807Z",
          "wordCount": 565,
          "title": "Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1\">Dobrik Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1\">Dmitry Kazhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>",
          "description": "Recent research on graph neural network (GNN) models successfully applied\nGNNs to classical graph algorithms and combinatorial optimisation problems.\nThis has numerous benefits, such as allowing applications of algorithms when\npreconditions are not satisfied, or reusing learned models when sufficient\ntraining data is not available or can't be generated. Unfortunately, a key\nhindrance of these approaches is their lack of explainability, since GNNs are\nblack-box models that cannot be interpreted directly. In this work, we address\nthis limitation by applying existing work on concept-based explanations to GNN\nmodels. We introduce concept-bottleneck GNNs, which rely on a modification to\nthe GNN readout mechanism. Using three case studies we demonstrate that: (i)\nour proposed model is capable of accurately learning concepts and extracting\npropositional formulas based on the learned concepts for each target class;\n(ii) our concept-based GNN models achieve comparative performance with\nstate-of-the-art models; (iii) we can derive global graph concepts, without\nexplicitly providing any supervision on graph-level concepts.",
          "link": "http://arxiv.org/abs/2107.07493",
          "publishedOn": "2021-07-16T00:48:24.799Z",
          "wordCount": 584,
          "title": "Algorithmic Concept-based Explainable Reasoning. (arXiv:2107.07493v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1\">Vijay Keswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1\">L. Elisa Celis</a>",
          "description": "Assessing the diversity of a dataset of information associated with people is\ncrucial before using such data for downstream applications. For a given\ndataset, this often involves computing the imbalance or disparity in the\nempirical marginal distribution of a protected attribute (e.g. gender, dialect,\netc.). However, real-world datasets, such as images from Google Search or\ncollections of Twitter posts, often do not have protected attributes labeled.\nConsequently, to derive disparity measures for such datasets, the elements need\nto hand-labeled or crowd-annotated, which are expensive processes.\n\nWe propose a cost-effective approach to approximate the disparity of a given\nunlabeled dataset, with respect to a protected attribute, using a control set\nof labeled representative examples. Our proposed algorithm uses the pairwise\nsimilarity between elements in the dataset and elements in the control set to\neffectively bootstrap an approximation to the disparity of the dataset.\nImportantly, we show that using a control set whose size is much smaller than\nthe size of the dataset is sufficient to achieve a small approximation error.\nFurther, based on our theoretical framework, we also provide an algorithm to\nconstruct adaptive control sets that achieve smaller approximation errors than\nrandomly chosen control sets. Simulations on two image datasets and one Twitter\ndataset demonstrate the efficacy of our approach (using random and adaptive\ncontrol sets) in auditing the diversity of a wide variety of datasets.",
          "link": "http://arxiv.org/abs/2107.07393",
          "publishedOn": "2021-07-16T00:48:24.792Z",
          "wordCount": 659,
          "title": "Auditing for Diversity using Representative Examples. (arXiv:2107.07393v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valdes_G/0/1/0/all/0/1\">Gilmer Valdes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelo_W/0/1/0/all/0/1\">Wilmer Arbelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Interian_Y/0/1/0/all/0/1\">Yannet Interian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1\">Jerome H. Friedman</a>",
          "description": "Many regression and classification procedures fit a parameterized function\n$f(x;w)$ of predictor variables $x$ to data $\\{x_{i},y_{i}\\}_1^N$ based on some\nloss criterion $L(y,f)$. Often, regularization is applied to improve accuracy\nby placing a constraint $P(w)\\leq t$ on the values of the parameters $w$.\nAlthough efficient methods exist for finding solutions to these constrained\noptimization problems for all values of $t\\geq0$ in the special case when $f$\nis a linear function, none are available when $f$ is non-linear (e.g. Neural\nNetworks). Here we present a fast algorithm that provides all such solutions\nfor any differentiable function $f$ and loss $L$, and any constraint $P$ that\nis an increasing monotone function of the absolute value of each parameter.\nApplications involving sparsity inducing regularization of arbitrary Neural\nNetworks are discussed. Empirical results indicate that these sparse solutions\nare usually superior to their dense counterparts in both accuracy and\ninterpretability. This improvement in accuracy can often make Neural Networks\ncompetitive with, and sometimes superior to, state-of-the-art methods in the\nanalysis of tabular data.",
          "link": "http://arxiv.org/abs/2107.07160",
          "publishedOn": "2021-07-16T00:48:24.785Z",
          "wordCount": 601,
          "title": "Lockout: Sparse Regularization of Neural Networks. (arXiv:2107.07160v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1\">Han shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip L.H. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Transformer-based pre-trained language models like BERT and its variants have\nrecently achieved promising performance in various natural language processing\n(NLP) tasks. However, the conventional paradigm constructs the backbone by\npurely stacking the manually designed global self-attention layers, introducing\ninductive bias and thus leading to sub-optimal. In this work, we propose an\nOperation-Priority Neural Architecture Search (OP-NAS) algorithm to\nautomatically search for promising hybrid backbone architectures. Our\nwell-designed search space (i) contains primitive math operations in the\nintra-layer level to explore novel attention structures, and (ii) leverages\nconvolution blocks to be the supplementary for attention structure in the\ninter-layer level to better learn local dependency. We optimize both the search\nalgorithm and evaluation of candidate models to boost the efficiency of our\nproposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution\nstrategy to facilitate model search via balancing exploration and exploitation.\nFurthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for\nfast model evaluation. Extensive experiments show that the searched\narchitecture (named AutoBERT-Zero) significantly outperforms BERT and its\nvariants of different model capacities in various downstream tasks, proving the\narchitecture's transfer and generalization abilities. Remarkably,\nAutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and\nBERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE\ntest set. Code and pre-trained models will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.07445",
          "publishedOn": "2021-07-16T00:48:24.767Z",
          "wordCount": 660,
          "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gohel_P/0/1/0/all/0/1\">Prashant Gohel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Priyanka Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_M/0/1/0/all/0/1\">Manoranjan Mohanty</a>",
          "description": "Explainable Artificial Intelligence (XAI) is an emerging area of research in\nthe field of Artificial Intelligence (AI). XAI can explain how AI obtained a\nparticular solution (e.g., classification or object detection) and can also\nanswer other \"wh\" questions. This explainability is not possible in traditional\nAI. Explainability is essential for critical applications, such as defense,\nhealth care, law and order, and autonomous driving vehicles, etc, where the\nknow-how is required for trust and transparency. A number of XAI techniques so\nfar have been purposed for such applications. This paper provides an overview\nof these techniques from a multimedia (i.e., text, image, audio, and video)\npoint of view. The advantages and shortcomings of these techniques have been\ndiscussed, and pointers to some future directions have also been provided.",
          "link": "http://arxiv.org/abs/2107.07045",
          "publishedOn": "2021-07-16T00:48:24.755Z",
          "wordCount": 556,
          "title": "Explainable AI: current status and future directions. (arXiv:2107.07045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07105",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Stokes_J/0/1/0/all/0/1\">James Stokes</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+De_S/0/1/0/all/0/1\">Saibal De</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Veerapaneni_S/0/1/0/all/0/1\">Shravan Veerapaneni</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1\">Giuseppe Carleo</a>",
          "description": "We initiate the study of neural-network quantum state algorithms for\nanalyzing continuous-variable lattice quantum systems in first quantization. A\nsimple family of continuous-variable trial wavefunctons is introduced which\nnaturally generalizes the restricted Boltzmann machine (RBM) wavefunction\nintroduced for analyzing quantum spin systems. By virtue of its simplicity, the\nsame variational Monte Carlo training algorithms that have been developed for\nground state determination and time evolution of spin systems have natural\nanalogues in the continuum. We offer a proof of principle demonstration in the\ncontext of ground state determination of a stoquastic quantum rotor\nHamiltonian. Results are compared against those obtained from partial\ndifferential equation (PDE) based scalable eigensolvers. This study serves as a\nbenchmark against which future investigation of continuous-variable neural\nquantum states can be compared, and points to the need to consider deep network\narchitectures and more sophisticated training algorithms.",
          "link": "http://arxiv.org/abs/2107.07105",
          "publishedOn": "2021-07-16T00:48:24.740Z",
          "wordCount": 591,
          "title": "Continuous-variable neural-network quantum states and the quantum rotor model. (arXiv:2107.07105v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>",
          "description": "Few-shot NLP research is highly active, yet conducted in disjoint research\nthreads with evaluation suites that lack challenging-yet-realistic testing\nsetups and fail to employ careful experimental design. Consequently, the\ncommunity does not know which techniques perform best or even if they\noutperform simple baselines. We formulate desiderata for an ideal few-shot NLP\nbenchmark and present FLEX, the first benchmark, public leaderboard, and\nframework that provides unified, comprehensive measurement for few-shot NLP\ntechniques. FLEX incorporates and introduces new best practices for few-shot\nevaluation, including measurement of four transfer settings, textual labels for\nzero-shot evaluation, and a principled approach to benchmark design that\noptimizes statistical accuracy while keeping evaluation costs accessible to\nresearchers without large compute resources. In addition, we present UniFew, a\nsimple yet strong prompt-based model for few-shot learning which unifies the\npretraining and finetuning prompt formats, eschewing complex machinery of\nrecent prompt-based approaches in adapting downstream task formats to language\nmodel pretraining objectives. We demonstrate that despite simplicity UniFew\nachieves results competitive with both popular meta-learning and prompt-based\napproaches.",
          "link": "http://arxiv.org/abs/2107.07170",
          "publishedOn": "2021-07-16T00:48:24.711Z",
          "wordCount": 619,
          "title": "FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nam_A/0/1/0/all/0/1\">Andrew Joohun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a> (Stanford University)",
          "description": "Despite the groundbreaking successes of neural networks, contemporary models\nrequire extensive training with massive datasets and exhibit poor out-of-sample\ngeneralization. One proposed solution is to build systematicity and\ndomain-specific constraints into the model, echoing the tenets of classical,\nsymbolic cognitive architectures. In this paper, we consider the limitations of\nthis approach by examining human adults' ability to learn an abstract reasoning\ntask from a brief instructional tutorial and explanatory feedback for incorrect\nresponses, demonstrating that human learning dynamics and ability to generalize\noutside the range of the training examples differ drastically from those of a\nrepresentative neural network model, and that the model is brittle to changes\nin features not anticipated by its authors. We present further evidence from\nhuman data that the ability to consistently solve the puzzles was associated\nwith education, particularly basic mathematics education, and with the ability\nto provide a reliably identifiable, valid description of the strategy used. We\npropose that rapid learning and systematic generalization in humans may depend\non a gradual, experience-dependent process of learning-to-learn using\ninstructions and explanations to guide the construction of explicit abstract\nrules that support generalizable inferences.",
          "link": "http://arxiv.org/abs/2107.06994",
          "publishedOn": "2021-07-16T00:48:24.704Z",
          "wordCount": 644,
          "title": "What underlies rapid learning and systematic generalization in humans. (arXiv:2107.06994v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07049",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Keshavamurthy_B/0/1/0/all/0/1\">Bharath Keshavamurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Michelusi_N/0/1/0/all/0/1\">Nicolo Michelusi</a>",
          "description": "A novel LEarning-based Spectrum Sensing and Access (LESSA) framework is\nproposed, wherein a cognitive radio (CR) learns a time-frequency correlation\nmodel underlying spectrum occupancy of licensed users (LUs) in a radio\necosystem; concurrently, it devises an approximately optimal spectrum sensing\nand access policy under sensing constraints. A Baum-Welch algorithm is proposed\nto learn a parametric Markov transition model of LU spectrum occupancy based on\nnoisy spectrum measurements. Spectrum sensing and access are cast as a\nPartially-Observable Markov Decision Process, approximately optimized via\nrandomized point-based value iteration. Fragmentation, Hamming-distance state\nfilters and Monte-Carlo methods are proposed to alleviate the inherent\ncomputational complexity, and a weighted reward metric to regulate the\ntrade-off between CR throughput and LU interference. Numerical evaluations\ndemonstrate that LESSA performs within 5 percent of a genie-aided upper bound\nwith foreknowledge of LU spectrum occupancy, and outperforms state-of-the-art\nalgorithms across the entire trade-off region: 71 percent over\ncorrelation-based clustering, 26 percent over Neyman-Pearson detection, 6\npercent over the Viterbi algorithm, and 9 percent over an adaptive Deep\nQ-Network. LESSA is then extended to a distributed Multi-Agent setting\n(MA-LESSA), by proposing novel neighbor discovery and channel access rank\nallocation. MA-LESSA improves CR throughput by 43 percent over cooperative\nTD-SARSA, 84 percent over cooperative greedy distributed learning, and 3x over\nnon-cooperative learning via g-statistics and ACKs. Finally, MA-LESSA is\nimplemented on the DARPA SC2 platform, manifesting superior performance over\ncompetitors in a real-world TDWR-UNII WLAN emulation; its implementation\nfeasibility is further validated on a testbed of ESP32 radios, exhibiting 96\npercent success probability.",
          "link": "http://arxiv.org/abs/2107.07049",
          "publishedOn": "2021-07-16T00:48:24.684Z",
          "wordCount": 712,
          "title": "Learning-based Spectrum Sensing and Access in Cognitive Radios via Approximate POMDPs. (arXiv:2107.07049v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07098",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dowling_M/0/1/0/all/0/1\">Matthew Dowling</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sokol_P/0/1/0/all/0/1\">Piotr Sok&#xf3;&#x142;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Park_I/0/1/0/all/0/1\">Il Memming Park</a>",
          "description": "We present the class of Hida-Mat\\'ern kernels, which is the canonical family\nof covariance functions over the entire space of stationary Gauss-Markov\nProcesses. It extends upon Mat\\'ern kernels, by allowing for flexible\nconstruction of priors over processes with oscillatory components. Any\nstationary kernel, including the widely used squared-exponential and spectral\nmixture kernels, are either directly within this class or are appropriate\nasymptotic limits, demonstrating the generality of this class. Taking advantage\nof its Markovian nature we show how to represent such processes as state space\nmodels using only the kernel and its derivatives. In turn this allows us to\nperform Gaussian Process inference more efficiently and side step the usual\ncomputational burdens. We also show how exploiting special properties of the\nstate space representation enables improved numerical stability in addition to\nfurther reductions of computational complexity.",
          "link": "http://arxiv.org/abs/2107.07098",
          "publishedOn": "2021-07-16T00:48:24.677Z",
          "wordCount": 556,
          "title": "Hida-Mat\\'ern Kernel. (arXiv:2107.07098v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kyeongbo Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junggi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_Y/0/1/0/all/0/1\">Youngchul Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Young-Rae Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seong-Eun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Woo-Jin Song</a>",
          "description": "Because deep learning is vulnerable to noisy labels, sample selection\ntechniques, which train networks with only clean labeled data, have attracted a\ngreat attention. However, if the labels are dominantly corrupted by few\nclasses, these noisy samples are called dominant-noisy-labeled samples, the\nnetwork also learns dominant-noisy-labeled samples rapidly via content-aware\noptimization. In this study, we propose a compelling criteria to penalize\ndominant-noisy-labeled samples intensively through class-wise penalty labels.\nBy averaging prediction confidences for the each observed label, we obtain\nsuitable penalty labels that have high values if the labels are largely\ncorrupted by some classes. Experiments were performed using benchmarks\n(CIFAR-10, CIFAR-100, Tiny-ImageNet) and real-world datasets (ANIMAL-10N,\nClothing1M) to evaluate the proposed criteria in various scenarios with\ndifferent noise rates. Using the proposed sample selection, the learning\nprocess of the network becomes significantly robust to noisy labels compared to\nexisting methods in several noise types.",
          "link": "http://arxiv.org/abs/2107.07041",
          "publishedOn": "2021-07-16T00:48:24.619Z",
          "wordCount": 600,
          "title": "Mitigating Memorization in Sample Selection for Learning with Noisy Labels. (arXiv:2107.07041v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haoxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fojtik_M/0/1/0/all/0/1\">Matthew Fojtik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1\">Brucek Khailany</a>",
          "description": "High quality standard cell layout automation in advanced technology nodes is\nstill challenging in the industry today because of complex design rules. In\nthis paper we introduce an automatic standard cell layout generator called\nNVCell that can generate layouts with equal or smaller area for over 90% of\nsingle row cells in an industry standard cell library on an advanced technology\nnode. NVCell leverages reinforcement learning (RL) to fix design rule\nviolations during routing and to generate efficient placements.",
          "link": "http://arxiv.org/abs/2107.07044",
          "publishedOn": "2021-07-16T00:48:24.611Z",
          "wordCount": 513,
          "title": "NVCell: Standard Cell Layout in Advanced Technology Nodes with Reinforcement Learning. (arXiv:2107.07044v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07087",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Finkelstein_N/0/1/0/all/0/1\">Noam Finkelstein</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zjawin_B/0/1/0/all/0/1\">Beata Zjawin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wolfe_E/0/1/0/all/0/1\">Elie Wolfe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1\">Ilya Shpitser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spekkens_R/0/1/0/all/0/1\">Robert W. Spekkens</a>",
          "description": "Directed acyclic graphs (DAGs) with hidden variables are often used to\ncharacterize causal relations between variables in a system. When some\nvariables are unobserved, DAGs imply a notoriously complicated set of\nconstraints on the distribution of observed variables. In this work, we present\nentropic inequality constraints that are implied by $e$-separation relations in\nhidden variable DAGs with discrete observed variables. The constraints can\nintuitively be understood to follow from the fact that the capacity of\nvariables along a causal pathway to convey information is restricted by their\nentropy; e.g. at the extreme case, a variable with entropy $0$ can convey no\ninformation. We show how these constraints can be used to learn about the true\ncausal model from an observed data distribution. In addition, we propose a\nmeasure of causal influence called the minimal mediary entropy, and demonstrate\nthat it can augment traditional measures such as the average causal effect.",
          "link": "http://arxiv.org/abs/2107.07087",
          "publishedOn": "2021-07-16T00:48:24.603Z",
          "wordCount": 636,
          "title": "Entropic Inequality Constraints from $e$-separation Relations in Directed Acyclic Graphs with Hidden Variables. (arXiv:2107.07087v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-16T00:48:24.585Z",
          "wordCount": 707,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyoungjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myungchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Laihyuk Park</a>",
          "description": "Video streaming services strive to support high-quality videos at higher\nresolutions and frame rates to improve the quality of experience (QoE).\nHowever, high-quality videos consume considerable amounts of energy on mobile\ndevices. This paper proposes NeuSaver, which reduces the power consumption of\nmobile devices when streaming videos by applying an adaptive frame rate to each\nvideo chunk without compromising user experience. NeuSaver generates an optimal\npolicy that determines the appropriate frame rate for each video chunk using\nreinforcement learning (RL). The RL model automatically learns the policy that\nmaximizes the QoE goals based on previous observations. NeuSaver also uses an\nasynchronous advantage actor-critic algorithm to reinforce the RL model quickly\nand robustly. Streaming servers that support NeuSaver preprocesses videos into\nsegments with various frame rates, which is similar to the process of creating\nvideos with multiple bit rates in dynamic adaptive streaming over HTTP.\nNeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in\nvarious experiments and a user study through four video categories along with\nthe state-of-the-art model. Our experiments showed that NeuSaver effectively\nreduces the power consumption of mobile devices when streaming video by an\naverage of 16.14% and up to 23.12% while achieving high QoE.",
          "link": "http://arxiv.org/abs/2107.07127",
          "publishedOn": "2021-07-16T00:48:24.579Z",
          "wordCount": 672,
          "title": "NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Feng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chonghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashar_M/0/1/0/all/0/1\">Mohammad Khairul Bashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_N/0/1/0/all/0/1\">Nikhil Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vijaykrishnan Narayanan</a>",
          "description": "CNF-based SAT and MaxSAT solvers are central to logic synthesis and\nverification systems. The increasing popularity of these constraint problems in\nelectronic design automation encourages studies on different SAT problems and\ntheir properties for further computational efficiency. There has been both\ntheoretical and practical success of modern Conflict-driven clause learning SAT\nsolvers, which allows solving very large industrial instances in a relatively\nshort amount of time. Recently, machine learning approaches provide a new\ndimension to solving this challenging problem. Neural symbolic models could\nserve as generic solvers that can be specialized for specific domains based on\ndata without any changes to the structure of the model. In this work, we\npropose a one-shot model derived from the Transformer architecture to solve the\nMaxSAT problem, which is the optimization version of SAT where the goal is to\nsatisfy the maximum number of clauses. Our model has a scale-free structure\nwhich could process varying size of instances. We use meta-path and\nself-attention mechanism to capture interactions among homogeneous nodes. We\nadopt cross-attention mechanisms on the bipartite graph to capture interactions\namong heterogeneous nodes. We further apply an iterative algorithm to our model\nto satisfy additional clauses, enabling a solution approaching that of an\nexact-SAT problem. The attention mechanisms leverage the parallelism for\nspeedup. Our evaluation indicates improved speedup compared to heuristic\napproaches and improved completion rate compared to machine learning\napproaches.",
          "link": "http://arxiv.org/abs/2107.07116",
          "publishedOn": "2021-07-16T00:48:24.572Z",
          "wordCount": 680,
          "title": "Transformer-based Machine Learning for Fast SAT Solvers and Logic Synthesis. (arXiv:2107.07116v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Bayesian neural networks provide a direct and natural way to extend standard\ndeep neural networks to support probabilistic deep learning through the use of\nprobabilistic layers that, traditionally, encode weight (and bias) uncertainty.\nIn particular, hybrid Bayesian neural networks utilize standard deterministic\nlayers together with few probabilistic layers judicially positioned in the\nnetworks for uncertainty estimation. A major aspect and benefit of Bayesian\ninference is that priors, in principle, provide the means to encode prior\nknowledge for use in inference and prediction. However, it is difficult to\nspecify priors on weights since the weights have no intuitive interpretation.\nFurther, the relationships of priors on weights to the functions computed by\nnetworks are difficult to characterize. In contrast, functions are intuitive to\ninterpret and are direct since they map inputs to outputs. Therefore, it is\nnatural to specify priors on functions to encode prior knowledge, and to use\nthem in inference and prediction based on functions. To support this, we\npropose hybrid Bayesian neural networks with functional probabilistic layers\nthat encode function (and activation) uncertainty. We discuss their foundations\nin functional Bayesian inference, functional variational inference, sparse\nGaussian processes, and sparse variational Gaussian processes. We further\nperform few proof-of-concept experiments using GPflus, a new library that\nprovides Gaussian process layers and supports their use with deterministic\nKeras layers to form hybrid neural network and Gaussian process models.",
          "link": "http://arxiv.org/abs/2107.07014",
          "publishedOn": "2021-07-16T00:48:24.565Z",
          "wordCount": 656,
          "title": "Hybrid Bayesian Neural Networks with Functional Probabilistic Layers. (arXiv:2107.07014v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiray_B/0/1/0/all/0/1\">Bekir Demiray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "The frequency and impact of floods are expected to increase due to climate\nchange. It is crucial to predict streamflow, consequently flooding, in order to\nprepare and mitigate its consequences in terms of property damage and\nfatalities. This paper presents a Graph Convolutional GRUs based model to\npredict the next 36 hours of streamflow for a sensor location using the\nupstream river network. As shown in experiment results, the model presented in\nthis study provides better performance than the persistence baseline and a\nConvolutional Bidirectional GRU network for the selected study area in\nshort-term streamflow prediction.",
          "link": "http://arxiv.org/abs/2107.07039",
          "publishedOn": "2021-07-16T00:48:24.552Z",
          "wordCount": 546,
          "title": "Short-term Hourly Streamflow Prediction with Graph Convolutional GRU Networks. (arXiv:2107.07039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guohua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yongming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Mingfeng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1\">Witold Pedrycz</a>",
          "description": "Vehicle routing problem (VRP) is a typical discrete combinatorial\noptimization problem, and many models and algorithms have been proposed to\nsolve VRP and variants. Although existing approaches has contributed a lot to\nthe development of this field, these approaches either are limited in problem\nsize or need manual intervening in choosing parameters. To tackle these\ndifficulties, many studies consider learning-based optimization algorithms to\nsolve VRP. This paper reviews recent advances in this field and divides\nrelevant approaches into end-to-end approaches and step-by-step approaches. We\ndesign three part experiments to justly evaluate performance of four\nrepresentative learning-based optimization algorithms and conclude that\ncombining heuristic search can effectively improve learning ability and sampled\nefficiency of LBO models. Finally we point out that research trend of LBO\nalgorithms is to solve large-scale and multiple constraints problems from real\nworld.",
          "link": "http://arxiv.org/abs/2107.07076",
          "publishedOn": "2021-07-16T00:48:24.544Z",
          "wordCount": 583,
          "title": "An Overview and Experimental Study of Learning-based Optimization Algorithms for Vehicle Routing Problem. (arXiv:2107.07076v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07115",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ishibashi_H/0/1/0/all/0/1\">Hideaki Ishibashi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Akaho_S/0/1/0/all/0/1\">Shotaro Akaho</a>",
          "description": "This paper proposes an extension of principal component analysis for Gaussian\nprocess posteriors denoted by GP-PCA. Since GP-PCA estimates a low-dimensional\nspace of GP posteriors, it can be used for meta-learning, which is a framework\nfor improving the precision of a new task by estimating a structure of a set of\ntasks. The issue is how to define a structure of a set of GPs with an\ninfinite-dimensional parameter, such as coordinate system and a divergence. In\nthis study, we reduce the infiniteness of GP to the finite-dimensional case\nunder the information geometrical framework by considering a space of GP\nposteriors that has the same prior. In addition, we propose an approximation\nmethod of GP-PCA based on variational inference and demonstrate the\neffectiveness of GP-PCA as meta-learning through experiments.",
          "link": "http://arxiv.org/abs/2107.07115",
          "publishedOn": "2021-07-16T00:48:24.525Z",
          "wordCount": 558,
          "title": "Principal component analysis for Gaussian process posteriors. (arXiv:2107.07115v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ayush Manish Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tendle_A/0/1/0/all/0/1\">Atharva Tendle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_H/0/1/0/all/0/1\">Harshvardhan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>",
          "description": "Interpreting the learning dynamics of neural networks can provide useful\ninsights into how networks learn and the development of better training and\ndesign approaches. We present an approach to interpret learning in neural\nnetworks by measuring relative weight change on a per layer basis and\ndynamically aggregating emerging trends through combination of dimensionality\nreduction and clustering which allows us to scale to very deep networks. We use\nthis approach to investigate learning in the context of vision tasks across a\nvariety of state-of-the-art networks and provide insights into the learning\nbehavior of these networks, including how task complexity affects layer-wise\nlearning in deeper layers of networks.",
          "link": "http://arxiv.org/abs/2107.07005",
          "publishedOn": "2021-07-16T00:48:24.513Z",
          "wordCount": 550,
          "title": "WeightScale: Interpreting Weight Change in Neural Networks. (arXiv:2107.07005v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1\">Vincenzo Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1\">Kaveh Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1\">Daniele Grandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1\">Bryony DuPont</a>",
          "description": "Function is defined as the ensemble of tasks that enable the product to\ncomplete the designed purpose. Functional tools, such as functional modeling,\noffer decision guidance in the early phase of product design, where explicit\ndesign decisions are yet to be made. Function-based design data is often sparse\nand grounded in individual interpretation. As such, function-based design tools\ncan benefit from automatic function classification to increase data fidelity\nand provide function representation models that enable function-based\nintelligent design agents. Function-based design data is commonly stored in\nmanually generated design repositories. These design repositories are a\ncollection of expert knowledge and interpretations of function in product\ndesign bounded by function-flow and component taxonomies. In this work, we\nrepresent a structured taxonomy-based design repository as assembly-flow\ngraphs, then leverage a graph neural network (GNN) model to perform automatic\nfunction classification. We support automated function classification by\nlearning from repository data to establish the ground truth of component\nfunction assignment. Experimental results show that our GNN model achieves a\nmicro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and\n0.783 for tier 3 (specific) functions. Given the imbalance of data features,\nthe results are encouraging. Our efforts in this paper can be a starting point\nfor more sophisticated applications in knowledge-based CAD systems and\nDesign-for-X consideration in function-based design.",
          "link": "http://arxiv.org/abs/2107.07042",
          "publishedOn": "2021-07-16T00:48:24.472Z",
          "wordCount": 660,
          "title": "Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-16T00:48:24.464Z",
          "wordCount": 600,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1\">Hugo Flores Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_A/0/1/0/all/0/1\">Aldo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1\">Ethan Manilow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_B/0/1/0/all/0/1\">Bryan Pardo</a>",
          "description": "Deep learning work on musical instrument recognition has generally focused on\ninstrument classes for which we have abundant data. In this work, we exploit\nhierarchical relationships between instruments in a few-shot learning setup to\nenable classification of a wider set of musical instruments, given a few\nexamples at inference. We apply a hierarchical loss function to the training of\nprototypical networks, combined with a method to aggregate prototypes\nhierarchically, mirroring the structure of a predefined musical instrument\nhierarchy. These extensions require no changes to the network architecture and\nnew levels can be easily added or removed. Compared to a non-hierarchical\nfew-shot baseline, our method leads to a significant increase in classification\naccuracy and significant decrease mistake severity on instrument classes unseen\nin training.",
          "link": "http://arxiv.org/abs/2107.07029",
          "publishedOn": "2021-07-16T00:48:24.445Z",
          "wordCount": 563,
          "title": "Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition. (arXiv:2107.07029v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Piqueras_M/0/1/0/all/0/1\">Manuel Garcia-Piqueras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1\">Jos&#xe9; Hern&#xe1;ndez-Orallo</a>",
          "description": "Recent research in machine teaching has explored the instruction of any\nconcept expressed in a universal language. In this compositional context, new\nexperimental results have shown that there exist data teaching sets\nsurprisingly shorter than the concept description itself. However, there exists\na bound for those remarkable experimental findings through teaching size and\nconcept complexity that we further explore here. As concepts are rarely taught\nin isolation we investigate the best configuration of concepts to teach a given\nset of concepts, where those that have been acquired first can be reused for\nthe description of new ones. This new notion of conditional teaching size\nuncovers new insights, such as the interposition phenomenon: certain prior\nknowledge generates simpler compatible concepts that increase the teaching size\nof the concept that we want to teach. This does not happen for conditional\nKolmogorov complexity. Furthermore, we provide an algorithm that constructs\noptimal curricula based on interposition avoidance. This paper presents a\nseries of theoretical results, including their proofs, and some directions for\nfuture work. New research possibilities in curriculum teaching in compositional\nscenarios are now wide open to exploration.",
          "link": "http://arxiv.org/abs/2107.07038",
          "publishedOn": "2021-07-16T00:48:24.440Z",
          "wordCount": 618,
          "title": "Conditional Teaching Size. (arXiv:2107.07038v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shabani_M/0/1/0/all/0/1\">Mostafa Shabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Financial market analysis, especially the prediction of movements of stock\nprices, is a challenging problem. The nature of financial time-series data,\nbeing non-stationary and nonlinear, is the main cause of these challenges. Deep\nlearning models have led to significant performance improvements in many\nproblems coming from different domains, including prediction problems of\nfinancial time-series data. Although the prediction performance is the main\ngoal of such models, dealing with ultra high-frequency data sets restrictions\nin terms of the number of model parameters and its inference speed. The\nTemporal Attention-Augmented Bilinear network was recently proposed as an\nefficient and high-performing model for Limit Order Book time-series\nforecasting. In this paper, we propose a low-rank tensor approximation of the\nmodel to further reduce the number of trainable parameters and increase its\nspeed.",
          "link": "http://arxiv.org/abs/2107.06995",
          "publishedOn": "2021-07-16T00:48:24.433Z",
          "wordCount": 558,
          "title": "Low-Rank Temporal Attention-Augmented Bilinear Network for financial time-series forecasting. (arXiv:2107.06995v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Han-Chih Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "In this research, we consider the problem of verifying user identity based on\nkeystroke dynamics obtained from free-text. We employ a novel feature\nengineering method that generates image-like transition matrices. For this\nimage-like feature, a convolution neural network (CNN) with cutout achieves the\nbest results. A hybrid model consisting of a CNN and a recurrent neural network\n(RNN) is also shown to outperform previous research in this field.",
          "link": "http://arxiv.org/abs/2107.07009",
          "publishedOn": "2021-07-16T00:48:24.419Z",
          "wordCount": 491,
          "title": "Free-Text Keystroke Dynamics for User Authentication. (arXiv:2107.07009v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1\">Tazin Afrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Elaine Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1\">Lindsay C. Matsumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1\">Richard Correnti</a>",
          "description": "Automated writing evaluation systems can improve students' writing insofar as\nstudents attend to the feedback provided and revise their essay drafts in ways\naligned with such feedback. Existing research on revision of argumentative\nwriting in such systems, however, has focused on the types of revisions\nstudents make (e.g., surface vs. content) rather than the extent to which\nrevisions actually respond to the feedback provided and improve the essay. We\nintroduce an annotation scheme to capture the nature of sentence-level\nrevisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-\nand 6th-grade students' argumentative essays. We show that reliable manual\nannotation can be achieved and that revision annotations correlate with a\nholistic assessment of essay improvement in line with the feedback provided.\nFurthermore, we explore the feasibility of automatically classifying revisions\naccording to our scheme.",
          "link": "http://arxiv.org/abs/2107.06990",
          "publishedOn": "2021-07-16T00:48:24.413Z",
          "wordCount": 605,
          "title": "Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1\">Zachary Charles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1\">Gauri Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1\">H. Brendan McMahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1\">Maruan Al-Shedivat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrew_G/0/1/0/all/0/1\">Galen Andrew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1\">Katharine Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1\">Deepesh Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichner_H/0/1/0/all/0/1\">Hubert Eichner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadhikar_A/0/1/0/all/0/1\">Advait Gadhikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1\">Zachary Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girgis_A/0/1/0/all/0/1\">Antonious M. Girgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanzely_F/0/1/0/all/0/1\">Filip Hanzely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hard_A/0/1/0/all/0/1\">Andrew Hard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1\">Samuel Horvath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingerman_A/0/1/0/all/0/1\">Alex Ingerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1\">Tara Javidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1\">Peter Kairouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1\">Satyen Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konecny_J/0/1/0/all/0/1\">Jakub Konecny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Luyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1\">Sashank J. Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richtarik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1\">Karan Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weikang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1\">Blake Woodworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shanshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Felix X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Honglin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chunxiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, et al. (1 additional author not shown)",
          "description": "Federated learning and analytics are a distributed approach for\ncollaboratively learning models (or statistics) from decentralized data,\nmotivated by and designed for privacy protection. The distributed learning\nprocess can be formulated as solving federated optimization problems, which\nemphasize communication efficiency, data heterogeneity, compatibility with\nprivacy and system requirements, and other constraints that are not primary\nconsiderations in other problem settings. This paper provides recommendations\nand guidelines on formulating, designing, evaluating and analyzing federated\noptimization algorithms through concrete examples and practical implementation,\nwith a focus on conducting effective simulations to infer real-world\nperformance. The goal of this work is not to survey the current literature, but\nto inspire researchers and practitioners to design federated learning\nalgorithms that can be used in various practical applications.",
          "link": "http://arxiv.org/abs/2107.06917",
          "publishedOn": "2021-07-16T00:48:24.404Z",
          "wordCount": 656,
          "title": "A Field Guide to Federated Optimization. (arXiv:2107.06917v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07064",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1\">Dae-Hyeok Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sung-Jin Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Brain-computer interface (BCI) is one of the tools which enables the\ncommunication between humans and devices by reflecting intention and status of\nhumans. With the development of artificial intelligence, the interest in\ncommunication between humans and drones using electroencephalogram (EEG) is\nincreased. Especially, in the case of controlling drone swarms such as\ndirection or formation, there are many advantages compared with controlling a\ndrone unit. Imagined speech is one of the endogenous BCI paradigms, which can\nidentify intentions of users. When conducting imagined speech, the users\nimagine the pronunciation as if actually speaking. In contrast, overt speech is\na task in which the users directly pronounce the words. When controlling drone\nswarms using imagined speech, complex commands can be delivered more\nintuitively, but decoding performance is lower than that of other endogenous\nBCI paradigms. We proposed the Deep-autoleaner (DAL) to learn EEG features of\novert speech for imagined speech-based EEG signals classification. To the best\nof our knowledge, this study is the first attempt to use EEG features of overt\nspeech to decode imagined speech-based EEG signals with an autoencoder. A total\nof eight subjects participated in the experiment. When classifying four words,\nthe average accuracy of the DAL was 48.41%. In addition, when comparing the\nperformance between w/o and w/ EEG features of overt speech, there was a\nperformance improvement of 7.42% when including EEG features of overt speech.\nHence, we demonstrated that EEG features of overt speech could improve the\ndecoding performance of imagined speech.",
          "link": "http://arxiv.org/abs/2107.07064",
          "publishedOn": "2021-07-16T00:48:24.396Z",
          "wordCount": 715,
          "title": "DAL: Feature Learning from Overt Speech to Decode Imagined Speech-based EEG Signals with Convolutional Autoencoder. (arXiv:2107.07064v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farley_J/0/1/0/all/0/1\">Jackson Farley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstlauer_A/0/1/0/all/0/1\">Andreas Gerstlauer</a>",
          "description": "A rising research challenge is running costly machine learning (ML) networks\nlocally on resource-constrained edge devices. ML networks with large\nconvolutional layers can easily exceed available memory, increasing latency due\nto excessive swapping. Previous memory reduction techniques such as pruning and\nquantization reduce model accuracy and often require retraining. Alternatively,\ndistributed methods partition the convolutions into equivalent smaller\nsub-computations, but the implementations introduce communication costs and\nrequire a network of devices. However, a distributed partitioning approach can\nalso be used to run in a reduced memory footprint on a single device by\nsubdividing the network into smaller operations.\n\nThis report extends prior work on distributed partitioning using tiling and\nfusing of convolutional layers into a memory-aware execution on a single\ndevice. Our approach extends prior fusing strategies to allow for two groups of\nconvolutional layers that are fused and tiled independently. This approach\nreduces overhead via data reuse, and reduces the memory footprint further. We\nalso propose a memory usage predictor coupled with a search algorithm to\nprovide fusing and tiling configurations for an arbitrary set of convolutional\nlayers. When applied to the YOLOv2 object detection network, results show that\nour approach can run in less than half the memory, and with a speedup of up to\n2.78 under severe memory constraints. Additionally, our algorithm will return a\nconfiguration with a latency that is within 6% of the best latency measured in\na manual search.",
          "link": "http://arxiv.org/abs/2107.06960",
          "publishedOn": "2021-07-16T00:48:24.390Z",
          "wordCount": 676,
          "title": "Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jingyang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoniu Yang</a>",
          "description": "Deep Neural Networks (DNN) are known to be vulnerable to adversarial samples,\nthe detection of which is crucial for the wide application of these DNN models.\nRecently, a number of deep testing methods in software engineering were\nproposed to find the vulnerability of DNN systems, and one of them, i.e., Model\nMutation Testing (MMT), was used to successfully detect various adversarial\nsamples generated by different kinds of adversarial attacks. However, the\nmutated models in MMT are always huge in number (e.g., over 100 models) and\nlack diversity (e.g., can be easily circumvented by high-confidence adversarial\nsamples), which makes it less efficient in real applications and less effective\nin detecting high-confidence adversarial samples. In this study, we propose\nGraph-Guided Testing (GGT) for adversarial sample detection to overcome these\naforementioned challenges. GGT generates pruned models with the guide of graph\ncharacteristics, each of them has only about 5% parameters of the mutated model\nin MMT, and graph guided models have higher diversity. The experiments on\nCIFAR10 and SVHN validate that GGT performs much better than MMT with respect\nto both effectiveness and efficiency.",
          "link": "http://arxiv.org/abs/2107.07043",
          "publishedOn": "2021-07-16T00:48:24.382Z",
          "wordCount": 635,
          "title": "GGT: Graph-Guided Testing for Adversarial Sample Detection of Deep Neural Network. (arXiv:2107.07043v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zheng Yan</a>",
          "description": "The troposphere is one of the atmospheric layers where most weather phenomena\noccur. Temperature variations in the troposphere, especially at 500 hPa, a\ntypical level of the middle troposphere, are significant indicators of future\nweather changes. Numerical weather prediction is effective for temperature\nprediction, but its computational complexity hinders a timely response. This\npaper proposes a novel temperature prediction approach in framework\nofphysics-informed deep learning. The new model, called PGnet, builds upon a\ngenerative neural network with a mask matrix. The mask is designed to\ndistinguish the low-quality predicted regions generated by the first physical\nstage. The generative neural network takes the mask as prior for the\nsecond-stage refined predictions. A mask-loss and a jump pattern strategy are\ndeveloped to train the generative neural network without accumulating errors\nduring making time-series predictions. Experiments on ERA5 demonstrate that\nPGnet can generate more refined temperature predictions than the\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2107.06991",
          "publishedOn": "2021-07-16T00:48:24.377Z",
          "wordCount": 590,
          "title": "Physics-informed generative neural network: an application to troposphere temperature prediction. (arXiv:2107.06991v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pinzon_C/0/1/0/all/0/1\">Carlos Pinz&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1\">Catuscia Palamidessi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valencia_F/0/1/0/all/0/1\">Frank Valencia</a>",
          "description": "One of the main concerns about fairness in machine learning (ML) is that, in\norder to achieve it, one may have to renounce to some accuracy. Having this\ntrade-off in mind, Hardt et al. have proposed the notion of equal opportunities\n(EO), designed so as to be compatible with accuracy. In fact, it can be shown\nthat if the source of input data is deterministic, the two notions go well\nalong with each other. In the probabilistic case, however, things change.\n\nAs we show, there are probabilistic data sources for which EO can only be\nachieved at the total detriment of accuracy, i.e. among the models that achieve\nEO, those whose prediction does not depend on the input have the highest\naccuracy.",
          "link": "http://arxiv.org/abs/2107.06944",
          "publishedOn": "2021-07-16T00:48:24.361Z",
          "wordCount": 561,
          "title": "On the impossibility of non-trivial accuracy under fairness constraints. (arXiv:2107.06944v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of data efficiency, than plain text prompts do for\nexisting LMs, and that HTLM is highly effective at auto-prompting itself, by\nsimply generating the most likely hyper-text formatting for any available\ntraining data. We will release all code and models to support future HTLM\nresearch.",
          "link": "http://arxiv.org/abs/2107.06955",
          "publishedOn": "2021-07-16T00:48:24.349Z",
          "wordCount": 633,
          "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>",
          "description": "While many existing graph neural networks (GNNs) have been proven to perform\n$\\ell_2$-based graph smoothing that enforces smoothness globally, in this work\nwe aim to further enhance the local smoothness adaptivity of GNNs via\n$\\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs\n(Elastic GNNs) based on $\\ell_1$ and $\\ell_2$-based graph smoothing. In\nparticular, we propose a novel and general message passing scheme into GNNs.\nThis message passing algorithm is not only friendly to back-propagation\ntraining but also achieves the desired smoothing properties with a theoretical\nconvergence guarantee. Experiments on semi-supervised learning tasks\ndemonstrate that the proposed Elastic GNNs obtain better adaptivity on\nbenchmark datasets and are significantly robust to graph adversarial attacks.\nThe implementation of Elastic GNNs is available at\n\\url{https://github.com/lxiaorui/ElasticGNN}.",
          "link": "http://arxiv.org/abs/2107.06996",
          "publishedOn": "2021-07-16T00:48:24.332Z",
          "wordCount": 575,
          "title": "Elastic Graph Neural Networks. (arXiv:2107.06996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1\">Harshita Diddee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulay_A/0/1/0/all/0/1\">Ajinkya Mulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1\">Aleti Vardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzam_A/0/1/0/all/0/1\">Ahmed Zamzam</a>",
          "description": "In recent years, machine learning techniques utilizing large-scale datasets\nhave achieved remarkable performance. Differential privacy, by means of adding\nnoise, provides strong privacy guarantees for such learning algorithms. The\ncost of differential privacy is often a reduced model accuracy and a lowered\nconvergence speed. This paper investigates the impact of differential privacy\non learning algorithms in terms of their carbon footprint due to either longer\nrun-times or failed experiments. Through extensive experiments, further\nguidance is provided on choosing the noise levels which can strike a balance\nbetween desired privacy levels and reduced carbon emissions.",
          "link": "http://arxiv.org/abs/2107.06946",
          "publishedOn": "2021-07-16T00:48:24.316Z",
          "wordCount": 550,
          "title": "Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning. (arXiv:2107.06946v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sourav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Suresh Sundaram</a>",
          "description": "In this paper, a novel confidence conditioned knowledge distillation (CCKD)\nscheme for transferring the knowledge from a teacher model to a student model\nis proposed. Existing state-of-the-art methods employ fixed loss functions for\nthis purpose and ignore the different levels of information that need to be\ntransferred for different samples. In addition to that, these methods are also\ninefficient in terms of data usage. CCKD addresses these issues by leveraging\nthe confidence assigned by the teacher model to the correct class to devise\nsample-specific loss functions (CCKD-L formulation) and targets (CCKD-T\nformulation). Further, CCKD improves the data efficiency by employing\nself-regulation to stop those samples from participating in the distillation\nprocess on which the student model learns faster. Empirical evaluations on\nseveral benchmark datasets show that CCKD methods achieve at least as much\ngeneralization performance levels as other state-of-the-art methods while being\ndata efficient in the process. Student models trained through CCKD methods do\nnot retain most of the misclassifications commited by the teacher model on the\ntraining set. Distillation through CCKD methods improves the resilience of the\nstudent models against adversarial attacks compared to the conventional KD\nmethod. Experiments show at least 3% increase in performance against\nadversarial attacks for the MNIST and the Fashion MNIST datasets, and at least\n6% increase for the CIFAR10 dataset.",
          "link": "http://arxiv.org/abs/2107.06993",
          "publishedOn": "2021-07-16T00:48:24.295Z",
          "wordCount": 650,
          "title": "Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06898",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Erdmenger_J/0/1/0/all/0/1\">Johanna Erdmenger</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Grosvenor_K/0/1/0/all/0/1\">Kevin T. Grosvenor</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Jefferson_R/0/1/0/all/0/1\">Ro Jefferson</a>",
          "description": "We investigate the analogy between the renormalization group (RG) and deep\nneural networks, wherein subsequent layers of neurons are analogous to\nsuccessive steps along the RG. In particular, we quantify the flow of\ninformation by explicitly computing the relative entropy or Kullback-Leibler\ndivergence in both the one- and two-dimensional Ising models under decimation\nRG, as well as in a feedforward neural network as a function of depth. We\nobserve qualitatively identical behavior characterized by the monotonic\nincrease to a parameter-dependent asymptotic value. On the quantum field theory\nside, the monotonic increase confirms the connection between the relative\nentropy and the c-theorem. For the neural networks, the asymptotic behavior may\nhave implications for various information maximization methods in machine\nlearning, as well as for disentangling compactness and generalizability.\nFurthermore, while both the two-dimensional Ising model and the random neural\nnetworks we consider exhibit non-trivial critical points, the relative entropy\nappears insensitive to the phase structure of either system. In this sense,\nmore refined probes are required in order to fully elucidate the flow of\ninformation in these models.",
          "link": "http://arxiv.org/abs/2107.06898",
          "publishedOn": "2021-07-16T00:48:24.270Z",
          "wordCount": 650,
          "title": "Towards quantifying information flows: relative entropy in deep neural networks and the renormalization group. (arXiv:2107.06898v1 [hep-th])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06936",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Barbier_J/0/1/0/all/0/1\">Jean Barbier</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Kuo Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Panchenko_D/0/1/0/all/0/1\">Dmitry Panchenko</a>, <a href=\"http://arxiv.org/find/math/1/au:+Saenz_M/0/1/0/all/0/1\">Manuel S&#xe1;enz</a>",
          "description": "For a model of high-dimensional linear regression with random design, we\nanalyze the performance of an estimator given by the mean of a log-concave\nBayesian posterior distribution with gaussian prior. The model is mismatched in\nthe following sense: like the model assumed by the statistician, the\nlabels-generating process is linear in the input data, but both the classifier\nground-truth prior and gaussian noise variance are unknown to her. This\ninference model can be rephrased as a version of the Gardner model in spin\nglasses and, using the cavity method, we provide fixed point equations for\nvarious overlap order parameters, yielding in particular an expression for the\nmean-square reconstruction error on the classifier (under an assumption of\nuniqueness of solutions). As a direct corollary we obtain an expression for the\nfree energy. Similar models have already been studied by Shcherbina and Tirozzi\nand by Talagrand, but our arguments are more straightforward and some\nassumptions are relaxed. An interesting consequence of our analysis is that in\nthe random design setting of ridge regression, the performance of the posterior\nmean is independent of the noise variance (or \"temperature\") assumed by the\nstatistician, and matches the one of the usual (zero temperature) ridge\nestimator.",
          "link": "http://arxiv.org/abs/2107.06936",
          "publishedOn": "2021-07-16T00:48:24.262Z",
          "wordCount": 660,
          "title": "Performance of Bayesian linear regression in a model with mismatch. (arXiv:2107.06936v1 [math.PR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shigang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1\">Torsten Hoefler</a>",
          "description": "Training large deep learning models at scale is very challenging. This paper\nproposes Chimera, a novel pipeline parallelism scheme which combines\nbidirectional pipelines for efficiently training large-scale models. Chimera is\na synchronous approach and therefore no loss of accuracy, which is more\nconvergence-friendly than asynchronous approaches. Compared with the latest\nsynchronous pipeline approach, Chimera reduces the number of bubbles by up to\n50%; benefiting from the sophisticated scheduling of bidirectional pipelines,\nChimera has a more balanced activation memory consumption. Evaluations are\nconducted on Transformer based language models. For a GPT-2 model with 1.3\nbillion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer,\nChimera improves the training throughput by 1.16x-2.34x over the\nstate-of-the-art synchronous and asynchronous pipeline approaches.",
          "link": "http://arxiv.org/abs/2107.06925",
          "publishedOn": "2021-07-16T00:48:24.229Z",
          "wordCount": 585,
          "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. (arXiv:2107.06925v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lily H. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_M/0/1/0/all/0/1\">Mark Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Deep generative models (DGMs) seem a natural fit for detecting\nout-of-distribution (OOD) inputs, but such models have been shown to assign\nhigher probabilities or densities to OOD images than images from the training\ndistribution. In this work, we explain why this behavior should be attributed\nto model misestimation. We first prove that no method can guarantee performance\nbeyond random chance without assumptions on which out-distributions are\nrelevant. We then interrogate the typical set hypothesis, the claim that\nrelevant out-distributions can lie in high likelihood regions of the data\ndistribution, and that OOD detection should be defined based on the data\ndistribution's typical set. We highlight the consequences implied by assuming\nsupport overlap between in- and out-distributions, as well as the arbitrariness\nof the typical set for OOD detection. Our results suggest that estimation error\nis a more plausible explanation than the misalignment between likelihood-based\nOOD detection and out-distributions of interest, and we illustrate how even\nminimal estimation error can lead to OOD detection failures, yielding\nimplications for future work in deep generative modeling and OOD detection.",
          "link": "http://arxiv.org/abs/2107.06908",
          "publishedOn": "2021-07-16T00:48:24.223Z",
          "wordCount": 611,
          "title": "Understanding Failures in Out-of-Distribution Detection with Deep Generative Models. (arXiv:2107.06908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1\">Tomasz W&#x142;odarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1\">Adam Klasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "In this paper, we propose an end-to-end multi-task neural network called\nFetalNet with an attention mechanism and stacked module for spatio-temporal\nfetal ultrasound scan video analysis. Fetal biometric measurement is a standard\nexamination during pregnancy used for the fetus growth monitoring and\nestimation of gestational age and fetal weight. The main goal in fetal\nultrasound scan video analysis is to find proper standard planes to measure the\nfetal head, abdomen and femur. Due to natural high speckle noise and shadows in\nultrasound data, medical expertise and sonographic experience are required to\nfind the appropriate acquisition plane and perform accurate measurements of the\nfetus. In addition, existing computer-aided methods for fetal US biometric\nmeasurement address only one single image frame without considering temporal\nfeatures. To address these shortcomings, we propose an end-to-end multi-task\nneural network for spatio-temporal ultrasound scan video analysis to\nsimultaneously localize, classify and measure the fetal body parts. We propose\na new encoder-decoder segmentation architecture that incorporates a\nclassification branch. Additionally, we employ an attention mechanism with a\nstacked module to learn salient maps to suppress irrelevant US regions and\nefficient scan plane localization. We trained on the fetal ultrasound video\ncomes from routine examinations of 700 different patients. Our method called\nFetalNet outperforms existing state-of-the-art methods in both classification\nand segmentation in fetal ultrasound video recordings.",
          "link": "http://arxiv.org/abs/2107.06943",
          "publishedOn": "2021-07-16T00:48:24.213Z",
          "wordCount": 682,
          "title": "FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02266",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Khamaru_K/0/1/0/all/0/1\">Koulik Khamaru</a>, <a href=\"http://arxiv.org/find/math/1/au:+Deshpande_Y/0/1/0/all/0/1\">Yash Deshpande</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wainwright_M/0/1/0/all/0/1\">Martin J. Wainwright</a>",
          "description": "When data is collected in an adaptive manner, even simple methods like\nordinary least squares can exhibit non-normal asymptotic behavior. As an\nundesirable consequence, hypothesis tests and confidence intervals based on\nasymptotic normality can lead to erroneous results. We propose an online\ndebiasing estimator to correct these distributional anomalies in least squares\nestimation. Our proposed method takes advantage of the covariance structure\npresent in the dataset and provides sharper estimates in directions for which\nmore information has accrued. We establish an asymptotic normality property for\nour proposed online debiasing estimator under mild conditions on the data\ncollection process, and provide asymptotically exact confidence intervals. We\nadditionally prove a minimax lower bound for the adaptive linear regression\nproblem, thereby providing a baseline by which to compare estimators. There are\nvarious conditions under which our proposed estimator achieves the minimax\nlower bound up to logarithmic factors. We demonstrate the usefulness of our\ntheory via applications to multi-armed bandit, autoregressive time series\nestimation, and active learning with exploration.",
          "link": "http://arxiv.org/abs/2107.02266",
          "publishedOn": "2021-07-15T01:59:05.080Z",
          "wordCount": 623,
          "title": "Near-optimal inference in adaptive linear regression. (arXiv:2107.02266v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1\">Lan V. Truong</a>",
          "description": "This paper estimates free energy, average mutual information, and minimum\nmean square error (MMSE) of a linear model under two assumptions: (1) the\nsource is generated by a Markov chain, (2) the source is generated via a hidden\nMarkov model. Our estimates are based on the replica method in statistical\nphysics. We show that under the posterior mean estimator, the linear model with\nMarkov sources or hidden Markov sources is decoupled into single-input AWGN\nchannels with state information available at both encoder and decoder where the\nstate distribution follows the left Perron-Frobenius eigenvector with unit\nManhattan norm of the stochastic matrix of Markov chains. Numerical results\nshow that the free energies and MSEs obtained via the replica method closely\napproximate to their counterparts achieved by the Metropolis-Hastings algorithm\nor some well-known approximate message passing algorithms in the research\nliterature.",
          "link": "http://arxiv.org/abs/2009.13370",
          "publishedOn": "2021-07-15T01:59:04.915Z",
          "wordCount": 617,
          "title": "Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1\">Andrei Atanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Averaging predictions over a set of models -- an ensemble -- is widely used\nto improve predictive performance and uncertainty estimation of deep learning\nmodels. At the same time, many machine learning systems, such as search,\nmatching, and recommendation systems, heavily rely on embeddings.\nUnfortunately, due to misalignment of features of independently trained models,\nembeddings, cannot be improved with a naive deep ensemble like approach. In\nthis work, we look at the ensembling of representations and propose mean\nembeddings with test-time augmentation (MeTTA) simple yet well-performing\nrecipe for ensembling representations. Empirically we demonstrate that MeTTA\nsignificantly boosts the quality of linear evaluation on ImageNet for both\nsupervised and self-supervised models. Even more exciting, we draw connections\nbetween MeTTA, image retrieval, and transformation invariant models. We believe\nthat spreading the success of ensembles to inference higher-quality\nrepresentations is the important step that will open many new applications of\nensembling.",
          "link": "http://arxiv.org/abs/2106.08038",
          "publishedOn": "2021-07-15T01:59:04.856Z",
          "wordCount": 615,
          "title": "Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08583",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1\">Pujan Pokhrel</a>",
          "description": "This paper proposes a machine learning method based on the Extra Trees (ET)\nalgorithm for forecasting Significant Wave Heights in oceanic waters. To derive\nmultiple features from the CDIP buoys, which make point measurements, we first\nnowcast various parameters and then forecast them at 30-min intervals. The\nproposed algorithm has Scatter Index (SI), Bias, Correlation Coefficient, Root\nMean Squared Error (RMSE) of 0.130, -0.002, 0.97, and 0.14, respectively, for\none day ahead prediction and 0.110, -0.001, 0.98, and 0.122, respectively, for\n14-day ahead prediction on the testing dataset. While other state-of-the-art\nmethods can only forecast up to 120 hours ahead, we extend it further to 14\ndays. Our proposed setup includes spectral features, hv-block cross-validation,\nand stringent QC criteria. The proposed algorithm performs significantly better\nthan the state-of-the-art methods commonly used for significant wave height\nforecasting for one-day ahead prediction. Moreover, the improved performance of\nthe proposed machine learning method compared to the numerical methods shows\nthat this performance can be extended to even longer periods allowing for early\nprediction of significant wave heights in oceanic waters.",
          "link": "http://arxiv.org/abs/2105.08583",
          "publishedOn": "2021-07-15T01:59:04.842Z",
          "wordCount": 648,
          "title": "Machine Learning in weakly nonlinear systems: A Case study on Significant wave heights. (arXiv:2105.08583v3 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1806.00421",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Beck_C/0/1/0/all/0/1\">Christian Beck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Becker_S/0/1/0/all/0/1\">Sebastian Becker</a>, <a href=\"http://arxiv.org/find/math/1/au:+Grohs_P/0/1/0/all/0/1\">Philipp Grohs</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jaafari_N/0/1/0/all/0/1\">Nor Jaafari</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>",
          "description": "Stochastic differential equations (SDEs) and the Kolmogorov partial\ndifferential equations (PDEs) associated to them have been widely used in\nmodels from engineering, finance, and the natural sciences. In particular, SDEs\nand Kolmogorov PDEs, respectively, are highly employed in models for the\napproximative pricing of financial derivatives. Kolmogorov PDEs and SDEs,\nrespectively, can typically not be solved explicitly and it has been and still\nis an active topic of research to design and analyze numerical methods which\nare able to approximately solve Kolmogorov PDEs and SDEs, respectively. Nearly\nall approximation methods for Kolmogorov PDEs in the literature suffer under\nthe curse of dimensionality or only provide approximations of the solution of\nthe PDE at a single fixed space-time point. In this paper we derive and propose\na numerical approximation method which aims to overcome both of the above\nmentioned drawbacks and intends to deliver a numerical approximation of the\nKolmogorov PDE on an entire region $[a,b]^d$ without suffering from the curse\nof dimensionality. Numerical results on examples including the heat equation,\nthe Black-Scholes model, the stochastic Lorenz equation, and the Heston model\nsuggest that the proposed approximation algorithm is quite effective in high\ndimensions in terms of both accuracy and speed.",
          "link": "http://arxiv.org/abs/1806.00421",
          "publishedOn": "2021-07-15T01:59:04.827Z",
          "wordCount": 685,
          "title": "Solving the Kolmogorov PDE by means of deep learning. (arXiv:1806.00421v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06773",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ding_Y/0/1/0/all/0/1\">Yan Ding</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kim_Y/0/1/0/all/0/1\">Yejin Kim</a>",
          "description": "The evaluation of the BBB penetrating ability of drug molecules is a critical\nstep in brain drug development. Computational prediction based on machine\nlearning has proved to be an efficient way to conduct the evaluation. However,\nperformance of the established models has been limited by their incapability of\ndealing with the interactions between drugs and proteins, which play an\nimportant role in the mechanism behind BBB penetrating behaviors. To address\nthis issue, we employed the relational graph convolutional network (RGCN) to\nhandle the drug-protein (denoted by the encoding gene) relations as well as the\nfeatures of each individual drug. In addition, drug-drug similarity was also\nintroduced to connect structurally similar drugs in the graph. The RGCN model\nwas initially trained without input of any drug features. And the performance\nwas already promising, demonstrating the significant role of the\ndrug-protein/drug-drug relations in the prediction of BBB permeability.\nMoreover, molecular embeddings from a pre-trained knowledge graph were used as\nthe drug features to further enhance the predictive ability of the model.\nFinally, the best performing RGCN model was built with a large number of\nunlabeled drugs integrated into the graph.",
          "link": "http://arxiv.org/abs/2107.06773",
          "publishedOn": "2021-07-15T01:59:04.821Z",
          "wordCount": 628,
          "title": "Relational graph convolutional networks for predicting blood-brain barrier penetration of drug molecules. (arXiv:2107.06773v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palmes_P/0/1/0/all/0/1\">Paulito P. Palmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kishimoto_A/0/1/0/all/0/1\">Akihiro Kishimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1\">Radu Marinescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1\">Parikshit Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_E/0/1/0/all/0/1\">Elizabeth Daly</a>",
          "description": "The pipeline optimization problem in machine learning requires simultaneous\noptimization of pipeline structures and parameter adaptation of their elements.\nHaving an elegant way to express these structures can help lessen the\ncomplexity in the management and analysis of their performances together with\nthe different choices of optimization strategies. With these issues in mind, we\ncreated the AutoMLPipeline (AMLP) toolkit which facilitates the creation and\nevaluation of complex machine learning pipeline structures using simple\nexpressions. We use AMLP to find optimal pipeline signatures, datamine them,\nand use these datamined features to speed-up learning and prediction. We\nformulated a two-stage pipeline optimization with surrogate modeling in AMLP\nwhich outperforms other AutoML approaches with a 4-hour time budget in less\nthan 5 minutes of AMLP computation time.",
          "link": "http://arxiv.org/abs/2107.01253",
          "publishedOn": "2021-07-15T01:59:04.805Z",
          "wordCount": 582,
          "title": "Designing Machine Learning Pipeline Toolkit for AutoML Surrogate Modeling Optimization. (arXiv:2107.01253v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jahin_M/0/1/0/all/0/1\">Md. Abrar Jahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krutsylo_A/0/1/0/all/0/1\">Andrii Krutsylo</a>",
          "description": "The research internship at UiT - The Arctic University of Norway was offered\nfor our team being the winner of the 'Smart Roads - Winter Road Maintenance\n2021' Hackathon. The internship commenced on 3 May 2021 and ended on 21 May\n2021 with meetings happening twice each week. In spite of having different\nnationalities and educational backgrounds, we both interns tried to collaborate\nas a team as much as possible. The most alluring part was working on this\nproject made us realize the critical conditions faced by the arctic people,\nwhere it was hard to gain such a unique experience from our residence. We\ndeveloped and implemented several deep learning models to classify the states\n(dry, moist, wet, icy, snowy, slushy). Depending upon the best model, the\nweather forecast app will predict the state taking the Ta, Tsurf, Height,\nSpeed, Water, etc. into consideration. The crucial part was to define a safety\nmetric which is the product of the accident rates based on friction and the\naccident rates based on states. We developed a regressor that will predict the\nsafety metric depending upon the state obtained from the classifier and the\nfriction obtained from the sensor data. A pathfinding algorithm has been\ndesigned using the sensor data, open street map data, weather data.",
          "link": "http://arxiv.org/abs/2107.06755",
          "publishedOn": "2021-07-15T01:59:04.797Z",
          "wordCount": 636,
          "title": "DIT4BEARs Smart Roads Internship. (arXiv:2107.06755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongge Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boning_D/0/1/0/all/0/1\">Duane Boning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "A deep reinforcement learning (DRL) agent observes its states through\nobservations, which may contain natural measurement errors or adversarial\nnoises. Since the observations deviate from the true states, they can mislead\nthe agent into making suboptimal actions. Several works have shown this\nvulnerability via adversarial attacks, but existing approaches on improving the\nrobustness of DRL under this setting have limited success and lack for\ntheoretical principles. We show that naively applying existing techniques on\nimproving robustness for classification tasks, like adversarial training, is\nineffective for many RL tasks. We propose the state-adversarial Markov decision\nprocess (SA-MDP) to study the fundamental properties of this problem, and\ndevelop a theoretically principled policy regularization which can be applied\nto a large family of DRL algorithms, including proximal policy optimization\n(PPO), deep deterministic policy gradient (DDPG) and deep Q networks (DQN), for\nboth discrete and continuous action control problems. We significantly improve\nthe robustness of PPO, DDPG and DQN agents under a suite of strong white box\nadversarial attacks, including new attacks of our own. Additionally, we find\nthat a robust policy noticeably improves DRL performance even without an\nadversary in a number of environments. Our code is available at\nhttps://github.com/chenhongge/StateAdvDRL.",
          "link": "http://arxiv.org/abs/2003.08938",
          "publishedOn": "2021-07-15T01:59:04.765Z",
          "wordCount": 740,
          "title": "Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations. (arXiv:2003.08938v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1\">Sheng Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dingwen Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1\">Franck Cappello</a>",
          "description": "Error-bounded lossy compression is becoming an indispensable technique for\nthe success of today's scientific projects with vast volumes of data produced\nduring the simulations or instrument data acquisitions. Not only can it\nsignificantly reduce data size, but it also can control the compression errors\nbased on user-specified error bounds. Autoencoder (AE) models have been widely\nused in image compression, but few AE-based compression approaches support\nerror-bounding features, which are highly required by scientific applications.\nTo address this issue, we explore using convolutional autoencoders to improve\nerror-bounded lossy compression for scientific data, with the following three\nkey contributions. (1) We provide an in-depth investigation of the\ncharacteristics of various autoencoder models and develop an error-bounded\nautoencoder-based framework in terms of the SZ model. (2) We optimize the\ncompression quality for main stages in our designed AE-based error-bounded\ncompression framework, fine-tuning the block sizes and latent sizes and also\noptimizing the compression efficiency of latent vectors. (3) We evaluate our\nproposed solution using five real-world scientific datasets and comparing them\nwith six other related works. Experiments show that our solution exhibits a\nvery competitive compression quality from among all the compressors in our\ntests. In absolute terms, it can obtain a much better compression quality (100%\n~ 800% improvement in compression ratio with the same data distortion) compared\nwith SZ2.1 and ZFP in cases with a high compression ratio.",
          "link": "http://arxiv.org/abs/2105.11730",
          "publishedOn": "2021-07-15T01:59:04.751Z",
          "wordCount": 705,
          "title": "Exploring Autoencoder-based Error-bounded Compression for Scientific Data. (arXiv:2105.11730v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1\">Zhaobin Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xuan Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Rongye Shi</a>",
          "description": "Car-following behavior has been extensively studied using physics-based\nmodels, such as the Intelligent Driver Model. These models successfully\ninterpret traffic phenomena observed in the real-world but may not fully\ncapture the complex cognitive process of driving. Deep learning models, on the\nother hand, have demonstrated their power in capturing observed traffic\nphenomena but require a large amount of driving data to train. This paper aims\nto develop a family of neural network based car-following models that are\ninformed by physics-based models, which leverage the advantage of both\nphysics-based (being data-efficient and interpretable) and deep learning based\n(being generalizable) models. We design physics-informed deep learning\ncar-following (PIDL-CF) architectures encoded with two popular physics-based\nmodels - IDM and OVM, on which acceleration is predicted for four traffic\nregimes: acceleration, deceleration, cruising, and emergency braking. Two types\nof PIDL-CFM problems are studied, one to predict acceleration only and the\nother to jointly predict acceleration and discover model parameters. We also\ndemonstrate the superior performance of PIDL with the Next Generation\nSIMulation (NGSIM) dataset over baselines, especially when the training data is\nsparse. The results demonstrate the superior performance of neural networks\ninformed by physics over those without. The developed PIDL-CF framework holds\nthe potential for system identification of driving models and for the\ndevelopment of driving-based controls for automated vehicles.",
          "link": "http://arxiv.org/abs/2012.13376",
          "publishedOn": "2021-07-15T01:59:04.732Z",
          "wordCount": 697,
          "title": "A Physics-Informed Deep Learning Paradigm for Car-Following Models. (arXiv:2012.13376v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "The rise of big data analytics on top of NLP increases the computational\nburden for text processing at scale. The problems faced in NLP are very high\ndimensional text, so it takes a high computation resource. The MapReduce allows\nparallelization of large computations and can improve the efficiency of text\nprocessing. This research aims to study the effect of big data processing on\nNLP tasks based on a deep learning approach. We classify a big text of news\ntopics with fine-tuning BERT used pre-trained models. Five pre-trained models\nwith a different number of parameters were used in this study. To measure the\nefficiency of this method, we compared the performance of the BERT with the\npipelines from Spark NLP. The result shows that BERT without Spark NLP gives\nhigher accuracy compared to BERT with Spark NLP. The accuracy average and\ntraining time of all models using BERT is 0.9187 and 35 minutes while using\nBERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will\ntake more computation resources and need a longer time to complete the tasks.\nHowever, the accuracy of BERT with Spark NLP only decreased by an average of\n5.7%, while the training time was reduced significantly by 62.9% compared to\nBERT without Spark NLP.",
          "link": "http://arxiv.org/abs/2107.06785",
          "publishedOn": "2021-07-15T01:59:04.696Z",
          "wordCount": 653,
          "title": "Large-Scale News Classification using BERT Language Model: Spark NLP Approach. (arXiv:2107.06785v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06845",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1\">Basit O. Alawode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alfarraj_M/0/1/0/all/0/1\">Motaz Alfarraj</a>",
          "description": "The recent application of deep learning (DL) to various tasks has seen the\nperformance of classical techniques surpassed by their DL-based counterparts.\nAs a result, DL has equally seen application in the removal of noise from\nimages. In particular, the use of deep feed-forward convolutional neural\nnetworks (DnCNNs) has been investigated for denoising. It utilizes advances in\nDL techniques such as deep architecture, residual learning, and batch\nnormalization to achieve better denoising performance when compared with the\nother classical state-of-the-art denoising algorithms. However, its deep\narchitecture resulted in a huge set of trainable parameters. Meta-optimization\nis a training approach of enabling algorithms to learn to train themselves by\nthemselves. Training algorithms using meta-optimizers have been shown to enable\nalgorithms to achieve better performance when compared to the classical\ngradient descent-based training approach. In this work, we investigate the\napplication of the meta-optimization training approach to the DnCNN denoising\nalgorithm to enhance its denoising capability. Our preliminary experiments on\nsimpler algorithms reveal the prospects of utilizing the meta-optimization\ntraining approach towards the enhancement of the DnCNN denoising capability.",
          "link": "http://arxiv.org/abs/2107.06845",
          "publishedOn": "2021-07-15T01:59:04.688Z",
          "wordCount": 621,
          "title": "Meta-Optimization of Deep CNN for Image Denoising Using LSTM. (arXiv:2107.06845v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.14227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>",
          "description": "Negative sampling, which samples negative triplets from non-observed ones in\nknowledge graph (KG), is an essential step in KG embedding. Recently,\ngenerative adversarial network (GAN), has been introduced in negative sampling.\nBy sampling negative triplets with large gradients, these methods avoid the\nproblem of vanishing gradient and thus obtain better performance. However, they\nmake the original model more complex and harder to train. In this paper,\nmotivated by the observation that negative triplets with large gradients are\nimportant but rare, we propose to directly keep track of them with the cache.\nIn this way, our method acts as a \"distilled\" version of previous GAN-based\nmethods, which does not waste training time on additional parameters to fit the\nfull distribution of negative triplets. However, how to sample from and update\nthe cache are two critical questions. We propose to solve these issues by\nautomated machine learning techniques. The automated version also covers\nGAN-based methods as special cases. Theoretical explanation of NSCaching is\nalso provided, justifying the superior over fixed sampling scheme. Besides, we\nfurther extend NSCaching with skip-gram model for graph embedding. Finally,\nextensive experiments show that our method can gain significant improvements on\nvarious KG embedding models and the skip-gram model, and outperforms the\nstate-of-the-art negative sampling methods.",
          "link": "http://arxiv.org/abs/2010.14227",
          "publishedOn": "2021-07-15T01:59:04.682Z",
          "wordCount": 686,
          "title": "Efficient, Simple and Automated Negative Sampling for Knowledge Graph Embedding. (arXiv:2010.14227v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_M/0/1/0/all/0/1\">Moran Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harshaw_C/0/1/0/all/0/1\">Christopher Harshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1\">Amin Karbasi</a>",
          "description": "We present SimultaneousGreedys, a deterministic algorithm for constrained\nsubmodular maximization. At a high level, the algorithm maintains $\\ell$\nsolutions and greedily updates them in a simultaneous fashion.\nSimultaneousGreedys achieves the tightest known approximation guarantees for\nboth $k$-extendible systems and the more general $k$-systems, which are\n$(k+1)^2/k = k + \\mathcal{O}(1)$ and $(1 + \\sqrt{k+2})^2 = k +\n\\mathcal{O}(\\sqrt{k})$, respectively. This is in contrast to previous\nalgorithms, which are designed to provide tight approximation guarantees in one\nsetting, but not both. We also improve the analysis of RepeatedGreedy, showing\nthat it achieves an approximation ratio of $k + \\mathcal{O}(\\sqrt{k})$ for\n$k$-systems when allowed to run for $\\mathcal{O}(\\sqrt{k})$ iterations, an\nimprovement in both the runtime and approximation over previous analyses. We\ndemonstrate that both algorithms may be modified to run in nearly linear time\nwith an arbitrarily small loss in the approximation.\n\nBoth SimultaneousGreedys and RepeatedGreedy are flexible enough to\nincorporate the intersection of $m$ additional knapsack constraints, while\nretaining similar approximation guarantees: both algorithms yield an\napproximation guarantee of roughly $k + 2m + \\mathcal{O}(\\sqrt{k+m})$ for\n$k$-systems and SimultaneousGreedys enjoys an improved approximation guarantee\nof $k+2m + \\mathcal{O}(\\sqrt{m})$ for $k$-extendible systems. To complement our\nalgorithmic contributions, we provide a hardness result which states that no\nalgorithm making polynomially many oracle queries can achieve an approximation\nbetter than $k + 1/2 + \\varepsilon$. We also present SubmodularGreedy.jl, a\nJulia package which implements these algorithms and may be downloaded at\nhttps://github.com/crharshaw/SubmodularGreedy.jl . Finally, we test the\neffectiveness of these algorithms on real datasets.",
          "link": "http://arxiv.org/abs/2009.13998",
          "publishedOn": "2021-07-15T01:59:04.656Z",
          "wordCount": 734,
          "title": "How Do You Want Your Greedy: Simultaneous or Repeated?. (arXiv:2009.13998v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1\">Felix Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1\">Christian Borgelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1\">Oliver Deussen</a>",
          "description": "Sorting and ranking supervision is a method for training neural networks\nend-to-end based on ordering constraints. That is, the ground truth order of\nsets of samples is known, while their absolute values remain unsupervised. For\nthat, we propose differentiable sorting networks by relaxing their pairwise\nconditional swap operations. To address the problems of vanishing gradients and\nextensive blurring that arise with larger numbers of layers, we propose mapping\nactivations to regions with moderate gradients. We consider odd-even as well as\nbitonic sorting networks, which outperform existing relaxations of the sorting\noperation. We show that bitonic sorting networks can achieve stable training on\nlarge input sets of up to 1024 elements.",
          "link": "http://arxiv.org/abs/2105.04019",
          "publishedOn": "2021-07-15T01:59:04.650Z",
          "wordCount": 595,
          "title": "Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision. (arXiv:2105.04019v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reinhold_J/0/1/0/all/0/1\">Jacob C. Reinhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carass_A/0/1/0/all/0/1\">Aaron Carass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>",
          "description": "Precision medicine involves answering counterfactual questions such as \"Would\nthis patient respond better to treatment A or treatment B?\" These types of\nquestions are causal in nature and require the tools of causal inference to be\nanswered, e.g., with a structural causal model (SCM). In this work, we develop\nan SCM that models the interaction between demographic information, disease\ncovariates, and magnetic resonance (MR) images of the brain for people with\nmultiple sclerosis. Inference in the SCM generates counterfactual images that\nshow what an MR image of the brain would look like if demographic or disease\ncovariates are changed. These images can be used for modeling disease\nprogression or used for image processing tasks where controlling for\nconfounders is necessary.",
          "link": "http://arxiv.org/abs/2103.03158",
          "publishedOn": "2021-07-15T01:59:04.644Z",
          "wordCount": 616,
          "title": "A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shim_J/0/1/0/all/0/1\">Jae-hun Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kyeongbo Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Suk-Ju Kang</a>",
          "description": "Neural architecture search (NAS), an important branch of automatic machine\nlearning, has become an effective approach to automate the design of deep\nlearning models. However, the major issue in NAS is how to reduce the large\nsearch time imposed by the heavy computational burden. While most recent\napproaches focus on pruning redundant sets or developing new search\nmethodologies, this paper attempts to formulate the problem based on the data\ncuration manner. Our key strategy is to search the architecture using\nsummarized data distribution, i.e., core-set. Typically, many NAS algorithms\nseparate searching and training stages, and the proposed core-set methodology\nis only used in search stage, thus their performance degradation can be\nminimized. In our experiments, we were able to save overall computational time\nfrom 30.8 hours to 3.5 hours, 8.8x reduction, on a single RTX 3090 GPU without\nsacrificing accuracy.",
          "link": "http://arxiv.org/abs/2107.06869",
          "publishedOn": "2021-07-15T01:59:04.638Z",
          "wordCount": 589,
          "title": "Core-set Sampling for Efficient Neural Architecture Search. (arXiv:2107.06869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tengyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1\">John Langford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1\">Paul Mineiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>",
          "description": "Consider a prosthetic arm, learning to adapt to its user's control signals.\nWe propose Interaction-Grounded Learning for this novel setting, in which a\nlearner's goal is to interact with the environment with no grounding or\nexplicit reward to optimize its policies. Such a problem evades common RL\nsolutions which require an explicit reward. The learning agent observes a\nmultidimensional context vector, takes an action, and then observes a\nmultidimensional feedback vector. This multidimensional feedback vector has no\nexplicit reward information. In order to succeed, the algorithm must learn how\nto evaluate the feedback vector to discover a latent reward signal, with which\nit can ground its policies without supervision. We show that in an\nInteraction-Grounded Learning setting, with certain natural assumptions, a\nlearner can discover the latent reward and ground its policy for successful\ninteraction. We provide theoretical guarantees and a proof-of-concept empirical\nevaluation to demonstrate the effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2106.04887",
          "publishedOn": "2021-07-15T01:59:04.623Z",
          "wordCount": 613,
          "title": "Interaction-Grounded Learning. (arXiv:2106.04887v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wairagkar_M/0/1/0/all/0/1\">Maitreyee Wairagkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villeneuve_E/0/1/0/all/0/1\">Emma Villeneuve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1\">Rachel King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janko_B/0/1/0/all/0/1\">Balazs Janko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnett_M/0/1/0/all/0/1\">Malcolm Burnett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashburn_A/0/1/0/all/0/1\">Ann Ashburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Veena Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherratt_R/0/1/0/all/0/1\">R. Simon Sherratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holderbaum_W/0/1/0/all/0/1\">William Holderbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwin_W/0/1/0/all/0/1\">William Harwin</a>",
          "description": "Sit-to-stand transitions are an important part of activities of daily living\nand play a key role in functional mobility in humans. The sit-to-stand movement\nis often affected in older adults due to frailty and in patients with motor\nimpairments such as Parkinson's disease leading to falls. Studying kinematics\nof sit-to-stand transitions can provide insight in assessment, monitoring and\ndeveloping rehabilitation strategies for the affected populations. We propose a\nthree-segment body model for estimating sit-to-stand kinematics using only two\nwearable inertial sensors, placed on the shank and back. Reducing the number of\nsensors to two instead of one per body segment facilitates monitoring and\nclassifying movements over extended periods, making it more comfortable to wear\nwhile reducing the power requirements of sensors. We applied this model on 10\nyounger healthy adults (YH), 12 older healthy adults (OH) and 12 people with\nParkinson's disease (PwP). We have achieved this by incorporating unique\nsit-to-stand classification technique using unsupervised learning in the model\nbased reconstruction of angular kinematics using extended Kalman filter. Our\nproposed model showed that it was possible to successfully estimate thigh\nkinematics despite not measuring the thigh motion with inertial sensor. We\nclassified sit-to-stand transitions, sitting and standing states with the\naccuracies of 98.67%, 94.20% and 91.41% for YH, OH and PwP respectively. We\nhave proposed a novel integrated approach of modelling and classification for\nestimating the body kinematics during sit-to-stand motion and successfully\napplied it on YH, OH and PwP groups.",
          "link": "http://arxiv.org/abs/2107.06859",
          "publishedOn": "2021-07-15T01:59:04.617Z",
          "wordCount": 699,
          "title": "A novel approach for modelling and classifying sit-to-stand kinematics using inertial sensors. (arXiv:2107.06859v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06677",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_Estevez_M/0/1/0/all/0/1\">M. A. Gutierrez-Estevez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kasparick_M/0/1/0/all/0/1\">Martin Kasparick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cavalvante_R/0/1/0/all/0/1\">Renato L. G. Cavalvante</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stanczak_S/0/1/0/all/0/1\">S&#x142;awomir Sta&#x144;czak</a>",
          "description": "Learning any-to-any (A2A) path loss maps, where the objective is the\nreconstruction of path loss between any two given points in a map, might be a\nkey enabler for many applications that rely on device-to-device (D2D)\ncommunication. Such applications include machine-type communications (MTC) or\nvehicle-to-vehicle (V2V) communications. Current approaches for learning A2A\nmaps are either model-based methods, or pure data-driven methods. Model-based\nmethods have the advantage that they can generate reliable estimations with low\ncomputational complexity, but they cannot exploit information coming from data.\nPure data-driven methods can achieve good performance without assuming any\nphysical model, but their complexity and their lack of robustness is not\nacceptable for many applications. In this paper, we propose a novel hybrid\nmodel and data-driven approach that fuses information obtained from datasets\nand models in an online fashion. To that end, we leverage the framework of\nstochastic learning to deal with the sequential arrival of samples and propose\nan online algorithm that alternatively and sequentially minimizes the original\nnon-convex problem. A proof of convergence is presented, along with experiments\nbased firstly on synthetic data, and secondly on a more realistic dataset for\nV2X, with both experiments showing promising results.",
          "link": "http://arxiv.org/abs/2107.06677",
          "publishedOn": "2021-07-15T01:59:04.609Z",
          "wordCount": 650,
          "title": "Hybrid Model and Data Driven Algorithm for Online Learning of Any-to-Any Path Loss Maps. (arXiv:2107.06677v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00120",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1\">Timo Lohrenz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>",
          "description": "Stream fusion, also known as system combination, is a common technique in\nautomatic speech recognition for traditional hybrid hidden Markov model\napproaches, yet mostly unexplored for modern deep neural network end-to-end\nmodel architectures. Here, we investigate various fusion techniques for the\nall-attention-based encoder-decoder architecture known as the transformer,\nstriving to achieve optimal fusion by investigating different fusion levels in\nan example single-microphone setting with fusion of standard magnitude and\nphase features. We introduce a novel multi-encoder learning method that\nperforms a weighted combination of two encoder-decoder multi-head attention\noutputs only during training. Employing then only the magnitude feature encoder\nin inference, we are able to show consistent improvement on Wall Street Journal\n(WSJ) with language model and on Librispeech, without increase in runtime or\nparameters. Combining two such multi-encoder trained models by a simple late\nfusion in inference, we achieve state-of-the-art performance for\ntransformer-based models on WSJ with a significant WER reduction of 19%\nrelative compared to the current benchmark approach.",
          "link": "http://arxiv.org/abs/2104.00120",
          "publishedOn": "2021-07-15T01:59:04.603Z",
          "wordCount": 638,
          "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition. (arXiv:2104.00120v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08817",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Graf_F/0/1/0/all/0/1\">Florian Graf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hofer_C/0/1/0/all/0/1\">Christoph D. Hofer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kwitt_R/0/1/0/all/0/1\">Roland Kwitt</a>",
          "description": "Minimizing cross-entropy over the softmax scores of a linear map composed\nwith a high-capacity encoder is arguably the most popular choice for training\nneural networks on supervised learning tasks. However, recent works show that\none can directly optimize the encoder instead, to obtain equally (or even more)\ndiscriminative representations via a supervised variant of a contrastive\nobjective. In this work, we address the question whether there are fundamental\ndifferences in the sought-for representation geometry in the output space of\nthe encoder at minimal loss. Specifically, we prove, under mild assumptions,\nthat both losses attain their minimum once the representations of each class\ncollapse to the vertices of a regular simplex, inscribed in a hypersphere. We\nprovide empirical evidence that this configuration is attained in practice and\nthat reaching a close-to-optimal state typically indicates good generalization\nperformance. Yet, the two losses show remarkably different optimization\nbehavior. The number of iterations required to perfectly fit to data scales\nsuperlinearly with the amount of randomly flipped labels for the supervised\ncontrastive loss. This is in contrast to the approximately linear scaling\npreviously reported for networks trained with cross-entropy.",
          "link": "http://arxiv.org/abs/2102.08817",
          "publishedOn": "2021-07-15T01:59:04.598Z",
          "wordCount": 644,
          "title": "Dissecting Supervised Contrastive Learning. (arXiv:2102.08817v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaowei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "The label noise transition matrix, characterizing the probabilities of a\ntraining instance being wrongly annotated, is crucial to designing popular\nsolutions to learning with noisy labels. Existing works heavily rely on finding\n\"anchor points\" or their approximates, defined as instances belonging to a\nparticular class almost surely. Nonetheless, finding anchor points remains a\nnon-trivial task, and the estimation accuracy is also often throttled by the\nnumber of available anchor points. In this paper, we propose an alternative\noption to the above task. Our main contribution is the discovery of an\nefficient estimation procedure based on a clusterability condition. We prove\nthat with clusterable representations of features, using up to third-order\nconsensuses of noisy labels among neighbor representations is sufficient to\nestimate a unique transition matrix. Compared with methods using anchor points,\nour approach uses substantially more instances and benefits from a much better\nsample complexity. We demonstrate the estimation accuracy and advantages of our\nestimates using both synthetic noisy labels (on CIFAR-10/100) and real\nhuman-level noisy labels (on Clothing1M and our self-collected human-annotated\nCIFAR-10). Our code and human-level noisy CIFAR-10 labels are available at\nhttps://github.com/UCSC-REAL/HOC.",
          "link": "http://arxiv.org/abs/2102.05291",
          "publishedOn": "2021-07-15T01:59:04.592Z",
          "wordCount": 662,
          "title": "Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels. (arXiv:2102.05291v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yinghua Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yuangang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W.Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xin Yao</a>",
          "description": "This paper proposes Differential-Critic Generative Adversarial Network\n(DiCGAN) to learn the distribution of user-desired data when only partial\ninstead of the entire dataset possesses the desired property, which generates\ndesired data that meets user's expectations and can assist in designing\nbiological products with desired properties. Existing approaches select the\ndesired samples first and train regular GANs on the selected samples to derive\nthe user-desired data distribution. However, the selection of the desired data\nrelies on an expert criterion and supervision over the entire dataset. DiCGAN\nintroduces a differential critic that can learn the preference direction from\nthe pairwise preferences, which is amateur knowledge and can be defined on part\nof the training data. The resultant critic guides the generation of the desired\ndata instead of the whole data. Specifically, apart from the Wasserstein GAN\nloss, a ranking loss of the pairwise preferences is defined over the critic. It\nendows the difference of critic values between each pair of samples with the\npairwise preference relation. The higher critic value indicates that the sample\nis preferred by the user. Thus training the generative model for higher critic\nvalues encourages the generation of user-preferred samples. Extensive\nexperiments show that our DiCGAN achieves state-of-the-art performance in\nlearning the user-desired data distributions, especially in the cases of\ninsufficient desired data and limited supervision.",
          "link": "http://arxiv.org/abs/2107.06700",
          "publishedOn": "2021-07-15T01:59:04.558Z",
          "wordCount": 654,
          "title": "Differential-Critic GAN: Generating What You Want by a Cue of Preferences. (arXiv:2107.06700v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sangmin Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungnyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jongwoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gihun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1\">Seungjong Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>",
          "description": "This paper proposes a novel contrastive learning framework, coined as\nSelf-Contrastive (SelfCon) Learning, that self-contrasts within multiple\noutputs from the different levels of a network. We confirmed that SelfCon loss\nguarantees the lower bound of mutual information (MI) between the intermediate\nand last representations. Besides, we empirically showed, via various MI\nestimators, that SelfCon loss highly correlates to the increase of MI and\nbetter classification performance. In our experiments, SelfCon surpasses\nsupervised contrastive (SupCon) learning without the need for a multi-viewed\nbatch and with the cheaper computational cost. Especially on ResNet-18, we\nachieved top-1 classification accuracy of 76.45% for the CIFAR-100 dataset,\nwhich is 2.87% and 4.36% higher than SupCon and cross-entropy loss,\nrespectively. We found that mitigating both vanishing gradient and overfitting\nissue makes our method outperform the counterparts.",
          "link": "http://arxiv.org/abs/2106.15499",
          "publishedOn": "2021-07-15T01:59:04.552Z",
          "wordCount": 572,
          "title": "Self-Contrastive Learning. (arXiv:2106.15499v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01926",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Coulombe_P/0/1/0/all/0/1\">Philippe Goulet Coulombe</a>",
          "description": "Random Forest's performance can be matched by a single slow-growing tree\n(SGT), which uses a learning rate to tame CART's greedy algorithm. SGT exploits\nthe view that CART is an extreme case of an iterative weighted least square\nprocedure. Moreover, a unifying view of Boosted Trees (BT) and Random Forests\n(RF) is presented. Greedy ML algorithms' outcomes can be improved using either\n\"slow learning\" or diversification. SGT applies the former to estimate a single\ndeep tree, and Booging (bagging stochastic BT with a high learning rate) uses\nthe latter with additive shallow trees. The performance of this tree ensemble\nquaternity (Booging, BT, SGT, RF) is assessed on simulated and real regression\ntasks.",
          "link": "http://arxiv.org/abs/2103.01926",
          "publishedOn": "2021-07-15T01:59:04.532Z",
          "wordCount": 554,
          "title": "Slow-Growing Trees. (arXiv:2103.01926v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_R/0/1/0/all/0/1\">Reshma Rastogi</a> (nee. Khemchandani), <a href=\"http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1\">Aman Pal</a>",
          "description": "In any learning framework, an expert knowledge always plays a crucial role.\nBut, in the field of machine learning, the knowledge offered by an expert is\nrarely used. Moreover, machine learning algorithms (SVM based) generally use\nhinge loss function which is sensitive towards the noise. Thus, in order to get\nthe advantage from an expert knowledge and to reduce the sensitivity towards\nthe noise, in this paper, we propose privileged information based Twin Pinball\nSupport Vector Machine classifier (Pin-TWSVMPI) where expert's knowledge is in\nthe form of privileged information. The proposed Pin-TWSVMPI incorporates\nprivileged information by using correcting function so as to obtain two\nnonparallel decision hyperplanes. Further, in order to make computations more\nefficient and fast, we use Sequential Minimal Optimization (SMO) technique for\nobtaining the classifier and have also shown its application for Pedestrian\ndetection and Handwritten digit recognition. Further, for UCI datasets, we\nfirst implement a procedure which extracts privileged information from the\nfeatures of the dataset which are then further utilized by Pin-TWSVMPI that\nleads to enhancement in classification accuracy with comparatively lesser\ncomputational time.",
          "link": "http://arxiv.org/abs/2107.06744",
          "publishedOn": "2021-07-15T01:59:04.461Z",
          "wordCount": 612,
          "title": "Efficient Learning of Pinball TWSVM using Privileged Information and its applications. (arXiv:2107.06744v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zheyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1\">Da-Cheng Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaobo Sharon Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>",
          "description": "Emerging device-based Computing-in-memory (CiM) has been proved to be a\npromising candidate for high-energy efficiency deep neural network (DNN)\ncomputations. However, most emerging devices suffer uncertainty issues,\nresulting in a difference between actual data stored and the weight value it is\ndesigned to be. This leads to an accuracy drop from trained models to actually\ndeployed platforms. In this work, we offer a thorough analysis of the effect of\nsuch uncertainties-induced changes in DNN models. To reduce the impact of\ndevice uncertainties, we propose UAE, an uncertainty-aware Neural Architecture\nSearch scheme to identify a DNN model that is both accurate and robust against\ndevice uncertainties.",
          "link": "http://arxiv.org/abs/2107.06871",
          "publishedOn": "2021-07-15T01:59:04.454Z",
          "wordCount": 553,
          "title": "Uncertainty Modeling of Emerging Device-based Computing-in-Memory Neural Accelerators with Application to Neural Architecture Search. (arXiv:2107.06871v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bighashdel_A/0/1/0/all/0/1\">Ariyan Bighashdel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meletis_P/0/1/0/all/0/1\">Panagiotis Meletis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jancura_P/0/1/0/all/0/1\">Pavol Jancura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1\">Gijs Dubbelman</a>",
          "description": "This paper presents a deep Inverse Reinforcement Learning (IRL) framework\nthat can learn an a priori unknown number of nonlinear reward functions from\nunlabeled experts' demonstrations. For this purpose, we employ the tools from\nDirichlet processes and propose an adaptive approach to simultaneously account\nfor both complex and unknown number of reward functions. Using the conditional\nmaximum entropy principle, we model the experts' multi-intention behaviors as a\nmixture of latent intention distributions and derive two algorithms to estimate\nthe parameters of the deep reward network along with the number of experts'\nintentions from unlabeled demonstrations. The proposed algorithms are evaluated\non three benchmarks, two of which have been specifically extended in this study\nfor multi-intention IRL, and compared with well-known baselines. We demonstrate\nthrough several experiments the advantages of our algorithms over the existing\napproaches and the benefits of online inferring, rather than fixing beforehand,\nthe number of expert's intentions.",
          "link": "http://arxiv.org/abs/2107.06692",
          "publishedOn": "2021-07-15T01:59:04.448Z",
          "wordCount": 586,
          "title": "Deep Adaptive Multi-Intention Inverse Reinforcement Learning. (arXiv:2107.06692v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06658",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Levine_M/0/1/0/all/0/1\">Matthew E. Levine</a>, <a href=\"http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1\">Andrew M. Stuart</a>",
          "description": "The development of data-informed predictive models for dynamical systems is\nof widespread interest in many disciplines. We present a unifying framework for\nblending mechanistic and machine-learning approaches to identify dynamical\nsystems from data. We compare pure data-driven learning with hybrid models\nwhich incorporate imperfect domain knowledge. We cast the problem in both\ncontinuous- and discrete-time, for problems in which the model error is\nmemoryless and in which it has significant memory, and we compare data-driven\nand hybrid approaches experimentally. Our formulation is agnostic to the chosen\nmachine learning model.\n\nUsing Lorenz '63 and Lorenz '96 Multiscale systems, we find that hybrid\nmethods substantially outperform solely data-driven approaches in terms of data\nhunger, demands for model complexity, and overall predictive performance. We\nalso find that, while a continuous-time framing allows for robustness to\nirregular sampling and desirable domain-interpretability, a discrete-time\nframing can provide similar or better predictive performance, especially when\ndata are undersampled and the vector field cannot be resolved.\n\nWe study model error from the learning theory perspective, defining excess\nrisk and generalization error; for a linear model of the error used to learn\nabout ergodic dynamical systems, both errors are bounded by terms that diminish\nwith the square-root of T. We also illustrate scenarios that benefit from\nmodeling with memory, proving that continuous-time recurrent neural networks\n(RNNs) can, in principle, learn memory-dependent model error and reconstruct\nthe original system arbitrarily well; numerical results depict challenges in\nrepresenting memory by this approach. We also connect RNNs to reservoir\ncomputing and thereby relate the learning of memory-dependent error to recent\nwork on supervised learning between Banach spaces using random features.",
          "link": "http://arxiv.org/abs/2107.06658",
          "publishedOn": "2021-07-15T01:59:04.443Z",
          "wordCount": 711,
          "title": "A Framework for Machine Learning of Model Error in Dynamical Systems. (arXiv:2107.06658v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Shohin Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1\">Arsalan Mousavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishman_A/0/1/0/all/0/1\">Adam Fishman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhachev_M/0/1/0/all/0/1\">Maxim Likhachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "Zero-shot execution of unseen robotic tasks is important to allowing robots\nto perform a wide variety of tasks in human environments, but collecting the\namounts of data necessary to train end-to-end policies in the real-world is\noften infeasible. We describe an approach for sim-to-real training that can\naccomplish unseen robotic tasks using models learned in simulation to ground\ncomponents of a simple task planner. We learn a library of parameterized\nskills, along with a set of predicates-based preconditions and termination\nconditions, entirely in simulation. We explore a block-stacking task because it\nhas a clear structure, where multiple skills must be chained together, but our\nmethods are applicable to a wide range of other problems and domains, and can\ntransfer from simulation to the real-world with no fine tuning. The system is\nable to recognize failures and accomplish long-horizon tasks from perceptual\ninput, which is critical for real-world execution. We evaluate our proposed\napproach in both simulation and in the real-world, showing an increase in\nsuccess rate from 91.6% to 98% in simulation and from 10% to 80% success rate\nin the real-world as compared with naive baselines. For experiment videos\nincluding both real-world and simulation, see:\nhttps://www.youtube.com/playlist?list=PL-oD0xHUngeLfQmpngYkGFZarstfPOXqX",
          "link": "http://arxiv.org/abs/2011.08694",
          "publishedOn": "2021-07-15T01:59:04.437Z",
          "wordCount": 681,
          "title": "Reactive Long Horizon Task Execution via Visual Skill and Precondition Models. (arXiv:2011.08694v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsouvalas_V/0/1/0/all/0/1\">Vasileios Tsouvalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1\">Aaqib Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozcelebi_T/0/1/0/all/0/1\">Tanir Ozcelebi</a>",
          "description": "Federated Learning is a distributed machine learning paradigm dealing with\ndecentralized and personal datasets. Since data reside on devices like\nsmartphones and virtual assistants, labeling is entrusted to the clients, or\nlabels are extracted in an automated way. Specifically, in the case of audio\ndata, acquiring semantic annotations can be prohibitively expensive and\ntime-consuming. As a result, an abundance of audio data remains unlabeled and\nunexploited on users' devices. Most existing federated learning approaches\nfocus on supervised learning without harnessing the unlabeled data. In this\nwork, we study the problem of semi-supervised learning of audio models via\nself-training in conjunction with federated learning. We propose FedSTAR to\nexploit large-scale on-device unlabeled data to improve the generalization of\naudio recognition models. We further demonstrate that self-supervised\npre-trained models can accelerate the training of on-device models,\nsignificantly improving convergence to within fewer training rounds. We conduct\nexperiments on diverse public audio classification datasets and investigate the\nperformance of our models under varying percentages of labeled and unlabeled\ndata. Notably, we show that with as little as 3% labeled data available,\nFedSTAR on average can improve the recognition rate by 13.28% compared to the\nfully supervised federated model.",
          "link": "http://arxiv.org/abs/2107.06877",
          "publishedOn": "2021-07-15T01:59:04.431Z",
          "wordCount": 636,
          "title": "Federated Self-Training for Semi-Supervised Audio Recognition. (arXiv:2107.06877v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deguerre_B/0/1/0/all/0/1\">Benjamin Deguerre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1\">Clement Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1\">Gilles Gasso</a>",
          "description": "Object detection in images has reached unprecedented performances. The\nstate-of-the-art methods rely on deep architectures that extract salient\nfeatures and predict bounding boxes enclosing the objects of interest. These\nmethods essentially run on RGB images. However, the RGB images are often\ncompressed by the acquisition devices for storage purpose and transfer\nefficiency. Hence, their decompression is required for object detectors. To\ngain in efficiency, this paper proposes to take advantage of the compressed\nrepresentation of images to carry out object detection usable in constrained\nresources conditions.\n\nSpecifically, we focus on JPEG images and propose a thorough analysis of\ndetection architectures newly designed in regard of the peculiarities of the\nJPEG norm. This leads to a $\\times 1.7$ speed up in comparison with a standard\nRGB-based architecture, while only reducing the detection performance by 5.5%.\nAdditionally, our empirical findings demonstrate that only part of the\ncompressed JPEG information, namely the luminance component, may be required to\nmatch detection accuracy of the full input methods.",
          "link": "http://arxiv.org/abs/2006.05732",
          "publishedOn": "2021-07-15T01:59:04.425Z",
          "wordCount": 645,
          "title": "Object Detection in the DCT Domain: is Luminance the Solution?. (arXiv:2006.05732v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elkabetz_O/0/1/0/all/0/1\">Omer Elkabetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Nadav Cohen</a>",
          "description": "Existing analyses of optimization in deep learning are either continuous,\nfocusing on (variants of) gradient flow, or discrete, directly treating\n(variants of) gradient descent. Gradient flow is amenable to theoretical\nanalysis, but is stylized and disregards computational efficiency. The extent\nto which it represents gradient descent is an open question in deep learning\ntheory. The current paper studies this question. Viewing gradient descent as an\napproximate numerical solution to the initial value problem of gradient flow,\nwe find that the degree of approximation depends on the curvature along the\nlatter's trajectory. We then show that over deep neural networks with\nhomogeneous activations, gradient flow trajectories enjoy favorable curvature,\nsuggesting they are well approximated by gradient descent. This finding allows\nus to translate an analysis of gradient flow over deep linear neural networks\ninto a guarantee that gradient descent efficiently converges to global minimum\nalmost surely under random initialization. Experiments suggest that over simple\ndeep neural networks, gradient descent with conventional step size is indeed\nclose to the continuous limit. We hypothesize that the theory of gradient flows\nwill be central to unraveling mysteries behind deep learning.",
          "link": "http://arxiv.org/abs/2107.06608",
          "publishedOn": "2021-07-15T01:59:04.408Z",
          "wordCount": 622,
          "title": "Continuous vs. Discrete Optimization of Deep Neural Networks. (arXiv:2107.06608v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Civitarese_D/0/1/0/all/0/1\">Daniel Salles Civitarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szwarcman_D/0/1/0/all/0/1\">Daniela Szwarcman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1\">Bianca Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1\">Campbell Watson</a>",
          "description": "An impact of climate change is the increase in frequency and intensity of\nextreme precipitation events. However, confidently predicting the likelihood of\nextreme precipitation at seasonal scales remains an outstanding challenge.\nHere, we present an approach to forecasting the quantiles of the maximum daily\nprecipitation in each week up to six months ahead using the temporal fusion\ntransformer (TFT) model. Through experiments in two regions, we compare TFT\npredictions with those of two baselines: climatology and a calibrated ECMWF\nSEAS5 ensemble forecast (S5). Our results show that, in terms of quantile risk\nat six month lead time, the TFT predictions significantly outperform those from\nS5 and show an overall small improvement compared to climatology. The TFT also\nresponds positively to departures from normal that climatology cannot.",
          "link": "http://arxiv.org/abs/2107.06846",
          "publishedOn": "2021-07-15T01:59:04.368Z",
          "wordCount": 563,
          "title": "Extreme Precipitation Seasonal Forecast Using a Transformer Neural Network. (arXiv:2107.06846v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trabucco_B/0/1/0/all/0/1\">Brandon Trabucco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aviral Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xinyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Computational design problems arise in a number of settings, from synthetic\nbiology to computer architectures. In this paper, we aim to solve data-driven\nmodel-based optimization (MBO) problems, where the goal is to find a design\ninput that maximizes an unknown objective function provided access to only a\nstatic dataset of prior experiments. Such data-driven optimization procedures\nare the only practical methods in many real-world domains where active data\ncollection is expensive (e.g., when optimizing over proteins) or dangerous\n(e.g., when optimizing over aircraft designs). Typical methods for MBO that\noptimize the design against a learned model suffer from distributional shift:\nit is easy to find a design that \"fools\" the model into predicting a high\nvalue. To overcome this, we propose conservative objective models (COMs), a\nmethod that learns a model of the objective function that lower bounds the\nactual value of the ground-truth objective on out-of-distribution inputs, and\nuses it for optimization. Structurally, COMs resemble adversarial training\nmethods used to overcome adversarial examples. COMs are simple to implement and\noutperform a number of existing methods on a wide range of MBO problems,\nincluding optimizing protein sequences, robot morphologies, neural network\nweights, and superconducting materials.",
          "link": "http://arxiv.org/abs/2107.06882",
          "publishedOn": "2021-07-15T01:59:04.363Z",
          "wordCount": 637,
          "title": "Conservative Objective Models for Effective Offline Model-Based Optimization. (arXiv:2107.06882v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengjun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_N/0/1/0/all/0/1\">Niklas Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_B/0/1/0/all/0/1\">Bharatbhushan Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitts_B/0/1/0/all/0/1\">Brendan Kitts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligorijevic_D/0/1/0/all/0/1\">Djordje Gligorijevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gultekin_S/0/1/0/all/0/1\">San Gultekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1\">Tingyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_A/0/1/0/all/0/1\">Aaron Flores</a>",
          "description": "Since 2019, most ad exchanges and sell-side platforms (SSPs), in the online\nadvertising industry, shifted from second to first price auctions. Due to the\nfundamental difference between these auctions, demand-side platforms (DSPs)\nhave had to update their bidding strategies to avoid bidding unnecessarily high\nand hence overpaying. Bid shading was proposed to adjust the bid price intended\nfor second-price auctions, in order to balance cost and winning probability in\na first-price auction setup. In this study, we introduce a novel deep\ndistribution network for optimal bidding in both open (non-censored) and closed\n(censored) online first-price auctions. Offline and online A/B testing results\nshow that our algorithm outperforms previous state-of-art algorithms in terms\nof both surplus and effective cost per action (eCPX) metrics. Furthermore, the\nalgorithm is optimized in run-time and has been deployed into VerizonMedia DSP\nas production algorithm, serving hundreds of billions of bid requests per day.\nOnline A/B test shows that advertiser's ROI are improved by +2.4%, +2.4%, and\n+8.6% for impression based (CPM), click based (CPC), and conversion based (CPA)\ncampaigns respectively.",
          "link": "http://arxiv.org/abs/2107.06650",
          "publishedOn": "2021-07-15T01:59:04.356Z",
          "wordCount": 643,
          "title": "An Efficient Deep Distribution Network for Bid Shading in First-Price Auctions. (arXiv:2107.06650v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/1908.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiao Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tingzhang Zhao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojun Ye</a> (2) ((1) Department of Computer Science, University of Science and Technology Beijing (2) School of Software, Tsinghua University)",
          "description": "Feature selection, an effective technique for dimensionality reduction, plays\nan important role in many machine learning systems. Supervised knowledge can\nsignificantly improve the performance. However, faced with the rapid growth of\nnewly emerging concepts, existing supervised methods might easily suffer from\nthe scarcity and validity of labeled data for training. In this paper, the\nauthors study the problem of zero-shot feature selection (i.e., building a\nfeature selection model that generalizes well to \"unseen\" concepts with limited\ntraining data of \"seen\" concepts). Specifically, they adopt class-semantic\ndescriptions (i.e., attributes) as supervision for feature selection, so as to\nutilize the supervised knowledge transferred from the seen concepts. For more\nreliable discriminative features, they further propose the\ncenter-characteristic loss which encourages the selected features to capture\nthe central characteristics of seen concepts. Extensive experiments conducted\non various real-world datasets demonstrate the effectiveness of the method.",
          "link": "http://arxiv.org/abs/1908.03464",
          "publishedOn": "2021-07-15T01:59:04.348Z",
          "wordCount": 643,
          "title": "Zero-Shot Feature Selection via Transferring Supervised Knowledge. (arXiv:1908.03464v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dempster_A/0/1/0/all/0/1\">Angus Dempster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Daniel F. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1\">Geoffrey I. Webb</a>",
          "description": "Until recently, the most accurate methods for time series classification were\nlimited by high computational complexity. ROCKET achieves state-of-the-art\naccuracy with a fraction of the computational expense of most existing methods\nby transforming input time series using random convolutional kernels, and using\nthe transformed features to train a linear classifier. We reformulate ROCKET\ninto a new method, MINIROCKET, making it up to 75 times faster on larger\ndatasets, and making it almost deterministic (and optionally, with additional\ncomputational expense, fully deterministic), while maintaining essentially the\nsame accuracy. Using this method, it is possible to train and test a classifier\non all of 109 datasets from the UCR archive to state-of-the-art accuracy in\nless than 10 minutes. MINIROCKET is significantly faster than any other method\nof comparable accuracy (including ROCKET), and significantly more accurate than\nany other method of even roughly-similar computational expense. As such, we\nsuggest that MINIROCKET should now be considered and used as the default\nvariant of ROCKET.",
          "link": "http://arxiv.org/abs/2012.08791",
          "publishedOn": "2021-07-15T01:59:04.340Z",
          "wordCount": 641,
          "title": "MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. (arXiv:2012.08791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klicpera_J/0/1/0/all/0/1\">Johannes Klicpera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1\">Marten Lienen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "The current best practice for computing optimal transport (OT) is via entropy\nregularization and Sinkhorn iterations. This algorithm runs in quadratic time\nas it requires the full pairwise cost matrix, which is prohibitively expensive\nfor large sets of objects. In this work we propose two effective log-linear\ntime approximations of the cost matrix: First, a sparse approximation based on\nlocality-sensitive hashing (LSH) and, second, a Nystr\\\"om approximation with\nLSH-based sparse corrections, which we call locally corrected Nystr\\\"om (LCN).\nThese approximations enable general log-linear time algorithms for\nentropy-regularized OT that perform well even for the complex, high-dimensional\nspaces common in deep learning. We analyse these approximations theoretically\nand evaluate them experimentally both directly and end-to-end as a component\nfor real-world applications. Using our approximations for unsupervised word\nembedding alignment enables us to speed up a state-of-the-art method by a\nfactor of 3 while also improving the accuracy by 3.1 percentage points without\nany additional model changes. For graph distance regression we propose the\ngraph transport network (GTN), which combines graph neural networks (GNNs) with\nenhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales\nlog-linearly in the number of nodes.",
          "link": "http://arxiv.org/abs/2107.06876",
          "publishedOn": "2021-07-15T01:59:04.324Z",
          "wordCount": 658,
          "title": "Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1\">Amir Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novin_R/0/1/0/all/0/1\">Roya Sabbagh Novin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merryweather_A/0/1/0/all/0/1\">Andrew Merryweather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1\">Tucker Hermans</a>",
          "description": "Ergonomics and human comfort are essential concerns in physical human-robot\ninteraction applications. Defining an accurate and easy-to-use ergonomic\nassessment model stands as an important step in providing feedback for postural\ncorrection to improve operator health and comfort. In order to enable efficient\ncomputation, previously proposed automated ergonomic assessment and correction\ntools make approximations or simplifications to gold-standard assessment tools\nused by ergonomists in practice. In order to retain assessment quality, while\nimproving computational considerations, we introduce DULA, a differentiable and\ncontinuous ergonomics model learned to replicate the popular and scientifically\nvalidated RULA assessment. We show that DULA provides assessment comparable to\nRULA while providing computational benefits. We highlight DULA's strength in a\ndemonstration of gradient-based postural optimization for a simulated\nteleoperation task.",
          "link": "http://arxiv.org/abs/2107.06875",
          "publishedOn": "2021-07-15T01:59:04.318Z",
          "wordCount": 569,
          "title": "DULA: A Differentiable Ergonomics Model for Postural Optimization in Physical HRI. (arXiv:2107.06875v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ashudeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1\">David Kempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1\">Thorsten Joachims</a>",
          "description": "Fairness has emerged as an important consideration in algorithmic\ndecision-making. Unfairness occurs when an agent with higher merit obtains a\nworse outcome than an agent with lower merit. Our central point is that a\nprimary cause of unfairness is uncertainty. A principal or algorithm making\ndecisions never has access to the agents' true merit, and instead uses proxy\nfeatures that only imperfectly predict merit (e.g., GPA, star ratings,\nrecommendation letters). None of these ever fully capture an agent's merit; yet\nexisting approaches have mostly been defining fairness notions directly based\non observed features and outcomes.\n\nOur primary point is that it is more principled to acknowledge and model the\nuncertainty explicitly. The role of observed features is to give rise to a\nposterior distribution of the agents' merits. We use this viewpoint to define a\nnotion of approximate fairness in ranking. We call an algorithm $\\phi$-fair\n(for $\\phi \\in [0,1]$) if it has the following property for all agents $x$ and\nall $k$: if agent $x$ is among the top $k$ agents with respect to merit with\nprobability at least $\\rho$ (according to the posterior merit distribution),\nthen the algorithm places the agent among the top $k$ agents in its ranking\nwith probability at least $\\phi \\rho$.\n\nWe show how to compute rankings that optimally trade off approximate fairness\nagainst utility to the principal. In addition to the theoretical\ncharacterization, we present an empirical analysis of the potential impact of\nthe approach in simulation studies. For real-world validation, we applied the\napproach in the context of a paper recommendation system that we built and\nfielded at a large conference.",
          "link": "http://arxiv.org/abs/2107.06720",
          "publishedOn": "2021-07-15T01:59:04.312Z",
          "wordCount": 708,
          "title": "Fairness in Ranking under Uncertainty. (arXiv:2107.06720v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1\">Dario Rossi</a>",
          "description": "The increased success of Deep Learning (DL) has recently sparked large-scale\ndeployment of DL models in many diverse industry segments. Yet, a crucial\nweakness of supervised model is the inherent difficulty in handling\nout-of-distribution samples, i.e., samples belonging to classes that were not\npresented to the model at training time. We propose in this paper a novel way\nto formulate the out-of-distribution detection problem, tailored for DL models.\nOur method does not require fine tuning process on training data, yet is\nsignificantly more accurate than the state of the art for out-of-distribution\ndetection.",
          "link": "http://arxiv.org/abs/2107.06668",
          "publishedOn": "2021-07-15T01:59:04.306Z",
          "wordCount": 529,
          "title": "Thinkback: Task-SpecificOut-of-Distribution Detection. (arXiv:2107.06668v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lermusiaux_P/0/1/0/all/0/1\">Pierre F.J. Lermusiaux</a>",
          "description": "Complex dynamical systems are used for predictions in many domains. Because\nof computational costs, models are truncated, coarsened, or aggregated. As the\nneglected and unresolved terms become important, the utility of model\npredictions diminishes. We develop a novel, versatile, and rigorous methodology\nto learn non-Markovian closure parameterizations for known-physics/low-fidelity\nmodels using data from high-fidelity simulations. The new \"neural closure\nmodels\" augment low-fidelity models with neural delay differential equations\n(nDDEs), motivated by the Mori-Zwanzig formulation and the inherent delays in\ncomplex dynamical systems. We demonstrate that neural closures efficiently\naccount for truncated modes in reduced-order-models, capture the effects of\nsubgrid-scale processes in coarse models, and augment the simplification of\ncomplex biological and physical-biogeochemical models. We find that using\nnon-Markovian over Markovian closures improves long-term prediction accuracy\nand requires smaller networks. We derive adjoint equations and network\narchitectures needed to efficiently implement the new discrete and distributed\nnDDEs, for any time-integration schemes and allowing nonuniformly-spaced\ntemporal training data. The performance of discrete over distributed delays in\nclosure models is explained using information theory, and we find an optimal\namount of past information for a specified architecture. Finally, we analyze\ncomputational complexity and explain the limited additional cost due to neural\nclosure models.",
          "link": "http://arxiv.org/abs/2012.13869",
          "publishedOn": "2021-07-15T01:59:04.300Z",
          "wordCount": 689,
          "title": "Neural Closure Models for Dynamical Systems. (arXiv:2012.13869v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.08352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1\">Chen-Chou Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Szu-Wei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "Existing objective evaluation metrics for voice conversion (VC) are not\nalways correlated with human perception. Therefore, training VC models with\nsuch criteria may not effectively improve naturalness and similarity of\nconverted speech. In this paper, we propose deep learning-based assessment\nmodels to predict human ratings of converted speech. We adopt the convolutional\nand recurrent neural network models to build a mean opinion score (MOS)\npredictor, termed as MOSNet. The proposed models are tested on large-scale\nlistening test results of the Voice Conversion Challenge (VCC) 2018.\nExperimental results show that the predicted scores of the proposed MOSNet are\nhighly correlated with human MOS ratings at the system level while being fairly\ncorrelated with human MOS ratings at the utterance level. Meanwhile, we have\nmodified MOSNet to predict the similarity scores, and the preliminary results\nshow that the predicted scores are also fairly correlated with human ratings.\nThese results confirm that the proposed models could be used as a computational\nevaluator to measure the MOS of VC systems to reduce the need for expensive\nhuman rating.",
          "link": "http://arxiv.org/abs/1904.08352",
          "publishedOn": "2021-07-15T01:59:04.294Z",
          "wordCount": 665,
          "title": "MOSNet: Deep Learning based Objective Assessment for Voice Conversion. (arXiv:1904.08352v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukmadewa_A/0/1/0/all/0/1\">Anantha Yullian Sukmadewa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DW_H/0/1/0/all/0/1\">Haftittah Wuswilahaken DW</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachtiar_F/0/1/0/all/0/1\">Fitra Abdurrachman Bachtiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "User reviews have an essential role in the success of the developed mobile\napps. User reviews in the textual form are unstructured data, creating a very\nhigh complexity when processed for sentiment analysis. Previous approaches that\nhave been used often ignore the context of reviews. In addition, the relatively\nsmall data makes the model overfitting. A new approach, BERT, has been\nintroduced as a transfer learning model with a pre-trained model that has\npreviously been trained to have a better context representation. This study\nexamines the effectiveness of fine-tuning BERT for sentiment analysis using two\ndifferent pre-trained models. Besides the multilingual pre-trained model, we\nuse the pre-trained model that only has been trained in Indonesian. The dataset\nused is Indonesian user reviews of the ten best apps in 2020 in Google Play\nsites. We also perform hyper-parameter tuning to find the optimum trained\nmodel. Two training data labeling approaches were also tested to determine the\neffectiveness of the model, which is score-based and lexicon-based. The\nexperimental results show that pre-trained models trained in Indonesian have\nbetter average accuracy on lexicon-based data. The pre-trained Indonesian model\nhighest accuracy is 84%, with 25 epochs and a training time of 24 minutes.\nThese results are better than all of the machine learning and multilingual\npre-trained models.",
          "link": "http://arxiv.org/abs/2107.06802",
          "publishedOn": "2021-07-15T01:59:04.278Z",
          "wordCount": 657,
          "title": "BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps Reviews. (arXiv:2107.06802v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1\">Alexander Mordvintsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1\">Ettore Randazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1\">Eyvind Niklasson</a>",
          "description": "Reaction-Diffusion (RD) systems provide a computational framework that\ngoverns many pattern formation processes in nature. Current RD system design\npractices boil down to trial-and-error parameter search. We propose a\ndifferentiable optimization method for learning the RD system parameters to\nperform example-based texture synthesis on a 2D plane. We do this by\nrepresenting the RD system as a variant of Neural Cellular Automata and using\ntask-specific differentiable loss functions. RD systems generated by our method\nexhibit robust, non-trivial 'life-like' behavior.",
          "link": "http://arxiv.org/abs/2107.06862",
          "publishedOn": "2021-07-15T01:59:04.272Z",
          "wordCount": 522,
          "title": "Differentiable Programming of Reaction-Diffusion Patterns. (arXiv:2107.06862v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bassily_R/0/1/0/all/0/1\">Raef Bassily</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1\">Crist&#xf3;bal Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menart_M/0/1/0/all/0/1\">Michael Menart</a>",
          "description": "We study differentially private stochastic optimization in convex and\nnon-convex settings. For the convex case, we focus on the family of non-smooth\ngeneralized linear losses (GLLs). Our algorithm for the $\\ell_2$ setting\nachieves optimal excess population risk in near-linear time, while the best\nknown differentially private algorithms for general convex losses run in\nsuper-linear time. Our algorithm for the $\\ell_1$ setting has nearly-optimal\nexcess population risk $\\tilde{O}\\big(\\sqrt{\\frac{\\log{d}}{n}}\\big)$, and\ncircumvents the dimension dependent lower bound of [AFKT21] for general\nnon-smooth convex losses. In the differentially private non-convex setting, we\nprovide several new algorithms for approximating stationary points of the\npopulation risk. For the $\\ell_1$-case with smooth losses and polyhedral\nconstraint, we provide the first nearly dimension independent rate, $\\tilde\nO\\big(\\frac{\\log^{2/3}{d}}{{n^{1/3}}}\\big)$ in linear time. For the constrained\n$\\ell_2$-case, with smooth losses, we obtain a linear-time algorithm with rate\n$\\tilde O\\big(\\frac{1}{n^{3/10}d^{1/10}}+\\big(\\frac{d}{n^2}\\big)^{1/5}\\big)$.\nFinally, for the $\\ell_2$-case we provide the first method for {\\em non-smooth\nweakly convex} stochastic optimization with rate $\\tilde\nO\\big(\\frac{1}{n^{1/4}}+\\big(\\frac{d}{n^2}\\big)^{1/6}\\big)$ which matches the\nbest existing non-private algorithm when $d= O(\\sqrt{n})$. We also extend all\nour results above for the non-convex $\\ell_2$ setting to the $\\ell_p$ setting,\nwhere $1 < p \\leq 2$, with only polylogarithmic (in the dimension) overhead in\nthe rates.",
          "link": "http://arxiv.org/abs/2107.05585",
          "publishedOn": "2021-07-15T01:59:04.266Z",
          "wordCount": 662,
          "title": "Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings. (arXiv:2107.05585v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdul Rafae Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varsanyi_P/0/1/0/all/0/1\">Peter Varsanyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pabreja_R/0/1/0/all/0/1\">Rachit Pabreja</a>",
          "description": "While predictive policing has become increasingly common in assisting with\ndecisions in the criminal justice system, the use of these results is still\ncontroversial. Some software based on deep learning lacks accuracy (e.g., in\nF-1), and importantly many decision processes are not transparent, causing\ndoubt about decision bias, such as perceived racial and age disparities. This\npaper addresses bias issues with post-hoc explanations to provide a trustable\nprediction of whether a person will receive future criminal charges given one's\nprevious criminal records by learning temporal behavior patterns over twenty\nyears. Bi-LSTM relieves the vanishing gradient problem, attentional mechanisms\nallow learning and interpretation of feature importance, and complex-valued\nnetworks inspired quantum physics to facilitate a certain level of transparency\nin modeling the decision process. Our approach shows a consistent and reliable\nprediction precision and recall on a real-life dataset. Our analysis of the\nimportance of each input feature shows the critical causal impact on\ndecision-making, suggesting that criminal histories are statistically\nsignificant factors, while identifiers, such as race and age, are not. Finally,\nour algorithm indicates that a suspect tends to rather than suddenly increase\ncrime severity level over time gradually.",
          "link": "http://arxiv.org/abs/2106.13456",
          "publishedOn": "2021-07-15T01:59:04.254Z",
          "wordCount": 663,
          "title": "Interpreting Criminal Charge Prediction and Its Algorithmic Bias via Quantum-Inspired Complex Valued Networks. (arXiv:2106.13456v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reisser_M/0/1/0/all/0/1\">Matthias Reisser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louizos_C/0/1/0/all/0/1\">Christos Louizos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "Federated learning (FL) has emerged as the predominant approach for\ncollaborative training of neural network models across multiple users, without\nthe need to gather the data at a central location. One of the important\nchallenges in this setting is data heterogeneity, i.e. different users have\ndifferent data characteristics. For this reason, training and using a single\nglobal model might be suboptimal when considering the performance of each of\nthe individual user's data. In this work, we tackle this problem via Federated\nMixture of Experts, FedMix, a framework that allows us to train an ensemble of\nspecialized models. FedMix adaptively selects and trains a user-specific\nselection of the ensemble members. We show that users with similar data\ncharacteristics select the same members and therefore share statistical\nstrength while mitigating the effect of non-i.i.d data. Empirically, we show\nthrough an extensive experimental evaluation that FedMix improves performance\ncompared to using a single global model across a variety of different sources\nof non-i.i.d.-ness.",
          "link": "http://arxiv.org/abs/2107.06724",
          "publishedOn": "2021-07-15T01:59:04.236Z",
          "wordCount": 588,
          "title": "Federated Mixture of Experts. (arXiv:2107.06724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06675",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ziel_F/0/1/0/all/0/1\">Florian Ziel</a>",
          "description": "The M5 competition uncertainty track aims for probabilistic forecasting of\nsales of thousands of Walmart retail goods. We show that the M5 competition\ndata faces strong overdispersion and sporadic demand, especially zero demand.\nWe discuss resulting modeling issues concerning adequate probabilistic\nforecasting of such count data processes. Unfortunately, the majority of\npopular prediction methods used in the M5 competition (e.g. lightgbm and\nxgboost GBMs) fails to address the data characteristics due to the considered\nobjective functions. The distributional forecasting provides a suitable\nmodeling approach for to the overcome those problems. The GAMLSS framework\nallows flexible probabilistic forecasting using low dimensional distributions.\nWe illustrate, how the GAMLSS approach can be applied for the M5 competition\ndata by modeling the location and scale parameter of various distributions,\ne.g. the negative binomial distribution. Finally, we discuss software packages\nfor distributional modeling and their drawback, like the R package gamlss with\nits package extensions, and (deep) distributional forecasting libraries such as\nTensorFlow Probability.",
          "link": "http://arxiv.org/abs/2107.06675",
          "publishedOn": "2021-07-15T01:59:04.229Z",
          "wordCount": 599,
          "title": "M5 Competition Uncertainty: Overdispersion, distributional forecasting, GAMLSS and beyond. (arXiv:2107.06675v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06762",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Mestre_G/0/1/0/all/0/1\">Gon&#xe7;alo Mestre</a> (1 and 2), <a href=\"http://arxiv.org/find/q-bio/1/au:+Barbulescu_R/0/1/0/all/0/1\">Ruxandra Barbulescu</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Oliveira_A/0/1/0/all/0/1\">Arlindo L. Oliveira</a> (1 and 2), <a href=\"http://arxiv.org/find/q-bio/1/au:+Silveira_L/0/1/0/all/0/1\">L. Miguel Silveira</a> (1 and 2) ((1) INESC-ID, Rua Alves Redol 9, 1000-029 Lisboa, (2) IST Tecnico Lisboa, Universidade de Lisboa, Av. Rovisco Pais 1, 1049-001 Lisboa)",
          "description": "Given the inner complexity of the human nervous system, insight into the\ndynamics of brain activity can be gained from understanding smaller and simpler\norganisms, such as the nematode C. Elegans. The behavioural and structural\nbiology of these organisms is well-known, making them prime candidates for\nbenchmarking modelling and simulation techniques. In these complex neuronal\ncollections, classical, white-box modelling techniques based on intrinsic\nstructural or behavioural information are either unable to capture the profound\nnonlinearities of the neuronal response to different stimuli or generate\nextremely complex models, which are computationally intractable. In this paper\nwe show how the nervous system of C. Elegans can be modelled and simulated with\ndata-driven models using different neural network architectures. Specifically,\nwe target the use of state of the art recurrent neural networks architectures\nsuch as LSTMs and GRUs and compare these architectures in terms of their\nproperties and their accuracy as well as the complexity of the resulting\nmodels. We show that GRU models with a hidden layer size of 4 units are able to\naccurately reproduce with high accuracy the system's response to very different\nstimuli.",
          "link": "http://arxiv.org/abs/2107.06762",
          "publishedOn": "2021-07-15T01:59:04.223Z",
          "wordCount": 669,
          "title": "Modelling Neuronal Behaviour with Time Series Regression: Recurrent Neural Networks on C. Elegans Data. (arXiv:2107.06762v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1\">Kai Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1\">Guohao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shulin Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>",
          "description": "In this work, we propose a low-bit training framework for convolutional\nneural networks, which is built around a novel multi-level scaling (MLS) tensor\nformat. Our framework focuses on reducing the energy consumption of convolution\noperations by quantizing all the convolution operands to low bit-width format.\nSpecifically, we propose the MLS tensor format, in which the element-wise\nbit-width can be largely reduced. Then, we describe the dynamic quantization\nand the low-bit tensor convolution arithmetic to leverage the MLS tensor format\nefficiently. Experiments show that our framework achieves a superior trade-off\nbetween the accuracy and the bit-width than previous low-bit training\nframeworks. For training a variety of models on CIFAR-10, using 1-bit mantissa\nand 2-bit exponent is adequate to keep the accuracy loss within $1\\%$. And on\nlarger datasets like ImageNet, using 4-bit mantissa and 2-bit exponent is\nadequate to keep the accuracy loss within $1\\%$. Through the energy consumption\nsimulation of the computing units, we can estimate that training a variety of\nmodels with our framework could achieve $8.3\\sim10.2\\times$ and\n$1.9\\sim2.3\\times$ higher energy efficiency than training with full-precision\nand 8-bit floating-point arithmetic, respectively.",
          "link": "http://arxiv.org/abs/2006.02804",
          "publishedOn": "2021-07-15T01:59:04.217Z",
          "wordCount": 683,
          "title": "Exploring the Potential of Low-bit Training of Convolutional Neural Networks. (arXiv:2006.02804v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1\">Akshay L Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sai Vikas Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Active Learning (AL) techniques aim to minimize the training data required to\ntrain a model for a given task. Pool-based AL techniques start with a small\ninitial labeled pool and then iteratively pick batches of the most informative\nsamples for labeling. Generally, the initial pool is sampled randomly and\nlabeled to seed the AL iterations. While recent studies have focused on\nevaluating the robustness of various query functions in AL, little to no\nattention has been given to the design of the initial labeled pool for deep\nactive learning. Given the recent successes of learning representations in\nself-supervised/unsupervised ways, we study if an intelligently sampled initial\nlabeled pool can improve deep AL performance. We investigate the effect of\nintelligently sampled initial labeled pools, including the use of\nself-supervised and unsupervised strategies, on deep AL methods. The setup,\nhypotheses, methodology, and implementation details were evaluated by peer\nreview before experiments were conducted. Experimental results could not\nconclusively prove that intelligently sampled initial pools are better for AL\nthan random initial pools in the long run, although a Variational\nAutoencoder-based initial pool sampling strategy showed interesting trends that\nmerit deeper investigation.",
          "link": "http://arxiv.org/abs/2011.14696",
          "publishedOn": "2021-07-15T01:59:04.210Z",
          "wordCount": 683,
          "title": "On Initial Pools for Deep Active Learning. (arXiv:2011.14696v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08721",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1\">Pujan Pokhrel</a>",
          "description": "In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast\ndominant wave periods in oceanic waters. First, we use the data collected from\nCDIP buoys and apply various data filtering methods. The data filtering methods\nallow us to obtain a high-quality dataset for training and validation purposes.\nWe then extract various wave-based features like wave heights, periods,\nskewness, kurtosis, etc., and atmospheric features like humidity, pressure, and\nair temperature for the buoys. Afterward, we train algorithms that use LightGBM\nand Extra Trees through a hv-block cross-validation scheme to forecast dominant\nwave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,\nand 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,\nExtra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,\n15-day ahead, and 30 day ahead prediction. In case of the test dataset,\nLightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and\n30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day\nahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both\ntraining and the test dataset suggests that the machine learning models\ndeveloped in this paper are robust. Since the LightGBM algorithm outperforms ET\nfor all the windows tested, it is taken as the final algorithm. Note that the\nperformance of both methods does not decrease significantly as the forecast\nhorizon increases. Likewise, the proposed method outperforms the numerical\napproaches included in this paper in the test dataset. For 1 day ahead\nprediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,\n0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre\nfor Medium-range Weather Forecasts (ECMWF) model, which outperforms all the\nother methods in the test dataset.",
          "link": "http://arxiv.org/abs/2105.08721",
          "publishedOn": "2021-07-15T01:59:04.193Z",
          "wordCount": 783,
          "title": "A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v4 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1\">Maxim Rakhuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We study low-rank parameterizations of weight matrices with embedded spectral\nproperties in the Deep Learning context. The low-rank property leads to\nparameter efficiency and permits taking computational shortcuts when computing\nmappings. Spectral properties are often subject to constraints in optimization\nproblems, leading to better models and stability of optimization. We start by\nlooking at the compact SVD parameterization of weight matrices and identifying\nredundancy sources in the parameterization. We further apply the Tensor Train\n(TT) decomposition to the compact SVD components, and propose a non-redundant\ndifferentiable parameterization of fixed TT-rank tensor manifolds, termed the\nSpectral Tensor Train Parameterization (STTP). We demonstrate the effects of\nneural network compression in the image classification setting and both\ncompression and improved training stability in the generative adversarial\ntraining setting.",
          "link": "http://arxiv.org/abs/2103.04217",
          "publishedOn": "2021-07-15T01:59:04.186Z",
          "wordCount": 609,
          "title": "Spectral Tensor Train Parameterization of Deep Learning Layers. (arXiv:2103.04217v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.03886",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mei_K/0/1/0/all/0/1\">Kai Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaochen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajatheva_N/0/1/0/all/0/1\">Nandana Rajatheva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1\">Jibo Wei</a>",
          "description": "Recently, machine learning-based channel estimation has attracted much\nattention. The performance of machine learning-based estimation has been\nvalidated by simulation experiments. However, little attention has been paid to\nthe theoretical performance analysis. In this paper, we investigate the mean\nsquare error (MSE) performance of machine learning-based estimation. Hypothesis\ntesting is employed to analyze its MSE upper bound. Furthermore, we build a\nstatistical model for hypothesis testing, which holds when the linear learning\nmodule with a low input dimension is used in machine learning-based channel\nestimation, and derive a clear analytical relation between the size of the\ntraining data and performance. Then, we simulate the machine learning-based\nchannel estimation in orthogonal frequency division multiplexing (OFDM) systems\nto verify our analysis results. Finally, the design considerations for the\nsituation where only limited training data is available are discussed. In this\nsituation, our analysis results can be applied to assess the performance and\nsupport the design of machine learning-based channel estimation.",
          "link": "http://arxiv.org/abs/1911.03886",
          "publishedOn": "2021-07-15T01:59:04.178Z",
          "wordCount": 628,
          "title": "Performance Analysis on Machine Learning-Based Channel Estimation. (arXiv:1911.03886v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13275",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Taheri_H/0/1/0/all/0/1\">Hossein Taheri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pedarsani_R/0/1/0/all/0/1\">Ramtin Pedarsani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1\">Christos Thrampoulidis</a>",
          "description": "It has been consistently reported that many machine learning models are\nsusceptible to adversarial attacks i.e., small additive adversarial\nperturbations applied to data points can cause misclassification. Adversarial\ntraining using empirical risk minimization is considered to be the\nstate-of-the-art method for defense against adversarial attacks. Despite being\nsuccessful in practice, several problems in understanding generalization\nperformance of adversarial training remain open. In this paper, we derive\nprecise theoretical predictions for the performance of adversarial training in\nbinary classification. We consider the high-dimensional regime where the\ndimension of data grows with the size of the training data-set at a constant\nratio. Our results provide exact asymptotics for standard and adversarial test\nerrors of the estimators obtained by adversarial training with $\\ell_q$-norm\nbounded perturbations ($q \\ge 1$) for both discriminative binary models and\ngenerative Gaussian-mixture models with correlated features. Furthermore, we\nuse these sharp predictions to uncover several intriguing observations on the\nrole of various parameters including the over-parameterization ratio, the data\nmodel, and the attack budget on the adversarial and standard errors.",
          "link": "http://arxiv.org/abs/2010.13275",
          "publishedOn": "2021-07-15T01:59:04.172Z",
          "wordCount": 649,
          "title": "Asymptotic Behavior of Adversarial Training in Binary Classification. (arXiv:2010.13275v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schubert_I/0/1/0/all/0/1\">Ingmar Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_O/0/1/0/all/0/1\">Ozgur S. Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1\">Marc Toussaint</a>",
          "description": "In high-dimensional state spaces, the usefulness of Reinforcement Learning\n(RL) is limited by the problem of exploration. This issue has been addressed\nusing potential-based reward shaping (PB-RS) previously. In the present work,\nwe introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the\nstrict optimality guarantees of PB-RS to a guarantee of preserved long-term\nbehavior. Being less restrictive, FV-RS allows for reward shaping functions\nthat are even better suited for improving the sample efficiency of RL\nalgorithms. In particular, we consider settings in which the agent has access\nto an approximate plan. Here, we use examples of simulated robotic manipulation\ntasks to demonstrate that plan-based FV-RS can indeed significantly improve the\nsample efficiency of RL over plan-based PB-RS.",
          "link": "http://arxiv.org/abs/2107.06661",
          "publishedOn": "2021-07-15T01:59:04.166Z",
          "wordCount": 569,
          "title": "Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks. (arXiv:2107.06661v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tithi_J/0/1/0/all/0/1\">Jesmin Jahan Tithi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrini_F/0/1/0/all/0/1\">Fabrizio Petrini</a>",
          "description": "The Word Movers Distance (WMD) measures the semantic dissimilarity between\ntwo text documents by computing the cost of optimally moving all words of a\nsource/query document to the most similar words of a target document. Computing\nWMD between two documents is costly because it requires solving an optimization\nproblem that costs $O (V^3 \\log(V)) $ where $V$ is the number of unique words\nin the document. Fortunately, WMD can be framed as an Earth Mover's Distance\n(EMD) for which the algorithmic complexity can be reduced to $O(V^2)$ by adding\nan entropy penalty to the optimization problem and solving it using the\nSinkhorn-Knopp algorithm. Additionally, the computation can be made highly\nparallel by computing the WMD of a single query document against multiple\ntarget documents at once, for example by finding whether a given tweet is\nsimilar to any other tweets of a given day.\n\nIn this paper, we first present a shared-memory parallel Sinkhorn-Knopp\nalgorithm to compute the WMD of one document against many other documents by\nadopting the $ O(V^2)$ EMD algorithm. We then algorithmically transform the\noriginal $O(V^2)$ dense compute-heavy version into an equivalent sparse one\nwhich is mapped onto the new Intel Programmable Integrated Unified Memory\nArchitecture (PIUMA) system. The WMD parallel implementation achieves 67x\nspeedup on 96 cores across 4 NUMA sockets of an Intel Cascade Lake system. We\nalso show that PIUMA cores are around 1.2-2.6x faster than Xeon cores on\nSinkhorn-WMD and also provide better strong scaling.",
          "link": "http://arxiv.org/abs/2107.06433",
          "publishedOn": "2021-07-15T01:59:04.149Z",
          "wordCount": 715,
          "title": "A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its Performance on PIUMA and Xeon CPU. (arXiv:2107.06433v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mark Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tworek_J/0/1/0/all/0/1\">Jerry Tworek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_H/0/1/0/all/0/1\">Heewoo Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1\">Qiming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_H/0/1/0/all/0/1\">Henrique Ponde de Oliveira Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_H/0/1/0/all/0/1\">Harri Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burda_Y/0/1/0/all/0/1\">Yuri Burda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockman_G/0/1/0/all/0/1\">Greg Brockman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Alex Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Raul Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_M/0/1/0/all/0/1\">Michael Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khlaaf_H/0/1/0/all/0/1\">Heidy Khlaaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_G/0/1/0/all/0/1\">Girish Sastry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1\">Pamela Mishkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_B/0/1/0/all/0/1\">Brooke Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_S/0/1/0/all/0/1\">Scott Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryder_N/0/1/0/all/0/1\">Nick Ryder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlov_M/0/1/0/all/0/1\">Mikhail Pavlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1\">Alethea Power</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1\">Lukasz Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bavarian_M/0/1/0/all/0/1\">Mohammad Bavarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1\">Clemens Winter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tillet_P/0/1/0/all/0/1\">Philippe Tillet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Such_F/0/1/0/all/0/1\">Felipe Petroski Such</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1\">Dave Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plappert_M/0/1/0/all/0/1\">Matthias Plappert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chantzis_F/0/1/0/all/0/1\">Fotios Chantzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1\">Elizabeth Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1\">Ariel Herbert-Voss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guss_W/0/1/0/all/0/1\">William Hebgen Guss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1\">Alex Nichol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paino_A/0/1/0/all/0/1\">Alex Paino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tezak_N/0/1/0/all/0/1\">Nikolas Tezak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuschkin_I/0/1/0/all/0/1\">Igor Babuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1\">Suchir Balaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shantanu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1\">William Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesse_C/0/1/0/all/0/1\">Christopher Hesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carr_A/0/1/0/all/0/1\">Andrew N. Carr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achiam_J/0/1/0/all/0/1\">Josh Achiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morikawa_E/0/1/0/all/0/1\">Evan Morikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1\">Alec Radford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_M/0/1/0/all/0/1\">Matthew Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1\">Miles Brundage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murati_M/0/1/0/all/0/1\">Mira Murati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_K/0/1/0/all/0/1\">Katie Mayer</a>, et al. (6 additional authors not shown)",
          "description": "We introduce Codex, a GPT language model fine-tuned on publicly available\ncode from GitHub, and study its Python code-writing capabilities. A distinct\nproduction version of Codex powers GitHub Copilot. On HumanEval, a new\nevaluation set we release to measure functional correctness for synthesizing\nprograms from docstrings, our model solves 28.8% of the problems, while GPT-3\nsolves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling\nfrom the model is a surprisingly effective strategy for producing working\nsolutions to difficult prompts. Using this method, we solve 70.2% of our\nproblems with 100 samples per problem. Careful investigation of our model\nreveals its limitations, including difficulty with docstrings describing long\nchains of operations and with binding operations to variables. Finally, we\ndiscuss the potential broader impacts of deploying powerful code generation\ntechnologies, covering safety, security, and economics.",
          "link": "http://arxiv.org/abs/2107.03374",
          "publishedOn": "2021-07-15T01:59:04.143Z",
          "wordCount": 705,
          "title": "Evaluating Large Language Models Trained on Code. (arXiv:2107.03374v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>",
          "description": "Active learning (AL) aims at reducing labeling effort by identifying the most\nvaluable unlabeled data points from a large pool. Traditional AL frameworks\nhave two limitations: First, they perform data selection in a multi-round\nmanner, which is time-consuming and impractical. Second, they usually assume\nthat there are a small amount of labeled data points available in the same\ndomain as the data in the unlabeled pool. Recent work proposes a solution for\none-round active learning based on data utility learning and optimization,\nwhich fixes the first issue but still requires the initially labeled data\npoints in the same domain. In this paper, we propose $\\mathrm{D^2ULO}$ as a\nsolution that solves both issues. Specifically, $\\mathrm{D^2ULO}$ leverages the\nidea of domain adaptation (DA) to train a data utility model which can\neffectively predict the utility for any given unlabeled data in the target\ndomain once labeled. The trained data utility model can then be used to select\nhigh-utility data and at the same time, provide an estimate for the utility of\nthe selected data. Our algorithm does not rely on any feedback from annotators\nin the target domain and hence, can be used to perform zero-round active\nlearning or warm-start existing multi-round active learning strategies. Our\nexperiments show that $\\mathrm{D^2ULO}$ outperforms the existing\nstate-of-the-art AL strategies equipped with domain adaptation over various\ndomain shift settings (e.g., real-to-real data and synthetic-to-real data).\nParticularly, $\\mathrm{D^2ULO}$ are applicable to the scenario where source and\ntarget labels have mismatches, which is not supported by the existing works.",
          "link": "http://arxiv.org/abs/2107.06703",
          "publishedOn": "2021-07-15T01:59:04.135Z",
          "wordCount": 668,
          "title": "Zero-Round Active Learning. (arXiv:2107.06703v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>",
          "description": "As communication systems are foreseen to enable new services such as joint\ncommunication and sensing and utilize parts of the sub-THz spectrum, the design\nof novel waveforms that can support these emerging applications becomes\nincreasingly challenging. We present in this work an end-to-end learning\napproach to design waveforms through joint learning of pulse shaping and\nconstellation geometry, together with a neural network (NN)-based receiver.\nOptimization is performed to maximize an achievable information rate, while\nsatisfying constraints on out-of-band emission and power envelope. Our results\nshow that the proposed approach enables up to orders of magnitude smaller\nadjacent channel leakage ratios (ACLRs) with peak-to-average power ratios\n(PAPRs) competitive with traditional filters, without significant loss of\ninformation rate on an additive white Gaussian noise (AWGN) channel, and no\nadditional complexity at the transmitter.",
          "link": "http://arxiv.org/abs/2106.15158",
          "publishedOn": "2021-07-15T01:59:04.130Z",
          "wordCount": 593,
          "title": "End-to-end Waveform Learning Through Joint Optimization of Pulse and Constellation Shaping. (arXiv:2106.15158v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1\">Lan V. Truong</a>",
          "description": "We establish exact asymptotic expressions for the normalized mutual\ninformation and minimum mean-square-error (MMSE) of sparse linear regression in\nthe sub-linear sparsity regime. Our result is achieved by a generalization of\nthe adaptive interpolation method in Bayesian inference for linear regimes to\nsub-linear ones. A modification of the well-known approximate message passing\nalgorithm to approach the MMSE fundamental limit is also proposed, and its\nstate evolution is rigorously analyzed. Our results show that the traditional\nlinear assumption between the signal dimension and number of observations in\nthe replica and adaptive interpolation methods is not necessary for sparse\nsignals. They also show how to modify the existing well-known AMP algorithms\nfor linear regimes to sub-linear ones.",
          "link": "http://arxiv.org/abs/2101.11156",
          "publishedOn": "2021-07-15T01:59:04.124Z",
          "wordCount": 614,
          "title": "Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v4 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07900",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1\">Chaoqi Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1\">Cheng Qian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1\">Navjot Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1\">M Brandon Westover</a>, <a href=\"http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1\">Edgar Solomonik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>",
          "description": "Tensor decompositions are powerful tools for dimensionality reduction and\nfeature interpretation of multidimensional data such as signals. Existing\ntensor decomposition objectives (e.g., Frobenius norm) are designed for fitting\nraw data under statistical assumptions, which may not align with downstream\nclassification tasks. Also, real-world tensor data are usually high-ordered and\nhave large dimensions with millions or billions of entries. Thus, it is\nexpensive to decompose the whole tensor with traditional algorithms. In\npractice, raw tensor data also contains redundant information while data\naugmentation techniques may be used to smooth out noise in samples. This paper\naddresses the above challenges by proposing augmented tensor decomposition\n(ATD), which effectively incorporates data augmentations to boost downstream\nclassification. To reduce the memory footprint of the decomposition, we propose\na stochastic algorithm that updates the factor matrices in a batch fashion. We\nevaluate ATD on multiple signal datasets. It shows comparable or better\nperformance (e.g., up to 15% in accuracy) over self-supervised and autoencoder\nbaselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy\ngain over other tensor-based baselines, and reduces the memory footprint by 9X\nwhen compared to standard tensor decomposition algorithms.",
          "link": "http://arxiv.org/abs/2106.07900",
          "publishedOn": "2021-07-15T01:59:04.103Z",
          "wordCount": 657,
          "title": "Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Richter_C/0/1/0/all/0/1\">Cedric Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehrheim_H/0/1/0/all/0/1\">Heike Wehrheim</a>",
          "description": "Learning-based bug detectors promise to find bugs in large code bases by\nexploiting natural hints such as names of variables and functions or comments.\nStill, existing techniques tend to underperform when presented with realistic\nbugs. We believe bug detector learning to currently suffer from a lack of\nrealistic defective training examples. In fact, real world bugs are scarce\nwhich has driven existing methods to train on artificially created and mostly\nunrealistic mutants. In this work, we propose a novel contextual mutation\noperator which incorporates knowledge about the mutation context to dynamically\ninject natural and more realistic faults into code. Our approach employs a\nmasked language model to produce a context-dependent distribution over feasible\ntoken replacements. The evaluation shows that sampling from a language model\ndoes not only produce mutants which more accurately represent real bugs but\nalso lead to better performing bug detectors, both on artificial benchmarks and\non real world source code.",
          "link": "http://arxiv.org/abs/2107.06657",
          "publishedOn": "2021-07-15T01:59:04.096Z",
          "wordCount": 587,
          "title": "DeepMutants: Training neural bug detectors with contextual mutations. (arXiv:2107.06657v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1\">Jeff Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowers_J/0/1/0/all/0/1\">Jeffrey S. Bowers</a>",
          "description": "That shared features between train and test data are required for\ngeneralisation in artificial neural networks has been a common assumption of\nboth proponents and critics of these models. Here, we show that convolutional\narchitectures avoid this limitation by applying them to two well known\nchallenges, based on learning the identity function and learning rules\ngoverning sequences of words. In each case, successful performance on the test\nset requires generalising to features that were not present in the training\ndata, which is typically not feasible for standard connectionist models.\nHowever, our experiments demonstrate that neural networks can succeed on such\nproblems when they incorporate the weight sharing employed by convolutional\narchitectures. In the image processing domain, such architectures are intended\nto reflect the symmetry under spatial translations of the natural world that\nsuch images depict. We discuss the role of symmetry in the two tasks and its\nconnection to generalisation.",
          "link": "http://arxiv.org/abs/2107.06872",
          "publishedOn": "2021-07-15T01:59:04.091Z",
          "wordCount": 606,
          "title": "Generalisation in Neural Networks Does not Require Feature Overlap. (arXiv:2107.06872v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yinan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1\">Bei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1\">Guojun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1\">Lu Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "The rapid progress of photorealistic synthesis techniques has reached at a\ncritical point where the boundary between real and manipulated images starts to\nblur. Thus, benchmarking and advancing digital forgery analysis have become a\npressing issue. However, existing face forgery datasets either have limited\ndiversity or only support coarse-grained analysis. To counter this emerging\nthreat, we construct the ForgeryNet dataset, an extremely large face forgery\ndataset with unified annotations in image- and video-level data across four\ntasks: 1) Image Forgery Classification, including two-way (real / fake),\nthree-way (real / fake with identity-replaced forgery approaches / fake with\nidentity-remained forgery approaches), and n-way (real and 15 respective\nforgery approaches) classification. 2) Spatial Forgery Localization, which\nsegments the manipulated area of fake images compared to their corresponding\nsource real images. 3) Video Forgery Classification, which re-defines the\nvideo-level forgery classification with manipulated frames in random positions.\nThis task is important because attackers in real world are free to manipulate\nany target frame. and 4) Temporal Forgery Localization, to localize the\ntemporal segments which are manipulated. ForgeryNet is by far the largest\npublicly available deep face forgery dataset in terms of data-scale (2.9\nmillion images, 221,247 videos), manipulations (7 image-level approaches, 8\nvideo-level approaches), perturbations (36 independent and more mixed\nperturbations) and annotations (6.3 million classification labels, 2.9 million\nmanipulated area annotations and 221,247 temporal forgery segment labels). We\nperform extensive benchmarking and studies of existing face forensics methods\nand obtain several valuable observations.",
          "link": "http://arxiv.org/abs/2103.05630",
          "publishedOn": "2021-07-15T01:59:04.085Z",
          "wordCount": 735,
          "title": "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. (arXiv:2103.05630v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vijayan_N/0/1/0/all/0/1\">Nithia Vijayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+A_P/0/1/0/all/0/1\">Prashanth L. A</a>",
          "description": "We propose two policy gradient algorithms for solving the problem of control\nin an off-policy reinforcement learning (RL) context. Both algorithms\nincorporate a smoothed functional (SF) based gradient estimation scheme. The\nfirst algorithm is a straightforward combination of importance sampling-based\noff-policy evaluation with SF-based gradient estimation. The second algorithm,\ninspired by the stochastic variance-reduced gradient (SVRG) algorithm,\nincorporates variance reduction in the update iteration. For both algorithms,\nwe derive non-asymptotic bounds that establish convergence to an approximate\nstationary point. From these results, we infer that the first algorithm\nconverges at a rate that is comparable to the well-known REINFORCE algorithm in\nan off-policy RL context, while the second algorithm exhibits an improved rate\nof convergence.",
          "link": "http://arxiv.org/abs/2101.02137",
          "publishedOn": "2021-07-15T01:59:04.078Z",
          "wordCount": 594,
          "title": "Smoothed functional-based gradient algorithms for off-policy reinforcement learning: A non-asymptotic viewpoint. (arXiv:2101.02137v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhenzhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hongtao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Da Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dagui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guihai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>",
          "description": "In e-commerce advertising, it is crucial to jointly consider various\nperformance metrics, e.g., user experience, advertiser utility, and platform\nrevenue. Traditional auction mechanisms, such as GSP and VCG auctions, can be\nsuboptimal due to their fixed allocation rules to optimize a single performance\nmetric (e.g., revenue or social welfare). Recently, data-driven auctions,\nlearned directly from auction outcomes to optimize multiple performance\nmetrics, have attracted increasing research interests. However, the procedure\nof auction mechanisms involves various discrete calculation operations, making\nit challenging to be compatible with continuous optimization pipelines in\nmachine learning. In this paper, we design \\underline{D}eep \\underline{N}eural\n\\underline{A}uctions (DNAs) to enable end-to-end auction learning by proposing\na differentiable model to relax the discrete sorting operation, a key component\nin auctions. We optimize the performance metrics by developing deep models to\nefficiently extract contexts from auctions, providing rich features for auction\ndesign. We further integrate the game theoretical conditions within the model\ndesign, to guarantee the stability of the auctions. DNAs have been successfully\ndeployed in the e-commerce advertising system at Taobao. Experimental\nevaluation results on both large-scale data set as well as online A/B test\ndemonstrated that DNAs significantly outperformed other mechanisms widely\nadopted in industry.",
          "link": "http://arxiv.org/abs/2106.03593",
          "publishedOn": "2021-07-15T01:59:04.062Z",
          "wordCount": 714,
          "title": "Neural Auction: End-to-End Learning of Auction Mechanisms for E-Commerce Advertising. (arXiv:2106.03593v2 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kao_S/0/1/0/all/0/1\">Sheng-Chun Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Suvinay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1\">Gaurav Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1\">Tushar Krishna</a>",
          "description": "Attention mechanisms form the backbone of state-of-the-art machine learning\nmodels for a variety of tasks. Deploying them on deep neural network (DNN)\naccelerators, however, is prohibitively challenging especially under long\nsequences. Operators in attention layers exhibit limited reuse and quadratic\ngrowth in memory footprint, leading to severe memory-boundedness. This paper\nintroduces a new attention-tailored dataflow, termed FLAT, which leverages\noperator fusion, loop-nest optimizations, and interleaved execution. It\nincreases the effective memory bandwidth by efficiently utilizing the\nhigh-bandwidth, low-capacity on-chip buffer and thus achieves better run time\nand compute resource utilization. We term FLAT-compatible accelerators ATTACC.\nIn our evaluation, ATTACC achieves 1.94x and 1.76x speedup and 49% and 42% of\nenergy reduction comparing to state-of-the-art edge and cloud accelerators.",
          "link": "http://arxiv.org/abs/2107.06419",
          "publishedOn": "2021-07-15T01:59:04.056Z",
          "wordCount": 549,
          "title": "ATTACC the Quadratic Bottleneck of Attention Layers. (arXiv:2107.06419v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Forouzesh_M/0/1/0/all/0/1\">Mahsa Forouzesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiran_P/0/1/0/all/0/1\">Patrick Thiran</a>",
          "description": "We propose a metric for evaluating the generalization ability of deep neural\nnetworks trained with mini-batch gradient descent. Our metric, called gradient\ndisparity, is the $\\ell_2$ norm distance between the gradient vectors of two\nmini-batches drawn from the training set. It is derived from a probabilistic\nupper bound on the difference between the classification errors over a given\nmini-batch, when the network is trained on this mini-batch and when the network\nis trained on another mini-batch of points sampled from the same dataset. We\nempirically show that gradient disparity is a very promising early-stopping\ncriterion (i) when data is limited, as it uses all the samples for training and\n(ii) when available data has noisy labels, as it signals overfitting better\nthan the validation data. Furthermore, we show in a wide range of experimental\nsettings that gradient disparity is strongly related to the generalization\nerror between the training and test sets, and that it is also very informative\nabout the level of label noise.",
          "link": "http://arxiv.org/abs/2107.06665",
          "publishedOn": "2021-07-15T01:59:04.050Z",
          "wordCount": 592,
          "title": "Disparity Between Batches as a Signal for Early Stopping. (arXiv:2107.06665v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingkun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yujie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Faqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jing Pei</a>",
          "description": "Biological spiking neurons with intrinsic dynamics underlie the powerful\nrepresentation and learning capabilities of the brain for processing multimodal\ninformation in complex environments. Despite recent tremendous progress in\nspiking neural networks (SNNs) for handling Euclidean-space tasks, it still\nremains challenging to exploit SNNs in processing non-Euclidean-space data\nrepresented by graph data, mainly due to the lack of effective modeling\nframework and useful training techniques. Here we present a general spike-based\nmodeling framework that enables the direct training of SNNs for graph learning.\nThrough spatial-temporal unfolding for spiking data flows of node features, we\nincorporate graph convolution filters into spiking dynamics and formalize a\nsynergistic learning paradigm. Considering the unique features of spike\nrepresentation and spiking dynamics, we propose a spatial-temporal feature\nnormalization (STFN) technique suitable for SNN to accelerate convergence. We\ninstantiate our methods into two spiking graph models, including graph\nconvolution SNNs and graph attention SNNs, and validate their performance on\nthree node-classification benchmarks, including Cora, Citeseer, and Pubmed. Our\nmodel can achieve comparable performance with the state-of-the-art graph neural\nnetwork (GNN) models with much lower computation costs, demonstrating great\nbenefits for the execution on neuromorphic hardware and prompting neuromorphic\napplications in graphical scenarios.",
          "link": "http://arxiv.org/abs/2107.06865",
          "publishedOn": "2021-07-15T01:59:04.044Z",
          "wordCount": 645,
          "title": "Exploiting Spiking Dynamics with Spatial-temporal Feature Normalization in Graph Learning. (arXiv:2107.06865v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sessa_P/0/1/0/all/0/1\">Pier Giuseppe Sessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogunovic_I/0/1/0/all/0/1\">Ilija Bogunovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamgarpour_M/0/1/0/all/0/1\">Maryam Kamgarpour</a>",
          "description": "We formulate the novel class of contextual games, a type of repeated games\ndriven by contextual information at each round. By means of kernel-based\nregularity assumptions, we model the correlation between different contexts and\ngame outcomes and propose a novel online (meta) algorithm that exploits such\ncorrelations to minimize the contextual regret of individual players. We define\ngame-theoretic notions of contextual Coarse Correlated Equilibria (c-CCE) and\noptimal contextual welfare for this new class of games and show that c-CCEs and\noptimal welfare can be approached whenever players' contextual regrets vanish.\nFinally, we empirically validate our results in a traffic routing experiment,\nwhere our algorithm leads to better performance and higher welfare compared to\nbaselines that do not exploit the available contextual information or the\ncorrelations present in the game.",
          "link": "http://arxiv.org/abs/2107.06327",
          "publishedOn": "2021-07-15T01:59:04.038Z",
          "wordCount": 579,
          "title": "Contextual Games: Multi-Agent Learning with Side Information. (arXiv:2107.06327v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Duhun Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eunjung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1\">Wonjong Rhee</a>",
          "description": "We propose an AID-purifier that can boost the robustness of\nadversarially-trained networks by purifying their inputs. AID-purifier is an\nauxiliary network that works as an add-on to an already trained main\nclassifier. To keep it computationally light, it is trained as a discriminator\nwith a binary cross-entropy loss. To obtain additionally useful information\nfrom the adversarial examples, the architecture design is closely related to\ninformation maximization principles where two layers of the main classification\nnetwork are piped to the auxiliary network. To assist the iterative\noptimization procedure of purification, the auxiliary network is trained with\nAVmixup. AID-purifier can be used together with other purifiers such as\nPixelDefend for an extra enhancement. The overall results indicate that the\nbest performing adversarially-trained networks can be enhanced by the best\nperforming purification networks, where AID-purifier is a competitive candidate\nthat is light and robust.",
          "link": "http://arxiv.org/abs/2107.06456",
          "publishedOn": "2021-07-15T01:59:04.021Z",
          "wordCount": 590,
          "title": "AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense. (arXiv:2107.06456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04313",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Gibbs_J/0/1/0/all/0/1\">Joe Gibbs</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gili_K/0/1/0/all/0/1\">Kaitlin Gili</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Holmes_Z/0/1/0/all/0/1\">Zo&#xeb; Holmes</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Commeau_B/0/1/0/all/0/1\">Benjamin Commeau</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Arrasmith_A/0/1/0/all/0/1\">Andrew Arrasmith</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1\">Lukasz Cincio</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1\">Patrick J. Coles</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sornborger_A/0/1/0/all/0/1\">Andrew Sornborger</a>",
          "description": "Moderate-size quantum computers are now publicly accessible over the cloud,\nopening the exciting possibility of performing dynamical simulations of quantum\nsystems. However, while rapidly improving, these devices have short coherence\ntimes, limiting the depth of algorithms that may be successfully implemented.\nHere we demonstrate that, despite these limitations, it is possible to\nimplement long-time, high fidelity simulations on current hardware.\nSpecifically, we simulate an XY-model spin chain on the Rigetti and IBM quantum\ncomputers, maintaining a fidelity of at least 0.9 for over 600 time steps. This\nis a factor of 150 longer than is possible using the iterated Trotter method.\nOur simulations are performed using a new algorithm that we call the fixed\nstate Variational Fast Forwarding (fsVFF) algorithm. This algorithm decreases\nthe circuit depth and width required for a quantum simulation by finding an\napproximate diagonalization of a short time evolution unitary. Crucially, fsVFF\nonly requires finding a diagonalization on the subspace spanned by the initial\nstate, rather than on the total Hilbert space as with previous methods,\nsubstantially reducing the required resources. We further demonstrate the\nviability of fsVFF through large numerical implementations of the algorithm, as\nwell as an analysis of its noise resilience and the scaling of simulation\nerrors.",
          "link": "http://arxiv.org/abs/2102.04313",
          "publishedOn": "2021-07-15T01:59:04.014Z",
          "wordCount": 682,
          "title": "Long-time simulations with high fidelity on quantum hardware. (arXiv:2102.04313v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1\">Kacper Sokol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flach_P/0/1/0/all/0/1\">Peter Flach</a>",
          "description": "Academic trade requires juggling multiple variants of the same content\npublished in different formats: manuscripts, presentations, posters and\ncomputational notebooks. The need to track versions to accommodate for the\nwrite--review--rebut--revise life-cycle adds another layer of complexity. We\npropose to significantly reduce this burden by maintaining a single source\ndocument in a version-controlled environment (such as git), adding\nfunctionality to generate a collection of output formats popular in academia.\nTo this end, we utilise various open-source tools from the Jupyter scientific\ncomputing ecosystem and operationalise selected software engineering concepts.\nWe offer a proof-of-concept workflow that composes Jupyter Book (an online\ndocument), Jupyter Notebook (a computational narrative) and reveal.js slides\nfrom a single markdown source file. Hosted on GitHub, our approach supports\nchange tracking and versioning, as well as a transparent review process based\non the underlying code issue management infrastructure. An exhibit of our\nworkflow can be previewed at https://so-cool.github.io/you-only-write-thrice/.",
          "link": "http://arxiv.org/abs/2107.06639",
          "publishedOn": "2021-07-15T01:59:04.007Z",
          "wordCount": 614,
          "title": "You Only Write Thrice: Creating Documents, Computational Notebooks and Presentations From a Single Source. (arXiv:2107.06639v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagrecha_K/0/1/0/all/0/1\">Kabir Nagrecha</a>",
          "description": "As deep learning becomes more expensive, both in terms of time and compute,\ninefficiencies in machine learning (ML) training prevent practical usage of\nstate-of-the-art models for most users. The newest model architectures are\nsimply too large to be fit onto a single processor. To address the issue, many\nML practitioners have turned to model parallelism as a method of distributing\nthe computational requirements across several devices. Unfortunately, the\nsequential nature of neural networks causes very low efficiency and device\nutilization in model parallel training jobs. We propose a new form of \"shard\nparallelism\" combining task and model parallelism, then package it into a\nframework we name Hydra. Hydra recasts the problem of model parallelism in the\nmulti-model context to produce a fine-grained parallel workload of independent\nmodel shards, rather than independent models. This new parallel design promises\ndramatic speedups relative to the traditional model parallelism paradigm.",
          "link": "http://arxiv.org/abs/2107.06469",
          "publishedOn": "2021-07-15T01:59:03.992Z",
          "wordCount": 606,
          "title": "Model-Parallel Model Selection for Deep Learning Systems. (arXiv:2107.06469v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2007.06093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albarghouthi_A/0/1/0/all/0/1\">Aws Albarghouthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakriya_G/0/1/0/all/0/1\">Gautam Prakriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "To verify safety and robustness of neural networks, researchers have\nsuccessfully applied abstract interpretation, primarily using the interval\nabstract domain. In this paper, we study the theoretical power and limits of\nthe interval domain for neural-network verification.\n\nFirst, we introduce the interval universal approximation (IUA) theorem. IUA\nshows that neural networks not only can approximate any continuous function $f$\n(universal approximation) as we have known for decades, but we can find a\nneural network, using any well-behaved activation function, whose interval\nbounds are an arbitrarily close approximation of the set semantics of $f$ (the\nresult of applying $f$ to a set of inputs). We call this notion of\napproximation interval approximation. Our theorem generalizes the recent result\nof Baader et al. (2020) from ReLUs to a rich class of activation functions that\nwe call squashable functions. Additionally, the IUA theorem implies that we can\nalways construct provably robust neural networks under $\\ell_\\infty$-norm using\nalmost any practical activation function.\n\nSecond, we study the computational complexity of constructing neural networks\nthat are amenable to precise interval analysis. This is a crucial question, as\nour constructive proof of IUA is exponential in the size of the approximation\ndomain. We boil this question down to the problem of approximating the range of\na neural network with squashable activation functions. We show that the range\napproximation problem (RA) is a $\\Delta_2$-intermediate problem, which is\nstrictly harder than $\\mathsf{NP}$-complete problems, assuming\n$\\mathsf{coNP}\\not\\subset \\mathsf{NP}$. As a result, IUA is an inherently hard\nproblem: No matter what abstract domain or computational tools we consider to\nachieve interval approximation, there is no efficient construction of such a\nuniversal approximator.",
          "link": "http://arxiv.org/abs/2007.06093",
          "publishedOn": "2021-07-15T01:59:03.984Z",
          "wordCount": 760,
          "title": "Interval Universal Approximation for Neural Networks. (arXiv:2007.06093v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Camargo_A/0/1/0/all/0/1\">Augusto Camargo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1\">Wesley Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peressim_F/0/1/0/all/0/1\">Felipe Peressim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_A/0/1/0/all/0/1\">Alan Barzilay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1\">Marcelo Finger</a>",
          "description": "In this paper, we studied whether models based on BiLSTM and BERT can predict\nhashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a\nsizable financial impact on Ecommerce. We processed a corpus of Ecommerce\nreviews as inputs, and predicted hashtags as outputs. We evaluated the results\nusing four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A\nword cloud was used as a qualitative metric. While all computer-generated\nmetrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results\nproduced amazing scores. We concluded that the texts predicted by the neural\nnetworks are very promising for use as hashtags for products on Ecommerce\nwebsites. The code for this work is available at\nhttps://github.com/augustocamargo/text-to-hashtag.",
          "link": "http://arxiv.org/abs/2102.00904",
          "publishedOn": "2021-07-15T01:59:03.938Z",
          "wordCount": 573,
          "title": "Text-to-hashtag Generation using Seq2seq Learning. (arXiv:2102.00904v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Svedin_M/0/1/0/all/0/1\">Martin Svedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1\">Artur Podobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Steven W. D. Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1\">Stefano Markidis</a>",
          "description": "One of the most promising approaches for data analysis and exploration of\nlarge data sets is Machine Learning techniques that are inspired by brain\nmodels. Such methods use alternative learning rules potentially more\nefficiently than established learning rules. In this work, we focus on the\npotential of brain-inspired ML for exploiting High-Performance Computing (HPC)\nresources to solve ML problems: we discuss the BCPNN and an HPC implementation,\ncalled StreamBrain, its computational cost, suitability to HPC systems. As an\nexample, we use StreamBrain to analyze the Higgs Boson dataset from High Energy\nPhysics and discriminate between background and signal classes in collisions of\nhigh-energy particle colliders. Overall, we reach up to 69.15% accuracy and\n76.4% Area Under the Curve (AUC) performance.",
          "link": "http://arxiv.org/abs/2107.06676",
          "publishedOn": "2021-07-15T01:59:03.932Z",
          "wordCount": 591,
          "title": "Higgs Boson Classification: Brain-inspired BCPNN Learning with StreamBrain. (arXiv:2107.06676v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akarmazyan_S/0/1/0/all/0/1\">Siranush Akarmazyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengaud_E/0/1/0/all/0/1\">Eric Armengaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacco_M/0/1/0/all/0/1\">Manlio Bacco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravos_G/0/1/0/all/0/1\">George Bravos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calandra_C/0/1/0/all/0/1\">Calogero Calandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_E/0/1/0/all/0/1\">Emanuele Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassara_P/0/1/0/all/0/1\">Pietro Cassara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_M/0/1/0/all/0/1\">Massimo Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davalas_C/0/1/0/all/0/1\">Charalampos Davalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dazzi_P/0/1/0/all/0/1\">Patrizio Dazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Degennaro_M/0/1/0/all/0/1\">Maria Carmela Degennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarli_D/0/1/0/all/0/1\">Daniele Di Sarli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobaj_J/0/1/0/all/0/1\">J&#xfc;rgen Dobaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1\">Claudio Gallicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girbal_S/0/1/0/all/0/1\">Sylvain Girbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotta_A/0/1/0/all/0/1\">Alberto Gotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groppo_R/0/1/0/all/0/1\">Riccardo Groppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macher_G/0/1/0/all/0/1\">Georg Macher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzei_D/0/1/0/all/0/1\">Daniele Mazzei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mencagli_G/0/1/0/all/0/1\">Gabriele Mencagli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1\">Dimitrios Michail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1\">Alessio Micheli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroglio_R/0/1/0/all/0/1\">Roberta Peroglio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_S/0/1/0/all/0/1\">Salvatore Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potenza_R/0/1/0/all/0/1\">Rosaria Potenza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourdanesh_F/0/1/0/all/0/1\">Farank Pourdanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardianos_C/0/1/0/all/0/1\">Christos Sardianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tserpes_K/0/1/0/all/0/1\">Konstantinos Tserpes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabo_F/0/1/0/all/0/1\">Fulvio Tagliab&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valtl_J/0/1/0/all/0/1\">Jakob Valtl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varlamis_I/0/1/0/all/0/1\">Iraklis Varlamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veledar_O/0/1/0/all/0/1\">Omar Veledar</a>",
          "description": "This paper discusses the perspective of the H2020 TEACHING project on the\nnext generation of autonomous applications running in a distributed and highly\nheterogeneous environment comprising both virtual and physical resources\nspanning the edge-cloud continuum. TEACHING puts forward a human-centred vision\nleveraging the physiological, emotional, and cognitive state of the users as a\ndriver for the adaptation and optimization of the autonomous applications. It\ndoes so by building a distributed, embedded and federated learning system\ncomplemented by methods and tools to enforce its dependability, security and\nprivacy preservation. The paper discusses the main concepts of the TEACHING\napproach and singles out the main AI-related research challenges associated\nwith it. Further, we provide a discussion of the design choices for the\nTEACHING system to tackle the aforementioned challenges",
          "link": "http://arxiv.org/abs/2107.06543",
          "publishedOn": "2021-07-15T01:59:03.922Z",
          "wordCount": 627,
          "title": "TEACHING -- Trustworthy autonomous cyber-physical applications through human-centred intelligence. (arXiv:2107.06543v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06642",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Luong_M/0/1/0/all/0/1\">Manh Luong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1\">Viet Anh Tran</a>",
          "description": "Voice conversion is a challenging task which transforms the voice\ncharacteristics of a source speaker to a target speaker without changing\nlinguistic content. Recently, there have been many works on many-to-many Voice\nConversion (VC) based on Variational Autoencoder (VAEs) achieving good results,\nhowever, these methods lack the ability to disentangle speaker identity and\nlinguistic content to achieve good performance on unseen speaker scenarios. In\nthis paper, we propose a new method based on feature disentanglement to tackle\nmany to many voice conversion. The method has the capability to disentangle\nspeaker identity and linguistic content from utterances, it can convert from\nmany source speakers to many target speakers with a single autoencoder network.\nMoreover, it naturally deals with the unseen target speaker scenarios. We\nperform both objective and subjective evaluations to show the competitive\nperformance of our proposed method compared with other state-of-the-art models\nin terms of naturalness and target speaker similarity.",
          "link": "http://arxiv.org/abs/2107.06642",
          "publishedOn": "2021-07-15T01:59:03.909Z",
          "wordCount": 594,
          "title": "Many-to-Many Voice Conversion based Feature Disentanglement using Variational Autoencoder. (arXiv:2107.06642v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Munteanu_A/0/1/0/all/0/1\">Alexander Munteanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omlor_S/0/1/0/all/0/1\">Simon Omlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David Woodruff</a>",
          "description": "What guarantees are possible for solving logistic regression in one pass over\na data stream? To answer this question, we present the first data oblivious\nsketch for logistic regression. Our sketch can be computed in input sparsity\ntime over a turnstile data stream and reduces the size of a $d$-dimensional\ndata set from $n$ to only $\\operatorname{poly}(\\mu d\\log n)$ weighted points,\nwhere $\\mu$ is a useful parameter which captures the complexity of compressing\nthe data. Solving (weighted) logistic regression on the sketch gives an $O(\\log\nn)$-approximation to the original problem on the full data set. We also show\nhow to obtain an $O(1)$-approximation with slight modifications. Our sketches\nare fast, simple, easy to implement, and our experiments demonstrate their\npracticality.",
          "link": "http://arxiv.org/abs/2107.06615",
          "publishedOn": "2021-07-15T01:59:03.883Z",
          "wordCount": 558,
          "title": "Oblivious sketching for logistic regression. (arXiv:2107.06615v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartal_Y/0/1/0/all/0/1\">Yair Bartal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fandina_O/0/1/0/all/0/1\">Ora Nova Fandina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1\">Kasper Green Larsen</a>",
          "description": "It is well known that the Johnson-Lindenstrauss dimensionality reduction\nmethod is optimal for worst case distortion. While in practice many other\nmethods and heuristics are used, not much is known in terms of bounds on their\nperformance. The question of whether the JL method is optimal for practical\nmeasures of distortion was recently raised in \\cite{BFN19} (NeurIPS'19). They\nprovided upper bounds on its quality for a wide range of practical measures and\nshowed that indeed these are best possible in many cases. Yet, some of the most\nimportant cases, including the fundamental case of average distortion were left\nopen. In particular, they show that the JL transform has $1+\\epsilon$ average\ndistortion for embedding into $k$-dimensional Euclidean space, where\n$k=O(1/\\eps^2)$, and for more general $q$-norms of distortion, $k =\nO(\\max\\{1/\\eps^2,q/\\eps\\})$, whereas tight lower bounds were established only\nfor large values of $q$ via reduction to the worst case.\n\nIn this paper we prove that these bounds are best possible for any\ndimensionality reduction method, for any $1 \\leq q \\leq O(\\frac{\\log (2\\eps^2\nn)}{\\eps})$ and $\\epsilon \\geq \\frac{1}{\\sqrt{n}}$, where $n$ is the size of\nthe subset of Euclidean space.\n\nOur results imply that the JL method is optimal for various distortion\nmeasures commonly used in practice, such as {\\it stress, energy} and {\\it\nrelative error}. We prove that if any of these measures is bounded by $\\eps$\nthen $k=\\Omega(1/\\eps^2)$, for any $\\epsilon \\geq \\frac{1}{\\sqrt{n}}$, matching\nthe upper bounds of \\cite{BFN19} and extending their tightness results for the\nfull range moment analysis.\n\nOur results may indicate that the JL dimensionality reduction method should\nbe considered more often in practical applications, and the bounds we provide\nfor its quality should be served as a measure for comparison when evaluating\nthe performance of other methods and heuristics.",
          "link": "http://arxiv.org/abs/2107.06626",
          "publishedOn": "2021-07-15T01:59:03.876Z",
          "wordCount": 741,
          "title": "Optimality of the Johnson-Lindenstrauss Dimensionality Reduction for Practical Measures. (arXiv:2107.06626v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>",
          "description": "For most machine learning (ML) tasks, evaluating learning performance on a\ngiven dataset requires intensive computation. On the other hand, the ability to\nefficiently estimate learning performance may benefit a wide spectrum of\napplications, such as active learning, data quality management, and data\nvaluation. Recent empirical studies show that for many common ML models, one\ncan accurately learn a parametric model that predicts learning performance for\nany given input datasets using a small amount of samples. However, the\ntheoretical underpinning of the learnability of such performance prediction\nmodels is still missing. In this work, we develop the first theoretical\nanalysis of the ML performance learning problem. We propose a relaxed notion\nfor submodularity that can well describe the behavior of learning performance\nas a function of input datasets. We give a learning algorithm that achieves a\nconstant-factor approximation under certain assumptions. Further, we give a\nlearning algorithm that achieves arbitrarily small error based on a newly\nderived structural result. We then discuss a natural, important use case of\nlearning performance learning -- data valuation, which is known to suffer\ncomputational challenges due to the requirement of estimating learning\nperformance for many data combinations. We show that performance learning can\nsignificantly improve the accuracy of data valuation.",
          "link": "http://arxiv.org/abs/2107.06336",
          "publishedOn": "2021-07-15T01:59:03.863Z",
          "wordCount": 638,
          "title": "Learnability of Learning Performance and Its Application to Data Valuation. (arXiv:2107.06336v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenqi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Siqin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>",
          "description": "Recurrent neural networks for language models like long short-term memory\n(LSTM) have been utilized as a tool for modeling and predicting long term\ndynamics of complex stochastic molecular systems. Recently successful examples\non learning slow dynamics by LSTM are given with simulation data of low\ndimensional reaction coordinate. However, in this report we show that the\nfollowing three key factors significantly affect the performance of language\nmodel learning, namely dimensionality of reaction coordinates, temporal\nresolution and state partition. When applying recurrent neural networks to\nmolecular dynamics simulation trajectories of high dimensionality, we find that\nrare events corresponding to the slow dynamics might be obscured by other\nfaster dynamics of the system, and cannot be efficiently learned. Under such\nconditions, we find that coarse graining the conformational space into\nmetastable states and removing recrossing events when estimating transition\nprobabilities between states could greatly help improve the accuracy of slow\ndynamics learning in molecular dynamics. Moreover, we also explore other models\nlike Transformer, which do not show superior performance than LSTM in\novercoming these issues. Therefore, to learn rare events of slow molecular\ndynamics by LSTM and Transformer, it is critical to choose proper temporal\nresolution (i.e., saving intervals of MD simulation trajectories) and state\npartition in high resolution data, since deep neural network models might not\nautomatically disentangle slow dynamics from fast dynamics when both are\npresent in data influencing each other.",
          "link": "http://arxiv.org/abs/2107.06573",
          "publishedOn": "2021-07-15T01:59:03.848Z",
          "wordCount": 680,
          "title": "A Note on Learning Rare Events in Molecular Dynamics using LSTM and Transformer. (arXiv:2107.06573v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual\nencoders, using a relatively small set of manually-annotated data (as compared\nto web-crawled data), to perceive the visual world. However, it has been\nobserved that large-scale pretraining usually can result in better\ngeneralization performance, e.g., CLIP (Contrastive Language-Image\nPre-training), trained on a massive amount of image-caption pairs, has shown a\nstrong zero-shot capability on various vision tasks. To further study the\nadvantage brought by CLIP, we propose to use CLIP as the visual encoder in\nvarious V&L models in two typical scenarios: 1) plugging CLIP into\ntask-specific fine-tuning; 2) combining CLIP with V&L pre-training and\ntransferring to downstream tasks. We show that CLIP significantly outperforms\nwidely-used visual encoders trained with in-domain annotated data, such as\nBottomUp-TopDown. We achieve competitive or better results on diverse V&L\ntasks, while establishing new state-of-the-art results on Visual Question\nAnswering, Visual Entailment, and V&L Navigation tasks. We release our code at\nhttps://github.com/clip-vil/CLIP-ViL.",
          "link": "http://arxiv.org/abs/2107.06383",
          "publishedOn": "2021-07-15T01:59:03.826Z",
          "wordCount": 618,
          "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roschewitz_D/0/1/0/all/0/1\">David Roschewitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_M/0/1/0/all/0/1\">Mary-Anne Hartley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corinzia_L/0/1/0/all/0/1\">Luca Corinzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Recently, the ever-growing demand for privacy-oriented machine learning has\nmotivated researchers to develop federated and decentralized learning\ntechniques, allowing individual clients to train models collaboratively without\ndisclosing their private datasets. However, widespread adoption has been\nlimited in domains relying on high levels of user trust, where assessment of\ndata compatibility is essential. In this work, we define and address low\ninteroperability induced by underlying client data inconsistencies in federated\nlearning for tabular data. The proposed method, iFedAvg, builds on federated\naveraging adding local element-wise affine layers to allow for a personalized\nand granular understanding of the collaborative learning process. Thus,\nenabling the detection of outlier datasets in the federation and also learning\nthe compensation for local data distribution shifts without sharing any\noriginal data. We evaluate iFedAvg using several public benchmarks and a\npreviously unstudied collection of real-world datasets from the 2014 - 2016\nWest African Ebola epidemic, jointly forming the largest such dataset in the\nworld. In all evaluations, iFedAvg achieves competitive average performance\nwith negligible overhead. It additionally shows substantial improvement on\noutlier clients, highlighting increased robustness to individual dataset\nshifts. Most importantly, our method provides valuable client-specific insights\nat a fine-grained level to guide interoperable federated learning.",
          "link": "http://arxiv.org/abs/2107.06580",
          "publishedOn": "2021-07-15T01:59:03.817Z",
          "wordCount": 633,
          "title": "IFedAvg: Interpretable Data-Interoperability for Federated Learning. (arXiv:2107.06580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1\">Patryk Orzechowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1\">Jason H. Moore</a>",
          "description": "Understanding the strengths and weaknesses of machine learning (ML)\nalgorithms is crucial for determine their scope of application. Here, we\nintroduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of\nsynthetic datasets for comprehensive, reproducible, and interpretable\nbenchmarking of machine learning algorithms for classification of binary\noutcomes. The DIGEN resource consists of 40 mathematical functions which map\ncontinuous features to discrete endpoints for creating synthetic datasets.\nThese 40 functions were discovered using a heuristic algorithm designed to\nmaximize the diversity of performance among multiple popular machine learning\nalgorithms thus providing a useful test suite for evaluating and comparing new\nmethods. Access to the generative functions facilitates understanding of why a\nmethod performs poorly compared to other algorithms thus providing ideas for\nimprovement. The resource with extensive documentation and analyses is\nopen-source and available on GitHub.",
          "link": "http://arxiv.org/abs/2107.06475",
          "publishedOn": "2021-07-15T01:59:03.790Z",
          "wordCount": 615,
          "title": "Generative and reproducible benchmarks for comprehensive evaluation of machine learning classifiers. (arXiv:2107.06475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sungryull Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungtae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jongwook Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seijen_H/0/1/0/all/0/1\">Harm van Seijen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1\">Mehdi Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>",
          "description": "We propose the k-Shortest-Path (k-SP) constraint: a novel constraint on the\nagent's trajectory that improves the sample efficiency in sparse-reward MDPs.\nWe show that any optimal policy necessarily satisfies the k-SP constraint.\nNotably, the k-SP constraint prevents the policy from exploring state-action\npairs along the non-k-SP trajectories (e.g., going back and forth). However, in\npractice, excluding state-action pairs may hinder the convergence of RL\nalgorithms. To overcome this, we propose a novel cost function that penalizes\nthe policy violating SP constraint, instead of completely excluding it. Our\nnumerical experiment in a tabular RL setting demonstrates that the SP\nconstraint can significantly reduce the trajectory space of policy. As a\nresult, our constraint enables more sample efficient learning by suppressing\nredundant exploration and exploitation. Our experiments on MiniGrid, DeepMind\nLab, Atari, and Fetch show that the proposed method significantly improves\nproximal policy optimization (PPO) and outperforms existing novelty-seeking\nexploration methods including count-based exploration even in continuous\ncontrol tasks, indicating that it improves the sample efficiency by preventing\nthe agent from taking redundant actions.",
          "link": "http://arxiv.org/abs/2107.06405",
          "publishedOn": "2021-07-15T01:59:03.772Z",
          "wordCount": 619,
          "title": "Shortest-Path Constrained Reinforcement Learning for Sparse Reward Tasks. (arXiv:2107.06405v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1\">Weikai Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">Geguang Pu</a>",
          "description": "High-level representation-guided pixel denoising and adversarial training are\nindependent solutions to enhance the robustness of CNNs against adversarial\nattacks by pre-processing input data and re-training models, respectively. Most\nrecently, adversarial training techniques have been widely studied and improved\nwhile the pixel denoising-based method is getting less attractive. However, it\nis still questionable whether there exists a more advanced pixel\ndenoising-based method and whether the combination of the two solutions\nbenefits each other. To this end, we first comprehensively investigate two\nkinds of pixel denoising methods for adversarial robustness enhancement (i.e.,\nexisting additive-based and unexplored filtering-based methods) under the loss\nfunctions of image-level and semantic-level restorations, respectively, showing\nthat pixel-wise filtering can obtain much higher image quality (e.g., higher\nPSNR) as well as higher robustness (e.g., higher accuracy on adversarial\nexamples) than existing pixel-wise additive-based method. However, we also\nobserve that the robustness results of the filtering-based method rely on the\nperturbation amplitude of adversarial examples used for training. To address\nthis problem, we propose predictive perturbation-aware pixel-wise filtering,\nwhere dual-perturbation filtering and an uncertainty-aware fusion module are\ndesigned and employed to automatically perceive the perturbation amplitude\nduring the training and testing process. The proposed method is termed as\nAdvFilter. Moreover, we combine adversarial pixel denoising methods with three\nadversarial training-based methods, hinting that considering data and models\njointly is able to achieve more robust CNNs. The experiments conduct on\nNeurIPS-2017DEV, SVHN, and CIFAR10 datasets and show the advantages over\nenhancing CNNs' robustness, high generalization to different models, and noise\nlevels.",
          "link": "http://arxiv.org/abs/2107.06501",
          "publishedOn": "2021-07-15T01:59:03.766Z",
          "wordCount": 716,
          "title": "AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tuan Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1\">Katherine M. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1\">Luke Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1\">Kevin Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Siddharth N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1\">Samuel J. Gershman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>",
          "description": "Modeling complex phenomena typically involves the use of both discrete and\ncontinuous variables. Such a setting applies across a wide range of problems,\nfrom identifying trends in time-series data to performing effective\ncompositional scene understanding in images. Here, we propose Hybrid Memoised\nWake-Sleep (HMWS), an algorithm for effective inference in such hybrid\ndiscrete-continuous models. Prior approaches to learning suffer as they need to\nperform repeated expensive inner-loop discrete inference. We build on a recent\napproach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by\nmemoising discrete variables, and extend it to allow for a principled and\neffective way to handle continuous variables by learning a separate recognition\nmodel used for importance-sampling based approximate inference and\nmarginalization. We evaluate HMWS in the GP-kernel learning and 3D scene\nunderstanding domains, and show that it outperforms current state-of-the-art\ninference methods.",
          "link": "http://arxiv.org/abs/2107.06393",
          "publishedOn": "2021-07-15T01:59:03.756Z",
          "wordCount": 595,
          "title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06396",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1\">Ushnish Sengupta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Waxenegger_Wilfing_G/0/1/0/all/0/1\">G&#xfc;nther Waxenegger-Wilfing</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hardi_J/0/1/0/all/0/1\">Justin Hardi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1\">Matthew P. Juniper</a>",
          "description": "The 100 MW cryogenic liquid oxygen/hydrogen multi-injector combustor BKD\noperated by the DLR Institute of Space Propulsion is a research platform that\nallows the study of thermoacoustic instabilities under realistic conditions,\nrepresentative of small upper stage rocket engines. We use data from BKD\nexperimental campaigns in which the static chamber pressure and fuel-oxidizer\nratio are varied such that the first tangential mode of the combustor is\nexcited under some conditions. We train an autoregressive Bayesian neural\nnetwork model to forecast the amplitude of the dynamic pressure time series,\ninputting multiple sensor measurements (injector pressure/ temperature\nmeasurements, static chamber pressure, high-frequency dynamic pressure\nmeasurements, high-frequency OH* chemiluminescence measurements) and future\nflow rate control signals. The Bayesian nature of our algorithms allows us to\nwork with a dataset whose size is restricted by the expense of each\nexperimental run, without making overconfident extrapolations. We find that the\nnetworks are able to accurately forecast the evolution of the pressure\namplitude and anticipate instability events on unseen experimental runs 500\nmilliseconds in advance. We compare the predictive accuracy of multiple models\nusing different combinations of sensor inputs. We find that the high-frequency\ndynamic pressure signal is particularly informative. We also use the technique\nof integrated gradients to interpret the influence of different sensor inputs\non the model prediction. The negative log-likelihood of data points in the test\ndataset indicates that predictive uncertainties are well-characterized by our\nBayesian model and simulating a sensor failure event results as expected in a\ndramatic increase in the epistemic component of the uncertainty.",
          "link": "http://arxiv.org/abs/2107.06396",
          "publishedOn": "2021-07-15T01:59:03.748Z",
          "wordCount": 705,
          "title": "Forecasting Thermoacoustic Instabilities in Liquid Propellant Rocket Engines Using Multimodal Bayesian Deep Learning. (arXiv:2107.06396v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham M. Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaqi Yang</a>",
          "description": "Deep Reinforcement Learning (RL) powered by neural net approximation of the Q\nfunction has had enormous empirical success. While the theory of RL has\ntraditionally focused on linear function approximation (or eluder dimension)\napproaches, little is known about nonlinear RL with neural net approximations\nof the Q functions. This is the focus of this work, where we study function\napproximation with two-layer neural networks (considering both ReLU and\npolynomial activation functions). Our first result is a computationally and\nstatistically efficient algorithm in the generative model setting under\ncompleteness for two-layer neural networks. Our second result considers this\nsetting but under only realizability of the neural net function class. Here,\nassuming deterministic dynamics, the sample complexity scales linearly in the\nalgebraic dimension. In all cases, our results significantly improve upon what\ncan be attained with linear (or eluder dimension) methods.",
          "link": "http://arxiv.org/abs/2107.06466",
          "publishedOn": "2021-07-15T01:59:03.741Z",
          "wordCount": 583,
          "title": "Going Beyond Linear RL: Sample Efficient Neural Function Approximation. (arXiv:2107.06466v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06473",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Fang_W/0/1/0/all/0/1\">Wenqi Fang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_G/0/1/0/all/0/1\">Guanlin Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cao_J/0/1/0/all/0/1\">Jiang Cao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ping_Y/0/1/0/all/0/1\">Yang Ping</a>",
          "description": "Spectral approximation and variational inducing learning for the Gaussian\nprocess are two popular methods to reduce computational complexity. However, in\nprevious research, those methods always tend to adopt the orthonormal basis\nfunctions, such as eigenvectors in the Hilbert space, in the spectrum method,\nor decoupled orthogonal components in the variational framework. In this paper,\ninspired by quantum physics, we introduce a novel basis function, which is\ntunable, local and bounded, to approximate the kernel function in the Gaussian\nprocess. There are two adjustable parameters in these functions, which control\ntheir orthogonality to each other and limit their boundedness. And we conduct\nextensive experiments on open-source datasets to testify its performance.\nCompared to several state-of-the-art methods, it turns out that the proposed\nmethod can obtain satisfactory or even better results, especially with poorly\nchosen kernel functions.",
          "link": "http://arxiv.org/abs/2107.06473",
          "publishedOn": "2021-07-15T01:59:03.734Z",
          "wordCount": 576,
          "title": "Spectrum Gaussian Processes Based On Tunable Basis Functions. (arXiv:2107.06473v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awalina_A/0/1/0/all/0/1\">Aisyah Awalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaid_J/0/1/0/all/0/1\">Jibran Fawaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krisnabayu_R/0/1/0/all/0/1\">Rifky Yunus Krisnabayu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "Fake news is a problem faced by society in this era. It is not rare for fake\nnews to cause provocation and problem for the people. Indonesia, as a country\nwith the 4th largest population, has a problem in dealing with fake news. More\nthan 30% of rural and urban population are deceived by this fake news problem.\nAs we have been studying, there is only few literatures on preventing the\nspread of fake news in Bahasa Indonesia. So, this research is conducted to\nprevent these problems. The dataset used in this research was obtained from a\nnews portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on\nthis page, we got 1116 data consisting of valid news and fake news. The dataset\ncan be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This\ndataset will be combined with other available datasets. The methods used are\nCNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This\nresearch shows that the BERT method with Transformer Network has the best\nresults with an accuracy of up to 90%.",
          "link": "http://arxiv.org/abs/2107.06796",
          "publishedOn": "2021-07-15T01:59:03.710Z",
          "wordCount": 608,
          "title": "Indonesia's Fake News Detection using Transformer Network. (arXiv:2107.06796v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1\">Larisa Markeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1\">Ilya Tolstikhin</a>",
          "description": "We introduce a generalization to the lottery ticket hypothesis in which the\nnotion of \"sparsity\" is relaxed by choosing an arbitrary basis in the space of\nparameters. We present evidence that the original results reported for the\ncanonical basis continue to hold in this broader setting. We describe how\nstructured pruning methods, including pruning units or factorizing\nfully-connected layers into products of low-rank matrices, can be cast as\nparticular instances of this \"generalized\" lottery ticket hypothesis. The\ninvestigations reported here are preliminary and are provided to encourage\nfurther research along this direction.",
          "link": "http://arxiv.org/abs/2107.06825",
          "publishedOn": "2021-07-15T01:59:03.703Z",
          "wordCount": 542,
          "title": "A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingcheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenjian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanbo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Wenjie Liang</a>",
          "description": "Accurate capacitance extraction is becoming more important for designing\nintegrated circuits under advanced process technology. The pattern matching\nbased full-chip extraction methodology delivers fast computational speed, but\nsuffers from large error, and tedious efforts on building capacitance models of\nthe increasing structure patterns. In this work, we propose an effective method\nfor building convolutional neural network (CNN) based capacitance models\n(called CNN-Cap) for two-dimensional (2-D) structures in full-chip capacitance\nextraction. With a novel grid-based data representation, the proposed method is\nable to model the pattern with a variable number of conductors, so that largely\nreduce the number of patterns. Based on the ability of ResNet architecture on\ncapturing spatial information and the proposed training skills, the obtained\nCNN-Cap exhibits much better performance over the multilayer perception neural\nnetwork based capacitance model while being more versatile. Extensive\nexperiments on a 55nm and a 15nm process technologies have demonstrated that\nthe error of total capacitance produced with CNN-Cap is always within 1.3% and\nthe error of produced coupling capacitance is less than 10% in over 99.5%\nprobability. CNN-Cap runs more than 4000X faster than 2-D field solver on a GPU\nserver, while it consumes negligible memory compared to the look-up table based\ncapacitance model.",
          "link": "http://arxiv.org/abs/2107.06511",
          "publishedOn": "2021-07-15T01:59:03.695Z",
          "wordCount": 659,
          "title": "CNN-Cap: Effective Convolutional Neural Network Based Capacitance Models for Full-Chip Parasitic Extraction. (arXiv:2107.06511v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskare_P/0/1/0/all/0/1\">Pranjal Bhaskare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "This paper presents a deep learning approach for the classification of\nEngineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to\nthe availability of large annotated datasets and also enough computational\npower in the form of GPUs, many deep learning-based solutions for object\nclassification have been proposed of late, especially in the domain of images\nand graphical models. Nevertheless, very few solutions have been proposed for\nthe task of functional classification of CAD models. Hence, for this research,\nCAD models have been collected from Engineering Shape Benchmark (ESB), National\nDesign Repository (NDR) and augmented with newer models created using a\nmodelling software to form a dataset - 'CADNET'. It is proposed to use a\nresidual network architecture for CADNET, inspired by the popular ResNet. A\nweighted Light Field Descriptor (LFD) scheme is chosen as the method of feature\nextraction, and the generated images are fed as inputs to the CNN. The problem\nof class imbalance in the dataset is addressed using a class weights approach.\nExperiments have been conducted with other signatures such as geodesic distance\netc. using deep networks as well as other network architectures on the CADNET.\nThe LFD-based CNN approach using the proposed network architecture, along with\ngradient boosting yielded the best classification accuracy on CADNET.",
          "link": "http://arxiv.org/abs/2107.06481",
          "publishedOn": "2021-07-15T01:59:03.667Z",
          "wordCount": 671,
          "title": "A Convolutional Neural Network Approach to the Classification of Engineering Models. (arXiv:2107.06481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sato_M/0/1/0/all/0/1\">Masahiro Sato</a>",
          "description": "Evaluating the causal effect of recommendations is an important objective\nbecause the causal effect on user interactions can directly leads to an\nincrease in sales and user engagement. To select an optimal recommendation\nmodel, it is common to conduct A/B testing to compare model performance.\nHowever, A/B testing of causal effects requires a large number of users, making\nsuch experiments costly and risky. We therefore propose the first interleaving\nmethods that can efficiently compare recommendation models in terms of causal\neffects. In contrast to conventional interleaving methods, we measure the\noutcomes of both items on an interleaved list and items not on the interleaved\nlist, since the causal effect is the difference between outcomes with and\nwithout recommendations. To ensure that the evaluations are unbiased, we either\nselect items with equal probability or weight the outcomes using inverse\npropensity scores. We then verify the unbiasedness and efficiency of online\nevaluation methods through simulated online experiments. The results indicate\nthat our proposed methods are unbiased and that they have superior efficiency\nto A/B testing.",
          "link": "http://arxiv.org/abs/2107.06630",
          "publishedOn": "2021-07-15T01:59:03.658Z",
          "wordCount": 604,
          "title": "Online Evaluation Methods for the Causal Effect of Recommendations. (arXiv:2107.06630v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogdanovic_M/0/1/0/all/0/1\">Miroslav Bogdanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadiv_M/0/1/0/all/0/1\">Majid Khadiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1\">Ludovic Righetti</a>",
          "description": "In this work we present a general, two-stage reinforcement learning approach\nfor going from a single demonstration trajectory to a robust policy that can be\ndeployed on hardware without any additional training. The demonstration is used\nin the first stage as a starting point to facilitate initial exploration. In\nthe second stage, the relevant task reward is optimized directly and a policy\nrobust to environment uncertainties is computed. We demonstrate and examine in\ndetail performance and robustness of our approach on highly dynamic hopping and\nbounding tasks on a real quadruped robot.",
          "link": "http://arxiv.org/abs/2107.06629",
          "publishedOn": "2021-07-15T01:59:03.650Z",
          "wordCount": 532,
          "title": "Model-free Reinforcement Learning for Robust Locomotion Using Trajectory Optimization for Exploration. (arXiv:2107.06629v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bernhardt_M/0/1/0/all/0/1\">Melanie Bernhardt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1\">Rajesh Jena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nushi_B/0/1/0/all/0/1\">Besmira Nushi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wadhwani_S/0/1/0/all/0/1\">Sharan Wadhwani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Natarajan_K/0/1/0/all/0/1\">Kal Natarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashraf_S/0/1/0/all/0/1\">Shazad Ashraf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>",
          "description": "Chest radiography has been a recommended procedure for patient triaging and\nresource management in intensive care units (ICUs) throughout the COVID-19\npandemic. The machine learning efforts to augment this workflow have been long\nchallenged due to deficiencies in reporting, model evaluation, and failure mode\nanalysis. To address some of those shortcomings, we model radiological features\nwith a human-interpretable class hierarchy that aligns with the radiological\ndecision process. Also, we propose the use of a data-driven error analysis\nmethodology to uncover the blind spots of our model, providing further\ntransparency on its clinical utility. For example, our experiments show that\nmodel failures highly correlate with ICU imaging conditions and with the\ninherent difficulty in distinguishing certain types of radiological features.\nAlso, our hierarchical interpretation and analysis facilitates the comparison\nwith respect to radiologists' findings and inter-variability, which in return\nhelps us to better assess the clinical applicability of models.",
          "link": "http://arxiv.org/abs/2107.06618",
          "publishedOn": "2021-07-15T01:59:03.643Z",
          "wordCount": 673,
          "title": "Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs. (arXiv:2107.06618v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaghouri_A/0/1/0/all/0/1\">Anas Al Shaghouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhatib_R/0/1/0/all/0/1\">Rami Alkhatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berjaoui_S/0/1/0/all/0/1\">Samir Berjaoui</a>",
          "description": "Roads are connecting line between different places, and used daily. Roads'\nperiodic maintenance keeps them safe and functional. Detecting and reporting\nthe existence of potholes to responsible departments can help in eliminating\nthem. This study deployed and tested on different deep learning architecture to\ndetect potholes. The images used for training were collected by cellphone\nmounted on the windshield of the car, in addition to many images downloaded\nfrom the internet to increase the size and variability of the database. Second,\nvarious object detection algorithms are employed and compared to detect\npotholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53.\nYOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39%\nmean Average Precision (mAP). The speed of processing was 20 frame per second.\nThe system was able to detect potholes from a range on 100 meters away from the\ncamera. The system can increase the safety of drivers and improve the\nperformance of self-driving cars by detecting pothole time ahead.",
          "link": "http://arxiv.org/abs/2107.06356",
          "publishedOn": "2021-07-15T01:59:03.628Z",
          "wordCount": 606,
          "title": "Real-Time Pothole Detection Using Deep Learning. (arXiv:2107.06356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosel_F/0/1/0/all/0/1\">Fabian R&#xf6;sel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahrenkrog_Petersen_S/0/1/0/all/0/1\">Stephan A. Fahrenkrog-Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1\">Han van der Aa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1\">Matthias Weidlich</a>",
          "description": "To enable process analysis based on an event log without compromising the\nprivacy of individuals involved in process execution, a log may be anonymized.\nSuch anonymization strives to transform a log so that it satisfies provable\nprivacy guarantees, while largely maintaining its utility for process analysis.\nExisting techniques perform anonymization using simple, syntactic measures to\nidentify suitable transformation operations. This way, the semantics of the\nactivities referenced by the events in a trace are neglected, potentially\nleading to transformations in which events of unrelated activities are merged.\nTo avoid this and incorporate the semantics of activities during anonymization,\nwe propose to instead incorporate a distance measure based on feature learning.\nSpecifically, we show how embeddings of events enable the definition of a\ndistance measure for traces to guide event log anonymization. Our experiments\nwith real-world data indicate that anonymization using this measure, compared\nto a syntactic one, yields logs that are closer to the original log in various\ndimensions and, hence, have higher utility for process analysis.",
          "link": "http://arxiv.org/abs/2107.06578",
          "publishedOn": "2021-07-15T01:59:03.611Z",
          "wordCount": 629,
          "title": "A Distance Measure for Privacy-preserving Process Mining based on Feature Learning. (arXiv:2107.06578v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alishahia_A/0/1/0/all/0/1\">Afra Alishahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1\">Alejandrina Cristia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1\">Bertrand Higy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1\">Marvin Lavechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chen Yu</a>",
          "description": "We present the visually-grounded language modelling track that was introduced\nin the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the\nnew track and discuss participation rules in detail. We also present the two\nbaseline systems that were developed for this track.",
          "link": "http://arxiv.org/abs/2107.06546",
          "publishedOn": "2021-07-15T01:59:03.596Z",
          "wordCount": 501,
          "title": "ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition. (arXiv:2107.06546v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nystrom_A/0/1/0/all/0/1\">Andrew Nystrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1\">Douglas Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
          "link": "http://arxiv.org/abs/2107.06499",
          "publishedOn": "2021-07-15T01:59:03.560Z",
          "wordCount": 571,
          "title": "Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06428",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Trippe_B/0/1/0/all/0/1\">Brian L. Trippe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Finucane_H/0/1/0/all/0/1\">Hilary K. Finucane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1\">Tamara Broderick</a>",
          "description": "Hierarchical Bayesian methods enable information sharing across multiple\nrelated regression problems. While standard practice is to model regression\nparameters (effects) as (1) exchangeable across datasets and (2) correlated to\ndiffering degrees across covariates, we show that this approach exhibits poor\nstatistical performance when the number of covariates exceeds the number of\ndatasets. For instance, in statistical genetics, we might regress dozens of\ntraits (defining datasets) for thousands of individuals (responses) on up to\nmillions of genetic variants (covariates). When an analyst has more covariates\nthan datasets, we argue that it is often more natural to instead model effects\nas (1) exchangeable across covariates and (2) correlated to differing degrees\nacross datasets. To this end, we propose a hierarchical model expressing our\nalternative perspective. We devise an empirical Bayes estimator for learning\nthe degree of correlation between datasets. We develop theory that demonstrates\nthat our method outperforms the classic approach when the number of covariates\ndominates the number of datasets, and corroborate this result empirically on\nseveral high-dimensional multiple regression and classification problems.",
          "link": "http://arxiv.org/abs/2107.06428",
          "publishedOn": "2021-07-15T01:59:03.550Z",
          "wordCount": 625,
          "title": "For high-dimensional hierarchical models, consider exchangeability of effects across covariates instead of across datasets. (arXiv:2107.06428v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwala_S/0/1/0/all/0/1\">Susama Agarwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dees_B/0/1/0/all/0/1\">Benjamin Dees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gearhart_A/0/1/0/all/0/1\">Andrew Gearhart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowman_C/0/1/0/all/0/1\">Corey Lowman</a>",
          "description": "We study the deformation of the input space by a trained autoencoder via the\nJacobians of the trained weight matrices. In doing so, we prove bounds for the\nmean squared errors for points in the input space, under assumptions regarding\nthe orthogonality of the eigenvectors. We also show that the trace and the\nproduct of the eigenvalues of the Jacobian matrices is a good predictor of the\nMSE on test points. This is a dataset independent means of testing an\nautoencoder's ability to generalize on new input. Namely, no knowledge of the\ndataset on which the network was trained is needed, only the parameters of the\ntrained model.",
          "link": "http://arxiv.org/abs/2107.06386",
          "publishedOn": "2021-07-15T01:59:03.539Z",
          "wordCount": 558,
          "title": "Geometry and Generalization: Eigenvalues as predictors of where a network will fail to generalize. (arXiv:2107.06386v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1\">Vanessa D&#x27;Amario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Datasets often contain input dimensions that are unnecessary to predict the\noutput label, e.g. background in object recognition, which lead to more\ntrainable parameters. Deep Neural Networks (DNNs) are robust to increasing the\nnumber of parameters in the hidden layers, but it is unclear whether this holds\ntrue for the input layer. In this letter, we investigate the impact of\nunnecessary input dimensions on a central issue of DNNs: their data efficiency,\nie. the amount of examples needed to achieve certain generalization\nperformance. Our results show that unnecessary input dimensions that are\ntask-unrelated substantially degrade data efficiency. This highlights the need\nfor mechanisms that remove {task-unrelated} dimensions to enable data\nefficiency gains.",
          "link": "http://arxiv.org/abs/2107.06409",
          "publishedOn": "2021-07-15T01:59:03.528Z",
          "wordCount": 554,
          "title": "The Foes of Neural Network's Data Efficiency Among Unnecessary Input Dimensions. (arXiv:2107.06409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grbic_D/0/1/0/all/0/1\">Djordje Grbic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1\">Sebastian Risi</a>",
          "description": "Random exploration is one of the main mechanisms through which reinforcement\nlearning (RL) finds well-performing policies. However, it can lead to\nundesirable or catastrophic outcomes when learning online in safety-critical\nenvironments. In fact, safe learning is one of the major obstacles towards\nreal-world agents that can learn during deployment. One way of ensuring that\nagents respect hard limitations is to explicitly configure boundaries in which\nthey can operate. While this might work in some cases, we do not always have\nclear a-priori information which states and actions can lead dangerously close\nto hazardous states. Here, we present an approach where an additional policy\ncan override the main policy and offer a safer alternative action. In our\ninstinct-regulated RL (IR^2L) approach, an \"instinctual\" network is trained to\nrecognize undesirable situations, while guarding the learning policy against\nentering them. The instinct network is pre-trained on a single task where it is\nsafe to make mistakes, and transferred to environments in which learning a new\ntask safely is critical. We demonstrate IR^2L in the OpenAI Safety gym domain,\nin which it receives a significantly lower number of safety violations during\ntraining than a baseline RL approach while reaching similar task performance.",
          "link": "http://arxiv.org/abs/2107.06686",
          "publishedOn": "2021-07-15T01:59:03.517Z",
          "wordCount": 644,
          "title": "Safer Reinforcement Learning through Transferable Instinct Networks. (arXiv:2107.06686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krotov_D/0/1/0/all/0/1\">Dmitry Krotov</a>",
          "description": "Dense Associative Memories or Modern Hopfield Networks have many appealing\nproperties of associative memory. They can do pattern completion, store a large\nnumber of memories, and can be described using a recurrent neural network with\na degree of biological plausibility and rich feedback between the neurons. At\nthe same time, up until now all the models of this class have had only one\nhidden layer, and have only been formulated with densely connected network\narchitectures, two aspects that hinder their machine learning applications.\nThis paper tackles this gap and describes a fully recurrent model of\nassociative memory with an arbitrary large number of layers, some of which can\nbe locally connected (convolutional), and a corresponding energy function that\ndecreases on the dynamical trajectory of the neurons' activations. The memories\nof the full network are dynamically \"assembled\" using primitives encoded in the\nsynaptic weights of the lower layers, with the \"assembling rules\" encoded in\nthe synaptic weights of the higher layers. In addition to the bottom-up\npropagation of information, typical of commonly used feedforward neural\nnetworks, the model described has rich top-down feedback from higher layers\nthat help the lower-layer neurons to decide on their response to the input\nstimuli.",
          "link": "http://arxiv.org/abs/2107.06446",
          "publishedOn": "2021-07-15T01:59:03.494Z",
          "wordCount": 621,
          "title": "Hierarchical Associative Memory. (arXiv:2107.06446v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavakoli_M/0/1/0/all/0/1\">Mohammadamin Tavakoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1\">Pierre Baldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadowski_P/0/1/0/all/0/1\">Peter Sadowski</a>",
          "description": "In a physical neural system, backpropagation is faced with a number of\nobstacles including: the need for labeled data, the violation of the locality\nlearning principle, the need for symmetric connections, and the lack of\nmodularity. Tourbillon is a new architecture that addresses all these\nlimitations. At its core, it consists of a stack of circular autoencoders\nfollowed by an output layer. The circular autoencoders are trained in\nself-supervised mode by recirculation algorithms and the top layer in\nsupervised mode by stochastic gradient descent, with the option of propagating\nerror information through the entire stack using non-symmetric connections.\nWhile the Tourbillon architecture is meant primarily to address physical\nconstraints, and not to improve current engineering applications of deep\nlearning, we demonstrate its viability on standard benchmark datasets including\nMNIST, Fashion MNIST, and CIFAR10. We show that Tourbillon can achieve\ncomparable performance to models trained with backpropagation and outperform\nmodels that are trained with other physically plausible algorithms, such as\nfeedback alignment.",
          "link": "http://arxiv.org/abs/2107.06424",
          "publishedOn": "2021-07-15T01:59:03.480Z",
          "wordCount": 588,
          "title": "Tourbillon: a Physically Plausible Neural Architecture. (arXiv:2107.06424v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06767",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Racz_M/0/1/0/all/0/1\">Miklos Z. Racz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sridhar_A/0/1/0/all/0/1\">Anirudh Sridhar</a>",
          "description": "We consider the task of learning latent community structure from multiple\ncorrelated networks. First, we study the problem of learning the latent vertex\ncorrespondence between two edge-correlated stochastic block models, focusing on\nthe regime where the average degree is logarithmic in the number of vertices.\nWe derive the precise information-theoretic threshold for exact recovery: above\nthe threshold there exists an estimator that outputs the true correspondence\nwith probability close to 1, while below it no estimator can recover the true\ncorrespondence with probability bounded away from 0. As an application of our\nresults, we show how one can exactly recover the latent communities using\nmultiple correlated graphs in parameter regimes where it is\ninformation-theoretically impossible to do so using just a single graph.",
          "link": "http://arxiv.org/abs/2107.06767",
          "publishedOn": "2021-07-15T01:59:03.461Z",
          "wordCount": 584,
          "title": "Correlated Stochastic Block Models: Exact Graph Matching with Applications to Recovering Communities. (arXiv:2107.06767v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06534",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zeeshan Akhtar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rajawat_K/0/1/0/all/0/1\">Ketan Rajawat</a>",
          "description": "This paper considers stochastic convex optimization problems with two sets of\nconstraints: (a) deterministic constraints on the domain of the optimization\nvariable, which are difficult to project onto; and (b) deterministic or\nstochastic constraints that admit efficient projection. Problems of this form\narise frequently in the context of semidefinite programming as well as when\nvarious NP-hard problems are solved approximately via semidefinite relaxation.\nSince projection onto the first set of constraints is difficult, it becomes\nnecessary to explore projection-free algorithms, such as the stochastic\nFrank-Wolfe (FW) algorithm. On the other hand, the second set of constraints\ncannot be handled in the same way, and must be incorporated as an indicator\nfunction within the objective function, thereby complicating the application of\nFW methods. Similar problems have been studied before, and solved using\nfirst-order stochastic FW algorithms by applying homotopy and Nesterov's\nsmoothing techniques to the indicator function. This work improves upon these\nexisting results and puts forth momentum-based first-order methods that yield\nimproved convergence rates, at par with the best known rates for problems\nwithout the second set of constraints. Zeroth-order variants of the proposed\nalgorithms are also developed and again improve upon the state-of-the-art rate\nresults. The efficacy of the proposed algorithms is tested on relevant\napplications of sparse matrix estimation, clustering via semidefinite\nrelaxation, and uniform sparsest cut problem.",
          "link": "http://arxiv.org/abs/2107.06534",
          "publishedOn": "2021-07-15T01:59:03.453Z",
          "wordCount": 666,
          "title": "Zeroth and First Order Stochastic Frank-Wolfe Algorithms for Constrained Optimization. (arXiv:2107.06534v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thordsen_E/0/1/0/all/0/1\">Erik Thordsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1\">Erich Schubert</a>",
          "description": "Many approaches in the field of machine learning and data analysis rely on\nthe assumption that the observed data lies on lower-dimensional manifolds. This\nassumption has been verified empirically for many real data sets. To make use\nof this manifold assumption one generally requires the manifold to be locally\nsampled to a certain density such that features of the manifold can be\nobserved. However, for increasing intrinsic dimensionality of a data set the\nrequired data density introduces the need for very large data sets, resulting\nin one of the many faces of the curse of dimensionality. To combat the\nincreased requirement for local data density we propose a framework to generate\nvirtual data points that faithful to an approximate embedding function\nunderlying the manifold observable in the data.",
          "link": "http://arxiv.org/abs/2107.06566",
          "publishedOn": "2021-07-15T01:59:03.441Z",
          "wordCount": 559,
          "title": "MESS: Manifold Embedding Motivated Super Sampling. (arXiv:2107.06566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1\">Alihan H&#xfc;y&#xfc;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrett_D/0/1/0/all/0/1\">Daniel Jarrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "Understanding an agent's priorities by observing their behavior is critical\nfor transparency and accountability in decision processes, such as in\nhealthcare. While conventional approaches to policy learning almost invariably\nassume stationarity in behavior, this is hardly true in practice: Medical\npractice is constantly evolving, and clinical professionals are constantly\nfine-tuning their priorities. We desire an approach to policy learning that\nprovides (1) interpretable representations of decision-making, accounts for (2)\nnon-stationarity in behavior, as well as operating in an (3) offline manner.\nFirst, we model the behavior of learning agents in terms of contextual bandits,\nand formalize the problem of inverse contextual bandits (ICB). Second, we\npropose two algorithms to tackle ICB, each making varying degrees of\nassumptions regarding the agent's learning strategy. Finally, through both real\nand simulated data for liver transplantations, we illustrate the applicability\nand explainability of our method, as well as validating its accuracy.",
          "link": "http://arxiv.org/abs/2107.06317",
          "publishedOn": "2021-07-15T01:59:03.424Z",
          "wordCount": 583,
          "title": "Inverse Contextual Bandits: Learning How Behavior Evolves over Time. (arXiv:2107.06317v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suneung-Kim/0/1/0/all/0/1\">Suneung-Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Due to the recent outbreak of COVID-19, many classes, exams, and meetings\nhave been conducted non-face-to-face. However, the foundation for video\nconferencing solutions is still insufficient. So this technology has become an\nimportant issue. In particular, these technologies are essential for\nnon-face-to-face testing, and technology dissemination is urgent. In this\npaper, we present a single video conferencing solution using gaze estimation in\npreparation for these problems. Gaze is an important cue for the tasks such as\nanalysis of human behavior. Hence, numerous studies have been proposed to solve\ngaze estimation using deep learning, which is one of the most prominent methods\nup to date. We use these gaze estimation methods to detect abnormal behavior of\nvideo conferencing participants. Our contribution is as follows. i) We find and\napply the optimal network for the gaze estimation method and apply a\nself-supervised method to improve accuracy. ii) For anomaly detection, we\npresent a new dataset that aggregates the values of a new gaze, head pose, etc.\niii) We train newly created data on Multi Layer Perceptron (MLP) models to\ndetect anomaly behavior based on deep learning. We demonstrate the robustness\nof our method through experiments.",
          "link": "http://arxiv.org/abs/2107.06530",
          "publishedOn": "2021-07-15T01:59:03.417Z",
          "wordCount": 685,
          "title": "Detection of Abnormal Behavior with Self-Supervised Gaze Estimation. (arXiv:2107.06530v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_A/0/1/0/all/0/1\">Allen Z. Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Anirudha Majumdar</a>",
          "description": "Our goal is to train control policies that generalize well to unseen\nenvironments. Inspired by the Distributionally Robust Optimization (DRO)\nframework, we propose DRAGEN - Distributionally Robust policy learning via\nAdversarial Generation of ENvironments - for iteratively improving robustness\nof policies to realistic distribution shifts by generating adversarial\nenvironments. The key idea is to learn a generative model for environments\nwhose latent variables capture cost-predictive and realistic variations in\nenvironments. We perform DRO with respect to a Wasserstein ball around the\nempirical distribution of environments by generating realistic adversarial\nenvironments via gradient ascent on the latent space. We demonstrate strong\nOut-of-Distribution (OoD) generalization in simulation for (i) swinging up a\npendulum with onboard vision and (ii) grasping realistic 2D/3D objects.\nGrasping experiments on hardware demonstrate better sim2real performance\ncompared to domain randomization.",
          "link": "http://arxiv.org/abs/2107.06353",
          "publishedOn": "2021-07-15T01:59:03.411Z",
          "wordCount": 561,
          "title": "Distributionally Robust Policy Learning via Adversarial Environment Generation. (arXiv:2107.06353v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>",
          "description": "Understanding the behavior and vulnerability of pre-trained deep neural\nnetworks (DNNs) can help to improve them. Analysis can be performed via\nreversing the network's flow to generate inputs from internal representations.\nMost existing work relies on priors or data-intensive optimization to invert a\nmodel, yet struggles to scale to deep architectures and complex datasets. This\npaper presents a zero-shot direct model inversion framework that recovers the\ninput to the trained model given only the internal representation. The crux of\nour method is to inverse the DNN in a divide-and-conquer manner while\nre-syncing the inverted layers via cycle-consistency guidance with the help of\nsynthesized data. As a result, we obtain a single feed-forward model capable of\ninversion with a single forward pass without seeing any real data of the\noriginal task. With the proposed approach, we scale zero-shot direct inversion\nto deep architectures and complex datasets. We empirically show that modern\nclassification models on ImageNet can, surprisingly, be inverted, allowing an\napproximate recovery of the original 224x224px images from a representation\nafter more than 20 layers. Moreover, inversion of generators in GANs unveils\nlatent code of a given synthesized face image at 128x128px, which can even, in\nturn, improve defective synthesized images from GANs.",
          "link": "http://arxiv.org/abs/2107.06304",
          "publishedOn": "2021-07-15T01:59:03.404Z",
          "wordCount": 673,
          "title": "Deep Neural Networks are Surprisingly Reversible: A Baseline for Zero-Shot Inversion. (arXiv:2107.06304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1\">Alaa Awad Abdellatif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mhaisen_N/0/1/0/all/0/1\">Naram Mhaisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1\">Mohsen Guizani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawy_Z/0/1/0/all/0/1\">Zaher Dawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasreddine_W/0/1/0/all/0/1\">Wassim Nasreddine</a>",
          "description": "Federated learning (FL) is a distributed learning methodology that allows\nmultiple nodes to cooperatively train a deep learning model, without the need\nto share their local data. It is a promising solution for telemonitoring\nsystems that demand intensive data collection, for detection, classification,\nand prediction of future events, from different locations while maintaining a\nstrict privacy constraint. Due to privacy concerns and critical communication\nbottlenecks, it can become impractical to send the FL updated models to a\ncentralized server. Thus, this paper studies the potential of hierarchical FL\nin IoT heterogeneous systems and propose an optimized solution for user\nassignment and resource allocation on multiple edge nodes. In particular, this\nwork focuses on a generic class of machine learning models that are trained\nusing gradient-descent-based schemes while considering the practical\nconstraints of non-uniformly distributed data across different users. We\nevaluate the proposed system using two real-world datasets, and we show that it\noutperforms state-of-the-art FL solutions. In particular, our numerical results\nhighlight the effectiveness of our approach and its ability to provide 4-6%\nincrease in the classification accuracy, with respect to hierarchical FL\nschemes that consider distance-based user assignment. Furthermore, the proposed\napproach could significantly accelerate FL training and reduce communication\noverhead by providing 75-85% reduction in the communication rounds between edge\nnodes and the centralized server, for the same model accuracy.",
          "link": "http://arxiv.org/abs/2107.06548",
          "publishedOn": "2021-07-15T01:59:03.386Z",
          "wordCount": 698,
          "title": "Communication-Efficient Hierarchical Federated Learning for IoT Heterogeneous Systems with Imbalanced Data. (arXiv:2107.06548v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1\">Tuomo Lahtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1\">Hannu Turtiainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1\">Andrei Costin</a>",
          "description": "Image annotation and large annotated datasets are crucial parts within the\nComputer Vision and Artificial Intelligence fields.At the same time, it is\nwell-known and acknowledged by the research community that the image annotation\nprocess is challenging, time-consuming and hard to scale. Therefore, the\nresearchers and practitioners are always seeking ways to perform the\nannotations easier, faster, and at higher quality. Even though several widely\nused tools exist and the tools' landscape evolved considerably, most of the\ntools still require intricate technical setups and high levels of technical\nsavviness from its operators and crowdsource contributors.\n\nIn order to address such challenges, we develop and present BRIMA -- a\nflexible and open-source browser extension that allows BRowser-only IMage\nAnnotation at considerably lower overheads. Once added to the browser, it\ninstantly allows the user to annotate images easily and efficiently directly\nfrom the browser without any installation or setup on the client-side. It also\nfeatures cross-browser and cross-platform functionality thus presenting itself\nas a neat tool for researchers within the Computer Vision, Artificial\nIntelligence, and privacy-related fields.",
          "link": "http://arxiv.org/abs/2107.06351",
          "publishedOn": "2021-07-15T01:59:03.380Z",
          "wordCount": 612,
          "title": "BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint). (arXiv:2107.06351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozkan_M/0/1/0/all/0/1\">Mehmet Fatih Ozkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yao Ma</a>",
          "description": "Drivers have unique and rich driving behaviors when operating vehicles in\ntraffic. This paper presents a novel driver behavior learning approach that\ncaptures the uniqueness and richness of human driver behavior in realistic\ndriving scenarios. A stochastic inverse reinforcement learning (SIRL) approach\nis proposed to learn a distribution of cost function, which represents the\nrichness of the human driver behavior with a given set of driver-specific\ndemonstrations. Evaluations are conducted on the realistic driving data\ncollected from the 3D driver-in-the-loop driving simulation. The results show\nthat the learned stochastic driver model is capable of expressing the richness\nof the human driving strategies under different realistic driving scenarios.\nCompared to the deterministic baseline driver model, the results reveal that\nthe proposed stochastic driver behavior model can better replicate the driver's\nunique and rich driving strategies in a variety of traffic conditions.",
          "link": "http://arxiv.org/abs/2107.06344",
          "publishedOn": "2021-07-15T01:59:03.329Z",
          "wordCount": 583,
          "title": "Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning. (arXiv:2107.06344v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1\">Julian Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhtarian_I/0/1/0/all/0/1\">Ilia Mokhtarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1\">Houshang Darabi</a>",
          "description": "Process mining algorithms discover a process model from an event log. The\nresulting process model is supposed to describe all possible event sequences of\nthe underlying system. Generalization is a process model quality dimension of\ninterest. A generalization metric should quantify the extent to which a process\nmodel represents the observed event sequences contained in the event log and\nthe unobserved event sequences of the system. Most of the available metrics in\nthe literature cannot properly quantify the generalization of a process model.\nA recently published method [1] called Adversarial System Variant Approximation\nleverages Generative Adversarial Networks to approximate the underlying event\nsequence distribution of a system from an event log. While this method\ndemonstrated performance gains over existing methods in measuring the\ngeneralization of process models, its experimental evaluations have been\nperformed under ideal conditions. This paper experimentally investigates the\nperformance of Adversarial System Variant Approximation under non-ideal\nconditions such as biased and limited event logs. Moreover, experiments are\nperformed to investigate the originally proposed sampling hyperparameter value\nof the method on its performance to measure the generalization. The results\nconfirm the need to raise awareness about the working conditions of the\nAdversarial System Variant Approximation method. The outcomes of this paper\nalso serve to initiate future research directions.\n\n[1] Theis, Julian, and Houshang Darabi. \"Adversarial System Variant\nApproximation to Quantify Process Model Generalization.\" IEEE Access 8 (2020):\n194410-194427.",
          "link": "http://arxiv.org/abs/2107.06319",
          "publishedOn": "2021-07-15T01:59:03.321Z",
          "wordCount": 678,
          "title": "On the Performance Analysis of the Adversarial System Variant Approximation Method to Quantify Process Model Generalization. (arXiv:2107.06319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06281",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Mhiri_I/0/1/0/all/0/1\">Islem Mhiri</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nebli_A/0/1/0/all/0/1\">Ahmed Nebli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mahjoub_M/0/1/0/all/0/1\">Mohamed Ali Mahjoub</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>",
          "description": "Brain graph synthesis marked a new era for predicting a target brain graph\nfrom a source one without incurring the high acquisition cost and processing\ntime of neuroimaging data. However, existing multi-modal graph synthesis\nframeworks have several limitations. First, they mainly focus on generating\ngraphs from the same domain (intra-modality), overlooking the rich multimodal\nrepresentations of brain connectivity (inter-modality). Second, they can only\nhandle isomorphic graph generation tasks, limiting their generalizability to\nsynthesizing target graphs with a different node size and topological structure\nfrom those of the source one. More importantly, both target and source domains\nmight have different distributions, which causes a domain fracture between them\n(i.e., distribution misalignment). To address such challenges, we propose an\ninter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to\ninfer a target graph modality based on a given modality. Our three core\ncontributions lie in (i) predicting a target graph (e.g., functional) from a\nsource graph (e.g., morphological) based on a novel graph generative\nadversarial network (gGAN); (ii) using non-isomorphic graphs for both source\nand target domains with a different number of nodes, edges and structure; and\n(iii) enforcing the predicted target distribution to match that of the ground\ntruth graphs using a graph autoencoder to relax the designed loss oprimization.\nTo handle the unstable behavior of gGAN, we design a new Ground\nTruth-Preserving (GT-P) loss function to guide the generator in learning the\ntopological structure of ground truth brain graphs. Our comprehensive\nexperiments on predicting functional from morphological graphs demonstrate the\noutperformance of IMANGraphNet in comparison with its variants. This can be\nfurther leveraged for integrative and holistic brain mapping in health and\ndisease.",
          "link": "http://arxiv.org/abs/2107.06281",
          "publishedOn": "2021-07-15T01:59:03.285Z",
          "wordCount": 719,
          "title": "Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic Brain Mapping. (arXiv:2107.06281v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kammerer_L/0/1/0/all/0/1\">Lukas Kammerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1\">Michael Kommenda</a>",
          "description": "We describe a method for the identification of models for dynamical systems\nfrom observational data. The method is based on the concept of symbolic\nregression and uses genetic programming to evolve a system of ordinary\ndifferential equations (ODE). The novelty is that we add a step of\ngradient-based optimization of the ODE parameters. For this we calculate the\nsensitivities of the solution to the initial value problem (IVP) using\nautomatic differentiation. The proposed approach is tested on a set of 19\nproblem instances taken from the literature which includes datasets from\nsimulated systems as well as datasets captured from mechanical systems. We find\nthat gradient-based optimization of parameters improves predictive accuracy of\nthe models. The best results are obtained when we first fit the individual\nequations to the numeric differences and then subsequently fine-tune the\nidentified parameter values by fitting the IVP solution to the observed\nvariable values.",
          "link": "http://arxiv.org/abs/2107.06131",
          "publishedOn": "2021-07-14T01:41:52.131Z",
          "wordCount": 612,
          "title": "Identification of Dynamical Systems using Symbolic Regression. (arXiv:2107.06131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1\">Tolulope A. Odetola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Syed Rafay Hasan</a>",
          "description": "Security of inference phase deployment of Convolutional neural network (CNN)\ninto resource constrained embedded systems (e.g. low end FPGAs) is a growing\nresearch area. Using secure practices, third party FPGA designers can be\nprovided with no knowledge of initial and final classification layers. In this\nwork, we demonstrate that hardware intrinsic attack (HIA) in such a \"secure\"\ndesign is still possible. Proposed HIA is inserted inside mathematical\noperations of individual layers of CNN, which propagates erroneous operations\nin all the subsequent CNN layers that lead to misclassification. The attack is\nnon-periodic and completely random, hence it becomes difficult to detect. Five\ndifferent attack scenarios with respect to each CNN layer are designed and\nevaluated based on the overhead resources and the rate of triggering in\ncomparison to the original implementation. Our results for two CNN\narchitectures show that in all the attack scenarios, additional latency is\nnegligible (<0.61%), increment in DSP, LUT, FF is also less than 2.36%. Three\nattack scenarios do not require any additional BRAM resources, while in two\nscenarios BRAM increases, which compensates with the corresponding decrease in\nFF and LUTs. To the authors' best knowledge this work is the first to address\nthe hardware intrinsic CNN attack with the attacker does not have knowledge of\nthe full CNN.",
          "link": "http://arxiv.org/abs/2103.09327",
          "publishedOn": "2021-07-14T01:41:52.125Z",
          "wordCount": 702,
          "title": "SoWaF: Shuffling of Weights and Feature Maps: A Novel Hardware Intrinsic Attack (HIA) on Convolutional Neural Network (CNN). (arXiv:2103.09327v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09907",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liao_L/0/1/0/all/0/1\">Luofeng Liao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fu_Z/0/1/0/all/0/1\">Zuyue Fu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1\">Mladen Kolar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>",
          "description": "In offline reinforcement learning (RL) an optimal policy is learnt solely\nfrom a priori collected observational data. However, in observational data,\nactions are often confounded by unobserved variables. Instrumental variables\n(IVs), in the context of RL, are the variables whose influence on the state\nvariables are all mediated through the action. When a valid instrument is\npresent, we can recover the confounded transition dynamics through\nobservational data. We study a confounded Markov decision process where the\ntransition dynamics admit an additive nonlinear functional form. Using IVs, we\nderive a conditional moment restriction (CMR) through which we can identify\ntransition dynamics based on observational data. We propose a provably\nefficient IV-aided Value Iteration (IVVI) algorithm based on a primal-dual\nreformulation of CMR. To the best of our knowledge, this is the first provably\nefficient algorithm for instrument-aided offline RL.",
          "link": "http://arxiv.org/abs/2102.09907",
          "publishedOn": "2021-07-14T01:41:52.107Z",
          "wordCount": 598,
          "title": "Instrumental Variable Value Iteration for Causal Offline Reinforcement Learning. (arXiv:2102.09907v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.00773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>",
          "description": "This paper analyzes challenges in cloze-style reading comprehension on\nmultiparty dialogue and suggests two new tasks for more comprehensive\npredictions of personal entities in daily conversations. We first demonstrate\nthat there are substantial limitations to the evaluation methods of previous\nwork, namely that randomized assignment of samples to training and test data\nsubstantially decreases the complexity of cloze-style reading comprehension.\nAccording to our analysis, replacing the random data split with a chronological\ndata split reduces test accuracy on previous single-variable passage completion\ntask from 72\\% to 34\\%, that leaves much more room to improve. Our proposed\ntasks extend the previous single-variable passage completion task by replacing\nmore character mentions with variables. Several deep learning models are\ndeveloped to validate these three tasks. A thorough error analysis is provided\nto understand the challenges and guide the future direction of this research.",
          "link": "http://arxiv.org/abs/1911.00773",
          "publishedOn": "2021-07-14T01:41:52.100Z",
          "wordCount": 611,
          "title": "Design and Challenges of Cloze-Style Reading Comprehension Tasks on Multiparty Dialogue. (arXiv:1911.00773v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1\">Mathieu Blondel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berthet_Q/0/1/0/all/0/1\">Quentin Berthet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frostig_R/0/1/0/all/0/1\">Roy Frostig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyer_S/0/1/0/all/0/1\">Stephan Hoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llinares_Lopez_F/0/1/0/all/0/1\">Felipe Llinares-L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1\">Fabian Pedregosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vert_J/0/1/0/all/0/1\">Jean-Philippe Vert</a>",
          "description": "Automatic differentiation (autodiff) has revolutionized machine learning. It\nallows expressing complex computations by composing elementary ones in creative\nways and removes the burden of computing their derivatives by hand. More\nrecently, differentiation of optimization problem solutions has attracted\nwidespread attention with applications such as optimization as a layer, and in\nbi-level problems such as hyper-parameter optimization and meta-learning.\nHowever, the formulas for these derivatives often involve case-by-case tedious\nmathematical derivations. In this paper, we propose a unified, efficient and\nmodular approach for implicit differentiation of optimization problems. In our\napproach, the user defines (in Python in the case of our implementation) a\nfunction $F$ capturing the optimality conditions of the problem to be\ndifferentiated. Once this is done, we leverage autodiff of $F$ and implicit\ndifferentiation to automatically differentiate the optimization problem. Our\napproach thus combines the benefits of implicit differentiation and autodiff.\nIt is efficient as it can be added on top of any state-of-the-art solver and\nmodular as the optimality condition specification is decoupled from the\nimplicit differentiation mechanism. We show that seemingly simple principles\nallow to recover many recently proposed implicit differentiation methods and\ncreate new ones easily. We demonstrate the ease of formulating and solving\nbi-level optimization problems using our framework. We also showcase an\napplication to the sensitivity analysis of molecular dynamics.",
          "link": "http://arxiv.org/abs/2105.15183",
          "publishedOn": "2021-07-14T01:41:52.043Z",
          "wordCount": 699,
          "title": "Efficient and Modular Implicit Differentiation. (arXiv:2105.15183v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00138",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grodecki_K/0/1/0/all/0/1\">Kajetan Grodecki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Killekar_A/0/1/0/all/0/1\">Aditya Killekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1\">Andrew Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cadet_S/0/1/0/all/0/1\">Sebastien Cadet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McElhinney_P/0/1/0/all/0/1\">Priscilla McElhinney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razipour_A/0/1/0/all/0/1\">Aryabod Razipour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Cato Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pressman_B/0/1/0/all/0/1\">Barry D. Pressman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Julien_P/0/1/0/all/0/1\">Peter Julien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simon_J/0/1/0/all/0/1\">Judit Simon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maurovich_Horvat_P/0/1/0/all/0/1\">Pal Maurovich-Horvat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaibazzi_N/0/1/0/all/0/1\">Nicola Gaibazzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thakur_U/0/1/0/all/0/1\">Udit Thakur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mancini_E/0/1/0/all/0/1\">Elisabetta Mancini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agalbato_C/0/1/0/all/0/1\">Cecilia Agalbato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munechika_J/0/1/0/all/0/1\">Jiro Munechika</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matsumoto_H/0/1/0/all/0/1\">Hidenari Matsumoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mene_R/0/1/0/all/0/1\">Roberto Men&#xe8;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parati_G/0/1/0/all/0/1\">Gianfranco Parati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cernigliaro_F/0/1/0/all/0/1\">Franco Cernigliaro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nerlekar_N/0/1/0/all/0/1\">Nitesh Nerlekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torlasco_C/0/1/0/all/0/1\">Camilla Torlasco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pontone_G/0/1/0/all/0/1\">Gianluca Pontone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_D/0/1/0/all/0/1\">Damini Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slomka_P/0/1/0/all/0/1\">Piotr J. Slomka</a>",
          "description": "Quantitative lung measures derived from computed tomography (CT) have been\ndemonstrated to improve prognostication in coronavirus disease (COVID-19)\npatients, but are not part of the clinical routine since required manual\nsegmentation of lung lesions is prohibitively time-consuming. We propose a new\nfully automated deep learning framework for rapid quantification and\ndifferentiation between lung lesions in COVID-19 pneumonia from both contrast\nand non-contrast CT images using convolutional Long Short-Term Memory\n(ConvLSTM) networks. Utilizing the expert annotations, model training was\nperformed 5 times with separate hold-out sets using 5-fold cross-validation to\nsegment ground-glass opacity and high opacity (including consolidation and\npleural effusion). The performance of the method was evaluated on CT data sets\nfrom 197 patients with positive reverse transcription polymerase chain reaction\ntest result for SARS-CoV-2. Strong agreement between expert manual and\nautomatic segmentation was obtained for lung lesions with a Dice score\ncoefficient of 0.876 $\\pm$ 0.005; excellent correlations of 0.978 and 0.981 for\nground-glass opacity and high opacity volumes. In the external validation set\nof 67 patients, there was dice score coefficient of 0.767 $\\pm$ 0.009 as well\nas excellent correlations of 0.989 and 0.996 for ground-glass opacity and high\nopacity volumes. Computations for a CT scan comprising 120 slices were\nperformed under 2 seconds on a personal computer equipped with NVIDIA Titan RTX\ngraphics processing unit. Therefore, our deep learning-based method allows\nrapid fully-automated quantitative measurement of pneumonia burden from CT and\nmay generate results with an accuracy similar to the expert readers.",
          "link": "http://arxiv.org/abs/2104.00138",
          "publishedOn": "2021-07-14T01:41:52.030Z",
          "wordCount": 824,
          "title": "Rapid quantification of COVID-19 pneumonia burden from computed tomography with convolutional LSTM networks. (arXiv:2104.00138v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yunwen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenhuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1\">Yiming Ying</a>",
          "description": "Many machine learning problems can be formulated as minimax problems such as\nGenerative Adversarial Networks (GANs), AUC maximization and robust estimation,\nto mention but a few. A substantial amount of studies are devoted to studying\nthe convergence behavior of their stochastic gradient-type algorithms. In\ncontrast, there is relatively little work on their generalization, i.e., how\nthe learning models built from training examples would behave on test examples.\nIn this paper, we provide a comprehensive generalization analysis of stochastic\ngradient methods for minimax problems under both convex-concave and\nnonconvex-nonconcave cases through the lens of algorithmic stability. We\nestablish a quantitative connection between stability and several\ngeneralization measures both in expectation and with high probability. For the\nconvex-concave setting, our stability analysis shows that stochastic gradient\ndescent ascent attains optimal generalization bounds for both smooth and\nnonsmooth minimax problems. We also establish generalization bounds for both\nweakly-convex-weakly-concave and gradient-dominated problems.",
          "link": "http://arxiv.org/abs/2105.03793",
          "publishedOn": "2021-07-14T01:41:52.022Z",
          "wordCount": 623,
          "title": "Stability and Generalization of Stochastic Gradient Methods for Minimax Problems. (arXiv:2105.03793v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1\">Maxim Rakhuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We introduce T-Basis, a novel concept for a compact representation of a set\nof tensors, each of an arbitrary shape, which is often seen in Neural Networks.\nEach of the tensors in the set is modeled using Tensor Rings, though the\nconcept applies to other Tensor Networks. Owing its name to the T-shape of\nnodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally\nshaped three-dimensional tensors, used to represent Tensor Ring nodes. Such\nrepresentation allows us to parameterize the tensor set with a small number of\nparameters (coefficients of the T-Basis tensors), scaling logarithmically with\neach tensor's size in the set and linearly with the dimensionality of T-Basis.\nWe evaluate the proposed approach on the task of neural network compression and\ndemonstrate that it reaches high compression rates at acceptable performance\ndrops. Finally, we analyze memory and operation requirements of the compressed\nnetworks and conclude that T-Basis networks are equally well suited for\ntraining and inference in resource-constrained environments and usage on the\nedge devices.",
          "link": "http://arxiv.org/abs/2007.06631",
          "publishedOn": "2021-07-14T01:41:52.014Z",
          "wordCount": 650,
          "title": "T-Basis: a Compact Representation for Neural Networks. (arXiv:2007.06631v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lemkhenter_A/0/1/0/all/0/1\">Abdelhak Lemkhenter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1\">Adam Bielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_A/0/1/0/all/0/1\">Alp Eren Sari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "We introduce Kernel Density Discrimination GAN (KDD GAN), a novel method for\ngenerative adversarial learning. KDD GAN formulates the training as a\nlikelihood ratio optimization problem where the data distributions are written\nexplicitly via (local) Kernel Density Estimates (KDE). This is inspired by the\nrecent progress in contrastive learning and its relation to KDE. We define the\nKDEs directly in feature space and forgo the requirement of invertibility of\nthe kernel feature mappings. In our approach, features are no longer optimized\nfor linear separability, as in the original GAN formulation, but for the more\ngeneral discrimination of distributions in the feature space. We analyze the\ngradient of our loss with respect to the feature representation and show that\nit is better behaved than that of the original hinge loss. We perform\nexperiments with the proposed KDE-based loss, used either as a training loss or\na regularization term, on both CIFAR10 and scaled versions of ImageNet. We use\nBigGAN/SA-GAN as a backbone and baseline, since our focus is not to design the\narchitecture of the networks. We show a boost in the quality of generated\nsamples with respect to FID from 10% to 40% compared to the baseline. Code will\nbe made available.",
          "link": "http://arxiv.org/abs/2107.06197",
          "publishedOn": "2021-07-14T01:41:51.995Z",
          "wordCount": 638,
          "title": "Generative Adversarial Learning via Kernel Density Discrimination. (arXiv:2107.06197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Daniel Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oort_C/0/1/0/all/0/1\">Colin Van Oort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1\">Jonathan Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wshah_S/0/1/0/all/0/1\">Safwan Wshah</a>",
          "description": "Geo-localizing static objects from street images is challenging but also very\nimportant for road asset mapping and autonomous driving. In this paper we\npresent a two-stage framework that detects and geolocalizes traffic signs from\nlow frame rate street videos. Our proposed system uses a modified version of\nRetinaNet (GPS-RetinaNet), which predicts a positional offset for each sign\nrelative to the camera, in addition to performing the standard classification\nand bounding box regression. Candidate sign detections from GPS-RetinaNet are\ncondensed into geolocalized signs by our custom tracker, which consists of a\nlearned metric network and a variant of the Hungarian Algorithm. Our metric\nnetwork estimates the similarity between pairs of detections, then the\nHungarian Algorithm matches detections across images using the similarity\nscores provided by the metric network. Our models were trained using an updated\nversion of the ARTS dataset, which contains 25,544 images and 47.589 sign\nannotations ~\\cite{arts}. The proposed dataset covers a diverse set of\nenvironments gathered from a broad selection of roads. Each annotaiton contains\na sign class label, its geospatial location, an assembly label, a side of road\nindicator, and unique identifiers that aid in the evaluation. This dataset will\nsupport future progress in the field, and the proposed system demonstrates how\nto take advantage of some of the unique characteristics of a realistic\ngeolocalization dataset.",
          "link": "http://arxiv.org/abs/2107.06257",
          "publishedOn": "2021-07-14T01:41:51.983Z",
          "wordCount": 677,
          "title": "Object Tracking and Geo-localization from Street Images. (arXiv:2107.06257v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anari_N/0/1/0/all/0/1\">Nima Anari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thuy-Duong Vuong</a>",
          "description": "We show a connection between sampling and optimization on discrete domains.\nFor a family of distributions $\\mu$ defined on size $k$ subsets of a ground set\nof elements that is closed under external fields, we show that rapid mixing of\nnatural local random walks implies the existence of simple approximation\nalgorithms to find $\\max \\mu(\\cdot)$. More precisely we show that if\n(multi-step) down-up random walks have spectral gap at least inverse\npolynomially large in $k$, then (multi-step) local search can find $\\max\n\\mu(\\cdot)$ within a factor of $k^{O(k)}$. As the main application of our\nresult, we show a simple nearly-optimal $k^{O(k)}$-factor approximation\nalgorithm for MAP inference on nonsymmetric DPPs. This is the first nontrivial\nmultiplicative approximation for finding the largest size $k$ principal minor\nof a square (not-necessarily-symmetric) matrix $L$ with $L+L^\\intercal\\succeq\n0$.\n\nWe establish the connection between sampling and optimization by showing that\nan exchange inequality, a concept rooted in discrete convex analysis, can be\nderived from fast mixing of local random walks. We further connect exchange\ninequalities with composable core-sets for optimization, generalizing recent\nresults on composable core-sets for DPP maximization to arbitrary distributions\nthat satisfy either the strongly Rayleigh property or that have a log-concave\ngenerating polynomial.",
          "link": "http://arxiv.org/abs/2102.05347",
          "publishedOn": "2021-07-14T01:41:51.976Z",
          "wordCount": 677,
          "title": "From Sampling to Optimization on Discrete Domains withApplications to Determinant Maximization. (arXiv:2102.05347v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07671",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Simovic_P/0/1/0/all/0/1\">Petra Posedel &#x160;imovi&#x107;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Horvatic_D/0/1/0/all/0/1\">Davor Horvatic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_E/0/1/0/all/0/1\">Edward W. Sun</a>",
          "description": "Using big data to analyze consumer behavior can provide effective\ndecision-making tools for preventing customer attrition (churn) in customer\nrelationship management (CRM). Focusing on a CRM dataset with several different\ncategories of factors that impact customer heterogeneity (i.e., usage of\nself-care service channels, duration of service, and responsiveness to\nmarketing actions), we provide new predictive analytics of customer churn rate\nbased on a machine learning method that enhances the classification of logistic\nregression by adding a mixed penalty term. The proposed penalized logistic\nregression can prevent overfitting when dealing with big data and minimize the\nloss function when balancing the cost from the median (absolute value) and mean\n(squared value) regularization. We show the analytical properties of the\nproposed method and its computational advantage in this research. In addition,\nwe investigate the performance of the proposed method with a CRM data set (that\nhas a large number of features) under different settings by efficiently\neliminating the disturbance of (1) least important features and (2) sensitivity\nfrom the minority (churn) class. Our empirical results confirm the expected\nperformance of the proposed method in full compliance with the common\nclassification criteria (i.e., accuracy, precision, and recall) for evaluating\nmachine learning methods.",
          "link": "http://arxiv.org/abs/2105.07671",
          "publishedOn": "2021-07-14T01:41:51.968Z",
          "wordCount": 694,
          "title": "Classifying variety of customer's online engagement for churn prediction with mixed-penalty logistic regression. (arXiv:2105.07671v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Bowen Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eismann_S/0/1/0/all/0/1\">Stephan Eismann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_P/0/1/0/all/0/1\">Pratham N. Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Ron O. Dror</a>",
          "description": "Representing and reasoning about 3D structures of macromolecules is emerging\nas a distinct challenge in machine learning. Here, we extend recent work on\ngeometric vector perceptrons and apply equivariant graph neural networks to a\nwide range of tasks from structural biology. Our method outperforms all\nreference architectures on three out of eight tasks in the ATOM3D benchmark, is\ntied for first on two others, and is competitive with equivariant networks\nusing higher-order representations and spherical harmonic convolutions. In\naddition, we demonstrate that transfer learning can further improve performance\non certain downstream tasks. Code is available at\nhttps://github.com/drorlab/gvp-pytorch.",
          "link": "http://arxiv.org/abs/2106.03843",
          "publishedOn": "2021-07-14T01:41:51.961Z",
          "wordCount": 572,
          "title": "Equivariant Graph Neural Networks for 3D Macromolecular Structure. (arXiv:2106.03843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rossenbach_N/0/1/0/all/0/1\">Nick Rossenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilmes_B/0/1/0/all/0/1\">Benedikt Hilmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "Recent publications on automatic-speech-recognition (ASR) have a strong focus\non attention encoder-decoder (AED) architectures which tend to suffer from\nover-fitting in low resource scenarios. One solution to tackle this issue is to\ngenerate synthetic data with a trained text-to-speech system (TTS) if\nadditional text is available. This was successfully applied in many\npublications with AED systems, but only very limited in the context of other\nASR architectures. We investigate the effect of varying pre-processing, the\nspeaker embedding and input encoding of the TTS system w.r.t. the effectiveness\nof the synthesized data for AED-ASR training. Additionally, we also consider\ninternal language model subtraction for the first time, resulting in up to 38%\nrelative improvement. We compare the AED results to a state-of-the-art hybrid\nASR system, a monophone based system using\nconnectionist-temporal-classification (CTC) and a monotonic transducer based\nsystem. We show that for the later systems the addition of synthetic data has\nno relevant effect, but they still outperform the AED systems on\nLibriSpeech-100h. We achieve a final word-error-rate of 3.3%/10.0% with a\nhybrid system on the clean/noisy test-sets, surpassing any previous\nstate-of-the-art systems on Librispeech-100h that do not include unlabeled\naudio data.",
          "link": "http://arxiv.org/abs/2104.05379",
          "publishedOn": "2021-07-14T01:41:51.943Z",
          "wordCount": 681,
          "title": "Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures. (arXiv:2104.05379v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheinker_A/0/1/0/all/0/1\">Alexander Scheinker</a>",
          "description": "Machine learning (ML) tools such as encoder-decoder convolutional neural\nnetworks (CNN) can represent incredibly complex nonlinear functions which map\nbetween combinations of images and scalars. For example, CNNs can be used to\nmap combinations of accelerator parameters and images which are 2D projections\nof the 6D phase space distributions of charged particle beams as they are\ntransported between various particle accelerator locations. Despite their\nstrengths, applying ML to time-varying systems, or systems with shifting\ndistributions, is an open problem, especially for large systems for which\ncollecting new data for re-training is impractical or interrupts operations.\nParticle accelerators are one example of large time-varying systems for which\ncollecting detailed training data requires lengthy dedicated beam measurements\nwhich may no longer be available during regular operations. We present a\nrecently developed method of adaptive ML for time-varying systems. Our approach\nis to map very high (N>100k) dimensional inputs (a combination of scalar\nparameters and images) into the low dimensional (N~2) latent space at the\noutput of the encoder section of an encoder-decoder CNN. We then actively tune\nthe low dimensional latent space-based representation of complex system\ndynamics by the addition of an adaptively tuned feedback vector directly before\nthe decoder sections builds back up to our image-based high-dimensional phase\nspace density representations. This method allows us to learn correlations\nwithin and to quickly tune the characteristics of incredibly high parameter\nsystems and to track their evolution in real time based on feedback without\nmassive new data sets for re-training.",
          "link": "http://arxiv.org/abs/2107.06207",
          "publishedOn": "2021-07-14T01:41:51.936Z",
          "wordCount": 689,
          "title": "Adaptive Machine Learning for Time-Varying Systems: Low Dimensional Latent Space Tuning. (arXiv:2107.06207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05767",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Toledo_C/0/1/0/all/0/1\">Carmen Melo Toledo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bassedon_G/0/1/0/all/0/1\">Guilherme Mendes Bassedon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ferreira_J/0/1/0/all/0/1\">Jonathan Batista Ferreira</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gianvechio_L/0/1/0/all/0/1\">Lucka de Godoy Gianvechio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guatimosim_C/0/1/0/all/0/1\">Carlos Guatimosim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1\">Renato Vicente</a>",
          "description": "Student's grade retention is a key issue faced by many education systems,\nespecially those in developing countries. In this paper, we seek to gauge the\nrelevance of students' personality traits in predicting grade retention in\nBrazil. For that, we used data collected in 2012 and 2017, in the city of\nSertaozinho, countryside of the state of Sao Paulo, Brazil. The surveys taken\nin Sertaozinho included several socioeconomic questions, standardized tests,\nand a personality test. Moreover, students were in grades 4, 5, and 6 in 2012.\nOur approach was based on training machine learning models on the surveys' data\nto predict grade retention between 2012 and 2017 using information from 2012 or\nbefore, and then using some strategies to quantify personality traits'\npredictive power. We concluded that, besides proving to be fairly better than a\nrandom classifier when isolated, personality traits contribute to prediction\neven when using socioeconomic variables and standardized tests results.",
          "link": "http://arxiv.org/abs/2107.05767",
          "publishedOn": "2021-07-14T01:41:51.922Z",
          "wordCount": 606,
          "title": "Effects of personality traits in predicting grade retention of Brazilian students. (arXiv:2107.05767v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harris_K/0/1/0/all/0/1\">Keegan Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Daniel Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1\">Logan Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1\">Hoda Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>",
          "description": "Machine Learning algorithms often prompt individuals to strategically modify\ntheir observable attributes to receive more favorable predictions. As a result,\nthe distribution the predictive model is trained on may differ from the one it\noperates on in deployment. While such distribution shifts, in general, hinder\naccurate predictions, our work identifies a unique opportunity associated with\nshifts due to strategic responses: We show that we can use strategic responses\neffectively to recover causal relationships between the observable features and\noutcomes we wish to predict. More specifically, we study a game-theoretic model\nin which a principal deploys a sequence of models to predict an outcome of\ninterest (e.g., college GPA) for a sequence of strategic agents (e.g., college\napplicants). In response, strategic agents invest efforts and modify their\nfeatures for better predictions. In such settings, unobserved confounding\nvariables can influence both an agent's observable features (e.g., high school\nrecords) and outcomes. Therefore, standard regression methods generally produce\nbiased estimators. In order to address this issue, our work establishes a novel\nconnection between strategic responses to machine learning models and\ninstrumental variable (IV) regression, by observing that the sequence of\ndeployed models can be viewed as an instrument that affects agents' observable\nfeatures but does not directly influence their outcomes. Therefore, two-stage\nleast squares (2SLS) regression can recover the causal relationships between\nobservable features and outcomes. Beyond causal recovery, we can build on our\n2SLS method to address two additional relevant optimization objectives: agent\noutcome maximization and predictive risk minimization. Finally, our numerical\nsimulations on semi-synthetic data show that our methods significantly\noutperform OLS regression in causal relationship estimation.",
          "link": "http://arxiv.org/abs/2107.05762",
          "publishedOn": "2021-07-14T01:41:51.912Z",
          "wordCount": 703,
          "title": "Strategic Instrumental Variable Regression: Recovering Causal Relationships From Strategic Responses. (arXiv:2107.05762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zappella_G/0/1/0/all/0/1\">Giovanni Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salinas_D/0/1/0/all/0/1\">David Salinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1\">C&#xe9;dric Archambeau</a>",
          "description": "In this work we consider the problem of repeated hyperparameter and neural\narchitecture search (HNAS). We propose an extension of Successive Halving that\nis able to leverage information gained in previous HNAS problems with the goal\nof saving computational resources. We empirically demonstrate that our solution\nis able to drastically decrease costs while maintaining accuracy and being\nrobust to negative transfer. Our method is significantly simpler than competing\ntransfer learning approaches, setting a new baseline for transfer learning in\nHNAS.",
          "link": "http://arxiv.org/abs/2103.16111",
          "publishedOn": "2021-07-14T01:41:51.905Z",
          "wordCount": 550,
          "title": "A resource-efficient method for repeated HPO and NAS problems. (arXiv:2103.16111v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.08962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaza_K/0/1/0/all/0/1\">Kesav Kaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meshram_R/0/1/0/all/0/1\">Rahul Meshram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1\">Varun Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merchant_S/0/1/0/all/0/1\">S.N.Merchant</a>",
          "description": "This paper studies a class of constrained restless multi-armed bandits\n(CRMAB). The constraints are in the form of time varying set of actions (set of\navailable arms). This variation can be either stochastic or semi-deterministic.\nGiven a set of arms, a fixed number of them can be chosen to be played in each\ndecision interval. The play of each arm yields a state dependent reward. The\ncurrent states of arms are partially observable through binary feedback signals\nfrom arms that are played. The current availability of arms is fully\nobservable. The objective is to maximize long term cumulative reward. The\nuncertainty about future availability of arms along with partial state\ninformation makes this objective challenging. Applications for CRMAB abound in\nthe domain of cyber-physical systems. First, this optimization problem is\nanalyzed using Whittle's index policy. To this end, a constrained restless\nsingle-armed bandit is studied. It is shown to admit a threshold-type optimal\npolicy and is also indexable. An algorithm to compute Whittle's index is\npresented. An alternate solution method with lower complexity is also presented\nin the form of an online rollout policy. Further, upper bounds on the value\nfunction are derived in order to estimate the degree of sub-optimality of\nvarious solutions. The simulation study compares the performance of Whittle's\nindex, online rollout, myopic and modified Whittle's index policies.",
          "link": "http://arxiv.org/abs/1904.08962",
          "publishedOn": "2021-07-14T01:41:51.886Z",
          "wordCount": 713,
          "title": "Constrained Restless Bandits for Dynamic Scheduling in Cyber-Physical Systems. (arXiv:1904.08962v4 [cs.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10643",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1\">Jie Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gallego_B/0/1/0/all/0/1\">Blanca Gallego</a>",
          "description": "Causal inference in longitudinal observational health data often requires the\naccurate estimation of treatment effects on time-to-event outcomes in the\npresence of time-varying covariates. To tackle this sequential treatment effect\nestimation problem, we have developed a causal dynamic survival (CDS) model\nthat uses the potential outcomes framework with the recurrent sub-networks with\nrandom seed ensembles to estimate the difference in survival curves of its\nconfidence interval. Using simulated survival datasets, the CDS model has shown\ngood causal effect estimation performance across scenarios of sample dimension,\nevent rate, confounding and overlapping. However, increasing the sample size is\nnot effective to alleviate the adverse impact from high level of confounding.\nIn two large clinical cohort studies, our model identified the expected\nconditional average treatment effect and detected individual effect\nheterogeneity over time and patient subgroups. CDS provides individualised\nabsolute treatment effect estimations to improve clinical decisions.",
          "link": "http://arxiv.org/abs/2101.10643",
          "publishedOn": "2021-07-14T01:41:51.880Z",
          "wordCount": 652,
          "title": "Casual Inference using Deep Bayesian Dynamic Survival Model (CDS). (arXiv:2101.10643v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1\">Idan Achituve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1\">Aviv Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yemini_Y/0/1/0/all/0/1\">Yochai Yemini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>",
          "description": "Gaussian processes (GPs) are non-parametric, flexible, models that work well\nin many tasks. Combining GPs with deep learning methods via deep kernel\nlearning (DKL) is especially compelling due to the strong representational\npower induced by the network. However, inference in GPs, whether with or\nwithout DKL, can be computationally challenging on large datasets. Here, we\npropose GP-Tree, a novel method for multi-class classification with Gaussian\nprocesses and DKL. We develop a tree-based hierarchical model in which each\ninternal node of the tree fits a GP to the data using the P\\'olya Gamma\naugmentation scheme. As a result, our method scales well with both the number\nof classes and data size. We demonstrate the effectiveness of our method\nagainst other Gaussian process training baselines, and we show how our general\nGP approach achieves improved accuracy on standard incremental few-shot\nlearning benchmarks.",
          "link": "http://arxiv.org/abs/2102.07868",
          "publishedOn": "2021-07-14T01:41:51.873Z",
          "wordCount": 618,
          "title": "GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning. (arXiv:2102.07868v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.10516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruizhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Bo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1\">Greg Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehrmann_A/0/1/0/all/0/1\">Andreas Lehrmann</a>",
          "description": "Normalizing flows transform a simple base distribution into a complex target\ndistribution and have proved to be powerful models for data generation and\ndensity estimation. In this work, we propose a novel type of normalizing flow\ndriven by a differential deformation of the Wiener process. As a result, we\nobtain a rich time series model whose observable process inherits many of the\nappealing properties of its base process, such as efficient computation of\nlikelihoods and marginals. Furthermore, our continuous treatment provides a\nnatural framework for irregular time series with an independent arrival\nprocess, including straightforward interpolation. We illustrate the desirable\nproperties of the proposed model on popular stochastic processes and\ndemonstrate its superior flexibility to variational RNN and latent ODE\nbaselines in a series of experiments on synthetic and real-world data.",
          "link": "http://arxiv.org/abs/2002.10516",
          "publishedOn": "2021-07-14T01:41:51.866Z",
          "wordCount": 624,
          "title": "Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows. (arXiv:2002.10516v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duval_A/0/1/0/all/0/1\">Alexandre Duval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1\">Fragkiskos D. Malliaros</a>",
          "description": "Graph Neural Networks (GNNs) achieve significant performance for various\nlearning tasks on geometric data due to the incorporation of graph structure\ninto the learning of node representations, which renders their comprehension\nchallenging. In this paper, we first propose a unified framework satisfied by\nmost existing GNN explainers. Then, we introduce GraphSVX, a post hoc local\nmodel-agnostic explanation method specifically designed for GNNs. GraphSVX is a\ndecomposition technique that captures the \"fair\" contribution of each feature\nand node towards the explained prediction by constructing a surrogate model on\na perturbed dataset. It extends to graphs and ultimately provides as\nexplanation the Shapley Values from game theory. Experiments on real-world and\nsynthetic datasets demonstrate that GraphSVX achieves state-of-the-art\nperformance compared to baseline models while presenting core theoretical and\nhuman-centric properties.",
          "link": "http://arxiv.org/abs/2104.10482",
          "publishedOn": "2021-07-14T01:41:51.858Z",
          "wordCount": 586,
          "title": "GraphSVX: Shapley Value Explanations for Graph Neural Networks. (arXiv:2104.10482v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguntola_I/0/1/0/all/0/1\">Ini Oguntola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_D/0/1/0/all/0/1\">Dana Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1\">Katia Sycara</a>",
          "description": "When developing AI systems that interact with humans, it is essential to\ndesign both a system that can understand humans, and a system that humans can\nunderstand. Most deep network based agent-modeling approaches are 1) not\ninterpretable and 2) only model external behavior, ignoring internal mental\nstates, which potentially limits their capability for assistance,\ninterventions, discovering false beliefs, etc. To this end, we develop an\ninterpretable modular neural framework for modeling the intentions of other\nobserved entities. We demonstrate the efficacy of our approach with experiments\non data from human participants on a search and rescue task in Minecraft, and\nshow that incorporating interpretability can significantly increase predictive\nperformance under the right conditions.",
          "link": "http://arxiv.org/abs/2104.02938",
          "publishedOn": "2021-07-14T01:41:51.840Z",
          "wordCount": 574,
          "title": "Deep Interpretable Models of Theory of Mind. (arXiv:2104.02938v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Busk_J/0/1/0/all/0/1\">Jonas Busk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorgensen_P/0/1/0/all/0/1\">Peter Bj&#xf8;rn J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmik_A/0/1/0/all/0/1\">Arghya Bhowmik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Mikkel N. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegge_T/0/1/0/all/0/1\">Tejs Vegge</a>",
          "description": "Data-driven methods based on machine learning have the potential to\naccelerate analysis of atomic structures. However, machine learning models can\nproduce overconfident predictions and it is therefore crucial to detect and\nhandle uncertainty carefully. Here, we extend a message passing neural network\ndesigned specifically for predicting properties of molecules and materials with\na calibrated probabilistic predictive distribution. The method presented in\nthis paper differs from the previous work by considering both aleatoric and\nepistemic uncertainty in a unified framework, and by re-calibrating the\npredictive distribution on unseen data. Through computer experiments, we show\nthat our approach results in accurate models for predicting molecular formation\nenergies with calibrated uncertainty in and out of the training data\ndistribution on two public molecular benchmark datasets, QM9 and PC9. The\nproposed method provides a general framework for training and evaluating neural\nnetwork ensemble models that are able to produce accurate predictions of\nproperties of molecules with calibrated uncertainty.",
          "link": "http://arxiv.org/abs/2107.06068",
          "publishedOn": "2021-07-14T01:41:51.833Z",
          "wordCount": 605,
          "title": "Calibrated Uncertainty for Molecular Property Prediction using Ensembles of Message Passing Neural Networks. (arXiv:2107.06068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1\">Mineto Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1\">Hiroki Matsutani</a>",
          "description": "Currently there has been increasing demand for real-time training on\nresource-limited IoT devices such as smart sensors, which realizes standalone\nonline adaptation for streaming data without data transfers to remote servers.\nOS-ELM (Online Sequential Extreme Learning Machine) has been one of promising\nneural-network-based online algorithms for on-chip learning because it can\nperform online training at low computational cost and is easy to implement as a\ndigital circuit. Existing OS-ELM digital circuits employ fixed-point data\nformat and the bit-widths are often manually tuned, however, this may cause\noverflow or underflow which can lead to unexpected behavior of the circuit. For\non-chip learning systems, an overflow/underflow-free design has a great impact\nsince online training is continuously performed and the intervals of\nintermediate variables will dynamically change as time goes by. In this paper,\nwe propose an overflow/underflow-free bit-width optimization method for\nfixed-point digital circuits of OS-ELM. Experimental results show that our\nmethod realizes overflow/underflow-free OS-ELM digital circuits with 1.0x -\n1.5x more area cost compared to the baseline simulation method where overflow\nor underflow can happen.",
          "link": "http://arxiv.org/abs/2103.09791",
          "publishedOn": "2021-07-14T01:41:51.827Z",
          "wordCount": 636,
          "title": "An Overflow/Underflow-Free Fixed-Point Bit-Width Optimization Method for OS-ELM Digital Circuit. (arXiv:2103.09791v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sah_R/0/1/0/all/0/1\">Ramesh Kumar Sah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1\">Hassan Ghasemzadeh</a>",
          "description": "Stress detection and monitoring is an active area of research with important\nimplications for the personal, professional, and social health of an\nindividual. Current approaches for affective state classification use\ntraditional machine learning algorithms with features computed from multiple\nsensor modalities. These methods are data-intensive and rely on hand-crafted\nfeatures which impede the practical applicability of these sensor systems in\ndaily lives. To overcome these shortcomings, we propose a novel Convolutional\nNeural Network (CNN) based stress detection and classification framework\nwithout any feature computation using data from only one sensor modality. Our\nmethod is competitive and outperforms current state-of-the-art techniques and\nachieves a classification accuracy of $92.85\\%$ and an $f1$ score of $0.89$.\nThrough our leave-one-subject-out analysis, we also show the importance of\npersonalizing stress models.",
          "link": "http://arxiv.org/abs/2107.05666",
          "publishedOn": "2021-07-14T01:41:51.820Z",
          "wordCount": 577,
          "title": "Stress Classification and Personalization: Getting the most out of the least. (arXiv:2107.05666v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Dibya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahme_J/0/1/0/all/0/1\">Jad Rahme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aviral Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amy Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_R/0/1/0/all/0/1\">Ryan P. Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Generalization is a central challenge for the deployment of reinforcement\nlearning (RL) systems in the real world. In this paper, we show that the\nsequential structure of the RL problem necessitates new approaches to\ngeneralization beyond the well-studied techniques used in supervised learning.\nWhile supervised learning methods can generalize effectively without explicitly\naccounting for epistemic uncertainty, we show that, perhaps surprisingly, this\nis not the case in RL. We show that generalization to unseen test conditions\nfrom a limited number of training conditions induces implicit partial\nobservability, effectively turning even fully-observed MDPs into POMDPs.\nInformed by this observation, we recast the problem of generalization in RL as\nsolving the induced partially observed Markov decision process, which we call\nthe epistemic POMDP. We demonstrate the failure modes of algorithms that do not\nappropriately handle this partial observability, and suggest a simple\nensemble-based technique for approximately solving the partially observed\nproblem. Empirically, we demonstrate that our simple algorithm derived from the\nepistemic POMDP achieves significant gains in generalization over current\nmethods on the Procgen benchmark suite.",
          "link": "http://arxiv.org/abs/2107.06277",
          "publishedOn": "2021-07-14T01:41:51.813Z",
          "wordCount": 632,
          "title": "Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability. (arXiv:2107.06277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiusheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1\">Nikhil Bhendawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Ting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1\">Desheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1\">Bingyu Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>",
          "description": "Transformer-based models have made tremendous impacts in natural language\ngeneration. However the inference speed is a bottleneck due to large model size\nand intensive computing involved in auto-regressive decoding process. We\ndevelop FastSeq framework to accelerate sequence generation without accuracy\nloss. The proposed optimization techniques include an attention cache\noptimization, an efficient algorithm for detecting repeated n-grams, and an\nasynchronous generation pipeline with parallel I/O. These optimizations are\ngeneral enough to be applicable to Transformer-based models (e.g., T5, GPT2,\nand UniLM). Our benchmark results on a set of widely used and diverse models\ndemonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use\nwith a simple one-line code change. The source code is available at\nhttps://github.com/microsoft/fastseq.",
          "link": "http://arxiv.org/abs/2106.04718",
          "publishedOn": "2021-07-14T01:41:51.795Z",
          "wordCount": 595,
          "title": "FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06181",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gecgel_S/0/1/0/all/0/1\">Selen Gecgel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurt_G/0/1/0/all/0/1\">Gunes Karabulut Kurt</a>",
          "description": "Towards sixth-generation networks (6G), satellite communication systems,\nespecially based on Low Earth Orbit (LEO) networks, become promising due to\ntheir unique and comprehensive capabilities. These advantages are accompanied\nby a variety of challenges such as security vulnerabilities, management of\nhybrid systems, and high mobility. In this paper, firstly, a security\ndeficiency in the physical layer is addressed with a conceptual framework,\nconsidering the cyber-physical nature of the satellite systems, highlighting\nthe potential attacks. Secondly, a learning-driven detection scheme is\nproposed, and the lightweight convolutional neural network (CNN) is designed.\nThe performance of the designed CNN architecture is compared with a prevalent\nmachine learning algorithm, support vector machine (SVM). The results show that\ndeficiency attacks against the satellite systems can be detected by employing\nthe proposed scheme.",
          "link": "http://arxiv.org/abs/2107.06181",
          "publishedOn": "2021-07-14T01:41:51.788Z",
          "wordCount": 574,
          "title": "Intermittent Jamming against Telemetry and Telecommand of Satellite Systems and A Learning-driven Detection Strategy. (arXiv:2107.06181v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1\">Syed Muhammad Arsalan Bashir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mahrukh Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yilong Niu</a>",
          "description": "Image super-resolution (SR) is one of the vital image processing methods that\nimprove the resolution of an image in the field of computer vision. In the last\ntwo decades, significant progress has been made in the field of\nsuper-resolution, especially by utilizing deep learning methods. This survey is\nan effort to provide a detailed survey of recent progress in single-image\nsuper-resolution in the perspective of deep learning while also informing about\nthe initial classical methods used for image super-resolution. The survey\nclassifies the image SR methods into four categories, i.e., classical methods,\nsupervised learning-based methods, unsupervised learning-based methods, and\ndomain-specific SR methods. We also introduce the problem of SR to provide\nintuition about image quality metrics, available reference datasets, and SR\nchallenges. Deep learning-based approaches of SR are evaluated using a\nreference dataset. Some of the reviewed state-of-the-art image SR methods\ninclude the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN),\nmultiscale residual network (MSRN), meta residual dense network (Meta-RDN),\nrecurrent back-projection network (RBPN), second-order attention network (SAN),\nSR feedback network (SRFBN) and the wavelet-based residual attention network\n(WRAN). Finally, this survey is concluded with future directions and trends in\nSR and open problems in SR to be addressed by the researchers.",
          "link": "http://arxiv.org/abs/2102.09351",
          "publishedOn": "2021-07-14T01:41:51.781Z",
          "wordCount": 699,
          "title": "A Comprehensive Review of Deep Learning-based Single Image Super-resolution. (arXiv:2102.09351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>",
          "description": "The SARS-CoV-2 virus and COVID-19 disease have posed unprecedented and\noverwhelming challenges and opportunities to data and domain-driven modeling.\nThis paper makes a comprehensive review of the challenges, tasks, methods, gaps\nand opportunities on modeling COVID-19 problems and data. It constructs a\nresearch landscape of COVID-19 modeling, and further categorizes, compares and\ndiscusses the related work on modeling COVID-19 epidemic transmission processes\nand dynamics, case identification and tracing, infection diagnosis and trends,\nmedical treatments, non-pharmaceutical intervention effect, drug and vaccine\ndevelopment, psychological, economic and social impact, and misinformation,\netc. The modeling methods involve mathematical and statistical models,\ndomain-driven modeling by epidemiological compartmental models, medical and\nbiomedical analysis, data-driven learning by shallow and deep machine learning,\nsimulation systems, social science methods, and hybrid methods.",
          "link": "http://arxiv.org/abs/2104.12556",
          "publishedOn": "2021-07-14T01:41:51.774Z",
          "wordCount": 626,
          "title": "COVID-19 Modeling: A Review. (arXiv:2104.12556v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01874",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bramburger_J/0/1/0/all/0/1\">Jason J. Bramburger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Brunton_S/0/1/0/all/0/1\">Steven L. Brunton</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kutz_J/0/1/0/all/0/1\">J. Nathan Kutz</a>",
          "description": "Despite many of the most common chaotic dynamical systems being continuous in\ntime, it is through discrete time mappings that much of the understanding of\nchaos is formed. Henri Poincar\\'e first made this connection by tracking\nconsecutive iterations of the continuous flow with a lower-dimensional,\ntransverse subspace. The mapping that iterates the dynamics through consecutive\nintersections of the flow with the subspace is now referred to as a Poincar\\'e\nmap, and it is the primary method available for interpreting and classifying\nchaotic dynamics. Unfortunately, in all but the simplest systems, an explicit\nform for such a mapping remains outstanding. This work proposes a method for\nobtaining explicit Poincar\\'e mappings by using deep learning to construct an\ninvertible coordinate transformation into a conjugate representation where the\ndynamics are governed by a relatively simple chaotic mapping. The invertible\nchange of variable is based on an autoencoder, which allows for dimensionality\nreduction, and has the advantage of classifying chaotic systems using the\nequivalence relation of topological conjugacies. Indeed, the enforcement of\ntopological conjugacies is the critical neural network regularization for\nlearning the coordinate and dynamics pairing. We provide expository\napplications of the method to low-dimensional systems such as the R\\\"ossler and\nLorenz systems, while also demonstrating the utility of the method on\ninfinite-dimensional systems, such as the Kuramoto--Sivashinsky equation.",
          "link": "http://arxiv.org/abs/2104.01874",
          "publishedOn": "2021-07-14T01:41:51.767Z",
          "wordCount": 662,
          "title": "Deep Learning of Conjugate Mappings. (arXiv:2104.01874v2 [math.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bressan_M/0/1/0/all/0/1\">Marco Bressan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1\">Nicol&#xf2; Cesa-Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1\">Silvio Lattanzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudice_A/0/1/0/all/0/1\">Andrea Paudice</a>",
          "description": "We investigate the problem of exact cluster recovery using oracle queries.\nPrevious results show that clusters in Euclidean spaces that are convex and\nseparated with a margin can be reconstructed exactly using only $O(\\log n)$\nsame-cluster queries, where $n$ is the number of input points. In this work, we\nstudy this problem in the more challenging non-convex setting. We introduce a\nstructural characterization of clusters, called $(\\beta,\\gamma)$-convexity,\nthat can be applied to any finite set of points equipped with a metric (or even\na semimetric, as the triangle inequality is not needed). Using\n$(\\beta,\\gamma)$-convexity, we can translate natural density properties of\nclusters (which include, for instance, clusters that are strongly non-convex in\n$\\mathbb{R}^d$) into a graph-theoretic notion of convexity. By exploiting this\nconvexity notion, we design a deterministic algorithm that recovers\n$(\\beta,\\gamma)$-convex clusters using $O(k^2 \\log n + k^2\n(6/\\beta\\gamma)^{dens(X)})$ same-cluster queries, where $k$ is the number of\nclusters and $dens(X)$ is the density dimension of the semimetric. We show that\nan exponential dependence on the density dimension is necessary, and we also\nshow that, if we are allowed to make $O(k^2 + k\\log n)$ additional queries to a\n\"cluster separation\" oracle, then we can recover clusters that have different\nand arbitrary scales, even when the scale of each cluster is unknown.",
          "link": "http://arxiv.org/abs/2102.00504",
          "publishedOn": "2021-07-14T01:41:51.747Z",
          "wordCount": 694,
          "title": "Exact Recovery of Clusters in Finite Metric Spaces Using Oracle Queries. (arXiv:2102.00504v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao-Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "Modern keyboards allow a musician to play multiple instruments at the same\ntime by assigning zones -- fixed pitch ranges of the keyboard -- to different\ninstruments. In this paper, we aim to further extend this idea and examine the\nfeasibility of automatic instrumentation -- dynamically assigning instruments\nto notes in solo music during performance. In addition to the online,\nreal-time-capable setting for performative use cases, automatic instrumentation\ncan also find applications in assistive composing tools in an offline setting.\nDue to the lack of paired data of original solo music and their full\narrangements, we approach automatic instrumentation by learning to separate\nparts (e.g., voices, instruments and tracks) from their mixture in symbolic\nmultitrack music, assuming that the mixture is to be played on a keyboard. We\nframe the task of part separation as a sequential multi-class classification\nproblem and adopt machine learning to map sequences of notes into sequences of\npart labels. To examine the effectiveness of our proposed models, we conduct a\ncomprehensive empirical evaluation over four diverse datasets of different\ngenres and ensembles -- Bach chorales, string quartets, game music and pop\nmusic. Our experiments show that the proposed models outperform various\nbaselines. We also demonstrate the potential for our proposed models to produce\nalternative convincing instrumentations for an existing arrangement by\nseparating its mixture into parts. All source code and audio samples can be\nfound at https://salu133445.github.io/arranger/ .",
          "link": "http://arxiv.org/abs/2107.05916",
          "publishedOn": "2021-07-14T01:41:51.741Z",
          "wordCount": 691,
          "title": "Towards Automatic Instrumentation by Learning to Separate Parts in Symbolic Multitrack Music. (arXiv:2107.05916v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Revay_M/0/1/0/all/0/1\">Max Revay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manchester_I/0/1/0/all/0/1\">Ian R. Manchester</a>",
          "description": "This paper introduces recurrent equilibrium networks (RENs), a new class of\nnonlinear dynamical models for applications in machine learning, system\nidentification and control. The new model class has ``built in'' guarantees of\nstability and robustness: all models in the class are contracting - a strong\nform of nonlinear stability - and models can satisfy prescribed incremental\nintegral quadratic constraints (IQC), including Lipschitz bounds and\nincremental passivity. RENs are otherwise very flexible: they can represent all\nstable linear systems, all previously-known sets of contracting recurrent\nneural networks and echo state networks, all deep feedforward neural networks,\nand all stable Wiener/Hammerstein models. RENs are parameterized directly by a\nvector in R^N, i.e. stability and robustness are ensured without parameter\nconstraints, which simplifies learning since generic methods for unconstrained\noptimization can be used. The performance and robustness of the new model set\nis evaluated on benchmark nonlinear system identification problems, and the\npaper also presents applications in data-driven nonlinear observer design and\ncontrol with stability guarantees.",
          "link": "http://arxiv.org/abs/2104.05942",
          "publishedOn": "2021-07-14T01:41:51.734Z",
          "wordCount": 653,
          "title": "Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness. (arXiv:2104.05942v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhui/0/1/0/all/0/1\">Minhui</a> (Jason)Xue, <a href=\"http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1\">Stjepan Picek</a>",
          "description": "Backdoor attacks represent a serious threat to neural network models. A\nbackdoored model will misclassify the trigger-embedded inputs into an\nattacker-chosen target label while performing normally on other benign inputs.\nThere are already numerous works on backdoor attacks on neural networks, but\nonly a few works consider graph neural networks (GNNs). As such, there is no\nintensive research on explaining the impact of trigger injecting position on\nthe performance of backdoor attacks on GNNs.\n\nTo bridge this gap, we conduct an experimental investigation on the\nperformance of backdoor attacks on GNNs. We apply two powerful GNN\nexplainability approaches to select the optimal trigger injecting position to\nachieve two attacker objectives -- high attack success rate and low clean\naccuracy drop. Our empirical results on benchmark datasets and state-of-the-art\nneural network models demonstrate the proposed method's effectiveness in\nselecting trigger injecting position for backdoor attacks on GNNs. For\ninstance, on the node classification task, the backdoor attack with trigger\ninjecting position selected by GraphLIME reaches over $84 \\%$ attack success\nrate with less than $2.5 \\%$ accuracy drop",
          "link": "http://arxiv.org/abs/2104.03674",
          "publishedOn": "2021-07-14T01:41:51.727Z",
          "wordCount": 640,
          "title": "Explainability-based Backdoor Attacks Against Graph Neural Networks. (arXiv:2104.03674v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of obese human mesh recovery, i.e., fitting a\nparametric human mesh to images of obese people. Despite obese person mesh\nfitting being an important problem with numerous applications (e.g.,\nhealthcare), much recent progress in mesh recovery has been restricted to\nimages of non-obese people. In this work, we identify this crucial gap in the\ncurrent literature by presenting and discussing limitations of existing\nalgorithms. Next, we present a simple baseline to address this problem that is\nscalable and can be easily used in conjunction with existing algorithms to\nimprove their performance. Finally, we present a generalized human mesh\noptimization algorithm that substantially improves the performance of existing\nmethods on both obese person images as well as community-standard benchmark\ndatasets. A key innovation of this technique is that it does not rely on\nsupervision from expensive-to-create mesh parameters. Instead, starting from\nwidely and cheaply available 2D keypoints annotations, our method automatically\ngenerates mesh parameters that can in turn be used to re-train and fine-tune\nany existing mesh estimation algorithm. This way, we show our method acts as a\ndrop-in to improve the performance of a wide variety of contemporary mesh\nestimation methods. We conduct extensive experiments on multiple datasets\ncomprising both standard and obese person images and demonstrate the efficacy\nof our proposed techniques.",
          "link": "http://arxiv.org/abs/2107.06239",
          "publishedOn": "2021-07-14T01:41:51.720Z",
          "wordCount": 677,
          "title": "Everybody Is Unique: Towards Unbiased Human Mesh Recovery. (arXiv:2107.06239v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "This paper aims to provide understandings for the effect of an\nover-parameterized model, e.g. a deep neural network, memorizing\ninstance-dependent noisy labels. We first quantify the harms caused by\nmemorizing noisy instances, and show the disparate impacts of noisy labels for\nsample instances with different representation frequencies. We then analyze how\nseveral popular solutions for learning with noisy labels mitigate this harm at\nthe instance level. Our analysis reveals that existing approaches lead to\ndisparate treatments when handling noisy instances. While higher-frequency\ninstances often enjoy a high probability of an improvement by applying these\nsolutions, lower-frequency instances do not. Our analysis reveals new\nunderstandings for when these approaches work, and provides theoretical\njustifications for previously reported empirical observations. This observation\nrequires us to rethink the distribution of label noise across instances and\ncalls for different treatments for instances in different regimes.",
          "link": "http://arxiv.org/abs/2102.05336",
          "publishedOn": "2021-07-14T01:41:51.701Z",
          "wordCount": 606,
          "title": "Understanding Instance-Level Label Noise: Disparate Impacts and Treatments. (arXiv:2102.05336v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">Mohit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1\">Bernhard A. Moser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1\">Lukas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freudenthaler_B/0/1/0/all/0/1\">Bernhard Freudenthaler</a>",
          "description": "Guidelines and principles of trustworthy AI should be adhered to in practice\nduring the development of AI systems. This work suggests a novel information\ntheoretic trustworthy AI framework based on the hypothesis that information\ntheory enables taking into account the ethical AI principles during the\ndevelopment of machine learning and deep learning models via providing a way to\nstudy and optimize the inherent tradeoffs between trustworthy AI principles.\nUnder the proposed framework, a unified approach to ``privacy-preserving\ninterpretable and transferable learning'' is considered to introduce the\ninformation theoretic measures for privacy-leakage, interpretability, and\ntransferability. A technique based on variational optimization, employing\n\\emph{conditionally deep autoencoders}, is developed for practically\ncalculating the defined information theoretic measures for privacy-leakage,\ninterpretability, and transferability.",
          "link": "http://arxiv.org/abs/2106.06046",
          "publishedOn": "2021-07-14T01:41:51.694Z",
          "wordCount": 618,
          "title": "Information Theoretic Evaluation of Privacy-Leakage, Interpretability, and Transferability for a Novel Trustworthy AI Framework. (arXiv:2106.06046v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhouzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1\">Kun Feng</a>",
          "description": "While the beta-VAE family is aiming to find disentangled representations and\nacquire human-interpretable generative factors, like what an ICA (from the\nlinear domain) does, we propose Full Encoder, a novel unified autoencoder\nframework as a correspondence to PCA in the non-linear domain. The idea is to\ntrain an autoencoder with one latent variable first, then involve more latent\nvariables progressively to refine the reconstruction results. The Full Encoder\nis also a latent variable predictive model that the latent variables acquired\nare stable and robust, as they always learn the same representation regardless\nof the network initial states. Full Encoder can be used to determine the\ndegrees of freedom in a simple non-linear system and can be useful for data\ncompression or anomaly detection. Full Encoder can also be combined with the\nbeta-VAE framework to sort out the importance of the generative factors,\nproviding more insights for non-linear system analysis. These qualities will\nmake FE useful for analyzing real-life industrial non-linear systems. To\nvalidate, we created a toy dataset with a custom-made non-linear system to test\nit and compare its properties to those of VAE and beta-VAE's.",
          "link": "http://arxiv.org/abs/2103.14082",
          "publishedOn": "2021-07-14T01:41:51.688Z",
          "wordCount": 636,
          "title": "Learning Stable Representations with Full Encoder. (arXiv:2103.14082v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rim_D/0/1/0/all/0/1\">Daniela N. Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_I/0/1/0/all/0/1\">Inseon Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>",
          "description": "Recent achievements in end-to-end deep learning have encouraged the\nexploration of tasks dealing with highly structured data with unified deep\nnetwork models. Having such models for compressing audio signals has been\nchallenging since it requires discrete representations that are not easy to\ntrain with end-to-end backpropagation. In this paper, we present an end-to-end\ndeep learning approach that combines recurrent neural networks (RNNs) within\nthe training strategy of variational autoencoders (VAEs) with a binary\nrepresentation of the latent space. We apply a reparametrization trick for the\nBernoulli distribution for the discrete representations, which allows smooth\nbackpropagation. In addition, our approach allows the separation of the encoder\nand decoder, which is necessary for compression tasks. To our best knowledge,\nthis is the first end-to-end learning for a single audio compression model with\nRNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.",
          "link": "http://arxiv.org/abs/2105.11681",
          "publishedOn": "2021-07-14T01:41:51.682Z",
          "wordCount": 611,
          "title": "Deep Neural Networks and End-to-End Learning for Audio Compression. (arXiv:2105.11681v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jiangeng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaoze Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Mengling Feng</a>",
          "description": "In this paper, we propose a deep residual network-based method, namely the\nDiCOVA-Net, to identify COVID-19 infected patients based on the acoustic\nrecording of their coughs. Since there are far more healthy people than\ninfected patients, this classification problem faces the challenge of\nimbalanced data. To improve the model's ability to recognize minority class\n(the infected patients), we introduce data augmentation and cost-sensitive\nmethods into our model. Besides, considering the particularity of this task, we\ndeploy some fine-tuning techniques to adjust the pre-training ResNet50.\nFurthermore, to improve the model's generalizability, we use ensemble learning\nto integrate prediction results from multiple base classifiers generated using\ndifferent random seeds. To evaluate the proposed DiCOVA-Net's performance, we\nconducted experiments with the DiCOVA challenge dataset. The results show that\nour method has achieved 85.43\\% in AUC, among the top of all competing teams.",
          "link": "http://arxiv.org/abs/2107.06126",
          "publishedOn": "2021-07-14T01:41:51.675Z",
          "wordCount": 637,
          "title": "DiCOVA-Net: Diagnosing COVID-19 using Acoustics based on Deep Residual Network for the DiCOVA Challenge 2021. (arXiv:2107.06126v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Chen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfrommer_T/0/1/0/all/0/1\">Timo Pfrommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1\">Marius Kloft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Maja Rudolph</a>",
          "description": "Data transformations (e.g. rotations, reflections, and cropping) play an\nimportant role in self-supervised learning. Typically, images are transformed\ninto different views, and neural networks trained on tasks involving these\nviews produce useful feature representations for downstream tasks, including\nanomaly detection. However, for anomaly detection beyond image data, it is\noften unclear which transformations to use. Here we present a simple end-to-end\nprocedure for anomaly detection with learnable transformations. The key idea is\nto embed the transformed data into a semantic space such that the transformed\ndata still resemble their untransformed form, while different transformations\nare easily distinguishable. Extensive experiments on time series demonstrate\nthat our proposed method outperforms existing approaches in the one-vs.-rest\nsetting and is competitive in the more challenging n-vs.-rest anomaly detection\ntask. On tabular datasets from the medical and cyber-security domains, our\nmethod learns domain-specific transformations and detects anomalies more\naccurately than previous work.",
          "link": "http://arxiv.org/abs/2103.16440",
          "publishedOn": "2021-07-14T01:41:51.658Z",
          "wordCount": 636,
          "title": "Neural Transformation Learning for Deep Anomaly Detection Beyond Images. (arXiv:2103.16440v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yatong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiaheng Wei</a>",
          "description": "We formulate the problem of induced domain adaptation (IDA) when the\nunderlying distribution/domain shift is introduced by the model being deployed.\nOur formulation is motivated by applications where the deployed machine\nlearning models interact with human agents, and will ultimately face responsive\nand interactive data distributions. We formalize the discussions of the\ntransferability of learning in our IDA setting by studying how the model\ntrained on the available source distribution (data) would translate to the\nperformance on the induced domain. We provide both upper bounds for the\nperformance gap due to the induced domain shift, as well as lower bound for the\ntrade-offs a classifier has to suffer on either the source training\ndistribution or the induced target distribution. We provide further\ninstantiated analysis for two popular domain adaptation settings with covariate\nshift and label shift. We highlight some key properties of IDA, as well as\ncomputational and learning challenges.",
          "link": "http://arxiv.org/abs/2107.05911",
          "publishedOn": "2021-07-14T01:41:51.651Z",
          "wordCount": 577,
          "title": "Induced Domain Adaptation. (arXiv:2107.05911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bozkir_E/0/1/0/all/0/1\">Efe Bozkir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1\">Ali Burak &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1\">Mete Akg&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1\">Nico Pfeifer</a>",
          "description": "Eye tracking is handled as one of the key technologies for applications that\nassess and evaluate human attention, behavior, and biometrics, especially using\ngaze, pupillary, and blink behaviors. One of the challenges with regard to the\nsocial acceptance of eye tracking technology is however the preserving of\nsensitive and personal information. To tackle this challenge, we employ a\nprivacy-preserving framework based on randomized encoding to train a Support\nVector Regression model using synthetic eye images privately to estimate the\nhuman gaze. During the computation, none of the parties learn about the data or\nthe result that any other party has. Furthermore, the party that trains the\nmodel cannot reconstruct pupil, blinks or visual scanpath. The experimental\nresults show that our privacy-preserving framework is capable of working in\nreal-time, with the same accuracy as compared to non-private version and could\nbe extended to other eye tracking related problems.",
          "link": "http://arxiv.org/abs/1911.07936",
          "publishedOn": "2021-07-14T01:41:51.644Z",
          "wordCount": 688,
          "title": "Privacy Preserving Gaze Estimation using Synthetic Images via a Randomized Encoding Based Framework. (arXiv:1911.07936v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigner_C/0/1/0/all/0/1\">Christina Aigner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>",
          "description": "Deep Neural Networks (DNNs) have an enormous potential to learn from complex\nbiomedical data. In particular, DNNs have been used to seamlessly fuse\nheterogeneous information from neuroanatomy, genetics, biomarkers, and\nneuropsychological tests for highly accurate Alzheimer's disease diagnosis. On\nthe other hand, their black-box nature is still a barrier for the adoption of\nsuch a system in the clinic, where interpretability is absolutely essential. We\npropose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for\nexplaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of\nthe neuroanatomy and tabular biomarkers. Our explanations are based on the\nShapley value, which is the unique method that satisfies all fundamental axioms\nfor local explanations previously established in the literature. Thus, SVEHNN\nhas many desirable characteristics that previous work on interpretability for\nmedical decision making is lacking. To avoid the exponential time complexity of\nthe Shapley value, we propose to transform a given DNN into a Lightweight\nProbabilistic Deep Network without re-training, thus achieving a complexity\nonly quadratic in the number of features. In our experiments on synthetic and\nreal data, we show that we can closely approximate the exact Shapley value with\na dramatically reduced runtime and can reveal the hidden knowledge the network\nhas learned from the data.",
          "link": "http://arxiv.org/abs/2107.05997",
          "publishedOn": "2021-07-14T01:41:51.636Z",
          "wordCount": 669,
          "title": "Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from Heterogeneous Data. (arXiv:2107.05997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cruttwell_G/0/1/0/all/0/1\">G.S.H. Cruttwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavranovic_B/0/1/0/all/0/1\">Bruno Gavranovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghani_N/0/1/0/all/0/1\">Neil Ghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_P/0/1/0/all/0/1\">Paul Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanasi_F/0/1/0/all/0/1\">Fabio Zanasi</a>",
          "description": "We propose a categorical semantics of gradient-based machine learning\nalgorithms in terms of lenses, parametrised maps, and reverse derivative\ncategories. This foundation provides a powerful explanatory and unifying\nframework: it encompasses a variety of gradient descent algorithms such as\nADAM, AdaGrad, and Nesterov momentum, as well as a variety of loss functions\nsuch as as MSE and Softmax cross-entropy, shedding new light on their\nsimilarities and differences. Our approach to gradient-based learning has\nexamples generalising beyond the familiar continuous domains (modelled in\ncategories of smooth maps) and can be realized in the discrete setting of\nboolean circuits. Finally, we demonstrate the practical significance of our\nframework with an implementation in Python.",
          "link": "http://arxiv.org/abs/2103.01931",
          "publishedOn": "2021-07-14T01:41:51.629Z",
          "wordCount": 575,
          "title": "Categorical Foundations of Gradient-Based Learning. (arXiv:2103.01931v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jianyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojima_I/0/1/0/all/0/1\">Iwao Ojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fusheng Wang</a>",
          "description": "Artificial intelligence (AI) has been transforming the practice of drug\ndiscovery in the past decade. Various AI techniques have been used in a wide\nrange of applications, such as virtual screening and drug design. In this\nsurvey, we first give an overview on drug discovery and discuss related\napplications, which can be reduced to two major tasks, i.e., molecular property\nprediction and molecule generation. We then discuss common data resources,\nmolecule representations and benchmark platforms. Furthermore, to summarize the\nprogress of AI in drug discovery, we present the relevant AI techniques\nincluding model architectures and learning paradigms in the papers surveyed. We\nexpect that this survey will serve as a guide for researchers who are\ninterested in working at the interface of artificial intelligence and drug\ndiscovery. We also provide a GitHub repository\n(https://github.com/dengjianyuan/Survey_AI_Drug_Discovery) with the collection\nof papers and codes, if applicable, as a learning resource, which is regularly\nupdated.",
          "link": "http://arxiv.org/abs/2106.05386",
          "publishedOn": "2021-07-14T01:41:51.613Z",
          "wordCount": 630,
          "title": "Artificial Intelligence in Drug Discovery: Applications and Techniques. (arXiv:2106.05386v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.01129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nabi_S/0/1/0/all/0/1\">Sareh Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassif_H/0/1/0/all/0/1\">Houssam Nassif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joseph Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamani_H/0/1/0/all/0/1\">Hamed Mamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imbens_G/0/1/0/all/0/1\">Guido Imbens</a>",
          "description": "Adding domain knowledge to a learning system is known to improve results. In\nmulti-parameter Bayesian frameworks, such knowledge is incorporated as a prior.\nOn the other hand, various model parameters can have different learning rates\nin real-world problems, especially with skewed data. Two often-faced challenges\nin Operation Management and Management Science applications are the absence of\ninformative priors, and the inability to control parameter learning rates. In\nthis study, we propose a hierarchical Empirical Bayes approach that addresses\nboth challenges, and that can generalize to any Bayesian framework. Our method\nlearns empirical meta-priors from the data itself and uses them to decouple the\nlearning rates of first-order and second-order features (or any other given\nfeature grouping) in a Generalized Linear Model. As the first-order features\nare likely to have a more pronounced effect on the outcome, focusing on\nlearning first-order weights first is likely to improve performance and\nconvergence time. Our Empirical Bayes method clamps features in each group\ntogether and uses the deployed model's observed data to empirically compute a\nhierarchical prior in hindsight. We report theoretical results for the\nunbiasedness, strong consistency, and optimal frequentist cumulative regret\nproperties of our meta-prior variance estimator. We apply our method to a\nstandard supervised learning optimization problem, as well as an online\ncombinatorial optimization problem in a contextual bandit setting implemented\nin an Amazon production system. Both during simulations and live experiments,\nour method shows marked improvements, especially in cases of small traffic. Our\nfindings are promising, as optimizing over sparse data is often a challenge.",
          "link": "http://arxiv.org/abs/2002.01129",
          "publishedOn": "2021-07-14T01:41:51.606Z",
          "wordCount": 741,
          "title": "Bayesian Meta-Prior Learning Using Empirical Bayes. (arXiv:2002.01129v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foti_S/0/1/0/all/0/1\">Simone Foti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1\">Bongjin Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowrick_T/0/1/0/all/0/1\">Thomas Dowrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalhinho_J/0/1/0/all/0/1\">Joao Ramalhinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allam_M/0/1/0/all/0/1\">Moustafa Allam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1\">Brian Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>",
          "description": "In this work we propose a method based on geometric deep learning to predict\nthe complete surface of the liver, given a partial point cloud of the organ\nobtained during the surgical laparoscopic procedure. We introduce a new data\naugmentation technique that randomly perturbs shapes in their frequency domain\nto compensate the limited size of our dataset. The core of our method is a\nvariational autoencoder (VAE) that is trained to learn a latent space for\ncomplete shapes of the liver. At inference time, the generative part of the\nmodel is embedded in an optimisation procedure where the latent representation\nis iteratively updated to generate a model that matches the intraoperative\npartial point cloud. The effect of this optimisation is a progressive non-rigid\ndeformation of the initially generated shape. Our method is qualitatively\nevaluated on real data and quantitatively evaluated on synthetic data. We\ncompared with a state-of-the-art rigid registration algorithm, that our method\noutperformed in visible areas.",
          "link": "http://arxiv.org/abs/2009.03871",
          "publishedOn": "2021-07-14T01:41:51.600Z",
          "wordCount": 640,
          "title": "Intraoperative Liver Surface Completion with Graph Convolutional VAE. (arXiv:2009.03871v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yeong-Dae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jinho Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byoungjip Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1\">Iljoo Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1\">Youngjune Gwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Seungjai Min</a>",
          "description": "In neural combinatorial optimization (CO), reinforcement learning (RL) can\nturn a deep neural net into a fast, powerful heuristic solver of NP-hard\nproblems. This approach has a great potential in practical applications because\nit allows near-optimal solutions to be found without expert guides armed with\nsubstantial domain knowledge. We introduce Policy Optimization with Multiple\nOptima (POMO), an end-to-end approach for building such a heuristic solver.\nPOMO is applicable to a wide range of CO problems. It is designed to exploit\nthe symmetries in the representation of a CO solution. POMO uses a modified\nREINFORCE algorithm that forces diverse rollouts towards all optimal solutions.\nEmpirically, the low-variance baseline of POMO makes RL training fast and\nstable, and it is more resistant to local minima compared to previous\napproaches. We also introduce a new augmentation-based inference method, which\naccompanies POMO nicely. We demonstrate the effectiveness of POMO by solving\nthree popular NP-hard problems, namely, traveling salesman (TSP), capacitated\nvehicle routing (CVRP), and 0-1 knapsack (KP). For all three, our solver based\non POMO shows a significant improvement in performance over all recent learned\nheuristics. In particular, we achieve the optimality gap of 0.14% with TSP100\nwhile reducing inference time by more than an order of magnitude.",
          "link": "http://arxiv.org/abs/2010.16011",
          "publishedOn": "2021-07-14T01:41:51.593Z",
          "wordCount": 682,
          "title": "POMO: Policy Optimization with Multiple Optima for Reinforcement Learning. (arXiv:2010.16011v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Saurabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhanson_J/0/1/0/all/0/1\">Joshua Zhanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisotto_E/0/1/0/all/0/1\">Emilio Parisotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Adarsh Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1\">Sivaraman Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>",
          "description": "Modern policy gradient algorithms such as Proximal Policy Optimization (PPO)\nrely on an arsenal of heuristics, including loss clipping and gradient\nclipping, to ensure successful learning. These heuristics are reminiscent of\ntechniques from robust statistics, commonly used for estimation in outlier-rich\n(``heavy-tailed'') regimes. In this paper, we present a detailed empirical\nstudy to characterize the heavy-tailed nature of the gradients of the PPO\nsurrogate reward function. We demonstrate that the gradients, especially for\nthe actor network, exhibit pronounced heavy-tailedness and that it increases as\nthe agent's policy diverges from the behavioral policy (i.e., as the agent goes\nfurther off policy). Further examination implicates the likelihood ratios and\nadvantages in the surrogate reward as the main sources of the observed\nheavy-tailedness. We then highlight issues arising due to the heavy-tailed\nnature of the gradients. In this light, we study the effects of the standard\nPPO clipping heuristics, demonstrating that these tricks primarily serve to\noffset heavy-tailedness in gradients. Thus motivated, we propose incorporating\nGMOM, a high-dimensional robust estimator, into PPO as a substitute for three\nclipping tricks. Despite requiring less hyperparameter tuning, our method\nmatches the performance of PPO (with all heuristics enabled) on a battery of\nMuJoCo continuous control tasks.",
          "link": "http://arxiv.org/abs/2102.10264",
          "publishedOn": "2021-07-14T01:41:51.586Z",
          "wordCount": 680,
          "title": "On Proximal Policy Optimization's Heavy-tailed Gradients. (arXiv:2102.10264v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Soumya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>",
          "description": "We propose ${\\tt AdaTS}$, a Thompson sampling algorithm that adapts\nsequentially to bandit tasks that it interacts with. The key idea in ${\\tt\nAdaTS}$ is to adapt to an unknown task prior distribution by maintaining a\ndistribution over its parameters. When solving a bandit task, that uncertainty\nis marginalized out and properly accounted for. ${\\tt AdaTS}$ is a\nfully-Bayesian algorithm that can be implemented efficiently in several classes\nof bandit problems. We derive upper bounds on its Bayes regret that quantify\nthe loss due to not knowing the task prior, and show that it is small. Our\ntheory is supported by experiments, where ${\\tt AdaTS}$ outperforms prior\nalgorithms and works well even in challenging real-world problems.",
          "link": "http://arxiv.org/abs/2107.06196",
          "publishedOn": "2021-07-14T01:41:51.568Z",
          "wordCount": 546,
          "title": "No Regrets for Learning the Prior in Bandits. (arXiv:2107.06196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1\">Christos Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1\">Kiran Vodrahalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_M/0/1/0/all/0/1\">Mihalis Yannakakis</a>",
          "description": "On-line firms deploy suites of software platforms, where each platform is\ndesigned to interact with users during a certain activity, such as browsing,\nchatting, socializing, emailing, driving, etc. The economic and incentive\nstructure of this exchange, as well as its algorithmic nature, have not been\nexplored to our knowledge. We model this interaction as a Stackelberg game\nbetween a Designer and one or more Agents. We model an Agent as a Markov chain\nwhose states are activities; we assume that the Agent's utility is a linear\nfunction of the steady-state distribution of this chain. The Designer may\ndesign a platform for each of these activities/states; if a platform is adopted\nby the Agent, the transition probabilities of the Markov chain are affected,\nand so is the objective of the Agent. The Designer's utility is a linear\nfunction of the steady state probabilities of the accessible states minus the\ndevelopment cost of the platforms. The underlying optimization problem of the\nAgent -- how to choose the states for which to adopt the platform -- is an MDP.\nIf this MDP has a simple yet plausible structure (the transition probabilities\nfrom one state to another only depend on the target state and the recurrent\nprobability of the current state) the Agent's problem can be solved by a greedy\nalgorithm. The Designer's optimization problem (designing a custom suite for\nthe Agent so as to optimize, through the Agent's optimum reaction, the\nDesigner's revenue), is NP-hard to approximate within any finite ratio;\nhowever, the special case, while still NP-hard, has an FPTAS. These results\ngeneralize from a single Agent to a distribution of Agents with finite support,\nas well as to the setting where the Designer must find the best response to the\nexisting strategies of other Designers. We discuss other implications of our\nresults and directions of future research.",
          "link": "http://arxiv.org/abs/2009.06117",
          "publishedOn": "2021-07-14T01:41:51.560Z",
          "wordCount": 783,
          "title": "The Platform Design Problem. (arXiv:2009.06117v2 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Jui Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>",
          "description": "In recent times, BERT based transformer models have become an inseparable\npart of the 'tech stack' of text processing models. Similar progress is being\nobserved in the speech domain with a multitude of models observing\nstate-of-the-art results by using audio transformer models to encode speech.\nThis begs the question of what are these audio transformer models learning.\nMoreover, although the standard methodology is to choose the last layer\nembedding for any downstream task, but is it the optimal choice? We try to\nanswer these questions for the two recent audio transformer models, Mockingjay\nand wave2vec2.0. We compare them on a comprehensive set of language delivery\nand structure features including audio, fluency and pronunciation features.\nAdditionally, we probe the audio models' understanding of textual surface,\nsyntax, and semantic features and compare them to BERT. We do this over\nexhaustive settings for native, non-native, synthetic, read and spontaneous\nspeech datasets",
          "link": "http://arxiv.org/abs/2101.00387",
          "publishedOn": "2021-07-14T01:41:51.552Z",
          "wordCount": 641,
          "title": "What all do audio transformer models hear? Probing Acoustic Representations for Language Delivery and its Structure. (arXiv:2101.00387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06104",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tajini_B/0/1/0/all/0/1\">Badr Tajini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Richard_H/0/1/0/all/0/1\">Hugo Richard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thirion_B/0/1/0/all/0/1\">Bertrand Thirion</a>",
          "description": "Advances in computational cognitive neuroimaging research are related to the\navailability of large amounts of labeled brain imaging data, but such data are\nscarce and expensive to generate. While powerful data generation mechanisms,\nsuch as Generative Adversarial Networks (GANs), have been designed in the last\ndecade for computer vision, such improvements have not yet carried over to\nbrain imaging. A likely reason is that GANs training is ill-suited to the\nnoisy, high-dimensional and small-sample data available in functional\nneuroimaging.In this paper, we introduce Conditional Independent Components\nAnalysis (Conditional ICA): a fast functional Magnetic Resonance Imaging (fMRI)\ndata augmentation technique, that leverages abundant resting-state data to\ncreate images by sampling from an ICA decomposition. We then propose a\nmechanism to condition the generator on classes observed with few samples. We\nfirst show that the generative mechanism is successful at synthesizing data\nindistinguishable from observations, and that it yields gains in classification\naccuracy in brain decoding problems. In particular it outperforms GANs while\nbeing much easier to optimize and interpret. Lastly, Conditional ICA enhances\nclassification accuracy in eight datasets without further parameters tuning.",
          "link": "http://arxiv.org/abs/2107.06104",
          "publishedOn": "2021-07-14T01:41:51.545Z",
          "wordCount": 631,
          "title": "Functional Magnetic Resonance Imaging data augmentation through conditional ICA. (arXiv:2107.06104v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.13988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hangfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "Algorithmic stability is a key characteristic to ensure the generalization\nability of a learning algorithm. Among different notions of stability,\n\\emph{uniform stability} is arguably the most popular one, which yields\nexponential generalization bounds. However, uniform stability only considers\nthe worst-case loss change (or so-called sensitivity) by removing a single data\npoint, which is distribution-independent and therefore undesirable. There are\nmany cases that the worst-case sensitivity of the loss is much larger than the\naverage sensitivity taken over the single data point that is removed,\nespecially in some advanced models such as random feature models or neural\nnetworks. Many previous works try to mitigate the distribution independent\nissue by proposing weaker notions of stability, however, they either only yield\npolynomial bounds or the bounds derived do not vanish as sample size goes to\ninfinity. Given that, we propose \\emph{locally elastic stability} as a weaker\nand distribution-dependent stability notion, which still yields exponential\ngeneralization bounds. We further demonstrate that locally elastic stability\nimplies tighter generalization bounds than those derived based on uniform\nstability in many situations by revisiting the examples of bounded support\nvector machines, regularized least square regressions, and stochastic gradient\ndescent.",
          "link": "http://arxiv.org/abs/2010.13988",
          "publishedOn": "2021-07-14T01:41:51.538Z",
          "wordCount": 660,
          "title": "Toward Better Generalization Bounds with Locally Elastic Stability. (arXiv:2010.13988v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yidong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>",
          "description": "Machine learning systems generally assume that the training and testing\ndistributions are the same. To this end, a key requirement is to develop models\nthat can generalize to unseen distributions. Domain generalization (DG), i.e.,\nout-of-distribution generalization, has attracted increasing interests in\nrecent years. Domain generalization deals with a challenging setting where one\nor several different but related domain(s) are given, and the goal is to learn\na model that can generalize to an unseen test domain. Great progress has been\nmade in the area of domain generalization for years. This paper presents the\nfirst review of recent advances in this area. First, we provide a formal\ndefinition of domain generalization and discuss several related fields. We then\nthoroughly review the theories related to domain generalization and carefully\nanalyze the theory behind generalization. We categorize recent algorithms into\nthree classes: data manipulation, representation learning, and learning\nstrategy, and present several popular algorithms in detail for each category.\nThird, we introduce the commonly used datasets and applications. Finally, we\nsummarize existing literature and present some potential research topics for\nthe future.",
          "link": "http://arxiv.org/abs/2103.03097",
          "publishedOn": "2021-07-14T01:41:51.520Z",
          "wordCount": 690,
          "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.08579",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lyu_X/0/1/0/all/0/1\">Xiong Lyu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ludkovski_M/0/1/0/all/0/1\">Mike Ludkovski</a>",
          "description": "We develop adaptive replicated designs for Gaussian process metamodels of\nstochastic experiments. Adaptive batching is a natural extension of sequential\ndesign heuristics with the benefit of replication growing as response features\nare learned, inputs concentrate, and the metamodeling overhead rises. Motivated\nby the problem of learning the level set of the mean simulator response we\ndevelop four novel schemes: Multi-Level Batching (MLB), Ratchet Batching (RB),\nAdaptive Batched Stepwise Uncertainty Reduction (ABSUR), Adaptive Design with\nStepwise Allocation (ADSA) and Deterministic Design with Stepwise Allocation\n(DDSA). Our algorithms simultaneously (MLB, RB and ABSUR) or sequentially (ADSA\nand DDSA) determine the sequential design inputs and the respective number of\nreplicates. Illustrations using synthetic examples and an application in\nquantitative finance (Bermudan option pricing via Regression Monte Carlo) show\nthat adaptive batching brings significant computational speed-ups with minimal\nloss of modeling fidelity.",
          "link": "http://arxiv.org/abs/2003.08579",
          "publishedOn": "2021-07-14T01:41:51.513Z",
          "wordCount": 600,
          "title": "Adaptive Batching for Gaussian Process Surrogates with Application in Noisy Level Set Estimation. (arXiv:2003.08579v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Mai Lan Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1\">Volker Blanz</a>",
          "description": "Discriminative features play an important role in image and object\nclassification and also in other fields of research such as semi-supervised\nlearning, fine-grained classification, out of distribution detection. Inspired\nby Linear Discriminant Analysis (LDA), we propose an optimization called Neural\nDiscriminant Analysis (NDA) for Deep Convolutional Neural Networks (DCNNs). NDA\ntransforms deep features to become more discriminative and, therefore, improves\nthe performances in various tasks. Our proposed optimization has two primary\ngoals for inter- and intra-class variances. The first one is to minimize\nvariances within each individual class. The second goal is to maximize pairwise\ndistances between features coming from different classes. We evaluate our NDA\noptimization in different research fields: general supervised classification,\nfine-grained classification, semi-supervised learning, and out of distribution\ndetection. We achieve performance improvements in all the fields compared to\nbaseline methods that do not use NDA. Besides, using NDA, we also surpass the\nstate of the art on the four tasks on various testing datasets.",
          "link": "http://arxiv.org/abs/2107.06209",
          "publishedOn": "2021-07-14T01:41:51.484Z",
          "wordCount": 602,
          "title": "Learning a Discriminant Latent Space with Neural Discriminant Analysis. (arXiv:2107.06209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05834",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoxiao Sun</a>",
          "description": "The divide-and-conquer method has been widely used for estimating large-scale\nkernel ridge regression estimates. Unfortunately, when the response variable is\nhighly skewed, the divide-and-conquer kernel ridge regression (dacKRR) may\noverlook the underrepresented region and result in unacceptable results. We\ndevelop a novel response-adaptive partition strategy to overcome the\nlimitation. In particular, we propose to allocate the replicates of some\ncarefully identified informative observations to multiple nodes (local\nprocessors). The idea is analogous to the popular oversampling technique.\nAlthough such a technique has been widely used for addressing discrete label\nskewness, extending it to the dacKRR setting is nontrivial. We provide both\ntheoretical and practical guidance on how to effectively over-sample the\nobservations under the dacKRR setting. Furthermore, we show the proposed\nestimate has a smaller asymptotic mean squared error (AMSE) than that of the\nclassical dacKRR estimate under mild conditions. Our theoretical findings are\nsupported by both simulated and real-data analyses.",
          "link": "http://arxiv.org/abs/2107.05834",
          "publishedOn": "2021-07-14T01:41:51.474Z",
          "wordCount": 579,
          "title": "Oversampling Divide-and-conquer for Response-skewed Kernel Ridge Regression. (arXiv:2107.05834v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.13620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sohini Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shalmali Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As predictive models are increasingly being deployed in high-stakes decision\nmaking (e.g., loan approvals), there has been growing interest in post hoc\ntechniques which provide recourse to affected individuals. These techniques\ngenerate recourses under the assumption that the underlying predictive model\ndoes not change. However, in practice, models are often regularly updated for a\nvariety of reasons (e.g., dataset shifts), thereby rendering previously\nprescribed recourses ineffective. To address this problem, we propose a novel\nframework, RObust Algorithmic Recourse (ROAR), that leverages adversarial\ntraining for finding recourses that are robust to model shifts. To the best of\nour knowledge, this work proposes the first solution to this critical problem.\nWe also carry out detailed theoretical analysis which underscores the\nimportance of constructing recourses that are robust to model shifts: 1) we\nderive a lower bound on the probability of invalidation of recourses generated\nby existing approaches which are not robust to model shifts. 2) we prove that\nthe additional cost incurred due to the robust recourses output by our\nframework is bounded. Experimental evaluation on multiple synthetic and\nreal-world datasets demonstrates the efficacy of the proposed framework and\nsupports our theoretical findings.",
          "link": "http://arxiv.org/abs/2102.13620",
          "publishedOn": "2021-07-14T01:41:51.451Z",
          "wordCount": 646,
          "title": "Towards Robust and Reliable Algorithmic Recourse. (arXiv:2102.13620v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yecheng Jason Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>",
          "description": "Many reinforcement learning (RL) problems in practice are offline, learning\npurely from observational data. A key challenge is how to ensure the learned\npolicy is safe, which requires quantifying the risk associated with different\nactions. In the online setting, distributional RL algorithms do so by learning\nthe distribution over returns (i.e., cumulative rewards) instead of the\nexpected return; beyond quantifying risk, they have also been shown to learn\nbetter representations for planning. We propose Conservative Offline\nDistributional Actor Critic (CODAC), an offline RL algorithm suitable for both\nrisk-neutral and risk-averse domains. CODAC adapts distributional RL to the\noffline setting by penalizing the predicted quantiles of the return for\nout-of-distribution actions. We prove that CODAC learns a conservative return\ndistribution -- in particular, for finite MDPs, CODAC converges to an uniform\nlower bound on the quantiles of the return distribution; our proof relies on a\nnovel analysis of the distributional Bellman operator. In our experiments, on\ntwo challenging robot navigation tasks, CODAC successfully learns risk-averse\npolicies using offline data collected purely from risk-neutral agents.\nFurthermore, CODAC is state-of-the-art on the D4RL MuJoCo benchmark in terms of\nboth expected and risk-sensitive performance.",
          "link": "http://arxiv.org/abs/2107.06106",
          "publishedOn": "2021-07-14T01:41:51.371Z",
          "wordCount": 614,
          "title": "Conservative Offline Distributional Reinforcement Learning. (arXiv:2107.06106v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.02627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verdoja_F/0/1/0/all/0/1\">Francesco Verdoja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>",
          "description": "Among the various options to estimate uncertainty in deep neural networks,\nMonte-Carlo dropout is widely popular for its simplicity and effectiveness.\nHowever the quality of the uncertainty estimated through this method varies and\nchoices in architecture design and in training procedures have to be carefully\nconsidered and tested to obtain satisfactory results. In this paper we present\na study offering a different point of view on the behavior of Monte-Carlo\ndropout, which enables us to observe a few interesting properties of the\ntechnique to keep in mind when considering its use for uncertainty estimation.",
          "link": "http://arxiv.org/abs/2008.02627",
          "publishedOn": "2021-07-14T01:41:51.354Z",
          "wordCount": 564,
          "title": "Notes on the Behavior of MC Dropout. (arXiv:2008.02627v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets.",
          "link": "http://arxiv.org/abs/2107.05948",
          "publishedOn": "2021-07-14T01:41:51.347Z",
          "wordCount": 681,
          "title": "On Designing Good Representation Learning Models. (arXiv:2107.05948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11619",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kehoe_A/0/1/0/all/0/1\">Aidan Kehoe</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wittek_P/0/1/0/all/0/1\">Peter Wittek</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xue_Y/0/1/0/all/0/1\">Yanbo Xue</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pozas_Kerstjens_A/0/1/0/all/0/1\">Alejandro Pozas-Kerstjens</a>",
          "description": "We provide a robust defence to adversarial attacks on discriminative\nalgorithms. Neural networks are naturally vulnerable to small, tailored\nperturbations in the input data that lead to wrong predictions. On the\ncontrary, generative models attempt to learn the distribution underlying a\ndataset, making them inherently more robust to small perturbations. We use\nBoltzmann machines for discrimination purposes as attack-resistant classifiers,\nand compare them against standard state-of-the-art adversarial defences. We\nfind improvements ranging from 5% to 72% against attacks with Boltzmann\nmachines on the MNIST dataset. We furthermore complement the training with\nquantum-enhanced sampling from the D-Wave 2000Q annealer, finding results\ncomparable with classical techniques and with marginal improvements in some\ncases. These results underline the relevance of probabilistic methods in\nconstructing neural networks and highlight a novel scenario of practical\nrelevance where quantum computers, even with limited hardware capabilites,\ncould provide advantages over classical computers. This work is dedicated to\nthe memory of Peter Wittek.",
          "link": "http://arxiv.org/abs/2012.11619",
          "publishedOn": "2021-07-14T01:41:51.338Z",
          "wordCount": 639,
          "title": "Defence against adversarial attacks using classical and quantum-enhanced Boltzmann machines. (arXiv:2012.11619v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1\">Ramon Manzorro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1\">Joshua L. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Binh Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1\">Dev Yashpal Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1\">Eero P. Simoncelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteson_D/0/1/0/all/0/1\">David S. Matteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1\">Peter A. Crozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>",
          "description": "Denoising is a fundamental challenge in scientific imaging. Deep\nconvolutional neural networks (CNNs) provide the current state of the art in\ndenoising natural images, where they produce impressive results. However, their\npotential has barely been explored in the context of scientific imaging.\nDenoising CNNs are typically trained on real natural images artificially\ncorrupted with simulated noise. In contrast, in scientific applications,\nnoiseless ground-truth images are usually not available. To address this issue,\nwe propose a simulation-based denoising (SBD) framework, in which CNNs are\ntrained on simulated images. We test the framework on data obtained from\ntransmission electron microscopy (TEM), an imaging technique with widespread\napplications in material science, biology, and medicine. SBD outperforms\nexisting techniques by a wide margin on a simulated benchmark dataset, as well\nas on real data. Apart from the denoised images, SBD generates likelihood maps\nto visualize the agreement between the structure of the denoised image and the\nobserved data. Our results reveal shortcomings of state-of-the-art denoising\narchitectures, such as their small field-of-view: substantially increasing the\nfield-of-view of the CNNs allows them to exploit non-local periodic patterns in\nthe data, which is crucial at high noise levels. In addition, we analyze the\ngeneralization capability of SBD, demonstrating that the trained networks are\nrobust to variations of imaging parameters and of the underlying signal\nstructure. Finally, we release the first publicly available benchmark dataset\nof TEM images, containing 18,000 examples.",
          "link": "http://arxiv.org/abs/2010.12970",
          "publishedOn": "2021-07-14T01:41:51.321Z",
          "wordCount": 750,
          "title": "Deep Denoising For Scientific Discovery: A Case Study In Electron Microscopy. (arXiv:2010.12970v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06264",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Kanishk/0/1/0/all/0/1\">Kanishk</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nandal_T/0/1/0/all/0/1\">Tanishk Nandal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tyagi_P/0/1/0/all/0/1\">Prince Tyagi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_R/0/1/0/all/0/1\">Raj Kumar Singh</a>",
          "description": "Autoencoders and generative neural network models have recently gained\npopularity in fluid mechanics due to their spontaneity and low processing time\ninstead of high fidelity CFD simulations. Auto encoders are used as model order\nreduction tools in applications of fluid mechanics by compressing input\nhigh-dimensional data using an encoder to map the input space into a\nlower-dimensional latent space. Whereas, generative models such as Variational\nAuto-encoders (VAEs) and Generative Adversarial Networks (GANs) are proving to\nbe effective in generating solutions to chaotic models with high 'randomness'\nsuch as turbulent flows. In this study, forced isotropic turbulence flow is\ngenerated by parameterizing into some basic statistical characteristics. The\nmodels trained on pre-simulated data from dependencies on these characteristics\nand the flow generation is then affected by varying these parameters. The\nlatent vectors pushed along the generator models like the decoders and\ngenerators contain independent entries which can be used to create different\noutputs with similar properties. The use of neural network-based architecture\nremoves the need for dependency on the classical mesh-based Navier-Stoke\nequation estimation which is prominent in many CFD softwares.",
          "link": "http://arxiv.org/abs/2107.06264",
          "publishedOn": "2021-07-14T01:41:51.313Z",
          "wordCount": 630,
          "title": "Parameterization of Forced Isotropic Turbulent Flow using Autoencoders and Generative Adversarial Networks. (arXiv:2107.06264v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1\">Shubham Dhayarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1\">V.K. Viekash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.",
          "link": "http://arxiv.org/abs/2107.06212",
          "publishedOn": "2021-07-14T01:41:51.306Z",
          "wordCount": 684,
          "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ehsan_M/0/1/0/all/0/1\">Md Amimul Ehsan</a>",
          "description": "Electricity generation from burning fossil fuels is one of the major\ncontributors to global warming. Renewable energy sources are a viable\nalternative to produce electrical energy and to reduce the emission from the\npower industry. These energy sources are the building blocks of green energy,\nwhich all have different characteristics. Their availabilities are also\ndiverse, depending on geographical locations and other parameters. Low\nimplementation cost and distributed availability all over the world uplifts\ntheir popularity exponentially. Therefore, it has unlocked opportunities for\nconsumers to produce electricity locally and use it on-site, which reduces\ndependency on centralized utility companies. The research considers two main\nobjectives: the prediction of wind speed that simplifies wind farm planning and\nfeasibility study. Secondly, the need to understand the dependency structure of\nthe wind speeds of multiple distant locations. To address the first objective,\ntwelve artificial intelligence algorithms were used for wind speed prediction\nfrom collected meteorological parameters. The model performances were compared\nto determine the wind speed prediction accuracy. The results show a deep\nlearning approach, long short-term memory (LSTM) outperforms other models with\nthe highest accuracy of 97.8%. For dependency, a multivariate cumulative\ndistribution function, Copula, was used to find the joint distribution of two\nor more distant location wind speeds, followed by a case study. We found that\nthe appropriate copula family and the parameters vary based on the distance in\nbetween. For the case study, Joe-Frank (BB8) copula shows an efficient joint\ndistribution fit for a wind speed pair with a standard error of 0.0094.\nFinally, some insights about the uncertainty aspects of wind speed dependency\nwere addressed.",
          "link": "http://arxiv.org/abs/2107.06182",
          "publishedOn": "2021-07-14T01:41:51.299Z",
          "wordCount": 740,
          "title": "Predictive models for wind speed using artificial intelligence and copula. (arXiv:2107.06182v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_D/0/1/0/all/0/1\">Dimitris Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Swayambhoo Jain</a>",
          "description": "Despite many modern applications of Deep Neural Networks (DNNs), the large\nnumber of parameters in the hidden layers makes them unattractive for\ndeployment on devices with storage capacity constraints. In this paper we\npropose a Data-Driven Low-rank (DDLR) method to reduce the number of parameters\nof pretrained DNNs and expedite inference by imposing low-rank structure on the\nfully connected layers, while controlling for the overall accuracy and without\nrequiring any retraining. We pose the problem as finding the lowest rank\napproximation of each fully connected layer with given performance guarantees\nand relax it to a tractable convex optimization problem. We show that it is\npossible to significantly reduce the number of parameters in common DNN\narchitectures with only a small reduction in classification accuracy. We\ncompare DDLR with Net-Trim, which is another data-driven DNN compression\ntechnique based on sparsity and show that DDLR consistently produces more\ncompressed neural networks while maintaining higher accuracy.",
          "link": "http://arxiv.org/abs/2107.05787",
          "publishedOn": "2021-07-14T01:41:51.293Z",
          "wordCount": 573,
          "title": "Data-Driven Low-Rank Neural Network Compression. (arXiv:2107.05787v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_M/0/1/0/all/0/1\">Mike A. Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>",
          "description": "While deep learning has revolutionized research and applications in NLP and\ncomputer vision, this has not yet been the case for behavioral modeling and\nbehavioral health applications. This is because the domain's datasets are\nsmaller, have heterogeneous datatypes, and typically exhibit a large degree of\nmissingness. Therefore, off-the-shelf deep learning models require significant,\noften prohibitive, adaptation. Accordingly, many research applications still\nrely on manually coded features with boosted tree models, sometimes with\ntask-specific features handcrafted by experts. Here, we address these\nchallenges by providing a neural architecture framework for mobile sensing data\nthat can learn generalizable feature representations from time series and\ndemonstrates the feasibility of transfer learning on small data domains through\nfinetuning. This architecture combines benefits from CNN and Trans-former\narchitectures to (1) enable better prediction performance by learning directly\nfrom raw minute-level sensor data without the need for handcrafted features by\nup to 0.33 ROC AUC, and (2) use pretraining to outperform simpler neural models\nand boosted decision trees with data from as few a dozen participants.",
          "link": "http://arxiv.org/abs/2107.06097",
          "publishedOn": "2021-07-14T01:41:51.286Z",
          "wordCount": 611,
          "title": "Transformer-Based Behavioral Representation Learning Enables Transfer Learning for Mobile Sensing in Small Datasets. (arXiv:2107.06097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.11117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carletti_M/0/1/0/all/0/1\">Mattia Carletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terzi_M/0/1/0/all/0/1\">Matteo Terzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susto_G/0/1/0/all/0/1\">Gian Antonio Susto</a>",
          "description": "Anomaly Detection is an unsupervised learning task aimed at detecting\nanomalous behaviours with respect to historical data. In particular,\nmultivariate Anomaly Detection has an important role in many applications\nthanks to the capability of summarizing the status of a complex system or\nobserved phenomenon with a single indicator (typically called `Anomaly Score')\nand thanks to the unsupervised nature of the task that does not require human\ntagging. The Isolation Forest is one of the most commonly adopted algorithms in\nthe field of Anomaly Detection, due to its proven effectiveness and low\ncomputational complexity. A major problem affecting Isolation Forest is\nrepresented by the lack of interpretability, an effect of the inherent\nrandomness governing the splits performed by the Isolation Trees, the building\nblocks of the Isolation Forest. In this paper we propose effective, yet\ncomputationally inexpensive, methods to define feature importance scores at\nboth global and local level for the Isolation Forest. Moreover, we define a\nprocedure to perform unsupervised feature selection for Anomaly Detection\nproblems based on our interpretability method; such procedure also serves the\npurpose of tackling the challenging task of feature importance evaluation in\nunsupervised anomaly detection. We assess the performance on several synthetic\nand real-world datasets, including comparisons against state-of-the-art\ninterpretability techniques, and make the code publicly available to enhance\nreproducibility and foster research in the field.",
          "link": "http://arxiv.org/abs/2007.11117",
          "publishedOn": "2021-07-14T01:41:51.280Z",
          "wordCount": 697,
          "title": "Interpretable Anomaly Detection with DIFFI: Depth-based Isolation Forest Feature Importance. (arXiv:2007.11117v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01724",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1\">Bingyan Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yan_Y/0/1/0/all/0/1\">Yuling Yan</a>",
          "description": "We investigate the effectiveness of convex relaxation and nonconvex\noptimization in solving bilinear systems of equations under two different\ndesigns (i.e.$~$a sort of random Fourier design and Gaussian design). Despite\nthe wide applicability, the theoretical understanding about these two paradigms\nremains largely inadequate in the presence of random noise. The current paper\nmakes two contributions by demonstrating that: (1) a two-stage nonconvex\nalgorithm attains minimax-optimal accuracy within a logarithmic number of\niterations. (2) convex relaxation also achieves minimax-optimal statistical\naccuracy vis-\\`a-vis random noise. Both results significantly improve upon the\nstate-of-the-art theoretical guarantees.",
          "link": "http://arxiv.org/abs/2008.01724",
          "publishedOn": "2021-07-14T01:41:51.273Z",
          "wordCount": 576,
          "title": "Convex and Nonconvex Optimization Are Both Minimax-Optimal for Noisy Blind Deconvolution under Random Designs. (arXiv:2008.01724v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Chuanqiang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_H/0/1/0/all/0/1\">Huiyun Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>",
          "description": "In recent years, the fast rise in number of studies on graph neural network\n(GNN) has put it from the theories research to reality application stage.\nDespite the encouraging performance achieved by GNN, less attention has been\npaid to the privacy-preserving training and inference over distributed graph\ndata in the related literature. Due to the particularity of graph structure, it\nis challenging to extend the existing private learning framework to GNN.\nMotivated by the idea of split learning, we propose a \\textbf{S}erver\n\\textbf{A}ided \\textbf{P}rivacy-preserving \\textbf{GNN} (SAPGNN) for the node\nlevel task on horizontally partitioned cross-silo scenario. It offers a natural\nextension of centralized GNN to isolated graph with max/min pooling\naggregation, while guaranteeing that all the private data involved in\ncomputation still stays at local data holders. To further enhancing the data\nprivacy, a secure pooling aggregation mechanism is proposed. Theoretical and\nexperimental results show that the proposed model achieves the same accuracy as\nthe one learned over the combined data.",
          "link": "http://arxiv.org/abs/2107.05917",
          "publishedOn": "2021-07-14T01:41:51.266Z",
          "wordCount": 596,
          "title": "Towards Representation Identical Privacy-Preserving Graph Neural Network via Split Learning. (arXiv:2107.05917v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "We study model-based offline Reinforcement Learning with general function\napproximation. We present an algorithm named Constrained Pessimistic Policy\nOptimization (CPPO) which leverages a general function class and uses a\nconstraint to encode pessimism. Under the assumption that the ground truth\nmodel belongs to our function class, CPPO can learn with the offline data only\nproviding partial coverage, i.e., it can learn a policy that completes against\nany policy that is covered by the offline data, in polynomial sample complexity\nwith respect to the statistical complexity of the function class. We then\ndemonstrate that this algorithmic framework can be applied to many specialized\nMarkov Decision Processes where the additional structural assumptions can\nfurther refine the concept of partial coverage. One notable example is low-rank\nMDP with representation learning where the partial coverage is defined using\nthe concept of relative condition number measured by the underlying unknown\nground truth feature representation. Finally, we introduce and study the\nBayesian setting in offline RL. The key benefit of Bayesian offline RL is that\nalgorithmically, we do not need to explicitly construct pessimism or reward\npenalty which could be hard beyond models with linear structures. We present a\nposterior sampling-based incremental policy optimization algorithm (PS-PO)\nwhich proceeds by iteratively sampling a model from the posterior distribution\nand performing one-step incremental policy optimization inside the sampled\nmodel. Theoretically, in expectation with respect to the prior distribution,\nPS-PO can learn a near optimal policy under partial coverage with polynomial\nsample complexity.",
          "link": "http://arxiv.org/abs/2107.06226",
          "publishedOn": "2021-07-14T01:41:51.239Z",
          "wordCount": 687,
          "title": "Pessimistic Model-based Offline RL: PAC Bounds and Posterior Sampling under Partial Coverage. (arXiv:2107.06226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breen_J/0/1/0/all/0/1\">Joe Breen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M. Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merwe_J/0/1/0/all/0/1\">Jacobus Van der Merwe</a>",
          "description": "Network traffic classification that is widely applicable and highly accurate\nis valuable for many network security and management tasks. A flexible and\neasily configurable classification framework is ideal, as it can be customized\nfor use in a wide variety of networks. In this paper, we propose a highly\nconfigurable and flexible machine learning traffic classification method that\nrelies only on statistics of sequences of packets to distinguish known, or\napproved, traffic from unknown traffic. Our method is based on likelihood\nestimation, provides a measure of certainty for classification decisions, and\ncan classify traffic at adjustable certainty levels. Our classification method\ncan also be applied in different classification scenarios, each prioritizing a\ndifferent classification goal. We demonstrate how our classification scheme and\nall its configurations perform well on real-world traffic from a high\nperformance computing network environment.",
          "link": "http://arxiv.org/abs/2107.06080",
          "publishedOn": "2021-07-14T01:41:51.227Z",
          "wordCount": 586,
          "title": "Practical and Configurable Network Traffic Classification Using Probabilistic Machine Learning. (arXiv:2107.06080v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06008",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Samuel_R/0/1/0/all/0/1\">Rikli Samuel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nico_B/0/1/0/all/0/1\">Bigler Daniel Nico</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moritz_P/0/1/0/all/0/1\">Pfenninger Moritz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Joerg_O/0/1/0/all/0/1\">Osterrieder Joerg</a>",
          "description": "Modeling financial time series is challenging due to their high volatility\nand unexpected happenings on the market. Most financial models and algorithms\ntrying to fill the lack of historical financial time series struggle to perform\nand are highly vulnerable to overfitting. As an alternative, we introduce in\nthis paper a deep neural network called the WGAN-GP, a data-driven model that\nfocuses on sample generation. The WGAN-GP consists of a generator and\ndiscriminator function which utilize an LSTM architecture. The WGAN-GP is\nsupposed to learn the underlying structure of the input data, which in our\ncase, is the Bitcoin. Bitcoin is unique in its behavior; the prices fluctuate\nwhat makes guessing the price trend hardly impossible. Through adversarial\ntraining, the WGAN-GP should learn the underlying structure of the bitcoin and\ngenerate very similar samples of the bitcoin distribution. The generated\nsynthetic time series are visually indistinguishable from the real data. But\nthe numerical results show that the generated data were close to the real data\ndistribution but distinguishable. The model mainly shows a stable learning\nbehavior. However, the model has space for optimization, which could be\nachieved by adjusting the hyperparameters.",
          "link": "http://arxiv.org/abs/2107.06008",
          "publishedOn": "2021-07-14T01:41:51.219Z",
          "wordCount": 629,
          "title": "Wasserstein GAN: Deep Generation applied on Bitcoins financial time series. (arXiv:2107.06008v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengsheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1\">Miguel Angel Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1\">Alex Colburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulbricht_D/0/1/0/all/0/1\">Daniel Ulbricht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1\">Qi Shan</a>",
          "description": "We study the problem of novel view synthesis of a scene comprised of 3D\nobjects. We propose a simple yet effective approach that is neither continuous\nnor implicit, challenging recent trends on view synthesis. We demonstrate that\nalthough continuous radiance field representations have gained a lot of\nattention due to their expressive power, our simple approach obtains comparable\nor even better novel view reconstruction quality comparing with\nstate-of-the-art baselines while increasing rendering speed by over 400x. Our\nmodel is trained in a category-agnostic manner and does not require\nscene-specific optimization. Therefore, it is able to generalize novel view\nsynthesis to object categories not seen during training. In addition, we show\nthat with our simple formulation, we can use view synthesis as a\nself-supervision signal for efficient learning of 3D geometry without explicit\n3D supervision.",
          "link": "http://arxiv.org/abs/2107.05775",
          "publishedOn": "2021-07-14T01:41:51.190Z",
          "wordCount": 581,
          "title": "Fast and Explicit Neural View Synthesis. (arXiv:2107.05775v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11436",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>",
          "description": "Despite remarkable success in a variety of applications, it is well-known\nthat deep learning can fail catastrophically when presented with\nout-of-distribution data. Toward addressing this challenge, we consider the\ndomain generalization problem, wherein predictors are trained using data drawn\nfrom a family of related training domains and then evaluated on a distinct and\nunseen test domain. We show that under a natural model of data generation and a\nconcomitant invariance condition, the domain generalization problem is\nequivalent to an infinite-dimensional constrained statistical learning problem;\nthis problem forms the basis of our approach, which we call Model-Based Domain\nGeneralization. Due to the inherent challenges in solving constrained\noptimization problems in deep learning, we exploit nonconvex duality theory to\ndevelop unconstrained relaxations of this statistical problem with tight bounds\non the duality gap. Based on this theoretical motivation, we propose a novel\ndomain generalization algorithm with convergence guarantees. In our\nexperiments, we report improvements of up to 30 percentage points over\nstate-of-the-art domain generalization baselines on several benchmarks\nincluding ColoredMNIST, Camelyon17-WILDS, FMoW-WILDS, and PACS.",
          "link": "http://arxiv.org/abs/2102.11436",
          "publishedOn": "2021-07-14T01:41:51.174Z",
          "wordCount": 625,
          "title": "Model-Based Domain Generalization. (arXiv:2102.11436v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03594",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zadorozhnyi_O/0/1/0/all/0/1\">Oleksandr Zadorozhnyi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gaillard_P/0/1/0/all/0/1\">Pierre Gaillard</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gerschinovitz_S/0/1/0/all/0/1\">Sebastien Gerschinovitz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "In this work we investigate the variation of the online kernelized ridge\nregression algorithm in the setting of $d-$dimensional adversarial\nnonparametric regression. We derive the regret upper bounds on the classes of\nSobolev spaces $W_{p}^{\\beta}(\\mathcal{X})$, $p\\geq 2, \\beta>\\frac{d}{p}$. The\nupper bounds are supported by the minimax regret analysis, which reveals that\nin the cases $\\beta> \\frac{d}{2}$ or $p=\\infty$ these rates are (essentially)\noptimal. Finally, we compare the performance of the kernelized ridge regression\nforecaster to the known non-parametric forecasters in terms of the regret rates\nand their computational complexity as well as to the excess risk rates in the\nsetting of statistical (i.i.d.) nonparametric regression.",
          "link": "http://arxiv.org/abs/2102.03594",
          "publishedOn": "2021-07-14T01:41:51.159Z",
          "wordCount": 566,
          "title": "Online nonparametric regression with Sobolev kernels. (arXiv:2102.03594v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziel_F/0/1/0/all/0/1\">Florian Ziel</a>",
          "description": "We present a winning method of the IEEE DataPort Competition on Day-Ahead\nElectricity Demand Forecasting: Post-COVID Paradigm. The day-ahead load\nforecasting approach is based on online forecast combination of multiple point\nprediction models. It contains four steps: i) data cleaning and preprocessing,\nii) a holiday adjustment procedure, iii) training of individual forecasting\nmodels, iv) forecast combination by smoothed Bernstein Online Aggregation\n(BOA). The approach is flexible and can quickly adopt to new energy system\nsituations as they occurred during and after COVID-19 shutdowns. The pool of\nindividual prediction models ranges from rather simple time series models to\nsophisticated models like generalized additive models (GAMs) and\nhigh-dimensional linear models estimated by lasso. They incorporate\nautoregressive, calendar and weather effects efficiently. All steps contain\nnovel concepts that contribute to the excellent forecasting performance of the\nproposed method. This holds particularly for the holiday adjustment procedure\nand the fully adaptive smoothed BOA approach.",
          "link": "http://arxiv.org/abs/2107.06268",
          "publishedOn": "2021-07-14T01:41:51.151Z",
          "wordCount": 654,
          "title": "Smoothed Bernstein Online Aggregation for Day-Ahead Electricity Demand Forecasting. (arXiv:2107.06268v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wen-Sheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>",
          "description": "We present Retrieve in Style (RIS), an unsupervised framework for\nfine-grained facial feature transfer and retrieval on real images. Recent work\nshows that it is possible to learn a catalog that allows local semantic\ntransfers of facial features on generated images by capitalizing on the\ndisentanglement property of the StyleGAN latent space. RIS improves existing\nart on: 1) feature disentanglement and allows for challenging transfers (i.e.,\nhair and pose) that were not shown possible in SoTA methods. 2) eliminating the\nneed for per-image hyperparameter tuning, and for computing a catalog over a\nlarge batch of images. 3) enabling face retrieval using the proposed facial\nfeatures (e.g., eyes), and to our best knowledge, is the first work to retrieve\nface images at the fine-grained level. 4) robustness and natural application to\nreal images. Our qualitative and quantitative analyses show RIS achieves both\nhigh-fidelity feature transfers and accurate fine-grained retrievals on real\nimages. We discuss the responsible application of RIS.",
          "link": "http://arxiv.org/abs/2107.06256",
          "publishedOn": "2021-07-14T01:41:51.144Z",
          "wordCount": 607,
          "title": "Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval. (arXiv:2107.06256v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junkun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Anpeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Runze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>",
          "description": "Instrumental variables (IVs), sources of treatment randomization that are\nconditionally independent of the outcome, play an important role in causal\ninference with unobserved confounders. However, the existing IV-based\ncounterfactual prediction methods need well-predefined IVs, while it's an art\nrather than science to find valid IVs in many real-world scenes. Moreover, the\npredefined hand-made IVs could be weak or erroneous by violating the conditions\nof valid IVs. These thorny facts hinder the application of the IV-based\ncounterfactual prediction methods. In this paper, we propose a novel Automatic\nInstrumental Variable decomposition (AutoIV) algorithm to automatically\ngenerate representations serving the role of IVs from observed variables (IV\ncandidates). Specifically, we let the learned IV representations satisfy the\nrelevance condition with the treatment and exclusion condition with the outcome\nvia mutual information maximization and minimization constraints, respectively.\nWe also learn confounder representations by encouraging them to be relevant to\nboth the treatment and the outcome. The IV and confounder representations\ncompete for the information with their constraints in an adversarial game,\nwhich allows us to get valid IV representations for IV-based counterfactual\nprediction. Extensive experiments demonstrate that our method generates valid\nIV representations for accurate IV-based counterfactual prediction.",
          "link": "http://arxiv.org/abs/2107.05884",
          "publishedOn": "2021-07-14T01:41:51.115Z",
          "wordCount": 638,
          "title": "Auto IV: Counterfactual Prediction via Automatic Instrumental Variable Decomposition. (arXiv:2107.05884v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04891",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Subhro Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>",
          "description": "This paper considers online optimal control with affine constraints on the\nstates and actions under linear dynamics with bounded random disturbances. The\nsystem dynamics and constraints are assumed to be known and time-invariant but\nthe convex stage cost functions change adversarially. To solve this problem, we\npropose Online Gradient Descent with Buffer Zones (OGD-BZ). Theoretically, we\nshow that OGD-BZ with proper parameters can guarantee the system to satisfy all\nthe constraints despite any admissible disturbances. Further, we investigate\nthe policy regret of OGD-BZ, which compares OGD-BZ's performance with the\nperformance of the optimal linear policy in hindsight. We show that OGD-BZ can\nachieve a policy regret upper bound that is the square root of the horizon\nlength multiplied by some logarithmic terms of the horizon length under proper\nalgorithm parameters.",
          "link": "http://arxiv.org/abs/2010.04891",
          "publishedOn": "2021-07-14T01:41:51.104Z",
          "wordCount": 593,
          "title": "Online Optimal Control with Affine Constraints. (arXiv:2010.04891v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Priya_S/0/1/0/all/0/1\">Shruti Priya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhadra_S/0/1/0/all/0/1\">Shubhankar Bhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimalakonda_S/0/1/0/all/0/1\">Sridhar Chimalakonda</a>",
          "description": "Today, Machine Learning (ML) is of a great importance to society due to the\navailability of huge data and high computational resources. This ultimately led\nto the introduction of ML concepts at multiple levels of education including\nK-12 students to promote computational thinking. However, teaching these\nconcepts to K-12 through traditional methodologies such as video lectures and\nbooks is challenging. Many studies in the literature have reported that using\ninteractive environments such as games to teach computational thinking and\nprogramming improves retention capacity and motivation among students.\nTherefore, introducing ML concepts using a game might enhance students'\nunderstanding of the subject and motivate them to learn further. However, we\nare not aware of any existing game which explicitly focuses on introducing ML\nconcepts to students using game play. Hence, in this paper, we propose\nML-Quest, a 3D video game to provide conceptual overview of three ML concepts:\nSupervised Learning, Gradient Descent and K-Nearest Neighbor (KNN)\nClassification. The crux of the game is to introduce the definition and working\nof these concepts, which we call conceptual overview, in a simulated scenario\nwithout overwhelming students with the intricacies of ML. The game has been\npredominantly evaluated for its usefulness and player experience using the\nTechnology Acceptance Model (TAM) model with the help of 23 higher-secondary\nschool students. The survey result shows that around 70% of the participants\neither agree or strongly agree that the ML-Quest is quite interactive and\nuseful in introducing them to ML concepts.",
          "link": "http://arxiv.org/abs/2107.06206",
          "publishedOn": "2021-07-14T01:41:51.097Z",
          "wordCount": 693,
          "title": "ML-Quest: A Game for Introducing Machine Learning Concepts to K-12 Students. (arXiv:2107.06206v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11046",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bauer_M/0/1/0/all/0/1\">Matthias Bauer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mnih_A/0/1/0/all/0/1\">Andriy Mnih</a>",
          "description": "Efficient low-variance gradient estimation enabled by the reparameterization\ntrick (RT) has been essential to the success of variational autoencoders.\nDoubly-reparameterized gradients (DReGs) improve on the RT for multi-sample\nvariational bounds by applying reparameterization a second time for an\nadditional reduction in variance. Here, we develop two generalizations of the\nDReGs estimator and show that they can be used to train conditional and\nhierarchical VAEs on image modelling tasks more effectively. First, we extend\nthe estimator to hierarchical models with several stochastic layers by showing\nhow to treat additional score function terms due to the hierarchical\nvariational posterior. We then generalize DReGs to score functions of arbitrary\ndistributions instead of just those of the sampling distribution, which makes\nthe estimator applicable to the parameters of the prior in addition to those of\nthe posterior.",
          "link": "http://arxiv.org/abs/2101.11046",
          "publishedOn": "2021-07-14T01:41:51.090Z",
          "wordCount": 584,
          "title": "Generalized Doubly Reparameterized Gradient Estimators. (arXiv:2101.11046v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06174",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Juyong Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_Y/0/1/0/all/0/1\">Youngsang Cho</a>",
          "description": "As the volatility of electricity demand increases owing to climate change and\nelectrification, the importance of accurate peak load forecasting is\nincreasing. Traditional peak load forecasting has been conducted through time\nseries-based models; however, recently, new models based on machine or deep\nlearning are being introduced. This study performs a comparative analysis to\ndetermine the most accurate peak load-forecasting model for Korea, by comparing\nthe performance of time series, machine learning, and hybrid models. Seasonal\nautoregressive integrated moving average with exogenous variables (SARIMAX) is\nused for the time series model. Artificial neural network (ANN), support vector\nregression (SVR), and long short-term memory (LSTM) are used for the machine\nlearning models. SARIMAX-ANN, SARIMAX-SVR, and SARIMAX-LSTM are used for the\nhybrid models. The results indicate that the hybrid models exhibit significant\nimprovement over the SARIMAX model. The LSTM-based models outperformed the\nothers; the single and hybrid LSTM models did not exhibit a significant\nperformance difference. In the case of Korea's highest peak load in 2019, the\npredictive power of the LSTM model proved to be greater than that of the\nSARIMAX-LSTM model. The LSTM, SARIMAX-SVR, and SARIMAX-LSTM models outperformed\nthe current time series-based forecasting model used in Korea. Thus, Korea's\npeak load-forecasting performance can be improved by including machine learning\nor hybrid models.",
          "link": "http://arxiv.org/abs/2107.06174",
          "publishedOn": "2021-07-14T01:41:51.082Z",
          "wordCount": 660,
          "title": "National-scale electricity peak load forecasting: Traditional, machine learning, or hybrid model?. (arXiv:2107.06174v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafar_H/0/1/0/all/0/1\">Hammad Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utkovski_Z/0/1/0/all/0/1\">Zoran Utkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasparick_M/0/1/0/all/0/1\">Martin Kasparick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_S/0/1/0/all/0/1\">Slawomir Stanczak</a>",
          "description": "This paper addresses the problem of decentralized spectrum sharing in\nvehicle-to-everything (V2X) communication networks. The aim is to provide\nresource-efficient coexistence of vehicle-to-infrastructure(V2I) and\nvehicle-to-vehicle(V2V) links. A recent work on the topic proposes a\nmulti-agent reinforcement learning (MARL) approach based on deep Q-learning,\nwhich leverages a fingerprint-based deep Q-network (DQN) architecture. This\nwork considers an extension of this framework by combining Double Q-learning\n(via Double DQN) and transfer learning. The motivation behind is that Double\nQ-learning can alleviate the problem of overestimation of the action values\npresent in conventional Q-learning, while transfer learning can leverage\nknowledge acquired by an expert model to accelerate learning in the MARL\nsetting. The proposed algorithm is evaluated in a realistic V2X setting, with\nsynthetic data generated based on a geometry-based propagation model that\nincorporates location-specific geographical descriptors of the simulated\nenvironment(outlines of buildings, foliage, and vehicles). The advantages of\nthe proposed approach are demonstrated via numerical simulations.",
          "link": "http://arxiv.org/abs/2107.06195",
          "publishedOn": "2021-07-14T01:41:51.075Z",
          "wordCount": 614,
          "title": "Transfer Learning in Multi-Agent Reinforcement Learning with Double Q-Networks for Distributed Resource Sharing in V2X Communication. (arXiv:2107.06195v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoxin Liu</a>",
          "description": "Domain generalization (DG) aims to help models trained on a set of source\ndomains generalize better on unseen target domains. The performances of current\nDG methods largely rely on sufficient labeled data, which however are usually\ncostly or unavailable. While unlabeled data are far more accessible, we seek to\nexplore how unsupervised learning can help deep models generalizes across\ndomains. Specifically, we study a novel generalization problem called\nunsupervised domain generalization, which aims to learn generalizable models\nwith unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised\nLearning (DIUL) method to cope with the significant and misleading\nheterogeneity within unlabeled data and severe distribution shifts between\nsource and target data. Surprisingly we observe that DIUL can not only\ncounterbalance the scarcity of labeled data but also further strengthen the\ngeneralization ability of models when the labeled data are sufficient. As a\npretraining approach, DIUL shows superior to ImageNet pretraining protocol even\nwhen the available data are unlabeled and of a greatly smaller amount compared\nto ImageNet. Extensive experiments clearly demonstrate the effectiveness of our\nmethod compared with state-of-the-art unsupervised learning counterparts.",
          "link": "http://arxiv.org/abs/2107.06219",
          "publishedOn": "2021-07-14T01:41:51.056Z",
          "wordCount": 625,
          "title": "Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization. (arXiv:2107.06219v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Mai Lan Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1\">Volker Blanz</a>",
          "description": "We propose a simple modification from a fixed margin triplet loss to an\nadaptive margin triplet loss. While the original triplet loss is used widely in\nclassification problems such as face recognition, face re-identification and\nfine-grained similarity, our proposed loss is well suited for rating datasets\nin which the ratings are continuous values. In contrast to original triplet\nloss where we have to sample data carefully, in out method, we can generate\ntriplets using the whole dataset, and the optimization can still converge\nwithout frequently running into a model collapsing issue. The adaptive margins\nonly need to be computed once before the training, which is much less expensive\nthan generating triplets after every epoch as in the fixed margin case. Besides\nsubstantially improved training stability (the proposed model never collapsed\nin our experiments compared to a couple of times that the training collapsed on\nexisting triplet loss), we achieved slightly better performance than the\noriginal triplet loss on various rating datasets and network architectures.",
          "link": "http://arxiv.org/abs/2107.06187",
          "publishedOn": "2021-07-14T01:41:51.047Z",
          "wordCount": 599,
          "title": "Deep Ranking with Adaptive Margin Triplet Loss. (arXiv:2107.06187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leng_G/0/1/0/all/0/1\">Guangjie Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yeku Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>",
          "description": "Empirical works suggest that various semantics emerge in the latent space of\nGenerative Adversarial Networks (GANs) when being trained to generate images.\nTo perform real image editing, it requires an accurate mapping from the real\nimage to the latent space to leveraging these learned semantics, which is\nimportant yet difficult. An in-domain GAN inversion approach is recently\nproposed to constraint the inverted code within the latent space by forcing the\nreconstructed image obtained from the inverted code within the real image\nspace. Empirically, we find that the inverted code by the in-domain GAN can\ndeviate from the latent space significantly. To solve this problem, we propose\na force-in-domain GAN based on the in-domain GAN, which utilizes a\ndiscriminator to force the inverted code within the latent space. The\nforce-in-domain GAN can also be interpreted by a cycle-GAN with slight\nmodification. Extensive experiments show that our force-in-domain GAN not only\nreconstructs the target image at the pixel level, but also align the inverted\ncode with the latent space well for semantic editing.",
          "link": "http://arxiv.org/abs/2107.06050",
          "publishedOn": "2021-07-14T01:41:51.041Z",
          "wordCount": 606,
          "title": "Force-in-domain GAN inversion. (arXiv:2107.06050v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davis_Q/0/1/0/all/0/1\">Q. Tyrell Davis</a>",
          "description": "This paper is both an introduction and an invitation. It is an introduction\nto CARLE, a Life-like cellular automata simulator and reinforcement learning\nenvironment. It is also an invitation to Carle's Game, a challenge in\nopen-ended machine exploration and creativity. Inducing machine agents to excel\nat creating interesting patterns across multiple cellular automata universes is\na substantial challenge, and approaching this challenge is likely to require\ncontributions from the fields of artificial life, AI, machine learning, and\ncomplexity, at multiple levels of interest. Carle's Game is based on machine\nagent interaction with CARLE, a Cellular Automata Reinforcement Learning\nEnvironment. CARLE is flexible, capable of simulating any of the 262,144\ndifferent rules defining Life-like cellular automaton universes. CARLE is also\nfast and can simulate automata universes at a rate of tens of thousands of\nsteps per second through a combination of vectorization and GPU acceleration.\nFinally, CARLE is simple. Compared to high-fidelity physics simulators and\nvideo games designed for human players, CARLE's two-dimensional grid world\noffers a discrete, deterministic, and atomic universal playground, despite its\ncomplexity. In combination with CARLE, Carle's Game offers an initial set of\nagent policies, learning and meta-learning algorithms, and reward wrappers that\ncan be tailored to encourage exploration or specific tasks.",
          "link": "http://arxiv.org/abs/2107.05786",
          "publishedOn": "2021-07-14T01:41:51.034Z",
          "wordCount": 652,
          "title": "Carle's Game: An Open-Ended Challenge in Exploratory Machine Creativity. (arXiv:2107.05786v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Olivan_C/0/1/0/all/0/1\">Carlos Hernandez-Olivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltran_J/0/1/0/all/0/1\">Jose R. Beltran</a>",
          "description": "The aim of this work is to define a model based on deep learning that is able\nto identify different instrument timbres with as few parameters as possible.\nFor this purpose, we have worked with classical orchestral instruments played\nwith different dynamics, which are part of a few instrument families and which\nplay notes in the same pitch range. It has been possible to assess the ability\nto classify instruments by timbre even if the instruments are playing the same\nnote with the same intensity. The network employed uses a multi-head attention\nmechanism, with 8 heads and a dense network at the output taking as input the\nlog-mel magnitude spectrograms of the sound samples. This network allows the\nidentification of 20 instrument classes of the classical orchestra, achieving\nan overall F$_1$ value of 0.62. An analysis of the weights of the attention\nlayer has been performed and the confusion matrix of the model is presented,\nallowing us to assess the ability of the proposed architecture to distinguish\ntimbre and to establish the aspects on which future work should focus.",
          "link": "http://arxiv.org/abs/2107.06231",
          "publishedOn": "2021-07-14T01:41:51.027Z",
          "wordCount": 622,
          "title": "Timbre Classification of Musical Instruments with a Deep Learning Multi-Head Attention-Based Model. (arXiv:2107.06231v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belghazi_M/0/1/0/all/0/1\">Mohamed Ishmael Belghazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Paz_D/0/1/0/all/0/1\">David Lopez-Paz</a>",
          "description": "Being uncertain when facing the unknown is key to intelligent decision\nmaking. However, machine learning algorithms lack reliable estimates about\ntheir predictive uncertainty. This leads to wrong and overly-confident\ndecisions when encountering classes unseen during training. Despite the\nimportance of equipping classifiers with uncertainty estimates ready for the\nreal world, prior work has focused on small datasets and little or no class\ndiscrepancy between training and testing data. To close this gap, we introduce\nUIMNET: a realistic, ImageNet-scale test-bed to evaluate predictive uncertainty\nestimates for deep image classifiers. Our benchmark provides implementations of\neight state-of-the-art algorithms, six uncertainty measures, four in-domain\nmetrics, three out-domain metrics, and a fully automated pipeline to train,\ncalibrate, ensemble, select, and evaluate models. Our test-bed is open-source\nand all of our results are reproducible from a fixed commit in our repository.\nAdding new datasets, algorithms, measures, or metrics is a matter of a few\nlines of code-in so hoping that UIMNET becomes a stepping stone towards\nrealistic, rigorous, and reproducible research in uncertainty estimation. Our\nresults show that ensembles of ERM classifiers as well as single MIMO\nclassifiers are the two best alternatives currently available to measure\nuncertainty about both in-domain and out-domain classes.",
          "link": "http://arxiv.org/abs/2107.06217",
          "publishedOn": "2021-07-14T01:41:51.008Z",
          "wordCount": 630,
          "title": "What classifiers know what they don't?. (arXiv:2107.06217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quinones_M/0/1/0/all/0/1\">Miguel Paredes Qui&#xf1;ones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zortea_M/0/1/0/all/0/1\">Maciel Zortea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_L/0/1/0/all/0/1\">Leonardo S. A. Martins</a>",
          "description": "Streamflow forecasting is key to effectively managing water resources and\npreparing for the occurrence of natural calamities being exacerbated by climate\nchange. Here we use the concept of fast and slow flow components to create a\nnew mass-conserving Long Short-Term Memory (LSTM) neural network model. It uses\nhydrometeorological time series and catchment attributes to predict daily river\ndischarges. Preliminary results evidence improvement in skills for different\nscores compared to the recent literature.",
          "link": "http://arxiv.org/abs/2107.06057",
          "publishedOn": "2021-07-14T01:41:50.959Z",
          "wordCount": 528,
          "title": "Fast-Slow Streamflow Model Using Mass-Conserving LSTM. (arXiv:2107.06057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06115",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhenning Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guohui Zhang</a>",
          "description": "Inefficient traffic signal control methods may cause numerous problems, such\nas traffic congestion and waste of energy. Reinforcement learning (RL) is a\ntrending data-driven approach for adaptive traffic signal control in complex\nurban traffic networks. Although the development of deep neural networks (DNN)\nfurther enhances its learning capability, there are still some challenges in\napplying deep RLs to transportation networks with multiple signalized\nintersections, including non-stationarity environment, exploration-exploitation\ndilemma, multi-agent training schemes, continuous action spaces, etc. In order\nto address these issues, this paper first proposes a multi-agent deep\ndeterministic policy gradient (MADDPG) method by extending the actor-critic\npolicy gradient algorithms. MADDPG has a centralized learning and decentralized\nexecution paradigm in which critics use additional information to streamline\nthe training process, while actors act on their own local observations. The\nmodel is evaluated via simulation on the Simulation of Urban MObility (SUMO)\nplatform. Model comparison results show the efficiency of the proposed\nalgorithm in controlling traffic lights.",
          "link": "http://arxiv.org/abs/2107.06115",
          "publishedOn": "2021-07-14T01:41:50.951Z",
          "wordCount": 599,
          "title": "A Deep Reinforcement Learning Approach for Traffic Signal Control Optimization. (arXiv:2107.06115v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06099",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1\">Haiyang Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhou_G/0/1/0/all/0/1\">Guangyu Zhou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_S/0/1/0/all/0/1\">Siqi Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jiang_J/0/1/0/all/0/1\">Jyun-Yu Jiang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Motivation: Predicting Drug-Target Interaction (DTI) is a well-studied topic\nin bioinformatics due to its relevance in the fields of proteomics and\npharmaceutical research. Although many machine learning methods have been\nsuccessfully applied in this task, few of them aim at leveraging the inherent\nheterogeneous graph structure in the DTI network to address the challenge. For\nbetter learning and interpreting the DTI topological structure and the\nsimilarity, it is desirable to have methods specifically for predicting\ninteractions from the graph structure.\n\nResults: We present an end-to-end framework, DTI-GAT (Drug-Target Interaction\nprediction with Graph Attention networks) for DTI predictions. DTI-GAT\nincorporates a deep neural network architecture that operates on\ngraph-structured data with the attention mechanism, which leverages both the\ninteraction patterns and the features of drug and protein sequences. DTI-GAT\nfacilitates the interpretation of the DTI topological structure by assigning\ndifferent attention weights to each node with the self-attention mechanism.\nExperimental evaluations show that DTI-GAT outperforms various state-of-the-art\nsystems on the binary DTI prediction problem. Moreover, the independent study\nresults further demonstrate that our model can be generalized better than other\nconventional methods.\n\nAvailability: The source code and all datasets are available at\nhttps://github.com/Haiyang-W/DTI-GRAPH",
          "link": "http://arxiv.org/abs/2107.06099",
          "publishedOn": "2021-07-14T01:41:50.945Z",
          "wordCount": 632,
          "title": "Drug-Target Interaction Prediction with Graph Attention networks. (arXiv:2107.06099v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_C/0/1/0/all/0/1\">Chikara Nakamura</a>",
          "description": "Extreme value theory (EVT) is a statistical tool for analysis of extreme\nevents. It has a strong theoretical background, however, we need to choose\nhyper-parameters\n\nto apply EVT. In recent studies of machine learning, techniques of choosing\nhyper-parameters have been well-studied. In this paper, we propose a new method\nof choosing hyper-parameters in EVT based on machine learning techniques. We\nalso experiment our method to real-world data and show good usability of our\nmethod.",
          "link": "http://arxiv.org/abs/2107.06074",
          "publishedOn": "2021-07-14T01:41:50.938Z",
          "wordCount": 508,
          "title": "On Choice of Hyper-parameter in Extreme Value Theory based on Machine Learning Techniques. (arXiv:2107.06074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahim Shahriar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1\">Mueeze Al Mushabbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>",
          "description": "Chatbots are intelligent software built to be used as a replacement for human\ninteraction. However, existing studies typically do not provide enough support\nfor low-resource languages like Bangla. Moreover, due to the increasing\npopularity of social media, we can also see the rise of interactions in Bangla\ntransliteration (mostly in English) among the native Bangla speakers. In this\npaper, we propose a novel approach to build a Bangla chatbot aimed to be used\nas a business assistant which can communicate in Bangla and Bangla\nTransliteration in English with high confidence consistently. Since annotated\ndata was not available for this purpose, we had to work on the whole machine\nlearning life cycle (data preparation, machine learning modeling, and model\ndeployment) using Rasa Open Source Framework, fastText embeddings, Polyglot\nembeddings, Flask, and other systems as building blocks. While working with the\nskewed annotated dataset, we try out different setups and pipelines to evaluate\nwhich works best and provide possible reasoning behind the observed results.\nFinally, we present a pipeline for intent classification and entity extraction\nwhich achieves reasonable performance (accuracy: 83.02\\%, precision: 80.82\\%,\nrecall: 83.02\\%, F1-score: 80\\%).",
          "link": "http://arxiv.org/abs/2107.05541",
          "publishedOn": "2021-07-14T01:41:50.921Z",
          "wordCount": 652,
          "title": "End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05847",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Binder_M/0/1/0/all/0/1\">Martin Binder</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Lang_M/0/1/0/all/0/1\">Michel Lang</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Pielok_T/0/1/0/all/0/1\">Tobias Pielok</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Richter_J/0/1/0/all/0/1\">Jakob Richter</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Coors_S/0/1/0/all/0/1\">Stefan Coors</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Thomas_J/0/1/0/all/0/1\">Janek Thomas</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Ullmann_T/0/1/0/all/0/1\">Theresa Ullmann</a> (2), <a href=\"http://arxiv.org/find/stat/1/au:+Becker_M/0/1/0/all/0/1\">Marc Becker</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Boulesteix_A/0/1/0/all/0/1\">Anne-Laure Boulesteix</a> (2), <a href=\"http://arxiv.org/find/stat/1/au:+Deng_D/0/1/0/all/0/1\">Difan Deng</a> (3), <a href=\"http://arxiv.org/find/stat/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a> (3) ((1) Department of Statistics, Ludwig Maximilian University Munich, (2) Institute for Medical Information Processing, Biometry and Epidemiology, Ludwig Maximilian University Munich, (3) Institute for Information Processing, Leibniz University Hannover)",
          "description": "Most machine learning algorithms are configured by one or several\nhyperparameters that must be carefully chosen and often considerably impact\nperformance. To avoid a time consuming and unreproducible manual\ntrial-and-error process to find well-performing hyperparameter configurations,\nvarious automatic hyperparameter optimization (HPO) methods, e.g., based on\nresampling error estimation for supervised machine learning, can be employed.\nAfter introducing HPO from a general perspective, this paper reviews important\nHPO methods such as grid or random search, evolutionary algorithms, Bayesian\noptimization, Hyperband and racing. It gives practical recommendations\nregarding important choices to be made when conducting HPO, including the HPO\nalgorithms themselves, performance evaluation, how to combine HPO with ML\npipelines, runtime improvements, and parallelization.",
          "link": "http://arxiv.org/abs/2107.05847",
          "publishedOn": "2021-07-14T01:41:50.914Z",
          "wordCount": 654,
          "title": "Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges. (arXiv:2107.05847v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Han Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Feng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1\">Marcus Eng Hock Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yilin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chee_M/0/1/0/all/0/1\">Marcel Lucas Chee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_S/0/1/0/all/0/1\">Seyed Ehsan Saffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1\">Hairil Rizal Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_B/0/1/0/all/0/1\">Benjamin Alan Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1\">Bibhas Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>",
          "description": "Background: Medical decision-making impacts both individual and public\nhealth. Clinical scores are commonly used among a wide variety of\ndecision-making models for determining the degree of disease deterioration at\nthe bedside. AutoScore was proposed as a useful clinical score generator based\non machine learning and a generalized linear model. Its current framework,\nhowever, still leaves room for improvement when addressing unbalanced data of\nrare events. Methods: Using machine intelligence approaches, we developed\nAutoScore-Imbalance, which comprises three components: training dataset\noptimization, sample weight optimization, and adjusted AutoScore. All scoring\nmodels were evaluated on the basis of their area under the curve (AUC) in the\nreceiver operating characteristic analysis and balanced accuracy (i.e., mean\nvalue of sensitivity and specificity). By utilizing a publicly accessible\ndataset from Beth Israel Deaconess Medical Center, we assessed the proposed\nmodel and baseline approaches in the prediction of inpatient mortality.\nResults: AutoScore-Imbalance outperformed baselines in terms of AUC and\nbalanced accuracy. The nine-variable AutoScore-Imbalance sub-model achieved the\nhighest AUC of 0.786 (0.732-0.839) while the eleven-variable original AutoScore\nobtained an AUC of 0.723 (0.663-0.783), and the logistic regression with 21\nvariables obtained an AUC of 0.743 (0.685-0.800). The AutoScore-Imbalance\nsub-model (using down-sampling algorithm) yielded an AUC of 0. 0.771\n(0.718-0.823) with only five variables, demonstrating a good balance between\nperformance and variable sparsity. Conclusions: The AutoScore-Imbalance tool\nhas the potential to be applied to highly unbalanced datasets to gain further\ninsight into rare medical events and to facilitate real-world clinical\ndecision-making.",
          "link": "http://arxiv.org/abs/2107.06039",
          "publishedOn": "2021-07-14T01:41:50.902Z",
          "wordCount": 703,
          "title": "AutoScore-Imbalance: An interpretable machine learning tool for development of clinical scores with rare events data. (arXiv:2107.06039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Derakhshani_M/0/1/0/all/0/1\">Mohammad Mahdi Derakhshani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "This paper introduces kernel continual learning, a simple but effective\nvariant of continual learning that leverages the non-parametric nature of\nkernel methods to tackle catastrophic forgetting. We deploy an episodic memory\nunit that stores a subset of samples for each task to learn task-specific\nclassifiers based on kernel ridge regression. This does not require memory\nreplay and systematically avoids task interference in the classifiers. We\nfurther introduce variational random features to learn a data-driven kernel for\neach task. To do so, we formulate kernel continual learning as a variational\ninference problem, where a random Fourier basis is incorporated as the latent\nvariable. The variational posterior distribution over the random Fourier basis\nis inferred from the coreset of each task. In this way, we are able to generate\nmore informative kernels specific to each task, and, more importantly, the\ncoreset size can be reduced to achieve more compact memory, resulting in more\nefficient continual learning based on episodic memory. Extensive evaluation on\nfour benchmarks demonstrates the effectiveness and promise of kernels for\ncontinual learning.",
          "link": "http://arxiv.org/abs/2107.05757",
          "publishedOn": "2021-07-14T01:41:50.895Z",
          "wordCount": 604,
          "title": "Kernel Continual Learning. (arXiv:2107.05757v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Riehle_D/0/1/0/all/0/1\">Dirk Riehle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harutyunyan_N/0/1/0/all/0/1\">Nikolay Harutyunyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barcomb_A/0/1/0/all/0/1\">Ann Barcomb</a>",
          "description": "Pattern discovery, the process of discovering previously unrecognized\npatterns, is often performed as an ad-hoc process with little resulting\ncertainty in the quality of the proposed patterns. Pattern validation, the\nprocess of validating the accuracy of proposed patterns, remains dominated by\nthe simple heuristic of \"the rule of three\". This article shows how to use\nestablished scientific research methods for the purpose of pattern discovery\nand validation. We present a specific approach, called the handbook method,\nthat uses the qualitative survey, action research, and case study research for\npattern discovery and evaluation, and we discuss the underlying principle of\nusing scientific methods in general. We evaluate the handbook method using\nthree exploratory studies and demonstrate its usefulness.",
          "link": "http://arxiv.org/abs/2107.06065",
          "publishedOn": "2021-07-14T01:41:50.883Z",
          "wordCount": 550,
          "title": "Pattern Discovery and Validation Using Scientific Research Methods. (arXiv:2107.06065v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05849",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1\">Avishek Ghosh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sayak Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>",
          "description": "We address the problem of model selection for the finite horizon episodic\nReinforcement Learning (RL) problem where the transition kernel $P^*$ belongs\nto a family of models $\\mathcal{P}^*$ with finite metric entropy. In the model\nselection framework, instead of $\\mathcal{P}^*$, we are given $M$ nested\nfamilies of transition kernels $\\cP_1 \\subset \\cP_2 \\subset \\ldots \\subset\n\\cP_M$. We propose and analyze a novel algorithm, namely \\emph{Adaptive\nReinforcement Learning (General)} (\\texttt{ARL-GEN}) that adapts to the\nsmallest such family where the true transition kernel $P^*$ lies.\n\\texttt{ARL-GEN} uses the Upper Confidence Reinforcement Learning\n(\\texttt{UCRL}) algorithm with value targeted regression as a blackbox and puts\na model selection module at the beginning of each epoch. Under a mild\nseparability assumption on the model classes, we show that \\texttt{ARL-GEN}\nobtains a regret of\n$\\Tilde{\\mathcal{O}}(d_{\\mathcal{E}}^*H^2+\\sqrt{d_{\\mathcal{E}}^* \\mathbb{M}^*\nH^2 T})$, with high probability, where $H$ is the horizon length, $T$ is the\ntotal number of steps, $d_{\\mathcal{E}}^*$ is the Eluder dimension and\n$\\mathbb{M}^*$ is the metric entropy corresponding to $\\mathcal{P}^*$. Note\nthat this regret scaling matches that of an oracle that knows $\\mathcal{P}^*$\nin advance. We show that the cost of model selection for \\texttt{ARL-GEN} is an\nadditive term in the regret having a weak dependence on $T$. Subsequently, we\nremove the separability assumption and consider the setup of linear mixture\nMDPs, where the transition kernel $P^*$ has a linear function approximation.\nWith this low rank structure, we propose novel adaptive algorithms for model\nselection, and obtain (order-wise) regret identical to that of an oracle with\nknowledge of the true model class.",
          "link": "http://arxiv.org/abs/2107.05849",
          "publishedOn": "2021-07-14T01:41:50.861Z",
          "wordCount": 707,
          "title": "Model Selection with Near Optimal Rates for Reinforcement Learning with General Model Classes. (arXiv:2107.05849v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilie_A/0/1/0/all/0/1\">Andrei Ilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanescu_A/0/1/0/all/0/1\">Alin Stefanescu</a>",
          "description": "Recent work has shown how easily white-box adversarial attacks can be applied\nto state-of-the-art image classifiers. However, real-life scenarios resemble\nmore the black-box adversarial conditions, lacking transparency and usually\nimposing natural, hard constraints on the query budget.\n\nWe propose $\\textbf{EvoBA}$, a black-box adversarial attack based on a\nsurprisingly simple evolutionary search strategy. $\\textbf{EvoBA}$ is\nquery-efficient, minimizes $L_0$ adversarial perturbations, and does not\nrequire any form of training.\n\n$\\textbf{EvoBA}$ shows efficiency and efficacy through results that are in\nline with much more complex state-of-the-art black-box attacks such as\n$\\textbf{AutoZOOM}$. It is more query-efficient than $\\textbf{SimBA}$, a simple\nand powerful baseline black-box attack, and has a similar level of complexity.\nTherefore, we propose it both as a new strong baseline for black-box\nadversarial attacks and as a fast and general tool for gaining empirical\ninsight into how robust image classifiers are with respect to $L_0$ adversarial\nperturbations.\n\nThere exist fast and reliable $L_2$ black-box attacks, such as\n$\\textbf{SimBA}$, and $L_{\\infty}$ black-box attacks, such as\n$\\textbf{DeepSearch}$. We propose $\\textbf{EvoBA}$ as a query-efficient $L_0$\nblack-box adversarial attack which, together with the aforementioned methods,\ncan serve as a generic tool to assess the empirical robustness of image\nclassifiers. The main advantages of such methods are that they run fast, are\nquery-efficient, and can easily be integrated in image classifiers development\npipelines.\n\nWhile our attack minimises the $L_0$ adversarial perturbation, we also report\n$L_2$, and notice that we compare favorably to the state-of-the-art $L_2$\nblack-box attack, $\\textbf{AutoZOOM}$, and of the $L_2$ strong baseline,\n$\\textbf{SimBA}$.",
          "link": "http://arxiv.org/abs/2107.05754",
          "publishedOn": "2021-07-14T01:41:50.854Z",
          "wordCount": 694,
          "title": "EvoBA: An Evolution Strategy as a Strong Baseline forBlack-Box Adversarial Attacks. (arXiv:2107.05754v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogert_K/0/1/0/all/0/1\">Kenneth Bogert</a> (University of North Carolina Asheville), <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1\">Prashant Doshi</a> (University of Georgia)",
          "description": "Robots learning from observations in the real world using inverse\nreinforcement learning (IRL) may encounter objects or agents in the\nenvironment, other than the expert, that cause nuisance observations during the\ndemonstration. These confounding elements are typically removed in\nfully-controlled environments such as virtual simulations or lab settings. When\ncomplete removal is impossible the nuisance observations must be filtered out.\nHowever, identifying the source of observations when large amounts of\nobservations are made is difficult. To address this, we present a hierarchical\nBayesian model that incorporates both the expert's and the confounding\nelements' observations thereby explicitly modeling the diverse observations a\nrobot may receive. We extend an existing IRL algorithm originally designed to\nwork under partial occlusion of the expert to consider the diverse\nobservations. In a simulated robotic sorting domain containing both occlusion\nand confounding elements, we demonstrate the model's effectiveness. In\nparticular, our technique outperforms several other comparative methods, second\nonly to having perfect knowledge of the subject's trajectory.",
          "link": "http://arxiv.org/abs/2107.05818",
          "publishedOn": "2021-07-14T01:41:50.847Z",
          "wordCount": 611,
          "title": "A Hierarchical Bayesian model for Inverse RL in Partially-Controlled Environments. (arXiv:2107.05818v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1\">Rianne van den Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalchbrenner_N/0/1/0/all/0/1\">Nal Kalchbrenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1\">Hanie Sedghi</a>",
          "description": "We focus on the problem of domain adaptation when the goal is shifting the\nmodel towards the target distribution, rather than learning domain invariant\nrepresentations. It has been shown that under the following two assumptions:\n(a) access to samples from intermediate distributions, and (b) samples being\nannotated with the amount of change from the source distribution, self-training\ncan be successfully applied on gradually shifted samples to adapt the model\ntoward the target distribution. We hypothesize having (a) is enough to enable\niterative self-training to slowly adapt the model to the target distribution,\nby making use of an implicit curriculum. In the case where (a) does not hold,\nwe observe that iterative self-training falls short. We propose GIFT, a method\nthat creates virtual samples from intermediate distributions by interpolating\nrepresentations of examples from source and target domains. We evaluate an\niterative-self-training method on datasets with natural distribution shifts,\nand show that when applied on top of other domain adaptation methods, it\nimproves the performance of the model on the target dataset. We run an analysis\non a synthetic dataset to show that in the presence of (a)\niterative-self-training naturally forms a curriculum of samples. Furthermore,\nwe show that when (a) does not hold, GIFT performs better than iterative\nself-training.",
          "link": "http://arxiv.org/abs/2106.06080",
          "publishedOn": "2021-07-14T01:41:50.840Z",
          "wordCount": 685,
          "title": "Gradual Domain Adaptation in the Wild:When Intermediate Distributions are Absent. (arXiv:2106.06080v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhuheir_M/0/1/0/all/0/1\">Marwan Dhuheir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1\">Abdullatif Albaseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1\">Emna Baccour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1\">Mohamed Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1\">Mounir Hamdi</a>",
          "description": "Recognizing the patient's emotions using deep learning techniques has\nattracted significant attention recently due to technological advancements.\nAutomatically identifying the emotions can help build smart healthcare centers\nthat can detect depression and stress among the patients in order to start the\nmedication early. Using advanced technology to identify emotions is one of the\nmost exciting topics as it defines the relationships between humans and\nmachines. Machines learned how to predict emotions by adopting various methods.\nIn this survey, we present recent research in the field of using neural\nnetworks to recognize emotions. We focus on studying emotions' recognition from\nspeech, facial expressions, and audio-visual input and show the different\ntechniques of deploying these algorithms in the real world. These three emotion\nrecognition techniques can be used as a surveillance system in healthcare\ncenters to monitor patients. We conclude the survey with a presentation of the\nchallenges and the related future work to provide an insight into the\napplications of using emotion recognition.",
          "link": "http://arxiv.org/abs/2107.05989",
          "publishedOn": "2021-07-14T01:41:50.832Z",
          "wordCount": 629,
          "title": "Emotion Recognition for Healthcare Surveillance Systems Using Neural Networks: A Survey. (arXiv:2107.05989v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "It is well noted that coordinate based MLPs benefit greatly -- in terms of\npreserving high-frequency information -- through the encoding of coordinate\npositions as an array of Fourier features. Hitherto, the rationale for the\neffectiveness of these positional encodings has been solely studied through a\nFourier lens. In this paper, we strive to broaden this understanding by showing\nthat alternative non-Fourier embedding functions can indeed be used for\npositional encoding. Moreover, we show that their performance is entirely\ndetermined by a trade-off between the stable rank of the embedded matrix and\nthe distance preservation between embedded coordinates. We further establish\nthat the now ubiquitous Fourier feature mapping of position is a special case\nthat fulfills these conditions. Consequently, we present a more general theory\nto analyze positional encoding in terms of shifted basis functions. To this\nend, we develop the necessary theoretical formulae and empirically verify that\nour theoretical claims hold in practice. Codes available at\nhttps://github.com/osiriszjq/Rethinking-positional-encoding.",
          "link": "http://arxiv.org/abs/2107.02561",
          "publishedOn": "2021-07-14T01:41:50.813Z",
          "wordCount": 603,
          "title": "Rethinking Positional Encoding. (arXiv:2107.02561v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianshen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azam_N/0/1/0/all/0/1\">Naveed Ahmed Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_K/0/1/0/all/0/1\">Kazuya Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagamochi_H/0/1/0/all/0/1\">Hiroshi Nagamochi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akutsu_T/0/1/0/all/0/1\">Tatsuya Akutsu</a>",
          "description": "Recently a novel framework has been proposed for designing the molecular\nstructure of chemical compounds using both artificial neural networks (ANNs)\nand mixed integer linear programming (MILP). In the framework, we first define\na feature vector $f(C)$ of a chemical graph $C$ and construct an ANN that maps\n$x=f(C)$ to a predicted value $\\eta(x)$ of a chemical property $\\pi$ to $C$.\nAfter this, we formulate an MILP that simulates the computation process of\n$f(C)$ from $C$ and that of $\\eta(x)$ from $x$. Given a target value $y^*$ of\nthe chemical property $\\pi$, we infer a chemical graph $C^\\dagger$ such that\n$\\eta(f(C^\\dagger))=y^*$ by solving the MILP. In this paper, we use linear\nregression to construct a prediction function $\\eta$ instead of ANNs. For this,\nwe derive an MILP formulation that simulates the computation process of a\nprediction function by linear regression. The results of computational\nexperiments suggest our method can infer chemical graphs with around up to 50\nnon-hydrogen atoms.",
          "link": "http://arxiv.org/abs/2107.02381",
          "publishedOn": "2021-07-14T01:41:50.806Z",
          "wordCount": 624,
          "title": "An Inverse QSAR Method Based on Linear Regression and Integer Programming. (arXiv:2107.02381v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiajun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Changnan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>",
          "description": "Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning\n(DRL) via combining deep learning (DL) with reinforcement learning (RL), which\nhas noticed that the distribution of the acquired data would change during the\ntraining process. DQN found this property might cause instability for training,\nso it proposed effective methods to handle the downside of the property.\nInstead of focusing on the unfavourable aspects, we find it critical for RL to\nease the gap between the estimated data distribution and the ground truth data\ndistribution while supervised learning (SL) fails to do so. From this new\nperspective, we extend the basic paradigm of RL called the Generalized Policy\nIteration (GPI) into a more generalized version, which is called the\nGeneralized Data Distribution Iteration (GDI). We see massive RL algorithms and\ntechniques can be unified into the GDI paradigm, which can be considered as one\nof the special cases of GDI. We provide theoretical proof of why GDI is better\nthan GPI and how it works. Several practical algorithms based on GDI have been\nproposed to verify the effectiveness and extensiveness of it. Empirical\nexperiments prove our state-of-the-art (SOTA) performance on Arcade Learning\nEnvironment (ALE), wherein our algorithm has achieved 9620.98% mean human\nnormalized score (HNS), 1146.39% median HNS and 22 human world record\nbreakthroughs (HWRB) using only 200 training frames. Our work aims to lead the\nRL research to step into the journey of conquering the human world records and\nseek real superhuman agents on both performance and efficiency.",
          "link": "http://arxiv.org/abs/2106.06232",
          "publishedOn": "2021-07-14T01:41:50.795Z",
          "wordCount": 728,
          "title": "GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hesen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "Data augmentation is a commonly used approach to improving the generalization\nof deep learning models. Recent works show that learned data augmentation\npolicies can achieve better generalization than hand-crafted ones. However,\nmost of these works use unified augmentation policies for all samples in a\ndataset, which is observed not necessarily beneficial for all labels in\nmulti-label classification tasks, i.e., some policies may have negative impacts\non some labels while benefitting the others. To tackle this problem, we propose\na novel Label-Based AutoAugmentation (LB-Aug) method for multi-label scenarios,\nwhere augmentation policies are generated with respect to labels by an\naugmentation-policy network. The policies are learned via reinforcement\nlearning using policy gradient methods, providing a mapping from instance\nlabels to their optimal augmentation policies. Numerical experiments show that\nour LB-Aug outperforms previous state-of-the-art augmentation methods by large\nmargins in multiple benchmarks on image and video classification.",
          "link": "http://arxiv.org/abs/2107.05384",
          "publishedOn": "2021-07-14T01:41:50.787Z",
          "wordCount": 598,
          "title": "Fine-Grained AutoAugmentation for Multi-Label Classification. (arXiv:2107.05384v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kingma_D/0/1/0/all/0/1\">Diederik P. Kingma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1\">Ben Poole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>",
          "description": "Diffusion-based generative models have demonstrated a capacity for\nperceptually impressive synthesis, but can they also be great likelihood-based\nmodels? We answer this in the affirmative, and introduce a family of\ndiffusion-based generative models that obtain state-of-the-art likelihoods on\nstandard image density estimation benchmarks. Unlike other diffusion-based\nmodels, our method allows for efficient optimization of the noise schedule\njointly with the rest of the model. We show that the variational lower bound\n(VLB) simplifies to a remarkably short expression in terms of the\nsignal-to-noise ratio of the diffused data, thereby improving our theoretical\nunderstanding of this model class. Using this insight, we prove an equivalence\nbetween several models proposed in the literature. In addition, we show that\nthe continuous-time VLB is invariant to the noise schedule, except for the\nsignal-to-noise ratio at its endpoints. This enables us to learn a noise\nschedule that minimizes the variance of the resulting VLB estimator, leading to\nfaster optimization. Combining these advances with architectural improvements,\nwe obtain state-of-the-art likelihoods on image density estimation benchmarks,\noutperforming autoregressive models that have dominated these benchmarks for\nmany years, with often significantly faster optimization. In addition, we show\nhow to turn the model into a bits-back compression scheme, and demonstrate\nlossless compression rates close to the theoretical optimum.",
          "link": "http://arxiv.org/abs/2107.00630",
          "publishedOn": "2021-07-14T01:41:50.779Z",
          "wordCount": 653,
          "title": "Variational Diffusion Models. (arXiv:2107.00630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1\">Pierre Marza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1\">Laetitia Matignon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1\">Olivier Simonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>",
          "description": "In the context of visual navigation, the capacity to map a novel environment\nis necessary for an agent to exploit its observation history in the considered\nplace and efficiently reach known goals. This ability can be associated with\nspatial reasoning, where an agent is able to perceive spatial relationships and\nregularities, and discover object affordances. In classical Reinforcement\nLearning (RL) setups, this capacity is learned from reward alone. We introduce\nsupplementary supervision in the form of auxiliary tasks designed to favor the\nemergence of spatial perception capabilities in agents trained for a\ngoal-reaching downstream objective. We show that learning to estimate metrics\nquantifying the spatial relationships between an agent at a given location and\na goal to reach has a high positive impact in Multi-Object Navigation settings.\nOur method significantly improves the performance of different baseline agents,\nthat either build an explicit or implicit representation of the environment,\neven matching the performance of incomparable oracle agents taking ground-truth\nmaps as input.",
          "link": "http://arxiv.org/abs/2107.06011",
          "publishedOn": "2021-07-14T01:41:50.760Z",
          "wordCount": 608,
          "title": "Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junhyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1\">Byungyoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Charmgil Hong</a>",
          "description": "We present Multi-Scale Label Dependence Relation Networks (MSDN), a novel\napproach to multi-label classification (MLC) using 1-dimensional convolution\nkernels to learn label dependencies at multi-scale. Modern multi-label\nclassifiers have been adopting recurrent neural networks (RNNs) as a memory\nstructure to capture and exploit label dependency relations. The RNN-based MLC\nmodels however tend to introduce a very large number of parameters that may\ncause under-/over-fitting problems. The proposed method uses the 1-dimensional\nconvolutional neural network (1D-CNN) to serve the same purpose in a more\nefficient manner. By training a model with multiple kernel sizes, the method is\nable to learn the dependency relations among labels at multiple scales, while\nit uses a drastically smaller number of parameters. With public benchmark\ndatasets, we demonstrate that our model can achieve better accuracies with much\nsmaller number of model parameters compared to RNN-based MLC models.",
          "link": "http://arxiv.org/abs/2107.05941",
          "publishedOn": "2021-07-14T01:41:50.754Z",
          "wordCount": 576,
          "title": "Multi-Scale Label Relation Learning for Multi-Label Classification Using 1-Dimensional Convolutional Neural Networks. (arXiv:2107.05941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>",
          "description": "Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.",
          "link": "http://arxiv.org/abs/2107.05768",
          "publishedOn": "2021-07-14T01:41:50.747Z",
          "wordCount": 664,
          "title": "Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+We_S/0/1/0/all/0/1\">Shi-jie We</a>",
          "description": "This paper presents the participation of the MiniTrue team in the FinSim-3\nshared task on learning semantic similarities for the financial domain in\nEnglish language. Our approach combines contextual embeddings learned by\ntransformer-based language models with network structures embeddings extracted\non external knowledge sources, to create more meaningful representations of\nfinancial domain entities and terms. For this, two BERT based language models\nand a knowledge graph embedding model are used. Besides, we propose a voting\nfunction to joint three basic models for the final inference. Experimental\nresults show that the model with the knowledge graph embeddings has achieved a\nsuperior result than these models with only contextual embeddings.\nNevertheless, we also observe that our voting function brings an extra benefit\nto the final system.",
          "link": "http://arxiv.org/abs/2107.05885",
          "publishedOn": "2021-07-14T01:41:50.741Z",
          "wordCount": 566,
          "title": "Exploiting Network Structures to Improve Semantic Representation for the Financial Domain. (arXiv:2107.05885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sumedha Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_S/0/1/0/all/0/1\">Stephen Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triantafillou_S/0/1/0/all/0/1\">Sofia Triantafillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Model explainability is essential for the creation of trustworthy Machine\nLearning models in healthcare. An ideal explanation resembles the\ndecision-making process of a domain expert and is expressed using concepts or\nterminology that is meaningful to the clinicians. To provide such an\nexplanation, we first associate the hidden units of the classifier to\nclinically relevant concepts. We take advantage of radiology reports\naccompanying the chest X-ray images to define concepts. We discover sparse\nassociations between concepts and hidden units using a linear sparse logistic\nregression. To ensure that the identified units truly influence the\nclassifier's outcome, we adopt tools from Causal Inference literature and, more\nspecifically, mediation analysis through counterfactual interventions. Finally,\nwe construct a low-depth decision tree to translate all the discovered concepts\ninto a straightforward decision rule, expressed to the radiologist. We\nevaluated our approach on a large chest x-ray dataset, where our model produces\na global explanation consistent with clinical knowledge.",
          "link": "http://arxiv.org/abs/2107.06098",
          "publishedOn": "2021-07-14T01:41:50.734Z",
          "wordCount": 595,
          "title": "Using Causal Analysis for Conceptual Deep Learning Explanation. (arXiv:2107.06098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05975",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gotkowski_K/0/1/0/all/0/1\">Karol Gotkowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bucher_A/0/1/0/all/0/1\">Andreas Bucher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischbach_R/0/1/0/all/0/1\">Ricarda Fischbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaltenborn_I/0/1/0/all/0/1\">Isabel Kaltenborn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Automatic segmentation of lung lesions in computer tomography has the\npotential to ease the burden of clinicians during the Covid-19 pandemic. Yet\npredictive deep learning models are not trusted in the clinical routine due to\nfailing silently in out-of-distribution (OOD) data. We propose a lightweight\nOOD detection method that exploits the Mahalanobis distance in the feature\nspace. The proposed approach can be seamlessly integrated into state-of-the-art\nsegmentation pipelines without requiring changes in model architecture or\ntraining procedure, and can therefore be used to assess the suitability of\npre-trained models to new data. We validate our method with a patch-based\nnnU-Net architecture trained with a multi-institutional dataset and find that\nit effectively detects samples that the model segments incorrectly.",
          "link": "http://arxiv.org/abs/2107.05975",
          "publishedOn": "2021-07-14T01:41:50.727Z",
          "wordCount": 617,
          "title": "Detecting when pre-trained nnU-Net models fail silently for Covid-19. (arXiv:2107.05975v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>",
          "description": "In this paper, we answer the question when inserting label noise (less\ninformative labels) can instead return us more accurate and fair models. We are\nprimarily inspired by two observations that 1) increasing a certain class of\ninstances' label noise to balance the noise rates (increasing-to-balancing)\nresults in an easier learning problem; 2) Increasing-to-balancing improves\nfairness guarantees against label bias. In this paper, we will first quantify\nthe trade-offs introduced by increasing a certain group of instances' label\nnoise rate w.r.t. the learning difficulties and performance guarantees. We\nanalytically demonstrate when such an increase proves to be beneficial, in\nterms of either improved generalization errors or the fairness guarantees. Then\nwe present a method to leverage our idea of inserting label noise for the task\nof learning with noisy labels, either without or with a fairness constraint.\nThe primary technical challenge we face is due to the fact that we would not\nknow which data instances are suffering from higher noise, and we would not\nhave the ground truth labels to verify any possible hypothesis. We propose a\ndetection method that informs us which group of labels might suffer from higher\nnoise, without using ground truth information. We formally establish the\neffectiveness of the proposed solution and demonstrate it with extensive\nexperiments.",
          "link": "http://arxiv.org/abs/2107.05913",
          "publishedOn": "2021-07-14T01:41:50.708Z",
          "wordCount": 652,
          "title": "Can Less be More? When Increasing-to-Balancing Label Noise Rates Considered Beneficial. (arXiv:2107.05913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Angelica Louren&#xe7;o Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1\">Marcos Eduardo Valle</a>",
          "description": "This paper presents a hybrid morphological neural network for regression\ntasks called linear dilation-erosion regression ($\\ell$-DER). In few words, an\n$\\ell$-DER model is given by a convex combination of the composition of linear\nand elementary morphological operators. As a result, they yield continuous\npiecewise linear functions and, thus, are universal approximators. Apart from\nintroducing the $\\ell$-DER models, we present three approaches for training\nthese models: one based on stochastic descent gradient and two based on the\ndifference of convex programming problems. Finally, we evaluate the performance\nof the $\\ell$-DER model using 14 regression tasks. Although the approach based\non SDG revealed faster than the other two, the $\\ell$-DER trained using a\ndisciplined convex-concave programming problem outperformed the others in terms\nof the least mean absolute error score.",
          "link": "http://arxiv.org/abs/2107.05682",
          "publishedOn": "2021-07-14T01:41:50.701Z",
          "wordCount": 597,
          "title": "Least-Squares Linear Dilation-Erosion Regressor Trained using Stochastic Descent Gradient or the Difference of Convex Methods. (arXiv:2107.05682v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1\">Dylan J. Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentile_C/0/1/0/all/0/1\">Claudio Gentile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmert_J/0/1/0/all/0/1\">Julian Zimmert</a>",
          "description": "A major research direction in contextual bandits is to develop algorithms\nthat are computationally efficient, yet support flexible, general-purpose\nfunction approximation. Algorithms based on modeling rewards have shown strong\nempirical performance, but typically require a well-specified model, and can\nfail when this assumption does not hold. Can we design algorithms that are\nefficient and flexible, yet degrade gracefully in the face of model\nmisspecification? We introduce a new family of oracle-efficient algorithms for\n$\\varepsilon$-misspecified contextual bandits that adapt to unknown model\nmisspecification -- both for finite and infinite action settings. Given access\nto an online oracle for square loss regression, our algorithm attains optimal\nregret and -- in particular -- optimal dependence on the misspecification\nlevel, with no prior knowledge. Specializing to linear contextual bandits with\ninfinite actions in $d$ dimensions, we obtain the first algorithm that achieves\nthe optimal $O(d\\sqrt{T} + \\varepsilon\\sqrt{d}T)$ regret bound for unknown\nmisspecification level $\\varepsilon$.\n\nOn a conceptual level, our results are enabled by a new optimization-based\nperspective on the regression oracle reduction framework of Foster and Rakhlin,\nwhich we anticipate will find broader use.",
          "link": "http://arxiv.org/abs/2107.05745",
          "publishedOn": "2021-07-14T01:41:50.694Z",
          "wordCount": 618,
          "title": "Adapting to Misspecification in Contextual Bandits. (arXiv:2107.05745v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hirn_J/0/1/0/all/0/1\">J. Hirn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1\">J. E. Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesinos_Navarro_A/0/1/0/all/0/1\">A. Montesinos-Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Martin_R/0/1/0/all/0/1\">R. Sanchez-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanz_V/0/1/0/all/0/1\">V. Sanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verdu_M/0/1/0/all/0/1\">M. Verd&#xfa;</a>",
          "description": "1. Deciphering coexistence patterns is a current challenge to understanding\ndiversity maintenance, especially in rich communities where the complexity of\nthese patterns is magnified through indirect interactions that prevent their\napproximation with classical experimental approaches. 2. We explore\ncutting-edge Machine Learning techniques called Generative Artificial\nIntelligence (GenAI) to decipher species coexistence patterns in vegetation\npatches, training generative adversarial networks (GAN) and variational\nAutoEncoders (VAE) that are then used to unravel some of the mechanisms behind\ncommunity assemblage. 3. The GAN accurately reproduces the species composition\nof real patches as well as the affinity of plant species to different soil\ntypes, and the VAE also reaches a high level of accuracy, above 99%. Using the\nartificially generated patches, we found that high order interactions tend to\nsuppress the positive effects of low order interactions. Finally, by\nreconstructing successional trajectories we could identify the pioneer species\nwith larger potential to generate a high diversity of distinct patches in terms\nof species composition. 4. Understanding the complexity of species coexistence\npatterns in diverse ecological communities requires new approaches beyond\nheuristic rules. Generative Artificial Intelligence can be a powerful tool to\nthis end as it allows to overcome the inherent dimensionality of this\nchallenge.",
          "link": "http://arxiv.org/abs/2107.06020",
          "publishedOn": "2021-07-14T01:41:50.687Z",
          "wordCount": 649,
          "title": "A Deep Generative Artificial Intelligence system to decipher species coexistence patterns. (arXiv:2107.06020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amor_M/0/1/0/all/0/1\">M. Ben Amor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stier_J/0/1/0/all/0/1\">J. Stier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1\">M. Granitzer</a>",
          "description": "Deep learning models have been shown to be vulnerable to adversarial attacks.\nThis perception led to analyzing deep learning models not only from the\nperspective of their performance measures but also their robustness to certain\ntypes of adversarial attacks. We take another step forward in relating the\narchitectural structure of neural networks from a graph theoretic perspective\nto their robustness. We aim to investigate any existing correlations between\ngraph theoretic properties and the robustness of Sparse Neural Networks. Our\nhypothesis is, that graph theoretic properties as a prior of neural network\nstructures are related to their robustness. To answer to this hypothesis, we\ndesigned an empirical study with neural network models obtained through random\ngraphs used as sparse structural priors for the networks. We additionally\ninvestigated the evaluation of a randomly pruned fully connected network as a\npoint of reference.\n\nWe found that robustness measures are independent of initialization methods\nbut show weak correlations with graph properties: higher graph densities\ncorrelate with lower robustness, but higher average path lengths and average\nnode eccentricities show negative correlations with robustness measures. We\nhope to motivate further empirical and analytical research to tightening an\nanswer to our hypothesis.",
          "link": "http://arxiv.org/abs/2107.06158",
          "publishedOn": "2021-07-14T01:41:50.669Z",
          "wordCount": 642,
          "title": "Correlation Analysis between the Robustness of Sparse Neural Networks and their Random Hidden Structural Priors. (arXiv:2107.06158v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuangbin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Wenwei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuxin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>",
          "description": "Logs have been an imperative resource to ensure the reliability and\ncontinuity of many software systems, especially large-scale distributed\nsystems. They faithfully record runtime information to facilitate system\ntroubleshooting and behavior understanding. Due to the large scale and\ncomplexity of modern software systems, the volume of logs has reached an\nunprecedented level. Consequently, for log-based anomaly detection,\nconventional methods of manual inspection or even traditional machine\nlearning-based methods become impractical, which serve as a catalyst for the\nrapid development of deep learning-based solutions. However, there is currently\na lack of rigorous comparison among the representative log-based anomaly\ndetectors which resort to neural network models. Moreover, the\nre-implementation process demands non-trivial efforts and bias can be easily\nintroduced. To better understand the characteristics of different anomaly\ndetectors, in this paper, we provide a comprehensive review and evaluation on\nfive popular models used by six state-of-the-art methods. Particularly, four of\nthe selected methods are unsupervised and the remaining two are supervised.\nThese methods are evaluated with two publicly-available log datasets, which\ncontain nearly 16 millions log messages and 0.4 million anomaly instances in\ntotal. We believe our work can serve as a basis in this field and contribute to\nthe future academic researches and industrial applications.",
          "link": "http://arxiv.org/abs/2107.05908",
          "publishedOn": "2021-07-14T01:41:50.663Z",
          "wordCount": 644,
          "title": "Experience Report: Deep Learning-based System Log Analysis for Anomaly Detection. (arXiv:2107.05908v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05709",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Morales_G/0/1/0/all/0/1\">Guillermo B. Morales</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Munoz_M/0/1/0/all/0/1\">Miguel A. Mu&#xf1;oz</a>",
          "description": "Shedding light onto how biological systems represent, process and store\ninformation in noisy environments is a key and challenging goal. A stimulating,\nthough controversial, hypothesis poses that operating in dynamical regimes near\nthe edge of a phase transition, i.e. at criticality or the \"edge of chaos\", can\nprovide information-processing living systems with important operational\nadvantages, creating, e.g., an optimal trade-off between robustness and\nflexibility. Here, we elaborate on a recent theoretical result, which\nestablishes that the spectrum of covariance matrices of neural networks\nrepresenting complex inputs in a robust way needs to decay as a power-law of\nthe rank, with an exponent close to unity, a result that has been indeed\nexperimentally verified in neurons of the mouse visual cortex. Aimed at\nunderstanding and mimicking these results, we construct an artificial neural\nnetwork and train it to classify images. Remarkably, we find that the best\nperformance in such a task is obtained when the network operates near the\ncritical point, at which the eigenspectrum of the covariance matrix follows the\nvery same statistics as actual neurons do. Thus, we conclude that operating\nnear criticality can also have -- besides the usually alleged virtues -- the\nadvantage of allowing for flexible, robust and efficient input representations.",
          "link": "http://arxiv.org/abs/2107.05709",
          "publishedOn": "2021-07-14T01:41:50.656Z",
          "wordCount": 662,
          "title": "Optimal input representation in neural systems at the edge of chaos. (arXiv:2107.05709v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1\">Manuel W&#xfc;thrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmaier_F/0/1/0/all/0/1\">Felix Widmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>",
          "description": "Learning data representations that are useful for various downstream tasks is\na cornerstone of artificial intelligence. While existing methods are typically\nevaluated on downstream tasks such as classification or generative image\nquality, we propose to assess representations through their usefulness in\ndownstream control tasks, such as reaching or pushing objects. By training over\n10,000 reinforcement learning policies, we extensively evaluate to what extent\ndifferent representation properties affect out-of-distribution (OOD)\ngeneralization. Finally, we demonstrate zero-shot transfer of these policies\nfrom simulation to the real world, without any domain randomization or\nfine-tuning. This paper aims to establish the first systematic characterization\nof the usefulness of learned representations for real-world OOD downstream\ntasks.",
          "link": "http://arxiv.org/abs/2107.05686",
          "publishedOn": "2021-07-14T01:41:50.650Z",
          "wordCount": 557,
          "title": "Representation Learning for Out-Of-Distribution Generalization in Reinforcement Learning. (arXiv:2107.05686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05990",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolf_T/0/1/0/all/0/1\">Tom Nuno Wolf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>",
          "description": "Prior work on diagnosing Alzheimer's disease from magnetic resonance images\nof the brain established that convolutional neural networks (CNNs) can leverage\nthe high-dimensional image information for classifying patients. However,\nlittle research focused on how these models can utilize the usually\nlow-dimensional tabular information, such as patient demographics or laboratory\nmeasurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a\ngeneral-purpose module for CNNs that dynamically rescales and shifts the\nfeature maps of a convolutional layer, conditional on a patient's tabular\nclinical information. We show that DAFT is highly effective in combining 3D\nimage and tabular information for diagnosis and time-to-dementia prediction,\nwhere it outperforms competing CNNs with a mean balanced accuracy of 0.622 and\nmean c-index of 0.748, respectively. Our extensive ablation study provides\nvaluable insights into the architectural properties of DAFT. Our implementation\nis available at https://github.com/ai-med/DAFT.",
          "link": "http://arxiv.org/abs/2107.05990",
          "publishedOn": "2021-07-14T01:41:50.643Z",
          "wordCount": 614,
          "title": "Combining 3D Image and Tabular Data via the Dynamic Affine Feature Map Transform. (arXiv:2107.05990v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Atreya Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bocquet_M/0/1/0/all/0/1\">Marc Bocquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirtzlin_T/0/1/0/all/0/1\">Tifenn Hirtzlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laborieux_A/0/1/0/all/0/1\">Axel Laborieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">Jacques-Olivier Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowak_E/0/1/0/all/0/1\">Etienne Nowak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vianello_E/0/1/0/all/0/1\">Elisa Vianello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portal_J/0/1/0/all/0/1\">Jean-Michel Portal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Querlioz_D/0/1/0/all/0/1\">Damien Querlioz</a>",
          "description": "The implementation of current deep learning training algorithms is\npower-hungry, owing to data transfer between memory and logic units.\nOxide-based RRAMs are outstanding candidates to implement in-memory computing,\nwhich is less power-intensive. Their weak RESET regime, is particularly\nattractive for learning, as it allows tuning the resistance of the devices with\nremarkable endurance. However, the resistive change behavior in this regime\nsuffers many fluctuations and is particularly challenging to model, especially\nin a way compatible with tools used for simulating deep learning. In this work,\nwe present a model of the weak RESET process in hafnium oxide RRAM and\nintegrate this model within the PyTorch deep learning framework. Validated on\nexperiments on a hybrid CMOS/RRAM technology, our model reproduces both the\nnoisy progressive behavior and the device-to-device (D2D) variability. We use\nthis tool to train Binarized Neural Networks for the MNIST handwritten digit\nrecognition task and the CIFAR-10 object classification task. We simulate our\nmodel with and without various aspects of device imperfections to understand\ntheir impact on the training process and identify that the D2D variability is\nthe most detrimental aspect. The framework can be used in the same manner for\nother types of memories to identify the device imperfections that cause the\nmost degradation, which can, in turn, be used to optimize the devices to reduce\nthe impact of these imperfections.",
          "link": "http://arxiv.org/abs/2107.06064",
          "publishedOn": "2021-07-14T01:41:50.637Z",
          "wordCount": 679,
          "title": "Model of the Weak Reset Process in HfOx Resistive Memory for Deep Learning Frameworks. (arXiv:2107.06064v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torabi_F/0/1/0/all/0/1\">Faraz Torabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>",
          "description": "A longstanding goal of artificial intelligence is to create artificial agents\ncapable of learning to perform tasks that require sequential decision making.\nImportantly, while it is the artificial agent that learns and acts, it is still\nup to humans to specify the particular task to be performed. Classical\ntask-specification approaches typically involve humans providing stationary\nreward functions or explicit demonstrations of the desired tasks. However,\nthere has recently been a great deal of research energy invested in exploring\nalternative ways in which humans may guide learning agents that may, e.g., be\nmore suitable for certain tasks or require less human effort. This survey\nprovides a high-level overview of five recent machine learning frameworks that\nprimarily rely on human guidance apart from pre-specified reward functions or\nconventional, step-by-step action demonstrations. We review the motivation,\nassumptions, and implementation of each framework, and we discuss possible\nfuture research directions.",
          "link": "http://arxiv.org/abs/2107.05825",
          "publishedOn": "2021-07-14T01:41:50.621Z",
          "wordCount": 601,
          "title": "Recent Advances in Leveraging Human Guidance for Sequential Decision-Making Tasks. (arXiv:2107.05825v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05984",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Moreno_Pino_F/0/1/0/all/0/1\">Fernando Moreno-Pino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Olmos_P/0/1/0/all/0/1\">Pablo M. Olmos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Artes_Rodriguez_A/0/1/0/all/0/1\">Antonio Art&#xe9;s-Rodr&#xed;guez</a>",
          "description": "Time series forecasting is an important problem across many domains, playing\na crucial role in multiple real-world applications. In this paper, we propose a\nforecasting architecture that combines deep autoregressive models with a\nSpectral Attention (SA) module, which merges global and local frequency domain\ninformation in the model's embedded space. By characterizing in the spectral\ndomain the embedding of the time series as occurrences of a random process, our\nmethod can identify global trends and seasonality patterns. Two spectral\nattention models, global and local to the time series, integrate this\ninformation within the forecast and perform spectral filtering to remove time\nseries's noise. The proposed architecture has a number of useful properties: it\ncan be effectively incorporated into well-know forecast architectures,\nrequiring a low number of parameters and producing interpretable results that\nimprove forecasting accuracy. We test the Spectral Attention Autoregressive\nModel (SAAM) on several well-know forecast datasets, consistently demonstrating\nthat our model compares favorably to state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.05984",
          "publishedOn": "2021-07-14T01:41:50.615Z",
          "wordCount": 588,
          "title": "Deep Autoregressive Models with Spectral Attention. (arXiv:2107.05984v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larsen_B/0/1/0/all/0/1\">Brett W. Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_N/0/1/0/all/0/1\">Nic Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>",
          "description": "A variety of recent works, spanning pruning, lottery tickets, and training\nwithin random subspaces, have shown that deep neural networks can be trained\nusing far fewer degrees of freedom than the total number of parameters. We\nexplain this phenomenon by first examining the success probability of hitting a\ntraining loss sub-level set when training within a random subspace of a given\ntraining dimensionality. We find a sharp phase transition in the success\nprobability from $0$ to $1$ as the training dimension surpasses a threshold.\nThis threshold training dimension increases as the desired final loss\ndecreases, but decreases as the initial loss decreases. We then theoretically\nexplain the origin of this phase transition, and its dependence on\ninitialization and final desired loss, in terms of precise properties of the\nhigh dimensional geometry of the loss landscape. In particular, we show via\nGordon's escape theorem, that the training dimension plus the Gaussian width of\nthe desired loss sub-level set, projected onto a unit sphere surrounding the\ninitialization, must exceed the total number of parameters for the success\nprobability to be large. In several architectures and datasets, we measure the\nthreshold training dimension as a function of initialization and demonstrate\nthat it is a small fraction of the total number of parameters, thereby\nimplying, by our theory, that successful training with so few dimensions is\npossible precisely because the Gaussian width of low loss sub-level sets is\nvery large. Moreover, this threshold training dimension provides a strong null\nmodel for assessing the efficacy of more sophisticated ways to reduce training\ndegrees of freedom, including lottery tickets as well a more optimal method we\nintroduce: lottery subspaces.",
          "link": "http://arxiv.org/abs/2107.05802",
          "publishedOn": "2021-07-14T01:41:50.608Z",
          "wordCount": 724,
          "title": "How many degrees of freedom do we need to train deep networks: a loss landscape perspective. (arXiv:2107.05802v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Dan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>",
          "description": "The Graph Convolutional Networks (GCNs) proposed by Kipf and Welling are\neffective models for semi-supervised learning, but facing the obstacle of\nover-smoothing, which will weaken the representation ability of GCNs. Recently\nsome works are proposed to tackle with above limitation by randomly perturbing\ngraph topology or feature matrix to generate data augmentations as input for\ntraining. However, these operations have to pay the price of information\nstructure integrity breaking, and inevitably sacrifice information\nstochastically from original graph. In this paper, we introduce a novel graph\nentropy definition as an quantitative index to evaluate feature information\ndiffusion among a graph. Under considerations of preserving graph entropy, we\npropose an effective strategy to generate perturbed training data using a\nstochastic mechanism but guaranteeing graph topology integrity and with only a\nsmall amount of graph entropy decaying. Extensive experiments have been\nconducted on real-world datasets and the results verify the effectiveness of\nour proposed method in improving semi-supervised node classification accuracy\ncompared with a surge of baselines. Beyond that, our proposed approach\nsignificantly enhances the robustness and generalization ability of GCNs during\nthe training process.",
          "link": "http://arxiv.org/abs/2107.06048",
          "publishedOn": "2021-07-14T01:41:50.581Z",
          "wordCount": 613,
          "title": "A Graph Data Augmentation Strategy with Entropy Preserving. (arXiv:2107.06048v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korshunova_I/0/1/0/all/0/1\">Iryna Korshunova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1\">David Stutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1\">Alexander A. Alemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiles_O/0/1/0/all/0/1\">Olivia Wiles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1\">Sven Gowal</a>",
          "description": "We study the adversarial robustness of information bottleneck models for\nclassification. Previous works showed that the robustness of models trained\nwith information bottlenecks can improve upon adversarial training. Our\nevaluation under a diverse range of white-box $l_{\\infty}$ attacks suggests\nthat information bottlenecks alone are not a strong defense strategy, and that\nprevious results were likely influenced by gradient obfuscation.",
          "link": "http://arxiv.org/abs/2107.05712",
          "publishedOn": "2021-07-14T01:41:50.574Z",
          "wordCount": 498,
          "title": "A Closer Look at the Adversarial Robustness of Information Bottleneck Models. (arXiv:2107.05712v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1\">Toshinori Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takamitsu Matsubara</a>",
          "description": "In this paper, we propose cautious policy programming (CPP), a novel\nvalue-based reinforcement learning (RL) algorithm that can ensure monotonic\npolicy improvement during learning. Based on the nature of entropy-regularized\nRL, we derive a new entropy regularization-aware lower bound of policy\nimprovement that only requires estimating the expected policy advantage\nfunction. CPP leverages this lower bound as a criterion for adjusting the\ndegree of a policy update for alleviating policy oscillation. Different from\nsimilar algorithms that are mostly theory-oriented, we also propose a novel\ninterpolation scheme that makes CPP better scale in high dimensional control\nproblems. We demonstrate that the proposed algorithm can trade o? performance\nand stability in both didactic classic control problems and challenging\nhigh-dimensional Atari games.",
          "link": "http://arxiv.org/abs/2107.05798",
          "publishedOn": "2021-07-14T01:41:50.568Z",
          "wordCount": 571,
          "title": "Cautious Policy Programming: Exploiting KL Regularization in Monotonic Policy Improvement for Reinforcement Learning. (arXiv:2107.05798v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05719",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhao_S/0/1/0/all/0/1\">Shengjia Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kim_M/0/1/0/all/0/1\">Michael P. Kim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sahoo_R/0/1/0/all/0/1\">Roshni Sahoo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "When facing uncertainty, decision-makers want predictions they can trust. A\nmachine learning provider can convey confidence to decision-makers by\nguaranteeing their predictions are distribution calibrated -- amongst the\ninputs that receive a predicted class probabilities vector $q$, the actual\ndistribution over classes is $q$. For multi-class prediction problems, however,\nachieving distribution calibration tends to be infeasible, requiring sample\ncomplexity exponential in the number of classes $C$. In this work, we introduce\na new notion -- \\emph{decision calibration} -- that requires the predicted\ndistribution and true distribution to be ``indistinguishable'' to a set of\ndownstream decision-makers. When all possible decision makers are under\nconsideration, decision calibration is the same as distribution calibration.\nHowever, when we only consider decision makers choosing between a bounded\nnumber of actions (e.g. polynomial in $C$), our main result shows that\ndecisions calibration becomes feasible -- we design a recalibration algorithm\nthat requires sample complexity polynomial in the number of actions and the\nnumber of classes. We validate our recalibration algorithm empirically:\ncompared to existing methods, decision calibration improves decision-making on\nskin lesion and ImageNet classification with modern neural network predictors.",
          "link": "http://arxiv.org/abs/2107.05719",
          "publishedOn": "2021-07-14T01:41:50.562Z",
          "wordCount": 628,
          "title": "Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration. (arXiv:2107.05719v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>",
          "description": "Deep neural networks suffer from catastrophic forgetting when learning\nmultiple knowledge sequentially, and a growing number of approaches have been\nproposed to mitigate this problem. Some of these methods achieved considerable\nperformance by associating the flat local minima with forgetting mitigation in\ncontinual learning. However, they inevitably need (1) tedious hyperparameters\ntuning, and (2) additional computational cost. To alleviate these problems, in\nthis paper, we propose a simple yet effective optimization method, called\nAlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we\nconduct gradient descent and ascent alternatively when the network tends to\nconverge at each session of learning new knowledge. Moreover, we theoretically\nprove that such a strategy can encourage the optimization to converge to a flat\nminima. We verify AlterSGD on continual learning benchmark for semantic\nsegmentation and the empirical results show that we can significantly mitigate\nthe forgetting and outperform the state-of-the-art methods with a large margin\nunder challenging continual learning protocols.",
          "link": "http://arxiv.org/abs/2107.05804",
          "publishedOn": "2021-07-14T01:41:50.556Z",
          "wordCount": 605,
          "title": "AlterSGD: Finding Flat Minima for Continual Learning by Alternative Training. (arXiv:2107.05804v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05991",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gholipoor_N/0/1/0/all/0/1\">Narges Gholipoor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nouruzi_A/0/1/0/all/0/1\">Ali Nouruzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salarhosseini_S/0/1/0/all/0/1\">Shima Salarhosseini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Javan_M/0/1/0/all/0/1\">Mohammad Reza Javan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mokari_N/0/1/0/all/0/1\">Nader Mokari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorswieck_E/0/1/0/all/0/1\">Eduard A. Jorswieck</a>",
          "description": "In this paper, we propose a joint radio and core resource allocation\nframework for NFV-enabled networks. In the proposed system model, the goal is\nto maximize energy efficiency (EE), by guaranteeing end-to-end (E2E) quality of\nservice (QoS) for different service types. To this end, we formulate an\noptimization problem in which power and spectrum resources are allocated in the\nradio part. In the core part, the chaining, placement, and scheduling of\nfunctions are performed to ensure the QoS of all users. This joint optimization\nproblem is modeled as a Markov decision process (MDP), considering time-varying\ncharacteristics of the available resources and wireless channels. A soft\nactor-critic deep reinforcement learning (SAC-DRL) algorithm based on the\nmaximum entropy framework is subsequently utilized to solve the above MDP.\nNumerical results reveal that the proposed joint approach based on the SAC-DRL\nalgorithm could significantly reduce energy consumption compared to the case in\nwhich R-RA and NFV-RA problems are optimized separately.",
          "link": "http://arxiv.org/abs/2107.05991",
          "publishedOn": "2021-07-14T01:41:50.549Z",
          "wordCount": 623,
          "title": "Learning based E2E Energy Efficient in Joint Radio and NFV Resource Allocation for 5G and Beyond Networks. (arXiv:2107.05991v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1\">Umang Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_I/0/1/0/all/0/1\">Isabel Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Bilal Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "As the complexity of machine learning (ML) models increases, resulting in a\nlack of prediction explainability, several methods have been developed to\nexplain a model's behavior in terms of the training data points that most\ninfluence the model. However, these methods tend to mark outliers as highly\ninfluential points, limiting the insights that practitioners can draw from\npoints that are not representative of the training data. In this work, we take\na step towards finding influential training points that also represent the\ntraining data well. We first review methods for assigning importance scores to\ntraining points. Given importance scores, we propose a method to select a set\nof DIVerse INfluEntial (DIVINE) training points as a useful explanation of\nmodel behavior. As practitioners might not only be interested in finding data\npoints influential with respect to model accuracy, but also with respect to\nother important metrics, we show how to evaluate training data points on the\nbasis of group fairness. Our method can identify unfairness-inducing training\npoints, which can be removed to improve fairness outcomes. Our quantitative\nexperiments and user studies show that visualizing DIVINE points helps\npractitioners understand and explain model behavior better than earlier\napproaches.",
          "link": "http://arxiv.org/abs/2107.05978",
          "publishedOn": "2021-07-14T01:41:50.531Z",
          "wordCount": 647,
          "title": "DIVINE: Diverse Influential Training Points for Data Visualization and Model Refinement. (arXiv:2107.05978v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>",
          "description": "Active learning is the iterative construction of a classification model\nthrough targeted labeling, enabling significant labeling cost savings. As most\nresearch on active learning has been carried out before transformer-based\nlanguage models (\"transformers\") became popular, despite its practical\nimportance, comparably few papers have investigated how transformers can be\ncombined with active learning to date. This can be attributed to the fact that\nusing state-of-the-art query strategies for transformers induces a prohibitive\nruntime overhead, which effectively cancels out, or even outweighs\naforementioned cost savings. In this paper, we revisit uncertainty-based query\nstrategies, which had been largely outperformed before, but are particularly\nsuited in the context of fine-tuning transformers. In an extensive evaluation\non five widely used text classification benchmarks, we show that considerable\nimprovements of up to 14.4 percentage points in area under the learning curve\nare achieved, as well as a final accuracy close to the state of the art for all\nbut one benchmark, using only between 0.4% and 15% of the training data.",
          "link": "http://arxiv.org/abs/2107.05687",
          "publishedOn": "2021-07-14T01:41:50.525Z",
          "wordCount": 599,
          "title": "Uncertainty-based Query Strategies for Active Learning with Transformers. (arXiv:2107.05687v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1\">Yicheng Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1\">Xaq Pitkow</a>",
          "description": "Probabilistic graphical models provide a powerful tool to describe complex\nstatistical structure, with many real-world applications in science and\nengineering from controlling robotic arms to understanding neuronal\ncomputations. A major challenge for these graphical models is that inferences\nsuch as marginalization are intractable for general graphs. These inferences\nare often approximated by a distributed message-passing algorithm such as\nBelief Propagation, which does not always perform well on graphs with cycles,\nnor can it always be easily specified for complex continuous probability\ndistributions. Such difficulties arise frequently in expressive graphical\nmodels that include intractable higher-order interactions. In this paper we\nconstruct iterative message-passing algorithms using Graph Neural Networks\ndefined on factor graphs to achieve fast approximate inference on graphical\nmodels that involve many-variable interactions. Experimental results on several\nfamilies of graphical models demonstrate the out-of-distribution generalization\ncapability of our method to different sized graphs, and indicate the domain in\nwhich our method gains advantage over Belief Propagation.",
          "link": "http://arxiv.org/abs/2107.05729",
          "publishedOn": "2021-07-14T01:41:50.518Z",
          "wordCount": 594,
          "title": "Generalization of graph network inferences in higher-order probabilistic graphical models. (arXiv:2107.05729v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>",
          "description": "The Jeffreys divergence is a renown symmetrization of the statistical\nKullback-Leibler divergence which is often used in machine learning, signal\nprocessing, and information sciences. Since the Jeffreys divergence between the\nubiquitous Gaussian Mixture Models are not available in closed-form, many\ntechniques with various pros and cons have been proposed in the literature to\neither (i) estimate, (ii) approximate, or (iii) lower and upper bound this\ndivergence. In this work, we propose a simple yet fast heuristic to approximate\nthe Jeffreys divergence between two GMMs of arbitrary number of components. The\nheuristic relies on converting GMMs into pairs of dually parameterized\nprobability densities belonging to exponential families. In particular, we\nconsider Polynomial Exponential Densities, and design a goodness-of-fit\ncriterion to measure the dissimilarity between a GMM and a PED which is a\ngeneralization of the Hyv\\\"arinen divergence. This criterion allows one to\nselect the orders of the PEDs to approximate the GMMs. We demonstrate\nexperimentally that the computational time of our heuristic improves over the\nstochastic Monte Carlo estimation baseline by several orders of magnitude while\napproximating reasonably well the Jeffreys divergence, specially when the\nunivariate mixtures have a small number of modes.",
          "link": "http://arxiv.org/abs/2107.05901",
          "publishedOn": "2021-07-14T01:41:50.501Z",
          "wordCount": 639,
          "title": "Fast approximations of the Jeffreys divergence between univariate Gaussian mixture models via exponential polynomial densities. (arXiv:2107.05901v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Osa_T/0/1/0/all/0/1\">Takayuki Osa</a>",
          "description": "The objective function used in trajectory optimization is often non-convex\nand can have an infinite set of local optima. In such cases, there are diverse\nsolutions to perform a given task. Although there are a few methods to find\nmultiple solutions for motion planning, they are limited to generating a finite\nset of solutions. To address this issue, we presents an optimization method\nthat learns an infinite set of solutions in trajectory optimization. In our\nframework, diverse solutions are obtained by learning latent representations of\nsolutions. Our approach can be interpreted as training a deep generative model\nof collision-free trajectories for motion planning. The experimental results\nindicate that the trained model represents an infinite set of homotopic\nsolutions for motion planning problems.",
          "link": "http://arxiv.org/abs/2107.05842",
          "publishedOn": "2021-07-14T01:41:50.495Z",
          "wordCount": 564,
          "title": "Motion Planning by Learning the Solution Manifold in Trajectory Optimization. (arXiv:2107.05842v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chiheon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Saehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwoong Kim</a>",
          "description": "Large-batch training has been essential in leveraging large-scale datasets\nand models in deep learning. While it is computationally beneficial to use\nlarge batch sizes, it often requires a specially designed learning rate (LR)\nschedule to achieve a comparable level of performance as in smaller batch\ntraining. Especially, when the number of training epochs is constrained, the\nuse of a large LR and a warmup strategy is critical in the final performance of\nlarge-batch training due to the reduced number of updating steps. In this work,\nwe propose an automated LR scheduling algorithm which is effective for neural\nnetwork training with a large batch size under the given epoch budget. In\nspecific, the whole schedule consists of two phases: adaptive warmup and\npredefined decay, where the LR is increased until the training loss no longer\ndecreases and decreased to zero until the end of training. Here, whether the\ntraining loss has reached the minimum value is robustly checked with Gaussian\nprocess smoothing in an online manner with a low computational burden. Coupled\nwith adaptive stochastic optimizers such as AdamP and LAMB, the proposed\nscheduler successfully adjusts the LRs without cumbersome hyperparameter tuning\nand achieves comparable or better performances than tuned baselines on various\nimage classification benchmarks and architectures with a wide range of batch\nsizes.",
          "link": "http://arxiv.org/abs/2107.05855",
          "publishedOn": "2021-07-14T01:41:50.489Z",
          "wordCount": 659,
          "title": "Automated Learning Rate Scheduler for Large-batch Training. (arXiv:2107.05855v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parsaeefard_S/0/1/0/all/0/1\">Saeedeh Parsaeefard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_Garcia_A/0/1/0/all/0/1\">Alberto Leon-Garcia</a>",
          "description": "6G networks will greatly expand the support for data-oriented, autonomous\napplications for over the top (OTT) and networking use cases. The success of\nthese use cases will depend on the availability of big data sets which is not\npractical in many real scenarios due to the highly dynamic behavior of systems\nand the cost of data collection procedures. Transfer learning (TL) is a\npromising approach to deal with these challenges through the sharing of\nknowledge among diverse learning algorithms. with TL, the learning rate and\nlearning accuracy can be considerably improved. However, there are\nimplementation challenges to efficiently deploy and utilize TL in 6G. In this\npaper, we initiate this discussion by providing some performance metrics to\nmeasure the TL success. Then, we show how infrastructure, application,\nmanagement, and training planes of 6G can be adapted to handle TL. We provide\nexamples of TL in 6G and highlight the spatio-temporal features of data in 6G\nthat can lead to efficient TL. By simulation results, we demonstrate how\ntransferring the quantized neural network weights between two use cases can\nmake a trade-off between overheads and performance and attain more efficient TL\nin 6G. We also provide a list of future research directions in TL for 6G.",
          "link": "http://arxiv.org/abs/2107.05728",
          "publishedOn": "2021-07-14T01:41:50.482Z",
          "wordCount": 634,
          "title": "Toward Efficient Transfer Learning in 6G. (arXiv:2107.05728v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05707",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Herath_S/0/1/0/all/0/1\">Sumudu Herath</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xiao_X/0/1/0/all/0/1\">Xiao Xiao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cirak_F/0/1/0/all/0/1\">Fehmi Cirak</a>",
          "description": "Knitting is an effective technique for producing complex three-dimensional\nsurfaces owing to the inherent flexibility of interlooped yarns and recent\nadvances in manufacturing providing better control of local stitch patterns.\nFully yarn-level modelling of large-scale knitted membranes is not feasible.\nTherefore, we consider a two-scale homogenisation approach and model the\nmembrane as a Kirchhoff-Love shell on the macroscale and as Euler-Bernoulli\nrods on the microscale. The governing equations for both the shell and the rod\nare discretised with cubic B-spline basis functions. The solution of the\nnonlinear microscale problem requires a significant amount of time due to the\nlarge deformations and the enforcement of contact constraints, rendering\nconventional online computational homogenisation approaches infeasible. To\nsidestep this problem, we use a pre-trained statistical Gaussian Process\nRegression (GPR) model to map the macroscale deformations to macroscale\nstresses. During the offline learning phase, the GPR model is trained by\nsolving the microscale problem for a sufficiently rich set of deformation\nstates obtained by either uniform or Sobol sampling. The trained GPR model\nencodes the nonlinearities and anisotropies present in the microscale and\nserves as a material model for the macroscale Kirchhoff-Love shell. After\nverifying and validating the different components of the proposed approach, we\nintroduce several examples involving membranes subjected to tension and shear\nto demonstrate its versatility and good performance.",
          "link": "http://arxiv.org/abs/2107.05707",
          "publishedOn": "2021-07-14T01:41:50.476Z",
          "wordCount": 656,
          "title": "Computational modelling and data-driven homogenisation of knitted membranes. (arXiv:2107.05707v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1\">Rodrigo Castellon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We demonstrate that language models pre-trained on codified\n(discretely-encoded) music audio learn representations that are useful for\ndownstream MIR tasks. Specifically, we explore representations from Jukebox\n(Dhariwal et al. 2020): a music generation system containing a language model\ntrained on codified audio from 1M songs. To determine if Jukebox's\nrepresentations contain useful information for MIR, we use them as input\nfeatures to train shallow models on several MIR tasks. Relative to\nrepresentations from conventional MIR models which are pre-trained on tagging,\nwe find that using representations from Jukebox as input features yields 30%\nstronger performance on average across four MIR tasks: tagging, genre\nclassification, emotion recognition, and key detection. For key detection, we\nobserve that representations from Jukebox are considerably stronger than those\nfrom models pre-trained on tagging, suggesting that pre-training via codified\naudio language modeling may address blind spots in conventional approaches. We\ninterpret the strength of Jukebox's representations as evidence that modeling\naudio instead of tags provides richer representations for MIR.",
          "link": "http://arxiv.org/abs/2107.05677",
          "publishedOn": "2021-07-14T01:41:50.461Z",
          "wordCount": 621,
          "title": "Codified audio language modeling learns useful representations for music information retrieval. (arXiv:2107.05677v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1\">Ali Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1\">Madhusudhanan Balasubramanian</a>",
          "description": "Dense optical flow estimation is challenging when there are large\ndisplacements in a scene with heterogeneous motion dynamics, occlusion, and\nscene homogeneity. Traditional approaches to handle these challenges include\nhierarchical and multiresolution processing methods. Learning-based optical\nflow methods typically use a multiresolution approach with image warping when a\nbroad range of flow velocities and heterogeneous motion is present. Accuracy of\nsuch coarse-to-fine methods is affected by the ghosting artifacts when images\nare warped across multiple resolutions and by the vanishing problem in smaller\nscene extents with higher motion contrast. Previously, we devised strategies\nfor building compact dense prediction networks guided by the effective\nreceptive field (ERF) characteristics of the network (DDCNet). The DDCNet\ndesign was intentionally simple and compact allowing it to be used as a\nbuilding block for designing more complex yet compact networks. In this work,\nwe extend the DDCNet strategies to handle heterogeneous motion dynamics by\ncascading DDCNet based sub-nets with decreasing extents of their ERF. Our\nDDCNet with multiresolution capability (DDCNet-Multires) is compact without any\nspecialized network layers. We evaluate the performance of the DDCNet-Multires\nnetwork using standard optical flow benchmark datasets. Our experiments\ndemonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and\nprovides optical flow estimates with accuracy comparable to similar lightweight\nlearning-based methods.",
          "link": "http://arxiv.org/abs/2107.05634",
          "publishedOn": "2021-07-14T01:41:50.421Z",
          "wordCount": 664,
          "title": "DDCNet-Multires: Effective Receptive Field Guided Multiresolution CNN for Dense Prediction. (arXiv:2107.05634v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toichkin_D/0/1/0/all/0/1\">Dmitry Toichkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1\">Yansong Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qinghai Guo</a>",
          "description": "State-of-the-art artificial neural networks (ANNs) require labelled data or\nfeedback between layers, are often biologically implausible, and are vulnerable\nto adversarial attacks that humans are not susceptible to. On the other hand,\nHebbian learning in winner-take-all (WTA) networks, is unsupervised,\nfeed-forward, and biologically plausible. However, an objective optimization\ntheory for WTA networks has been missing, except under very limiting\nassumptions. Here we derive formally such a theory, based on biologically\nplausible but generic ANN elements. Through Hebbian learning, network\nparameters maintain a Bayesian generative model of the data. There is no\nsupervisory loss function, but the network does minimize cross-entropy between\nits activations and the input distribution. The key is a \"soft\" WTA where there\nis no absolute \"hard\" winner neuron, and a specific type of Hebbian-like\nplasticity of weights and biases. We confirm our theory in practice, where, in\nhandwritten digit (MNIST) recognition, our Hebbian algorithm, SoftHebb,\nminimizes cross-entropy without having access to it, and outperforms the more\nfrequently used, hard-WTA-based method. Strikingly, it even outperforms\nsupervised end-to-end backpropagation, under certain conditions. Specifically,\nin a two-layered network, SoftHebb outperforms backpropagation when the\ntraining dataset is only presented once, when the testing data is noisy, and\nunder gradient-based adversarial attacks. Adversarial attacks that confuse\nSoftHebb are also confusing to the human eye. Finally, the model can generate\ninterpolations of objects from its input distribution.",
          "link": "http://arxiv.org/abs/2107.05747",
          "publishedOn": "2021-07-14T01:41:50.403Z",
          "wordCount": 679,
          "title": "SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all networks. (arXiv:2107.05747v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05675",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gunlu_O/0/1/0/all/0/1\">Onur G&#xfc;nl&#xfc;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaefer_R/0/1/0/all/0/1\">Rafael F. Schaefer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "We consider a secret key agreement problem in which noisy physical unclonable\nfunction (PUF) outputs facilitate reliable, secure, and private key agreement\nwith the help of public, noiseless, and authenticated storage. PUF outputs are\nhighly correlated, so transform coding methods have been combined with scalar\nquantizers to extract uncorrelated bit sequences with reliability guarantees.\nFor PUF circuits with continuous-valued outputs, the models for transformed\noutputs are made more realistic by replacing the fitted distributions with\ncorresponding truncated ones. The state-of-the-art PUF methods that provide\nreliability guarantees to each extracted bit are shown to be inadequate to\nguarantee the same reliability level for all PUF outputs. Thus, a quality of\nservice parameter is introduced to control the percentage of PUF outputs for\nwhich a target reliability level can be guaranteed. A public ring oscillator\n(RO) output dataset is used to illustrate that a truncated Gaussian\ndistribution can be fitted to transformed RO outputs that are inputs to uniform\nscalar quantizers such that reliability guarantees can be provided for each bit\nextracted from any PUF device under additive Gaussian noise components by\neliminating a small subset of PUF outputs. Furthermore, we conversely show that\nit is not possible to provide such reliability guarantees without eliminating\nany PUF output if no extra secrecy and privacy leakage is allowed.",
          "link": "http://arxiv.org/abs/2107.05675",
          "publishedOn": "2021-07-14T01:41:50.350Z",
          "wordCount": 669,
          "title": "Quality of Service Guarantees for Physical Unclonable Functions. (arXiv:2107.05675v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahiner_A/0/1/0/all/0/1\">Arda Sahiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1\">Tolga Ergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1\">Batu Ozturkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartan_B/0/1/0/all/0/1\">Burak Bartan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1\">Morteza Mardani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>",
          "description": "Generative Adversarial Networks (GANs) are commonly used for modeling complex\ndistributions of data. Both the generators and discriminators of GANs are often\nmodeled by neural networks, posing a non-transparent optimization problem which\nis non-convex and non-concave over the generator and discriminator,\nrespectively. Such networks are often heuristically optimized with gradient\ndescent-ascent (GDA), but it is unclear whether the optimization problem\ncontains any saddle points, or whether heuristic methods can find them in\npractice. In this work, we analyze the training of Wasserstein GANs with\ntwo-layer neural network discriminators through the lens of convex duality, and\nfor a variety of generators expose the conditions under which Wasserstein GANs\ncan be solved exactly with convex optimization approaches, or can be\nrepresented as convex-concave games. Using this convex duality interpretation,\nwe further demonstrate the impact of different activation functions of the\ndiscriminator. Our observations are verified with numerical results\ndemonstrating the power of the convex interpretation, with applications in\nprogressive training of convex architectures corresponding to linear generators\nand quadratic-activation discriminators for CelebA image generation. The code\nfor our experiments is available at https://github.com/ardasahiner/ProCoGAN.",
          "link": "http://arxiv.org/abs/2107.05680",
          "publishedOn": "2021-07-14T01:41:50.330Z",
          "wordCount": 662,
          "title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naylor_M/0/1/0/all/0/1\">Mitchell Naylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+French_C/0/1/0/all/0/1\">Christi French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terker_S/0/1/0/all/0/1\">Samantha Terker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_U/0/1/0/all/0/1\">Uday Kamath</a>",
          "description": "The healthcare domain is one of the most exciting application areas for\nmachine learning, but a lack of model transparency contributes to a lag in\nadoption within the industry. In this work, we explore the current art of\nexplainability and interpretability within a case study in clinical text\nclassification, using a task of mortality prediction within MIMIC-III clinical\nnotes. We demonstrate various visualization techniques for fully interpretable\nmethods as well as model-agnostic post hoc attributions, and we provide a\ngeneralized method for evaluating the quality of explanations using infidelity\nand local Lipschitz across model types from logistic regression to BERT\nvariants. With these metrics, we introduce a framework through which\npractitioners and researchers can assess the frontier between a model's\npredictive performance and the quality of its available explanations. We make\nour code available to encourage continued refinement of these methods.",
          "link": "http://arxiv.org/abs/2107.05693",
          "publishedOn": "2021-07-14T01:41:50.323Z",
          "wordCount": 598,
          "title": "Quantifying Explainability in NLP and Analyzing Algorithms for Performance-Explainability Tradeoff. (arXiv:2107.05693v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05630",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dinsdale_N/0/1/0/all/0/1\">Nicola K Dinsdale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bluemke_E/0/1/0/all/0/1\">Emma Bluemke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sundaresan_V/0/1/0/all/0/1\">Vaanathi Sundaresan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smith_S/0/1/0/all/0/1\">Stephen Smith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namburete_A/0/1/0/all/0/1\">Ana IL Namburete</a>",
          "description": "The combination of deep learning image analysis methods and large-scale\nimaging datasets offers many opportunities to imaging neuroscience and\nepidemiology. However, despite the success of deep learning when applied to\nmany neuroimaging tasks, there remain barriers to the clinical translation of\nlarge-scale datasets and processing tools. Here, we explore the main challenges\nand the approaches that have been explored to overcome them. We focus on issues\nrelating to data availability, interpretability, evaluation and logistical\nchallenges, and discuss the challenges we believe are still to be overcome to\nenable the full success of big data deep learning approaches to be experienced\noutside of the research field.",
          "link": "http://arxiv.org/abs/2107.05630",
          "publishedOn": "2021-07-14T01:41:50.309Z",
          "wordCount": 560,
          "title": "Challenges for machine learning in clinical translation of big data imaging studies. (arXiv:2107.05630v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiangzhu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chonghui Guo</a>",
          "description": "In recent years, multi-view learning technologies for various applications\nhave attracted a surge of interest. Due to more compatible and complementary\ninformation from multiple views, existing multi-view methods could achieve more\npromising performance than conventional single-view methods in most situations.\nHowever, there are still no sufficient researches on the unified framework in\nexisting multi-view works. Meanwhile, how to efficiently integrate multi-view\ninformation is still full of challenges. In this paper, we propose a novel\nmulti-view learning framework, which aims to leverage most existing graph\nembedding works into a unified formula via introducing the graph consensus\nterm. In particular, our method explores the graph structure in each view\nindependently to preserve the diversity property of graph embedding methods.\nMeanwhile, we choose heterogeneous graphs to construct the graph consensus term\nto explore the correlations among multiple views jointly. To this end, the\ndiversity and complementary information among different views could be\nsimultaneously considered. Furthermore, the proposed framework is utilized to\nimplement the multi-view extension of Locality Linear Embedding, named\nMulti-view Locality Linear Embedding (MvLLE), which could be efficiently solved\nby applying the alternating optimization strategy. Empirical validations\nconducted on six benchmark datasets can show the effectiveness of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/2105.11781",
          "publishedOn": "2021-07-13T01:59:37.467Z",
          "wordCount": 659,
          "title": "A unified framework based on graph consensus term for multi-view learning. (arXiv:2105.11781v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shichen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_K/0/1/0/all/0/1\">Kenric P. Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kon_M/0/1/0/all/0/1\">Mark A. Kon</a>",
          "description": "We present a coupled Variational Auto-Encoder (VAE) method that improves the\naccuracy and robustness of the probabilistic inferences on represented data.\nThe new method models the dependency between input feature vectors (images) and\nweighs the outliers with a higher penalty by generalizing the original loss\nfunction to the coupled entropy function, using the principles of nonlinear\nstatistical coupling. We evaluate the performance of the coupled VAE model\nusing the MNIST dataset. Compared with the traditional VAE algorithm, the\noutput images generated by the coupled VAE method are clearer and less blurry.\nThe visualization of the input images embedded in 2D latent variable space\nprovides a deeper insight into the structure of new model with coupled loss\nfunction: the latent variable has a smaller deviation and a more compact latent\nspace generates the output values. We analyze the histogram of the likelihoods\nof the input images using the generalized mean, which measures the model's\naccuracy as a function of the relative risk. The neutral accuracy, which is the\ngeometric mean and is consistent with a measure of the Shannon cross-entropy,\nis improved. The robust accuracy, measured by the -2/3 generalized mean, is\nalso improved.",
          "link": "http://arxiv.org/abs/1906.00536",
          "publishedOn": "2021-07-13T01:59:37.461Z",
          "wordCount": 688,
          "title": "Coupled VAE: Improved Accuracy and Robustness of a Variational Autoencoder. (arXiv:1906.00536v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Che Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>",
          "description": "There are two major classes of natural language grammar -- the dependency\ngrammar that models one-to-one correspondences between words and the\nconstituency grammar that models the assembly of one or several corresponded\nwords. While previous unsupervised parsing methods mostly focus on only\ninducing one class of grammars, we introduce a novel model, StructFormer, that\ncan simultaneously induce dependency and constituency structure. To achieve\nthis, we propose a new parsing framework that can jointly generate a\nconstituency tree and dependency graph. Then we integrate the induced\ndependency relations into the transformer, in a differentiable manner, through\na novel dependency-constrained self-attention mechanism. Experimental results\nshow that our model can achieve strong results on unsupervised constituency\nparsing, unsupervised dependency parsing, and masked language modeling at the\nsame time.",
          "link": "http://arxiv.org/abs/2012.00857",
          "publishedOn": "2021-07-13T01:59:37.448Z",
          "wordCount": 626,
          "title": "StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling. (arXiv:2012.00857v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07405",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1\">Wu Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>",
          "description": "Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank\ncovariances) is computationally challenging due to difficult Fisher-matrix\ncomputations. We address this issue by using \\emph{local-parameter coordinates}\nto obtain a flexible and efficient NGD method that works well for a\nwide-variety of structured parameterizations. We show four applications where\nour method (1) generalizes the exponential natural evolutionary strategy, (2)\nrecovers existing Newton-like algorithms, (3) yields new structured\nsecond-order algorithms, and (4) gives new algorithms to learn covariances of\nGaussian and Wishart-based distributions. We show results on a range of\nproblems from deep learning, variational inference, and evolution strategies.\nOur work opens a new direction for scalable structured geometric methods.",
          "link": "http://arxiv.org/abs/2102.07405",
          "publishedOn": "2021-07-13T01:59:37.442Z",
          "wordCount": 604,
          "title": "Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v6 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_D/0/1/0/all/0/1\">Debmalya Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medya_S/0/1/0/all/0/1\">Sourav Medya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzzi_B/0/1/0/all/0/1\">Brian Uzzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>",
          "description": "Graph Neural Networks (GNNs), a generalization of deep neural networks on\ngraph data have been widely used in various domains, ranging from drug\ndiscovery to recommender systems. However, GNNs on such applications are\nlimited when there are few available samples. Meta-learning has been an\nimportant framework to address the lack of samples in machine learning, and in\nrecent years, researchers have started to apply meta-learning to GNNs. In this\nwork, we provide a comprehensive survey of different meta-learning approaches\ninvolving GNNs on various graph problems showing the power of using these two\napproaches together. We categorize the literature based on proposed\narchitectures, shared representations, and applications. Finally, we discuss\nseveral exciting future research directions and open problems.",
          "link": "http://arxiv.org/abs/2103.00137",
          "publishedOn": "2021-07-13T01:59:37.436Z",
          "wordCount": 579,
          "title": "Meta-Learning with Graph Neural Networks: Methods and Applications. (arXiv:2103.00137v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanhua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiwen Xu</a>",
          "description": "Content feed, a type of product that recommends a sequence of items for users\nto browse and engage with, has gained tremendous popularity among social media\nplatforms. In this paper, we propose to study the diversity problem in such a\nscenario from an item sequence perspective using time series analysis\ntechniques. We derive a method called sliding spectrum decomposition (SSD) that\ncaptures users' perception of diversity in browsing a long item sequence. We\nalso share our experiences in designing and implementing a suitable item\nembedding method for accurate similarity measurement under long tail effect.\nCombined together, they are now fully implemented and deployed in Xiaohongshu\nApp's production recommender system that serves the main Explore Feed product\nfor tens of millions of users every day. We demonstrate the effectiveness and\nefficiency of the method through theoretical analysis, offline experiments and\nonline A/B tests.",
          "link": "http://arxiv.org/abs/2107.05204",
          "publishedOn": "2021-07-13T01:59:37.430Z",
          "wordCount": 596,
          "title": "Sliding Spectrum Decomposition for Diversified Recommendation. (arXiv:2107.05204v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+George_B/0/1/0/all/0/1\">Blessen George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K. Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>",
          "description": "Generative adversarial networks (GANs) are very popular to generate realistic\nimages, but they often suffer from the training instability issues and the\nphenomenon of mode loss. In order to attain greater diversity in GAN\nsynthesized data, it is critical to solving the problem of mode loss. Our work\nexplores probabilistic approaches to GAN modelling that could allow us to\ntackle these issues. We present Prb-GANs, a new variation that uses dropout to\ncreate a distribution over the network parameters with the posterior learnt\nusing variational inference. We describe theoretically and validate\nexperimentally using simple and complex datasets the benefits of such an\napproach. We look into further improvements using the concept of uncertainty\nmeasures. Through a set of further modifications to the loss functions for each\nnetwork of the GAN, we are able to get results that show the improvement of GAN\nperformance. Our methods are extremely simple and require very little\nmodification to existing GAN architecture.",
          "link": "http://arxiv.org/abs/2107.05241",
          "publishedOn": "2021-07-13T01:59:37.424Z",
          "wordCount": 597,
          "title": "Prb-GAN: A Probabilistic Framework for GAN Modelling. (arXiv:2107.05241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01863",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Mesuga_R/0/1/0/all/0/1\">Reymond Mesuga</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Bayanay_B/0/1/0/all/0/1\">Brian James Bayanay</a>",
          "description": "LIGO is considered the most sensitive and complicated gravitational\nexperiment ever built. Its main objective is to detect the gravitational wave\nfrom the strongest events in the universe by observing if the length of its\n4-kilometer arms change by a distance 10,000 times smaller than the diameter of\na proton. Due to its sensitivity, LIGO is prone to the disturbance of external\nnoises which affects the data being collected to detect the gravitational wave.\nThese noises are commonly called by the LIGO community as glitches. The\nobjective of this study is to evaluate the effeciency of various deep trasnfer\nlearning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch\nwaveform in gravitational wave data. The accuracy achieved by the said models\nare 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models\nachieved fairly high accuracy, it is observed that all of the model suffered\nfrom the lack of data for certain classes which is the main concern found in\nthe experiment.",
          "link": "http://arxiv.org/abs/2107.01863",
          "publishedOn": "2021-07-13T01:59:37.417Z",
          "wordCount": 650,
          "title": "On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data. (arXiv:2107.01863v2 [gr-qc] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06671",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1\">Thanh Nguyen-Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1\">Hung Tran-The</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "We study the statistical theory of offline reinforcement learning (RL) with\ndeep ReLU network function approximation. We analyze a variant of fitted-Q\niteration (FQI) algorithm under a new dynamic condition that we call Besov\ndynamic closure, which encompasses the conditions from prior analyses for deep\nneural network function approximation. Under Besov dynamic closure, we prove\nthat the FQI-type algorithm enjoys the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( \\kappa^{1 + d/\\alpha} \\cdot \\epsilon^{-2 -\n2d/\\alpha} \\right)$ where $\\kappa$ is a distribution shift measure, $d$ is the\ndimensionality of the state-action space, $\\alpha$ is the (possibly fractional)\nsmoothness parameter of the underlying MDP, and $\\epsilon$ is a user-specified\nprecision. This is an improvement over the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( K \\cdot \\kappa^{2 + d/\\alpha} \\cdot \\epsilon^{-2 -\nd/\\alpha} \\right)$ in the prior result [Yang et al., 2019] where $K$ is an\nalgorithmic iteration number which is arbitrarily large in practice.\nImportantly, our sample complexity is obtained under the new general dynamic\ncondition and a data-dependent structure where the latter is either ignored in\nprior algorithms or improperly handled by prior analyses. This is the first\ncomprehensive analysis for offline RL with deep ReLU network function\napproximation under a general setting.",
          "link": "http://arxiv.org/abs/2103.06671",
          "publishedOn": "2021-07-13T01:59:37.410Z",
          "wordCount": 674,
          "title": "Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks. (arXiv:2103.06671v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1\">Sneha Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithal_V/0/1/0/all/0/1\">Varun Mithal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polatkan_G/0/1/0/all/0/1\">Gungor Polatkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1\">Rohan Ramanath</a>",
          "description": "Attention Model has now become an important concept in neural networks that\nhas been researched within diverse application domains. This survey provides a\nstructured and comprehensive overview of the developments in modeling\nattention. In particular, we propose a taxonomy which groups existing\ntechniques into coherent categories. We review salient neural architectures in\nwhich attention has been incorporated, and discuss applications in which\nmodeling attention has shown a significant impact. We also describe how\nattention has been used to improve the interpretability of neural networks.\nFinally, we discuss some future research directions in attention. We hope this\nsurvey will provide a succinct introduction to attention models and guide\npractitioners while developing approaches for their applications.",
          "link": "http://arxiv.org/abs/1904.02874",
          "publishedOn": "2021-07-13T01:59:37.392Z",
          "wordCount": 593,
          "title": "An Attentive Survey of Attention Models. (arXiv:1904.02874v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>",
          "description": "We present a new learning-based framework to recover vehicle pose in SO(3)\nfrom a single RGB image. In contrast to previous works that map from local\nappearance to observation angles, we explore a progressive approach by\nextracting meaningful Intermediate Geometrical Representations (IGRs) to\nestimate egocentric vehicle orientation. This approach features a deep model\nthat transforms perceived intensities to IGRs, which are mapped to a 3D\nrepresentation encoding object orientation in the camera coordinate system.\nCore problems are what IGRs to use and how to learn them more effectively. We\nanswer the former question by designing IGRs based on an interpolated cuboid\nthat derives from primitive 3D annotation readily. The latter question\nmotivates us to incorporate geometry knowledge with a new loss function based\non a projective invariant. This loss function allows unlabeled data to be used\nin the training stage to improve representation learning. Without additional\nlabels, our system outperforms previous monocular RGB-based methods for joint\nvehicle detection and pose estimation on the KITTI benchmark, achieving\nperformance even comparable to stereo methods. Code and pre-trained models are\navailable at this https URL.",
          "link": "http://arxiv.org/abs/2011.08464",
          "publishedOn": "2021-07-13T01:59:37.383Z",
          "wordCount": 687,
          "title": "Exploring intermediate representation for monocular vehicle pose estimation. (arXiv:2011.08464v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Penco_L/0/1/0/all/0/1\">Luigi Penco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1\">Jean-Baptiste Mouret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivaldi_S/0/1/0/all/0/1\">Serena Ivaldi</a>",
          "description": "Humanoid robots could be versatile and intuitive human avatars that operate\nremotely in inaccessible places: the robot could reproduce in the remote\nlocation the movements of an operator equipped with a wearable motion capture\ndevice while sending visual feedback to the operator. While substantial\nprogress has been made on transferring (\"retargeting\") human motions to\nhumanoid robots, a major problem preventing the deployment of such systems in\nreal applications is the presence of communication delays between the human\ninput and the feedback from the robot: even a few hundred milliseconds of delay\ncan irreversibly disturb the operator, let alone a few seconds. To overcome\nthese delays, we introduce a system in which a humanoid robot executes commands\nbefore it actually receives them, so that the visual feedback appears to be\nsynchronized to the operator, whereas the robot executed the commands in the\npast. To do so, the robot continuously predicts future commands by querying a\nmachine learning model that is trained on past trajectories and conditioned on\nthe last received commands. In our experiments, an operator was able to\nsuccessfully control a humanoid robot (32 degrees of freedom) with stochastic\ndelays up to 2 seconds in several whole-body manipulation tasks, including\nreaching different targets, picking up, and placing a box at distinct\nlocations.",
          "link": "http://arxiv.org/abs/2107.01281",
          "publishedOn": "2021-07-13T01:59:37.375Z",
          "wordCount": 658,
          "title": "Prescient teleoperation of humanoid robots. (arXiv:2107.01281v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Atul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajesh_B/0/1/0/all/0/1\">Bulla Rajesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "Plant leaf diseases pose a significant danger to food security and they cause\ndepletion in quality and volume of production. Therefore accurate and timely\ndetection of leaf disease is very important to check the loss of the crops and\nmeet the growing food demand of the people. Conventional techniques depend on\nlab investigation and human skills which are generally costly and inaccessible.\nRecently, Deep Neural Networks have been exceptionally fruitful in image\nclassification. In this research paper, plant leaf disease detection employing\ntransfer learning is explored in the JPEG compressed domain. Here, the JPEG\ncompressed stream consisting of DCT coefficients is, directly fed into the\nNeural Network to improve the efficiency of classification. The experimental\nresults on JPEG compressed leaf dataset demonstrate the efficacy of the\nproposed model.",
          "link": "http://arxiv.org/abs/2107.04813",
          "publishedOn": "2021-07-13T01:59:37.352Z",
          "wordCount": 601,
          "title": "Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique. (arXiv:2107.04813v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunzong Xu</a>",
          "description": "We consider the general (stochastic) contextual bandit problem under the\nrealizability assumption, i.e., the expected reward, as a function of contexts\nand actions, belongs to a general function class $\\mathcal{F}$. We design a\nfast and simple algorithm that achieves the statistically optimal regret with\nonly ${O}(\\log T)$ calls to an offline regression oracle across all $T$ rounds.\nThe number of oracle calls can be further reduced to $O(\\log\\log T)$ if $T$ is\nknown in advance. Our results provide the first universal and optimal reduction\nfrom contextual bandits to offline regression, solving an important open\nproblem in the contextual bandit literature. A direct consequence of our\nresults is that any advances in offline regression immediately translate to\ncontextual bandits, statistically and computationally. This leads to faster\nalgorithms and improved regret guarantees for broader classes of contextual\nbandit problems.",
          "link": "http://arxiv.org/abs/2003.12699",
          "publishedOn": "2021-07-13T01:59:37.345Z",
          "wordCount": 647,
          "title": "Bypassing the Monster: A Faster and Simpler Optimal Algorithm for Contextual Bandits under Realizability. (arXiv:2003.12699v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yutao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deyi Li</a>",
          "description": "Most of the existing deep learning-based sequential recommendation approaches\nutilize the recurrent neural network architecture or self-attention to model\nthe sequential patterns and temporal influence among a user's historical\nbehavior and learn the user's preference at a specific time. However, these\nmethods have two main drawbacks. First, they focus on modeling users' dynamic\nstates from a user-centric perspective and always neglect the dynamics of items\nover time. Second, most of them deal with only the first-order user-item\ninteractions and do not consider the high-order connectivity between users and\nitems, which has recently been proved helpful for the sequential\nrecommendation. To address the above problems, in this article, we attempt to\nmodel user-item interactions by a bipartite graph structure and propose a new\nrecommendation approach based on a Position-enhanced and Time-aware Graph\nConvolutional Network (PTGCN) for the sequential recommendation. PTGCN models\nthe sequential patterns and temporal dynamics between user-item interactions by\ndefining a position-enhanced and time-aware graph convolution operation and\nlearning the dynamic representations of users and items simultaneously on the\nbipartite graph with a self-attention aggregator. Also, it realizes the\nhigh-order connectivity between users and items by stacking multi-layer graph\nconvolutions. To demonstrate the effectiveness of PTGCN, we carried out a\ncomprehensive evaluation of PTGCN on three real-world datasets of different\nsizes compared with a few competitive baselines. Experimental results indicate\nthat PTGCN outperforms several state-of-the-art models in terms of two\ncommonly-used evaluation metrics for ranking.",
          "link": "http://arxiv.org/abs/2107.05235",
          "publishedOn": "2021-07-13T01:59:37.330Z",
          "wordCount": 686,
          "title": "Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations. (arXiv:2107.05235v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.11645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsung-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosca_J/0/1/0/all/0/1\">Justinian Rosca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1\">Peter J. Ramadge</a>",
          "description": "We consider the problem of reinforcement learning when provided with (1) a\nbaseline control policy and (2) a set of constraints that the learner must\nsatisfy. The baseline policy can arise from demonstration data or a teacher\nagent and may provide useful cues for learning, but it might also be\nsub-optimal for the task at hand, and is not guaranteed to satisfy the\nspecified constraints, which might encode safety, fairness or other\napplication-specific requirements. In order to safely learn from baseline\npolicies, we propose an iterative policy optimization algorithm that alternates\nbetween maximizing expected return on the task, minimizing distance to the\nbaseline policy, and projecting the policy onto the constraint-satisfying set.\nWe analyze our algorithm theoretically and provide a finite-time convergence\nguarantee. In our experiments on five different control tasks, our algorithm\nconsistently outperforms several state-of-the-art baselines, achieving 10 times\nfewer constraint violations and 40% higher reward on average.",
          "link": "http://arxiv.org/abs/2006.11645",
          "publishedOn": "2021-07-13T01:59:37.315Z",
          "wordCount": 636,
          "title": "Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies. (arXiv:2006.11645v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menghan_H/0/1/0/all/0/1\">Hu Menghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guangtao_Z/0/1/0/all/0/1\">Zhai Guangtao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Ping_Z/0/1/0/all/0/1\">Zhang Xiao-Ping</a>",
          "description": "In recent years, computer-aided diagnosis has become an increasingly popular\ntopic. Methods based on convolutional neural networks have achieved good\nperformance in medical image segmentation and classification. Due to the\nlimitations of the convolution operation, the long-term spatial features are\noften not accurately obtained. Hence, we propose a TransClaw U-Net network\nstructure, which combines the convolution operation with the transformer\noperation in the encoding part. The convolution part is applied for extracting\nthe shallow spatial features to facilitate the recovery of the image resolution\nafter upsampling. The transformer part is used to encode the patches, and the\nself-attention mechanism is used to obtain global information between\nsequences. The decoding part retains the bottom upsampling structure for better\ndetail segmentation performance. The experimental results on Synapse\nMulti-organ Segmentation Datasets show that the performance of TransClaw U-Net\nis better than other network structures. The ablation experiments also prove\nthe generalization performance of TransClaw U-Net.",
          "link": "http://arxiv.org/abs/2107.05188",
          "publishedOn": "2021-07-13T01:59:37.309Z",
          "wordCount": 611,
          "title": "TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation. (arXiv:2107.05188v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04721",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1\">Shuyun Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_Z/0/1/0/all/0/1\">Ziming Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granley_J/0/1/0/all/0/1\">Jacob Granley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beyeler_M/0/1/0/all/0/1\">Michael Beyeler</a>",
          "description": "Fundus photography has routinely been used to document the presence and\nseverity of retinal degenerative diseases such as age-related macular\ndegeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical\npractice, for which the fovea and optic disc (OD) are important retinal\nlandmarks. However, the occurrence of lesions, drusen, and other retinal\nabnormalities during retinal degeneration severely complicates automatic\nlandmark detection and segmentation. Here we propose HBA-U-Net: a U-Net\nbackbone enriched with hierarchical bottleneck attention. The network consists\nof a novel bottleneck attention block that combines and refines self-attention,\nchannel attention, and relative-position attention to highlight retinal\nabnormalities that may be important for fovea and OD segmentation in the\ndegenerated retina. HBA-U-Net achieved state-of-the-art results on fovea\ndetection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of\n25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for\nAMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:\nED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for\nlandmark detection in the presence of a variety of retinal degenerative\ndiseases.",
          "link": "http://arxiv.org/abs/2107.04721",
          "publishedOn": "2021-07-13T01:59:37.292Z",
          "wordCount": 641,
          "title": "U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina. (arXiv:2107.04721v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14473",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1\">Tim De Ryck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>",
          "description": "Physics informed neural networks approximate solutions of PDEs by minimizing\npointwise residuals. We derive rigorous bounds on the error, incurred by PINNs\nin approximating the solutions of a large class of linear parabolic PDEs,\nnamely Kolmogorov equations that include the heat equation and Black-Scholes\nequation of option pricing, as examples. We construct neural networks, whose\nPINN residual (generalization error) can be made as small as desired. We also\nprove that the total $L^2$-error can be bounded by the generalization error,\nwhich in turn is bounded in terms of the training error, provided that a\nsufficient number of randomly chosen training (collocation) points is used.\nMoreover, we prove that the size of the PINNs and the number of training\nsamples only grow polynomially with the underlying dimension, enabling PINNs to\novercome the curse of dimensionality in this context. These results enable us\nto provide a comprehensive error analysis for PINNs in approximating Kolmogorov\nPDEs.",
          "link": "http://arxiv.org/abs/2106.14473",
          "publishedOn": "2021-07-13T01:59:37.287Z",
          "wordCount": 617,
          "title": "Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs. (arXiv:2106.14473v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shota Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Learning from positive and unlabeled (PU) data is an important problem in\nvarious applications. Most of the recent approaches for PU classification\nassume that the class-prior (the ratio of positive samples) in the training\nunlabeled dataset is identical to that of the test data, which does not hold in\nmany practical cases. In addition, we usually do not know the class-priors of\nthe training and test data, thus we have no clue on how to train a classifier\nwithout them. To address these problems, we propose a novel PU classification\nmethod based on density ratio estimation. A notable advantage of our proposed\nmethod is that it does not require the class-priors in the training phase;\nclass-prior shift is incorporated only in the test phase. We theoretically\njustify our proposed method and experimentally demonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2107.05045",
          "publishedOn": "2021-07-13T01:59:37.281Z",
          "wordCount": 576,
          "title": "Positive-Unlabeled Classification under Class-Prior Shift: A Prior-invariant Approach Based on Density Ratio Estimation. (arXiv:2107.05045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Starting from childhood, the human brain restructures and rewires throughout\nlife. Characterizing such complex brain development requires effective analysis\nof longitudinal and multi-modal neuroimaging data. Here, we propose such an\nanalysis approach named Longitudinal Correlation Analysis (LCA). LCA couples\nthe data of two modalities by first reducing the input from each modality to a\nlatent representation based on autoencoders. A self-supervised strategy then\nrelates the two latent spaces by jointly disentangling two directions, one in\neach space, such that the longitudinal changes in latent representations along\nthose directions are maximally correlated between modalities. We applied LCA to\nanalyze the longitudinal T1-weighted and diffusion-weighted MRIs of 679 youths\nfrom the National Consortium on Alcohol and Neurodevelopment in Adolescence.\nUnlike existing approaches that focus on either cross-sectional or single-modal\nmodeling, LCA successfully unraveled coupled macrostructural and\nmicrostructural brain development from morphological and diffusivity features\nextracted from the data. A retesting of LCA on raw 3D image volumes of those\nsubjects successfully replicated the findings from the feature-based analysis.\nLastly, the developmental effects revealed by LCA were inline with the current\nunderstanding of maturational patterns of the adolescent brain.",
          "link": "http://arxiv.org/abs/2107.04724",
          "publishedOn": "2021-07-13T01:59:37.276Z",
          "wordCount": 615,
          "title": "Longitudinal Correlation Analysis for Decoding Multi-Modal Brain Development. (arXiv:2107.04724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yi-Hui Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1\">I-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chin-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1\">Joann Ching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "This paper presents an attempt to employ the mask language modeling approach\nof BERT to pre-train a 12-layer Transformer model over 4,166 pieces of\npolyphonic piano MIDI files for tackling a number of symbolic-domain\ndiscriminative music understanding tasks. These include two note-level\nclassification tasks, i.e., melody extraction and velocity prediction, as well\nas two sequence-level classification tasks, i.e., composer classification and\nemotion classification. We find that, given a pre-trained Transformer, our\nmodels outperform recurrent neural network based baselines with less than 10\nepochs of fine-tuning. Ablation studies show that the pre-training remains\neffective even if none of the MIDI data of the downstream tasks are seen at the\npre-training stage, and that freezing the self-attention layers of the\nTransformer at the fine-tuning stage slightly degrades performance. All the\nfive datasets employed in this work are publicly available, as well as\ncheckpoints of our pre-trained and fine-tuned models. As such, our research can\nbe taken as a benchmark for symbolic-domain music understanding.",
          "link": "http://arxiv.org/abs/2107.05223",
          "publishedOn": "2021-07-13T01:59:37.270Z",
          "wordCount": 603,
          "title": "MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding. (arXiv:2107.05223v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1\">Toshinori Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takamitsu Matsubara</a>",
          "description": "The oscillating performance of off-policy learning and persisting errors in\nthe actor-critic (AC) setting call for algorithms that can conservatively learn\nto suit the stability-critical applications better. In this paper, we propose a\nnovel off-policy AC algorithm cautious actor-critic (CAC). The name cautious\ncomes from the doubly conservative nature that we exploit the classic policy\ninterpolation from conservative policy iteration for the actor and the\nentropy-regularization of conservative value iteration for the critic. Our key\nobservation is the entropy-regularized critic facilitates and simplifies the\nunwieldy interpolated actor update while still ensuring robust policy\nimprovement. We compare CAC to state-of-the-art AC methods on a set of\nchallenging continuous control problems and demonstrate that CAC achieves\ncomparable performance while significantly stabilizes learning.",
          "link": "http://arxiv.org/abs/2107.05217",
          "publishedOn": "2021-07-13T01:59:37.253Z",
          "wordCount": 544,
          "title": "Cautious Actor-Critic. (arXiv:2107.05217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devic_S/0/1/0/all/0/1\">Siddartha Devic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juba_B/0/1/0/all/0/1\">Brendan Juba</a>",
          "description": "Many reinforcement learning (RL) environments in practice feature enormous\nstate spaces that may be described compactly by a \"factored\" structure, that\nmay be modeled by Factored Markov Decision Processes (FMDPs). We present the\nfirst polynomial-time algorithm for RL with FMDPs that does not rely on an\noracle planner, and instead of requiring a linear transition model, only\nrequires a linear value function with a suitable local basis with respect to\nthe factorization. With this assumption, we can solve FMDPs in polynomial time\nby constructing an efficient separation oracle for convex optimization.\nImportantly, and in contrast to prior work, we do not assume that the\ntransitions on various factors are independent.",
          "link": "http://arxiv.org/abs/2107.05187",
          "publishedOn": "2021-07-13T01:59:37.247Z",
          "wordCount": 549,
          "title": "Polynomial Time Reinforcement Learning in Correlated FMDPs with Linear Value Functions. (arXiv:2107.05187v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.03745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1\">Mohamad H. Danesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anurag Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1\">Alan Fern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorram_S/0/1/0/all/0/1\">Saeed Khorram</a>",
          "description": "We introduce an approach for understanding control policies represented as\nrecurrent neural networks. Recent work has approached this problem by\ntransforming such recurrent policy networks into finite-state machines (FSM)\nand then analyzing the equivalent minimized FSM. While this led to interesting\ninsights, the minimization process can obscure a deeper understanding of a\nmachine's operation by merging states that are semantically distinct. To\naddress this issue, we introduce an analysis approach that starts with an\nunminimized FSM and applies more-interpretable reductions that preserve the key\ndecision points of the policy. We also contribute an attention tool to attain a\ndeeper understanding of the role of observations in the decisions. Our case\nstudies on 7 Atari games and 3 control benchmarks demonstrate that the approach\ncan reveal insights that have not been previously noticed.",
          "link": "http://arxiv.org/abs/2006.03745",
          "publishedOn": "2021-07-13T01:59:37.241Z",
          "wordCount": 607,
          "title": "Re-understanding Finite-State Representations of Recurrent Policy Networks. (arXiv:2006.03745v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05201",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Lin_H/0/1/0/all/0/1\">Hengxu Lin</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "Modeling and managing portfolio risk is perhaps the most important step to\nachieve growing and preserving investment performance. Within the modern\nportfolio construction framework that built on Markowitz's theory, the\ncovariance matrix of stock returns is required to model the portfolio risk.\nTraditional approaches to estimate the covariance matrix are based on human\ndesigned risk factors, which often requires tremendous time and effort to\ndesign better risk factors to improve the covariance estimation. In this work,\nwe formulate the quest of mining risk factors as a learning problem and propose\na deep learning solution to effectively \"design\" risk factors with neural\nnetworks. The learning objective is carefully set to ensure the learned risk\nfactors are effective in explaining stock returns as well as have desired\northogonality and stability. Our experiments on the stock market data\ndemonstrate the effectiveness of the proposed method: our method can obtain\n$1.9\\%$ higher explained variance measured by $R^2$ and also reduce the risk of\na global minimum variance portfolio. Incremental analysis further supports our\ndesign of both the architecture and the learning objective.",
          "link": "http://arxiv.org/abs/2107.05201",
          "publishedOn": "2021-07-13T01:59:37.235Z",
          "wordCount": 630,
          "title": "Deep Risk Model: A Deep Learning Solution for Mining Latent Risk Factors to Improve Covariance Matrix Estimation. (arXiv:2107.05201v1 [q-fin.RM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1\">Harikrishna Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>",
          "description": "Many modern machine learning applications come with complex and nuanced\ndesign goals such as minimizing the worst-case error, satisfying a given\nprecision or recall target, or enforcing group-fairness constraints. Popular\ntechniques for optimizing such non-decomposable objectives reduce the problem\ninto a sequence of cost-sensitive learning tasks, each of which is then solved\nby re-weighting the training loss with example-specific costs. We point out\nthat the standard approach of re-weighting the loss to incorporate label costs\ncan produce unsatisfactory results when used to train over-parameterized\nmodels. As a remedy, we propose new cost-sensitive losses that extend the\nclassical idea of logit adjustment to handle more general cost matrices. Our\nlosses are calibrated, and can be further improved with distilled labels from a\nteacher model. Through experiments on benchmark image datasets, we showcase the\neffectiveness of our approach in training ResNet models with common robust and\nconstrained optimization objectives.",
          "link": "http://arxiv.org/abs/2107.04641",
          "publishedOn": "2021-07-13T01:59:37.229Z",
          "wordCount": 574,
          "title": "Training Over-parameterized Models with Non-decomposable Objectives. (arXiv:2107.04641v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ly_R/0/1/0/all/0/1\">Racine Ly</a>",
          "description": "The improvement of computers' capacities, advancements in algorithmic\ntechniques, and the significant increase of available data have enabled the\nrecent developments of Artificial Intelligence (AI) technology. One of its\nbranches, called Machine Learning (ML), has shown strong capacities in\nmimicking characteristics attributed to human intelligence, such as vision,\nspeech, and problem-solving. However, as previous technological revolutions\nsuggest, their most significant impacts could be mostly expected on other\nsectors that were not traditional users of that technology. The agricultural\nsector is vital for African economies; improving yields, mitigating losses, and\neffective management of natural resources are crucial in a climate change era.\nMachine Learning is a technology with an added value in making predictions,\nhence the potential to reduce uncertainties and risk across sectors, in this\ncase, the agricultural sector. The purpose of this paper is to contextualize\nand discuss barriers to ML-based solutions for African agriculture. In the\nsecond section, we provided an overview of ML technology from a historical and\ntechnical perspective and its main driving force. In the third section, we\nprovided a brief review of the current use of ML in agriculture. Finally, in\nsection 4, we discuss ML growing interest in Africa and the potential barriers\nto creating and using ML-based solutions in the agricultural sector.",
          "link": "http://arxiv.org/abs/2107.05101",
          "publishedOn": "2021-07-13T01:59:37.223Z",
          "wordCount": 682,
          "title": "Machine Learning Challenges and Opportunities in the African Agricultural Sector -- A General Perspective. (arXiv:2107.05101v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhijie Sasha Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christenson_L/0/1/0/all/0/1\">Lauren Christenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulton_L/0/1/0/all/0/1\">Lawrence Fulton</a>",
          "description": "Social media has become an essential channel for posting disaster-related\ninformation, which provide governments and relief agencies real-time data for\nbetter disaster management. However, research in this field has not received\nsufficient attention and extracting useful information is still challenging.\nThis paper aims to improve disaster relief efficiency via mining and analyzing\nsocial media data like public attitudes towards disaster response and public\ndemands for targeted relief supplies during different types of disasters. We\nfocus on different natural disasters based on properties such as types,\ndurations, and damages, which contains a total of 41,993 tweets. In this paper,\npublic perception is assessed qualitatively by manually classified tweets,\nwhich contain information like the demand for targeted relief supplies,\nsatisfactions of disaster response, and public fear. Public attitudes to\nnatural disasters are studied via a quantitative analysis using eight machine\nlearning models. To better provide decision-makers with the appropriate model,\nthe comparison of machine learning models based on computational time and\nprediction accuracy is conducted. The change of public opinion during different\nnatural disasters and the evolution of people's behavior of using social media\nfor disaster relief in the face of the identical type of natural disasters as\nTwitter continues to evolve are studied. The results in this paper demonstrate\nthe feasibility and validation of the proposed research approach and provide\nrelief agencies with insights into better disaster management.",
          "link": "http://arxiv.org/abs/2005.07019",
          "publishedOn": "2021-07-13T01:59:37.204Z",
          "wordCount": 737,
          "title": "Social Media Information Sharing for Natural Disaster Response. (arXiv:2005.07019v5 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05342",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Celik_N/0/1/0/all/0/1\">Numan Celik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Soumya Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braden_B/0/1/0/all/0/1\">Barbara Braden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1\">Jens Rittscher</a>",
          "description": "Gastrointestinal (GI) cancer precursors require frequent monitoring for risk\nstratification of patients. Automated segmentation methods can help to assess\nrisk areas more accurately, and assist in therapeutic procedures or even\nremoval. In clinical practice, addition to the conventional white-light imaging\n(WLI), complimentary modalities such as narrow-band imaging (NBI) and\nfluorescence imaging are used. While, today most segmentation approaches are\nsupervised and only concentrated on a single modality dataset, this work\nexploits to use a target-independent unsupervised domain adaptation (UDA)\ntechnique that is capable to generalize to an unseen target modality. In this\ncontext, we propose a novel UDA-based segmentation method that couples the\nvariational autoencoder and U-Net with a common EfficientNet-B4 backbone, and\nuses a joint loss for latent-space optimization for target samples. We show\nthat our model can generalize to unseen target NBI (target) modality when\ntrained using only WLI (source) modality. Our experiments on both upper and\nlower GI endoscopy data show the effectiveness of our approach compared to\nnaive supervised approach and state-of-the-art UDA segmentation methods.",
          "link": "http://arxiv.org/abs/2107.05342",
          "publishedOn": "2021-07-13T01:59:37.198Z",
          "wordCount": 632,
          "title": "EndoUDA: A modality independent segmentation approach for endoscopy imaging. (arXiv:2107.05342v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Umer_M/0/1/0/all/0/1\">Muhammad Azmi Umer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_C/0/1/0/all/0/1\">Chuadhry Mujeeb Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jilani_M/0/1/0/all/0/1\">Muhammad Taha Jilani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Aditya P. Mathur</a>",
          "description": "Adversarial learning is used to test the robustness of machine learning\nalgorithms under attack and create attacks that deceive the anomaly detection\nmethods in Industrial Control System (ICS). Given that security assessment of\nan ICS demands that an exhaustive set of possible attack patterns is studied,\nin this work, we propose an association rule mining-based attack generation\ntechnique. The technique has been implemented using data from a secure Water\nTreatment plant. The proposed technique was able to generate more than 300,000\nattack patterns constituting a vast majority of new attack vectors which were\nnot seen before. Automatically generated attacks improve our understanding of\nthe potential attacks and enable the design of robust attack detection\ntechniques.",
          "link": "http://arxiv.org/abs/2107.05127",
          "publishedOn": "2021-07-13T01:59:37.192Z",
          "wordCount": 570,
          "title": "Attack Rules: An Adversarial Approach to Generate Attacks for Industrial Control Systems using Machine Learning. (arXiv:2107.05127v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/1807.11398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bengs_V/0/1/0/all/0/1\">Viktor Bengs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busa_Fekete_R/0/1/0/all/0/1\">Robert Busa-Fekete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesaoudi_Paul_A/0/1/0/all/0/1\">Adil El Mesaoudi-Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "In machine learning, the notion of multi-armed bandits refers to a class of\nonline learning problems, in which an agent is supposed to simultaneously\nexplore and exploit a given set of choice alternatives in the course of a\nsequential decision process. In the standard setting, the agent learns from\nstochastic feedback in the form of real-valued rewards. In many applications,\nhowever, numerical reward signals are not readily available -- instead, only\nweaker information is provided, in particular relative preferences in the form\nof qualitative comparisons between pairs of alternatives. This observation has\nmotivated the study of variants of the multi-armed bandit problem, in which\nmore general representations are used both for the type of feedback to learn\nfrom and the target of prediction. The aim of this paper is to provide a survey\nof the state of the art in this field, referred to as preference-based\nmulti-armed bandits or dueling bandits. To this end, we provide an overview of\nproblems that have been considered in the literature as well as methods for\ntackling them. Our taxonomy is mainly based on the assumptions made by these\nmethods about the data-generating process and, related to this, the properties\nof the preference-based feedback.",
          "link": "http://arxiv.org/abs/1807.11398",
          "publishedOn": "2021-07-13T01:59:37.185Z",
          "wordCount": 678,
          "title": "Preference-based Online Learning with Dueling Bandits: A Survey. (arXiv:1807.11398v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.12322",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salomone_R/0/1/0/all/0/1\">Robert Salomone</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roosta_F/0/1/0/all/0/1\">Fred Roosta</a>",
          "description": "For sampling from a log-concave density, we study implicit integrators\nresulting from $\\theta$-method discretization of the overdamped Langevin\ndiffusion stochastic differential equation. Theoretical and algorithmic\nproperties of the resulting sampling methods for $ \\theta \\in [0,1] $ and a\nrange of step sizes are established. Our results generalize and extend prior\nworks in several directions. In particular, for $\\theta\\ge1/2$, we prove\ngeometric ergodicity and stability of the resulting methods for all step sizes.\nWe show that obtaining subsequent samples amounts to solving a strongly-convex\noptimization problem, which is readily achievable using one of numerous\nexisting methods. Numerical examples supporting our theoretical analysis are\nalso presented.",
          "link": "http://arxiv.org/abs/1903.12322",
          "publishedOn": "2021-07-13T01:59:37.168Z",
          "wordCount": 556,
          "title": "Implicit Langevin Algorithms for Sampling From Log-concave Densities. (arXiv:1903.12322v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>",
          "description": "We propose a semi-supervised learning method for building end-to-end rich\ntranscription-style automatic speech recognition (RT-ASR) systems from\nsmall-scale rich transcription-style and large-scale common transcription-style\ndatasets. In spontaneous speech tasks, various speech phenomena such as\nfillers, word fragments, laughter and coughs, etc. are often included. While\ncommon transcriptions do not give special awareness to these phenomena, rich\ntranscriptions explicitly convert them into special phenomenon tokens as well\nas textual tokens. In previous studies, the textual and phenomenon tokens were\nsimultaneously estimated in an end-to-end manner. However, it is difficult to\nbuild accurate RT-ASR systems because large-scale rich transcription-style\ndatasets are often unavailable. To solve this problem, our training method uses\na limited rich transcription-style dataset and common transcription-style\ndataset simultaneously. The Key process in our semi-supervised learning is to\nconvert the common transcription-style dataset into a pseudo-rich\ntranscription-style dataset. To this end, we introduce style tokens which\ncontrol phenomenon tokens are generated or not into transformer-based\nautoregressive modeling. We use this modeling for generating the pseudo-rich\ntranscription-style datasets and for building RT-ASR system from the pseudo and\noriginal datasets. Our experiments on spontaneous ASR tasks showed the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.05382",
          "publishedOn": "2021-07-13T01:59:37.161Z",
          "wordCount": 648,
          "title": "End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning. (arXiv:2107.05382v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weina Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>",
          "description": "Being able to explain the prediction to clinical end-users is a necessity to\nleverage the power of AI models for clinical decision support. For medical\nimages, saliency maps are the most common form of explanation. The maps\nhighlight important features for AI model's prediction. Although many saliency\nmap methods have been proposed, it is unknown how well they perform on\nexplaining decisions on multi-modal medical images, where each modality/channel\ncarries distinct clinical meanings of the same underlying biomedical\nphenomenon. Understanding such modality-dependent features is essential for\nclinical users' interpretation of AI decisions. To tackle this clinically\nimportant but technically ignored problem, we propose the MSFI\n(Modality-Specific Feature Importance) metric to examine whether saliency maps\ncan highlight modality-specific important features. MSFI encodes the clinical\nrequirements on modality prioritization and modality-specific feature\nlocalization. Our evaluations on 16 commonly used saliency map methods,\nincluding a clinician user study, show that although most saliency map methods\ncaptured modality importance information in general, most of them failed to\nhighlight modality-specific important features consistently and precisely. The\nevaluation results guide the choices of saliency map methods and provide\ninsights to propose new ones targeting clinical applications.",
          "link": "http://arxiv.org/abs/2107.05047",
          "publishedOn": "2021-07-13T01:59:37.155Z",
          "wordCount": 646,
          "title": "One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images. (arXiv:2107.05047v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2001.02492",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_W/0/1/0/all/0/1\">Wenqing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1\">Chuhan Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jabari_S/0/1/0/all/0/1\">Saif Eddin Jabari</a>",
          "description": "This paper addresses the problem of short-term traffic prediction for\nsignalized traffic operations management. Specifically, we focus on predicting\nsensor states in high-resolution (second-by-second). This contrasts with\ntraditional traffic forecasting problems, which have focused on predicting\naggregated traffic variables, typically over intervals that are no shorter than\n5 minutes. Our contributions can be summarized as offering three insights:\nfirst, we show how the prediction problem can be modeled as a matrix completion\nproblem. Second, we employ a block-coordinate descent algorithm and demonstrate\nthat the algorithm converges in sub-linear time to a block coordinate-wise\noptimizer. This allows us to capitalize on the \"bigness\" of high-resolution\ndata in a computationally feasible way. Third, we develop an ensemble learning\n(or adaptive boosting) approach to reduce the training error to within any\narbitrary error threshold. The latter utilizes past days so that the boosting\ncan be interpreted as capturing periodic patterns in the data. The performance\nof the proposed method is analyzed theoretically and tested empirically using\nboth simulated data and a real-world high-resolution traffic dataset from Abu\nDhabi, UAE. Our experimental results show that the proposed method outperforms\nother state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2001.02492",
          "publishedOn": "2021-07-13T01:59:37.149Z",
          "wordCount": 664,
          "title": "Nonlinear Traffic Prediction as a Matrix Completion Problem with Ensemble Learning. (arXiv:2001.02492v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+YinchuanLi/0/1/0/all/0/1\">YinchuanLi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XiaofengLiu/0/1/0/all/0/1\">XiaofengLiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XuZhang/0/1/0/all/0/1\">XuZhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YunfengShao/0/1/0/all/0/1\">YunfengShao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+QingWang/0/1/0/all/0/1\">QingWang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YanhuiGeng/0/1/0/all/0/1\">YanhuiGeng</a>",
          "description": "Federated Learning (FL) is a collaborative machine learning technique to\ntrain a global model without obtaining clients' private data. The main\nchallenges in FL are statistical diversity among clients, limited computing\ncapability among client equipments and the excessive communication overhead and\nlong latency between server and clients. To address these problems,\n\nwe propose a novel personalized federated learning via maximizing correlation\npFedMac), and further extend it to sparse and hierarchical models. By\nminimizing loss functions including the properties of an approximated L1-norm\nand the hierarchical correlation, the performance on statistical diversity data\nis improved and the communicational and computational loads required in the\nnetwork are reduced. Theoretical proofs show that pFedMac performs better than\nthe L2-norm distance based personalization methods. Experimentally, we\ndemonstrate the benefits of this sparse hierarchical personalization\narchitecture compared with the state-of-the-art personalization methods and\ntheir extensions (e.g. pFedMac achieves 99.75% accuracy on MNIST and 87.27%\naccuracy on Synthetic under heterogeneous and non-i.i.d data distributions)",
          "link": "http://arxiv.org/abs/2107.05330",
          "publishedOn": "2021-07-13T01:59:37.143Z",
          "wordCount": 592,
          "title": "Personalized Federated Learning via Maximizing Correlation with Sparse and Hierarchical Extensions. (arXiv:2107.05330v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>",
          "description": "We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n\nHaving all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n\nWe solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article. Just like for linear convolution a\nmorphological convolution is specified by a kernel that we train in our\nPDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling\nand ReLUs as they are already subsumed by morphological convolutions.\n\nWe present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.",
          "link": "http://arxiv.org/abs/2001.09046",
          "publishedOn": "2021-07-13T01:59:37.138Z",
          "wordCount": 751,
          "title": "PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianwen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianwei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shenghuan He</a>",
          "description": "In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.",
          "link": "http://arxiv.org/abs/2107.05377",
          "publishedOn": "2021-07-13T01:59:37.103Z",
          "wordCount": 580,
          "title": "A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1\">Ilja Kuzborskij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>",
          "description": "We explore the ability of overparameterized shallow neural networks to learn\nLipschitz regression functions with and without label noise when trained by\nGradient Descent (GD). To avoid the problem that in the presence of noisy\nlabels, neural networks trained to nearly zero training error are inconsistent\non this class, we propose an early stopping rule that allows us to show optimal\nrates. This provides an alternative to the result of Hu et al. (2021) who\nstudied the performance of $\\ell 2$ -regularized GD for training shallow\nnetworks in nonparametric regression which fully relied on the infinite-width\nnetwork (Neural Tangent Kernel (NTK)) approximation. Here we present a simpler\nanalysis which is based on a partitioning argument of the input space (as in\nthe case of 1-nearest-neighbor rule) coupled with the fact that trained neural\nnetworks are smooth with respect to their inputs when trained by GD. In the\nnoise-free case the proof does not rely on any kernelization and can be\nregarded as a finite-width result. In the case of label noise, by slightly\nmodifying the proof, the noise is controlled using a technique of Yao, Rosasco,\nand Caponnetto (2007).",
          "link": "http://arxiv.org/abs/2107.05341",
          "publishedOn": "2021-07-13T01:59:37.083Z",
          "wordCount": 633,
          "title": "Nonparametric Regression with Shallow Overparameterized Neural Networks Trained by GD with Early Stopping. (arXiv:2107.05341v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Anish Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rudrajit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1\">Sujay Sanghavi</a>",
          "description": "In this paper we study test time decoding; an ubiquitous step in almost all\nsequential text generation task spanning across a wide array of natural\nlanguage processing (NLP) problems. Our main contribution is to develop a\ncontinuous relaxation framework for the combinatorial NP-hard decoding problem\nand propose Disco - an efficient algorithm based on standard first order\ngradient based. We provide tight analysis and show that our proposed algorithm\nlinearly converges to within $\\epsilon$ neighborhood of the optima. Finally, we\nperform preliminary experiments on the task of adversarial text generation and\nshow superior performance of Disco over several popular decoding approaches.",
          "link": "http://arxiv.org/abs/2107.05380",
          "publishedOn": "2021-07-13T01:59:37.077Z",
          "wordCount": 552,
          "title": "DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation. (arXiv:2107.05380v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2002.04276",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Woznica_K/0/1/0/all/0/1\">Katarzyna Wo&#x17a;nica</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Biecek_P/0/1/0/all/0/1\">Przemys&#x142;aw Biecek</a>",
          "description": "Meta-learning is a field that aims at discovering how different machine\nlearning algorithms perform on a wide range of predictive tasks. Such knowledge\nspeeds up the hyperparameter tuning or feature engineering. With the use of\nsurrogate models various aspects of the predictive task such as meta-features,\nlandmarker models e.t.c. are used to predict the expected performance. State of\nthe art approaches are focused on searching for the best meta-model but do not\nexplain how these different aspects contribute to its performance. However, to\nbuild a new generation of meta-models we need a deeper understanding of the\nimportance and effect of meta-features on the model tunability. In this paper,\nwe propose techniques developed for eXplainable Artificial Intelligence (XAI)\nto examine and extract knowledge from black-box surrogate models. To our\nknowledge, this is the first paper that shows how post-hoc explainability can\nbe used to improve the meta-learning.",
          "link": "http://arxiv.org/abs/2002.04276",
          "publishedOn": "2021-07-13T01:59:37.071Z",
          "wordCount": 583,
          "title": "Towards explainable meta-learning. (arXiv:2002.04276v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1\">Kenneth Enevoldsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielbo_K/0/1/0/all/0/1\">Kristoffer Nielbo</a>",
          "description": "Danish natural language processing (NLP) has in recent years obtained\nconsiderable improvements with the addition of multiple new datasets and\nmodels. However, at present, there is no coherent framework for applying\nstate-of-the-art models for Danish. We present DaCy: a unified framework for\nDanish NLP built on SpaCy. DaCy uses efficient multitask models which obtain\nstate-of-the-art performance on named entity recognition, part-of-speech\ntagging, and dependency parsing. DaCy contains tools for easy integration of\nexisting models such as for polarity, emotion, or subjectivity detection. In\naddition, we conduct a series of tests for biases and robustness of Danish NLP\npipelines through augmentation of the test set of DaNE. DaCy large compares\nfavorably and is especially robust to long input lengths and spelling\nvariations and errors. All models except DaCy large display significant biases\nrelated to ethnicity while only Polyglot shows a significant gender bias. We\nargue that for languages with limited benchmark sets, data augmentation can be\nparticularly useful for obtaining more realistic and fine-grained performance\nestimates. We provide a series of augmenters as a first step towards a more\nthorough evaluation of language models for low and medium resource languages\nand encourage further development.",
          "link": "http://arxiv.org/abs/2107.05295",
          "publishedOn": "2021-07-13T01:59:37.054Z",
          "wordCount": 632,
          "title": "DaCy: A Unified Framework for Danish NLP. (arXiv:2107.05295v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05320",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Peleg_A/0/1/0/all/0/1\">Amit Peleg</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pearl_N/0/1/0/all/0/1\">Naama Pearl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meir_R/0/1/0/all/0/1\">Ron Meir</a>",
          "description": "Fully Bayesian approaches to sequential decision-making assume that problem\nparameters are generated from a known prior, while in practice, such\ninformation is often lacking, and needs to be estimated through learning. This\nproblem is exacerbated in decision-making setups with partial information,\nwhere using a misspecified prior may lead to poor exploration and inferior\nperformance. In this work we prove, in the context of stochastic linear bandits\nand Gaussian priors, that as long as the prior estimate is sufficiently close\nto the true prior, the performance of an algorithm that uses the misspecified\nprior is close to that of the algorithm that uses the true prior. Next, we\naddress the task of learning the prior through metalearning, where a learner\nupdates its estimate of the prior across multiple task instances in order to\nimprove performance on future tasks. The estimated prior is then updated within\neach task based on incoming observations, while actions are selected in order\nto maximize expected reward. In this work we apply this scheme within a linear\nbandit setting, and provide algorithms and regret bounds, demonstrating its\neffectiveness, as compared to an algorithm that knows the correct prior. Our\nresults hold for a broad class of algorithms, including, for example, Thompson\nSampling and Information Directed Sampling.",
          "link": "http://arxiv.org/abs/2107.05320",
          "publishedOn": "2021-07-13T01:59:37.048Z",
          "wordCount": 642,
          "title": "Metalearning Linear Bandits by Prior Update. (arXiv:2107.05320v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1805.08845",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Muandet_K/0/1/0/all/0/1\">Krikamol Muandet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kanagawa_M/0/1/0/all/0/1\">Motonobu Kanagawa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saengkyongam_S/0/1/0/all/0/1\">Sorawit Saengkyongam</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Marukatat_S/0/1/0/all/0/1\">Sanparith Marukatat</a>",
          "description": "Counterfactual inference has become a ubiquitous tool in online\nadvertisement, recommendation systems, medical diagnosis, and econometrics.\nAccurate modeling of outcome distributions associated with different\ninterventions -- known as counterfactual distributions -- is crucial for the\nsuccess of these applications. In this work, we propose to model counterfactual\ndistributions using a novel Hilbert space representation called counterfactual\nmean embedding (CME). The CME embeds the associated counterfactual distribution\ninto a reproducing kernel Hilbert space (RKHS) endowed with a positive definite\nkernel, which allows us to perform causal inference over the entire landscape\nof the counterfactual distribution. Based on this representation, we propose a\ndistributional treatment effect (DTE) that can quantify the causal effect over\nentire outcome distributions. Our approach is nonparametric as the CME can be\nestimated under the unconfoundedness assumption from observational data without\nrequiring any parametric assumption about the underlying distributions. We also\nestablish a rate of convergence of the proposed estimator which depends on the\nsmoothness of the conditional mean and the Radon-Nikodym derivative of the\nunderlying marginal distributions. Furthermore, our framework allows for more\ncomplex outcomes such as images, sequences, and graphs. Our experimental\nresults on synthetic data and off-policy evaluation tasks demonstrate the\nadvantages of the proposed estimator.",
          "link": "http://arxiv.org/abs/1805.08845",
          "publishedOn": "2021-07-13T01:59:37.042Z",
          "wordCount": 663,
          "title": "Counterfactual Mean Embeddings. (arXiv:1805.08845v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1\">Or Bar-Shira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubstein_A/0/1/0/all/0/1\">Ahuva Grubstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapson_Y/0/1/0/all/0/1\">Yael Rapson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhami_D/0/1/0/all/0/1\">Dror Suhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atar_E/0/1/0/all/0/1\">Eli Atar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peri_Hanania_K/0/1/0/all/0/1\">Keren Peri-Hanania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_R/0/1/0/all/0/1\">Ronnie Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "Breast cancer is the most common malignancy in women. Mammographic findings\nsuch as microcalcifications and masses, as well as morphologic features of\nmasses in sonographic scans, are the main diagnostic targets for tumor\ndetection. However, improved specificity of these imaging modalities is\nrequired. A leading alternative target is neoangiogenesis. When pathological,\nit contributes to the development of numerous types of tumors, and the\nformation of metastases. Hence, demonstrating neoangiogenesis by visualization\nof the microvasculature may be of great importance. Super resolution ultrasound\nlocalization microscopy enables imaging of the microvasculature at the\ncapillary level. Yet, challenges such as long reconstruction time, dependency\non prior knowledge of the system Point Spread Function (PSF), and separability\nof the Ultrasound Contrast Agents (UCAs), need to be addressed for translation\nof super-resolution US into the clinic. In this work we use a deep neural\nnetwork architecture that makes effective use of signal structure to address\nthese challenges. We present in vivo human results of three different breast\nlesions acquired with a clinical US scanner. By leveraging our trained network,\nthe microvasculature structure is recovered in a short time, without prior PSF\nknowledge, and without requiring separability of the UCAs. Each of the\nrecoveries exhibits a different structure that corresponds with the known\nhistological structure. This study demonstrates the feasibility of in vivo\nhuman super resolution, based on a clinical scanner, to increase US specificity\nfor different breast lesions and promotes the use of US in the diagnosis of\nbreast pathologies.",
          "link": "http://arxiv.org/abs/2107.05270",
          "publishedOn": "2021-07-13T01:59:37.035Z",
          "wordCount": 704,
          "title": "Learned super resolution ultrasound for improved breast lesion characterization. (arXiv:2107.05270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1\">Enzo Tartaglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiandrotti_A/0/1/0/all/0/1\">Attilio Fiandrotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cagnazzo_M/0/1/0/all/0/1\">Marco Cagnazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangetto_M/0/1/0/all/0/1\">Marco Grangetto</a>",
          "description": "We formulate the entropy of a quantized artificial neural network as a\ndifferentiable function that can be plugged as a regularization term into the\ncost function minimized by gradient descent. Our formulation scales efficiently\nbeyond the first order and is agnostic of the quantization scheme. The network\ncan then be trained to minimize the entropy of the quantized parameters, so\nthat they can be optimally compressed via entropy coding. We experiment with\nour entropy formulation at quantizing and compressing well-known network\narchitectures over multiple datasets. Our approach compares favorably over\nsimilar methods, enjoying the benefits of higher order entropy estimate,\nshowing flexibility towards non-uniform quantization (we use Lloyd-max\nquantization), scalability towards any entropy order to be minimized and\nefficiency in terms of compression. We show that HEMP is able to work in\nsynergy with other approaches aiming at pruning or quantizing the model itself,\ndelivering significant benefits in terms of storage size compressibility\nwithout harming the model's performance.",
          "link": "http://arxiv.org/abs/2107.05298",
          "publishedOn": "2021-07-13T01:59:37.029Z",
          "wordCount": 602,
          "title": "HEMP: High-order Entropy Minimization for neural network comPression. (arXiv:2107.05298v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingshi Chen</a>",
          "description": "Transformer is the state of the art model for many language and visual tasks.\nIn this paper, we give a deep analysis of its multi-head self-attention (MHSA)\nmodule and find that: 1) Each token is a random variable in high dimensional\nfeature space. 2) After layer normalization, these variables are mapped to\npoints on the hyper-sphere. 3) The update of these tokens is a Brownian motion.\nThe Brownian motion has special properties, its second order item should not be\nignored. So we present a new second-order optimizer(an iterative K-FAC\nalgorithm) for the MHSA module.\n\nIn some short words: All tokens are mapped to high dimension hyper-sphere.\nThe Scaled Dot-Product Attention\n$softmax(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}})$ is just the Markov\ntransition matrix for the random walking on the sphere. And the deep learning\nprocess would learn proper kernel function to get proper positions of these\ntokens. The training process in the MHSA module corresponds to a Brownian\nmotion worthy of further study.",
          "link": "http://arxiv.org/abs/2107.05264",
          "publishedOn": "2021-07-13T01:59:37.013Z",
          "wordCount": 582,
          "title": "The Brownian motion in the transformer model. (arXiv:2107.05264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiacheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wensi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songze Li</a>",
          "description": "We propose OmniLytics, a blockchain-based secure data trading marketplace for\nmachine learning applications. Utilizing OmniLytics, many distributed data\nowners can contribute their private data to collectively train a ML model\nrequested by some model owners, and get compensated for data contribution.\nOmniLytics enables such model training while simultaneously providing 1) model\nsecurity against curious data owners; 2) data security against curious model\nand data owners; 3) resilience to malicious data owners who provide faulty\nresults to poison model training; and 4) resilience to malicious model owner\nwho intents to evade the payment. OmniLytics is implemented as a smart contract\non the Ethereum blockchain to guarantee the atomicity of payment. In\nOmniLytics, a model owner publishes encrypted initial model on the contract,\nover which the participating data owners compute gradients using their private\ndata, and securely aggregate the gradients through the contract. Finally, the\ncontract reimburses the data owners, and the model owner decrypts the\naggregated model update. We implement a working prototype of OmniLytics on\nEthereum, and perform extensive experiments to measure its gas cost and\nexecution time under various parameter combinations, demonstrating its high\ncomputation and cost efficiency and strong practicality.",
          "link": "http://arxiv.org/abs/2107.05252",
          "publishedOn": "2021-07-13T01:59:37.006Z",
          "wordCount": 668,
          "title": "OmniLytics: A Blockchain-based Secure Data Market for Decentralized Machine Learning. (arXiv:2107.05252v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Joshua Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Chau-Wai Wong</a>",
          "description": "Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory\nfunctionality and is receiving increasing attention during the COVID-19\npandemic. Clinical findings show that it is possible for COVID-19 patients to\nhave significantly low SpO$_2$ before any obvious symptoms. The prevalence of\ncameras has motivated researchers to investigate methods for monitoring SpO$_2$\nusing videos. Most prior schemes involving smartphones are contact-based: They\nrequire a fingertip to cover the phone's camera and the nearby light source to\ncapture re-emitted light from the illuminated tissue. In this paper, we propose\nthe first convolutional neural network based noncontact SpO$_2$ estimation\nscheme using smartphone cameras. The scheme analyzes the videos of a\nparticipant's hand for physiological sensing, which is convenient and\ncomfortable, and can protect their privacy and allow for keeping face masks on.\nWe design our neural network architectures inspired by the optophysiological\nmodels for SpO$_2$ measurement and demonstrate the explainability by\nvisualizing the weights for channel combination. Our proposed models outperform\nthe state-of-the-art model that is designed for contact-based SpO$_2$\nmeasurement, showing the potential of our proposed method to contribute to\npublic health. We also analyze the impact of skin type and the side of a hand\non SpO$_2$ estimation performance.",
          "link": "http://arxiv.org/abs/2107.05087",
          "publishedOn": "2021-07-13T01:59:37.000Z",
          "wordCount": 689,
          "title": "Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05085",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1\">Gorkem Polat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Serinagaoglu_Y/0/1/0/all/0/1\">Yesim Dogrusoz Serinagaoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halici_U/0/1/0/all/0/1\">Ugur Halici</a>",
          "description": "Recent studies have shown that lung cancer screening using annual low-dose\ncomputed tomography (CT) reduces lung cancer mortality by 20% compared to\ntraditional chest radiography. Therefore, CT lung screening has started to be\nused widely all across the world. However, analyzing these images is a serious\nburden for radiologists. The number of slices in a CT scan can be up to 600.\nTherefore, computer-aided-detection (CAD) systems are very important for faster\nand more accurate assessment of the data. In this study, we proposed a\nframework that analyzes CT lung screenings using convolutional neural networks\n(CNNs) to reduce false positives. We trained our model with different volume\nsizes and showed that volume size plays a critical role in the performance of\nthe system. We also used different fusions in order to show their power and\neffect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D\nconvolutional operations applied to 3D data could result in information loss.\nThe proposed framework has been tested on the dataset provided by the LUNA16\nChallenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.",
          "link": "http://arxiv.org/abs/2107.05085",
          "publishedOn": "2021-07-13T01:59:36.993Z",
          "wordCount": 661,
          "title": "Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks. (arXiv:2107.05085v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaze_R/0/1/0/all/0/1\">Rahul Vaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanawal_M/0/1/0/all/0/1\">Manjesh K. Hanawal</a>",
          "description": "We consider a continuous-time multi-arm bandit problem (CTMAB), where the\nlearner can sample arms any number of times in a given interval and obtain a\nrandom reward from each sample, however, increasing the frequency of sampling\nincurs an additive penalty/cost. Thus, there is a tradeoff between obtaining\nlarge reward and incurring sampling cost as a function of the sampling\nfrequency. The goal is to design a learning algorithm that minimizes regret,\nthat is defined as the difference of the payoff of the oracle policy and that\nof the learning algorithm. CTMAB is fundamentally different than the usual\nmulti-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial\nin CTMAB, since the optimal sampling frequency depends on the mean of the arm,\nwhich needs to be estimated. We first establish lower bounds on the regret\nachievable with any algorithm and then propose algorithms that achieve the\nlower bound up to logarithmic factors. For the single-arm case, we show that\nthe lower bound on the regret is $\\Omega((\\log T)^2/\\mu)$, where $\\mu$ is the\nmean of the arm, and $T$ is the time horizon. For the multiple arms case, we\nshow that the lower bound on the regret is $\\Omega((\\log T)^2 \\mu/\\Delta^2)$,\nwhere $\\mu$ now represents the mean of the best arm, and $\\Delta$ is the\ndifference of the mean of the best and the second-best arm. We then propose an\nalgorithm that achieves the bound up to constant terms.",
          "link": "http://arxiv.org/abs/2107.05289",
          "publishedOn": "2021-07-13T01:59:36.982Z",
          "wordCount": 670,
          "title": "Continuous Time Bandits With Sampling Costs. (arXiv:2107.05289v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Soham Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_Y/0/1/0/all/0/1\">Yash Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1\">Aditya Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shevade_S/0/1/0/all/0/1\">Shirish Shevade</a>",
          "description": "Machine-Learning-as-a-Service providers expose machine learning (ML) models\nthrough application programming interfaces (APIs) to developers. Recent work\nhas shown that attackers can exploit these APIs to extract good approximations\nof such ML models, by querying them with samples of their choosing. We propose\nVarDetect, a stateful monitor that tracks the distribution of queries made by\nusers of such a service, to detect model extraction attacks. Harnessing the\nlatent distributions learned by a modified variational autoencoder, VarDetect\nrobustly separates three types of attacker samples from benign samples, and\nsuccessfully raises an alarm for each. Further, with VarDetect deployed as an\nautomated defense mechanism, the extracted substitute models are found to\nexhibit poor performance and transferability, as intended. Finally, we\ndemonstrate that even adaptive attackers with prior knowledge of the deployment\nof VarDetect, are detected by it.",
          "link": "http://arxiv.org/abs/2107.05166",
          "publishedOn": "2021-07-13T01:59:36.966Z",
          "wordCount": 574,
          "title": "Stateful Detection of Model Extraction Attacks. (arXiv:2107.05166v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1\">Mohamad H Danesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1\">Alan Fern</a>",
          "description": "We study the problem of out-of-distribution dynamics (OODD) detection, which\ninvolves detecting when the dynamics of a temporal process change compared to\nthe training-distribution dynamics. This is relevant to applications in\ncontrol, reinforcement learning (RL), and multi-variate time-series, where\nchanges to test time dynamics can impact the performance of learning\ncontrollers/predictors in unknown ways. This problem is particularly important\nin the context of deep RL, where learned controllers often overfit to the\ntraining environment. Currently, however, there is a lack of established OODD\nbenchmarks for the types of environments commonly used in RL research. Our\nfirst contribution is to design a set of OODD benchmarks derived from common RL\nenvironments with varying types and intensities of OODD. Our second\ncontribution is to design a strong OODD baseline approach based on recurrent\nimplicit quantile networks (RIQNs), which monitors autoregressive prediction\nerrors for OODD detection. Our final contribution is to evaluate the RIQN\napproach on the benchmarks to provide baseline results for future comparison.",
          "link": "http://arxiv.org/abs/2107.04982",
          "publishedOn": "2021-07-13T01:59:36.960Z",
          "wordCount": 603,
          "title": "Out-of-Distribution Dynamics Detection: RL-Relevant Benchmarks and Results. (arXiv:2107.04982v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Domingo_Enrich_C/0/1/0/all/0/1\">Carles Domingo-Enrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bietti_A/0/1/0/all/0/1\">Alberto Bietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrie_M/0/1/0/all/0/1\">Marylou Gabri&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1\">Eric Vanden-Eijnden</a>",
          "description": "Energy-based models (EBMs) are generative models that are usually trained via\nmaximum likelihood estimation. This approach becomes challenging in generic\nsituations where the trained energy is nonconvex, due to the need to sample the\nGibbs distribution associated with this energy. Using general Fenchel duality\nresults, we derive variational principles dual to maximum likelihood EBMs with\nshallow overparametrized neural network energies, both in the active (aka\nfeature-learning) and lazy regimes. In the active regime, this dual formulation\nleads to a training algorithm in which one updates concurrently the particles\nin the sample space and the neurons in the parameter space of the energy. We\nalso consider a variant of this algorithm in which the particles are sometimes\nrestarted at random samples drawn from the data set, and show that performing\nthese restarts at every iteration step corresponds to score matching training.\nUsing intermediate parameter setups in our dual algorithm thereby gives a way\nto interpolate between maximum likelihood and score matching training. These\nresults are illustrated in simple numerical experiments.",
          "link": "http://arxiv.org/abs/2107.05134",
          "publishedOn": "2021-07-13T01:59:36.927Z",
          "wordCount": 616,
          "title": "Dual Training of Energy-Based Models with Overparametrized Shallow Neural Networks. (arXiv:2107.05134v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zakazov_I/0/1/0/all/0/1\">Ivan Zakazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirokikh_B/0/1/0/all/0/1\">Boris Shirokikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Alexey Chernyavskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>",
          "description": "Domain Adaptation (DA) methods are widely used in medical image segmentation\ntasks to tackle the problem of differently distributed train (source) and test\n(target) data. We consider the supervised DA task with a limited number of\nannotated samples from the target domain. It corresponds to one of the most\nrelevant clinical setups: building a sufficiently accurate model on the minimum\npossible amount of annotated data. Existing methods mostly fine-tune specific\nlayers of the pretrained Convolutional Neural Network (CNN). However, there is\nno consensus on which layers are better to fine-tune, e.g. the first layers for\nimages with low-level domain shift or the deeper layers for images with\nhigh-level domain shift. To this end, we propose SpotTUnet - a CNN architecture\nthat automatically chooses the layers which should be optimally fine-tuned.\nMore specifically, on the target domain, our method additionally learns the\npolicy that indicates whether a specific layer should be fine-tuned or reused\nfrom the pretrained network. We show that our method performs at the same level\nas the best of the nonflexible fine-tuning methods even under the extreme\nscarcity of annotated data. Secondly, we show that SpotTUnet policy provides a\nlayer-wise visualization of the domain shift impact on the network, which could\nbe further used to develop robust domain generalization methods. In order to\nextensively evaluate SpotTUnet performance, we use a publicly available dataset\nof brain MR images (CC359), characterized by explicit domain shift. We release\na reproducible experimental pipeline.",
          "link": "http://arxiv.org/abs/2107.04914",
          "publishedOn": "2021-07-13T01:59:36.921Z",
          "wordCount": 698,
          "title": "Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation. (arXiv:2107.04914v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaobo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shuo Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Haikun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>",
          "description": "Estimating the kernel mean in a reproducing kernel Hilbert space is a\ncritical component in many kernel learning algorithms. Given a finite sample,\nthe standard estimate of the target kernel mean is the empirical average.\nPrevious works have shown that better estimators can be constructed by\nshrinkage methods. In this work, we propose to corrupt data examples with noise\nfrom known distributions and present a new kernel mean estimator, called the\nmarginalized kernel mean estimator, which estimates kernel mean under the\ncorrupted distribution. Theoretically, we show that the marginalized kernel\nmean estimator introduces implicit regularization in kernel mean estimation.\nEmpirically, we show on a variety of datasets that the marginalized kernel mean\nestimator obtains much lower estimation error than the existing estimators.",
          "link": "http://arxiv.org/abs/2107.04855",
          "publishedOn": "2021-07-13T01:59:36.915Z",
          "wordCount": 561,
          "title": "Kernel Mean Estimation by Marginalized Corrupted Distributions. (arXiv:2107.04855v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1\">Lijun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hengshu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>",
          "description": "Real estate appraisal refers to the process of developing an unbiased opinion\nfor real property's market value, which plays a vital role in decision-making\nfor various players in the marketplace (e.g., real estate agents, appraisers,\nlenders, and buyers). However, it is a nontrivial task for accurate real estate\nappraisal because of three major challenges: (1) The complicated influencing\nfactors for property value; (2) The asynchronously spatiotemporal dependencies\namong real estate transactions; (3) The diversified correlations between\nresidential communities. To this end, we propose a Multi-Task Hierarchical\nGraph Representation Learning (MugRep) framework for accurate real estate\nappraisal. Specifically, by acquiring and integrating multi-source urban data,\nwe first construct a rich feature set to comprehensively profile the real\nestate from multiple perspectives (e.g., geographical distribution, human\nmobility distribution, and resident demographics distribution). Then, an\nevolving real estate transaction graph and a corresponding event graph\nconvolution module are proposed to incorporate asynchronously spatiotemporal\ndependencies among real estate transactions. Moreover, to further incorporate\nvaluable knowledge from the view of residential communities, we devise a\nhierarchical heterogeneous community graph convolution module to capture\ndiversified correlations between residential communities. Finally, an urban\ndistrict partitioned multi-task learning module is introduced to generate\ndifferently distributed value opinions for real estate. Extensive experiments\non two real-world datasets demonstrate the effectiveness of MugRep and its\ncomponents and features.",
          "link": "http://arxiv.org/abs/2107.05180",
          "publishedOn": "2021-07-13T01:59:36.898Z",
          "wordCount": 668,
          "title": "MugRep: A Multi-Task Hierarchical Graph Representation Learning Framework for Real Estate Appraisal. (arXiv:2107.05180v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-En Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">En-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_P/0/1/0/all/0/1\">Pei-Yung Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Li-Chen Fu</a>",
          "description": "Recently, there has been a panoptic segmentation task combining semantic and\ninstance segmentation, in which the goal is to classify each pixel with the\ncorresponding instance ID. In this work, we propose a solution to tackle the\npanoptic segmentation task. The overall structure combines the bottom-up method\nand the top-down method. Therefore, not only can there be better performance,\nbut also the execution speed can be maintained. The network mainly pays\nattention to the quality of the mask. In the previous work, we can see that the\nuneven contour of the object is more likely to appear, resulting in low-quality\nprediction. Accordingly, we propose enhancement features and corresponding loss\nfunctions for the silhouette of objects and backgrounds to improve the mask.\nMeanwhile, we use the new proposed confidence score to solve the occlusion\nproblem and make the network tend to use higher quality masks as prediction\nresults. To verify our research, we used the COCO dataset and CityScapes\ndataset to do experiments and obtained competitive results with fast inference\ntime.",
          "link": "http://arxiv.org/abs/2107.05093",
          "publishedOn": "2021-07-13T01:59:36.891Z",
          "wordCount": 614,
          "title": "SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network. (arXiv:2107.05093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1\">Qiyou Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghauch_H/0/1/0/all/0/1\">Hadi Ghauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>",
          "description": "Data representation techniques have made a substantial contribution to\nadvancing data processing and machine learning (ML). Improving predictive power\nwas the focus of previous representation techniques, which unfortunately\nperform rather poorly on the interpretability in terms of extracting underlying\ninsights of the data. Recently, Kolmogorov model (KM) was studied, which is an\ninterpretable and predictable representation approach to learning the\nunderlying probabilistic structure of a set of random variables. The existing\nKM learning algorithms using semi-definite relaxation with randomization\n(SDRwR) or discrete monotonic optimization (DMO) have, however, limited utility\nto big data applications because they do not scale well computationally. In\nthis paper, we propose a computationally scalable KM learning algorithm, based\non the regularized dual optimization combined with enhanced gradient descent\n(GD) method. To make our method more scalable to large-dimensional problems, we\npropose two acceleration schemes, namely, eigenvalue decomposition (EVD)\nelimination strategy and proximal EVD algorithm. Furthermore, a thresholding\ntechnique by exploiting the approximation error analysis and leveraging the\nnormalized Minkowski $\\ell_1$-norm and its bounds, is provided for the\nselection of the number of iterations of the proximal EVD algorithm. When\napplied to big data applications, it is demonstrated that the proposed method\ncan achieve compatible training/prediction performance with significantly\nreduced computational complexity; roughly two orders of magnitude improvement\nin terms of the time overhead, compared to the existing KM learning algorithms.\nFurthermore, it is shown that the accuracy of logical relation mining for\ninterpretability by using the proposed KM learning algorithm exceeds $80\\%$.",
          "link": "http://arxiv.org/abs/2107.05011",
          "publishedOn": "2021-07-13T01:59:36.873Z",
          "wordCount": 691,
          "title": "Dual Optimization for Kolmogorov Model Learning Using Enhanced Gradient Descent. (arXiv:2107.05011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wight_C/0/1/0/all/0/1\">Colby Wight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akers_S/0/1/0/all/0/1\">Sarah Akers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1\">Scott Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Woongjo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosink_L/0/1/0/all/0/1\">Luke Gosink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurrus_E/0/1/0/all/0/1\">Elizabeth Jurrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kappagantula_K/0/1/0/all/0/1\">Keerti Kappagantula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1\">Tegan H. Emerson</a>",
          "description": "As both machine learning models and the datasets on which they are evaluated\nhave grown in size and complexity, the practice of using a few summary\nstatistics to understand model performance has become increasingly problematic.\nThis is particularly true in real-world scenarios where understanding model\nfailure on certain subpopulations of the data is of critical importance. In\nthis paper we propose a topological framework for evaluating machine learning\nmodels in which a dataset is treated as a \"space\" on which a model operates.\nThis provides us with a principled way to organize information about model\nperformance at both the global level (over the entire test set) and also the\nlocal level (on specific subpopulations). Finally, we describe a topological\ndata structure, presheaves, which offer a convenient way to store and analyze\nmodel performance between different subpopulations.",
          "link": "http://arxiv.org/abs/2107.04714",
          "publishedOn": "2021-07-13T01:59:36.868Z",
          "wordCount": 598,
          "title": "A Topological-Framework to Improve Analysis of Machine Learning Model Performance. (arXiv:2107.04714v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Shalini Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_J/0/1/0/all/0/1\">Jaideep Srivastava</a>",
          "description": "Many machine learning models have been built to tackle information overload\nissues on Massive Open Online Courses (MOOC) platforms. These models rely on\nlearning powerful representations of MOOC entities. However, they suffer from\nthe problem of scarce expert label data. To overcome this problem, we propose\nto learn pre-trained representations of MOOC entities using abundant unlabeled\ndata from the structure of MOOCs which can directly be applied to the\ndownstream tasks. While existing pre-training methods have been successful in\nNLP areas as they learn powerful textual representation, their models do not\nleverage the richer information about MOOC entities. This richer information\nincludes the graph relationship between the lectures, concepts, and courses\nalong with the domain knowledge about the complexity of a concept. We develop\nMOOCRep, a novel method based on Transformer language model trained with two\npre-training objectives : 1) graph-based objective to capture the powerful\nsignal of entities and relations that exist in the graph, and 2)\ndomain-oriented objective to effectively incorporate the complexity level of\nconcepts. Our experiments reveal that MOOCRep's embeddings outperform\nstate-of-the-art representation learning methods on two tasks important for\neducation community, concept pre-requisite prediction and lecture\nrecommendation.",
          "link": "http://arxiv.org/abs/2107.05154",
          "publishedOn": "2021-07-13T01:59:36.861Z",
          "wordCount": 622,
          "title": "MOOCRep: A Unified Pre-trained Embedding of MOOC Entities. (arXiv:2107.05154v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haodong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yabo Chu</a>",
          "description": "Social-aware recommendation approaches have been recognized as an effective\nway to solve the data sparsity issue of traditional recommender systems. The\nassumption behind is that the knowledge in social user-user connections can be\nshared and transferred to the domain of user-item interactions, whereby to help\nlearn user preferences. However, most existing approaches merely adopt the\nfirst-order connections among users during transfer learning, ignoring those\nconnections in higher orders. We argue that better recommendation performance\ncan also benefit from high-order social relations. In this paper, we propose a\nnovel Propagation-aware Transfer Learning Network (PTLN) based on the\npropagation of social relations. We aim to better mine the sharing knowledge\nhidden in social networks and thus further improve recommendation performance.\nSpecifically, we explore social influence in two aspects: (a) higher-order\nfriends have been taken into consideration by order bias; (b) different friends\nin the same order will have distinct importance for recommendation by an\nattention mechanism. Besides, we design a novel regularization to bridge the\ngap between social relations and user-item interactions. We conduct extensive\nexperiments on two real-world datasets and beat other counterparts in terms of\nranking accuracy, especially for the cold-start users with few historical\ninteractions.",
          "link": "http://arxiv.org/abs/2107.04846",
          "publishedOn": "2021-07-13T01:59:36.843Z",
          "wordCount": 625,
          "title": "Propagation-aware Social Recommendation by Transfer Learning. (arXiv:2107.04846v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05007",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Meldgaard_S/0/1/0/all/0/1\">S&#xf8;ren Ager Meldgaard</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mortensen_H/0/1/0/all/0/1\">Henrik Lund Mortensen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Christiansen_M/0/1/0/all/0/1\">Mads-Peter V. Christiansen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Noe_F/0/1/0/all/0/1\">Frank No&#xe9;</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hammer_B/0/1/0/all/0/1\">Bj&#xf8;rk Hammer</a>",
          "description": "Chemical space is routinely explored by machine learning methods to discover\ninteresting molecules, before time-consuming experimental synthesizing is\nattempted. However, these methods often rely on a graph representation,\nignoring 3D information necessary for determining the stability of the\nmolecules. We propose a reinforcement learning approach for generating\nmolecules in cartesian coordinates allowing for quantum chemical prediction of\nthe stability. To improve sample-efficiency we learn basic chemical rules from\nimitation learning on the GDB-11 database to create an initial model applicable\nfor all stoichiometries. We then deploy multiple copies of the model\nconditioned on a specific stoichiometry in a reinforcement learning setting.\nThe models correctly identify low energy molecules in the database and produce\nnovel isomers not found in the training set. Finally, we apply the model to\nlarger molecules to show how reinforcement learning further refines the\nimitation learning model in domains far from the training data.",
          "link": "http://arxiv.org/abs/2107.05007",
          "publishedOn": "2021-07-13T01:59:36.837Z",
          "wordCount": 588,
          "title": "Generating stable molecules using imitation and reinforcement learning. (arXiv:2107.05007v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mehdi Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1\">Max Berrendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thost_V/0/1/0/all/0/1\">Veronika Thost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>",
          "description": "For many years, link prediction on knowledge graphs (KGs) has been a purely\ntransductive task, not allowing for reasoning on unseen entities. Recently,\nincreasing efforts are put into exploring semi- and fully inductive scenarios,\nenabling inference over unseen and emerging entities. Still, all these\napproaches only consider triple-based \\glspl{kg}, whereas their richer\ncounterparts, hyper-relational KGs (e.g., Wikidata), have not yet been properly\nstudied. In this work, we classify different inductive settings and study the\nbenefits of employing hyper-relational KGs on a wide range of semi- and fully\ninductive link prediction tasks powered by recent advancements in graph neural\nnetworks. Our experiments on a novel set of benchmarks show that qualifiers\nover typed edges can lead to performance improvements of 6% of absolute gains\n(for the Hits@10 metric) compared to triple-only baselines. Our code is\navailable at \\url{https://github.com/mali-git/hyper_relational_ilp}.",
          "link": "http://arxiv.org/abs/2107.04894",
          "publishedOn": "2021-07-13T01:59:36.831Z",
          "wordCount": 572,
          "title": "Improving Inductive Link Prediction Using Hyper-Relational Facts. (arXiv:2107.04894v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-13T01:59:36.826Z",
          "wordCount": 612,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1\">Gabriel de Souza P. Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1\">Sara Rabhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ak_R/0/1/0/all/0/1\">Ronay Ak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md Yasin Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1\">Even Oldridge</a>",
          "description": "Session-based recommendation is an important task for e-commerce services,\nwhere a large number of users browse anonymously or may have very distinct\ninterests for different sessions. In this paper we present one of the winning\nsolutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce\nData Challenge. Our solution was inspired by NLP techniques and consists of an\nensemble of two Transformer architectures - Transformer-XL and XLNet - trained\nwith autoregressive and autoencoding approaches. To leverage most of the rich\ndataset made available for the competition, we describe how we prepared\nmulti-model features by combining tabular events with textual and image\nvectors. We also present a model prediction analysis to better understand the\neffectiveness of our architectures for the session-based recommendation.",
          "link": "http://arxiv.org/abs/2107.05124",
          "publishedOn": "2021-07-13T01:59:36.819Z",
          "wordCount": 593,
          "title": "Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. (arXiv:2107.05124v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04766",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kang_L/0/1/0/all/0/1\">Lican Kang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1\">Yanyan Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1\">Youzhou Zhou</a>",
          "description": "Schr\\\"{o}dinger-F\\\"{o}llmer sampler (SFS) is a novel and efficient approach\nfor sampling from possibly unnormalized distributions without ergodicity. SFS\nis based on the Euler-Maruyama discretization of Schr\\\"{o}dinger-F\\\"{o}llmer\ndiffusion process $$\\mathrm{d} X_{t}=-\\nabla U\\left(X_t, t\\right) \\mathrm{d}\nt+\\mathrm{d} B_{t}, \\quad t \\in[0,1],\\quad X_0=0$$ on the unit interval, which\ntransports the degenerate distribution at time zero to the target distribution\nat time one. In \\cite{sfs21}, the consistency of SFS is established under a\nrestricted assumption that %the drift term $b(x,t)$ the potential $U(x,t)$ is\nuniformly (on $t$) strongly %concave convex (on $x$). In this paper we provide\na nonasymptotic error bound of SFS in Wasserstein distance under some smooth\nand bounded conditions on the density ratio of the target distribution over the\nstandard normal distribution, but without requiring the strongly convexity of\nthe potential.",
          "link": "http://arxiv.org/abs/2107.04766",
          "publishedOn": "2021-07-13T01:59:36.803Z",
          "wordCount": 568,
          "title": "Convergence Analysis of Schr{\\\"o}dinger-F{\\\"o}llmer Sampler without Convexity. (arXiv:2107.04766v1 [stat.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vomvas_M/0/1/0/all/0/1\">Marinos Vomvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_Huu_T/0/1/0/all/0/1\">Triet Vo-Huu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noubir_G/0/1/0/all/0/1\">Guevara Noubir</a>",
          "description": "RF emissions detection, classification, and spectro-temporal localization are\ncrucial not only for tasks relating to understanding, managing, and protecting\nthe RF spectrum, but also for safety and security applications such as\ndetecting intruding drones or jammers. Achieving this goal for wideband\nspectrum and in real-time performance is a challenging problem. We present\nWRIST, a Wideband, Real-time RF Identification system with Spectro-Temporal\ndetection, framework and system. Our resulting deep learning model is capable\nto detect, classify, and precisely locate RF emissions in time and frequency\nusing RF samples of 100 MHz spectrum in real-time (over 6Gbps incoming I&Q\nstreams). Such capabilities are made feasible by leveraging a deep-learning\nbased one-stage object detection framework, and transfer learning to a\nmulti-channel image-based RF signals representation. We also introduce an\niterative training approach which leverages synthesized and augmented RF data\nto efficiently build large labelled datasets of RF emissions (SPREAD). WRIST\ndetector achieves 90 mean Average Precision even in extremely congested\nenvironment in the wild. WRIST model classifies five technologies (Bluetooth,\nLightbridge, Wi-Fi, XPD, and ZigBee) and is easily extendable to others. We are\nmaking our curated and annotated dataset available to the whole community. It\nconsists of nearly 1 million fully labelled RF emissions collected from various\noff-the-shelf wireless radios in a range of environments and spanning the five\nclasses of emissions.",
          "link": "http://arxiv.org/abs/2107.05114",
          "publishedOn": "2021-07-13T01:59:36.796Z",
          "wordCount": 654,
          "title": "Spectro-Temporal RF Identification using Deep Learning. (arXiv:2107.05114v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04831",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pfitzinger_J/0/1/0/all/0/1\">Johann Pfitzinger</a>",
          "description": "Prediction tasks with high-dimensional nonorthogonal predictor sets pose a\nchallenge for least squares based fitting procedures. A large and productive\nliterature exists, discussing various regularized approaches to improving the\nout-of-sample robustness of parameter estimates. This paper proposes a novel\ncluster-based regularization - the hierarchical feature regression (HFR) -,\nwhich mobilizes insights from the domains of machine learning and graph theory\nto estimate parameters along a supervised hierarchical representation of the\npredictor set, shrinking parameters towards group targets. The method is\ninnovative in its ability to estimate optimal compositions of predictor groups,\nas well as the group targets endogenously. The HFR can be viewed as a\nsupervised factor regression, with the strength of shrinkage governed by a\npenalty on the extent of idiosyncratic variation captured in the fitting\nprocess. The method demonstrates good predictive accuracy and versatility,\noutperforming a panel of benchmark regularized estimators across a diverse set\nof simulated regression tasks, including dense, sparse and grouped data\ngenerating processes. An application to the prediction of economic growth is\nused to illustrate the HFR's effectiveness in an empirical setting, with\nfavorable comparisons to several frequentist and Bayesian alternatives.",
          "link": "http://arxiv.org/abs/2107.04831",
          "publishedOn": "2021-07-13T01:59:36.790Z",
          "wordCount": 613,
          "title": "Cluster Regularization via a Hierarchical Feature Regression. (arXiv:2107.04831v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Ye Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shao-Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sheng-Jun Huang</a>",
          "description": "Traditional supervised learning requires ground truth labels for the training\ndata, whose collection can be difficult in many cases. Recently, crowdsourcing\nhas established itself as an efficient labeling solution through resorting to\nnon-expert crowds. To reduce the labeling error effects, one common practice is\nto distribute each instance to multiple workers, whereas each worker only\nannotates a subset of data, resulting in the {\\it sparse annotation}\nphenomenon. In this paper, we note that when meeting with class-imbalance,\ni.e., when the ground truth labels are {\\it class-imbalanced}, the sparse\nannotations are prone to be skewly distributed, which thus can severely bias\nthe learning algorithm. To combat this issue, we propose one self-training\nbased approach named {\\it Self-Crowd} by progressively adding confident\npseudo-annotations and rebalancing the annotation distribution. Specifically,\nwe propose one distribution aware confidence measure to select confident\npseudo-annotations, which adopts the resampling strategy to oversample the\nminority annotations and undersample the majority annotations. On one\nreal-world crowdsourcing image classification task, we show that the proposed\nmethod yields more balanced annotations throughout training than the\ndistribution agnostic methods and substantially improves the learning\nperformance at different annotation sparsity levels.",
          "link": "http://arxiv.org/abs/2107.05039",
          "publishedOn": "2021-07-13T01:59:36.783Z",
          "wordCount": 616,
          "title": "Learning from Crowds with Sparse and Imbalanced Annotations. (arXiv:2107.05039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_B/0/1/0/all/0/1\">Ben Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully\ncausal approach to neural audio synthesis which operates directly in the\nwaveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU\ninference. The NEWT uses time-distributed multilayer perceptrons with periodic\nactivations to implicitly learn nonlinear transfer functions that encode the\ncharacteristics of a target timbre. Once trained, a NEWT can produce complex\ntimbral evolutions by simple affine transformations of its input and output\nsignals. We paired the NEWT with a differentiable noise synthesiser and reverb\nand found it capable of generating realistic musical instrument performances\nwith only 260k total model parameters, conditioned on F0 and loudness features.\nWe compared our method to state-of-the-art benchmarks with a multi-stimulus\nlistening test and the Fr\\'echet Audio Distance and found it performed\ncompetitively across the tested timbral domains. Our method significantly\noutperformed the benchmarks in terms of generation speed, and achieved\nreal-time performance on a consumer CPU, both with and without FastNEWT,\nsuggesting it is a viable basis for future creative sound design tools.",
          "link": "http://arxiv.org/abs/2107.05050",
          "publishedOn": "2021-07-13T01:59:36.777Z",
          "wordCount": 614,
          "title": "Neural Waveshaping Synthesis. (arXiv:2107.05050v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Rose McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>",
          "description": "It is challenging for humans to enable visual knowledge discovery in data\nwith more than 2-3 dimensions with a naked eye. This chapter explores the\nefficiency of discovering predictive machine learning models interactively\nusing new Elliptic Paired coordinates (EPC) visualizations. It is shown that\nEPC are capable to visualize multidimensional data and support visual machine\nlearning with preservation of multidimensional information in 2-D. Relative to\nparallel and radial coordinates, EPC visualization requires only a half of the\nvisual elements for each n-D point. An interactive software system EllipseVis,\nwhich is developed in this work, processes high-dimensional datasets, creates\nEPC visualizations, and produces predictive classification models by\ndiscovering dominance rules in EPC. By using interactive and automatic\nprocesses it discovers zones in EPC with a high dominance of a single class.\nThe EPC methodology has been successful in discovering non-linear predictive\nmodels with high coverage and precision in the computational experiments. This\ncan benefit multiple domains by producing visually appealing dominance rules.\nThis chapter presents results of successful testing the EPC non-linear\nmethodology in experiments using real and simulated data, EPC generalized to\nthe Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of\ncoordinates to optimize the visual discovery, introduction of an alternative\nEPC design and introduction of the concept of incompact machine learning\nmethodology based on EPC/DEPC.",
          "link": "http://arxiv.org/abs/2107.04974",
          "publishedOn": "2021-07-13T01:59:36.771Z",
          "wordCount": 660,
          "title": "Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates. (arXiv:2107.04974v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yunsong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stearrett_R/0/1/0/all/0/1\">Ryan Stearrett</a>",
          "description": "A cross-benchmark has been done on three critical aspects, data imputing,\nfeature selection and regression algorithms, for machine learning based\nchemical vapor deposition (CVD) virtual metrology (VM). The result reveals that\nlinear feature selection regression algorithm would extensively under-fit the\nVM data. Data imputing is also necessary to achieve a higher prediction\naccuracy as the data availability is only ~70% when optimal accuracy is\nobtained. This work suggests a nonlinear feature selection and regression\nalgorithm combined with nearest data imputing algorithm would provide a\nprediction accuracy as high as 0.7. This would lead to 70% reduced CVD\nprocessing variation, which is believed to will lead to reduced frequency of\nphysical metrology as well as more reliable mass-produced wafer with improved\nquality.",
          "link": "http://arxiv.org/abs/2107.05071",
          "publishedOn": "2021-07-13T01:59:36.754Z",
          "wordCount": 557,
          "title": "Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process. (arXiv:2107.05071v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>",
          "description": "The advancement of convolutional neural networks (CNNs) on various vision\napplications has attracted lots of attention. Yet the majority of CNNs are\nunable to satisfy the strict requirement for real-world deployment. To overcome\nthis, the recent popular network pruning is an effective method to reduce the\nredundancy of the models. However, the ranking of filters according to their\n\"importance\" on different pruning criteria may be inconsistent. One filter\ncould be important according to a certain criterion, while it is unnecessary\naccording to another one, which indicates that each criterion is only a partial\nview of the comprehensive \"importance\". From this motivation, we propose a\nnovel framework to integrate the existing filter pruning criteria by exploring\nthe criteria diversity. The proposed framework contains two stages: Criteria\nClustering and Filters Importance Calibration. First, we condense the pruning\ncriteria via layerwise clustering based on the rank of \"importance\" score.\nSecond, within each cluster, we propose a calibration factor to adjust their\nsignificance for each selected blending candidates and search for the optimal\nblending criterion via Evolutionary Algorithm. Quantitative results on the\nCIFAR-100 and ImageNet benchmarks show that our framework outperforms the\nstate-of-the-art baselines, regrading to the compact model performance after\npruning.",
          "link": "http://arxiv.org/abs/2107.05033",
          "publishedOn": "2021-07-13T01:59:36.746Z",
          "wordCount": 642,
          "title": "Blending Pruning Criteria for Convolutional Neural Networks. (arXiv:2107.05033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kousaridas_A/0/1/0/all/0/1\">Apostolos Kousaridas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_R/0/1/0/all/0/1\">Ramya Panthangi Manjunath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdomo_J/0/1/0/all/0/1\">Jose Mauricio Perdomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zielinski_E/0/1/0/all/0/1\">Ernst Zielinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitz_S/0/1/0/all/0/1\">Steffen Schmitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfadler_A/0/1/0/all/0/1\">Andreas Pfadler</a>",
          "description": "5G communication system can support the demanding quality-of-service (QoS)\nrequirements of many advanced vehicle-to-everything (V2X) use cases. However,\nthe safe and efficient driving, especially of automated vehicles, may be\naffected by sudden changes of the provided QoS. For that reason, the prediction\nof the QoS changes and the early notification of these predicted changes to the\nvehicles have been recently enabled by 5G communication systems. This solution\nenables the vehicles to avoid or mitigate the effect of sudden QoS changes at\nthe application level. This article describes how QoS prediction could be\ngenerated by a 5G communication system and delivered to a V2X application. The\ntele-operated driving use case is used as an example to analyze the feasibility\nof a QoS prediction scheme. Useful recommendations for the development of a QoS\nprediction solution are provided, while open research topics are identified.",
          "link": "http://arxiv.org/abs/2107.05000",
          "publishedOn": "2021-07-13T01:59:36.707Z",
          "wordCount": 603,
          "title": "QoS Prediction for 5G Connected and Automated Driving. (arXiv:2107.05000v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1\">Satyen Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1\">Ayush Sekhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1\">Karthik Sridharan</a>",
          "description": "Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the\nmethod of choice for learning with large over-parameterized models. A popular\ntheory for explaining why SGD works well in practice is that the algorithm has\nan implicit regularization that biases its output towards a good solution.\nPerhaps the theoretically most well understood learning setting for SGD is that\nof Stochastic Convex Optimization (SCO), where it is well known that SGD learns\nat a rate of $O(1/\\sqrt{n})$, where $n$ is the number of samples. In this\npaper, we consider the problem of SCO and explore the role of implicit\nregularization, batch size and multiple epochs for SGD. Our main contributions\nare threefold:\n\n(a) We show that for any regularizer, there is an SCO problem for which\nRegularized Empirical Risk Minimzation fails to learn. This automatically rules\nout any implicit regularization based explanation for the success of SGD.\n\n(b) We provide a separation between SGD and learning via Gradient Descent on\nempirical loss (GD) in terms of sample complexity. We show that there is an SCO\nproblem such that GD with any step size and number of iterations can only learn\nat a suboptimal rate: at least $\\widetilde{\\Omega}(1/n^{5/12})$.\n\n(c) We present a multi-epoch variant of SGD commonly used in practice. We\nprove that this algorithm is at least as good as single pass SGD in the worst\ncase. However, for certain SCO problems, taking multiple passes over the\ndataset can significantly outperform single pass SGD.\n\nWe extend our results to the general learning setting by showing a problem\nwhich is learnable for any data distribution, and for this problem, SGD is\nstrictly better than RERM for any regularization function. We conclude by\ndiscussing the implications of our results for deep learning, and show a\nseparation between SGD and ERM for two layer diagonal neural networks.",
          "link": "http://arxiv.org/abs/2107.05074",
          "publishedOn": "2021-07-13T01:59:36.660Z",
          "wordCount": 735,
          "title": "SGD: The Role of Implicit Regularization, Batch-size and Multiple-epochs. (arXiv:2107.05074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaozhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_E/0/1/0/all/0/1\">Ensheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Recently, deep learning methods have become mainstream in code search since\nthey do better at capturing semantic correlations between code snippets and\nsearch queries and have promising performance. However, code snippets have\ndiverse information from different dimensions, such as business logic, specific\nalgorithm, and hardware communication, so it is hard for a single code\nrepresentation module to cover all the perspectives. On the other hand, as a\nspecific query may focus on one or several perspectives, it is difficult for a\nsingle query representation module to represent different user intents. In this\npaper, we propose MuCoS, a multi-model ensemble learning architecture for\nsemantic code search. It combines several individual learners, each of which\nemphasizes a specific perspective of code snippets. We train the individual\nlearners on different datasets which contain different perspectives of code\ninformation, and we use a data augmentation strategy to get these different\ndatasets. Then we ensemble the learners to capture comprehensive features of\ncode snippets.",
          "link": "http://arxiv.org/abs/2107.04773",
          "publishedOn": "2021-07-13T01:59:36.653Z",
          "wordCount": 612,
          "title": "Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning for Semantic Code Search. (arXiv:2107.04773v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04973",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shankar_R/0/1/0/all/0/1\">Ravi Shankar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Venkataraman_A/0/1/0/all/0/1\">Archana Venkataraman</a>",
          "description": "We propose the first method to adaptively modify the duration of a given\nspeech signal. Our approach uses a Bayesian framework to define a latent\nattention map that links frames of the input and target utterances. We train a\nmasked convolutional encoder-decoder network to produce this attention map via\na stochastic version of the mean absolute error loss function; our model also\npredicts the length of the target speech signal using the encoder embeddings.\nThe predicted length determines the number of steps for the decoder operation.\nDuring inference, we generate the attention map as a proxy for the similarity\nmatrix between the given input speech and an unknown target speech signal.\nUsing this similarity matrix, we compute a warping path of alignment between\nthe two signals. Our experiments demonstrate that this adaptive framework\nproduces similar results to dynamic time warping, which relies on a known\ntarget signal, on both voice conversion and emotion conversion tasks. We also\nshow that our technique results in a high quality of generated speech that is\non par with state-of-the-art vocoders.",
          "link": "http://arxiv.org/abs/2107.04973",
          "publishedOn": "2021-07-13T01:59:36.645Z",
          "wordCount": 625,
          "title": "A Deep-Bayesian Framework for Adaptive Speech Duration Modification. (arXiv:2107.04973v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yuanyi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jian Peng</a>",
          "description": "The control variates (CV) method is widely used in policy gradient estimation\nto reduce the variance of the gradient estimators in practice. A control\nvariate is applied by subtracting a baseline function from the state-action\nvalue estimates. Then the variance-reduced policy gradient presumably leads to\nhigher learning efficiency. Recent research on control variates with deep\nneural net policies mainly focuses on scalar-valued baseline functions. The\neffect of vector-valued baselines is under-explored. This paper investigates\nvariance reduction with coordinate-wise and layer-wise control variates\nconstructed from vector-valued baselines for neural net policies. We present\nexperimental evidence suggesting that lower variance can be obtained with such\nbaselines than with the conventional scalar-valued baseline. We demonstrate how\nto equip the popular Proximal Policy Optimization (PPO) algorithm with these\nnew control variates. We show that the resulting algorithm with proper\nregularization can achieve higher sample efficiency than scalar control\nvariates in continuous control benchmarks.",
          "link": "http://arxiv.org/abs/2107.04987",
          "publishedOn": "2021-07-13T01:59:36.639Z",
          "wordCount": 580,
          "title": "Coordinate-wise Control Variates for Deep Policy Gradients. (arXiv:2107.04987v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05001",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nisimov_S/0/1/0/all/0/1\">Shami Nisimov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gurwicz_Y/0/1/0/all/0/1\">Yaniv Gurwicz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rohekar_R/0/1/0/all/0/1\">Raanan Y. Rohekar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Novik_G/0/1/0/all/0/1\">Gal Novik</a>",
          "description": "Causal discovery from observational data is an important tool in many\nbranches of science. Under certain assumptions it allows scientists to explain\nphenomena, predict, and make decisions. In the large sample limit, sound and\ncomplete causal discovery algorithms have been previously introduced, where a\ndirected acyclic graph (DAG), or its equivalence class, representing causal\nrelations is searched. However, in real-world cases, only finite training data\nis available, which limits the power of statistical tests used by these\nalgorithms, leading to errors in the inferred causal model. This is commonly\naddressed by devising a strategy for using as few as possible statistical\ntests. In this paper, we introduce such a strategy in the form of a recursive\nwrapper for existing constraint-based causal discovery algorithms, which\npreserves soundness and completeness. It recursively clusters the observed\nvariables using the normalized min-cut criterion from the outset, and uses a\nbaseline causal discovery algorithm during backtracking for learning local\nsub-graphs. It then combines them and ensures completeness. By an ablation\nstudy, using synthetic data, and by common real-world benchmarks, we\ndemonstrate that our approach requires significantly fewer statistical tests,\nlearns more accurate graphs, and requires shorter run-times than the baseline\nalgorithm.",
          "link": "http://arxiv.org/abs/2107.05001",
          "publishedOn": "2021-07-13T01:59:36.633Z",
          "wordCount": 657,
          "title": "Improving Efficiency and Accuracy of Causal Discovery Using a Hierarchical Wrapper. (arXiv:2107.05001v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1\">Rohan Taori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1\">Shiori Sagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "For machine learning systems to be reliable, we must understand their\nperformance in unseen, out-of-distribution environments. In this paper, we\nempirically show that out-of-distribution performance is strongly correlated\nwith in-distribution performance for a wide range of models and distribution\nshifts. Specifically, we demonstrate strong correlations between\nin-distribution and out-of-distribution performance on variants of CIFAR-10 &\nImageNet, a synthetic pose estimation task derived from YCB objects, satellite\nimagery classification in FMoW-WILDS, and wildlife classification in\niWildCam-WILDS. The strong correlations hold across model architectures,\nhyperparameters, training set size, and training duration, and are more precise\nthan what is expected from existing domain adaptation theory. To complete the\npicture, we also investigate cases where the correlation is weaker, for\ninstance some synthetic distribution shifts from CIFAR-10-C and the tissue\nclassification dataset Camelyon17-WILDS. Finally, we provide a candidate theory\nbased on a Gaussian data model that shows how changes in the data covariance\narising from distribution shift can affect the observed correlations.",
          "link": "http://arxiv.org/abs/2107.04649",
          "publishedOn": "2021-07-13T01:59:36.627Z",
          "wordCount": 614,
          "title": "Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. (arXiv:2107.04649v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kouagou_N/0/1/0/all/0/1\">N&#x27;Dah Jean Kouagou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1\">Stefan Heindorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1\">Caglar Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>",
          "description": "Concept learning approaches based on refinement operators explore partially\nordered solution spaces to compute concepts, which are used as binary\nclassification models for individuals. However, the refinement trees spanned by\nthese approaches can easily grow to millions of nodes for complex learning\nproblems. This leads to refinement-based approaches often failing to detect\noptimal concepts efficiently. In this paper, we propose a supervised machine\nlearning approach for learning concept lengths, which allows predicting the\nlength of the target concept and therefore facilitates the reduction of the\nsearch space during concept learning. To achieve this goal, we compare four\nneural architectures and evaluate them on four benchmark knowledge\ngraphs--Carcinogenesis, Mutagenesis, Semantic Bible, Family Benchmark. Our\nevaluation results suggest that recurrent neural network architectures perform\nbest at concept length prediction with an F-measure of up to 92%. We show that\nintegrating our concept length predictor into the CELOE (Class Expression\nLearner for Ontology Engineering) algorithm improves CELOE's runtime by a\nfactor of up to 13.4 without any significant changes to the quality of the\nresults it generates. For reproducibility, we provide our implementation in the\npublic GitHub repository at\nhttps://github.com/ConceptLengthLearner/ReproducibilityRepo",
          "link": "http://arxiv.org/abs/2107.04911",
          "publishedOn": "2021-07-13T01:59:36.620Z",
          "wordCount": 634,
          "title": "Prediction of concept lengths for fast concept learning in description logics. (arXiv:2107.04911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1\">Florian Tambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giulio Antoniol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>",
          "description": "Deep Neural Networks (DNN) applications are increasingly becoming a part of\nour everyday life, from medical applications to autonomous cars. Traditional\nvalidation of DNN relies on accuracy measures, however, the existence of\nadversarial examples has highlighted the limitations of these accuracy\nmeasures, raising concerns especially when DNN are integrated into\nsafety-critical systems. In this paper, we present HOMRS, an approach to boost\nmetamorphic testing by automatically building a small optimized set of high\norder metamorphic relations from an initial set of elementary metamorphic\nrelations. HOMRS' backbone is a multi-objective search; it exploits ideas drawn\nfrom traditional systems testing such as code coverage, test case, and path\ndiversity. We applied HOMRS to LeNet5 DNN with MNIST dataset and we report\nevidence that it builds a small but effective set of high order transformations\nachieving a 95% kill ratio. Five raters manually labeled a pool of images\nbefore and after high order transformation; Fleiss' Kappa and statistical tests\nconfirmed that they are metamorphic properties. HOMRS built-in relations are\nalso effective to confront adversarial or out-of-distribution examples; HOMRS\ndetected 92% of randomly sampled out-of-distribution images. HOMRS\ntransformations are also suitable for online real-time use.",
          "link": "http://arxiv.org/abs/2107.04863",
          "publishedOn": "2021-07-13T01:59:36.603Z",
          "wordCount": 632,
          "title": "HOMRS: High Order Metamorphic Relations Selector for Deep Neural Networks. (arXiv:2107.04863v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zonghan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>",
          "description": "Graph convolutional networks are becoming indispensable for deep learning\nfrom graph-structured data. Most of the existing graph convolutional networks\nshare two big shortcomings. First, they are essentially low-pass filters, thus\nthe potentially useful middle and high frequency band of graph signals are\nignored. Second, the bandwidth of existing graph convolutional filters is\nfixed. Parameters of a graph convolutional filter only transform the graph\ninputs without changing the curvature of a graph convolutional filter function.\nIn reality, we are uncertain about whether we should retain or cut off the\nfrequency at a certain point unless we have expert domain knowledge. In this\npaper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture\nthe full spectrum of graph signals and automatically update the bandwidth of\ngraph convolutional filters. While it is based on graph spectral theory, our\nAutoGCN is also localized in space and has a spatial form. Experimental results\nshow that AutoGCN achieves significant improvement over baseline methods which\nonly work as low-pass filters.",
          "link": "http://arxiv.org/abs/2107.04755",
          "publishedOn": "2021-07-13T01:59:36.593Z",
          "wordCount": 603,
          "title": "Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic Filtering. (arXiv:2107.04755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lantao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhangjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "Multi-agent imitation learning aims to train multiple agents to perform tasks\nfrom demonstrations by learning a mapping between observations and actions,\nwhich is essential for understanding physical, social, and team-play systems.\nHowever, most existing works on modeling multi-agent interactions typically\nassume that agents make independent decisions based on their observations,\nignoring the complex dependence among agents. In this paper, we propose to use\ncopula, a powerful statistical tool for capturing dependence among random\nvariables, to explicitly model the correlation and coordination in multi-agent\nsystems. Our proposed model is able to separately learn marginals that capture\nthe local behavioral patterns of each individual agent, as well as a copula\nfunction that solely and fully captures the dependence structure among agents.\nExtensive experiments on synthetic and real-world datasets show that our model\noutperforms state-of-the-art baselines across various scenarios in the action\nprediction task, and is able to generate new trajectories close to expert\ndemonstrations.",
          "link": "http://arxiv.org/abs/2107.04750",
          "publishedOn": "2021-07-13T01:59:36.585Z",
          "wordCount": 584,
          "title": "Multi-Agent Imitation Learning with Copulas. (arXiv:2107.04750v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uwimana1_A/0/1/0/all/0/1\">Anisie Uwimana1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>",
          "description": "Deep learning models have become a popular choice for medical image analysis.\nHowever, the poor generalization performance of deep learning models limits\nthem from being deployed in the real world as robustness is critical for\nmedical applications. For instance, the state-of-the-art Convolutional Neural\nNetworks (CNNs) fail to detect adversarial samples or samples drawn\nstatistically far away from the training distribution. In this work, we\nexperimentally evaluate the robustness of a Mahalanobis distance-based\nconfidence score, a simple yet effective method for detecting abnormal input\nsamples, in classifying malaria parasitized cells and uninfected cells. Results\nindicated that the Mahalanobis confidence score detector exhibits improved\nperformance and robustness of deep learning models, and achieves\nstateof-the-art performance on both out-of-distribution (OOD) and adversarial\nsamples.",
          "link": "http://arxiv.org/abs/2107.04882",
          "publishedOn": "2021-07-13T01:59:36.576Z",
          "wordCount": 578,
          "title": "Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis. (arXiv:2107.04882v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_A/0/1/0/all/0/1\">Albert Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1\">Ashwin Balakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>",
          "description": "Reinforcement learning (RL) algorithms have shown impressive success in\nexploring high-dimensional environments to learn complex, long-horizon tasks,\nbut can often exhibit unsafe behaviors and require extensive environment\ninteraction when exploration is unconstrained. A promising strategy for safe\nlearning in dynamically uncertain environments is requiring that the agent can\nrobustly return to states where task success (and therefore safety) can be\nguaranteed. While this approach has been successful in low-dimensions,\nenforcing this constraint in environments with high-dimensional state spaces,\nsuch as images, is challenging. We present Latent Space Safe Sets (LS3), which\nextends this strategy to iterative, long-horizon tasks with image observations\nby using suboptimal demonstrations and a learned dynamics model to restrict\nexploration to the neighborhood of a learned Safe Set where task completion is\nlikely. We evaluate LS3 on 4 domains, including a challenging sequential\npushing task in simulation and a physical cable routing task. We find that LS3\ncan use prior task successes to restrict exploration and learn more efficiently\nthan prior algorithms while satisfying constraints. See\nhttps://tinyurl.com/latent-ss for code and supplementary material.",
          "link": "http://arxiv.org/abs/2107.04775",
          "publishedOn": "2021-07-13T01:59:36.558Z",
          "wordCount": 633,
          "title": "LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Iterative Tasks. (arXiv:2107.04775v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "A unique cognitive capability of humans consists in their ability to acquire\nnew knowledge and skills from a sequence of experiences. Meanwhile, artificial\nintelligence systems are good at learning only the last given task without\nbeing able to remember the databases learnt in the past. We propose a novel\nlifelong learning methodology by employing a Teacher-Student network framework.\nWhile the Student module is trained with a new given database, the Teacher\nmodule would remind the Student about the information learnt in the past. The\nTeacher, implemented by a Generative Adversarial Network (GAN), is trained to\npreserve and replay past knowledge corresponding to the probabilistic\nrepresentations of previously learn databases. Meanwhile, the Student module is\nimplemented by a Variational Autoencoder (VAE) which infers its latent variable\nrepresentation from both the output of the Teacher module as well as from the\nnewly available database. Moreover, the Student module is trained to capture\nboth continuous and discrete underlying data representations across different\ndomains. The proposed lifelong learning framework is applied in supervised,\nsemi-supervised and unsupervised training. The code is available~:\n\\url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}",
          "link": "http://arxiv.org/abs/2107.04689",
          "publishedOn": "2021-07-13T01:59:36.535Z",
          "wordCount": 625,
          "title": "Lifelong Teacher-Student Network Learning. (arXiv:2107.04689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1\">Gaurav Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandok_S/0/1/0/all/0/1\">Shivam Chandok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "A common problem with most zero and few-shot learning approaches is they\nsuffer from bias towards seen classes resulting in sub-optimal performance.\nExisting efforts aim to utilize unlabeled images from unseen classes (i.e\ntransductive zero-shot) during training to enable generalization. However, this\nlimits their use in practical scenarios where data from target unseen classes\nis unavailable or infeasible to collect. In this work, we present a practical\nsetting of inductive zero and few-shot learning, where unlabeled images from\nother out-of-data classes, that do not belong to seen or unseen categories, can\nbe used to improve generalization in any-shot learning. We leverage a\nformulation based on product-of-experts and introduce a new AUD module that\nenables us to use unlabeled samples from out-of-data classes which are usually\neasily available and practically entail no annotation cost. In addition, we\nalso demonstrate the applicability of our model to address a more practical and\nchallenging, Generalized Zero-shot under a limited supervision setting, where\neven base seen classes do not have sufficient annotated samples.",
          "link": "http://arxiv.org/abs/2107.04952",
          "publishedOn": "2021-07-13T01:59:36.528Z",
          "wordCount": 614,
          "title": "Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision. (arXiv:2107.04952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Ben Green</a>",
          "description": "In the face of compounding crises of social and economic inequality, many\nhave turned to algorithmic decision-making to achieve greater fairness in\nsociety. As these efforts intensify, reasoning within the burgeoning field of\n\"algorithmic fairness\" increasingly shapes how fairness manifests in practice.\nThis paper interrogates whether algorithmic fairness provides the appropriate\nconceptual and practical tools for enhancing social equality. I argue that the\ndominant, \"formal\" approach to algorithmic fairness is ill-equipped as a\nframework for pursuing equality, as its narrow frame of analysis generates\nrestrictive approaches to reform. In light of these shortcomings, I propose an\nalternative: a \"substantive\" approach to algorithmic fairness that centers\nopposition to social hierarchies and provides a more expansive analysis of how\nto address inequality. This substantive approach enables more fruitful\ntheorizing about the role of algorithms in combatting oppression. The\ndistinction between formal and substantive algorithmic fairness is exemplified\nby each approach's responses to the \"impossibility of fairness\" (an\nincompatibility between mathematical definitions of algorithmic fairness).\nWhile the formal approach requires us to accept the \"impossibility of fairness\"\nas a harsh limit on efforts to enhance equality, the substantive approach\nallows us to escape the \"impossibility of fairness\" by suggesting reforms that\nare not subject to this false dilemma and that are better equipped to\nameliorate conditions of social oppression.",
          "link": "http://arxiv.org/abs/2107.04642",
          "publishedOn": "2021-07-13T01:59:36.507Z",
          "wordCount": 649,
          "title": "Impossibility of What? Formal and Substantive Equality in Algorithmic Fairness. (arXiv:2107.04642v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shuwei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Co-training, extended from self-training, is one of the frameworks for\nsemi-supervised learning. It works at the cost of training extra classifiers,\nwhere the algorithm should be delicately designed to prevent individual\nclassifiers from collapsing into each other. In this paper, we present a simple\nand efficient co-training algorithm, named Multi-Head Co-Training, for\nsemi-supervised image classification. By integrating base learners into a\nmulti-head structure, the model is in a minimal amount of extra parameters.\nEvery classification head in the unified model interacts with its peers through\na \"Weak and Strong Augmentation\" strategy, achieving single-view co-training\nwithout promoting diversity explicitly. The effectiveness of Multi-Head\nCo-Training is demonstrated in an empirical study on standard semi-supervised\nlearning benchmarks.",
          "link": "http://arxiv.org/abs/2107.04795",
          "publishedOn": "2021-07-13T01:59:36.500Z",
          "wordCount": 547,
          "title": "Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shoaib Ahmed Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>",
          "description": "Common neural network architectures are susceptible to attack by adversarial\nsamples. Neural network architectures are commonly thought of as divided into\nlow-level feature extraction layers and high-level classification layers;\nsusceptibility of networks to adversarial samples is often thought of as a\nproblem related to classification rather than feature extraction. We test this\nidea by selectively retraining different portions of VGG and ResNet\narchitectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and\nadversarial data. Our experimental results show that susceptibility to\nadversarial samples is associated with low-level feature extraction layers.\nTherefore, retraining high-level layers is insufficient for achieving\nrobustness. This phenomenon could have two explanations: either, adversarial\nattacks yield outputs from early layers that are indistinguishable from\nfeatures found in the attack classes, or adversarial attacks yield outputs from\nearly layers that differ statistically from features for non-adversarial\nsamples and do not permit consistent classification by subsequent layers. We\ntest this question by large-scale non-linear dimensionality reduction and\ndensity modeling on distributions of feature vectors in hidden layers and find\nthat the feature distributions between non-adversarial and adversarial samples\ndiffer substantially. Our results provide new insights into the statistical\norigins of adversarial samples and possible defenses.",
          "link": "http://arxiv.org/abs/2107.04827",
          "publishedOn": "2021-07-13T01:59:36.494Z",
          "wordCount": 630,
          "title": "Identifying Layers Susceptible to Adversarial Attacks. (arXiv:2107.04827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_B/0/1/0/all/0/1\">Beongjun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dong-Jun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jaekyun Moon</a>",
          "description": "Federated learning has been spotlighted as a way to train neural networks\nusing distributed data with no need for individual nodes to share data.\nUnfortunately, it has also been shown that adversaries may be able to extract\nlocal data contents off model parameters transmitted during federated learning.\nA recent solution based on the secure aggregation primitive enabled\nprivacy-preserving federated learning, but at the expense of significant extra\ncommunication/computational resources. In this paper, we propose a\nlow-complexity scheme that provides data privacy using substantially reduced\ncommunication/computational resources relative to the existing secure solution.\nThe key idea behind the suggested scheme is to design the topology of\nsecret-sharing nodes as a sparse random graph instead of the complete graph\ncorresponding to the existing solution. We first obtain the necessary and\nsufficient condition on the graph to guarantee both reliability and privacy. We\nthen suggest using the Erd\\H{o}s-R\\'enyi graph in particular and provide\ntheoretical guarantees on the reliability/privacy of the proposed scheme.\nThrough extensive real-world experiments, we demonstrate that our scheme, using\nonly $20 \\sim 30\\%$ of the resources required in the conventional scheme,\nmaintains virtually the same levels of reliability and data privacy in\npractical federated learning systems.",
          "link": "http://arxiv.org/abs/2012.05433",
          "publishedOn": "2021-07-13T01:59:36.419Z",
          "wordCount": 673,
          "title": "Communication-Computation Efficient Secure Aggregation for Federated Learning. (arXiv:2012.05433v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Variational autoencoders (VAEs) provide an effective and simple method for\nmodeling complex distributions. However, training VAEs often requires\nconsiderable hyperparameter tuning to determine the optimal amount of\ninformation retained by the latent variable. We study the impact of calibrated\ndecoders, which learn the uncertainty of the decoding distribution and can\ndetermine this amount of information automatically, on the VAE performance.\nWhile many methods for learning calibrated decoders have been proposed, many of\nthe recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc\nmodifications instead. We perform the first comprehensive comparative analysis\nof calibrated decoder and provide recommendations for simple and effective VAE\ntraining. Our analysis covers a range of image and video datasets and several\nsingle-image and sequential VAE models. We further propose a simple but novel\nmodification to the commonly used Gaussian decoder, which computes the\nprediction variance analytically. We observe empirically that using heuristic\nmodifications is not necessary with our method. Project website is at\nhttps://orybkin.github.io/sigma-vae/",
          "link": "http://arxiv.org/abs/2006.13202",
          "publishedOn": "2021-07-13T01:59:36.413Z",
          "wordCount": 661,
          "title": "Simple and Effective VAE Training with Calibrated Decoders. (arXiv:2006.13202v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1\">Stephen J. Maybank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Extracting accurate foregrounds from natural images benefits many downstream\napplications such as film production and augmented reality. However, the furry\ncharacteristics and various appearance of the foregrounds, e.g., animal and\nportrait, challenge existing matting methods, which usually require extra user\ninputs such as trimap or scribbles. To resolve these problems, we study the\ndistinct roles of semantics and details for image matting and decompose the\ntask into two parallel sub-tasks: high-level semantic segmentation and\nlow-level details matting. Specifically, we propose a novel Glance and Focus\nMatting network (GFM), which employs a shared encoder and two separate decoders\nto learn both tasks in a collaborative manner for end-to-end natural image\nmatting. Besides, due to the limitation of available natural images in the\nmatting task, previous methods typically adopt composite images for training\nand evaluation, which result in limited generalization ability on real-world\nimages. In this paper, we investigate the domain gap issue between composite\nimages and real-world images systematically by conducting comprehensive\nanalyses of various discrepancies between foreground and background images. We\nfind that a carefully designed composition route RSSN that aims to reduce the\ndiscrepancies can lead to a better model with remarkable generalization\nability. Furthermore, we provide a benchmark containing 2,000 high-resolution\nreal-world animal images and 10,000 portrait images along with their manually\nlabeled alpha mattes to serve as a test bed for evaluating matting model's\ngeneralization ability on real-world images. Comprehensive empirical studies\nhave demonstrated that GFM outperforms state-of-the-art methods and effectively\nreduces the generalization error. The code and the dataset will be released.",
          "link": "http://arxiv.org/abs/2010.16188",
          "publishedOn": "2021-07-13T01:59:36.391Z",
          "wordCount": 749,
          "title": "Bridge Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16912",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ibrahim_S/0/1/0/all/0/1\">Shahana Ibrahim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>",
          "description": "Learning the joint probability of random variables (RVs) is the cornerstone\nof statistical signal processing and machine learning. However, direct\nnonparametric estimation for high-dimensional joint probability is in general\nimpossible, due to the curse of dimensionality. Recent work has proposed to\nrecover the joint probability mass function (PMF) of an arbitrary number of RVs\nfrom three-dimensional marginals, leveraging the algebraic properties of\nlow-rank tensor decomposition and the (unknown) dependence among the RVs.\nNonetheless, accurately estimating three-dimensional marginals can still be\ncostly in terms of sample complexity, affecting the performance of this line of\nwork in practice in the sample-starved regime. Using three-dimensional\nmarginals also involves challenging tensor decomposition problems whose\ntractability is unclear. This work puts forth a new framework for learning the\njoint PMF using only pairwise marginals, which naturally enjoys a lower sample\ncomplexity relative to the third-order ones. A coupled nonnegative matrix\nfactorization (CNMF) framework is developed, and its joint PMF recovery\nguarantees under various conditions are analyzed. Our method also features a\nGram--Schmidt (GS)-like algorithm that exhibits competitive runtime\nperformance. The algorithm is shown to provably recover the joint PMF up to\nbounded error in finite iterations, under reasonable conditions. It is also\nshown that a recently proposed economical expectation maximization (EM)\nalgorithm guarantees to improve upon the GS-like algorithm's output, thereby\nfurther lifting up the accuracy and efficiency. Real-data experiments are\nemployed to showcase the effectiveness.",
          "link": "http://arxiv.org/abs/2006.16912",
          "publishedOn": "2021-07-13T01:59:36.384Z",
          "wordCount": 687,
          "title": "Recovering Joint Probability of Discrete Random Variables from Pairwise Marginals. (arXiv:2006.16912v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dromey_B/0/1/0/all/0/1\">Brian Dromey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napolitano_R/0/1/0/all/0/1\">Raffaele Napolitano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peebles_D/0/1/0/all/0/1\">Donald M. Peebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>",
          "description": "During pregnancy, ultrasound examination in the second trimester can assess\nfetal size according to standardized charts. To achieve a reproducible and\naccurate measurement, a sonographer needs to identify three standard 2D planes\nof the fetal anatomy (head, abdomen, femur) and manually mark the key\nanatomical landmarks on the image for accurate biometry and fetal weight\nestimation. This can be a time-consuming operator-dependent task, especially\nfor a trainee sonographer. Computer-assisted techniques can help in automating\nthe fetal biometry computation process. In this paper, we present a unified\nautomated framework for estimating all measurements needed for the fetal weight\nassessment. The proposed framework semantically segments the key fetal\nanatomies using state-of-the-art segmentation models, followed by region\nfitting and scale recovery for the biometry estimation. We present an ablation\nstudy of segmentation algorithms to show their robustness through 4-fold\ncross-validation on a dataset of 349 ultrasound standard plane images from 42\npregnancies. Moreover, we show that the network with the best segmentation\nperformance tends to be more accurate for biometry estimation. Furthermore, we\ndemonstrate that the error between clinically measured and predicted fetal\nbiometry is lower than the permissible error during routine clinical\nmeasurements.",
          "link": "http://arxiv.org/abs/2107.05255",
          "publishedOn": "2021-07-13T01:59:36.378Z",
          "wordCount": 653,
          "title": "AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2107.05255v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1\">Francesco Farina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1\">Emma Slade</a>",
          "description": "We introduce a novel architecture for graph networks which is equivariant to\nany transformation in the coordinate embeddings that preserves the distance\nbetween neighbouring nodes. In particular, it is equivariant to the Euclidean\nand conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance\nproperties, the proposed model is extremely more data efficient with respect to\nclassical graph architectures and also intrinsically equipped with a better\ninductive bias. We show that, learning on a minimal amount of data, the\narchitecture we propose can perfectly generalise to unseen data in a synthetic\nproblem, while much more training data are required from a standard model to\nreach comparable performance.",
          "link": "http://arxiv.org/abs/2106.13786",
          "publishedOn": "2021-07-13T01:59:36.359Z",
          "wordCount": 580,
          "title": "Data efficiency in graph networks through equivariance. (arXiv:2106.13786v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1\">Shaoduo Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xiangru Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianbin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hongmei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengzhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tengxu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Binhang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Recent years have witnessed a growing list of systems for distributed\ndata-parallel training. Existing systems largely fit into two paradigms, i.e.,\nparameter server and MPI-style collective operations. On the algorithmic side,\nresearchers have proposed a wide range of techniques to lower the communication\nvia system relaxations: quantization, decentralization, and communication\ndelay. However, most, if not all, existing systems only rely on standard\nsynchronous and asynchronous stochastic gradient (SG) based optimization,\ntherefore, cannot take advantage of all possible optimizations that the machine\nlearning community has been developing recently. Given this emerging gap\nbetween the current landscapes of systems and theory, we build BAGUA, a\ncommunication framework whose design goal is to provide a system abstraction\nthat is both flexible and modular to support state-of-the-art system relaxation\ntechniques of distributed training. Powered by the new system design, BAGUA has\na great ability to implement and extend various state-of-the-art distributed\nlearning algorithms. In a production cluster with up to 16 machines (128 GPUs),\nBAGUA can outperform PyTorch-DDP, Horovod and BytePS in the end-to-end training\ntime by a significant margin (up to 1.95 times) across a diverse range of\ntasks. Moreover, we conduct a rigorous tradeoff exploration showing that\ndifferent algorithms and system relaxations achieve the best performance over\ndifferent network conditions.",
          "link": "http://arxiv.org/abs/2107.01499",
          "publishedOn": "2021-07-13T01:59:36.353Z",
          "wordCount": 690,
          "title": "BAGUA: Scaling up Distributed Learning with System Relaxations. (arXiv:2107.01499v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04595",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Maevskiy_A/0/1/0/all/0/1\">A. Maevskiy</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ratnikov_F/0/1/0/all/0/1\">F. Ratnikov</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zinchenko_A/0/1/0/all/0/1\">A. Zinchenko</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Riabov_V/0/1/0/all/0/1\">V. Riabov</a>",
          "description": "High energy physics experiments rely heavily on the detailed detector\nsimulation models in many tasks. Running these detailed models typically\nrequires a notable amount of the computing time available to the experiments.\nIn this work, we demonstrate a new approach to speed up the simulation of the\nTime Projection Chamber tracker of the MPD experiment at the NICA accelerator\ncomplex. Our method is based on a Generative Adversarial Network - a deep\nlearning technique allowing for implicit estimation of the population\ndistribution for a given set of objects. This approach lets us learn and then\nsample from the distribution of raw detector responses, conditioned on the\nparameters of the charged particle tracks. To evaluate the quality of the\nproposed model, we integrate a prototype into the MPD software stack and\ndemonstrate that it produces high-quality events similar to the detailed\nsimulator, with a speed-up of at least an order of magnitude. The prototype is\ntrained on the responses from the inner part of the detector and, once expanded\nto the full detector, should be ready for use in physics tasks.",
          "link": "http://arxiv.org/abs/2012.04595",
          "publishedOn": "2021-07-13T01:59:36.346Z",
          "wordCount": 692,
          "title": "Simulating the Time Projection Chamber responses at the MPD detector using Generative Adversarial Networks. (arXiv:2012.04595v2 [physics.ins-det] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.11154",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1\">Fanghui Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yudong Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suykens_J/0/1/0/all/0/1\">Johan A.K. Suykens</a>",
          "description": "Random features is one of the most popular techniques to speed up kernel\nmethods in large-scale problems. Related works have been recognized by the\nNeurIPS Test-of-Time award in 2017 and the ICML Best Paper Finalist in 2019.\nThe body of work on random features has grown rapidly, and hence it is\ndesirable to have a comprehensive overview on this topic explaining the\nconnections among various algorithms and theoretical results. In this survey,\nwe systematically review the work on random features from the past ten years.\nFirst, the motivations, characteristics and contributions of representative\nrandom features based algorithms are summarized according to their sampling\nschemes, learning procedures, variance reduction properties and how they\nexploit training data. Second, we review theoretical results that center around\nthe following key question: how many random features are needed to ensure a\nhigh approximation quality or no loss in the empirical/expected risks of the\nlearned estimator. Third, we provide a comprehensive evaluation of popular\nrandom features based algorithms on several large-scale benchmark datasets and\ndiscuss their approximation quality and prediction performance for\nclassification. Last, we discuss the relationship between random features and\nmodern over-parameterized deep neural networks (DNNs), including the use of\nhigh dimensional random features in the analysis of DNNs as well as the gaps\nbetween current theoretical and empirical results. This survey may serve as a\ngentle introduction to this topic, and as a users' guide for practitioners\ninterested in applying the representative algorithms and understanding\ntheoretical results under various technical assumptions. We hope that this\nsurvey will facilitate discussion on the open problems in this topic, and more\nimportantly, shed light on future research directions.",
          "link": "http://arxiv.org/abs/2004.11154",
          "publishedOn": "2021-07-13T01:59:36.340Z",
          "wordCount": 774,
          "title": "Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond. (arXiv:2004.11154v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1\">George Michalopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKillop_I/0/1/0/all/0/1\">Ian McKillop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Helen Chen</a>",
          "description": "Lexical substitution is the task of generating meaningful substitutes for a\nword in a given textual context. Contextual word embedding models have achieved\nstate-of-the-art results in the lexical substitution task by relying on\ncontextual information extracted from the replaced word within the sentence.\nHowever, such models do not take into account structured knowledge that exists\nin external lexical databases.\n\nWe introduce LexSubCon, an end-to-end lexical substitution framework based on\ncontextual embedding models that can identify highly accurate substitute\ncandidates. This is achieved by combining contextual information with knowledge\nfrom structured lexical resources. Our approach involves: (i) introducing a\nnovel mix-up embedding strategy in the creation of the input embedding of the\ntarget word through linearly interpolating the pair of the target input\nembedding and the average embedding of its probable synonyms; (ii) considering\nthe similarity of the sentence-definition embeddings of the target word and its\nproposed candidates; and, (iii) calculating the effect of each substitution in\nthe semantics of the sentence through a fine-tuned sentence similarity model.\nOur experiments show that LexSubCon outperforms previous state-of-the-art\nmethods on LS07 and CoInCo benchmark datasets that are widely used for lexical\nsubstitution tasks.",
          "link": "http://arxiv.org/abs/2107.05132",
          "publishedOn": "2021-07-13T01:59:36.333Z",
          "wordCount": 633,
          "title": "LexSubCon: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution. (arXiv:2107.05132v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Satvik Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pundir_P/0/1/0/all/0/1\">Pradyumn Pundir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_H/0/1/0/all/0/1\">Himanshu Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_H/0/1/0/all/0/1\">Hemraj Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Somya Garg</a>",
          "description": "Precision agriculture system is an arising idea that refers to overseeing\nfarms utilizing current information and communication technologies to improve\nthe quantity and quality of yields while advancing the human work required. The\nautomation requires the assortment of information given by the sensors such as\nsoil, water, light, humidity, temperature for additional information to furnish\nthe operator with exact data to acquire excellent yield to farmers. In this\nwork, a study is proposed that incorporates all common state-of-the-art\napproaches for precision agriculture use. Technologies like the Internet of\nThings (IoT) for data collection, machine Learning for crop damage prediction,\nand deep learning for crop disease detection is used. The data collection using\nIoT is responsible for the measure of moisture levels for smart irrigation, n,\np, k estimations of fertilizers for best yield development. For crop damage\nprediction, various algorithms like Random Forest (RF), Light gradient boosting\nmachine (LGBM), XGBoost (XGB), Decision Tree (DT) and K Nearest Neighbor (KNN)\nare used. Subsequently, Pre-Trained Convolutional Neural Network (CNN) models\nsuch as VGG16, Resnet50, and DenseNet121 are also trained to check if the crop\nwas tainted with some illness or not.",
          "link": "http://arxiv.org/abs/2107.04895",
          "publishedOn": "2021-07-13T01:59:36.315Z",
          "wordCount": 658,
          "title": "Towards a Multimodal System for Precision Agriculture using IoT and Machine Learning. (arXiv:2107.04895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1\">Xuan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Relation prediction among entities in images is an important step in scene\ngraph generation (SGG), which further impacts various visual understanding and\nreasoning tasks. Existing SGG frameworks, however, require heavy training yet\nare incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we\nstress that such incapability is due to the lack of commonsense reasoning,i.e.,\nthe ability to associate similar entities and infer similar relations based on\ngeneral understanding of the world. To fill this gap, we propose\nCommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to\nintegrate commonsense knowledge for SGG, especially for zero-shot relation\nprediction. Specifically, we develop novel graph mining pipelines to model the\nneighborhoods and paths around entities in an external commonsense knowledge\ngraph, and integrate them on top of state-of-the-art SGG frameworks. Extensive\nquantitative evaluations and qualitative case studies on both original and\nmanipulated datasets from Visual Genome demonstrate the effectiveness of our\nproposed approach.",
          "link": "http://arxiv.org/abs/2107.05080",
          "publishedOn": "2021-07-13T01:59:36.309Z",
          "wordCount": 608,
          "title": "Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration. (arXiv:2107.05080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05115",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1\">Basit O. Alawode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1\">Mudassir Masood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1\">Tarig Ballal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1\">Tareq Al-Naffouri</a>",
          "description": "In spite of the improvements achieved by the several denoising algorithms\nover the years, many of them still fail at preserving the fine details of the\nimage after denoising. This is as a result of the smooth-out effect they have\non the images. Most neural network-based algorithms have achieved better\nquantitative performance than the classical denoising algorithms. However, they\nalso suffer from qualitative (visual) performance as a result of the smooth-out\neffect. In this paper, we propose an algorithm to address this shortcoming. We\npropose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image\ndenoising. This algorithm performs collaborative denoising of image patches in\nthe sparse domain using a set of optimized neural network models. This results\nin a fast algorithm that is able to excellently obtain a trade-off between\nnoise removal and details preservation. Extensive experiments show that the\nDeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and\nqualitatively (visually) better than many of the state-of-the-art denoising\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.05115",
          "publishedOn": "2021-07-13T01:59:36.302Z",
          "wordCount": 609,
          "title": "Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising. (arXiv:2107.05115v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagle_S/0/1/0/all/0/1\">Sridevi Narayana Wagle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>",
          "description": "Machine learning algorithms often produce models considered as complex\nblack-box models by both end users and developers. They fail to explain the\nmodel in terms of the domain they are designed for. The proposed Iterative\nVisual Logical Classifier (IVLC) is an interpretable machine learning algorithm\nthat allows end users to design a model and classify data with more confidence\nand without having to compromise on the accuracy. Such technique is especially\nhelpful when dealing with sensitive and crucial data like cancer data in the\nmedical domain with high cost of errors. With the help of the proposed\ninteractive and lossless multidimensional visualization, end users can identify\nthe pattern in the data based on which they can make explainable decisions.\nSuch options would not be possible in black box machine learning methodologies.\nThe interpretable IVLC algorithm is supported by the Interactive Shifted Paired\nCoordinates Software System (SPCVis). It is a lossless multidimensional data\nvisualization system with user interactive features. The interactive approach\nprovides flexibility to the end user to perform data classification as\nself-service without having to rely on a machine learning expert. Interactive\npattern discovery becomes challenging while dealing with large data sets with\nhundreds of dimensions/features. To overcome this problem, this chapter\nproposes an automated classification approach combined with new Coordinate\nOrder Optimizer (COO) algorithm and a Genetic algorithm. The COO algorithm\nautomatically generates the coordinate pair sequences that best represent the\ndata separation and the genetic algorithm helps optimizing the proposed IVLC\nalgorithm by automatically generating the areas for data classification. The\nfeasibility of the approach is shown by experiments on benchmark datasets\ncovering both interactive and automated processes used for data classification.",
          "link": "http://arxiv.org/abs/2107.04971",
          "publishedOn": "2021-07-13T01:59:36.295Z",
          "wordCount": 716,
          "title": "Self-service Data Classification Using Interactive Visualization and Interpretable Machine Learning. (arXiv:2107.04971v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haixu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiehui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "Extending the forecasting time is a critical demand for real applications,\nsuch as extreme weather early warning and long-term energy consumption\nplanning. This paper studies the \\textit{long-term forecasting} problem of time\nseries. Prior Transformer-based models adopt various self-attention mechanisms\nto discover the long-range dependencies. However, intricate temporal patterns\nof the long-term future prohibit the model from finding reliable dependencies.\nAlso, Transformers have to adopt the sparse versions of point-wise\nself-attentions for long series efficiency, resulting in the information\nutilization bottleneck. Towards these challenges, we propose Autoformer as a\nnovel decomposition architecture with an Auto-Correlation mechanism. We go\nbeyond the pre-processing convention of series decomposition and renovate it as\na basic inner block of deep models. This design empowers Autoformer with\nprogressive decomposition capacities for complex time series. Further, inspired\nby the stochastic process theory, we design the Auto-Correlation mechanism\nbased on the series periodicity, which conducts the dependencies discovery and\nrepresentation aggregation at the sub-series level. Auto-Correlation\noutperforms self-attention in both efficiency and accuracy. In long-term\nforecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative\nimprovement on six benchmarks, covering five practical applications: energy,\ntraffic, economics, weather and disease.",
          "link": "http://arxiv.org/abs/2106.13008",
          "publishedOn": "2021-07-13T01:59:36.289Z",
          "wordCount": 651,
          "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. (arXiv:2106.13008v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Interpretable brain network models for disease prediction are of great value\nfor the advancement of neuroscience. GNNs are promising to model complicated\nnetwork data, but they are prone to overfitting and suffer from poor\ninterpretability, which prevents their usage in decision-critical scenarios\nlike healthcare. To bridge this gap, we propose BrainNNExplainer, an\ninterpretable GNN framework for brain network analysis. It is mainly composed\nof two jointly learned modules: a backbone prediction model that is\nspecifically designed for brain networks and an explanation generator that\nhighlights disease-specific prominent brain network connections. Extensive\nexperimental results with visualizations on two challenging disease prediction\ndatasets demonstrate the unique interpretability and outstanding performance of\nBrainNNExplainer.",
          "link": "http://arxiv.org/abs/2107.05097",
          "publishedOn": "2021-07-13T01:59:36.273Z",
          "wordCount": 600,
          "title": "BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis. (arXiv:2107.05097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansourifar_H/0/1/0/all/0/1\">Hadi Mansourifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsagheer_D/0/1/0/all/0/1\">Dana Alsagheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_R/0/1/0/all/0/1\">Reza Fathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weidong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>",
          "description": "With the rise of voice chat rooms, a gigantic resource of data can be exposed\nto the research community for natural language processing tasks. Moderators in\nvoice chat rooms actively monitor the discussions and remove the participants\nwith offensive language. However, it makes the hate speech detection even more\ndifficult since some participants try to find creative ways to articulate hate\nspeech. This makes the hate speech detection challenging in new social media\nlike Clubhouse. To the best of our knowledge all the hate speech datasets have\nbeen collected from text resources like Twitter. In this paper, we take the\nfirst step to collect a significant dataset from Clubhouse as the rising star\nin social media industry. We analyze the collected instances from statistical\npoint of view using the Google Perspective Scores. Our experiments show that,\nthe Perspective Scores can outperform Bag of Words and Word2Vec as high level\ntext features.",
          "link": "http://arxiv.org/abs/2106.13238",
          "publishedOn": "2021-07-13T01:59:36.267Z",
          "wordCount": 617,
          "title": "Hate Speech Detection in Clubhouse. (arXiv:2106.13238v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08659",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1\">Aleksandr Beknazaryan</a>",
          "description": "In this paper it is shown that $C_\\beta$-smooth functions can be approximated\nby neural networks with parameters $\\{0,\\pm \\frac{1}{2}, \\pm 1, 2\\}$. The\ndepth, width and the number of active parameters of the constructed networks\nhave, up to a logarithmic factor, the same dependence on the approximation\nerror as the networks with parameters in $[-1,1]$. In particular, this means\nthat the nonparametric regression estimation with the constructed networks\nattains the same convergence rate as with sparse networks with parameters in\n$[-1,1]$.",
          "link": "http://arxiv.org/abs/2103.08659",
          "publishedOn": "2021-07-13T01:59:36.261Z",
          "wordCount": 536,
          "title": "Function approximation by deep neural networks with parameters $\\{0,\\pm \\frac{1}{2}, \\pm 1, 2\\}$. (arXiv:2103.08659v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14409",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+DSouza_N/0/1/0/all/0/1\">Niharika Shimona D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nebel_M/0/1/0/all/0/1\">Mary Beth Nebel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Crocetti_D/0/1/0/all/0/1\">Deana Crocetti</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wymbs_N/0/1/0/all/0/1\">Nicholas Wymbs</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_J/0/1/0/all/0/1\">Joshua Robinson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mostofsky_S/0/1/0/all/0/1\">Stewart Mostofsky</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Venkataraman_A/0/1/0/all/0/1\">Archana Venkataraman</a>",
          "description": "We propose a novel matrix autoencoder to map functional connectomes from\nresting state fMRI (rs-fMRI) to structural connectomes from Diffusion Tensor\nImaging (DTI), as guided by subject-level phenotypic measures. Our specialized\nautoencoder infers a low dimensional manifold embedding for the rs-fMRI\ncorrelation matrices that mimics a canonical outer-product decomposition. The\nembedding is simultaneously used to reconstruct DTI tractography matrices via a\nsecond manifold alignment decoder and to predict inter-subject phenotypic\nvariability via an artificial neural network. We validate our framework on a\ndataset of 275 healthy individuals from the Human Connectome Project database\nand on a second clinical dataset consisting of 57 subjects with Autism Spectrum\nDisorder. We demonstrate that the model reliably recovers structural\nconnectivity patterns across individuals, while robustly extracting predictive\nand interpretable brain biomarkers in a cross-validated setting. Finally, our\nframework outperforms several baselines at predicting behavioral phenotypes in\nboth real-world datasets.",
          "link": "http://arxiv.org/abs/2105.14409",
          "publishedOn": "2021-07-13T01:59:36.256Z",
          "wordCount": 630,
          "title": "A Matrix Autoencoder Framework to Align the Functional and Structural Connectivity Manifolds as Guided by Behavioral Phenotypes. (arXiv:2105.14409v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Changshun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcone_Y/0/1/0/all/0/1\">Yli&#xe8;s Falcone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensalem_S/0/1/0/all/0/1\">Saddek Bensalem</a>",
          "description": "Classification neural networks fail to detect inputs that do not fall inside\nthe classes they have been trained for. Runtime monitoring techniques on the\nneuron activation pattern can be used to detect such inputs. We present an\napproach for monitoring classification systems via data abstraction. Data\nabstraction relies on the notion of box with a resolution. Box-based\nabstraction consists in representing a set of values by its minimal and maximal\nvalues in each dimension. We augment boxes with a notion of resolution and\ndefine their clustering coverage, which is intuitively a quantitative metric\nthat indicates the abstraction quality. This allows studying the effect of\ndifferent clustering parameters on the constructed boxes and estimating an\ninterval of sub-optimal parameters. Moreover, we automatically construct\nmonitors that leverage both the correct and incorrect behaviors of a system.\nThis allows checking the size of the monitor abstractions and analyzing the\nseparability of the network. Monitors are obtained by combining the\nsub-monitors of each class of the system placed at some selected layers. Our\nexperiments demonstrate the effectiveness of our clustering coverage estimation\nand show how to assess the effectiveness and precision of monitors according to\nthe selected clustering parameter and monitored layers.",
          "link": "http://arxiv.org/abs/2104.14435",
          "publishedOn": "2021-07-13T01:59:36.249Z",
          "wordCount": 668,
          "title": "Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes. (arXiv:2104.14435v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Flaspohler_G/0/1/0/all/0/1\">Genevieve Flaspohler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orabona_F/0/1/0/all/0/1\">Francesco Orabona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1\">Judah Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouatadid_S/0/1/0/all/0/1\">Soukayna Mouatadid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1\">Miruna Oprescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orenstein_P/0/1/0/all/0/1\">Paulo Orenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "Inspired by the demands of real-time climate and weather forecasting, we\ndevelop optimistic online learning algorithms that require no parameter tuning\nand have optimal regret guarantees under delayed feedback. Our algorithms --\nDORM, DORM+, and AdaHedgeD -- arise from a novel reduction of delayed online\nlearning to optimistic online learning that reveals how optimistic hints can\nmitigate the regret penalty caused by delay. We pair this delay-as-optimism\nperspective with a new analysis of optimistic learning that exposes its\nrobustness to hinting errors and a new meta-algorithm for learning effective\nhinting strategies in the presence of delay. We conclude by benchmarking our\nalgorithms on four subseasonal climate forecasting tasks, demonstrating low\nregret relative to state-of-the-art forecasting models.",
          "link": "http://arxiv.org/abs/2106.06885",
          "publishedOn": "2021-07-13T01:59:36.243Z",
          "wordCount": 615,
          "title": "Online Learning with Optimism and Delay. (arXiv:2106.06885v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianbu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>",
          "description": "The monitoring of coastal wetlands is of great importance to the protection\nof marine and terrestrial ecosystems. However, due to the complex environment,\nsevere vegetation mixture, and difficulty of access, it is impossible to\naccurately classify coastal wetlands and identify their species with\ntraditional classifiers. Despite the integration of multisource remote sensing\ndata for performance enhancement, there are still challenges with acquiring and\nexploiting the complementary merits from multisource data. In this paper, the\nDeepwise Feature Interaction Network (DFINet) is proposed for wetland\nclassification. A depthwise cross attention module is designed to extract\nself-correlation and cross-correlation from multisource feature pairs. In this\nway, meaningful complementary information is emphasized for classification.\nDFINet is optimized by coordinating consistency loss, discrimination loss, and\nclassification loss. Accordingly, DFINet reaches the standard solution-space\nunder the regularity of loss functions, while the spatial consistency and\nfeature discrimination are preserved. Comprehensive experimental results on two\nhyperspectral and multispectral wetland datasets demonstrate that the proposed\nDFINet outperforms other competitive methods in terms of overall accuracy.",
          "link": "http://arxiv.org/abs/2106.06896",
          "publishedOn": "2021-07-13T01:59:36.227Z",
          "wordCount": 656,
          "title": "Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alcorn_M/0/1/0/all/0/1\">Michael A. Alcorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Order-agnostic autoregressive distribution (density) estimation (OADE), i.e.,\nautoregressive distribution estimation where the features can occur in an\narbitrary order, is a challenging problem in generative machine learning. Prior\nwork on OADE has encoded feature identity by assigning each feature to a\ndistinct fixed position in an input vector. As a result, architectures built\nfor these inputs must strategically mask either the input or model weights to\nlearn the various conditional distributions necessary for inferring the full\njoint distribution of the dataset in an order-agnostic way. In this paper, we\npropose an alternative approach for encoding feature identities, where each\nfeature's identity is included alongside its value in the input. This feature\nidentity encoding strategy allows neural architectures designed for sequential\ndata to be applied to the OADE task without modification. As a proof of\nconcept, we show that a Transformer trained on this input (which we refer to as\n\"the DEformer\", i.e., the distribution estimating Transformer) can effectively\nmodel binarized-MNIST, approaching the performance of fixed-order\nautoregressive distribution estimating algorithms while still being entirely\norder-agnostic. Additionally, we find that the DEformer surpasses the\nperformance of recent flow-based architectures when modeling a tabular dataset.",
          "link": "http://arxiv.org/abs/2106.06989",
          "publishedOn": "2021-07-13T01:59:36.221Z",
          "wordCount": 663,
          "title": "The DEformer: An Order-Agnostic Distribution Estimating Transformer. (arXiv:2106.06989v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>",
          "description": "In this paper, we propose an end-to-end Mandarin tone classification method\nfrom continuous speech utterances utilizing both the spectrogram and the\nshort-term context information as the input. Both spectrograms and context\nsegment features are used to train the tone classifier. We first divide the\nspectrogram frames into syllable segments using force alignment results\nproduced by an ASR model. Then we extract the short-term segment features to\ncapture the context information across multiple syllables. Feeding both the\nspectrogram and the short-term context segment features into an end-to-end\nmodel could significantly improve the performance. Experiments are performed on\na large-scale open-source Mandarin speech dataset to evaluate the proposed\nmethod. Results show that this method improves the classification accuracy from\n79.5% to 92.6% on the AISHELL3 database.",
          "link": "http://arxiv.org/abs/2104.05657",
          "publishedOn": "2021-07-13T01:59:36.215Z",
          "wordCount": 593,
          "title": "End-to-End Mandarin Tone Classification with Short Term Context Information. (arXiv:2104.05657v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07533",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Laves_M/0/1/0/all/0/1\">Max-Heinrich Laves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tolle_M/0/1/0/all/0/1\">Malte T&#xf6;lle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1\">Alexander Schlaefer</a>",
          "description": "Cold posteriors have been reported to perform better in practice in the\ncontext of Bayesian deep learning (Wenzel et al., 2020). In variational\ninference, it is common to employ only a partially tempered posterior by\nscaling the complexity term in the log-evidence lower bound (ELBO). In this\nwork, we optimize the ELBO for a fully tempered posterior in mean-field\nvariational inference and use Bayesian optimization to automatically find the\noptimal posterior temperature and prior scale. Choosing an appropriate\nposterior temperature leads to better predictive performance and improved\nuncertainty calibration, which we demonstrate for the task of denoising medical\nX-ray images.",
          "link": "http://arxiv.org/abs/2106.07533",
          "publishedOn": "2021-07-13T01:59:36.209Z",
          "wordCount": 555,
          "title": "Cold Posteriors Improve Bayesian Medical Image Post-Processing. (arXiv:2106.07533v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1\">Wolfgang Maass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storey_V/0/1/0/all/0/1\">Veda C. Storey</a>",
          "description": "Both conceptual modeling and machine learning have long been recognized as\nimportant areas of research. With the increasing emphasis on digitizing and\nprocessing large amounts of data for business and other applications, it would\nbe helpful to consider how these areas of research can complement each other.\nTo understand how they can be paired, we provide an overview of machine\nlearning foundations and development cycle. We then examine how conceptual\nmodeling can be applied to machine learning and propose a framework for\nincorporating conceptual modeling into data science projects. The framework is\nillustrated by applying it to a healthcare application. For the inverse\npairing, machine learning can impact conceptual modeling through text and rule\nmining, as well as knowledge graphs. The pairing of conceptual modeling and\nmachine learning in this this way should help lay the foundations for future\nresearch.",
          "link": "http://arxiv.org/abs/2106.14251",
          "publishedOn": "2021-07-13T01:59:36.203Z",
          "wordCount": 608,
          "title": "Pairing Conceptual Modeling with Machine Learning. (arXiv:2106.14251v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zejin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>",
          "description": "The increasing concerns about data privacy and security drive an emerging\nfield of studying privacy-preserving machine learning from isolated data\nsources, i.e., federated learning. A class of federated learning, vertical\nfederated learning, where different parties hold different features for common\nusers, has a great potential of driving a more variety of business cooperation\namong enterprises in many fields. In machine learning, decision tree ensembles\nsuch as gradient boosting decision tree (GBDT) and random forest are widely\napplied powerful models with high interpretability and modeling efficiency.\nHowever, the interpretability is compromised in state-of-the-art vertical\nfederated learning frameworks such as SecureBoost with anonymous features to\navoid possible data breaches. To address this issue in the inference process,\nin this paper, we propose Fed-EINI to protect data privacy and allow the\ndisclosure of feature meaning by concealing decision paths with a\ncommunication-efficient secure computation method for inference outputs. The\nadvantages of Fed-EINI will be demonstrated through both theoretical analysis\nand extensive numerical results.",
          "link": "http://arxiv.org/abs/2105.09540",
          "publishedOn": "2021-07-13T01:59:36.188Z",
          "wordCount": 655,
          "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borner_K/0/1/0/all/0/1\">Katy B&#xf6;rner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yingnan Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>",
          "description": "Ubiquitous internet access is reshaping the way we live, but it is\naccompanied by unprecedented challenges in preventing chronic diseases planted\nby long exposure to unhealthy lifestyles. This paper proposes leveraging online\nshopping behaviors as a proxy for personal lifestyle choices to improve chronic\ndisease prevention literacy targeted for times when e-commerce user experience\nhas been assimilated into most people's daily lives. Here, retrospective\nlongitudinal query logs and purchase records from millions of online shoppers\nwere accessed, constructing a broad spectrum of lifestyle features covering\nassorted product categories and buyer personas. Using the lifestyle-related\ninformation preceding their first purchases of prescription drugs, we could\ndetermine associations between online shoppers' past lifestyle choices and\nwhether they suffered from a particular chronic disease or not. Novel lifestyle\nrisk factors were discovered in two exemplars -- depression and diabetes, most\nof which showed cognitive congruence with existing healthcare knowledge.\nFurther, such empirical findings could be adopted to locate online shoppers at\nhigh risk of these chronic diseases with fair accuracy, closely matching the\nperformance of screening surveys benchmarked against medical diagnosis.\nUnobtrusive chronic disease surveillance via e-commerce sites may soon meet\nconsenting individuals in the digital space they already inhabit.",
          "link": "http://arxiv.org/abs/2104.14281",
          "publishedOn": "2021-07-13T01:59:36.182Z",
          "wordCount": 711,
          "title": "Leveraging Online Shopping Behaviors as a Proxy for Personal Lifestyle Choices: New Insights into Chronic Disease Prevention Literacy. (arXiv:2104.14281v3 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Viet Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanasusanto_G/0/1/0/all/0/1\">Grani A. Hanasusanto</a>",
          "description": "We propose a distributionally robust classification model with a fairness\nconstraint that encourages the classifier to be fair in view of the equality of\nopportunity criterion. We use a type-$\\infty$ Wasserstein ambiguity set\ncentered at the empirical distribution to model distributional uncertainty and\nderive a conservative reformulation for the worst-case equal opportunity\nunfairness measure. We establish that the model is equivalent to a mixed binary\noptimization problem, which can be solved by standard off-the-shelf solvers. To\nimprove scalability, we further propose a convex, hinge-loss-based model for\nlarge problem instances whose reformulation does not incur any binary\nvariables. Moreover, we also consider the distributionally robust learning\nproblem with a generic ground transportation cost to hedge against the\nuncertainties in the label and sensitive attribute. Finally, we numerically\ndemonstrate that our proposed approaches improve fairness with negligible loss\nof predictive accuracy.",
          "link": "http://arxiv.org/abs/2103.06828",
          "publishedOn": "2021-07-13T01:59:36.175Z",
          "wordCount": 603,
          "title": "Wasserstein Robust Classification with Fairness Constraints. (arXiv:2103.06828v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham M. Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovett_S/0/1/0/all/0/1\">Shachar Lovett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1\">Gaurav Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruosong Wang</a>",
          "description": "This work introduces Bilinear Classes, a new structural framework, which\npermit generalization in reinforcement learning in a wide variety of settings\nthrough the use of function approximation. The framework incorporates nearly\nall existing models in which a polynomial sample complexity is achievable, and,\nnotably, also includes new models, such as the Linear $Q^*/V^*$ model in which\nboth the optimal $Q$-function and the optimal $V$-function are linear in some\nknown feature space. Our main result provides an RL algorithm which has\npolynomial sample complexity for Bilinear Classes; notably, this sample\ncomplexity is stated in terms of a reduction to the generalization error of an\nunderlying supervised learning sub-problem. These bounds nearly match the best\nknown sample complexity bounds for existing models. Furthermore, this framework\nalso extends to the infinite dimensional (RKHS) setting: for the the Linear\n$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample\ncomplexities that have no explicit dependence on the explicit feature dimension\n(which could be infinite), but instead depends only on information theoretic\nquantities.",
          "link": "http://arxiv.org/abs/2103.10897",
          "publishedOn": "2021-07-13T01:59:36.169Z",
          "wordCount": 691,
          "title": "Bilinear Classes: A Structural Framework for Provable Generalization in RL. (arXiv:2103.10897v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12284",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jacaruso_L/0/1/0/all/0/1\">Lucas Cassiel Jacaruso</a>",
          "description": "Deep learning methods have shown suitability for time series classification\nin the health and medical domain, with promising results for electrocardiogram\ndata classification. Successful identification of myocardial infarction holds\nlife saving potential and any meaningful improvement upon deep learning models\nin this area is of great interest. Conventionally, data augmentation methods\nare applied universally to the training set when data are limited in order to\nameliorate data resolution or sample size. In the method proposed in this\nstudy, data augmentation was not applied in the context of data scarcity.\nInstead, samples that yield low confidence predictions were selectively\naugmented in order to bolster the model's sensitivity to features or patterns\nless strongly associated with a given class. This approach was tested for\nimproving the performance of a Fully Convolutional Network. The proposed\napproach achieved 90 percent accuracy for classifying myocardial infarction as\nopposed to 82 percent accuracy for the baseline, a marked improvement. Further,\nthe accuracy of the proposed approach was optimal near a defined upper\nthreshold for qualifying low confidence samples and decreased as this threshold\nwas raised to include higher confidence samples. This suggests exclusively\nselecting lower confidence samples for data augmentation comes with distinct\nbenefits for electrocardiogram data classification with Fully Convolutional\nNetworks.",
          "link": "http://arxiv.org/abs/2104.12284",
          "publishedOn": "2021-07-13T01:59:36.163Z",
          "wordCount": 666,
          "title": "Accuracy Improvement for Fully Convolutional Networks via Selective Augmentation with Applications to Electrocardiogram Data. (arXiv:2104.12284v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>",
          "description": "We introduce a novel embedding model, named NoGE, which aims to integrate\nco-occurrence among entities and relations into graph neural networks to\nimprove knowledge graph completion (i.e., link prediction). Given a knowledge\ngraph, NoGE constructs a single graph considering entities and relations as\nindividual nodes. NoGE then computes weights for edges among nodes based on the\nco-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion\nGraph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector\nrepresentations for entity and relation nodes. NoGE then adopts a score\nfunction to produce the triple scores. Comprehensive experimental results show\nthat NoGE obtains state-of-the-art results on three new and difficult benchmark\ndatasets CoDEx for knowledge graph completion.",
          "link": "http://arxiv.org/abs/2104.07396",
          "publishedOn": "2021-07-13T01:59:36.146Z",
          "wordCount": 593,
          "title": "Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction. (arXiv:2104.07396v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1\">Tobias Sebastian Finn</a>",
          "description": "Ensemble data from Earth system models has to be calibrated and\npost-processed. I propose a novel member-by-member post-processing approach\nwith neural networks. I bridge ideas from ensemble data assimilation with\nself-attention, resulting into the self-attentive ensemble transformer. Here,\ninteractions between ensemble members are represented as additive and dynamic\nself-attentive part. As proof-of-concept, I regress global ECMWF ensemble\nforecasts to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate\nthat the ensemble transformer can calibrate the ensemble spread and extract\nadditional information from the ensemble. As it is a member-by-member approach,\nthe ensemble transformer directly outputs multivariate and spatially-coherent\nensemble members. Therefore, self-attention and the transformer technique can\nbe a missing piece for a non-parametric post-processing of ensemble data with\nneural networks.",
          "link": "http://arxiv.org/abs/2106.13924",
          "publishedOn": "2021-07-13T01:59:36.130Z",
          "wordCount": 610,
          "title": "Self-Attentive Ensemble Transformer: Representing Ensemble Interactions in Neural Networks for Earth System Models. (arXiv:2106.13924v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingheng Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1\">Rob Gorbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1\">Dana Kuli&#x107;</a>",
          "description": "A promising characteristic of Deep Reinforcement Learning (DRL) is its\ncapability to learn optimal policy in an end-to-end manner without relying on\nfeature engineering. However, most approaches assume a fully observable state\nspace, i.e. fully observable Markov Decision Process (MDP). In real-world\nrobotics, this assumption is unpractical, because of the sensor issues such as\nsensors' capacity limitation and sensor noise, and the lack of knowledge about\nif the observation design is complete or not. These scenarios lead to Partially\nObservable MDP (POMDP) and need special treatment. In this paper, we propose\nLong-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient\n(LSTM-TD3) by introducing a memory component to TD3, and compare its\nperformance with other DRL algorithms in both MDPs and POMDPs. Our results\ndemonstrate the significant advantages of the memory component in addressing\nPOMDPs, including the ability to handle missing and noisy observation data.",
          "link": "http://arxiv.org/abs/2102.12344",
          "publishedOn": "2021-07-13T01:59:36.124Z",
          "wordCount": 614,
          "title": "Memory-based Deep Reinforcement Learning for POMDP. (arXiv:2102.12344v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karingula_S/0/1/0/all/0/1\">Sankeerth Rao Karingula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_N/0/1/0/all/0/1\">Nandini Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahmasbi_R/0/1/0/all/0/1\">Rasool Tahmasbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amjadi_M/0/1/0/all/0/1\">Mehrnaz Amjadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Deokwoo Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_R/0/1/0/all/0/1\">Ricky Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thimmisetty_C/0/1/0/all/0/1\">Charanraj Thimmisetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_L/0/1/0/all/0/1\">Luisa Polania Cabrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayer_M/0/1/0/all/0/1\">Marjorie Sayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1\">Claudionor Nunes Coelho Jr</a>",
          "description": "Time series forecasting is a fundamental task emerging from diverse\ndata-driven applications. Many advanced autoregressive methods such as ARIMA\nwere used to develop forecasting models. Recently, deep learning based methods\nsuch as DeepAr, NeuralProphet, Seq2Seq have been explored for time series\nforecasting problem. In this paper, we propose a novel time series forecast\nmodel, DeepGB. We formulate and implement a variant of Gradient boosting\nwherein the weak learners are DNNs whose weights are incrementally found in a\ngreedy manner over iterations. In particular, we develop a new embedding\narchitecture that improves the performance of many deep learning models on time\nseries using Gradient boosting variant. We demonstrate that our model\noutperforms existing comparable state-of-the-art models using real-world sensor\ndata and public dataset.",
          "link": "http://arxiv.org/abs/2104.04781",
          "publishedOn": "2021-07-13T01:59:36.084Z",
          "wordCount": 604,
          "title": "Boosted Embeddings for Time Series Forecasting. (arXiv:2104.04781v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1\">Andrew Wagenmaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1\">Kevin Jamieson</a>",
          "description": "Exploration in unknown environments is a fundamental problem in reinforcement\nlearning and control. In this work, we study task-guided exploration and\ndetermine what precisely an agent must learn about their environment in order\nto complete a particular task. Formally, we study a broad class of\ndecision-making problems in the setting of linear dynamical systems, a class\nthat includes the linear quadratic regulator problem. We provide instance- and\ntask-dependent lower bounds which explicitly quantify the difficulty of\ncompleting a task of interest. Motivated by our lower bound, we propose a\ncomputationally efficient experiment-design based exploration algorithm. We\nshow that it optimally explores the environment, collecting precisely the\ninformation needed to complete the task, and provide finite-time bounds\nguaranteeing that it achieves the instance- and task-optimal sample complexity,\nup to constant factors. Through several examples of the LQR problem, we show\nthat performing task-guided exploration provably improves on exploration\nschemes which do not take into account the task of interest. Along the way, we\nestablish that certainty equivalence decision making is instance- and\ntask-optimal, and obtain the first algorithm for the linear quadratic regulator\nproblem which is instance-optimal. We conclude with several experiments\nillustrating the effectiveness of our approach in practice.",
          "link": "http://arxiv.org/abs/2102.05214",
          "publishedOn": "2021-07-13T01:59:36.068Z",
          "wordCount": 662,
          "title": "Task-Optimal Exploration in Linear Dynamical Systems. (arXiv:2102.05214v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Huayi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhimeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1\">Xiaorong Pu</a>",
          "description": "Multi-view clustering is an important research topic due to its capability to\nutilize complementary information from multiple views. However, there are few\nmethods to consider the negative impact caused by certain views with unclear\nclustering structures, resulting in poor multi-view clustering performance. To\naddress this drawback, we propose self-supervised discriminative feature\nlearning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders\nare applied to learn embedded features for each view independently. To leverage\nthe multi-view complementary information, we concatenate all views' embedded\nfeatures to form the global features, which can overcome the negative impact of\nsome views' unclear clustering structures. In a self-supervised manner,\npseudo-labels are obtained to build a unified target distribution to perform\nmulti-view discriminative feature learning. During this process, global\ndiscriminative information can be mined to supervise all views to learn more\ndiscriminative features, which in turn are used to update the target\ndistribution. Besides, this unified target distribution can make SDMVC learn\nconsistent cluster assignments, which accomplishes the clustering consistency\nof multiple views while preserving their features' diversity. Experiments on\nvarious types of multi-view datasets show that SDMVC achieves state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2103.15069",
          "publishedOn": "2021-07-13T01:59:36.061Z",
          "wordCount": 656,
          "title": "Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering. (arXiv:2103.15069v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1\">Cynthia Rudin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semenova_L/0/1/0/all/0/1\">Lesia Semenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Chudi Zhong</a>",
          "description": "Interpretability in machine learning (ML) is crucial for high stakes\ndecisions and troubleshooting. In this work, we provide fundamental principles\nfor interpretable ML, and dispel common misunderstandings that dilute the\nimportance of this crucial topic. We also identify 10 technical challenge areas\nin interpretable machine learning and provide history and background on each\nproblem. Some of these problems are classically important, and some are recent\nproblems that have arisen in the last few years. These problems are: (1)\nOptimizing sparse logical models such as decision trees; (2) Optimization of\nscoring systems; (3) Placing constraints into generalized additive models to\nencourage sparsity and better interpretability; (4) Modern case-based\nreasoning, including neural networks and matching for causal inference; (5)\nComplete supervised disentanglement of neural networks; (6) Complete or even\npartial unsupervised disentanglement of neural networks; (7) Dimensionality\nreduction for data visualization; (8) Machine learning models that can\nincorporate physics and other generative or causal constraints; (9)\nCharacterization of the \"Rashomon set\" of good models; and (10) Interpretable\nreinforcement learning. This survey is suitable as a starting point for\nstatisticians and computer scientists interested in working in interpretable\nmachine learning.",
          "link": "http://arxiv.org/abs/2103.11251",
          "publishedOn": "2021-07-13T01:59:36.055Z",
          "wordCount": 662,
          "title": "Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges. (arXiv:2103.11251v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gamzu_I/0/1/0/all/0/1\">Iftah Gamzu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutiel_G/0/1/0/all/0/1\">Gilad Kutiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Ran Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1\">Eugene Agichtein</a>",
          "description": "In recent years online shopping has gained momentum and became an important\nvenue for customers wishing to save time and simplify their shopping process. A\nkey advantage of shopping online is the ability to read what other customers\nare saying about products of interest. In this work, we aim to maintain this\nadvantage in situations where extreme brevity is needed, for example, when\nshopping by voice. We suggest a novel task of extracting a single\nrepresentative helpful sentence from a set of reviews for a given product. The\nselected sentence should meet two conditions: first, it should be helpful for a\npurchase decision and second, the opinion it expresses should be supported by\nmultiple reviewers. This task is closely related to the task of Multi Document\nSummarization in the product reviews domain but differs in its objective and\nits level of conciseness. We collect a dataset in English of sentence\nhelpfulness scores via crowd-sourcing and demonstrate its reliability despite\nthe inherent subjectivity involved. Next, we describe a complete model that\nextracts representative helpful sentences with positive and negative sentiment\ntowards the product and demonstrate that it outperforms several baselines.",
          "link": "http://arxiv.org/abs/2104.09792",
          "publishedOn": "2021-07-13T01:59:36.046Z",
          "wordCount": 660,
          "title": "Identifying Helpful Sentences in Product Reviews. (arXiv:2104.09792v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1\">Rafa&#x142; Powalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1\">&#x141;ukasz Borchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1\">Dawid Jurkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1\">Tomasz Dwojak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1\">Micha&#x142; Pietruszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1\">Gabriela Pa&#x142;ka</a>",
          "description": "We address the challenging problem of Natural Language Comprehension beyond\nplain-text documents by introducing the TILT neural network architecture which\nsimultaneously learns layout information, visual features, and textual\nsemantics. Contrary to previous approaches, we rely on a decoder capable of\nunifying a variety of problems involving natural language. The layout is\nrepresented as an attention bias and complemented with contextualized visual\ninformation, while the core of our model is a pretrained encoder-decoder\nTransformer. Our novel approach achieves state-of-the-art results in extracting\ninformation from documents and answering questions which demand layout\nunderstanding (DocVQA, CORD, SROIE). At the same time, we simplify the process\nby employing an end-to-end model.",
          "link": "http://arxiv.org/abs/2102.09550",
          "publishedOn": "2021-07-13T01:59:36.039Z",
          "wordCount": 589,
          "title": "Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. (arXiv:2102.09550v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1\">Farzan Memarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goo_W/0/1/0/all/0/1\">Wonjoon Goo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1\">Rudolf Lioutikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>",
          "description": "We propose a novel reinforcement learning framework that performs\nself-supervised online reward shaping, yielding faster, sample efficient\nperformance in sparse-reward environments. The proposed framework alternates\nbetween updating a policy and inferring a reward function. While the policy\nupdate is performed with the inferred, potentially dense reward function, the\noriginal sparse reward is used to provide a self-supervisory signal for the\nreward update by serving as an ordering over the observed trajectories. The\nproposed framework is based on the theory that altering the reward function\ndoes not affect the optimal policy of the original MDP as long as certain\nrelations between the altered and the original reward are maintained. We name\nthe proposed framework ClAssification-based Reward Shaping (CaReS), since the\naltered reward is learned in a self-supervised manner using classifier-based\nreward inference. Experimental results on several sparse-reward environments\ndemonstrate that the proposed algorithm is not only significantly more sample\nefficient than the state-of-the-art reinforcement learning baseline but also\nachieves a similar sample efficiency to a baseline that uses hand-designed\ndense reward functions.",
          "link": "http://arxiv.org/abs/2103.04529",
          "publishedOn": "2021-07-13T01:59:36.032Z",
          "wordCount": 640,
          "title": "Self-Supervised Online Reward Shaping in Sparse-Reward Environments. (arXiv:2103.04529v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1\">Micah J. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cito_J/0/1/0/all/0/1\">J&#xfc;rgen Cito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kelvin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1\">Kalyan Veeramachaneni</a>",
          "description": "While the open-source software development model has led to successful\nlarge-scale collaborations in building software systems, data science projects\nare frequently developed by individuals or small teams. We describe challenges\nto scaling data science collaborations and present a conceptual framework and\nML programming model to address them. We instantiate these ideas in Ballet, a\nlightweight framework for collaborative, open-source data science through a\nfocus on feature engineering, and an accompanying cloud-based development\nenvironment. Using our framework, collaborators incrementally propose feature\ndefinitions to a repository which are each subjected to an ML performance\nevaluation and can be automatically merged into an executable feature\nengineering pipeline. We leverage Ballet to conduct a case study analysis of an\nincome prediction problem with 27 collaborators, and discuss implications for\nfuture designers of collaborative projects.",
          "link": "http://arxiv.org/abs/2012.07816",
          "publishedOn": "2021-07-13T01:59:36.000Z",
          "wordCount": 609,
          "title": "Enabling collaborative data science development with the Ballet framework. (arXiv:2012.07816v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1\">Shahana Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>",
          "description": "This work considers clustering nodes of a largely incomplete graph. Under the\nproblem setting, only a small amount of queries about the edges can be made,\nbut the entire graph is not observable. This problem finds applications in\nlarge-scale data clustering using limited annotations, community detection\nunder restricted survey resources, and graph topology inference under\nhidden/removed node interactions. Prior works tackled this problem from various\nperspectives, e.g., convex programming-based low-rank matrix completion and\nactive query-based clique finding. Nonetheless, many existing methods are\ndesigned for estimating the single-cluster membership of the nodes, but nodes\nmay often have mixed (i.e., multi-cluster) membership in practice. Some query\nand computational paradigms, e.g., the random query patterns and nuclear\nnorm-based optimization advocated in the convex approaches, may give rise to\nscalability and implementation challenges. This work aims at learning mixed\nmembership of nodes using queried edges. The proposed method is developed\ntogether with a systematic query principle that can be controlled and adjusted\nby the system designers to accommodate implementation challenges -- e.g., to\navoid querying edges that are physically hard to acquire. Our framework also\nfeatures a lightweight and scalable algorithm with membership learning\nguarantees. Real-data experiments on crowdclustering and community detection\nare used to showcase the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2011.12988",
          "publishedOn": "2021-07-13T01:59:35.994Z",
          "wordCount": 663,
          "title": "Mixed Membership Graph Clustering via Systematic Edge Query. (arXiv:2011.12988v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>",
          "description": "Recently, many plug-and-play self-attention modules are proposed to enhance\nthe model generalization by exploiting the internal information of deep\nconvolutional neural networks (CNNs). Previous works lay an emphasis on the\ndesign of attention module for specific functionality, e.g., light-weighted or\ntask-oriented attention. However, they ignore the importance of where to plug\nin the attention module since they connect the modules individually with each\nblock of the entire CNN backbone for granted, leading to incremental\ncomputational cost and number of parameters with the growth of network depth.\nThus, we propose a framework called Efficient Attention Network (EAN) to\nimprove the efficiency for the existing attention modules. In EAN, we leverage\nthe sharing mechanism (Huang et al. 2020) to share the attention module within\nthe backbone and search where to connect the shared attention module via\nreinforcement learning. Finally, we obtain the attention network with sparse\nconnections between the backbone and modules, while (1) maintaining accuracy\n(2) reducing extra parameter increment and (3) accelerating inference.\nExtensive experiments on widely-used benchmarks and popular attention networks\nshow the effectiveness of EAN. Furthermore, we empirically illustrate that our\nEAN has the capacity of transferring to other tasks and capturing the\ninformative features. The code is available at\nhttps://github.com/gbup-group/EAN-efficient-attention-network.",
          "link": "http://arxiv.org/abs/2011.14058",
          "publishedOn": "2021-07-13T01:59:35.987Z",
          "wordCount": 685,
          "title": "Efficient Attention Network: Accelerate Attention by Searching Where to Plug. (arXiv:2011.14058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linzhang Wang</a>",
          "description": "The last decade has witnessed a rapid advance in machine learning models.\nWhile the black-box nature of these systems allows powerful predictions, it\ncannot be directly explained, posing a threat to the continuing democratization\nof machine learning technology.\n\nTackling the challenge of model explainability, research has made significant\nprogress in demystifying the image classification models. In the same spirit of\nthese works, this paper studies code summarization models, particularly, given\nan input program for which a model makes a prediction, our goal is to reveal\nthe key features that the model uses for predicting the label of the program.\nWe realize our approach in HouYi, which we use to evaluate four prominent code\nsummarization models: extreme summarizer, code2vec, code2seq, and sequence GNN.\nResults show that all models base their predictions on syntactic and lexical\nproperties with little to none semantic implication. Based on this finding, we\npresent a novel approach to explaining the predictions of code summarization\nmodels through the lens of training data.\n\nOur work opens up this exciting, new direction of studying what models have\nlearned from source code.",
          "link": "http://arxiv.org/abs/2102.04625",
          "publishedOn": "2021-07-13T01:59:35.980Z",
          "wordCount": 659,
          "title": "WheaCha: A Method for Explaining the Predictions of Code Summarization Models. (arXiv:2102.04625v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vincent_Cuaz_C/0/1/0/all/0/1\">C&#xe9;dric Vincent-Cuaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vayer_T/0/1/0/all/0/1\">Titouan Vayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1\">R&#xe9;mi Flamary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corneli_M/0/1/0/all/0/1\">Marco Corneli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1\">Nicolas Courty</a>",
          "description": "Dictionary learning is a key tool for representation learning, that explains\nthe data as linear combination of few basic elements. Yet, this analysis is not\namenable in the context of graph learning, as graphs usually belong to\ndifferent metric spaces. We fill this gap by proposing a new online Graph\nDictionary Learning approach, which uses the Gromov Wasserstein divergence for\nthe data fitting term. In our work, graphs are encoded through their nodes'\npairwise relations and modeled as convex combination of graph atoms, i.e.\ndictionary elements, estimated thanks to an online stochastic algorithm, which\noperates on a dataset of unregistered graphs with potentially different number\nof nodes. Our approach naturally extends to labeled graphs, and is completed by\na novel upper bound that can be used as a fast approximation of Gromov\nWasserstein in the embedding space. We provide numerical evidences showing the\ninterest of our approach for unsupervised embedding of graph datasets and for\nonline graph subspace estimation and tracking.",
          "link": "http://arxiv.org/abs/2102.06555",
          "publishedOn": "2021-07-13T01:59:35.966Z",
          "wordCount": 613,
          "title": "Online Graph Dictionary Learning. (arXiv:2102.06555v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12876",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_G/0/1/0/all/0/1\">Gexin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zhu Liang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Z/0/1/0/all/0/1\">ZhengHui Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_F/0/1/0/all/0/1\">Feifei Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">YuanQing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jiawen Liang</a>",
          "description": "Electromagnetic source imaging (ESI) is a highly ill-posed inverse problem.\nTo find a unique solution, traditional ESI methods impose a variety of priors\nthat may not reflect the actual source properties. Such limitations of\ntraditional ESI methods hinder their further applications. Inspired by deep\nlearning approaches, a novel data-synthesized spatio-temporal denoising\nautoencoder method (DST-DAE) method was proposed to solve the ESI inverse\nproblem. Unlike the traditional methods, we utilize a neural network to\ndirectly seek generalized mapping from the measured E/MEG signals to the\ncortical sources. A novel data synthesis strategy is employed by introducing\nthe prior information of sources to the generated large-scale samples using the\nforward model of ESI. All the generated data are used to drive the neural\nnetwork to automatically learn inverse mapping. To achieve better estimation\nperformance, a denoising autoencoder (DAE) architecture with spatio-temporal\nfeature extraction blocks is designed. Compared with the traditional methods,\nwe show (1) that the novel deep learning approach provides an effective and\neasy-to-apply way to solve the ESI problem, that (2) compared to traditional\nmethods, DST-DAE with the data synthesis strategy can better consider the\ncharacteristics of real sources than the mathematical formulation of prior\nassumptions, and that (3) the specifically designed architecture of DAE can not\nonly provide a better estimation of source signals but also be robust to noise\npollution. Extensive numerical experiments show that the proposed method is\nsuperior to the traditional knowledge-driven ESI methods.",
          "link": "http://arxiv.org/abs/2010.12876",
          "publishedOn": "2021-07-13T01:59:35.929Z",
          "wordCount": 735,
          "title": "Electromagnetic Source Imaging via a Data-Synthesis-Based Denoising Autoencoder. (arXiv:2010.12876v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bugata_P/0/1/0/all/0/1\">Peter Bugata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drotar_P/0/1/0/all/0/1\">Peter Drotar</a>",
          "description": "Feature selection is important step in machine learning since it has shown to\nimprove prediction accuracy while depressing the curse of dimensionality of\nhigh dimensional data. The neural networks have experienced tremendous success\nin solving many nonlinear learning problems. Here, we propose new\nneural-network based feature selection approach that introduces two constrains,\nthe satisfying of which leads to sparse FS layer. We have performed extensive\nexperiments on synthetic and real world data to evaluate performance of the\nproposed FS. In experiments we focus on the high dimension, low sample size\ndata since those represent the main challenge for feature selection. The\nresults confirm that proposed Feature Selection Based on Sparse Neural Network\nLayer with Normalizing Constraints (SNEL-FS) is able to select the important\nfeatures and yields superior performance compared to other conventional FS\nmethods.",
          "link": "http://arxiv.org/abs/2012.06365",
          "publishedOn": "2021-07-13T01:59:35.923Z",
          "wordCount": 610,
          "title": "Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints. (arXiv:2012.06365v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1\">Jayadev Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canonne_C/0/1/0/all/0/1\">Cl&#xe9;ment L. Canonne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Ziteng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1\">Himanshu Tyagi</a>",
          "description": "We consider the task of distributed parameter estimation using interactive\nprotocols subject to local information constraints such as bandwidth\nlimitations, local differential privacy, and restricted measurements. We\nprovide a unified framework enabling us to derive a variety of (tight) minimax\nlower bounds for different parametric families of distributions, both\ncontinuous and discrete, under any $\\ell_p$ loss. Our lower bound framework is\nversatile and yields \"plug-and-play\" bounds that are widely applicable to a\nlarge range of estimation problems. In particular, our approach recovers bounds\nobtained using data processing inequalities and Cram\\'er--Rao bounds, two other\nalternative approaches for proving lower bounds in our setting of interest.\nFurther, for the families considered, we complement our lower bounds with\nmatching upper bounds.",
          "link": "http://arxiv.org/abs/2010.06562",
          "publishedOn": "2021-07-13T01:59:35.917Z",
          "wordCount": 636,
          "title": "Unified lower bounds for interactive high-dimensional estimation under information constraints. (arXiv:2010.06562v4 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1\">Micha&#x142; Derezi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zhenyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "For a tall $n\\times d$ matrix $A$ and a random $m\\times n$ sketching matrix\n$S$, the sketched estimate of the inverse covariance matrix $(A^\\top A)^{-1}$\nis typically biased: $E[(\\tilde A^\\top\\tilde A)^{-1}]\\ne(A^\\top A)^{-1}$, where\n$\\tilde A=SA$. This phenomenon, which we call inversion bias, arises, e.g., in\nstatistics and distributed optimization, when averaging multiple independently\nconstructed estimates of quantities that depend on the inverse covariance. We\ndevelop a framework for analyzing inversion bias, based on our proposed concept\nof an $(\\epsilon,\\delta)$-unbiased estimator for random matrices. We show that\nwhen the sketching matrix $S$ is dense and has i.i.d. sub-gaussian entries,\nthen after simple rescaling, the estimator $(\\frac m{m-d}\\tilde A^\\top\\tilde\nA)^{-1}$ is $(\\epsilon,\\delta)$-unbiased for $(A^\\top A)^{-1}$ with a sketch of\nsize $m=O(d+\\sqrt d/\\epsilon)$. This implies that for $m=O(d)$, the inversion\nbias of this estimator is $O(1/\\sqrt d)$, which is much smaller than the\n$\\Theta(1)$ approximation error obtained as a consequence of the subspace\nembedding guarantee for sub-gaussian sketches. We then propose a new sketching\ntechnique, called LEverage Score Sparsified (LESS) embeddings, which uses ideas\nfrom both data-oblivious sparse embeddings as well as data-aware leverage-based\nrow sampling methods, to get $\\epsilon$ inversion bias for sketch size\n$m=O(d\\log d+\\sqrt d/\\epsilon)$ in time $O(\\text{nnz}(A)\\log n+md^2)$, where\nnnz is the number of non-zeros. The key techniques enabling our analysis\ninclude an extension of a classical inequality of Bai and Silverstein for\nrandom quadratic forms, which we call the Restricted Bai-Silverstein\ninequality; and anti-concentration of the Binomial distribution via the\nPaley-Zygmund inequality, which we use to prove a lower bound showing that\nleverage score sampling sketches generally do not achieve small inversion bias.",
          "link": "http://arxiv.org/abs/2011.10695",
          "publishedOn": "2021-07-13T01:59:35.910Z",
          "wordCount": 735,
          "title": "Sparse sketches with small inversion bias. (arXiv:2011.10695v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1\">Serban Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>",
          "description": "Convolutional neural networks (CNNs) have led to significant improvements in\ntasks involving semantic segmentation of images. CNNs are vulnerable in the\narea of biomedical image segmentation because of distributional gap between two\nsource and target domains with different data modalities which leads to domain\nshift. Domain shift makes data annotations in new modalities necessary because\nmodels must be retrained from scratch. Unsupervised domain adaptation (UDA) is\nproposed to adapt a model to new modalities using solely unlabeled target\ndomain data. Common UDA algorithms require access to data points in the source\ndomain which may not be feasible in medical imaging due to privacy concerns. In\nthis work, we develop an algorithm for UDA in a privacy-constrained setting,\nwhere the source domain data is inaccessible. Our idea is based on encoding the\ninformation from the source samples into a prototypical distribution that is\nused as an intermediate distribution for aligning the target domain\ndistribution with the source domain distribution. We demonstrate the\neffectiveness of our algorithm by comparing it to state-of-the-art medical\nimage semantic segmentation approaches on two medical image semantic\nsegmentation datasets.",
          "link": "http://arxiv.org/abs/2101.00522",
          "publishedOn": "2021-07-13T01:59:35.904Z",
          "wordCount": 659,
          "title": "Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images. (arXiv:2101.00522v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Thi-Vinh Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong-Thai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1\">Thanh-Le Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_K/0/1/0/all/0/1\">Khac-Quy Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>",
          "description": "Prior works have demonstrated that a low-resource language pair can benefit\nfrom multilingual machine translation (MT) systems, which rely on many language\npairs' joint training. This paper proposes two simple strategies to address the\nrare word issue in multilingual MT systems for two low-resource language pairs:\nFrench-Vietnamese and English-Vietnamese. The first strategy is about dynamical\nlearning word similarity of tokens in the shared space among source languages\nwhile another one attempts to augment the translation ability of rare words\nthrough updating their embeddings during the training. Besides, we leverage\nmonolingual data for multilingual MT systems to increase the amount of\nsynthetic parallel corpora while dealing with the data sparsity problem. We\nhave shown significant improvements of up to +1.62 and +2.54 BLEU points over\nthe bilingual baseline systems for both language pairs and released our\ndatasets for the research community.",
          "link": "http://arxiv.org/abs/2012.08743",
          "publishedOn": "2021-07-13T01:59:35.898Z",
          "wordCount": 627,
          "title": "Improving Multilingual Neural Machine Translation For Low-Resource Languages: French,English - Vietnamese. (arXiv:2012.08743v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaotao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "Due to the excessive cost of large-scale language model pre-training,\nconsiderable efforts have been made to train BERT progressively -- start from\nan inferior but low-cost model and gradually grow the model to increase the\ncomputational complexity. Our objective is to advance the understanding of\nTransformer growth and discover principles that guide progressive training.\nFirst, we find that similar to network architecture search, Transformer growth\nalso favors compound scaling. Specifically, while existing methods only conduct\nnetwork growth in a single dimension, we observe that it is beneficial to use\ncompound growth operators and balance multiple dimensions (e.g., depth, width,\nand input length of the model). Moreover, we explore alternative growth\noperators in each dimension via controlled comparison to give operator\nselection practical guidance. In light of our analyses, the proposed method\nspeeds up BERT pre-training by 73.6% and 82.2% for the base and large models\nrespectively, while achieving comparable performances",
          "link": "http://arxiv.org/abs/2010.12562",
          "publishedOn": "2021-07-13T01:59:35.876Z",
          "wordCount": 632,
          "title": "On the Transformer Growth for Progressive BERT Training. (arXiv:2010.12562v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.07314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1\">Sadeep Jayasumana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1\">Ankit Singh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1\">Himanshu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>",
          "description": "Real-world classification problems typically exhibit an imbalanced or\nlong-tailed label distribution, wherein many labels are associated with only a\nfew samples. This poses a challenge for generalisation on such labels, and also\nmakes na\\\"ive learning biased towards dominant labels. In this paper, we\npresent two simple modifications of standard softmax cross-entropy training to\ncope with these challenges. Our techniques revisit the classic idea of logit\nadjustment based on the label frequencies, either applied post-hoc to a trained\nmodel, or enforced in the loss during training. Such adjustment encourages a\nlarge relative margin between logits of rare versus dominant labels. These\ntechniques unify and generalise several recent proposals in the literature,\nwhile possessing firmer statistical grounding and empirical performance.",
          "link": "http://arxiv.org/abs/2007.07314",
          "publishedOn": "2021-07-13T01:59:35.867Z",
          "wordCount": 594,
          "title": "Long-tail learning via logit adjustment. (arXiv:2007.07314v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehyeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Injune Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyundo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunseo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Won-Seok Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joseph J. Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>",
          "description": "Active learning is widely used to reduce labeling effort and training time by\nrepeatedly querying only the most beneficial samples from unlabeled data. In\nreal-world problems where data cannot be stored indefinitely due to limited\nstorage or privacy issues, the query selection and the model update should be\nperformed as soon as a new data sample is observed. Various online active\nlearning methods have been studied to deal with these challenges; however,\nthere are difficulties in selecting representative query samples and updating\nthe model efficiently without forgetting. In this study, we propose Message\nPassing Adaptive Resonance Theory (MPART) that learns the distribution and\ntopology of input data online. Through message passing on the topological\ngraph, MPART actively queries informative and representative samples, and\ncontinuously improves the classification performance using both labeled and\nunlabeled data. We evaluate our model in stream-based selective sampling\nscenarios with comparable query selection strategies, showing that MPART\nsignificantly outperforms competitive models.",
          "link": "http://arxiv.org/abs/2012.01227",
          "publishedOn": "2021-07-13T01:59:35.830Z",
          "wordCount": 642,
          "title": "Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning. (arXiv:2012.01227v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01807",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Hurwitz_C/0/1/0/all/0/1\">Cole Hurwitz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kudryashova_N/0/1/0/all/0/1\">Nina Kudryashova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1\">Arno Onken</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hennig_M/0/1/0/all/0/1\">Matthias H. Hennig</a>",
          "description": "Modern recording technologies now enable simultaneous recording from large\nnumbers of neurons. This has driven the development of new statistical models\nfor analyzing and interpreting neural population activity. Here we provide a\nbroad overview of recent developments in this area. We compare and contrast\ndifferent approaches, highlight strengths and limitations, and discuss\nbiological and mechanistic insights that these methods provide.",
          "link": "http://arxiv.org/abs/2102.01807",
          "publishedOn": "2021-07-13T01:59:35.812Z",
          "wordCount": 536,
          "title": "Building population models for large-scale neural recordings: opportunities and pitfalls. (arXiv:2102.01807v4 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13472",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vowels_M/0/1/0/all/0/1\">Matthew James Vowels</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Camgoz_N/0/1/0/all/0/1\">Necati Cihan Camgoz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "Undertaking causal inference with observational data is incredibly useful\nacross a wide range of tasks including the development of medical treatments,\nadvertisements and marketing, and policy making. There are two significant\nchallenges associated with undertaking causal inference using observational\ndata: treatment assignment heterogeneity (i.e., differences between the treated\nand untreated groups), and an absence of counterfactual data (i.e., not knowing\nwhat would have happened if an individual who did get treatment, were instead\nto have not been treated). We address these two challenges by combining\nstructured inference and targeted learning. In terms of structure, we factorize\nthe joint distribution into risk, confounding, instrumental, and miscellaneous\nfactors, and in terms of targeted learning, we apply a regularizer derived from\nthe influence curve in order to reduce residual bias. An ablation study is\nundertaken, and an evaluation on benchmark datasets demonstrates that TVAE has\ncompetitive and state of the art performance across.",
          "link": "http://arxiv.org/abs/2009.13472",
          "publishedOn": "2021-07-13T01:59:35.805Z",
          "wordCount": 607,
          "title": "Targeted VAE: Variational and Targeted Learning for Causal Inference. (arXiv:2009.13472v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04857",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1\">Basit O. Alawode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1\">Mudassir Masood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1\">Tarig Ballal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1\">Tareq Al-Naffouri</a>",
          "description": "Recently, deep learning (DL) methods such as convolutional neural networks\n(CNNs) have gained prominence in the area of image denoising. This is owing to\ntheir proven ability to surpass state-of-the-art classical image denoising\nalgorithms such as BM3D. Deep denoising CNNs (DnCNNs) use many feedforward\nconvolution layers with added regularization methods of batch normalization and\nresidual learning to improve denoising performance significantly. However, this\ncomes at the expense of a huge number of trainable parameters. In this paper,\nwe address this issue by reducing the number of parameters while achieving a\ncomparable level of performance. We derive motivation from the improved\nperformance obtained by training networks using the dense-sparse-dense (DSD)\ntraining approach. We extend this training approach to a reduced DnCNN (RDnCNN)\nnetwork resulting in a faster denoising network with significantly reduced\nparameters and comparable performance to the DnCNN.",
          "link": "http://arxiv.org/abs/2107.04857",
          "publishedOn": "2021-07-13T01:59:35.799Z",
          "wordCount": 582,
          "title": "Dense-Sparse Deep CNN Training for Image Denoising. (arXiv:2107.04857v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Summers_C/0/1/0/all/0/1\">Cecilia Summers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinneen_M/0/1/0/all/0/1\">Michael J. Dinneen</a>",
          "description": "Nondeterminism in neural network optimization produces uncertainty in\nperformance, making small improvements difficult to discern from run-to-run\nvariability. While uncertainty can be reduced by training multiple model\ncopies, doing so is time-consuming, costly, and harms reproducibility. In this\nwork, we establish an experimental protocol for understanding the effect of\noptimization nondeterminism on model diversity, allowing us to isolate the\neffects of a variety of sources of nondeterminism. Surprisingly, we find that\nall sources of nondeterminism have similar effects on measures of model\ndiversity. To explain this intriguing fact, we identify the instability of\nmodel training, taken as an end-to-end procedure, as the key determinant. We\nshow that even one-bit changes in initial parameters result in models\nconverging to vastly different values. Last, we propose two approaches for\nreducing the effects of instability on run-to-run variability.",
          "link": "http://arxiv.org/abs/2103.04514",
          "publishedOn": "2021-07-13T01:59:35.791Z",
          "wordCount": 600,
          "title": "Nondeterminism and Instability in Neural Network Optimization. (arXiv:2103.04514v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.15056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>",
          "description": "In this paper, we present a new class of invertible transformations with an\napplication to flow-based generative models. We indicate that many well-known\ninvertible transformations in reversible logic and reversible neural networks\ncould be derived from our proposition. Next, we propose two new coupling layers\nthat are important building blocks of flow-based generative models. In the\nexperiments on digit data, we present how these new coupling layers could be\nused in Integer Discrete Flows (IDF), and that they achieve better results than\nstandard coupling layers used in IDF and RealNVP.",
          "link": "http://arxiv.org/abs/2011.15056",
          "publishedOn": "2021-07-13T01:59:35.775Z",
          "wordCount": 556,
          "title": "General Invertible Transformations for Flow-based Generative Modeling. (arXiv:2011.15056v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Allan Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dangovski_R/0/1/0/all/0/1\">Rumen Dangovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugan_O/0/1/0/all/0/1\">Owen Dugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Samuel Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1\">Marin Solja&#x10d;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1\">Joseph Jacobson</a>",
          "description": "Deep learning owes much of its success to the astonishing expressiveness of\nneural networks. However, this comes at the cost of complex, black-boxed models\nthat extrapolate poorly beyond the domain of the training dataset, conflicting\nwith goals of finding analytic expressions to describe science, engineering and\nreal world data. Under the hypothesis that the hierarchical modularity of such\nlaws can be captured by training a neural network, we introduce OccamNet, a\nneural network model that finds interpretable, compact, and sparse solutions\nfor fitting data, \\`{a} la Occam's razor. Our model defines a probability\ndistribution over a non-differentiable function space. We introduce a two-step\noptimization method that samples functions and updates the weights with\nbackpropagation based on cross-entropy matching in an evolutionary strategy: we\ntrain by biasing the probability mass toward better fitting solutions. OccamNet\nis able to fit a variety of symbolic laws including simple analytic functions,\nrecursive programs, implicit functions, simple image classification, and can\noutperform noticeably state-of-the-art symbolic regression methods on real\nworld regression datasets. Our method requires minimal memory footprint, does\nnot require AI accelerators for efficient training, fits complicated functions\nin minutes of training on a single CPU, and demonstrates significant\nperformance gains when scaled on a GPU. Our implementation, demonstrations and\ninstructions for reproducing the experiments are available at\nhttps://github.com/druidowm/OccamNet_Public.",
          "link": "http://arxiv.org/abs/2007.10784",
          "publishedOn": "2021-07-13T01:59:35.767Z",
          "wordCount": 693,
          "title": "Fast Neural Models for Symbolic Regression at Scale. (arXiv:2007.10784v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chuyu Huang</a>",
          "description": "The metro ridership prediction has always received extensive attention from\ngovernments and researchers. Recent works focus on designing complicated graph\nconvolutional recurrent network architectures to capture spatial and temporal\npatterns. These works extract the information of spatial dimension well, but\nthe limitation of temporal dimension still exists. We extended Neural ODE\nalgorithms to the graph network and proposed the STR-GODEs network, which can\neffectively learn spatial, temporal, and ridership correlations without the\nlimitation of dividing data into equal-sized intervals on the timeline. While\nlearning the spatial relations and the temporal correlations, we modify the\nGODE-RNN cell to obtain the ridership feature and hidden states. Ridership\ninformation and its hidden states are added to the GODESolve to reduce the\nerror accumulation caused by long time series in prediction. Extensive\nexperiments on two large-scale datasets demonstrate the efficacy and robustness\nof our model.",
          "link": "http://arxiv.org/abs/2107.04980",
          "publishedOn": "2021-07-13T01:59:35.760Z",
          "wordCount": 564,
          "title": "STR-GODEs: Spatial-Temporal-Ridership Graph ODEs for Metro Ridership Prediction. (arXiv:2107.04980v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+OConnor_D/0/1/0/all/0/1\">Daniel O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinci_W/0/1/0/all/0/1\">Walter Vinci</a>",
          "description": "Efficient sampling of complex data distributions can be achieved using\ntrained invertible flows (IF), where the model distribution is generated by\npushing a simple base distribution through multiple non-linear bijective\ntransformations. However, the iterative nature of the transformations in IFs\ncan limit the approximation to the target distribution. In this paper we seek\nto mitigate this by implementing RBM-Flow, an IF model whose base distribution\nis a Restricted Boltzmann Machine (RBM) with a continuous smoothing applied. We\nshow that by using RBM-Flow we are able to improve the quality of samples\ngenerated, quantified by the Inception Scores (IS) and Frechet Inception\nDistance (FID), over baseline models with the same IF transformations, but with\nless expressive base distributions. Furthermore, we also obtain D-Flow, an IF\nmodel with uncorrelated discrete latent variables. We show that D-Flow achieves\nsimilar likelihoods and FID/IS scores to those of a typical IF with Gaussian\nbase variables, but with the additional benefit that global features are\nmeaningfully encoded as discrete labels in the latent space.",
          "link": "http://arxiv.org/abs/2012.13196",
          "publishedOn": "2021-07-13T01:59:35.753Z",
          "wordCount": 639,
          "title": "RBM-Flow and D-Flow: Invertible Flows with Discrete Energy Base Spaces. (arXiv:2012.13196v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.00339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruff_L/0/1/0/all/0/1\">Lukas Ruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandermeulen_R/0/1/0/all/0/1\">Robert A. Vandermeulen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franks_B/0/1/0/all/0/1\">Billy Joe Franks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1\">Marius Kloft</a>",
          "description": "Though anomaly detection (AD) can be viewed as a classification problem\n(nominal vs. anomalous) it is usually treated in an unsupervised manner since\none typically does not have access to, or it is infeasible to utilize, a\ndataset that sufficiently characterizes what it means to be \"anomalous.\" In\nthis paper we present results demonstrating that this intuition surprisingly\nseems not to extend to deep AD on images. For a recent AD benchmark on\nImageNet, classifiers trained to discern between normal samples and just a few\n(64) random natural images are able to outperform the current state of the art\nin deep AD. Experimentally we discover that the multiscale structure of image\ndata makes example anomalies exceptionally informative.",
          "link": "http://arxiv.org/abs/2006.00339",
          "publishedOn": "2021-07-13T01:59:35.747Z",
          "wordCount": 598,
          "title": "Rethinking Assumptions in Deep Anomaly Detection. (arXiv:2006.00339v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1\">Arnd Koeppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1\">Franz Bamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1\">Michael Selzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1\">Britta Nestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1\">Bernd Markert</a>",
          "description": "(Artificial) neural networks have become increasingly popular in mechanics to\naccelerate computations with model order reduction techniques and as universal\nmodels for a wide variety of materials. However, the major disadvantage of\nneural networks remains: their numerous parameters are challenging to interpret\nand explain. Thus, neural networks are often labeled as black boxes, and their\nresults often elude human interpretation. In mechanics, the new and active\nfield of physics-informed neural networks attempts to mitigate this\ndisadvantage by designing deep neural networks on the basis of mechanical\nknowledge. By using this a priori knowledge, deeper and more complex neural\nnetworks became feasible, since the mechanical assumptions could be explained.\nHowever, the internal reasoning and explanation of neural network parameters\nremain mysterious.\n\nComplementary to the physics-informed approach, we propose a first step\ntowards a physics-informing approach, which explains neural networks trained on\nmechanical data a posteriori. This novel explainable artificial intelligence\napproach aims at elucidating the black box of neural networks and their\nhigh-dimensional representations. Therein, the principal component analysis\ndecorrelates the distributed representations in cell states of RNNs and allows\nthe comparison to known and fundamental functions. The novel approach is\nsupported by a systematic hyperparameter search strategy that identifies the\nbest neural network architectures and training parameters. The findings of\nthree case studies on fundamental constitutive models (hyperelasticity,\nelastoplasticity, and viscoelasticity) imply that the proposed strategy can\nhelp identify numerical and analytical closed-form solutions to characterize\nnew materials.",
          "link": "http://arxiv.org/abs/2104.10683",
          "publishedOn": "2021-07-13T01:59:35.730Z",
          "wordCount": 736,
          "title": "Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.14536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>",
          "description": "It is commonly believed that networks cannot be both accurate and robust,\nthat gaining robustness means losing accuracy. It is also generally believed\nthat, unless making networks larger, network architectural elements would\notherwise matter little in improving adversarial robustness. Here we present\nevidence to challenge these common beliefs by a careful study about adversarial\ntraining. Our key observation is that the widely-used ReLU activation function\nsignificantly weakens adversarial training due to its non-smooth nature. Hence\nwe propose smooth adversarial training (SAT), in which we replace ReLU with its\nsmooth approximations to strengthen adversarial training. The purpose of smooth\nactivation functions in SAT is to allow it to find harder adversarial examples\nand compute better gradient updates during adversarial training.\n\nCompared to standard adversarial training, SAT improves adversarial\nrobustness for \"free\", i.e., no drop in accuracy and no increase in\ncomputational cost. For example, without introducing additional computations,\nSAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while\nalso improving accuracy by 0.9% on ImageNet. SAT also works well with larger\nnetworks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%\nrobustness on ImageNet, outperforming the previous state-of-the-art defense by\n9.5% for accuracy and 11.6% for robustness. Models are available at\nhttps://github.com/cihangxie/SmoothAdversarialTraining.",
          "link": "http://arxiv.org/abs/2006.14536",
          "publishedOn": "2021-07-13T01:59:35.723Z",
          "wordCount": 674,
          "title": "Smooth Adversarial Training. (arXiv:2006.14536v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shinzaki_M/0/1/0/all/0/1\">Masao Shinzaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koda_Y/0/1/0/all/0/1\">Yusuke Koda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1\">Koji Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1\">Takayuki Nishio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morikura_M/0/1/0/all/0/1\">Masahiro Morikura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirato_Y/0/1/0/all/0/1\">Yushi Shirato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_D/0/1/0/all/0/1\">Daisei Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kita_N/0/1/0/all/0/1\">Naoki Kita</a>",
          "description": "Millimeter wave (mmWave) beam-tracking based on machine learning enables the\ndevelopment of accurate tracking policies while obviating the need to\nperiodically solve beam-optimization problems. However, its applicability is\nstill arguable when training-test gaps exist in terms of environmental\nparameters that affect the node dynamics. From this skeptical point of view,\nthe contribution of this study is twofold. First, by considering an example\nscenario, we confirm that the training-test gap adversely affects the\nbeam-tracking performance. More specifically, we consider nodes placed on\noverhead messenger wires, where the node dynamics are affected by several\nenvironmental parameters, e.g, the wire mass and tension. Although these are\nparticular scenarios, they yield insight into the validation of the\ntraining-test gap problems. Second, we demonstrate the feasibility of\n\\textit{zero-shot adaptation} as a solution, where a learning agent adapts to\nenvironmental parameters unseen during training. This is achieved by leveraging\na robust adversarial reinforcement learning (RARL) technique, where such\ntraining-and-test gaps are regarded as disturbances by adversaries that are\njointly trained with a legitimate beam-tracking agent. Numerical evaluations\ndemonstrate that the beam-tracking policy learned via RARL can be applied to a\nwide range of environmental parameters without severely degrading the received\npower.",
          "link": "http://arxiv.org/abs/2102.08055",
          "publishedOn": "2021-07-13T01:59:35.716Z",
          "wordCount": 694,
          "title": "Zero-Shot Adaptation for mmWave Beam-Tracking on Overhead Messenger Wires through Robust Adversarial Reinforcement Learning. (arXiv:2102.08055v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikkelsen_K/0/1/0/all/0/1\">Kaare Mikkelsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_O/0/1/0/all/0/1\">Oliver Y. Ch&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Philipp Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertins_A/0/1/0/all/0/1\">Alfred Mertins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1\">Maarten De Vos</a>",
          "description": "Black-box skepticism is one of the main hindrances impeding\ndeep-learning-based automatic sleep scoring from being used in clinical\nenvironments. Towards interpretability, this work proposes a\nsequence-to-sequence sleep-staging model, namely SleepTransformer. It is based\non the transformer backbone whose self-attention scores offer interpretability\nof the model's decisions at both the epoch and sequence level. At the epoch\nlevel, the attention scores can be encoded as a heat map to highlight\nsleep-relevant features captured from the input EEG signal. At the sequence\nlevel, the attention scores are visualized as the influence of different\nneighboring epochs in an input sequence (i.e. the context) to recognition of a\ntarget epoch, mimicking the way manual scoring is done by human experts. We\nfurther propose a simple yet efficient method to quantify uncertainty in the\nmodel's decisions. The method, which is based on entropy, can serve as a metric\nfor deferring low-confidence epochs to a human expert for further inspection.\nAdditionally, we demonstrate that the proposed SleepTransformer outperforms\nexisting methods at a lower computational cost and achieves state-of-the-art\nperformance on two experimental databases of different sizes.",
          "link": "http://arxiv.org/abs/2105.11043",
          "publishedOn": "2021-07-13T01:59:35.706Z",
          "wordCount": 663,
          "title": "SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification. (arXiv:2105.11043v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1\">Yonathan Efroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlis_N/0/1/0/all/0/1\">Nadav Merlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aadirupa Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>",
          "description": "A core element in decision-making under uncertainty is the feedback on the\nquality of the performed actions. However, in many applications, such feedback\nis restricted. For example, in recommendation systems, repeatedly asking the\nuser to provide feedback on the quality of recommendations will annoy them. In\nthis work, we formalize decision-making problems with querying budget, where\nthere is a (possibly time-dependent) hard limit on the number of reward queries\nallowed. Specifically, we consider multi-armed bandits, linear bandits, and\nreinforcement learning problems. We start by analyzing the performance of\n`greedy' algorithms that query a reward whenever they can. We show that in\nfully stochastic settings, doing so performs surprisingly well, but in the\npresence of any adversity, this might lead to linear regret. To overcome this\nissue, we propose the Confidence-Budget Matching (CBM) principle that queries\nrewards when the confidence intervals are wider than the inverse square root of\nthe available budget. We analyze the performance of CBM based algorithms in\ndifferent settings and show that they perform well in the presence of adversity\nin the contexts, initial states, and budgets.",
          "link": "http://arxiv.org/abs/2102.03400",
          "publishedOn": "2021-07-13T01:59:35.700Z",
          "wordCount": 646,
          "title": "Confidence-Budget Matching for Sequential Budgeted Learning. (arXiv:2102.03400v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruize Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "The maximum mean discrepancy (MMD) test could in principle detect any\ndistributional discrepancy between two datasets. However, it has been shown\nthat the MMD test is unaware of adversarial attacks -- the MMD test failed to\ndetect the discrepancy between natural and adversarial data. Given this\nphenomenon, we raise a question: are natural and adversarial data really from\ndifferent distributions? The answer is affirmative -- the previous use of the\nMMD test on the purpose missed three key factors, and accordingly, we propose\nthree components. Firstly, the Gaussian kernel has limited representation\npower, and we replace it with an effective deep kernel. Secondly, the test\npower of the MMD test was neglected, and we maximize it following asymptotic\nstatistics. Finally, adversarial data may be non-independent, and we overcome\nthis issue with the wild bootstrap. By taking care of the three factors, we\nverify that the MMD test is aware of adversarial attacks, which lights up a\nnovel road for adversarial data detection based on two-sample tests.",
          "link": "http://arxiv.org/abs/2010.11415",
          "publishedOn": "2021-07-13T01:59:35.657Z",
          "wordCount": 649,
          "title": "Maximum Mean Discrepancy Test is Aware of Adversarial Attacks. (arXiv:2010.11415v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hoang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>",
          "description": "We develop a new algorithm for non-convex stochastic optimization that finds\nan $\\epsilon$-critical point in the optimal $O(\\epsilon^{-3})$ stochastic\ngradient and Hessian-vector product computations. Our algorithm uses\nHessian-vector products to \"correct\" a bias term in the momentum of SGD with\nmomentum. This leads to better gradient estimates in a manner analogous to\nvariance reduction methods. In contrast to prior work, we do not require\nexcessively large batch sizes, and are able to provide an adaptive algorithm\nwhose convergence rate automatically improves with decreasing variance in the\ngradient estimates. We validate our results on a variety of large-scale deep\nlearning architectures and benchmarks tasks.",
          "link": "http://arxiv.org/abs/2103.03265",
          "publishedOn": "2021-07-13T01:59:35.652Z",
          "wordCount": 555,
          "title": "Better SGD using Second-order Momentum. (arXiv:2103.03265v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+YinchuanLi/0/1/0/all/0/1\">YinchuanLi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XiaofengLiu/0/1/0/all/0/1\">XiaofengLiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YunfengShao/0/1/0/all/0/1\">YunfengShao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+QingWang/0/1/0/all/0/1\">QingWang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YanhuiGeng/0/1/0/all/0/1\">YanhuiGeng</a>",
          "description": "Structured pruning is an effective compression technique to reduce the\ncomputation of neural networks, which is usually achieved by adding\nperturbations to reduce network parameters at the cost of slightly increasing\ntraining loss. A more reasonable approach is to find a sparse minimizer along\nthe flat minimum valley found by optimizers, i.e. stochastic gradient descent,\nwhich keeps the training loss constant. To achieve this goal, we propose the\nstructured directional pruning based on orthogonal projecting the perturbations\nonto the flat minimum valley. We also propose a fast solver sDprun and further\nprove that it achieves directional pruning asymptotically after sufficient\ntraining. Experiments using VGG-Net and ResNet on CIFAR-10 and CIFAR-100\ndatasets show that our method obtains the state-of-the-art pruned accuracy\n(i.e. 93.97% on VGG16, CIFAR-10 task) without retraining. Experiments using\nDNN, VGG-Net and WRN28X10 on MNIST, CIFAR-10 and CIFAR-100 datasets demonstrate\nour method performs structured directional pruning, reaching the same minimum\nvalley as the optimizer.",
          "link": "http://arxiv.org/abs/2107.05328",
          "publishedOn": "2021-07-13T01:59:35.634Z",
          "wordCount": 585,
          "title": "Structured Directional Pruning via Perturbation Orthogonal Projection. (arXiv:2107.05328v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1\">Keisuke Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeishi_N/0/1/0/all/0/1\">Naoya Takeishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsutsui_K/0/1/0/all/0/1\">Kazushi Tsutsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujioka_E/0/1/0/all/0/1\">Emyo Fujioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishiumi_N/0/1/0/all/0/1\">Nozomi Nishiumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_R/0/1/0/all/0/1\">Ryooya Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukushiro_M/0/1/0/all/0/1\">Mika Fukushiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ide_K/0/1/0/all/0/1\">Kaoru Ide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohno_H/0/1/0/all/0/1\">Hiroyoshi Kohno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoda_K/0/1/0/all/0/1\">Ken Yoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_S/0/1/0/all/0/1\">Susumu Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiryu_S/0/1/0/all/0/1\">Shizuko Hiryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1\">Yoshinobu Kawahara</a>",
          "description": "Extracting the interaction rules of biological agents from moving sequences\npose challenges in various domains. Granger causality is a practical framework\nfor analyzing the interactions from observed time-series data; however, this\nframework ignores the structures of the generative process in animal behaviors,\nwhich may lead to interpretational problems and sometimes erroneous assessments\nof causality. In this paper, we propose a new framework for learning Granger\ncausality from multi-animal trajectories via augmented theory-based behavioral\nmodels with interpretable data-driven models. We adopt an approach for\naugmenting incomplete multi-agent behavioral models described by time-varying\ndynamical systems with neural networks. For efficient and interpretable\nlearning, our model leverages theory-based architectures separating navigation\nand motion processes, and the theory-guided regularization for reliable\nbehavioral modeling. This can provide interpretable signs of Granger-causal\neffects over time, i.e., when specific others cause the approach or separation.\nIn experiments using synthetic datasets, our method achieved better performance\nthan various baselines. We then analyzed multi-animal datasets of mice, flies,\nbirds, and bats, which verified our method and obtained novel biological\ninsights.",
          "link": "http://arxiv.org/abs/2107.05326",
          "publishedOn": "2021-07-13T01:59:35.625Z",
          "wordCount": 632,
          "title": "Learning interaction rules from multi-animal trajectories via augmented behavioral models. (arXiv:2107.05326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04248",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Dylewsky_D/0/1/0/all/0/1\">Daniel Dylewsky</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Barajas_Solano_D/0/1/0/all/0/1\">David Barajas-Solano</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ma_T/0/1/0/all/0/1\">Tong Ma</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1\">Alexandre M. Tartakovsky</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kutz_J/0/1/0/all/0/1\">J. Nathan Kutz</a>",
          "description": "Time series forecasting remains a central challenge problem in almost all\nscientific disciplines. We introduce a novel load forecasting method in which\nobserved dynamics are modeled as a forced linear system using Dynamic Mode\nDecomposition (DMD) in time delay coordinates. Central to this approach is the\ninsight that grid load, like many observables on complex real-world systems,\nhas an \"almost-periodic\" character, i.e., a continuous Fourier spectrum\npunctuated by dominant peaks, which capture regular (e.g., daily or weekly)\nrecurrences in the dynamics. The forecasting method presented takes advantage\nof this property by (i) regressing to a deterministic linear model whose\neigenspectrum maps onto those peaks, and (ii) simultaneously learning a\nstochastic Gaussian process regression (GPR) process to actuate this system.\nOur forecasting algorithm is compared against state-of-the-art forecasting\ntechniques not using additional explanatory variables and is shown to produce\nsuperior performance. Moreover, its use of linear intrinsic dynamics offers a\nnumber of desirable properties in terms of interpretability and parsimony.\nResults are presented for a test case using load data from an electrical grid.\nLoad forecasting is an essential challenge in power systems engineering, with\nmajor implications for real-time control, pricing, maintenance, and security\ndecisions.",
          "link": "http://arxiv.org/abs/2010.04248",
          "publishedOn": "2021-07-13T01:59:35.554Z",
          "wordCount": 661,
          "title": "Stochastically forced ensemble dynamic mode decomposition for forecasting and analysis of near-periodic systems. (arXiv:2010.04248v2 [physics.soc-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1\">Dylan Slack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1\">Sophie Hilgard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As black box explanations are increasingly being employed to establish model\ncredibility in high stakes settings, it is important to ensure that these\nexplanations are accurate and reliable. However, prior work demonstrates that\nexplanations generated by state-of-the-art techniques are inconsistent,\nunstable, and provide very little insight into their correctness and\nreliability. In addition, these methods are also computationally inefficient,\nand require significant hyper-parameter tuning. In this paper, we address the\naforementioned challenges by developing a novel Bayesian framework for\ngenerating local explanations along with their associated uncertainty. We\ninstantiate this framework to obtain Bayesian versions of LIME and KernelSHAP\nwhich output credible intervals for the feature importances, capturing the\nassociated uncertainty. The resulting explanations not only enable us to make\nconcrete inferences about their quality (e.g., there is a 95\\% chance that the\nfeature importance lies within the given range), but are also highly consistent\nand stable. We carry out a detailed theoretical analysis that leverages the\naforementioned uncertainty to estimate how many perturbations to sample, and\nhow to sample for faster convergence. This work makes the first attempt at\naddressing several critical issues with popular explanation methods in one\nshot, thereby generating consistent, stable, and reliable explanations with\nguarantees in a computationally efficient manner. Experimental evaluation with\nmultiple real world datasets and user studies demonstrate that the efficacy of\nthe proposed framework.",
          "link": "http://arxiv.org/abs/2008.05030",
          "publishedOn": "2021-07-13T01:59:35.535Z",
          "wordCount": 696,
          "title": "Reliable Post hoc Explanations: Modeling Uncertainty in Explainability. (arXiv:2008.05030v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martino_L/0/1/0/all/0/1\">Luca Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1\">Jesse Read</a>",
          "description": "The expressive power of Bayesian kernel-based methods has led them to become\nan important tool across many different facets of artificial intelligence, and\nuseful to a plethora of modern application domains, providing both power and\ninterpretability via uncertainty analysis. This article introduces and\ndiscusses two methods which straddle the areas of probabilistic Bayesian\nschemes and kernel methods for regression: Gaussian Processes and Relevance\nVector Machines. Our focus is on developing a common framework with which to\nview these methods, via intermediate methods a probabilistic version of the\nwell-known kernel ridge regression, and drawing connections among them, via\ndual formulations, and discussion of their application in the context of major\ntasks: regression, smoothing, interpolation, and filtering. Overall, we provide\nunderstanding of the mathematical concepts behind these models, and we\nsummarize and discuss in depth different interpretations and highlight the\nrelationship to other methods, such as linear kernel smoothers, Kalman\nfiltering and Fourier approximations. Throughout, we provide numerous figures\nto promote understanding, and we make numerous recommendations to\npractitioners. Benefits and drawbacks of the different techniques are\nhighlighted. To our knowledge, this is the most in-depth study of its kind to\ndate focused on these two methods, and will be relevant to theoretical\nunderstanding and practitioners throughout the domains of data-science, signal\nprocessing, machine learning, and artificial intelligence in general.",
          "link": "http://arxiv.org/abs/2009.09217",
          "publishedOn": "2021-07-13T01:59:35.529Z",
          "wordCount": 740,
          "title": "A Joint introduction to Gaussian Processes and Relevance Vector Machines with Connections to Kalman filtering and other Kernel Smoothers. (arXiv:2009.09217v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ronghang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>",
          "description": "Owing to the remarkable capability of extracting effective graph embeddings,\ngraph convolutional network (GCN) and its variants have been successfully\napplied to a broad range of tasks, such as node classification, link\nprediction, and graph classification. Traditional GCN models suffer from the\nissues of overfitting and oversmoothing, while some recent techniques like\nDropEdge could alleviate these issues and thus enable the development of deep\nGCN. However, training GCN models is non-trivial, as it is sensitive to the\nchoice of hyperparameters such as dropout rate and learning weight decay,\nespecially for deep GCN models. In this paper, we aim to automate the training\nof GCN models through hyperparameter optimization. To be specific, we propose a\nself-tuning GCN approach with an alternate training algorithm, and further\nextend our approach by incorporating the population based training scheme.\nExperimental results on three benchmark datasets demonstrate the effectiveness\nof our approaches on optimizing multi-layer GCN, compared with several\nrepresentative baselines.",
          "link": "http://arxiv.org/abs/2107.04713",
          "publishedOn": "2021-07-13T01:59:35.523Z",
          "wordCount": 600,
          "title": "Automated Graph Learning via Population Based Self-Tuning GCN. (arXiv:2107.04713v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perone_C/0/1/0/all/0/1\">Christian S. Perone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_R/0/1/0/all/0/1\">Roberto Pereira Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paula_T/0/1/0/all/0/1\">Thomas Paula</a>",
          "description": "Uncertainty quantification for deep neural networks has recently evolved\nthrough many techniques. In this work, we revisit Laplace approximation, a\nclassical approach for posterior approximation that is computationally\nattractive. However, instead of computing the curvature matrix, we show that,\nunder some regularity conditions, the Laplace approximation can be easily\nconstructed using the gradient second moment. This quantity is already\nestimated by many exponential moving average variants of Adagrad such as Adam\nand RMSprop, but is traditionally discarded after training. We show that our\nmethod (L2M) does not require changes in models or optimization, can be\nimplemented in a few lines of code to yield reasonable results, and it does not\nrequire any extra computational steps besides what is already being computed by\noptimizers, without introducing any new hyperparameter. We hope our method can\nopen new research directions on using quantities already computed by optimizers\nfor uncertainty estimation in deep neural networks.",
          "link": "http://arxiv.org/abs/2107.04695",
          "publishedOn": "2021-07-13T01:59:35.517Z",
          "wordCount": 601,
          "title": "L2M: Practical posterior Laplace approximation with optimization-driven second moment estimation. (arXiv:2107.04695v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09646",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this paper, we present a novel network for high resolution video\ngeneration. Our network uses ideas from Wasserstein GANs by enforcing\nk-Lipschitz constraint on the loss term and Conditional GANs using class labels\nfor training and testing. We present Generator and Discriminator network\nlayerwise details along with the combined network architecture, optimization\ndetails and algorithm used in this work. Our network uses a combination of two\nloss terms: mean square pixel loss and an adversarial loss. The datasets used\nfor training and testing our network are UCF101, Golf and Aeroplane Datasets.\nUsing Inception Score and Fr\\'echet Inception Distance as the evaluation\nmetrics, our network outperforms previous state of the art networks on\nunsupervised video generation.",
          "link": "http://arxiv.org/abs/2008.09646",
          "publishedOn": "2021-07-13T01:59:35.510Z",
          "wordCount": 596,
          "title": "HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN. (arXiv:2008.09646v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Ju-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>",
          "description": "Recently proposed self-supervised learning approaches have been successful\nfor pre-training speech representation models. The utility of these learned\nrepresentations has been observed empirically, but not much has been studied\nabout the type or extent of information encoded in the pre-trained\nrepresentations themselves. Developing such insights can help understand the\ncapabilities and limits of these models and enable the research community to\nmore efficiently develop their usage for downstream applications. In this work,\nwe begin to fill this gap by examining one recent and successful pre-trained\nmodel (wav2vec 2.0), via its intermediate representation vectors, using a suite\nof analysis tools. We use the metrics of canonical correlation, mutual\ninformation, and performance on simple downstream tasks with non-parametric\nprobes, in order to (i) query for acoustic and linguistic information content,\n(ii) characterize the evolution of information across model layers, and (iii)\nunderstand how fine-tuning the model for automatic speech recognition (ASR)\naffects these observations. Our findings motivate modifying the fine-tuning\nprotocol for ASR, which produces improved word error rates in a low-resource\nsetting.",
          "link": "http://arxiv.org/abs/2107.04734",
          "publishedOn": "2021-07-13T01:59:35.504Z",
          "wordCount": 612,
          "title": "Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1\">Sara Hajj Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1\">Mohamed Nassar</a>",
          "description": "Deep learning is a type of machine learning that adapts a deep hierarchy of\nconcepts. Deep learning classifiers link the most basic version of concepts at\nthe input layer to the most abstract version of concepts at the output layer,\nalso known as a class or label. However, once trained over a finite set of\nclasses, a deep learning model does not have the power to say that a given\ninput does not belong to any of the classes and simply cannot be linked.\nCorrectly invalidating the prediction of unrelated classes is a challenging\nproblem that has been tackled in many ways in the literature. Novelty detection\ngives deep learning the ability to output \"do not know\" for novel/unseen\nclasses. Still, no attention has been given to the security aspects of novelty\ndetection. In this paper, we consider the case study of abstraction-based\nnovelty detection and show that it is not robust against adversarial samples.\nMoreover, we show the feasibility of crafting adversarial samples that fool the\ndeep learning classifier and bypass the novelty detection monitoring at the\nsame time. In other words, these monitoring boxes are hackable. We demonstrate\nthat novelty detection itself ends up as an attack surface.",
          "link": "http://arxiv.org/abs/2107.04764",
          "publishedOn": "2021-07-13T01:59:35.488Z",
          "wordCount": 635,
          "title": "Hack The Box: Fooling Deep Learning Abstraction-Based Monitors. (arXiv:2107.04764v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_G/0/1/0/all/0/1\">Gaurav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>",
          "description": "Generating future frames given a few context (or past) frames is a\nchallenging task. It requires modeling the temporal coherence of videos and\nmulti-modality in terms of diversity in the potential future states. Current\nvariational approaches for video generation tend to marginalize over\nmulti-modal future outcomes. Instead, we propose to explicitly model the\nmulti-modality in the future outcomes and leverage it to sample diverse\nfutures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to\nlearn priors on future states given the past and maintains a probability\ndistribution over possible futures given a particular sample. In addition, we\nleverage the changes in this distribution over time to control the sampling of\ndiverse future states by estimating the end of ongoing sequences. That is, we\nuse the variance of GP over the output function space to trigger a change in an\naction sequence. We achieve state-of-the-art results on diverse future frame\ngeneration in terms of reconstruction quality and diversity of the generated\nsequences.",
          "link": "http://arxiv.org/abs/2107.04619",
          "publishedOn": "2021-07-13T01:59:35.476Z",
          "wordCount": 612,
          "title": "Diverse Video Generation using a Gaussian Process Trigger. (arXiv:2107.04619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sivaprasad_S/0/1/0/all/0/1\">Sarath Sivaprasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ankur Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manwani_N/0/1/0/all/0/1\">Naresh Manwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>",
          "description": "In this paper, we investigate a constrained formulation of neural networks\nwhere the output is a convex function of the input. We show that the convexity\nconstraints can be enforced on both fully connected and convolutional layers,\nmaking them applicable to most architectures. The convexity constraints include\nrestricting the weights (for all but the first layer) to be non-negative and\nusing a non-decreasing convex activation function. Albeit simple, these\nconstraints have profound implications on the generalization abilities of the\nnetwork. We draw three valuable insights: (a) Input Output Convex Neural\nNetworks (IOC-NNs) self regularize and reduce the problem of overfitting; (b)\nAlthough heavily constrained, they outperform the base multi layer perceptrons\nand achieve similar performance as compared to base convolutional architectures\nand (c) IOC-NNs show robustness to noise in train labels. We demonstrate the\nefficacy of the proposed idea using thorough experiments and ablation studies\non standard image classification datasets with three different neural network\narchitectures.",
          "link": "http://arxiv.org/abs/2006.05103",
          "publishedOn": "2021-07-13T01:59:35.464Z",
          "wordCount": 630,
          "title": "The Curious Case of Convex Neural Networks. (arXiv:2006.05103v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacques_B/0/1/0/all/0/1\">Brandon G. Jacques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiganj_Z/0/1/0/all/0/1\">Zoran Tiganj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Aakash Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_M/0/1/0/all/0/1\">Marc W. Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sederberg_P/0/1/0/all/0/1\">Per B. Sederberg</a>",
          "description": "In machine learning, convolutional neural networks (CNNs) have been extremely\ninfluential in both computer vision and in recognizing patterns extended over\ntime. In computer vision, part of the flexibility arises from the use of\nmax-pooling operations over the convolutions to attain translation invariance.\nIn the mammalian brain, neural representations of time use a set of temporal\nbasis functions. Critically, these basis functions appear to be arranged in a\ngeometric series such that the basis set is evenly distributed over logarithmic\ntime. This paper introduces a Scale-Invariant Temporal History Convolution\nnetwork (SITHCon) that uses a logarithmically-distributed temporal memory. A\nmax-pool over a logarithmically-distributed temporal memory results in\nscale-invariance in time. We compare performance of SITHCon to a Temporal\nConvolution Network (TCN) and demonstrate that, although both networks can\nlearn classification and regression problems on both univariate and\nmultivariate time series $f(t)$, only SITHCon has the property that it\ngeneralizes without retraining to rescaled versions of the input $f(at)$. This\nproperty, inspired by findings from neuroscience and psychology, could lead to\nlarge-scale networks with dramatically different capabilities, including faster\ntraining and greater generalizability, even with significantly fewer free\nparameters.",
          "link": "http://arxiv.org/abs/2107.04616",
          "publishedOn": "2021-07-13T01:59:35.450Z",
          "wordCount": 634,
          "title": "SITHCon: A neural network robust to variations in input scaling on the time dimension. (arXiv:2107.04616v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assaad_S/0/1/0/all/0/1\">Serge Assaad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shuxi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Henry Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>",
          "description": "We examine interval estimation of the effect of a treatment T on an outcome Y\ngiven the existence of an unobserved confounder U. Using H\\\"older's inequality,\nwe derive a set of bounds on the confounding bias |E[Y|T=t]-E[Y|do(T=t)]| based\non the degree of unmeasured confounding (i.e., the strength of the connection\nU->T, and the strength of U->Y). These bounds are tight either when U is\nindependent of T or when U is independent of Y given T (when there is no\nunobserved confounding). We focus on a special case of this bound depending on\nthe total variation distance between the distributions p(U) and p(U|T=t), as\nwell as the maximum (over all possible values of U) deviation of the\nconditional expected outcome E[Y|U=u,T=t] from the average expected outcome\nE[Y|T=t]. We discuss possible calibration strategies for this bound to get\ninterval estimates for treatment effects, and experimentally validate the bound\nusing synthetic and semi-synthetic datasets.",
          "link": "http://arxiv.org/abs/2107.04661",
          "publishedOn": "2021-07-13T01:59:35.443Z",
          "wordCount": 607,
          "title": "H\\\"older Bounds for Sensitivity Analysis in Causal Reasoning. (arXiv:2107.04661v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pareek_D/0/1/0/all/0/1\">Divyansh Pareek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>",
          "description": "Training and using modern neural-network based latent-variable generative\nmodels (like Variational Autoencoders) often require simultaneously training a\ngenerative direction along with an inferential(encoding) direction, which\napproximates the posterior distribution over the latent variables. Thus, the\nquestion arises: how complex does the inferential model need to be, in order to\nbe able to accurately model the posterior distribution of a given generative\nmodel?\n\nIn this paper, we identify an important property of the generative map\nimpacting the required size of the encoder. We show that if the generative map\nis \"strongly invertible\" (in a sense we suitably formalize), the inferential\nmodel need not be much more complex. Conversely, we prove that there exist\nnon-invertible generative maps, for which the encoding direction needs to be\nexponentially larger (under standard assumptions in computational complexity).\nImportantly, we do not require the generative model to be layerwise invertible,\nwhich a lot of the related literature assumes and isn't satisfied by many\narchitectures used in practice (e.g. convolution and pooling based networks).\nThus, we provide theoretical support for the empirical wisdom that learning\ndeep generative models is harder when data lies on a low-dimensional manifold.",
          "link": "http://arxiv.org/abs/2107.04652",
          "publishedOn": "2021-07-13T01:59:35.437Z",
          "wordCount": 634,
          "title": "The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders. (arXiv:2107.04652v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05222",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sreeram_A/0/1/0/all/0/1\">Anirudh Sreeram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehlman_N/0/1/0/all/0/1\">Nicholas Mehlman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peri_R/0/1/0/all/0/1\">Raghuveer Peri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knox_D/0/1/0/all/0/1\">Dillon Knox</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>",
          "description": "In this paper we investigate speech denoising as a defense against\nadversarial attacks on automatic speech recognition (ASR) systems. Adversarial\nattacks attempt to force misclassification by adding small perturbations to the\noriginal speech signal. We propose to counteract this by employing a\nneural-network based denoiser as a pre-processor in the ASR pipeline. The\ndenoiser is independent of the downstream ASR model, and thus can be rapidly\ndeployed in existing systems. We found that training the denoisier using a\nperceptually motivated loss function resulted in increased adversarial\nrobustness without compromising ASR performance on benign samples. Our defense\nwas evaluated (as a part of the DARPA GARD program) on the 'Kenansville' attack\nstrategy across a range of attack strengths and speech samples. An average\nimprovement in Word Error Rate (WER) of about 7.7% was observed over the\nundefended model at 20 dB signal-to-noise-ratio (SNR) attack strength.",
          "link": "http://arxiv.org/abs/2107.05222",
          "publishedOn": "2021-07-13T01:59:35.422Z",
          "wordCount": 608,
          "title": "Perceptual-based deep-learning denoiser as a defense against adversarial attacks on ASR systems. (arXiv:2107.05222v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "Learning disentangled and interpretable representations is an important step\ntowards accomplishing comprehensive data representations on the manifold. In\nthis paper, we propose a novel representation learning algorithm which combines\nthe inference abilities of Variational Autoencoders (VAE) with the\ngeneralization capability of Generative Adversarial Networks (GAN). The\nproposed model, called InfoVAEGAN, consists of three networks~: Encoder,\nGenerator and Discriminator. InfoVAEGAN aims to jointly learn discrete and\ncontinuous interpretable representations in an unsupervised manner by using two\ndifferent data-free log-likelihood functions onto the variables sampled from\nthe generator's distribution. We propose a two-stage algorithm for optimizing\nthe inference network separately from the generator training. Moreover, we\nenforce the learning of interpretable representations through the maximization\nof the mutual information between the existing latent variables and those\ncreated through generative and inference processes.",
          "link": "http://arxiv.org/abs/2107.04705",
          "publishedOn": "2021-07-13T01:59:35.400Z",
          "wordCount": 585,
          "title": "InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood. (arXiv:2107.04705v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1\">Alvaro Velasquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beckus_A/0/1/0/all/0/1\">Andre Beckus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohmen_T/0/1/0/all/0/1\">Taylor Dohmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Ashutosh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topper_N/0/1/0/all/0/1\">Noah Topper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1\">George Atia</a>",
          "description": "The success of reinforcement learning in typical settings is, in part,\npredicated on underlying Markovian assumptions on the reward signal by which an\nagent learns optimal policies. In recent years, the use of reward machines has\nrelaxed this assumption by enabling a structured representation of\nnon-Markovian rewards. In particular, such representations can be used to\naugment the state space of the underlying decision process, thereby\nfacilitating non-Markovian reinforcement learning. However, these reward\nmachines cannot capture the semantics of stochastic reward signals. In this\npaper, we make progress on this front by introducing probabilistic reward\nmachines (PRMs) as a representation of non-Markovian stochastic rewards. We\npresent an algorithm to learn PRMs from the underlying decision process as well\nas to learn the PRM representation of a given decision-making policy.",
          "link": "http://arxiv.org/abs/2107.04633",
          "publishedOn": "2021-07-13T01:59:35.381Z",
          "wordCount": 574,
          "title": "Learning Probabilistic Reward Machines from Non-Markovian Stochastic Reward Processes. (arXiv:2107.04633v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazzine_R/0/1/0/all/0/1\">Raphael Mazzine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martens_D/0/1/0/all/0/1\">David Martens</a>",
          "description": "Counterfactual explanations are viewed as an effective way to explain machine\nlearning predictions. This interest is reflected by a relatively young\nliterature with already dozens of algorithms aiming to generate such\nexplanations. These algorithms are focused on finding how features can be\nmodified to change the output classification. However, this rather general\nobjective can be achieved in different ways, which brings about the need for a\nmethodology to test and benchmark these algorithms. The contributions of this\nwork are manifold: First, a large benchmarking study of 10 algorithmic\napproaches on 22 tabular datasets is performed, using 9 relevant evaluation\nmetrics. Second, the introduction of a novel, first of its kind, framework to\ntest counterfactual generation algorithms. Third, a set of objective metrics to\nevaluate and compare counterfactual results. And finally, insight from the\nbenchmarking results that indicate which approaches obtain the best performance\non what type of dataset. This benchmarking study and framework can help\npractitioners in determining which technique and building blocks most suit\ntheir context, and can help researchers in the design and evaluation of current\nand future counterfactual generation algorithms. Our findings show that,\noverall, there's no single best algorithm to generate counterfactual\nexplanations as the performance highly depends on properties related to the\ndataset, model, score and factual point specificities.",
          "link": "http://arxiv.org/abs/2107.04680",
          "publishedOn": "2021-07-13T01:59:35.321Z",
          "wordCount": 651,
          "title": "A Framework and Benchmarking Study for Counterfactual Generating Methods on Tabular Data. (arXiv:2107.04680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "In this paper, we propose an end-to-end lifelong learning mixture of experts.\nEach expert is implemented by a Variational Autoencoder (VAE). The experts in\nthe mixture system are jointly trained by maximizing a mixture of individual\ncomponent evidence lower bounds (MELBO) on the log-likelihood of the given\ntraining samples. The mixing coefficients in the mixture, control the\ncontributions of each expert in the goal representation. These are sampled from\na Dirichlet distribution whose parameters are determined through non-parametric\nestimation during lifelong learning. The model can learn new tasks fast when\nthese are similar to those previously learnt. The proposed Lifelong mixture of\nVAE (L-MVAE) expands its architecture with new components when learning a\ncompletely new task. After the training, our model can automatically determine\nthe relevant expert to be used when fed with new data samples. This mechanism\nbenefits both the memory efficiency and the required computational cost as only\none expert is used during the inference. The L-MVAE inference model is able to\nperform interpolation in the joint latent space across the data domains\nassociated with different tasks and is shown to be efficient for disentangled\nlearning representation.",
          "link": "http://arxiv.org/abs/2107.04694",
          "publishedOn": "2021-07-13T01:59:35.269Z",
          "wordCount": 631,
          "title": "Lifelong Mixture of Variational Autoencoders. (arXiv:2107.04694v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1\">Dimitrios Vytiniotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1\">Grzegorz Swirszcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1\">Viorica Patraucean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.",
          "link": "http://arxiv.org/abs/2106.08318",
          "publishedOn": "2021-07-13T01:59:34.776Z",
          "wordCount": 668,
          "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seely_J/0/1/0/all/0/1\">Jeffrey Seely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1\">Awni Hannun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1\">Nicolas Usunier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>",
          "description": "Machine learning systems typically assume that the distributions of training\nand test sets match closely. However, a critical requirement of such systems in\nthe real world is their ability to generalize to unseen domains. Here, we\npropose an inter-domain gradient matching objective that targets domain\ngeneralization by maximizing the inner product between gradients from different\ndomains. Since direct optimization of the gradient inner product can be\ncomputationally prohibitive -- requires computation of second-order derivatives\n-- we derive a simpler first-order algorithm named Fish that approximates its\noptimization. We demonstrate the efficacy of Fish on 6 datasets from the Wilds\nbenchmark, which captures distribution shift across a diverse range of\nmodalities. Our method produces competitive results on these datasets and\nsurpasses all baselines on 4 of them. We perform experiments on both the Wilds\nbenchmark, which captures distribution shift in the real world, as well as\ndatasets in DomainBed benchmark that focuses more on synthetic-to-real\ntransfer. Our method produces competitive results on both benchmarks,\ndemonstrating its effectiveness across a wide range of domain generalization\ntasks.",
          "link": "http://arxiv.org/abs/2104.09937",
          "publishedOn": "2021-07-13T01:59:34.764Z",
          "wordCount": 641,
          "title": "Gradient Matching for Domain Generalization. (arXiv:2104.09937v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1\">Pratik Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1\">Christos Davatzikos</a>",
          "description": "Heterogeneity in medical data, e.g., from data collected at different sites\nand with different protocols in a clinical study, is a fundamental hurdle for\naccurate prediction using machine learning models, as such models often fail to\ngeneralize well. This paper leverages a recently proposed\nnormalizing-flow-based method to perform counterfactual inference upon a\nstructural causal model (SCM), in order to achieve harmonization of such data.\nA causal model is used to model observed effects (brain magnetic resonance\nimaging data) that result from known confounders (site, gender and age) and\nexogenous noise variables. Our formulation exploits the bijection induced by\nflow for the purpose of harmonization. We infer the posterior of exogenous\nvariables, intervene on observations, and draw samples from the resultant SCM\nto obtain counterfactuals. This approach is evaluated extensively on multiple,\nlarge, real-world medical datasets and displayed better cross-domain\ngeneralization compared to state-of-the-art algorithms. Further experiments\nthat evaluate the quality of confounder-independent data generated by our model\nusing regression and classification tasks are provided.",
          "link": "http://arxiv.org/abs/2106.06845",
          "publishedOn": "2021-07-13T01:59:34.758Z",
          "wordCount": 624,
          "title": "Harmonization with Flow-based Causal Inference. (arXiv:2106.06845v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14323",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Deziderio_N/0/1/0/all/0/1\">Nathalie Deziderio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carvalho_H/0/1/0/all/0/1\">Hugo Tremonte de Carvalho</a>",
          "description": "This work was developed aiming to employ Statistical techniques to the field\nof Music Emotion Recognition, a well-recognized area within the Signal\nProcessing world, but hardly explored from the statistical point of view. Here,\nwe opened several possibilities within the field, applying modern Bayesian\nStatistics techniques and developing efficient algorithms, focusing on the\napplicability of the results obtained. Although the motivation for this project\nwas the development of a emotion-based music recommendation system, its main\ncontribution is a highly adaptable multivariate model that can be useful\ninterpreting any database where there is an interest in applying regularization\nin an efficient manner. Broadly speaking, we will explore what role a sound\ntheoretical statistical analysis can play in the modeling of an algorithm that\nis able to understand a well-known database and what can be gained with this\nkind of approach.",
          "link": "http://arxiv.org/abs/2106.14323",
          "publishedOn": "2021-07-13T01:59:34.722Z",
          "wordCount": 597,
          "title": "Use of Variational Inference in Music Emotion Recognition. (arXiv:2106.14323v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malviya_P/0/1/0/all/0/1\">Pranshu Malviya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1\">Balaraman Ravindran</a>",
          "description": "When an agent encounters a continual stream of new tasks in the lifelong\nlearning setting, it leverages the knowledge it gained from the earlier tasks\nto help learn the new tasks better. In such a scenario, identifying an\nefficient knowledge representation becomes a challenging problem. Most research\nworks propose to either store a subset of examples from the past tasks in a\nreplay buffer, dedicate a separate set of parameters to each task or penalize\nexcessive updates over parameters by introducing a regularization term. While\nexisting methods employ the general task-agnostic stochastic gradient descent\nupdate rule, we propose a task-aware optimizer that adapts the learning rate\nbased on the relatedness among tasks. We utilize the directions taken by the\nparameters during the updates by accumulating the gradients specific to each\ntask. These task-based accumulated gradients act as a knowledge base that is\nmaintained and updated throughout the stream. We empirically show that our\nproposed adaptive learning rate not only accounts for catastrophic forgetting\nbut also allows positive backward transfer. We also show that our method\nperforms better than several state-of-the-art methods in lifelong learning on\ncomplex datasets with a large number of tasks.",
          "link": "http://arxiv.org/abs/2105.05155",
          "publishedOn": "2021-07-13T01:59:34.699Z",
          "wordCount": 647,
          "title": "TAG: Task-based Accumulated Gradients for Lifelong learning. (arXiv:2105.05155v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1\">Sebastian Monka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1\">Lavdim Halilaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1\">Stefan Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>",
          "description": "Traditional computer vision approaches, based on neural networks (NN), are\ntypically trained on a large amount of image data. By minimizing the\ncross-entropy loss between a prediction and a given class label, the NN and its\nvisual embedding space are learned to fulfill a given task. However, due to the\nsole dependence on the image data distribution of the training domain, these\nmodels tend to fail when applied to a target domain that differs from their\nsource domain. To learn a more robust NN to domain shifts, we propose the\nknowledge graph neural network (KG-NN), a neuro-symbolic approach that\nsupervises the training using image-data-invariant auxiliary knowledge. The\nauxiliary knowledge is first encoded in a knowledge graph with respective\nconcepts and their relationships, which is then transformed into a dense vector\nrepresentation via an embedding method. Using a contrastive loss function,\nKG-NN learns to adapt its visual embedding space and thus its weights according\nto the image-data invariant knowledge graph embedding space. We evaluate KG-NN\non visual transfer learning tasks for classification using the mini-ImageNet\ndataset and its derivatives, as well as road sign recognition datasets from\nGermany and China. The results show that a visual model trained with a\nknowledge graph as a trainer outperforms a model trained with cross-entropy in\nall experiments, in particular when the domain gap increases. Besides better\nperformance and stronger robustness to domain shifts, these KG-NN adapts to\nmultiple datasets and classes without suffering heavily from catastrophic\nforgetting.",
          "link": "http://arxiv.org/abs/2102.08747",
          "publishedOn": "2021-07-13T01:59:34.686Z",
          "wordCount": 728,
          "title": "Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1\">Sobhan Miryoosefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>",
          "description": "In constrained reinforcement learning (RL), a learning agent seeks to not\nonly optimize the overall reward but also satisfy the additional safety,\ndiversity, or budget constraints. Consequently, existing constrained RL\nsolutions require several new algorithmic ingredients that are notably\ndifferent from standard RL. On the other hand, reward-free RL is independently\ndeveloped in the unconstrained literature, which learns the transition dynamics\nwithout using the reward information, and thus naturally capable of addressing\nRL with multiple objectives under the common dynamics. This paper bridges\nreward-free RL and constrained RL. Particularly, we propose a simple\nmeta-algorithm such that given any reward-free RL oracle, the approachability\nand constrained RL problems can be directly solved with negligible overheads in\nsample complexity. Utilizing the existing reward-free RL solvers, our framework\nprovides sharp sample complexity results for constrained RL in the tabular MDP\nsetting, matching the best existing results up to a factor of horizon\ndependence; our framework directly extends to a setting of tabular two-player\nMarkov games, and gives a new result for constrained RL with linear function\napproximation.",
          "link": "http://arxiv.org/abs/2107.05216",
          "publishedOn": "2021-07-13T01:59:34.680Z",
          "wordCount": 605,
          "title": "A Simple Reward-free Approach to Constrained Reinforcement Learning. (arXiv:2107.05216v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1\">Lirong Xia</a>",
          "description": "Differential privacy (DP) is a widely-accepted and widely-applied notion of\nprivacy based on worst-case analysis. Often, DP classifies most mechanisms\nwithout external noise as non-private [Dwork et al., 2014], and external\nnoises, such as Gaussian noise or Laplacian noise [Dwork et al., 2006], are\nintroduced to improve privacy. In many real-world applications, however, adding\nexternal noise is undesirable and sometimes prohibited. For example,\npresidential elections often require a deterministic rule to be used [Liu et\nal., 2020], and small noises can lead to dramatic decreases in the prediction\naccuracy of deep neural networks, especially the underrepresented classes\n[Bagdasaryan et al., 2019].\n\nIn this paper, we propose a natural extension and relaxation of DP following\nthe worst average-case idea behind the celebrated smoothed analysis [Spielman\nand Teng, 2004]. Our notion, the smoothed DP, can effectively measure the\nprivacy leakage of mechanisms without external noises under realistic settings.\n\nWe prove several strong properties of the smoothed DP, including\ncomposability, robustness to post-processing and etc. We proved that any\ndiscrete mechanism with sampling procedures is more private than what DP\npredicts. In comparison, many continuous mechanisms with sampling procedures\nare still non-private under smoothed DP. Experimentally, we first verified that\nthe discrete sampling mechanisms are private in real-world elections. Then, we\napply the smoothed DP notion on quantized gradient descent, which indicates\nsome neural networks can be private without adding any extra noises. We believe\nthat these results contribute to the theoretical foundation of realistic\nprivacy measures beyond worst-case analysis.",
          "link": "http://arxiv.org/abs/2107.01559",
          "publishedOn": "2021-07-13T01:59:34.674Z",
          "wordCount": 693,
          "title": "Smoothed Differential Privacy. (arXiv:2107.01559v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1\">Michael Moor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennet_N/0/1/0/all/0/1\">Nicolas Bennet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plecko_D/0/1/0/all/0/1\">Drago Plecko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1\">Max Horn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1\">Bastian Rieck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinshausen_N/0/1/0/all/0/1\">Nicolai Meinshausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhlmann_P/0/1/0/all/0/1\">Peter B&#xfc;hlmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1\">Karsten Borgwardt</a>",
          "description": "Despite decades of clinical research, sepsis remains a global public health\ncrisis with high mortality, and morbidity. Currently, when sepsis is detected\nand the underlying pathogen is identified, organ damage may have already\nprogressed to irreversible stages. Effective sepsis management is therefore\nhighly time-sensitive. By systematically analysing trends in the plethora of\nclinical data available in the intensive care unit (ICU), an early prediction\nof sepsis could lead to earlier pathogen identification, resistance testing,\nand effective antibiotic and supportive treatment, and thereby become a\nlife-saving measure. Here, we developed and validated a machine learning (ML)\nsystem for the prediction of sepsis in the ICU. Our analysis represents the\nlargest multi-national, multi-centre in-ICU study for sepsis prediction using\nML to date. Our dataset contains $156,309$ unique ICU admissions, which\nrepresent a refined and harmonised subset of five large ICU databases\noriginating from three countries. Using the international consensus definition\nSepsis-3, we derived hourly-resolved sepsis label annotations, amounting to\n$26,734$ ($17.1\\%$) septic stays. We compared our approach, a deep\nself-attention model, to several clinical baselines as well as ML baselines and\nperformed an extensive internal and external validation within and across\ndatabases. On average, our model was able to predict sepsis with an AUROC of\n$0.847 \\pm 0.050$ (internal out-of sample validation) and $0.761 \\pm 0.052$\n(external validation). For a harmonised prevalence of $17\\%$, at $80\\%$ recall\nour model detects septic patients with $39\\%$ precision 3.7 hours in advance.",
          "link": "http://arxiv.org/abs/2107.05230",
          "publishedOn": "2021-07-13T01:59:34.640Z",
          "wordCount": 681,
          "title": "Predicting sepsis in multi-site, multi-national intensive care cohorts using deep learning. (arXiv:2107.05230v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}