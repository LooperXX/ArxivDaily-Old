{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.07148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiyan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenming Xiao</a>",
          "description": "Lexicon information and pre-trained models, such as BERT, have been combined\nto explore Chinese sequence labelling tasks due to their respective strengths.\nHowever, existing methods solely fuse lexicon features via a shallow and random\ninitialized sequence layer and do not integrate them into the bottom layers of\nBERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese\nsequence labelling, which integrates external lexicon knowledge into BERT\nlayers directly by a Lexicon Adapter layer. Compared with the existing methods,\nour model facilitates deep lexicon knowledge fusion at the lower layers of\nBERT. Experiments on ten Chinese datasets of three tasks including Named Entity\nRecognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT\nachieves the state-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.07148",
          "publishedOn": "2021-05-23T06:08:17.061Z",
          "wordCount": 566,
          "title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter. (arXiv:2105.07148v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>",
          "description": "Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of available\nEnglish datasets by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in low-resource languages. We project\npredictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,\nSpanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in\nTRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in\nOffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513\nF1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our\napproach compares favourably to the best systems submitted to recent shared\ntasks on these three languages. Additionally, we report competitive performance\non Arabic, and Turkish using the training and development sets of OffensEval\n2020 shared task. The results for all languages confirm the robustness of\ncross-lingual contextual embeddings and transfer learning for this task.",
          "link": "http://arxiv.org/abs/2105.05996",
          "publishedOn": "2021-05-23T06:08:17.026Z",
          "wordCount": 695,
          "title": "Multilingual Offensive Language Identification for Low-resource Languages. (arXiv:2105.05996v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1\">Grusha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>",
          "description": "When language models process syntactically complex sentences, do they use\nabstract syntactic information present in these sentences in a manner that is\nconsistent with the grammar of English, or do they rely solely on a set of\nheuristics? We propose a method to tackle this question, AlterRep. For any\nlinguistic feature in the sentence, AlterRep allows us to generate\ncounterfactual representations by altering how this feature is encoded, while\nleaving all other aspects of the original representation intact. Then, by\nmeasuring the change in a models' word prediction with these counterfactual\nrepresentations in different sentences, we can draw causal conclusions about\nthe contexts in which the model uses the linguistic feature (if any). Applying\nthis method to study how BERT uses relative clause (RC) span information, we\nfound that BERT uses information about RC spans during agreement prediction\nusing the linguistically correct strategy. We also found that counterfactual\nrepresentations generated for a specific RC subtype influenced the number\nprediction in sentences with other RC subtypes, suggesting that information\nabout RC boundaries was encoded abstractly in BERT's representation.",
          "link": "http://arxiv.org/abs/2105.06965",
          "publishedOn": "2021-05-23T06:08:16.967Z",
          "wordCount": 645,
          "title": "Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. (arXiv:2105.06965v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>",
          "description": "This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge\nTransfer), a novel method for the automatic transfer of explanatory knowledge\nthrough neural encoding mechanisms. We demonstrate that N-XKT is able to\nimprove accuracy and generalization on science Question Answering (QA).\nSpecifically, by leveraging facts from background explanatory knowledge\ncorpora, the N-XKT model shows a clear improvement on zero-shot QA.\nFurthermore, we show that N-XKT can be fine-tuned on a target QA dataset,\nenabling faster convergence and more accurate results. A systematic analysis is\nconducted to quantitatively analyze the performance of the N-XKT model and the\nimpact of different categories of knowledge on the zero-shot generalization\ntask.",
          "link": "http://arxiv.org/abs/2105.05737",
          "publishedOn": "2021-05-23T06:08:16.959Z",
          "wordCount": 554,
          "title": "Encoding Explanatory Knowledge for Zero-shot Science Question Answering. (arXiv:2105.05737v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Ed Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the accuracy of popular NAR models adopted in neural machine\ntranslation by a large margin.",
          "link": "http://arxiv.org/abs/2105.03842",
          "publishedOn": "2021-05-23T06:08:16.945Z",
          "wordCount": 751,
          "title": "FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_R/0/1/0/all/0/1\">Rihao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>",
          "description": "Learning prerequisite chains is an essential task for efficiently acquiring\nknowledge in both known and unknown domains. For example, one may be an expert\nin the natural language processing (NLP) domain but want to determine the best\norder to learn new concepts in an unfamiliar Computer Vision domain (CV). Both\ndomains share some common concepts, such as machine learning basics and deep\nlearning models. In this paper, we propose unsupervised cross-domain concept\nprerequisite chain learning using an optimized variational graph autoencoder.\nOur model learns to transfer concept prerequisite relations from an\ninformation-rich domain (source domain) to an information-poor domain (target\ndomain), substantially surpassing other baseline models. Also, we expand an\nexisting dataset by introducing two new domains: CV and Bioinformatics (BIO).\nThe annotated data and resources, as well as the code, will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2105.03505",
          "publishedOn": "2021-05-23T06:08:16.936Z",
          "wordCount": 588,
          "title": "Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders. (arXiv:2105.03505v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_J/0/1/0/all/0/1\">Jun Araki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Haibo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Recent works have shown that language models (LM) capture different types of\nknowledge regarding facts or common sense. However, because no model is\nperfect, they still fail to provide appropriate answers in many cases. In this\npaper, we ask the question \"how can we know when language models know, with\nconfidence, the answer to a particular query?\" We examine this question from\nthe point of view of calibration, the property of a probabilistic model's\npredicted probabilities actually being well correlated with the probabilities\nof correctness. We examine three strong generative models -- T5, BART, and\nGPT-2 -- and study whether their probabilities on QA tasks are well calibrated,\nfinding the answer is a relatively emphatic no. We then examine methods to\ncalibrate such models to make their confidence scores correlate better with the\nlikelihood of correctness through fine-tuning, post-hoc probability\nmodification, or adjustment of the predicted outputs or inputs. Experiments on\na diverse range of datasets demonstrate the effectiveness of our methods. We\nalso perform analysis to study the strengths and limitations of these methods,\nshedding light on further improvements that may be made in methods for\ncalibrating LMs. We have released the code at\nhttps://github.com/jzbjyb/lm-calibration.",
          "link": "http://arxiv.org/abs/2012.00955",
          "publishedOn": "2021-05-23T06:08:16.916Z",
          "wordCount": 672,
          "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering. (arXiv:2012.00955v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Can Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guocheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Existed pre-training methods either focus on single-modal tasks or\nmulti-modal tasks, and cannot effectively adapt to each other. They can only\nutilize single-modal data (i.e. text or image) or limited multi-modal data\n(i.e. image-text pairs). In this work, we propose a unified-modal pre-training\narchitecture, namely UNIMO, which can effectively adapt to both single-modal\nand multi-modal understanding and generation tasks. Large scale of free text\ncorpus and image collections can be utilized to improve the capability of\nvisual and textual understanding, and cross-modal contrastive learning (CMCL)\nis leveraged to align the textual and visual information into a unified\nsemantic space over a corpus of image-text pairs. As the non-paired\nsingle-modal data is very rich, our model can utilize much larger scale of data\nto learn more generalizable representations. Moreover, the textual knowledge\nand visual knowledge can enhance each other in the unified semantic space. The\nexperimental results show that UNIMO significantly improves the performance of\nseveral single-modal and multi-modal downstream tasks. Our code and pre-trained\nmodels are public at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO",
          "link": "http://arxiv.org/abs/2012.15409",
          "publishedOn": "2021-05-23T06:08:16.907Z",
          "wordCount": 665,
          "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. (arXiv:2012.15409v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1\">Dheeraj Pailla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Himanshu Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1\">Aadilmehdi Sanchawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>",
          "description": "The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.",
          "link": "http://arxiv.org/abs/2104.05947",
          "publishedOn": "2021-05-23T06:08:16.899Z",
          "wordCount": 668,
          "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1\">Rishi Hazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_S/0/1/0/all/0/1\">Sonu Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1\">Sayambhu Sen</a>",
          "description": "Human language has been described as a system that makes \\textit{use of\nfinite means to express an unlimited array of thoughts}. Of particular interest\nis the aspect of compositionality, whereby, the meaning of a compound language\nexpression can be deduced from the meaning of its constituent parts. If\nartificial agents can develop compositional communication protocols akin to\nhuman language, they can be made to seamlessly generalize to unseen\ncombinations. However, the real question is, how do we induce compositionality\nin emergent communication? Studies have recognized the role of curiosity in\nenabling linguistic development in children. It is this same intrinsic urge\nthat drives us to master complex tasks with decreasing amounts of explicit\nreward. In this paper, we seek to use this intrinsic feedback in inducing a\nsystematic and unambiguous protolanguage in artificial agents. We show how\nthese rewards can be leveraged in training agents to induce compositionality in\nabsence of any external feedback. Additionally, we introduce gComm, an\nenvironment for investigating grounded language acquisition in 2D-grid\nenvironments. Using this, we demonstrate how compositionality can enable agents\nto not only interact with unseen objects but also transfer skills from one task\nto another in a zero-shot setting: \\textit{Can an agent, trained to `pull' and\n`push twice', `pull twice'?}.",
          "link": "http://arxiv.org/abs/2012.05011",
          "publishedOn": "2021-05-23T06:08:16.830Z",
          "wordCount": 674,
          "title": "Infinite use of finite means: Zero-Shot Generalization using Compositional Emergent Protocols. (arXiv:2012.05011v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resabal_J/0/1/0/all/0/1\">Jose Kristian Resabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">James Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1\">Dan John Velasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Transformers represent the state-of-the-art in Natural Language Processing\n(NLP) in recent years, proving effective even in tasks done in low-resource\nlanguages. While pretrained transformers for these languages can be made, it is\nchallenging to measure their true performance and capacity due to the lack of\nhard benchmark datasets, as well as the difficulty and cost of producing them.\nIn this paper, we present three contributions: First, we propose a methodology\nfor automatically producing Natural Language Inference (NLI) benchmark datasets\nfor low-resource languages using published news articles. Through this, we\ncreate and release NewsPH-NLI, the first sentence entailment benchmark dataset\nin the low-resource Filipino language. Second, we produce new pretrained\ntransformers based on the ELECTRA technique to further alleviate the resource\nscarcity in Filipino, benchmarking them on our dataset against other\ncommonly-used transfer learning techniques. Lastly, we perform analyses on\ntransfer learning techniques to shed light on their true performance when\noperating in low-data domains through the use of degradation tests.",
          "link": "http://arxiv.org/abs/2010.11574",
          "publishedOn": "2021-05-23T06:08:16.822Z",
          "wordCount": 649,
          "title": "Exploiting News Article Structure for Automatic Corpus Generation. (arXiv:2010.11574v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Akul Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1\">Ethan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puranik_S/0/1/0/all/0/1\">Samir Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Horace He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "While programming is one of the most broadly applicable skills in modern\nsociety, modern machine learning models still cannot code solutions to basic\nproblems. It can be difficult to accurately assess code generation performance,\nand there has been surprisingly little work on evaluating code generation in a\nway that is both flexible and rigorous. To meet this challenge, we introduce\nAPPS, a benchmark for code generation. Unlike prior work in more restricted\nsettings, our benchmark measures the ability of models to take an arbitrary\nnatural language specification and generate Python code fulfilling this\nspecification. Similar to how companies assess candidate software developers,\nwe then evaluate models by checking their generated code on test cases. Our\nbenchmark includes 10,000 problems, which range from having simple one-line\nsolutions to being substantial algorithmic challenges. We fine-tune large\nlanguage models on both GitHub and our training set, and we find that the\nprevalence of syntax errors is decreasing exponentially. Recent models such as\nGPT-Neo can pass approximately 15% of the test cases of introductory problems,\nso we find that machine learning models are beginning to learn how to code. As\nthe social significance of automatic code generation increases over the coming\nyears, our benchmark can provide an important measure for tracking\nadvancements.",
          "link": "http://arxiv.org/abs/2105.09938",
          "publishedOn": "2021-05-23T06:08:16.792Z",
          "wordCount": 662,
          "title": "Measuring Coding Challenge Competence With APPS. (arXiv:2105.09938v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2006.16362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cordonnier_J/0/1/0/all/0/1\">Jean-Baptiste Cordonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1\">Andreas Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Attention layers are widely used in natural language processing (NLP) and are\nbeginning to influence computer vision architectures. Training very large\ntransformer models allowed significant improvement in both fields, but once\ntrained, these networks show symptoms of over-parameterization. For instance,\nit is known that many attention heads can be pruned without impacting accuracy.\nThis work aims to enhance current understanding on how multiple heads interact.\nMotivated by the observation that attention heads learn redundant key/query\nprojections, we propose a collaborative multi-head attention layer that enables\nheads to learn shared projections. Our scheme decreases the number of\nparameters in an attention layer and can be used as a drop-in replacement in\nany transformer architecture. Our experiments confirm that sharing key/query\ndimensions can be exploited in language understanding, machine translation and\nvision. We also show that it is possible to re-parametrize a pre-trained\nmulti-head attention layer into our collaborative attention layer.\nCollaborative multi-head attention reduces the size of the key and query\nprojections by 4 for same accuracy and speed. Our code is public.",
          "link": "http://arxiv.org/abs/2006.16362",
          "publishedOn": "2021-05-23T06:08:16.782Z",
          "wordCount": 628,
          "title": "Multi-Head Attention: Collaborate Instead of Concatenate. (arXiv:2006.16362v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1\">George Michalopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaka_H/0/1/0/all/0/1\">Hussam Kaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Helen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have\nachieved state-of-the-art results in biomedical natural language processing\ntasks by focusing their pre-training process on domain-specific corpora.\nHowever, such models do not take into consideration expert domain knowledge.\n\nIn this work, we introduced UmlsBERT, a contextual embedding model that\nintegrates domain knowledge during the pre-training process via a novel\nknowledge augmentation strategy. More specifically, the augmentation on\nUmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was\nperformed in two ways: i) connecting words that have the same underlying\n`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to\ncreate clinically meaningful input embeddings. By applying these two\nstrategies, UmlsBERT can encode clinical domain knowledge into word embeddings\nand outperform existing domain-specific models on common named-entity\nrecognition (NER) and clinical natural language inference clinical NLP tasks.",
          "link": "http://arxiv.org/abs/2010.10391",
          "publishedOn": "2021-05-23T06:08:16.766Z",
          "wordCount": 642,
          "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus. (arXiv:2010.10391v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>",
          "description": "NLP research has attained high performances in abusive language detection as\na supervised classification task. While in research settings, training and test\ndatasets are usually obtained from similar data samples, in practice systems\nare often applied on data that are different from the training set in topic and\nclass distributions. Also, the ambiguity in class definitions inherited in this\ntask aggravates the discrepancies between source and target datasets. We\nexplore the topic bias and the task formulation bias in cross-dataset\ngeneralization. We show that the benign examples in the Wikipedia Detox dataset\nare biased towards platform-specific topics. We identify these examples using\nunsupervised topic modeling and manual inspection of topics' keywords. Removing\nthese topics increases cross-dataset generalization, without reducing in-domain\nclassification performance. For a robust dataset design, we suggest applying\ninexpensive unsupervised methods to inspect the collected data and downsize the\nnon-generalizable content before manually annotating for class labels.",
          "link": "http://arxiv.org/abs/2010.07414",
          "publishedOn": "2021-05-23T06:08:16.758Z",
          "wordCount": 628,
          "title": "On Cross-Dataset Generalization in Automatic Detection of Online Abuse. (arXiv:2010.07414v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1\">Hansi Hettiarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>",
          "description": "This paper presents the team BRUMS submission to SemEval-2020 Task 3: Graded\nWord Similarity in Context. The system utilises state-of-the-art contextualised\nword embeddings, which have some task-specific adaptations, including stacked\nembeddings and average embeddings. Overall, the approach achieves good\nevaluation scores across all the languages, while maintaining simplicity.\nFollowing the final rankings, our approach is ranked within the top 5 solutions\nof each language while preserving the 1st position of Finnish subtask 2.",
          "link": "http://arxiv.org/abs/2010.06269",
          "publishedOn": "2021-05-23T06:08:16.745Z",
          "wordCount": 556,
          "title": "BRUMS at SemEval-2020 Task 3: Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity. (arXiv:2010.06269v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1\">Sukhdeep S. Sodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1\">Ellie Ka-In Chio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1\">Ambarish Jash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apte_A/0/1/0/all/0/1\">Ajit Apte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeje_A/0/1/0/all/0/1\">Ayooluwakunmi Jeje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1\">Dima Kuzmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_H/0/1/0/all/0/1\">Harry Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Effrat_J/0/1/0/all/0/1\">Jon Effrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_T/0/1/0/all/0/1\">Tarush Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1\">Nitin Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sarvjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Senqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tameen Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wankhede_A/0/1/0/all/0/1\">Amol Wankhede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1\">Moustafa Alzantot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Allen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1\">Tushar Chandra</a>",
          "description": "As more and more online search queries come from voice, automatic speech\nrecognition becomes a key component to deliver relevant search results. Errors\nintroduced by automatic speech recognition (ASR) lead to irrelevant search\nresults returned to the user, thus causing user dissatisfaction. In this paper,\nwe introduce an approach, Mondegreen, to correct voice queries in text space\nwithout depending on audio signals, which may not always be available due to\nsystem constraints or privacy or bandwidth (for example, some ASR systems run\non-device) considerations. We focus on voice queries transcribed via several\nproprietary commercial ASR systems. These queries come from users making\ninternet, or online service search queries. We first present an analysis\nshowing how different the language distribution coming from user voice queries\nis from that in traditional text corpora used to train off-the-shelf ASR\nsystems. We then demonstrate that Mondegreen can achieve significant\nimprovements in increased user interaction by correcting user voice queries in\none of the largest search systems in Google. Finally, we see Mondegreen as\ncomplementing existing highly-optimized production ASR systems, which may not\nbe frequently retrained and thus lag behind due to vocabulary drifts.",
          "link": "http://arxiv.org/abs/2105.09930",
          "publishedOn": "2021-05-23T06:08:16.721Z",
          "wordCount": 678,
          "title": "Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries. (arXiv:2105.09930v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2005.01107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1\">Luis Enrico Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1\">Diane Kathryn Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Question generation (QG) is a natural language generation task where a model\nis trained to ask questions corresponding to some input text. Most recent\napproaches frame QG as a sequence-to-sequence problem and rely on additional\nfeatures and mechanisms to increase performance; however, these often increase\nmodel complexity, and can rely on auxiliary data unavailable in practical use.\nA single Transformer-based unidirectional language model leveraging transfer\nlearning can be used to produce high quality questions while disposing of\nadditional task-specific complexity. Our QG model, finetuned from GPT-2 Small,\noutperforms several paragraph-level QG baselines on the SQuAD dataset by 0.95\nMETEOR points. Human evaluators rated questions as easy to answer, relevant to\ntheir context paragraph, and corresponding well to natural human speech. Also\nintroduced is a new set of baseline scores on the RACE dataset, which has not\npreviously been used for QG tasks. Further experimentation with varying model\ncapacities and datasets with non-identification type questions is recommended\nin order to further verify the robustness of pretrained Transformer-based LMs\nas question generators.",
          "link": "http://arxiv.org/abs/2005.01107",
          "publishedOn": "2021-05-23T06:08:16.708Z",
          "wordCount": 646,
          "title": "Simplifying Paragraph-level Question Generation via Transformer Language Models. (arXiv:2005.01107v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scontras_G/0/1/0/all/0/1\">Gregory Scontras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>",
          "description": "Recent advances in computational cognitive science (i.e., simulation-based\nprobabilistic programs) have paved the way for significant progress in formal,\nimplementable models of pragmatics. Rather than describing a pragmatic\nreasoning process in prose, these models formalize and implement one, deriving\nboth qualitative and quantitative predictions of human behavior -- predictions\nthat consistently prove correct, demonstrating the viability and value of the\nframework. The current paper provides a practical introduction to and critical\nassessment of the Bayesian Rational Speech Act modeling framework, unpacking\ntheoretical foundations, exploring technological innovations, and drawing\nconnections to issues beyond current applications.",
          "link": "http://arxiv.org/abs/2105.09867",
          "publishedOn": "2021-05-23T06:08:16.699Z",
          "wordCount": 522,
          "title": "A practical introduction to the Rational Speech Act modeling framework. (arXiv:2105.09867v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$~GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.",
          "link": "http://arxiv.org/abs/2105.09858",
          "publishedOn": "2021-05-23T06:08:16.671Z",
          "wordCount": 646,
          "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7~GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.",
          "link": "http://arxiv.org/abs/2105.09856",
          "publishedOn": "2021-05-23T06:08:16.662Z",
          "wordCount": 635,
          "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1\">Bhaskar Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1\">Nick Craswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "An emerging recipe for achieving state-of-the-art effectiveness in neural\ndocument re-ranking involves utilizing large pre-trained language models -\ne.g., BERT - to evaluate all individual passages in the document and then\naggregating the outputs by pooling or additional Transformer layers. A major\ndrawback of this approach is high query latency due to the cost of evaluating\nevery passage in the document with BERT. To make matters worse, this high\ninference cost and latency varies based on the length of the document, with\nlonger documents requiring more time and computation. To address this\nchallenge, we adopt an intra-document cascading strategy, which prunes passages\nof a candidate document using a less expensive model, called ESM, before\nrunning a scoring model that is more expensive and effective, called ETM. We\nfound it best to train ESM (short for Efficient Student Model) via knowledge\ndistillation from the ETM (short for Effective Teacher Model) e.g., BERT. This\npruning allows us to only run the ETM model on a smaller set of passages whose\nsize does not vary by document length. Our experiments on the MS MARCO and TREC\nDeep Learning Track benchmarks suggest that the proposed Intra-Document\nCascaded Ranking Model (IDCM) leads to over 400% lower query latency by\nproviding essentially the same effectiveness as the state-of-the-art BERT-based\ndocument ranking models.",
          "link": "http://arxiv.org/abs/2105.09816",
          "publishedOn": "2021-05-23T06:08:16.636Z",
          "wordCount": 659,
          "title": "Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Junru Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnow_K/0/1/0/all/0/1\">Kevin Parnow</a>",
          "description": "Constituent and dependency parsing, the two classic forms of syntactic\nparsing, have been found to benefit from joint training and decoding under a\nuniform formalism, Head-driven Phrase Structure Grammar (HPSG). However,\ndecoding this unified grammar has a higher time complexity ($O(n^5)$) than\ndecoding either form individually ($O(n^3)$) since more factors have to be\nconsidered during decoding. We thus propose an improved head scorer that helps\nachieve a novel performance-preserved parser in $O$($n^3$) time complexity.\nFurthermore, on the basis of this proposed practical HPSG parser, we\ninvestigated the strengths of HPSG-based parsing and explored the general\nmethod of training an HPSG-based parser from only a constituent or dependency\nannotations in a multilingual scenario. We thus present a more effective, more\nin-depth, and general work on HPSG parsing.",
          "link": "http://arxiv.org/abs/2105.09835",
          "publishedOn": "2021-05-23T06:08:16.627Z",
          "wordCount": 552,
          "title": "Head-driven Phrase Structure Parsing in O($n^3$) Time Complexity. (arXiv:2105.09835v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jihyung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jangwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chisung Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junseong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yongsook Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Taehwan Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joohong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Juhyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Sungwon Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Younghoon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Inkwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Sangwoo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Myeonghwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Seongbo Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_S/0/1/0/all/0/1\">Seungwon Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunkyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jamin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Lucy Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jungwoo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho Alice Oh Jungwoo Ha Kyunghyun Cho</a>",
          "description": "We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE\nis a collection of 8 Korean natural language understanding (NLU) tasks,\nincluding Topic Classification, Semantic Textual Similarity, Natural Language\nInference, Named Entity Recognition, Relation Extraction, Dependency Parsing,\nMachine Reading Comprehension, and Dialogue State Tracking. We build all of the\ntasks from scratch from diverse source corpora while respecting copyrights, to\nensure accessibility for anyone without any restrictions. With ethical\nconsiderations in mind, we carefully design annotation protocols. Along with\nthe benchmark tasks and data, we provide suitable evaluation metrics and\nfine-tuning recipes for pretrained language models for each task. We\nfurthermore release the pretrained language models (PLM), KLUE-BERT and\nKLUE-RoBERTa, to help reproduce baseline models on KLUE and thereby facilitate\nfuture research. We make a few interesting observations from the preliminary\nexperiments using the proposed KLUE benchmark suite, already demonstrating the\nusefulness of this new benchmark suite. First, we find KLUE-RoBERTa-large\noutperforms other baselines, including multilingual PLMs and existing\nopen-source Korean PLMs. Second, we see minimal degradation in performance even\nwhen we replace personally identifiable information from the pretraining\ncorpus, suggesting that privacy and NLU capability are not at odds with each\nother. Lastly, we find that using BPE tokenization in combination with\nmorpheme-level pre-tokenization is effective in tasks involving morpheme-level\ntagging, detection and generation. In addition to accelerating Korean NLP\nresearch, our comprehensive documentation on creating KLUE will facilitate\ncreating similar resources for other languages in the future. KLUE is available\nat this https URL (https://klue-benchmark.com/).",
          "link": "http://arxiv.org/abs/2105.09680",
          "publishedOn": "2021-05-23T06:08:16.503Z",
          "wordCount": 735,
          "title": "KLUE: Korean Language Understanding Evaluation. (arXiv:2105.09680v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1\">Keyue Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuzhuo Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Distantly supervised (DS) relation extraction (RE) has attracted much\nattention in the past few years as it can utilize large-scale auto-labeled\ndata. However, its evaluation has long been a problem: previous works either\ntook costly and inconsistent methods to manually examine a small sample of\nmodel predictions, or directly test models on auto-labeled data -- which, by\nour check, produce as much as 53% wrong labels at the entity pair level in the\npopular NYT10 dataset. This problem has not only led to inaccurate evaluation,\nbut also made it hard to understand where we are and what's left to improve in\nthe research of DS-RE. To evaluate DS-RE models in a more credible way, we\nbuild manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,\nand thoroughly evaluate several competitive models, especially the latest\npre-trained ones. The experimental results show that the manual evaluation can\nindicate very different conclusions from automatic ones, especially some\nunexpected observations, e.g., pre-trained models can achieve dominating\nperformance while being more susceptible to false-positives compared to\nprevious methods. We hope that both our manual test sets and novel observations\ncan help advance future DS-RE research.",
          "link": "http://arxiv.org/abs/2105.09543",
          "publishedOn": "2021-05-23T06:08:16.493Z",
          "wordCount": 643,
          "title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction. (arXiv:2105.09543v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Extensive research on target-dependent sentiment classification (TSC) has led\nto strong classification performances in domains where authors tend to\nexplicitly express sentiment about specific entities or topics, such as in\nreviews or on social media. We investigate TSC in news articles, a much less\nresearched domain, despite the importance of news as an essential information\nsource in individual and societal decision making. This article introduces\nNewsTSC, a manually annotated dataset to explore TSC on news articles.\nInvestigating characteristics of sentiment in news and contrasting them to\npopular TSC domains, we find that sentiment in the news is expressed less\nexplicitly, is more dependent on context and readership, and requires a greater\ndegree of interpretation. In an extensive evaluation, we find that the state of\nthe art in TSC performs worse on news articles than on other domains (average\nrecall AvgRec = 69.8 on NewsTSC compared to AvgRev = [75.6, 82.2] on\nestablished TSC datasets). Reasons include incorrectly resolved relation of\ntarget and sentiment-bearing phrases and off-context dependence. As a major\nimprovement over previous news TSC, we find that BERT's natural language\nunderstanding capabilities capture the less explicit sentiment used in news\narticles.",
          "link": "http://arxiv.org/abs/2105.09660",
          "publishedOn": "2021-05-23T06:08:16.482Z",
          "wordCount": 621,
          "title": "Towards Target-dependent Sentiment Classification in News Articles. (arXiv:2105.09660v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Profitlich_H/0/1/0/all/0/1\">Hans-J&#xfc;rgen Profitlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>",
          "description": "We describe our work on information extraction in medical documents written\nin German, especially detecting negations using an architecture based on the\nUIMA pipeline. Based on our previous work on software modules to cover medical\nconcepts like diagnoses, examinations, etc. we employ a version of the NegEx\nregular expression algorithm with a large set of triggers as a baseline. We\nshow how a significantly smaller trigger set is sufficient to achieve similar\nresults, in order to reduce adaptation times to new text types. We elaborate on\nthe question whether dependency parsing (based on the Stanford CoreNLP model)\nis a good alternative and describe the potentials and shortcomings of both\napproaches.",
          "link": "http://arxiv.org/abs/2105.09702",
          "publishedOn": "2021-05-23T06:08:16.455Z",
          "wordCount": 571,
          "title": "A Case Study on Pros and Cons of Regular Expression Detection and Dependency Parsing for Negation Extraction from German Medical Documents. Technical Report. (arXiv:2105.09702v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aashish Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zesch_T/0/1/0/all/0/1\">Torsten Zesch</a>",
          "description": "When evaluating the performance of automatic speech recognition models,\nusually word error rate within a certain dataset is used. Special care must be\ntaken in understanding the dataset in order to report realistic performance\nnumbers. We argue that many performance numbers reported probably underestimate\nthe expected error rate. We conduct experiments controlling for selection bias,\ngender as well as overlap (between training and test data) in content, voices,\nand recording conditions. We find that content overlap has the biggest impact,\nbut other factors like gender also play a role.",
          "link": "http://arxiv.org/abs/2105.09742",
          "publishedOn": "2021-05-23T06:08:16.444Z",
          "wordCount": 532,
          "title": "Robustness of end-to-end Automatic Speech Recognition Models -- A Case Study using Mozilla DeepSpeech. (arXiv:2105.09742v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giacomelli_G/0/1/0/all/0/1\">G. Giacomelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saran_T/0/1/0/all/0/1\">T. Saran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grippa_F/0/1/0/all/0/1\">F. Grippa</a>",
          "description": "We investigate the impact of a novel method called \"virtual mirroring\" to\npromote employee self-reflection and impact customer satisfaction. The method\nis based on measuring communication patterns, through social network and\nsemantic analysis, and mirroring them back to the individual. Our goal is to\ndemonstrate that self-reflection can trigger a change in communication\nbehaviors, which lead to increased customer satisfaction. We illustrate and\ntest our approach analyzing e-mails of a large global services company by\ncomparing changes in customer satisfaction associated with team leaders exposed\nto virtual mirroring (the experimental group). We find an increase in customer\nsatisfaction in the experimental group and a decrease in the control group\n(team leaders not involved in the virtual mirroring process). With regard to\nthe individual communication indicators, we find that customer satisfaction is\nhigher when employees are more responsive, use a simpler language, are embedded\nin less centralized communication networks, and show more stable leadership\npatterns.",
          "link": "http://arxiv.org/abs/2105.09571",
          "publishedOn": "2021-05-23T06:08:16.435Z",
          "wordCount": 611,
          "title": "The impact of virtual mirroring on customer satisfaction. (arXiv:2105.09571v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1\">Magnus Sahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeuniaux_P/0/1/0/all/0/1\">Patrick Jeuniaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyllensten_A/0/1/0/all/0/1\">Amaru Cuba Gyllensten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miliani_M/0/1/0/all/0/1\">Martina Miliani</a>",
          "description": "Distributional semantics has deeply changed in the last decades. First,\npredict models stole the thunder from traditional count ones, and more recently\nboth of them were replaced in many NLP applications by contextualized vectors\nproduced by Transformer neural language models. Although an extensive body of\nresearch has been devoted to Distributional Semantic Model (DSM) evaluation, we\nstill lack a thorough comparison with respect to tested models, semantic tasks,\nand benchmark datasets. Moreover, previous work has mostly focused on\ntask-driven evaluation, instead of exploring the differences between the way\nmodels represent the lexical semantic space. In this paper, we perform a\ncomprehensive evaluation of type distributional vectors, either produced by\nstatic DSMs or obtained by averaging the contextualized vectors generated by\nBERT. First of all, we investigate the performance of embeddings in several\nsemantic tasks, carrying out an in-depth statistical analysis to identify the\nmajor factors influencing the behavior of DSMs. The results show that i.) the\nalleged superiority of predict based models is more apparent than real, and\nsurely not ubiquitous and ii.) static DSMs surpass contextualized\nrepresentations in most out-of-context semantic tasks and datasets.\nFurthermore, we borrow from cognitive neuroscience the methodology of\nRepresentational Similarity Analysis (RSA) to inspect the semantic spaces\ngenerated by distributional models. RSA reveals important differences related\nto the frequency and part-of-speech of lexical items.",
          "link": "http://arxiv.org/abs/2105.09825",
          "publishedOn": "2021-05-23T06:08:16.424Z",
          "wordCount": 663,
          "title": "A comprehensive comparative evaluation and analysis of Distributional Semantic Models. (arXiv:2105.09825v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1\">Danilo Dessi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1\">Rim Helaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1\">Daniele Riboni</a>",
          "description": "Today, we are seeing an ever-increasing number of clinical notes that contain\nclinical results, images, and textual descriptions of patient's health state.\nAll these data can be analyzed and employed to cater novel services that can\nhelp people and domain experts with their common healthcare tasks. However,\nmany technologies such as Deep Learning and tools like Word Embeddings have\nstarted to be investigated only recently, and many challenges remain open when\nit comes to healthcare domain applications. To address these challenges, we\npropose the use of Deep Learning and Word Embeddings for identifying sixteen\nmorbidity types within textual descriptions of clinical records. For this\npurpose, we have used a Deep Learning model based on Bidirectional Long-Short\nTerm Memory (LSTM) layers which can exploit state-of-the-art vector\nrepresentations of data such as Word Embeddings. We have employed pre-trained\nWord Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained\non the target domain. Furthermore, we have compared the performances of the\ndeep learning approaches against the traditional tf-idf using Support Vector\nMachine and Multilayer perceptron (our baselines). From the obtained results it\nseems that the latter outperforms the combination of Deep Learning approaches\nusing any word embeddings. Our preliminary results indicate that there are\nspecific features that make the dataset biased in favour of traditional machine\nlearning approaches.",
          "link": "http://arxiv.org/abs/2105.09632",
          "publishedOn": "2021-05-23T06:08:16.414Z",
          "wordCount": 680,
          "title": "TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zixiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1\">Rim Helaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1\">Daniele Riboni</a>",
          "description": "Empathetic response from the therapist is key to the success of clinical\npsychotherapy, especially motivational interviewing. Previous work on\ncomputational modelling of empathy in motivational interviewing has focused on\noffline, session-level assessment of therapist empathy, where empathy captures\nall efforts that the therapist makes to understand the client's perspective and\nconvey that understanding to the client. In this position paper, we propose a\nnovel task of turn-level detection of client need for empathy. Concretely, we\npropose to leverage pre-trained language models and empathy-related general\nconversation corpora in a unique labeller-detector framework, where the\nlabeller automatically annotates a motivational interviewing conversation\ncorpus with empathy labels to train the detector that determines the need for\ntherapist empathy. We also lay out our strategies of extending the detector\nwith additional-input and multi-task setups to improve its detection and\nexplainability.",
          "link": "http://arxiv.org/abs/2105.09649",
          "publishedOn": "2021-05-23T06:08:16.391Z",
          "wordCount": 586,
          "title": "Towards Detecting Need for Empathetic Response in Motivational Interviewing. (arXiv:2105.09649v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1\">Dongfang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zhilin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>",
          "description": "We consider the problem of collectively detecting multiple events,\nparticularly in cross-sentence settings. The key to dealing with the problem is\nto encode semantic information and model event inter-dependency at a\ndocument-level. In this paper, we reformulate it as a Seq2Seq task and propose\na Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level\nassociation of events and semantic information simultaneously. Specifically, a\nbidirectional decoder is firstly devised to model event inter-dependency within\na sentence when decoding the event tag vector sequence. Secondly, an\ninformation aggregation module is employed to aggregate sentence-level semantic\nand event tag information. Finally, we stack multiple bidirectional decoders\nand feed cross-sentence information, forming a multi-layer bidirectional\ntagging architecture to iteratively propagate information across sentences. We\nshow that our approach provides significant improvement in performance compared\nto the current state-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.09458",
          "publishedOn": "2021-05-23T06:08:16.366Z",
          "wordCount": 569,
          "title": "MLBiNet: A Cross-Sentence Collective Event Detection Network. (arXiv:2105.09458v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>",
          "description": "This paper describes the system developed by the Laboratoire d'analyse\nstatistique des textes (LAST) for the Lexical Complexity Prediction shared task\nat SemEval-2021. The proposed system is made up of a LightGBM model fed with\nfeatures obtained from many word frequency lists, published lexical norms and\npsychometric data. For tackling the specificity of the multi-word task, it uses\nbigram association measures. Despite that the only contextual feature used was\nsentence length, the system achieved an honorable performance in the multi-word\ntask, but poorer in the single word task. The bigram association measures were\nfound useful, but to a limited extent.",
          "link": "http://arxiv.org/abs/2105.09653",
          "publishedOn": "2021-05-23T06:08:16.352Z",
          "wordCount": 533,
          "title": "LAST at SemEval-2021 Task 1: Improving Multi-Word Complexity Prediction Using Bigram Association Measures. (arXiv:2105.09653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose \\method, a\ntraining method to obtain a single unified multilingual translation model.\nmCOLT is empowered by two techniques: (i) a contrastive learning scheme to\nclose the gap among representations of different languages, and (ii) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mCOLT achieves\ncompetitive or even better performance than a strong pre-trained model mBART on\ntens of WMT benchmarks. For non-English directions, mCOLT achieves an\nimprovement of average 10+ BLEU compared with the multilingual baseline.",
          "link": "http://arxiv.org/abs/2105.09501",
          "publishedOn": "2021-05-23T06:08:16.338Z",
          "wordCount": 576,
          "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahlou_C/0/1/0/all/0/1\">Chuhong Lahlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crayton_A/0/1/0/all/0/1\">Ancil Crayton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1\">Caroline Trier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willett_E/0/1/0/all/0/1\">Evan Willett</a>",
          "description": "In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an\nArtificial Intelligence (AI) Health Outcomes Challenge seeking solutions to\npredict risk in value-based care for incorporation into CMS Innovation Center\npayment and service delivery models. Recently, modern language models have\nplayed key roles in a number of health related tasks. This paper presents, to\nthe best of our knowledge, the first application of these models to patient\nreadmission prediction. To facilitate this, we create a dataset of 1.2 million\nmedical history samples derived from the Limited Dataset (LDS) issued by CMS.\nMoreover, we propose a comprehensive modeling solution centered on a deep\nlearning framework for this data. To demonstrate the framework, we train an\nattention-based Transformer to learn Medicare semantics in support of\nperforming downstream prediction tasks thereby achieving 0.91 AUC and 0.91\nrecall on readmission classification. We also introduce a novel data\npre-processing pipeline and discuss pertinent deployment considerations\nsurrounding model explainability and bias.",
          "link": "http://arxiv.org/abs/2105.09428",
          "publishedOn": "2021-05-23T06:08:16.214Z",
          "wordCount": 592,
          "title": "Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder. (arXiv:2105.09428v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>",
          "description": "Dependency parsing is a crucial step towards deep language understanding and,\ntherefore, widely demanded by numerous Natural Language Processing\napplications. In particular, left-to-right and top-down transition-based\nalgorithms that rely on Pointer Networks are among the most accurate approaches\nfor performing dependency parsing. Additionally, it has been observed for the\ntop-down algorithm that Pointer Networks' sequential decoding can be improved\nby implementing a hierarchical variant, more adequate to model dependency\nstructures. Considering all this, we develop a bottom-up-oriented Hierarchical\nPointer Network for the left-to-right parser and propose two novel\ntransition-based alternatives: an approach that parses a sentence in\nright-to-left order and a variant that does it from the outside in. We\nempirically test the proposed neural architecture with the different algorithms\non a wide variety of languages, outperforming the original approach in\npractically all of them and setting new state-of-the-art results on the English\nand Chinese Penn Treebanks for non-contextualized and BERT-based embeddings.",
          "link": "http://arxiv.org/abs/2105.09611",
          "publishedOn": "2021-05-23T06:08:16.202Z",
          "wordCount": 583,
          "title": "Dependency Parsing with Bottom-up Hierarchical Pointer Networks. (arXiv:2105.09611v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>",
          "description": "Neural network approaches have been applied to computational morphology with\ngreat success, improving the performance of most tasks by a large margin and\nproviding new perspectives for modeling. This paper starts with a brief\nintroduction to computational morphology, followed by a review of recent work\non computational morphology with neural network approaches, to provide an\noverview of the area. In the end, we will analyze the advantages and problems\nof neural network approaches to computational morphology, and point out some\ndirections to be explored by future research and study.",
          "link": "http://arxiv.org/abs/2105.09404",
          "publishedOn": "2021-05-23T06:08:16.168Z",
          "wordCount": 505,
          "title": "Computational Morphology with Neural Network Approaches. (arXiv:2105.09404v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose {\\name}, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. {\\name} utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that {\\name} achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.",
          "link": "http://arxiv.org/abs/2105.09601",
          "publishedOn": "2021-05-23T06:08:16.148Z",
          "wordCount": 667,
          "title": "See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1\">Gengchen Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janowicz_K/0/1/0/all/0/1\">Krzysztof Janowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Ling Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>",
          "description": "As an important part of Artificial Intelligence (AI), Question Answering (QA)\naims at generating answers to questions phrased in natural language. While\nthere has been substantial progress in open-domain question answering, QA\nsystems are still struggling to answer questions which involve geographic\nentities or concepts and that require spatial operations. In this paper, we\ndiscuss the problem of geographic question answering (GeoQA). We first\ninvestigate the reasons why geographic questions are difficult to answer by\nanalyzing challenges of geographic questions. We discuss the uniqueness of\ngeographic questions compared to general QA. Then we review existing work on\nGeoQA and classify them by the types of questions they can address. Based on\nthis survey, we provide a generic classification framework for geographic\nquestions. Finally, we conclude our work by pointing out unique future research\ndirections for GeoQA.",
          "link": "http://arxiv.org/abs/2105.09392",
          "publishedOn": "2021-05-23T06:08:16.136Z",
          "wordCount": 595,
          "title": "Geographic Question Answering: Challenges, Uniqueness, Classification, and Future Directions. (arXiv:2105.09392v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shirong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sheng Bi</a>",
          "description": "Event detection (ED) aims at detecting event trigger words in sentences and\nclassifying them into specific event types. In real-world applications, ED\ntypically does not have sufficient labelled data, thus can be formulated as a\nfew-shot learning problem. To tackle the issue of low sample diversity in\nfew-shot ED, we propose a novel knowledge-based few-shot event detection method\nwhich uses a definition-based encoder to introduce external event knowledge as\nthe knowledge prior of event types. Furthermore, as external knowledge\ntypically provides limited and imperfect coverage of event types, we introduce\nan adaptive knowledge-enhanced Bayesian meta-learning method to dynamically\nadjust the knowledge prior of event types. Experiments show our method\nconsistently and substantially outperforms a number of baselines by at least 15\nabsolute F1 points under the same few-shot settings.",
          "link": "http://arxiv.org/abs/2105.09509",
          "publishedOn": "2021-05-23T06:08:16.101Z",
          "wordCount": 568,
          "title": "Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection. (arXiv:2105.09509v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lianwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yuan Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yuqian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Ling Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhaoyin Qi</a>",
          "description": "Recent studies constructing direct interactions between the claim and each\nsingle user response (a comment or a relevant article) to capture evidence have\nshown remarkable success in interpretable claim verification. Owing to\ndifferent single responses convey different cognition of individual users\n(i.e., audiences), the captured evidence belongs to the perspective of\nindividual cognition. However, individuals' cognition of social things is not\nalways able to truly reflect the objective. There may be one-sided or biased\nsemantics in their opinions on a claim. The captured evidence correspondingly\ncontains some unobjective and biased evidence fragments, deteriorating task\nperformance. In this paper, we propose a Dual-view model based on the views of\nCollective and Individual Cognition (CICD) for interpretable claim\nverification. From the view of the collective cognition, we not only capture\nthe word-level semantics based on individual users, but also focus on\nsentence-level semantics (i.e., the overall responses) among all users and\nadjust the proportion between them to generate global evidence. From the view\nof individual cognition, we select the top-$k$ articles with high degree of\ndifference and interact with the claim to explore the local key evidence\nfragments. To weaken the bias of individual cognition-view evidence, we devise\ninconsistent loss to suppress the divergence between global and local evidence\nfor strengthening the consistent shared evidence between the both. Experiments\non three benchmark datasets confirm that CICD achieves state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2105.09567",
          "publishedOn": "2021-05-23T06:08:16.080Z",
          "wordCount": 672,
          "title": "Unified Dual-view Cognitive Model for Interpretable Claim Verification. (arXiv:2105.09567v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2102.03848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamada_M/0/1/0/all/0/1\">Mohamed A. Hamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1\">Abdelrahman Abdallah</a>",
          "description": "Many computer systems for calculating the proper organization of memory are\namong the most critical issues. Using a tier cache memory (along with branching\nprediction) is an effective means of increasing modern multi-core processors'\nperformance. Designing high-performance processors is a complex task and\nrequires preliminary verification and analysis of the model level, usually used\nin analytical and simulation modeling. The refinement of extreme programming is\nan unfortunate challenge. Few experts disagree with the synthesis of access\npoints. This article demonstrates that Internet QoS and 16-bit architectures\nare always incompatible, but it's the same situation for write-back caches. The\nsolution to this problem can be implemented by analyzing simulation models of\ndifferent complexity in combination with the analytical evaluation of\nindividual algorithms. This work is devoted to designing a multi-parameter\nsimulation model of a multi-process for evaluating the performance of cache\nmemory algorithms and the optimality of the structure. Optimization of the\nstructures and algorithms of the cache memory allows you to accelerate the\ninteraction of the memory process and improve the performance of the entire\nsystem.",
          "link": "http://arxiv.org/abs/2102.03848",
          "publishedOn": "2021-05-23T06:08:17.168Z",
          "wordCount": 650,
          "title": "Estimate The Efficiency Of Multiprocessor's Cash Memory Work Algorithms. (arXiv:2102.03848v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1812.00002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1\">Sen Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Congzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>",
          "description": "Interactive news recommendation has been launched and attracted much\nattention recently. In this scenario, user's behavior evolves from single click\nbehavior to multiple behaviors including like, comment, share etc. However,\nmost of the existing methods still use single click behavior as the unique\ncriterion of judging user's preferences. Further, although heterogeneous graphs\nhave been applied in different areas, a proper way to construct a heterogeneous\ngraph for interactive news data with an appropriate learning mechanism on it is\nstill desired. To address the above concerns, we propose a graph-based\nbehavior-aware network, which simultaneously considers six different types of\nbehaviors as well as user's demand on the news diversity. We have three main\nsteps. First, we build an interaction behavior graph for multi-level and\nmulti-category data. Second, we apply DeepWalk on the behavior graph to obtain\nentity semantics, then build a graph-based convolutional neural network called\nG-CNN to learn news representations, and an attention-based LSTM to learn\nbehavior sequence representations. Third, we introduce core and coritivity\nfeatures for the behavior graph, which measure the concentration degree of\nuser's interests. These features affect the trade-off between accuracy and\ndiversity of our personalized recommendation system. Taking these features into\naccount, our system finally achieves recommending news to different users at\ntheir different levels of concentration degrees.",
          "link": "http://arxiv.org/abs/1812.00002",
          "publishedOn": "2021-05-23T06:08:17.152Z",
          "wordCount": 678,
          "title": "The Graph-Based Behavior-Aware Recommendation for Interactive News. (arXiv:1812.00002v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>",
          "description": "Conversational recommender systems (CRS) enable the traditional recommender\nsystems to explicitly acquire user preferences towards items and attributes\nthrough interactive conversations. Reinforcement learning (RL) is widely\nadopted to learn conversational recommendation policies to decide what\nattributes to ask, which items to recommend, and when to ask or recommend, at\neach conversation turn. However, existing methods mainly target at solving one\nor two of these three decision-making problems in CRS with separated\nconversation and recommendation components, which restrict the scalability and\ngenerality of CRS and fall short of preserving a stable training procedure. In\nthe light of these challenges, we propose to formulate these three\ndecision-making problems in CRS as a unified policy learning task. In order to\nsystematically integrate conversation and recommendation components, we develop\na dynamic weighted graph based RL method to learn a policy to select the action\nat each conversation turn, either asking an attribute or recommending items.\nFurther, to deal with the sample efficiency issue, we propose two action\nselection strategies for reducing the candidate action space according to the\npreference and entropy information. Experimental results on two benchmark CRS\ndatasets and a real-world E-Commerce application show that the proposed method\nnot only significantly outperforms state-of-the-art methods but also enhances\nthe scalability and stability of CRS.",
          "link": "http://arxiv.org/abs/2105.09710",
          "publishedOn": "2021-05-23T06:08:17.142Z",
          "wordCount": 642,
          "title": "Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning. (arXiv:2105.09710v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Proper_H/0/1/0/all/0/1\">H. A. Proper</a>",
          "description": "Effective information disclosure in the context of databases with a large\nconceptual schema is known to be a non-trivial problem. In particular the\nformulation of ad-hoc queries is a major problem in such contexts. Existing\napproaches for tackling this problem include graphical query interfaces, query\nby navigation, query by construction, and point to point queries. In this\nreport we propose an adoption of the query by navigation mechanism that is\nespecially geared towards the InfoAssistant product. Query by navigation is\nbased on ideas from the information retrieval world, in particular on the\nstratified hypermedia architecture. When using our approach to the formulations\nof queries, a user will first formulate a number of simple queries\ncorresponding to linear paths through the information structure. The\nformulation of the linear paths is the result of the {\\em explorative phase} of\nthe query formulation. Once users have specified a number of these linear\npaths, they may combine them to form more complex queries. Examples of such\ncombinations are: concatenation, union, intersection and selection. This last\nprocess is referred to as {\\em query by construction}, and is the {\\em\nconstructive phase} of the query formulation process.",
          "link": "http://arxiv.org/abs/2105.09562",
          "publishedOn": "2021-05-23T06:08:17.118Z",
          "wordCount": 609,
          "title": "Interactive Query Formulation using Query By Navigation. (arXiv:2105.09562v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bountrogiannis_K/0/1/0/all/0/1\">Konstantinos Bountrogiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzagkarakis_G/0/1/0/all/0/1\">George Tzagkarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalides_P/0/1/0/all/0/1\">Panagiotis Tsakalides</a>",
          "description": "Due to the importance of the lower bounding distances and the attractiveness\nof symbolic representations, the family of symbolic aggregate approximations\n(SAX) has been used extensively for encoding time series data. However, typical\nSAX-based methods rely on two restrictive assumptions; the Gaussian\ndistribution and equiprobable symbols. This paper proposes two novel\ndata-driven SAX-based symbolic representations, distinguished by their\ndiscretization steps. The first representation, oriented for general data\ncompaction and indexing scenarios, is based on the combination of kernel\ndensity estimation and Lloyd-Max quantization to minimize the information loss\nand mean squared error in the discretization step. The second method, oriented\nfor high-level mining tasks, employs the Mean-Shift clustering method and is\nshown to enhance anomaly detection in the lower-dimensional space. Besides, we\nverify on a theoretical basis a previously observed phenomenon of the intrinsic\nprocess that results in a lower than the expected variance of the intermediate\npiecewise aggregate approximation. This phenomenon causes an additional\ninformation loss but can be avoided with a simple modification. The proposed\nrepresentations possess all the attractive properties of the conventional SAX\nmethod. Furthermore, experimental evaluation on real-world datasets\ndemonstrates their superiority compared to the traditional SAX and an\nalternative data-driven SAX variant.",
          "link": "http://arxiv.org/abs/2105.09592",
          "publishedOn": "2021-05-23T06:08:17.108Z",
          "wordCount": 634,
          "title": "Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>",
          "description": "Recommender systems are gaining increasing and critical impacts on human and\nsociety since a growing number of users use them for information seeking and\ndecision making. Therefore, it is crucial to address the potential unfairness\nproblems in recommendations. Just like users have personalized preferences on\nitems, users' demands for fairness are also personalized in many scenarios.\nTherefore, it is important to provide personalized fair recommendations for\nusers to satisfy their personalized fairness demands. Besides, previous works\non fair recommendation mainly focus on association-based fairness. However, it\nis important to advance from associative fairness notions to causal fairness\nnotions for assessing fairness more properly in recommender systems. Based on\nthe above considerations, this paper focuses on achieving personalized\ncounterfactual fairness for users in recommender systems. To this end, we\nintroduce a framework for achieving counterfactually fair recommendations\nthrough adversary learning by generating feature-independent user embeddings\nfor recommendation. The framework allows recommender systems to achieve\npersonalized fairness for users while also covering non-personalized\nsituations. Experiments on two real-world datasets with shallow and deep\nrecommendation algorithms show that our method can generate fairer\nrecommendations for users with a desirable recommendation performance.",
          "link": "http://arxiv.org/abs/2105.09829",
          "publishedOn": "2021-05-23T06:08:17.079Z",
          "wordCount": 630,
          "title": "Towards Personalized Fairness based on Causal Notion. (arXiv:2105.09829v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanya_S/0/1/0/all/0/1\">Suhas Jayaram Subramanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_R/0/1/0/all/0/1\">Ravishankar Krishnaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simhadri_H/0/1/0/all/0/1\">Harsha Vardhan Simhadri</a>",
          "description": "Approximate nearest neighbor search (ANNS) is a fundamental building block in\ninformation retrieval with graph-based indices being the current\nstate-of-the-art and widely used in the industry. Recent advances in\ngraph-based indices have made it possible to index and search billion-point\ndatasets with high recall and millisecond-level latency on a single commodity\nmachine with an SSD.\n\nHowever, existing graph algorithms for ANNS support only static indices that\ncannot reflect real-time changes to the corpus required by many key real-world\nscenarios (e.g. index of sentences in documents, email, or a news index). To\novercome this drawback, the current industry practice for manifesting updates\ninto such indices is to periodically re-build these indices, which can be\nprohibitively expensive.\n\nIn this paper, we present the first graph-based ANNS index that reflects\ncorpus updates into the index in real-time without compromising on search\nperformance. Using update rules for this index, we design FreshDiskANN, a\nsystem that can index over a billion points on a workstation with an SSD and\nlimited memory, and support thousands of concurrent real-time inserts, deletes\nand searches per second each, while retaining $>95\\%$ 5-recall@5. This\nrepresents a 5-10x reduction in the cost of maintaining freshness in indices\nwhen compared to existing methods.",
          "link": "http://arxiv.org/abs/2105.09613",
          "publishedOn": "2021-05-23T06:08:17.044Z",
          "wordCount": 643,
          "title": "FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for Streaming Similarity Search. (arXiv:2105.09613v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1\">Bhaskar Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1\">Nick Craswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "An emerging recipe for achieving state-of-the-art effectiveness in neural\ndocument re-ranking involves utilizing large pre-trained language models -\ne.g., BERT - to evaluate all individual passages in the document and then\naggregating the outputs by pooling or additional Transformer layers. A major\ndrawback of this approach is high query latency due to the cost of evaluating\nevery passage in the document with BERT. To make matters worse, this high\ninference cost and latency varies based on the length of the document, with\nlonger documents requiring more time and computation. To address this\nchallenge, we adopt an intra-document cascading strategy, which prunes passages\nof a candidate document using a less expensive model, called ESM, before\nrunning a scoring model that is more expensive and effective, called ETM. We\nfound it best to train ESM (short for Efficient Student Model) via knowledge\ndistillation from the ETM (short for Effective Teacher Model) e.g., BERT. This\npruning allows us to only run the ETM model on a smaller set of passages whose\nsize does not vary by document length. Our experiments on the MS MARCO and TREC\nDeep Learning Track benchmarks suggest that the proposed Intra-Document\nCascaded Ranking Model (IDCM) leads to over 400% lower query latency by\nproviding essentially the same effectiveness as the state-of-the-art BERT-based\ndocument ranking models.",
          "link": "http://arxiv.org/abs/2105.09816",
          "publishedOn": "2021-05-23T06:08:17.014Z",
          "wordCount": 659,
          "title": "Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Joemon Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>",
          "description": "Learning from implicit feedback is one of the most common cases in the\napplication of recommender systems. Generally speaking, interacted examples are\nconsidered as positive while negative examples are sampled from uninteracted\nones. However, noisy examples are prevalent in real-world implicit feedback. A\nnoisy positive example could be interacted but it actually leads to negative\nuser preference. A noisy negative example which is uninteracted because of\nunawareness of the user could also denote potential positive user preference.\nConventional training methods overlook these noisy examples, leading to\nsub-optimal recommendation. In this work, we propose probabilistic and\nvariational recommendation denoising for implicit feedback. Through an\nempirical study, we find that different models make relatively similar\npredictions on clean examples which denote the real user preference, while the\npredictions on noisy examples vary much more across different models. Motivated\nby this observation, we propose denoising with probabilistic inference (DPI)\nwhich aims to minimize the KL-divergence between the real user preference\ndistributions parameterized by two recommendation models while maximize the\nlikelihood of data observation. We then show that DPI recovers the evidence\nlower bound of an variational auto-encoder when the real user preference is\nconsidered as the latent variables. This leads to our second learning framework\ndenoising with variational autoencoder (DVAE). We employ the proposed DPI and\nDVAE on four state-of-the-art recommendation models and conduct experiments on\nthree datasets. Experimental results demonstrate that DPI and DVAE\nsignificantly improve recommendation performance compared with normal training\nand other denoising methods. Codes will be open-sourced.",
          "link": "http://arxiv.org/abs/2105.09605",
          "publishedOn": "2021-05-23T06:08:16.990Z",
          "wordCount": 678,
          "title": "Probabilistic and Variational Recommendation Denoising. (arXiv:2105.09605v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2012.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>",
          "description": "The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.",
          "link": "http://arxiv.org/abs/2012.00641",
          "publishedOn": "2021-05-23T06:08:15.920Z",
          "wordCount": 678,
          "title": "A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1\">Dheeraj Pailla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Himanshu Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1\">Aadilmehdi Sanchawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>",
          "description": "The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.",
          "link": "http://arxiv.org/abs/2104.05947",
          "publishedOn": "2021-05-23T06:08:15.873Z",
          "wordCount": 668,
          "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2104.14066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>",
          "description": "Object detection can be regarded as a pixel clustering task, and its boundary\nis determined by four extreme points (leftmost, top, rightmost, and bottom).\nHowever, most studies focus on the center or corner points of the object, which\nare actually conditional results of the extreme points. In this paper, we\npresent an Extreme-Point-Prediction-Based object detector (EPP-Net), which\ndirectly regresses the relative displacement vector between each pixel and the\nfour extreme points. We also propose a new metric to measure the similarity\nbetween two groups of extreme points, namely, Extreme Intersection over Union\n(EIoU), and incorporate this EIoU as a new regression loss. Moreover, we\npropose a novel branch to predict the EIoU between the ground-truth and the\nprediction results, and combine it with the classification confidence as the\nranking keyword in non-maximum suppression. On the MS-COCO dataset, our method\nachieves an average precision (AP) of 44.0% with ResNet-50 and an AP of 48.3%\nwith ResNeXt-101-DCN. The proposed EPP-Net provides a new method to detect\nobjects and outperforms state-of-the-art anchor-free detectors.",
          "link": "http://arxiv.org/abs/2104.14066",
          "publishedOn": "2021-05-23T06:08:18.236Z",
          "wordCount": 613,
          "title": "Objects as Extreme Points. (arXiv:2104.14066v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leshchinskiy_B/0/1/0/all/0/1\">Brandon Leshchinskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Requena_Mesa_C/0/1/0/all/0/1\">Christian Requena-Mesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chishtie_F/0/1/0/all/0/1\">Farrukh Chishtie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1\">Natalia D&#xed;az-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1\">Oc&#xe9;ane Boulais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aruna Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pina_A/0/1/0/all/0/1\">Aaron Pi&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raissi_C/0/1/0/all/0/1\">Chedy Ra&#xef;ssi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1\">Alexander Lavin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1\">Dava Newman</a>",
          "description": "As climate change increases the intensity of natural disasters, society needs\nbetter tools for adaptation. Floods, for example, are the most frequent natural\ndisaster, and better tools for flood risk communication could increase the\nsupport for flood-resilient infrastructure development. Our work aims to enable\nmore visual communication of large-scale climate impacts via visualizing the\noutput of coastal flood models as satellite imagery. We propose the first deep\nlearning pipeline to ensure physical-consistency in synthetic visual satellite\nimagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it\nproduces imagery that is physically-consistent with the output of an\nexpert-validated storm surge model (NOAA SLOSH). By evaluating the imagery\nrelative to physics-based flood maps, we find that our proposed framework\noutperforms baseline models in both physical-consistency and photorealism. We\nenvision our work to be the first step towards a global visualization of how\nclimate change shapes our landscape. Continuing on this path, we show that the\nproposed pipeline generalizes to visualize arctic sea ice melt. We also publish\na dataset of over 25k labelled image-pairs to study image-to-image translation\nin Earth observation.",
          "link": "http://arxiv.org/abs/2104.04785",
          "publishedOn": "2021-05-23T06:08:18.215Z",
          "wordCount": 685,
          "title": "Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization. (arXiv:2104.04785v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13482",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_K/0/1/0/all/0/1\">Kang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoyun Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fakai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1\">Chihung Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Lingyun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chang-Fu Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_S/0/1/0/all/0/1\">Shun Miao</a>",
          "description": "Bone mineral density (BMD) is a clinically critical indicator of\nosteoporosis, usually measured by dual-energy X-ray absorptiometry (DEXA). Due\nto the limited accessibility of DEXA machines and examinations, osteoporosis is\noften under-diagnosed and under-treated, leading to increased fragility\nfracture risks. Thus it is highly desirable to obtain BMDs with alternative\ncost-effective and more accessible medical imaging examinations such as X-ray\nplain films. In this work, we formulate the BMD estimation from plain hip X-ray\nimages as a regression problem. Specifically, we propose a new semi-supervised\nself-training algorithm to train the BMD regression model using images coupled\nwith DEXA measured BMDs and unlabeled images with pseudo BMDs. Pseudo BMDs are\ngenerated and refined iteratively for unlabeled images during self-training. We\nalso present a novel adaptive triplet loss to improve the model's regression\naccuracy. On an in-house dataset of 1,090 images (819 unique patients), our BMD\nestimation method achieves a high Pearson correlation coefficient of 0.8805 to\nground-truth BMDs. It offers good feasibility to use the more accessible and\ncheaper X-ray imaging for opportunistic osteoporosis screening.",
          "link": "http://arxiv.org/abs/2103.13482",
          "publishedOn": "2021-05-23T06:08:18.206Z",
          "wordCount": 649,
          "title": "Semi-Supervised Learning for Bone Mineral Density Estimation in Hip X-ray Images. (arXiv:2103.13482v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruimin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiayi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baofeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunlei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jie Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongjiang Wei</a>",
          "description": "Quantitative susceptibility mapping (QSM) has demonstrated great potential in\nquantifying tissue susceptibility in various brain diseases. However, the\nintrinsic ill-posed inverse problem relating the tissue phase to the underlying\nsusceptibility distribution affects the accuracy for quantifying tissue\nsusceptibility. Recently, deep learning has shown promising results to improve\naccuracy by reducing the streaking artifacts. However, there exists a mismatch\nbetween the observed phase and the theoretical forward phase estimated by the\nsusceptibility label. In this study, we proposed a model-based deep learning\narchitecture that followed the STI (susceptibility tensor imaging) physical\nmodel, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the\nrelationship between STI-derived phase contrast induced by the susceptibility\ntensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The\nconvolution neural networks are embedded into the physical model to learn a\nregularization term containing prior information. ki33 and phase induced by\nki13 and ki23 terms were used as the labels for network training. Quantitative\nevaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed\ndeep learning QSM methods. The results showed that MoDL-QSM achieved superior\nperformance, demonstrating its potential for future applications.",
          "link": "http://arxiv.org/abs/2101.08413",
          "publishedOn": "2021-05-23T06:08:18.199Z",
          "wordCount": 660,
          "title": "MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility Mapping. (arXiv:2101.08413v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1\">Xiaoyu Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>",
          "description": "Human motion prediction aims to forecast future human poses given a sequence\nof past 3D skeletons. While this problem has recently received increasing\nattention, it has mostly been tackled for single humans in isolation. In this\npaper we explore this problem from a novel perspective, involving humans\nperforming collaborative tasks. We assume that the input of our system are two\nsequences of past skeletons for two interacting persons, and we aim to predict\nthe future motion for each of them. For this purpose, we devise a novel cross\ninteraction attention mechanism that exploits historical information of both\npersons and learns to predict cross dependencies between self poses and the\nposes of the other person in spite of their spatial or temporal distance. Since\nno dataset to train such interactive situations is available, we have captured\nExPI (Extreme Pose Interaction), a new lab-based person interaction dataset of\nprofessional dancers performing acrobatics. ExPI contains 115 sequences with\n30k frames and 60k instances with annotated 3D body poses and shapes. We\nthoroughly evaluate our cross-interaction network on this dataset and show that\nboth in short-term and long-term predictions, it consistently outperforms\nbaselines that independently reason for each person. We plan to release our\ncode jointly with the dataset and the train/test splits to spur future research\non the topic.",
          "link": "http://arxiv.org/abs/2105.08825",
          "publishedOn": "2021-05-23T06:08:18.192Z",
          "wordCount": 661,
          "title": "Multi-Person Extreme Motion Prediction with Cross-Interaction Attention. (arXiv:2105.08825v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>",
          "description": "The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.",
          "link": "http://arxiv.org/abs/2012.00641",
          "publishedOn": "2021-05-23T06:08:18.175Z",
          "wordCount": 678,
          "title": "A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08147",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rister_B/0/1/0/all/0/1\">Blaine Rister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L. Rubin</a>",
          "description": "Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently\nobtained to determine the extent of lung disease and are a valuable source of\ndata for creating artificial intelligence models. Most work to date assessing\ndisease severity on chest imaging has focused on segmenting computed tomography\n(CT) images; however, given that CTs are performed much less frequently than\nchest X-rays for COVID-19 patients, automated lung lesion segmentation on chest\nX-rays could be clinically valuable. There currently exists a universal\nshortage of chest X-rays with ground truth COVID-19 lung lesion annotations,\nand manually contouring lung opacities is a tedious, labor-intensive task. To\naccelerate severity detection and augment the amount of publicly available\nchest X-ray training data for supervised deep learning (DL) models, we leverage\nexisting annotated CT images to generate frontal projection \"chest X-ray\"\nimages for training COVID-19 chest X-ray models. In this paper, we propose an\nautomated pipeline for segmentation of COVID-19 lung lesions on chest X-rays\ncomprised of a Mask R-CNN trained on a mixed dataset of open-source chest\nX-rays and coronal X-ray projections computed from annotated volumetric CTs. On\na test set containing 40 chest X-rays of COVID-19 positive patients, our model\nachieved IoU scores of 0.81 $\\pm$ 0.03 and 0.79 $\\pm$ 0.03 when trained on a\ndataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50\nprojections from CTs, respectively. Our model far outperforms current baselines\nwith limited supervised training and may assist in automated COVID-19 severity\nquantification on chest X-rays.",
          "link": "http://arxiv.org/abs/2105.08147",
          "publishedOn": "2021-05-23T06:08:18.169Z",
          "wordCount": 779,
          "title": "COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs. (arXiv:2105.08147v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hingane_S/0/1/0/all/0/1\">Shreeshail Hingane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Artistic style transfer aims to transfer the style characteristics of one\nimage onto another image while retaining its content. Existing approaches\ncommonly leverage various normalization techniques, although these face\nlimitations in adequately transferring diverse textures to different spatial\nlocations. Self-Attention-based approaches have tackled this issue with partial\nsuccess but suffer from unwanted artifacts. Motivated by these observations,\nthis paper aims to combine the best of both worlds: self-attention and\nnormalization. That yields a new plug-and-play module that we name\nSelf-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially\na spatially adaptive normalization module whose parameters are inferred through\nattention on the content and style image. We demonstrate that plugging SAFIN\ninto the base network of another state-of-the-art method results in enhanced\nstylization. We also develop a novel base network composed of Wavelet Transform\nfor multi-scale style transfer, which when combined with SAFIN, produces\nvisually appealing results with lesser unwanted textures.",
          "link": "http://arxiv.org/abs/2105.06129",
          "publishedOn": "2021-05-23T06:08:18.162Z",
          "wordCount": 616,
          "title": "SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization. (arXiv:2105.06129v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Speth_J/0/1/0/all/0/1\">Jeremy Speth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vance_N/0/1/0/all/0/1\">Nathan Vance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1\">Patrick Flynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin Bowyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1\">Adam Czajka</a>",
          "description": "Remote photoplethysmography (rPPG), a family of techniques for monitoring\nblood volume changes, may be especially useful for widespread contactless\nhealth monitoring using face video from consumer-grade visible-light cameras.\nThe COVID-19 pandemic has caused the widespread use of protective face masks.\nWe found that occlusions from cloth face masks increased the mean absolute\nerror of heart rate estimation by more than 80\\% when deploying methods\ndesigned on unmasked faces. We show that augmenting unmasked face videos by\nadding patterned synthetic face masks forces the model to attend to the\nperiocular and forehead regions, improving performance and closing the gap\nbetween masked and unmasked pulse estimation. To our knowledge, this paper is\nthe first to analyse the impact of face masks on the accuracy of pulse\nestimation and offers several novel contributions: (a) 3D CNN-based method\ndesigned for remote photoplethysmography in a presence of face masks, (b) two\npublicly available pulse estimation datasets acquired from 86 unmasked and 61\nmasked subjects, (c) evaluations of handcrafted algorithms and a 3D CNN trained\non videos of unmasked faces and with masks synthetically added, and (d) data\naugmentation method to add a synthetic mask to a face video.",
          "link": "http://arxiv.org/abs/2101.04096",
          "publishedOn": "2021-05-23T06:08:18.154Z",
          "wordCount": 714,
          "title": "Remote Pulse Estimation in the Presence of Face Masks. (arXiv:2101.04096v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xinya Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaisiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>",
          "description": "Despite previous success in generating audio-driven talking heads, most of\nthe previous studies focus on the correlation between speech content and the\nmouth shape. Facial emotion, which is one of the most important features on\nnatural human faces, is always neglected in their methods. In this work, we\npresent Emotional Video Portraits (EVP), a system for synthesizing high-quality\nvideo portraits with vivid emotional dynamics driven by audios. Specifically,\nwe propose the Cross-Reconstructed Emotion Disentanglement technique to\ndecompose speech into two decoupled spaces, i.e., a duration-independent\nemotion space and a duration dependent content space. With the disentangled\nfeatures, dynamic 2D emotional facial landmarks can be deduced. Then we propose\nthe Target-Adaptive Face Synthesis technique to generate the final high-quality\nvideo portraits, by bridging the gap between the deduced landmarks and the\nnatural head poses of target videos. Extensive experiments demonstrate the\neffectiveness of our method both qualitatively and quantitatively.",
          "link": "http://arxiv.org/abs/2104.07452",
          "publishedOn": "2021-05-23T06:08:18.147Z",
          "wordCount": 612,
          "title": "Audio-Driven Emotional Video Portraits. (arXiv:2104.07452v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1\">Ekta U. Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingjian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis G. Banerjee</a>",
          "description": "Object recognition in unseen indoor environments remains a challenging\nproblem for visual perception of mobile robots. In this letter, we propose the\nuse of topologically persistent features, which rely on the objects' shape\ninformation, to address this challenge. In particular, we extract two kinds of\nfeatures, namely, sparse persistence image (PI) and amplitude, by applying\npersistent homology to multi-directional height function-based filtrations of\nthe cubical complexes representing the object segmentation maps. The features\nare then used to train a fully connected network for recognition. For\nperformance evaluation, in addition to a widely used shape dataset and a\nbenchmark indoor scenes dataset, we collect a new dataset, comprising scene\nimages from two different environments, namely, a living room and a mock\nwarehouse. The scenes are captured using varying camera poses under different\nillumination conditions and include up to five different objects from a given\nset of fourteen objects. On the benchmark indoor scenes dataset, sparse PI\nfeatures show better recognition performance in unseen environments than the\nfeatures learned using the widely used ResNetV2-56 and EfficientNet-B4 models.\nFurther, they provide slightly higher recall and accuracy values than Faster\nR-CNN, an end-to-end object detection method, and its state-of-the-art variant,\nDomain Adaptive Faster R-CNN. The performance of our methods also remains\nrelatively unchanged from the training environment (living room) to the unseen\nenvironment (mock warehouse) in the new dataset. In contrast, the performance\nof the object detection methods drops substantially. We also implement the\nproposed method on a real-world robot to demonstrate its usefulness.",
          "link": "http://arxiv.org/abs/2010.03196",
          "publishedOn": "2021-05-23T06:08:18.140Z",
          "wordCount": 761,
          "title": "Visual Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08506",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1\">Sara Atito Ali Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1\">Mehmet Can Yavuz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sen_M/0/1/0/all/0/1\">Mehmet Umut Sen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulsen_F/0/1/0/all/0/1\">Fatih Gulsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tutar_O/0/1/0/all/0/1\">Onur Tutar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korkmazer_B/0/1/0/all/0/1\">Bora Korkmazer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samanci_C/0/1/0/all/0/1\">Cesur Samanci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirolu_S/0/1/0/all/0/1\">Sabri Sirolu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamid_R/0/1/0/all/0/1\">Rauf Hamid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eryurekli_A/0/1/0/all/0/1\">Ali Ergun Eryurekli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mammadov_T/0/1/0/all/0/1\">Toghrul Mammadov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yanikoglu_B/0/1/0/all/0/1\">Berrin Yanikoglu</a>",
          "description": "Detecting COVID-19 in computed tomography (CT) or radiography images has been\nproposed as a supplement to the definitive RT-PCR test. We present a deep\nlearning ensemble for detecting COVID-19 infection, combining slice-based (2D)\nand volume-based (3D) approaches. The 2D system detects the infection on each\nCT slice independently, combining them to obtain the patient-level decision via\ndifferent methods (averaging and long-short term memory networks). The 3D\nsystem takes the whole CT volume to arrive to the patient-level decision in one\nstep. A new high resolution chest CT scan dataset, called the IST-C dataset, is\nalso collected in this work. The proposed ensemble, called IST-CovNet, obtains\n90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting\nCOVID-19 among normal controls and other types of lung pathologies; and 93.69%\naccuracy and 0.99 AUC score on the publicly available MosMed dataset that\nconsists of COVID-19 scans and normal controls only. The system is deployed at\nIstanbul University Cerrahpasa School of Medicine.",
          "link": "http://arxiv.org/abs/2105.08506",
          "publishedOn": "2021-05-23T06:08:18.131Z",
          "wordCount": 693,
          "title": "COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches. (arXiv:2105.08506v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "We show how to use random field theory in a supervised, energy-based model\nfor multiple pseudo image classification of 2D integer matrices. In the model,\neach row of a 2D integer matrix is a pseudo image where a local receptive field\nfocuses on multiple portions of individual rows for simultaneous learning. The\nmodel is used for a classification task consisting of presence of patient\nbiomarkers indicative of a particular disease.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-05-23T06:08:18.110Z",
          "wordCount": 540,
          "title": "Multiple Simultaneous Pseudo Image Classification with Random Fields and a Deep Belief Network for Disease Indication. (arXiv:2104.10762v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masaya Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "Various fonts give different impressions, such as legible, rough, and\ncomic-text.This paper aims to analyze the correlation between the local shapes,\nor parts, and the impression of fonts. By focusing on local shapes instead of\nthe whole letter shape, we can realize letter-shape independent and more\ngeneral analysis. The analysis is performed by newly combining SIFT and\nDeepSets, to extract an arbitrary number of essential parts from a particular\nfont and aggregate them to infer the font impressions by nonlinear regression.\nOur qualitative and quantitative analyses prove that (1)fonts with similar\nparts have similar impressions, (2)many impressions, such as legible and rough,\nlargely depend on specific parts, (3)several impressions are very irrelevant to\nparts.",
          "link": "http://arxiv.org/abs/2103.14216",
          "publishedOn": "2021-05-23T06:08:18.103Z",
          "wordCount": 569,
          "title": "Which Parts determine the Impression of the Font?. (arXiv:2103.14216v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retrain. Our key innovation is to redefine\nthe gradient to a new synaptic parameter, allowing better exploration of\nnetwork structures by taking full advantage of the competition between pruning\nand regrowth of connections. The experimental results show that the proposed\nmethod achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset\nso far. Moreover, it reaches a $\\sim$3.5% accuracy loss under unprecedented\n0.73% connectivity, which reveals remarkable structure refining capability in\nSNNs. Our work suggests that there exists extremely high redundancy in deep\nSNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring .",
          "link": "http://arxiv.org/abs/2105.04916",
          "publishedOn": "2021-05-23T06:08:18.096Z",
          "wordCount": 696,
          "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1\">Quentin Paletta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>",
          "description": "Improving irradiance forecasting is critical to further increase the share of\nsolar in the energy mix. On a short time scale, fish-eye cameras on the ground\nare used to capture cloud displacements causing the local variability of the\nelectricity production. As most of the solar radiation comes directly from the\nSun, current forecasting approaches use its position in the image as a\nreference to interpret the cloud cover dynamics. However, existing Sun tracking\nmethods rely on external data and a calibration of the camera, which requires\naccess to the device. To address these limitations, this study introduces an\nimage-based Sun tracking algorithm to localise the Sun in the image when it is\nvisible and interpolate its daily trajectory from past observations. We\nvalidate the method on a set of sky images collected over a year at SIRTA's\nlab. Experimental results show that the proposed method provides robust smooth\nSun trajectories with a mean absolute error below 1% of the image size.",
          "link": "http://arxiv.org/abs/2012.01059",
          "publishedOn": "2021-05-23T06:08:18.079Z",
          "wordCount": 634,
          "title": "A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuqing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1\">Olivia Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>",
          "description": "Policies trained in simulation often fail when transferred to the real world\ndue to the `reality gap' where the simulator is unable to accurately capture\nthe dynamics and visual properties of the real world. Current approaches to\ntackle this problem, such as domain randomization, require prior knowledge and\nengineering to determine how much to randomize system parameters in order to\nlearn a policy that is robust to sim-to-real transfer while also not being too\nconservative. We propose a method for automatically tuning simulator system\nparameters to match the real world using only raw RGB images of the real world\nwithout the need to define rewards or estimate state. Our key insight is to\nreframe the auto-tuning of parameters as a search problem where we iteratively\nshift the simulation system parameters to approach the real-world system\nparameters. We propose a Search Param Model (SPM) that, given a sequence of\nobservations and actions and a set of system parameters, predicts whether the\ngiven parameters are higher or lower than the true parameters used to generate\nthe observations. We evaluate our method on multiple robotic control tasks in\nboth sim-to-sim and sim-to-real transfer, demonstrating significant improvement\nover naive domain randomization. Project videos and code at\nhttps://yuqingd.github.io/autotuned-sim2real/",
          "link": "http://arxiv.org/abs/2104.07662",
          "publishedOn": "2021-05-23T06:08:18.072Z",
          "wordCount": 681,
          "title": "Auto-Tuned Sim-to-Real Transfer. (arXiv:2104.07662v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Aditya Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girishekar_E/0/1/0/all/0/1\">Eshwar Shamanna Girishekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Padmakar Anil Deshpande</a>",
          "description": "Automated image captioning is one of the applications of Deep Learning which\ninvolves fusion of work done in computer vision and natural language\nprocessing, and it is typically performed using Encoder-Decoder architectures.\nIn this project, we have implemented and experimented with various flavors of\nmulti-modal image captioning networks where ResNet101, DenseNet121 and VGG19\nbased CNN Encoders and Attention based LSTM Decoders were explored. We have\nstudied the effect of beam size and the use of pretrained word embeddings and\ncompared them to baseline CNN encoder and RNN decoder architecture. The goal is\nto analyze the performance of each approach using various evaluation metrics\nincluding BLEU, CIDEr, ROUGE and METEOR. We have also explored model\nexplainability using Visual Attention Maps (VAM) to highlight parts of the\nimages which has maximum contribution for predicting each word of the generated\ncaption.",
          "link": "http://arxiv.org/abs/2105.09906",
          "publishedOn": "2021-05-23T06:08:18.065Z",
          "wordCount": 582,
          "title": "Empirical Analysis of Image Caption Generation using Deep Learning. (arXiv:2105.09906v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">YiMin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yingjie Xi</a>",
          "description": "Detection faults in seismic data is a crucial step for seismic structural\ninterpretation, reservoir characterization and well placement. Some recent\nworks regard it as an image segmentation task. The task of image segmentation\nrequires huge labels, especially 3D seismic data, which has a complex structure\nand lots of noise. Therefore, its annotation requires expert experience and a\nhuge workload. In this study, we present {\\lambda}-BCE and {\\lambda}-smooth\nL1loss to effectively train 3D-CNN by some slices from 3D seismic data, so that\nthe model can learn the segmentation of 3D seismic data from a few 2D slices.\nIn order to fully extract information from limited data and suppress seismic\nnoise, we propose an attention module that can be used for active supervision\ntraining and embedded in the network. The attention heatmap target is generated\nby the original label, and letting it supervise the attention module using the\n{\\lambda}-smooth L1loss. The experiment proves the effectiveness of our loss\nfunction and attention module, it also shows that our method can extract 3D\nseismic features from a few 2D slices labels, and the segmentation effect\nachieves state-of-the-art. We only use 3.3% of the all labels, and we can\nachieve similar performance as using all labels. This work has been submitted\nto the IEEE for possible publication. Copyright may be transferred without\nnotice, after which this version may no longer be accessible.",
          "link": "http://arxiv.org/abs/2105.03857",
          "publishedOn": "2021-05-23T06:08:18.058Z",
          "wordCount": 724,
          "title": "Seismic Fault Segmentation via 3D-CNN Training by a Few 2D Slices Labels. (arXiv:2105.03857v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yubo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan-Cong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1\">Minh-Quan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh-Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thanh-Toan Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam V. Nguyen</a>",
          "description": "This paper pushes the envelope on camouflaged regions to decompose them into\nmeaningful components, namely, camouflaged instances. To promote the new task\nof camouflaged instance segmentation in-the-wild, we introduce a new dataset,\nnamely CAMO++, by extending our preliminary CAMO dataset (camouflaged object\nsegmentation) in terms of quantity and diversity. The new dataset substantially\nincreases the number of images with hierarchical pixel-wise ground-truths. We\nalso provide a benchmark suite for the task of camouflaged instance\nsegmentation. In particular, we conduct extensive evaluation of\nstate-of-the-art instance segmentation methods on our newly constructed CAMO++\ndataset in various scenarios. We also propose Camouflage Fusion Learning (CFL)\nframework for camouflaged instance segmentation to further improve the\nstate-of-the-art performance. The dataset, model, evaluation suite, and\nbenchmark will be publicly available at our project page.\n\\url{https://sites.google.com/view/ltnghia/research/camo\\_plus\\_plus}",
          "link": "http://arxiv.org/abs/2103.17123",
          "publishedOn": "2021-05-23T06:08:18.024Z",
          "wordCount": 611,
          "title": "Camouflaged Instance Segmentation In-The-Wild: Dataset And Benchmark Suite. (arXiv:2103.17123v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hornauer_S/0/1/0/all/0/1\">Sascha Hornauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffarzadegan_S/0/1/0/all/0/1\">Shabnam Ghaffarzadegan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>",
          "description": "Recent progress in network-based audio event classification has shown the\nbenefit of pre-training models on visual data such as ImageNet. While this\nprocess allows knowledge transfer across different domains, training a model on\nlarge-scale visual datasets is time consuming. On several audio event\nclassification benchmarks, we show a fast and effective alternative that\npre-trains the model unsupervised, only on audio data and yet delivers on-par\nperformance with ImageNet pre-training. Furthermore, we show that our\ndiscriminative audio learning can be used to transfer knowledge across audio\ndatasets and optionally include ImageNet pre-training.",
          "link": "http://arxiv.org/abs/2105.09279",
          "publishedOn": "2021-05-23T06:08:18.017Z",
          "wordCount": 573,
          "title": "Unsupervised Discriminative Learning of Sounds for Audio Event Classification. (arXiv:2105.09279v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03814",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1\">Phairot Autthasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1\">Rattanaphon Chaisaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1\">Thapanun Sudhawiyangkul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1\">Phurin Rangpong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1\">Suktipol Kiatthaveephong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1\">Nat Dilokthanakul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1\">Gun Bhakdisongkhram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>",
          "description": "Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)\nallow control of several applications by decoding neurophysiological phenomena,\nwhich are usually recorded by electroencephalography (EEG) using a non-invasive\ntechnique. Despite great advances in MI-based BCI, EEG rhythms are specific to\na subject and various changes over time. These issues point to significant\nchallenges to enhance the classification performance, especially in a\nsubject-independent manner. To overcome these challenges, we propose MIN2Net, a\nnovel end-to-end multi-task learning to tackle this task. We integrate deep\nmetric learning into a multi-task autoencoder to learn a compact and\ndiscriminative latent representation from EEG and perform classification\nsimultaneously. This approach reduces the complexity in pre-processing, results\nin significant performance improvement on EEG classification. Experimental\nresults in a subject-independent manner show that MIN2Net outperforms the\nstate-of-the-art techniques, achieving an F1-score improvement of 6.72%, and\n2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that\nMIN2Net improves discriminative information in the latent representation. This\nstudy indicates the possibility and practicality of using this model to develop\nMI-based BCI applications for new users without the need for calibration.",
          "link": "http://arxiv.org/abs/2102.03814",
          "publishedOn": "2021-05-23T06:08:18.005Z",
          "wordCount": 664,
          "title": "MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09750",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1\">Zongcai Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "Along with the rapid development of real-world applications, higher\nrequirements on the accuracy and efficiency of image super-resolution (SR) are\nbrought forward. Though existing methods have achieved remarkable success, the\nmajority of them demand plenty of computational resources and large amount of\nRAM, and thus they can not be well applied to mobile device. In this paper, we\naim at designing efficient architecture for 8-bit quantization and deploy it on\nmobile device. First, we conduct an experiment about meta-node latency by\ndecomposing lightweight SR architectures, which determines the portable\noperations we can utilize. Then, we dig deeper into what kind of architecture\nis beneficial to 8-bit quantization and propose anchor-based plain net (ABPN).\nFinally, we adopt quantization-aware training strategy to further boost the\nperformance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in\nterms of PSNR, while satisfying realistic needs at the same time. Code is\navaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.",
          "link": "http://arxiv.org/abs/2105.09750",
          "publishedOn": "2021-05-23T06:08:17.998Z",
          "wordCount": 596,
          "title": "Anchor-based Plain Net for Mobile Image Super-Resolution. (arXiv:2105.09750v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sibo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1\">Sertac Karaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Deep learning has been used to demonstrate end-to-end neural network learning\nfor autonomous vehicle control from raw sensory input. While LiDAR sensors\nprovide reliably accurate information, existing end-to-end driving solutions\nare mainly based on cameras since processing 3D data requires a large memory\nfootprint and computation cost. On the other hand, increasing the robustness of\nthese systems is also critical; however, even estimating the model's\nuncertainty is very challenging due to the cost of sampling-based methods. In\nthis paper, we present an efficient and robust LiDAR-based end-to-end\nnavigation framework. We first introduce Fast-LiDARNet that is based on sparse\nconvolution kernel optimization and hardware-aware model design. We then\npropose Hybrid Evidential Fusion that directly estimates the uncertainty of the\nprediction from only a single forward pass and then fuses the control\npredictions intelligently. We evaluate our system on a full-scale vehicle and\ndemonstrate lane-stable as well as navigation capabilities. In the presence of\nout-of-distribution events (e.g., sensor failures), our system significantly\nimproves robustness and reduces the number of takeovers in the real world.",
          "link": "http://arxiv.org/abs/2105.09932",
          "publishedOn": "2021-05-23T06:08:17.961Z",
          "wordCount": 618,
          "title": "Efficient and Robust LiDAR-Based End-to-End Navigation. (arXiv:2105.09932v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pahwa_R/0/1/0/all/0/1\">Ramanpreet S Pahwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1\">Soon Wee Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ren Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1\">Richard Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_O/0/1/0/all/0/1\">Oo Zaw Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_W/0/1/0/all/0/1\">Wang Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1\">Vempati Srinivasa Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nwe_T/0/1/0/all/0/1\">Tin Lay Nwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanjing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_J/0/1/0/all/0/1\">Jens Timo Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pichumani_R/0/1/0/all/0/1\">Ramani Pichumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregorich_T/0/1/0/all/0/1\">Thomas Gregorich</a>",
          "description": "For over 40 years lithographic silicon scaling has driven circuit integration\nand performance improvement in the semiconductor industry. As silicon scaling\nslows down, the industry is increasingly dependent on IC package technologies\nto contribute to further circuit integration and performance improvements. This\nis a paradigm shift and requires the IC package industry to reduce the size and\nincrease the density of internal interconnects on a scale which has never been\ndone before. Traditional package characterization and process optimization\nrelies on destructive techniques such as physical cross-sections and delayering\nto extract data from internal package features. These destructive techniques\nare not practical with today's advanced packages. In this paper we will\ndemonstrate how data acquired non-destructively with a 3D X-ray microscope can\nbe enhanced and optimized using machine learning, and can then be used to\nmeasure, characterize and optimize the design and production of buried\ninterconnects in advanced IC packages. Test vehicles replicating 2.5D and HBM\nconstruction were designed and fabricated, and digital data was extracted from\nthese test vehicles using 3D X-ray and machine learning techniques. The\nextracted digital data was used to characterize and optimize the design and\nproduction of the interconnects and demonstrates a superior alternative to\ndestructive physical analysis. We report an mAP of 0.96 for 3D object\ndetection, a dice score of 0.92 for 3D segmentation, and an average of 2.1um\nerror for 3D metrology on the test dataset. This paper is the first part of a\nmulti-part report.",
          "link": "http://arxiv.org/abs/2103.04838",
          "publishedOn": "2021-05-23T06:08:17.947Z",
          "wordCount": 754,
          "title": "Machine-learning based methodologies for 3d x-ray measurement, characterization and optimization for buried structures in advanced ic packages. (arXiv:2103.04838v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouhsain_S/0/1/0/all/0/1\">Smail Ait Bouhsain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1\">Saeed Saadatnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>",
          "description": "In order to be globally deployed, autonomous cars must guarantee the safety\nof pedestrians. This is the reason why forecasting pedestrians' intentions\nsufficiently in advance is one of the most critical and challenging tasks for\nautonomous vehicles. This work tries to solve this problem by jointly\npredicting the intention and visual states of pedestrians. In terms of visual\nstates, whereas previous work focused on x-y coordinates, we will also predict\nthe size and indeed the whole bounding box of the pedestrian. The method is a\nrecurrent neural network in a multi-task learning approach. It has one head\nthat predicts the intention of the pedestrian for each one of its future\nposition and another one predicting the visual states of the pedestrian.\nExperiments on the JAAD dataset show the superiority of the performance of our\nmethod compared to previous works for intention prediction. Also, although its\nsimple architecture (more than 2 times faster), the performance of the bounding\nbox prediction is comparable to the ones yielded by much more complex\narchitectures. Our code is available online.",
          "link": "http://arxiv.org/abs/2010.10270",
          "publishedOn": "2021-05-23T06:08:17.928Z",
          "wordCount": 647,
          "title": "Pedestrian Intention Prediction: A Multi-task Perspective. (arXiv:2010.10270v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanli Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden M. Lake</a>",
          "description": "Humans are highly efficient learners, with the ability to grasp the meaning\nof a new concept from just a few examples. Unlike popular computer vision\nsystems, humans can flexibly leverage the compositional structure of the visual\nworld, understanding new concepts as combinations of existing concepts. In the\ncurrent paper, we study how people learn different types of visual\ncompositions, using abstract visual forms with rich relational structure. We\nfind that people can make meaningful compositional generalizations from just a\nfew examples in a variety of scenarios, and we develop a Bayesian program\ninduction model that provides a close fit to the behavioral data. Unlike past\nwork examining special cases of compositionality, our work shows how a single\ncomputational approach can account for many distinct types of compositional\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09848",
          "publishedOn": "2021-05-23T06:08:17.922Z",
          "wordCount": 587,
          "title": "Flexible Compositional Learning of Structured Visual Concepts. (arXiv:2105.09848v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1\">Andrew Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1\">Vicky Kalogeiton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "The objective of this work is person-clustering in videos -- grouping\ncharacters according to their identity. Previous methods focus on the narrower\ntask of face-clustering, and for the most part ignore other cues such as the\nperson's voice, their overall appearance (hair, clothes, posture), and the\nediting structure of the videos. Similarly, most current datasets evaluate only\nthe task of face-clustering, rather than person-clustering. This limits their\napplicability to downstream applications such as story understanding which\nrequire person-level, rather than only face-level, reasoning. In this paper we\nmake contributions to address both these deficiencies: first, we introduce a\nMulti-Modal High-Precision Clustering algorithm for person-clustering in videos\nusing cues from several modalities (face, body, and voice). Second, we\nintroduce a Video Person-Clustering dataset, for evaluating multi-modal\nperson-clustering. It contains body-tracks for each annotated character,\nface-tracks when visible, and voice-tracks when speaking, with their associated\nfeatures. The dataset is by far the largest of its kind, and covers films and\nTV-shows representing a wide range of demographics. Finally, we show the\neffectiveness of using multiple modalities for person-clustering, explore the\nuse of this new broad task for story understanding through character\nco-occurrences, and achieve a new state of the art on all available datasets\nfor face and person-clustering.",
          "link": "http://arxiv.org/abs/2105.09939",
          "publishedOn": "2021-05-23T06:08:17.901Z",
          "wordCount": 633,
          "title": "Face, Body, Voice: Video Person-Clustering with Multiple Modalities. (arXiv:2105.09939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weidler_T/0/1/0/all/0/1\">Tonio Weidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehnen_J/0/1/0/all/0/1\">Julian Lehnen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_Q/0/1/0/all/0/1\">Quinton Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebok_D/0/1/0/all/0/1\">D&#xe1;vid Seb&#x151;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gerhard Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessens_K/0/1/0/all/0/1\">Kurt Driessens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senden_M/0/1/0/all/0/1\">Mario Senden</a>",
          "description": "Lateral connections play an important role for sensory processing in visual\ncortex by supporting discriminable neuronal responses even to highly similar\nfeatures. In the present work, we show that establishing a biologically\ninspired Mexican hat lateral connectivity profile along the filter domain can\nsignificantly improve the classification accuracy of a variety of lightweight\nconvolutional neural networks without the addition of trainable network\nparameters. Moreover, we demonstrate that it is possible to analytically\ndetermine the stationary distribution of modulated filter activations and\nthereby avoid using recurrence for modeling temporal dynamics. We furthermore\nreveal that the Mexican hat connectivity profile has the effect of ordering\nfilters in a sequence resembling the topographic organization of feature\nselectivity in early visual cortex. In an ordered filter sequence, this profile\nthen sharpens the filters' tuning curves.",
          "link": "http://arxiv.org/abs/2105.09830",
          "publishedOn": "2021-05-23T06:08:17.895Z",
          "wordCount": 573,
          "title": "Biologically Inspired Semantic Lateral Connectivity for Convolutional Neural Networks. (arXiv:2105.09830v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipayan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>",
          "description": "Reservoir Computing (RC) offers a viable option to deploy AI algorithms on\nlow-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired\nRC model that mimics the cortical microcircuits and uses spiking neural\nnetworks (SNN) that can be directly realized on neuromorphic hardware. In this\npaper, we present a novel Parallelized LSM (PLSM) architecture that\nincorporates spatio-temporal read-out layer and semantic constraints on model\noutput. To the best of our knowledge, such a formulation has been done for the\nfirst time in literature, and it offers a computationally lighter alternative\nto traditional deep-learning models. Additionally, we also present a\ncomprehensive algorithm for the implementation of parallelizable SNNs and LSMs\nthat are GPU-compatible. We implement the PLSM model to classify\nunintentional/accidental video clips, using the Oops dataset. From the\nexperimental results on detecting unintentional action in video, it can be\nobserved that our proposed model outperforms a self-supervised model and a\nfully supervised traditional deep learning model. All the implemented codes can\nbe found at our repository\nhttps://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.",
          "link": "http://arxiv.org/abs/2105.09909",
          "publishedOn": "2021-05-23T06:08:17.884Z",
          "wordCount": 613,
          "title": "PLSM: A Parallelized Liquid State Machine for Unintentional Action Detection. (arXiv:2105.09909v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1\">Nkechinyere N. Agu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Joy T. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hanqing Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1\">James Hendler</a>",
          "description": "Radiologists usually observe anatomical regions of chest X-ray images as well\nas the overall image before making a decision. However, most existing deep\nlearning models only look at the entire X-ray image for classification, failing\nto utilize important anatomical information. In this paper, we propose a novel\nmulti-label chest X-ray classification model that accurately classifies the\nimage finding and also localizes the findings to their correct anatomical\nregions. Specifically, our model consists of two modules, the detection module\nand the anatomical dependency module. The latter utilizes graph convolutional\nnetworks, which enable our model to learn not only the label dependency but\nalso the relationship between the anatomical regions in the chest X-ray. We\nfurther utilize a method to efficiently create an adjacency matrix for the\nanatomical regions using the correlation of the label across the different\nregions. Detailed experiments and analysis of our results show the\neffectiveness of our method when compared to the current state-of-the-art\nmulti-label chest X-ray image classification methods while also providing\naccurate location information.",
          "link": "http://arxiv.org/abs/2105.09937",
          "publishedOn": "2021-05-23T06:08:17.877Z",
          "wordCount": 619,
          "title": "AnaXNet: Anatomy Aware Multi-label Finding Classification in Chest X-ray. (arXiv:2105.09937v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wangyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from human's visual and intuitive perspective. We take\nthe first step to bridge the gap by proposing a deep learning-based technique\nto automatically classify road networks into four classes on a visual basis.\nThe method is implemented by generating an image of the street network (Colored\nRoad Hierarchy Diagram), which we introduce in this paper, and classifying it\nusing a deep convolutional neural network (ResNet-34). The model achieves an\noverall classification accuracy of 0.875. Nine cities around the world are\nselected as the study areas and their road networks are acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through a\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: the\neffectiveness of our human perception augmentation is examined by a case study\nof urban vitality prediction. An advanced tree-based regression model is for\nthe first time designated to establish the relationship between morphological\nindices and vitality indicators. A positive effect of human perception\naugmentation is detected in the comparative experiment of baseline model and\naugmented model. This work expands the toolkit of quantitative urban morphology\nstudy with new techniques, supporting further studies in the future.",
          "link": "http://arxiv.org/abs/2105.09908",
          "publishedOn": "2021-05-23T06:08:17.864Z",
          "wordCount": 703,
          "title": "Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.01165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ilke Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciftci_U/0/1/0/all/0/1\">Umur A. Ciftci</a>",
          "description": "Following the recent initiatives for the democratization of AI, deep fake\ngenerators have become increasingly popular and accessible, causing dystopian\nscenarios towards social erosion of trust. A particular domain, such as\nbiological signals, attracted attention towards detection methods that are\ncapable of exploiting authenticity signatures in real videos that are not yet\nfaked by generative approaches. In this paper, we first propose several\nprominent eye and gaze features that deep fakes exhibit differently. Second, we\ncompile those features into signatures and analyze and compare those of real\nand fake videos, formulating geometric, visual, metric, temporal, and spectral\nvariations. Third, we generalize this formulation to the deep fake detection\nproblem by a deep neural network, to classify any video in the wild as fake or\nreal. We evaluate our approach on several deep fake datasets, achieving 92.48%\naccuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on\nCelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most\ndeep and biological fake detectors with complex network architectures without\nthe proposed gaze signatures. We conduct ablation studies involving different\nfeatures, architectures, sequence durations, and post-processing artifacts.",
          "link": "http://arxiv.org/abs/2101.01165",
          "publishedOn": "2021-05-23T06:08:17.845Z",
          "wordCount": 660,
          "title": "Where Do Deep Fakes Look? Synthetic Face Detection via Gaze Tracking. (arXiv:2101.01165v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ran Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rujun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhuoling Xiao</a>",
          "description": "The technology for Visual Odometry (VO) that estimates the position and\norientation of the moving object through analyzing the image sequences captured\nby on-board cameras, has been well investigated with the rising interest in\nautonomous driving. This paper studies monocular VO from the perspective of\nDeep Learning (DL). Unlike most current learning-based methods, our approach,\ncalled DeepAVO, is established on the intuition that features contribute\ndiscriminately to different motion patterns. Specifically, we present a novel\nfour-branch network to learn the rotation and translation by leveraging\nConvolutional Neural Networks (CNNs) to focus on different quadrants of optical\nflow input. To enhance the ability of feature selection, we further introduce\nan effective channel-spatial attention mechanism to force each branch to\nexplicitly distill related information for specific Frame to Frame (F2F) motion\nestimation. Experiments on various datasets involving outdoor driving and\nindoor walking scenarios show that the proposed DeepAVO outperforms the\nstate-of-the-art monocular methods by a large margin, demonstrating competitive\nperformance to the stereo VO algorithm and verifying promising potential for\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09899",
          "publishedOn": "2021-05-23T06:08:17.838Z",
          "wordCount": 619,
          "title": "DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry. (arXiv:2105.09899v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.13934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>",
          "description": "Methods to quantify the complexity of trajectory datasets are still a missing\npiece in benchmarking human trajectory prediction models. In order to gain a\nbetter understanding of the complexity of trajectory prediction tasks and\nfollowing the intuition, that more complex datasets contain more information,\nan approach for quantifying the amount of information contained in a dataset\nfrom a prototype-based dataset representation is proposed. The dataset\nrepresentation is obtained by first employing a non-trivial spatial sequence\nalignment, which enables a subsequent learning vector quantization (LVQ) stage.\nA large-scale complexity analysis is conducted on several human trajectory\nprediction benchmarking datasets, followed by a brief discussion on indications\nfor human trajectory prediction and benchmarking.",
          "link": "http://arxiv.org/abs/2005.13934",
          "publishedOn": "2021-05-23T06:08:17.832Z",
          "wordCount": 600,
          "title": "Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction. (arXiv:2005.13934v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.01383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Jan Paul Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangrong Xu</a>",
          "description": "This paper proposes a novel automatically generating image masks method for\nthe state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method\nachieves the best results in object detection until now, however, it is very\ntime-consuming and laborious to get the object Masks for training, the proposed\nmethod is composed by a two-stage design, to automatically generating image\nmasks, the first stage implements a fully convolutional networks (FCN) based\nsegmentation network, the second stage network, a Mask R-CNN based object\ndetection network, which is trained on the object image masks from FCN output,\nthe original input image, and additional label information. Through\nexperimentation, our proposed method can obtain the image masks automatically\nto train Mask R-CNN, and it can achieve very high classification accuracy with\nan over 90% mean of average precision (mAP) for segmentation",
          "link": "http://arxiv.org/abs/2003.01383",
          "publishedOn": "2021-05-23T06:08:17.823Z",
          "wordCount": 603,
          "title": "Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN. (arXiv:2003.01383v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.02161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_N/0/1/0/all/0/1\">Nivedita Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Automated Parking is becoming a standard feature in modern vehicles. Existing\nparking systems build a local map to be able to plan for maneuvering towards a\ndetected slot. Next generation parking systems have an use case where they\nbuild a persistent map of the environment where the car is frequently parked,\nsay for example, home parking or office parking. The pre-built map helps in\nre-localizing the vehicle better when its trying to park the next time. This is\nachieved by augmenting the parking system with a Visual SLAM pipeline and the\nfeature is called trained trajectory parking in the automotive industry. In\nthis paper, we discuss the use cases, design and implementation of a trained\ntrajectory automated parking system. The proposed system is deployed on\ncommercial vehicles and the consumer application is illustrated in\n\\url{https://youtu.be/nRWF5KhyJZU}. The focus of this paper is on the\napplication and the details of vision algorithms are kept at high level.",
          "link": "http://arxiv.org/abs/2001.02161",
          "publishedOn": "2021-05-23T06:08:17.817Z",
          "wordCount": 642,
          "title": "Trained Trajectory based Automated Parking System using Visual SLAM on Surround View Cameras. (arXiv:2001.02161v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.13693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pares_F/0/1/0/all/0/1\">Ferran Par&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_Duart_A/0/1/0/all/0/1\">Anna Arias-Duart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1\">Dario Garcia-Gasulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campo_Frances_G/0/1/0/all/0/1\">Gema Campo-Franc&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viladrich_N/0/1/0/all/0/1\">Nina Viladrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1\">Eduard Ayguad&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1\">Jes&#xfa;s Labarta</a>",
          "description": "In the image classification task, the most common approach is to resize all\nimages in a dataset to a unique shape, while reducing their precision to a size\nwhich facilitates experimentation at scale. This practice has benefits from a\ncomputational perspective, but it entails negative side-effects on performance\ndue to loss of information and image deformation. In this work we introduce the\nMAMe dataset, an image classification dataset with remarkable high resolution\nand variable shape properties. The goal of MAMe is to provide a tool for\nstudying the impact of such properties in image classification, while\nmotivating research in the field. The MAMe dataset contains thousands of\nartworks from three different museums, and proposes a classification task\nconsisting on differentiating between 29 mediums (i.e. materials and\ntechniques) supervised by art experts. After reviewing the singularity of MAMe\nin the context of current image classification tasks, a thorough description of\nthe task is provided, together with dataset statistics. Experiments are\nconducted to evaluate the impact of using high resolution images, variable\nshape inputs and both properties at the same time. Results illustrate the\npositive impact in performance when using high resolution images, while\nhighlighting the lack of solutions to exploit variable shapes. An additional\nexperiment exposes the distinctiveness between the MAMe dataset and the\nprototypical ImageNet dataset. Finally, the baselines are inspected using\nexplainability methods and expert knowledge, to gain insights on the challenges\nthat remain ahead.",
          "link": "http://arxiv.org/abs/2007.13693",
          "publishedOn": "2021-05-23T06:08:17.797Z",
          "wordCount": 725,
          "title": "The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties. (arXiv:2007.13693v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Thai_B/0/1/0/all/0/1\">Binh Nguyen-Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1\">Catherine Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawi_N/0/1/0/all/0/1\">Nadia Badawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "The absence or abnormality of fidgety movements of joints or limbs is\nstrongly indicative of cerebral palsy in infants. Developing computer-based\nmethods for assessing infant movements in videos is pivotal for improved\ncerebral palsy screening. Most existing methods use appearance-based features\nand are thus sensitive to strong but irrelevant signals caused by background\nclutter or a moving camera. Moreover, these features are computed over the\nwhole frame, thus they measure gross whole body movements rather than specific\njoint/limb motion.\n\nAddressing these challenges, we develop and validate a new method for fidgety\nmovement assessment from consumer-grade videos using human poses extracted from\nshort clips. Human poses capture only relevant motion profiles of joints and\nlimbs and are thus free from irrelevant appearance artifacts. The dynamics and\ncoordination between joints are modeled using spatio-temporal graph\nconvolutional networks. Frames and body parts that contain discriminative\ninformation about fidgety movements are selected through a spatio-temporal\nattention mechanism. We validate the proposed model on the cerebral palsy\nscreening task using a real-life consumer-grade video dataset collected at an\nAustralian hospital through the Cerebral Palsy Alliance, Australia. Our\nexperiments show that the proposed method achieves the ROC-AUC score of 81.87%,\nsignificantly outperforming existing competing methods with better\ninterpretability.",
          "link": "http://arxiv.org/abs/2105.09783",
          "publishedOn": "2021-05-23T06:08:17.790Z",
          "wordCount": 657,
          "title": "A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos. (arXiv:2105.09783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xiaoguang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiankun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1\">Wenjie Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "In real-world scenarios, many factors may harm face recognition performance,\ne.g., large pose, bad illumination,low resolution, blur and noise. To address\nthese challenges, previous efforts usually first restore the low-quality faces\nto high-quality ones and then perform face recognition. However, most of these\nmethods are stage-wise, which is sub-optimal and deviates from the reality. In\nthis paper, we address all these challenges jointly for unconstrained face\nrecognition. We propose an Multi-Degradation Face Restoration (MDFR) model to\nrestore frontalized high-quality faces from the given low-quality ones under\narbitrary facial poses, with three distinct novelties. First, MDFR is a\nwell-designed encoder-decoder architecture which extracts feature\nrepresentation from an input face image with arbitrary low-quality factors and\nrestores it to a high-quality counterpart. Second, MDFR introduces a pose\nresidual learning strategy along with a 3D-based Pose Normalization Module\n(PNM), which can perceive the pose gap between the input initial pose and its\nreal-frontal pose to guide the face frontalization. Finally, MDFR can generate\nfrontalized high-quality face images by a single unified network, showing a\nstrong capability of preserving face identity. Qualitative and quantitative\nexperiments on both controlled and in-the-wild benchmarks demonstrate the\nsuperiority of MDFR over state-of-the-art methods on both face frontalization\nand face restoration.",
          "link": "http://arxiv.org/abs/2105.09907",
          "publishedOn": "2021-05-23T06:08:17.782Z",
          "wordCount": 647,
          "title": "Joint Face Image Restoration and Frontalization for Recognition. (arXiv:2105.09907v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clever_H/0/1/0/all/0/1\">Henry M. Clever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grady_P/0/1/0/all/0/1\">Patrick Grady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turk_G/0/1/0/all/0/1\">Greg Turk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1\">Charles C. Kemp</a>",
          "description": "Contact pressure between the human body and its surroundings has important\nimplications. For example, it plays a role in comfort, safety, posture, and\nhealth. We present a method that infers contact pressure between a human body\nand a mattress from a depth image. Specifically, we focus on using a depth\nimage from a downward facing camera to infer pressure on a body at rest in bed\noccluded by bedding, which is directly applicable to the prevention of pressure\ninjuries in healthcare. Our approach involves augmenting a real dataset with\nsynthetic data generated via a soft-body physics simulation of a human body, a\nmattress, a pressure sensing mat, and a blanket. We introduce a novel deep\nnetwork that we trained on an augmented dataset and evaluated with real data.\nThe network contains an embedded human body mesh model and uses a white-box\nmodel of depth and pressure image generation. Our network successfully infers\nbody pose, outperforming prior work. It also infers contact pressure across a\n3D mesh model of the human body, which is a novel capability, and does so in\nthe presence of occlusion from blankets.",
          "link": "http://arxiv.org/abs/2105.09936",
          "publishedOn": "2021-05-23T06:08:17.774Z",
          "wordCount": 633,
          "title": "BodyPressure -- Inferring Body Pose and Contact Pressure from a Depth Image. (arXiv:2105.09936v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.08797",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1\">Soufiane Hayou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1\">Jean-Francois Ton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Overparameterized Neural Networks (NN) display state-of-the-art performance.\nHowever, there is a growing need for smaller, energy-efficient, neural networks\ntobe able to use machine learning applications on devices with limited\ncomputational resources. A popular approach consists of using pruning\ntechniques. While these techniques have traditionally focused on pruning\npre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et\nal. (2018) has shown promising results when pruning at initialization. However,\nfor Deep NNs, such procedures remain unsatisfactory as the resulting pruned\nnetworks can be difficult to train and, for instance, they do not prevent one\nlayer from being fully pruned. In this paper, we provide a comprehensive\ntheoretical analysis of Magnitude and Gradient based pruning at initialization\nand training of sparse architectures. This allows us to propose novel\nprincipled approaches which we validate experimentally on a variety of NN\narchitectures.",
          "link": "http://arxiv.org/abs/2002.08797",
          "publishedOn": "2021-05-23T06:08:17.767Z",
          "wordCount": 619,
          "title": "Robust Pruning at Initialization. (arXiv:2002.08797v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>",
          "description": "Learned networks in the domain of visual recognition and cognition impress in\npart because even though they are trained with datasets many orders of\nmagnitude smaller than the full population of possible images, they exhibit\nsufficient generalization to be applicable to new and previously unseen data.\nAlthough many have examined issues regarding generalization from several\nperspectives, we wondered If a network is trained with a biased dataset that\nmisses particular samples corresponding to some defining domain attribute, can\nit generalize to the full domain from which that training dataset was\nextracted? It is certainly true that in vision, no current training set fully\ncaptures all visual information and this may lead to Selection Bias. Here, we\ntry a novel approach in the tradition of the Thought Experiment. We run this\nthought experiment on a real domain of visual objects that we can fully\ncharacterize and look at specific gaps in training data and their impact on\nperformance requirements. Our thought experiment points to three conclusions:\nfirst, that generalization behavior is dependent on how sufficiently the\nparticular dimensions of the domain are represented during training; second,\nthat the utility of any generalization is completely dependent on the\nacceptable system error; and third, that specific visual features of objects,\nsuch as pose orientations out of the imaging plane or colours, may not be\nrecoverable if not represented sufficiently in a training set. Any currently\nobserved generalization in modern deep learning networks may be more the result\nof coincidental alignments and whose utility needs to be confirmed with respect\nto a system's performance specification. Our Thought Experiment Probe approach,\ncoupled with the resulting Bias Breakdown can be very informative towards\nunderstanding the impact of biases.",
          "link": "http://arxiv.org/abs/2105.09934",
          "publishedOn": "2021-05-23T06:08:17.749Z",
          "wordCount": 730,
          "title": "Probing the Effect of Selection Bias on NN Generalization with a Thought Experiment. (arXiv:2105.09934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1\">Kasra Arnavaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1\">Jelena M. Krivokapic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1\">Silja Heilmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1\">Jakob Andreas B&#xe6;rentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1\">Pia Nyeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>",
          "description": "Motivated by a challenging tubular network segmentation task, this paper\ntackles two commonly encountered problems in biomedical imaging: Topological\nconsistency of the segmentation, and limited annotations. We propose a\ntopological score which measures both topological and geometric consistency\nbetween the predicted and ground truth segmentations, applied for model\nselection and validation. We apply our topological score in three scenarios: i.\na U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised\nU-net architecture, which offers a straightforward approach to jointly training\nthe network both as an autoencoder and a segmentation algorithm. This allows us\nto utilize un-annotated data for training a representation that generalizes\nacross test data variability, in spite of our annotated training data having\nvery limited variation. Our contributions are validated on a challenging\nsegmentation task, locating tubular structures in the fetal pancreas from noisy\nlive imaging confocal microscopy.",
          "link": "http://arxiv.org/abs/2105.09737",
          "publishedOn": "2021-05-23T06:08:17.742Z",
          "wordCount": 590,
          "title": "Semi-supervised, Topology-Aware Segmentation of Tubular Structures from Live Imaging 3D Microscopy. (arXiv:2105.09737v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1\">Rakshit Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1\">Umar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seonwook Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>",
          "description": "A major challenge for physically unconstrained gaze estimation is acquiring\ntraining data with 3D gaze annotations for in-the-wild and outdoor scenarios.\nIn contrast, videos of human interactions in unconstrained environments are\nabundantly available and can be much more easily annotated with frame-level\nactivity labels. In this work, we tackle the previously unexplored problem of\nweakly-supervised gaze estimation from videos of human interactions. We\nleverage the insight that strong gaze-related geometric constraints exist when\npeople perform the activity of \"looking at each other\" (LAEO). To acquire\nviable 3D gaze supervision from LAEO labels, we propose a training algorithm\nalong with several novel loss functions especially designed for the task. With\nweak supervision from two large scale CMU-Panoptic and AVA-LAEO activity\ndatasets, we show significant improvements in (a) the accuracy of\nsemi-supervised gaze estimation and (b) cross-domain generalization on the\nstate-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation\nbenchmark. We open source our code at\nhttps://github.com/NVlabs/weakly-supervised-gaze.",
          "link": "http://arxiv.org/abs/2105.09803",
          "publishedOn": "2021-05-23T06:08:17.735Z",
          "wordCount": 588,
          "title": "Weakly-Supervised Physically Unconstrained Gaze Estimation. (arXiv:2105.09803v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1\">Manav Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1\">Peter Jakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1\">Tobias Schmid-Schirling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Multi-view classification is inspired by the behavior of humans, especially\nwhen fine-grained features or in our case rarely occurring anomalies are to be\ndetected. Current contributions point to the problem of how high-dimensional\ndata can be fused. In this work, we build upon the deep support vector data\ndescription algorithm and address multi-perspective anomaly detection using\nthree different fusion techniques i.e. early fusion, late fusion, and late\nfusion with multiple decoders. We employ different augmentation techniques with\na denoising process to deal with scarce one-class data, which further improves\nthe performance (ROC AUC = 80\\%). Furthermore, we introduce the dices dataset\nthat consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g. drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed approach\nexceeds the state-of-the-art on both the MNIST and dices datasets. To the best\nof our knowledge, this is the first work that focuses on addressing\nmulti-perspective anomaly detection in images by jointly using different\nperspectives together with one single objective function for anomaly detection.",
          "link": "http://arxiv.org/abs/2105.09903",
          "publishedOn": "2021-05-23T06:08:17.679Z",
          "wordCount": 633,
          "title": "Multi-Perspective Anomaly Detection. (arXiv:2105.09903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fonder_M/0/1/0/all/0/1\">Micha&#xeb;l Fonder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>",
          "description": "Getting the distance to objects is crucial for autonomous vehicles. In\ninstances where depth sensors cannot be used, this distance has to be estimated\nfrom RGB cameras. As opposed to cars, the task of estimating depth from\non-board mounted cameras is made complex on drones because of the lack of\nconstrains on motion during flights. %In the case of drones, this task is even\nmore complex than for car-mounted cameras since the camera motion is\nunconstrained. In this paper, we present a method to estimate the distance of\nobjects seen by an on-board mounted camera by using its RGB video stream and\ndrone motion information. Our method is built upon a pyramidal convolutional\nneural network architecture and uses time recurrence in pair with geometric\nconstraints imposed by motion to produce pixel-wise depth maps. %from a RGB\nvideo stream of a camera attached to the drone In our architecture, each level\nof the pyramid is designed to produce its own depth estimate based on past\nobservations and information provided by the previous level in the pyramid. We\nintroduce a spatial reprojection layer to maintain the spatio-temporal\nconsistency of the data between the levels. We analyse the performance of our\napproach on Mid-Air, a public drone dataset featuring synthetic drone\ntrajectories recorded in a wide variety of unstructured outdoor environments.\nOur experiments show that our network outperforms state-of-the-art depth\nestimation methods and that the use of motion information is the main\ncontributing factor for this improvement. The code of our method is publicly\navailable on GitHub; see\n$\\href{https://github.com/michael-fonder/M4Depth}{\\text{https://github.com/michael-fonder/M4Depth}}$",
          "link": "http://arxiv.org/abs/2105.09847",
          "publishedOn": "2021-05-23T06:08:17.646Z",
          "wordCount": 705,
          "title": "M4Depth: A motion-based approach for monocular depth estimation on video sequences. (arXiv:2105.09847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09913",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1\">Shehan Perera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adhikari_S/0/1/0/all/0/1\">Srikar Adhikari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>",
          "description": "The rapid and seemingly endless expansion of COVID-19 can be traced back to\nthe inefficiency and shortage of testing kits that offer accurate results in a\ntimely manner. An emerging popular technique, which adopts improvements made in\nmobile ultrasound technology, allows for healthcare professionals to conduct\nrapid screenings on a large scale. We present an image-based solution that aims\nat automating the testing process which allows for rapid mass testing to be\nconducted with or without a trained medical professional that can be applied to\nrural environments and third world countries. Our contributions towards rapid\nlarge-scale testing include a novel deep learning architecture capable of\nanalyzing ultrasound data that can run in real-time and significantly improve\nthe current state-of-the-art detection accuracies using image-based COVID-19\ndetection.",
          "link": "http://arxiv.org/abs/2105.09913",
          "publishedOn": "2021-05-23T06:08:17.622Z",
          "wordCount": 617,
          "title": "POCFormer: A Lightweight Transformer Architecture for Detection of COVID-19 Using Point of Care Ultrasound. (arXiv:2105.09913v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McNally_W/0/1/0/all/0/1\">William McNally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walters_P/0/1/0/all/0/1\">Pascale Walters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McPhee_J/0/1/0/all/0/1\">John McPhee</a>",
          "description": "Existing multi-camera solutions for automatic scorekeeping in steel-tip darts\nare very expensive and thus inaccessible to most players. Motivated to develop\na more accessible low-cost solution, we present a new approach to keypoint\ndetection and apply it to predict dart scores from a single image taken from\nany camera angle. This problem involves detecting multiple keypoints that may\nbe of the same class and positioned in close proximity to one another. The\nwidely adopted framework for regressing keypoints using heatmaps is not\nwell-suited for this task. To address this issue, we instead propose to model\nkeypoints as objects. We develop a deep convolutional neural network around\nthis idea and use it to predict dart locations and dartboard calibration points\nwithin an overall pipeline for automatic dart scoring, which we call DeepDarts.\nAdditionally, we propose several task-specific data augmentation strategies to\nimprove the generalization of our method. As a proof of concept, two datasets\ncomprising 16k images originating from two different dartboard setups were\nmanually collected and annotated to evaluate the system. In the primary dataset\ncontaining 15k images captured from a face-on view of the dartboard using a\nsmartphone, DeepDarts predicted the total score correctly in 94.7% of the test\nimages. In a second more challenging dataset containing limited training data\n(830 images) and various camera angles, we utilize transfer learning and\nextensive data augmentation to achieve a test accuracy of 84.0%. Because\nDeepDarts relies only on single images, it has the potential to be deployed on\nedge devices, giving anyone with a smartphone access to an automatic dart\nscoring system for steel-tip darts. The code and datasets are available.",
          "link": "http://arxiv.org/abs/2105.09880",
          "publishedOn": "2021-05-23T06:08:17.580Z",
          "wordCount": 718,
          "title": "DeepDarts: Modeling Keypoints as Objects for Automatic Scorekeeping in Darts using a Single Camera. (arXiv:2105.09880v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borkar_J/0/1/0/all/0/1\">Jaydeep Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "There has been a rise in the use of Machine Learning as a Service (MLaaS)\nVision APIs as they offer multiple services including pre-built models and\nalgorithms, which otherwise take a huge amount of resources if built from\nscratch. As these APIs get deployed for high-stakes applications, it's very\nimportant that they are robust to different manipulations. Recent works have\nonly focused on typical adversarial attacks when evaluating the robustness of\nvision APIs. We propose two new aspects of adversarial image generation methods\nand evaluate them on the robustness of Google Cloud Vision API's optical\ncharacter recognition service and object detection APIs deployed in real-world\nsettings such as sightengine.com, picpurify.com, Google Cloud Vision API, and\nMicrosoft Azure's Computer Vision API. Specifically, we go beyond the\nconventional small-noise adversarial attacks and introduce secret embedding and\ntransparent adversarial examples as a simpler way to evaluate robustness. These\nmethods are so straightforward that even non-specialists can craft such\nattacks. As a result, they pose a serious threat where APIs are used for\nhigh-stakes applications. Our transparent adversarial examples successfully\nevade state-of-the art object detections APIs such as Azure Cloud Vision\n(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).\n90% of the images have a secret embedded text that successfully fools the\nvision of time-limited humans but is detected by Google Cloud Vision API's\noptical character recognition. Complementing to current research, our results\nprovide simple but unconventional methods on robustness evaluation.",
          "link": "http://arxiv.org/abs/2105.09685",
          "publishedOn": "2021-05-23T06:08:17.573Z",
          "wordCount": 694,
          "title": "Simple Transparent Adversarial Examples. (arXiv:2105.09685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xianzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jianyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuting He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "This paper introduces our solution for the Track2 in AI City Challenge 2021\n(AICITY21). The Track2 is a vehicle re-identification (ReID) task with both the\nreal-world data and synthetic data. We mainly focus on four points, i.e.\ntraining data, unsupervised domain-adaptive (UDA) training, post-processing,\nmodel ensembling in this challenge. (1) Both cropping training data and using\nsynthetic data can help the model learn more discriminative features. (2) Since\nthere is a new scenario in the test set that dose not appear in the training\nset, UDA methods perform well in the challenge. (3) Post-processing techniques\nincluding re-ranking, image-to-track retrieval, inter-camera fusion, etc,\nsignificantly improve final performance. (4) We ensemble CNN-based models and\ntransformer-based models which provide different representation diversity. With\naforementioned techniques, our method finally achieves 0.7445 mAP score,\nyielding the first place in the competition. Codes are available at\nhttps://github.com/michuanhaohao/AICITY2021_Track2_DMT.",
          "link": "http://arxiv.org/abs/2105.09701",
          "publishedOn": "2021-05-23T06:08:17.562Z",
          "wordCount": 610,
          "title": "An Empirical Study of Vehicle Re-Identification on the AI City Challenge. (arXiv:2105.09701v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yukai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>",
          "description": "Deep convolutional networks have attracted great attention in image\nrestoration and enhancement. Generally, restoration quality has been improved\nby building more and more convolutional block. However, these methods mostly\nlearn a specific model to handle all images and ignore difficulty diversity. In\nother words, an area in the image with high frequency tend to lose more\ninformation during compressing while an area with low frequency tends to lose\nless. In this article, we adrress the efficiency issue in image SR by\nincorporating a patch-wise rolling network(PRN) to content-adaptively recover\nimages according to difficulty levels. In contrast to existing studies that\nignore difficulty diversity, we adopt different stage of a neural network to\nperform image restoration. In addition, we propose a rolling strategy that\nutilizes the parameters of each stage more flexible. Extensive experiments\ndemonstrate that our model not only shows a significant acceleration but also\nmaintain state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2105.09645",
          "publishedOn": "2021-05-23T06:08:17.554Z",
          "wordCount": 572,
          "title": "Content-adaptive Representation Learning for Fast Image Super-resolution. (arXiv:2105.09645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09511",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shaohua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sui_X/0/1/0/all/0/1\">Xiuchao Sui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xinxing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>",
          "description": "Medical image segmentation is important for computer-aided diagnosis. Good\nsegmentation demands the model to see the big picture and fine details\nsimultaneously, i.e., to learn image features that incorporate large context\nwhile keep high spatial resolutions. To approach this goal, the most widely\nused methods -- U-Net and variants, extract and fuse multi-scale features.\nHowever, the fused features still have small \"effective receptive fields\" with\na focus on local image cues, limiting their performance. In this work, we\npropose Segtran, an alternative segmentation framework based on transformers,\nwhich have unlimited \"effective receptive fields\" even at high feature\nresolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer:\na squeezed attention block regularizes the self attention of transformers, and\nan expansion block learns diversified representations. Additionally, we propose\na new positional encoding scheme for transformers, imposing a continuity\ninductive bias for images. Experiments were performed on 2D and 3D medical\nimage segmentation tasks: optic disc/cup segmentation in fundus images\n(REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain\ntumor segmentation in MRI scans (BraTS'19 challenge). Compared with\nrepresentative existing methods, Segtran consistently achieved the highest\nsegmentation accuracy, and exhibited good cross-domain generalization\ncapabilities.",
          "link": "http://arxiv.org/abs/2105.09511",
          "publishedOn": "2021-05-23T06:08:17.533Z",
          "wordCount": 638,
          "title": "Medical Image Segmentation using Squeeze-and-Expansion Transformers. (arXiv:2105.09511v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09720",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mudiyanselage_T/0/1/0/all/0/1\">Thosini Bamunu Mudiyanselage</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Senanayake_N/0/1/0/all/0/1\">Nipuna Senanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1\">Chunyan Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yi Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanqing Zhang</a>",
          "description": "The novel corona virus (Covid-19) has introduced significant challenges due\nto its rapid spreading nature through respiratory transmission. As a result,\nthere is a huge demand for Artificial Intelligence (AI) based quick disease\ndiagnosis methods as an alternative to high demand tests such as Polymerase\nChain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective\nradiography technique due to resource availability and quick screening. But, a\nsufficient and systematic data collection that is required by complex deep\nleaning (DL) models is more difficult and hence there are recent efforts that\nutilize transfer learning to address this issue. Still these transfer learnt\nmodels suffer from lack of generalization and increased bias to the training\ndataset resulting poor performance for unseen data. Limited correlation of the\ntransferred features from the pre-trained model to a specific medical imaging\ndomain like X-ray and overfitting on fewer data can be reasons for this\ncircumstance. In this work, we propose a novel Graph Convolution Neural Network\n(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR\nimages and meta information about patients. The proposed method exploits\nimportant relational knowledge between data instances and their features using\ngraph representation and applies convolution to learn the graph data which is\nnot possible with conventional convolution on Euclidean domain. The results of\nextensive experiments of proposed model on binary (Covid vs normal) and three\nclass (Covid, normal, other pneumonia) classification problems outperform\ndifferent benchmark transfer learnt models, hence overcoming the aforementioned\ndrawbacks.",
          "link": "http://arxiv.org/abs/2105.09720",
          "publishedOn": "2021-05-23T06:08:17.526Z",
          "wordCount": 747,
          "title": "Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks. (arXiv:2105.09720v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09683",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_R/0/1/0/all/0/1\">Ruhui Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Laili Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>",
          "description": "Background and Objective: The new type of coronavirus is also called\nCOVID-19. It began to spread at the end of 2019 and has now spread across the\nworld. Until October 2020, It has infected around 37 million people and claimed\nabout 1 million lives. We propose a deep learning model that can help\nradiologists and clinicians use chest X-rays to diagnose COVID-19 cases and\nshow the diagnostic features of pneumonia. Methods: The approach in this study\nis: 1) we propose a data enhancement method to increase the diversity of the\ndata set, thereby improving the generalization performance of the model. 2) Our\ndeep convolution neural network model DPN-SE adds a self-attention mechanism to\nthe DPN network. The addition of a self-attention mechanism has greatly\nimproved the performance of the network. 3) Use the Lime interpretable library\nto mark the feature regions on the X-ray medical image that helps doctors more\nquickly diagnose COVID-19 in people. Results: Under the same network model, the\ndata with and without data enhancement is put into the model for training\nrespectively. At last, comparing two experimental results: among the 10 network\nmodels with different structures, 7 network models have improved their effects\nafter using data enhancement, with an average improvement of 1% in recognition\naccuracy. We propose that the accuracy and recall rates of the DPN-SE network\nare 93% and 98% of cases (COVID vs. pneumonia bacteria vs. viral pneumonia vs.\nnormal). Compared with the original DPN, the respective accuracy is improved by\n2%. Conclusion: The data augmentation method we used has achieved effective\nresults on a small amount of data set, showing that a reasonable data\naugmentation method can improve the recognition accuracy without changing the\nsample size and model structure. Overall, the proposed method and model can\neffectively become a very useful tool for clinical radiologists.",
          "link": "http://arxiv.org/abs/2105.09683",
          "publishedOn": "2021-05-23T06:08:17.519Z",
          "wordCount": 807,
          "title": "DPN-SENet:A self-attention mechanism neural network for detection and diagnosis of COVID-19 from chest x-ray images. (arXiv:2105.09683v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09600",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1\">Zhihao Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>",
          "description": "Learning based video compression attracts increasing attention in the past\nfew years. The previous hybrid coding approaches rely on pixel space operations\nto reduce spatial and temporal redundancy, which may suffer from inaccurate\nmotion estimation or less effective motion compensation. In this work, we\npropose a feature-space video coding network (FVC) by performing all major\noperations (i.e., motion estimation, motion compression, motion compensation\nand residual compression) in the feature space. Specifically, in the proposed\ndeformable compensation module, we first apply motion estimation in the feature\nspace to produce motion information (i.e., the offset maps), which will be\ncompressed by using the auto-encoder style network. Then we perform motion\ncompensation by using deformable convolution and generate the predicted\nfeature. After that, we compress the residual feature between the feature from\nthe current frame and the predicted feature from our deformable compensation\nmodule. For better frame reconstruction, the reference features from multiple\nprevious reconstructed frames are also fused by using the non-local attention\nmechanism in the multi-frame feature fusion module. Comprehensive experimental\nresults demonstrate that the proposed framework achieves the state-of-the-art\nperformance on four benchmark datasets including HEVC, UVG, VTL and MCL-JCV.",
          "link": "http://arxiv.org/abs/2105.09600",
          "publishedOn": "2021-05-23T06:08:17.511Z",
          "wordCount": 635,
          "title": "FVC: A New Framework towards Deep Video Compression in Feature Space. (arXiv:2105.09600v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kowalczyk_M/0/1/0/all/0/1\">Marcin Kowalczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1\">Tomasz Kryjak</a>",
          "description": "This work describes the hardware implementation of a connected component\nlabelling (CCL) module in reprogammable logic. The main novelty of the design\nis the \"full\", i.e. without any simplifications, support of a 4 pixel per clock\nformat (4 ppc) and real-time processing of a 4K/UltraHD video stream (3840 x\n2160 pixels) at 60 frames per second. To achieve this, a special labelling\nmethod was designed and a functionality that stops the input data stream in\norder to process pixel groups which require writing more than one merger into\nthe equivalence table. The proposed module was verified in simulation and in\nhardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the ZCU104 evaluation\nboard.",
          "link": "http://arxiv.org/abs/2105.09658",
          "publishedOn": "2021-05-23T06:08:17.504Z",
          "wordCount": 559,
          "title": "A Connected Component Labelling algorithm for multi-pixel per clock cycle video strea. (arXiv:2105.09658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09624",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1\">Janek Gr&#xf6;hl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1\">Melanie Schellenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1\">Kris Dreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1\">Niklas Holzwarth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>",
          "description": "Photoacoustic imaging has the potential to revolutionise healthcare due to\nthe valuable information on tissue physiology that is contained in\nmultispectral photoacoustic measurements. Clinical translation of the\ntechnology requires conversion of the high-dimensional acquired data into\nclinically relevant and interpretable information. In this work, we present a\ndeep learning-based approach to semantic segmentation of multispectral\nphotoacoustic images to facilitate the interpretability of recorded images.\nManually annotated multispectral photoacoustic imaging data are used as gold\nstandard reference annotations and enable the training of a deep learning-based\nsegmentation algorithm in a supervised manner. Based on a validation study with\nexperimentally acquired data of healthy human volunteers, we show that\nautomatic tissue segmentation can be used to create powerful analyses and\nvisualisations of multispectral photoacoustic images. Due to the intuitive\nrepresentation of high-dimensional information, such a processing algorithm\ncould be a valuable means to facilitate the clinical translation of\nphotoacoustic imaging.",
          "link": "http://arxiv.org/abs/2105.09624",
          "publishedOn": "2021-05-23T06:08:17.485Z",
          "wordCount": 608,
          "title": "Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barakat_B/0/1/0/all/0/1\">Berat Kurar Barakat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droby_A/0/1/0/all/0/1\">Ahmad Droby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saabni_R/0/1/0/all/0/1\">Raid Saabni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sana_J/0/1/0/all/0/1\">Jihad El-Sana</a>",
          "description": "Despite recent advances in the field of supervised deep learning for text\nline segmentation, unsupervised deep learning solutions are beginning to gain\npopularity. In this paper, we present an unsupervised deep learning method that\nembeds document image patches to a compact Euclidean space where distances\ncorrespond to a coarse text line pattern similarity. Once this space has been\nproduced, text line segmentation can be easily implemented using standard\ntechniques with the embedded feature vectors. To train the model, we extract\nrandom pairs of document image patches with the assumption that neighbour\npatches contain a similar coarse trend of text lines, whereas if one of them is\nrotated, they contain different coarse trends of text lines. Doing well on this\ntask requires the model to learn to recognize the text lines and their salient\nparts. The benefit of our approach is zero manual labelling effort. We evaluate\nthe method qualitatively and quantitatively on several variants of text line\nsegmentation datasets to demonstrate its effectivity.",
          "link": "http://arxiv.org/abs/2105.09405",
          "publishedOn": "2021-05-23T06:08:17.479Z",
          "wordCount": 596,
          "title": "Unsupervised learning of text line segmentationby differentiating coarse patterns. (arXiv:2105.09405v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shijie Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tong Lin</a>",
          "description": "Recently, collaborative learning proposed by Song and Chai has achieved\nremarkable improvements in image classification tasks by simultaneously\ntraining multiple classifier heads. However, huge memory footprints required by\nsuch multi-head structures may hinder the training of large-capacity baseline\nmodels. The natural question is how to achieve collaborative learning within a\nsingle network without duplicating any modules. In this paper, we propose four\nways of collaborative learning among different parts of a single network with\nnegligible engineering efforts. To improve the robustness of the network, we\nleverage the consistency of the output layer and intermediate layers for\ntraining under the collaborative learning framework. Besides, the similarity of\nintermediate representation and convolution kernel is also introduced to reduce\nthe reduce redundant in a neural network. Compared to the method of Song and\nChai, our framework further considers the collaboration inside a single model\nand takes smaller overhead. Extensive experiments on Cifar-10, Cifar-100,\nImageNet32 and STL-10 corroborate the effectiveness of these four ways\nseparately while combining them leads to further improvements. In particular,\ntest errors on the STL-10 dataset are decreased by $9.28\\%$ and $5.45\\%$ for\nResNet-18 and VGG-16 respectively. Moreover, our method is proven to be robust\nto label noise with experiments on Cifar-10 dataset. For example, our method\nhas $3.53\\%$ higher performance under $50\\%$ noise ratio setting.",
          "link": "http://arxiv.org/abs/2105.09590",
          "publishedOn": "2021-05-23T06:08:17.472Z",
          "wordCount": 638,
          "title": "Intra-Model Collaborative Learning of Neural Networks. (arXiv:2105.09590v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhibo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuchen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>",
          "description": "Recently few-shot object detection is widely adopted to deal with\ndata-limited situations. While most previous works merely focus on the\nperformance on few-shot categories, we claim that detecting all classes is\ncrucial as test samples may contain any instances in realistic applications,\nwhich requires the few-shot detector to learn new concepts without forgetting.\nThrough analysis on transfer learning based methods, some neglected but\nbeneficial properties are utilized to design a simple yet effective few-shot\ndetector, Retentive R-CNN. It consists of Bias-Balanced RPN to debias the\npretrained RPN and Re-detector to find few-shot class objects without\nforgetting previous knowledge. Extensive experiments on few-shot detection\nbenchmarks show that Retentive R-CNN significantly outperforms state-of-the-art\nmethods on overall performance among all settings as it can achieve competitive\nresults on few-shot classes and does not degrade the base class performance at\nall. Our approach has demonstrated that the long desired never-forgetting\nlearner is available in object detection.",
          "link": "http://arxiv.org/abs/2105.09491",
          "publishedOn": "2021-05-23T06:08:17.466Z",
          "wordCount": 583,
          "title": "Generalized Few-Shot Object Detection without Forgetting. (arXiv:2105.09491v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Heming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Object goal navigation aims to steer an agent towards a target object based\non observations of the agent. It is of pivotal importance to design effective\nvisual representations of the observed scene in determining navigation actions.\nIn this paper, we introduce a Visual Transformer Network (VTNet) for learning\ninformative visual representation in navigation. VTNet is a highly effective\nstructure that embodies two key properties for visual representations: First,\nthe relationships among all the object instances in a scene are exploited;\nSecond, the spatial locations of objects and image regions are emphasized so\nthat directional navigation signals can be learned. Furthermore, we also\ndevelop a pre-training scheme to associate the visual representations with\nnavigation signals, and thus facilitate navigation policy learning. In a\nnutshell, VTNet embeds object and region features with their location cues as\nspatial-aware descriptors and then incorporates all the encoded descriptors\nthrough attention operations to achieve informative representation for\nnavigation. Given such visual representations, agents are able to explore the\ncorrelations between visual observations and navigation actions. For example,\nan agent would prioritize \"turning right\" over \"turning left\" when the visual\nrepresentation emphasizes on the right side of activation map. Experiments in\nthe artificial environment AI2-Thor demonstrate that VTNet significantly\noutperforms state-of-the-art methods in unseen testing environments.",
          "link": "http://arxiv.org/abs/2105.09447",
          "publishedOn": "2021-05-23T06:08:17.458Z",
          "wordCount": 642,
          "title": "VTNet: Visual Transformer Network for Object Goal Navigation. (arXiv:2105.09447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoyue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Song Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">S.-H. Gary Chan</a>",
          "description": "Labeled crowd scene images are expensive and scarce. To significantly reduce\nthe requirement of the labeled images, we propose ColorCount, a novel CNN-based\napproach by combining self-supervised transfer colorization learning and global\nprior classification to leverage the abundantly available unlabeled data. The\nself-supervised colorization branch learns the semantics and surface texture of\nthe image by using its color components as pseudo labels. The classification\nbranch extracts global group priors by learning correlations among image\nclusters. Their fused resultant discriminative features (global priors,\nsemantics and textures) provide ample priors for counting, hence significantly\nreducing the requirement of labeled images. We conduct extensive experiments on\nfour challenging benchmarks. ColorCount achieves much better performance as\ncompared with other unsupervised approaches. Its performance is close to the\nsupervised baseline with substantially less labeled data (10\\% of the original\none).",
          "link": "http://arxiv.org/abs/2105.09684",
          "publishedOn": "2021-05-23T06:08:17.452Z",
          "wordCount": 570,
          "title": "Crowd Counting by Self-supervised Transfer Colorization Learning and Global Prior Classification. (arXiv:2105.09684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1\">Mayukh Bagchi</a>",
          "description": "We assume that substances in the world are represented by two types of\nconcepts, namely substance concepts and classification concepts, the former\ninstrumental to (visual) perception, the latter to (language based)\nclassification. Based on this distinction, we introduce a general methodology\nfor building lexico-semantic hierarchies of substance concepts, where nodes are\nannotated with the media, e.g.,videos or photos, from which substance concepts\nare extracted, and are associated with the corresponding classification\nconcepts. The methodology is based on Ranganathan's original faceted approach,\ncontextualized to the problem of classifying substance concepts. The key\nnovelty is that the hierarchy is built exploiting the visual properties of\nsubstance concepts, while the linguistically defined properties of\nclassification concepts are only used to describe substance concepts. The\nvalidity of the approach is exemplified by providing some highlights of an\nongoing project whose goal is to build a large scale multimedia multilingual\nconcept hierarchy.",
          "link": "http://arxiv.org/abs/2105.09422",
          "publishedOn": "2021-05-23T06:08:17.426Z",
          "wordCount": 567,
          "title": "Classifying concepts via visual properties. (arXiv:2105.09422v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1\">Pengxiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Joint relation modeling is a curial component in human motion prediction.\nMost existing methods tend to design skeletal-based graphs to build the\nrelations among joints, where local interactions between joint pairs are well\nlearned. However, the global coordination of all joints, which reflects human\nmotion's balance property, is usually weakened because it is learned from part\nto whole progressively and asynchronously. Thus, the final predicted motions\nare sometimes unnatural. To tackle this issue, we learn a medium, called\nbalance attractor (BA), from the spatiotemporal features of motion to\ncharacterize the global motion features, which is subsequently used to build\nnew joint relations. Through the BA, all joints are related synchronously, and\nthus the global coordination of all joints can be better learned. Based on the\nBA, we propose our framework, referred to Attractor-Guided Neural Network,\nmainly including Attractor-Based Joint Relation Extractor (AJRE) and\nMulti-timescale Dynamics Extractor (MTDE). The AJRE mainly includes Global\nCoordination Extractor (GCE) and Local Interaction Extractor (LIE). The former\npresents the global coordination of all joints, and the latter encodes local\ninteractions between joint pairs. The MTDE is designed to extract dynamic\ninformation from raw position information for effective prediction. Extensive\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods in both short and long-term predictions in H3.6M, CMU-Mocap, and 3DPW.",
          "link": "http://arxiv.org/abs/2105.09711",
          "publishedOn": "2021-05-23T06:08:17.418Z",
          "wordCount": 642,
          "title": "An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction. (arXiv:2105.09711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam V. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1\">Zhongliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>",
          "description": "Camouflaged objects attempt to conceal their texture into the background and\ndiscriminating them from the background is hard even for human beings. The main\nobjective of this paper is to explore the camouflaged object segmentation\nproblem, namely, segmenting the camouflaged object(s) for a given image. This\nproblem has not been well studied in spite of a wide range of potential\napplications including the preservation of wild animals and the discovery of\nnew species, surveillance systems, search-and-rescue missions in the event of\nnatural disasters such as earthquakes, floods or hurricanes. This paper\naddresses a new challenging problem of camouflaged object segmentation. To\naddress this problem, we provide a new image dataset of camouflaged objects for\nbenchmarking purposes. In addition, we propose a general end-to-end network,\ncalled the Anabranch Network, that leverages both classification and\nsegmentation tasks. Different from existing networks for segmentation, our\nproposed network possesses the second branch for classification to predict the\nprobability of containing camouflaged object(s) in an image, which is then\nfused into the main branch for segmentation to boost up the segmentation\naccuracy. Extensive experiments conducted on the newly built dataset\ndemonstrate the effectiveness of our network using various fully convolutional\nnetworks. \\url{https://sites.google.com/view/ltnghia/research/camo}",
          "link": "http://arxiv.org/abs/2105.09451",
          "publishedOn": "2021-05-23T06:08:17.411Z",
          "wordCount": 649,
          "title": "Anabranch Network for Camouflaged Object Segmentation. (arXiv:2105.09451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rundi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changxi Zheng</a>",
          "description": "Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation -- describing a shape as\na sequence of computer-aided design (CAD) operations. Unlike meshes and point\nclouds, CAD models encode the user creation process of 3D shapes, widely used\nin numerous industrial and engineering design tasks. However, the sequential\nand irregular structure of CAD operations poses significant challenges for\nexisting 3D generative models. Drawing an analogy between CAD operations and\nnatural language, we propose a CAD generative network based on the Transformer.\nWe demonstrate the performance of our model for both shape autoencoding and\nrandom shape generation. To train our network, we create a new CAD dataset\nconsisting of 179,133 models and their CAD construction sequences. We have made\nthis dataset publicly available to promote future research on this topic.",
          "link": "http://arxiv.org/abs/2105.09492",
          "publishedOn": "2021-05-23T06:08:17.404Z",
          "wordCount": 608,
          "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Dengqiang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shangqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qunlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xinzhe Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Registration networks have shown great application potentials in medical\nimage analysis. However, supervised training methods have a great demand for\nlarge and high-quality labeled datasets, which is time-consuming and sometimes\nimpractical due to data sharing issues. Unsupervised image registration\nalgorithms commonly employ intensity-based similarity measures as loss\nfunctions without any manual annotations. These methods estimate the\nparameterized transformations between pairs of moving and fixed images through\nthe optimization of the network parameters during training. However, these\nmethods become less effective when the image quality varies, e.g., some images\nare corrupted by substantial noise or artifacts. In this work, we propose a\nnovel approach based on a low-rank representation, i.e., Regnet-LRR, to tackle\nthe problem. We project noisy images into a noise-free low-rank space, and then\ncompute the similarity between the images. Based on the low-rank similarity\nmeasure, we train the registration network to predict the dense deformation\nfields of noisy image pairs. We highlight that the low-rank projection is\nreformulated in a way that the registration network can successfully update\ngradients. With two tasks, i.e., cardiac and abdominal intra-modality\nregistration, we demonstrate that the low-rank representation can boost the\ngeneralization ability and robustness of models as well as bring significant\nimprovements in noisy data registration scenarios.",
          "link": "http://arxiv.org/abs/2105.09548",
          "publishedOn": "2021-05-23T06:08:17.397Z",
          "wordCount": 645,
          "title": "A low-rank representation for unsupervised registration of medical images. (arXiv:2105.09548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Rui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>",
          "description": "Attention mechanisms have been widely applied to cross-modal tasks such as\nimage captioning and information retrieval, and have achieved remarkable\nimprovements due to its capability to learn fine-grained relevance across\ndifferent modalities. However, existing attention models could be sub-optimal\nand lack preciseness because there is no direct supervision involved during\ntraining. In this work, we propose Contrastive Content Re-sourcing (CCR) and\nContrastive Content Swapping (CCS) constraints to address such limitation.\nThese constraints supervise the training of attention models in a contrastive\nlearning manner without requiring explicit attention annotations. Additionally,\nwe introduce three metrics, namely Attention Precision, Recall and F1-Score, to\nquantitatively evaluate the attention quality. We evaluate the proposed\nconstraints with cross-modal retrieval (image-text matching) task. The\nexperiments on both Flickr30k and MS-COCO datasets demonstrate that integrating\nthese attention constraints into two state-of-the-art attention-based models\nimproves the model performance in terms of both retrieval accuracy and\nattention metrics.",
          "link": "http://arxiv.org/abs/2105.09597",
          "publishedOn": "2021-05-23T06:08:17.378Z",
          "wordCount": 587,
          "title": "More Than Just Attention: Learning Cross-Modal Attentions with Contrastive Constraints. (arXiv:2105.09597v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "Superpixels are higher-order perceptual groups of pixels in an image, often\ncarrying much more information than raw pixels. There is an inherent relational\nstructure to the relationship among different superpixels of an image. This\nrelational information can convey some form of domain information about the\nimage, e.g. relationship between superpixels representing two eyes in a cat\nimage. Our interest in this paper is to construct computer vision models,\nspecifically those based on Deep Neural Networks (DNNs) to incorporate these\nsuperpixels information. We propose a methodology to construct a hybrid model\nthat leverages (a) Convolutional Neural Network (CNN) to deal with spatial\ninformation in an image, and (b) Graph Neural Network (GNN) to deal with\nrelational superpixel information in the image. The proposed deep model is\nlearned using a generic hybrid loss function that we call a `hybrid' loss. We\nevaluate the predictive performance of our proposed hybrid vision model on four\npopular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.\nMoreover, we evaluate our method on three real-world classification tasks:\nCOVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint\nIdentification. The results demonstrate that the relational superpixel\ninformation provided via a GNN could improve the performance of standard\nCNN-based vision systems.",
          "link": "http://arxiv.org/abs/2105.09448",
          "publishedOn": "2021-05-23T06:08:17.371Z",
          "wordCount": 693,
          "title": "Superpixel-based Domain-Knowledge Infusion in Computer Vision. (arXiv:2105.09448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuncong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>",
          "description": "Recently, plenty of work has tried to introduce transformers into computer\nvision tasks, with good results. Unlike classic convolution networks, which\nextract features within a local receptive field, transformers can adaptively\naggregate similar features from a global view using self-attention mechanism.\nFor object detection, Feature Pyramid Network (FPN) proposes feature\ninteraction across layers and proves its extremely importance. However, its\ninteraction is still in a local manner, which leaves a lot of room for\nimprovement. Since transformer was originally designed for NLP tasks, adapting\nprocessing subject directly from text to image will cause unaffordable\ncomputation and space overhead. In this paper, we utilize a linearized\nattention function to overcome above problems and build a novel architecture,\nnamed Content-Augmented Feature Pyramid Network (CA-FPN), which proposes a\nglobal content extraction module and deeply combines with FPN through light\nlinear transformers. What's more, light transformers can further make the\napplication of multi-head attention mechanism easier. Most importantly, our\nCA-FPN can be readily plugged into existing FPN-based models. Extensive\nexperiments on the challenging COCO object detection dataset demonstrated that\nour CA-FPN significantly outperforms competitive baselines without bells and\nwhistles. Code will be made publicly available.",
          "link": "http://arxiv.org/abs/2105.09464",
          "publishedOn": "2021-05-23T06:08:17.363Z",
          "wordCount": 634,
          "title": "Content-Augmented Feature Pyramid Network with Light Linear Transformers. (arXiv:2105.09464v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1\">Ruhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1\">Kaida Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Laili Zhu</a>",
          "description": "Recently, the anchor-free object detection model has shown great potential\nfor accuracy and speed to exceed anchor-based object detection. Therefore, two\nissues are mainly studied in this article: (1) How to let the backbone network\nin the anchor-free object detection model learn feature extraction? (2) How to\nmake better use of the feature pyramid network? In order to solve the above\nproblems, Experiments show that our model has a certain improvement in accuracy\ncompared with the current popular detection models on the COCO dataset, the\ndesigned attention mechanism module can capture contextual information well,\nimprove detection accuracy, and use sepc network to help balance abstract and\ndetailed information, and reduce the problem of semantic gap in the feature\npyramid network. Whether it is anchor-based network model YOLOv3, Faster RCNN,\nor anchor-free network model Foveabox, FSAF, FCOS. Our optimal model can get\n39.5% COCO AP under the background of ResNet50.",
          "link": "http://arxiv.org/abs/2105.09596",
          "publishedOn": "2021-05-23T06:08:17.355Z",
          "wordCount": 596,
          "title": "AGSFCOS: Based on attention mechanism and Scale-Equalizing pyramid network of object detection. (arXiv:2105.09596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lingni Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somasundaram_K/0/1/0/all/0/1\">Kiran Somasundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>",
          "description": "Given a video captured from a first person perspective and recorded in a\nfamiliar environment, can we recognize what the person is doing and identify\nwhere the action occurs in the 3D space? We address this challenging problem of\njointly recognizing and localizing actions of a mobile user on a known 3D map\nfrom egocentric videos. To this end, we propose a novel deep probabilistic\nmodel. Our model takes the inputs of a Hierarchical Volumetric Representation\n(HVR) of the environment and an egocentric video, infers the 3D action location\nas a latent variable, and recognizes the action based on the video and\ncontextual cues surrounding its potential locations. To evaluate our model, we\nconduct extensive experiments on a newly collected egocentric video dataset, in\nwhich both human naturalistic actions and photo-realistic 3D environment\nreconstructions are captured. Our method demonstrates strong results on both\naction recognition and 3D action localization across seen and unseen\nenvironments. We believe our work points to an exciting research direction in\nthe intersection of egocentric vision, and 3D scene understanding.",
          "link": "http://arxiv.org/abs/2105.09544",
          "publishedOn": "2021-05-23T06:08:17.347Z",
          "wordCount": 612,
          "title": "Egocentric Activity Recognition and Localization on a 3D Map. (arXiv:2105.09544v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangeh_M/0/1/0/all/0/1\">Mehrdad J Gangeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plata_M/0/1/0/all/0/1\">Marcin Plata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motahari_H/0/1/0/all/0/1\">Hamid Motahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel P Duffy</a>",
          "description": "Removing noise from scanned pages is a vital step before their submission to\noptical character recognition (OCR) system. Most available image denoising\nmethods are supervised where the pairs of noisy/clean pages are required.\nHowever, this assumption is rarely met in real settings. Besides, there is no\nsingle model that can remove various noise types from documents. Here, we\npropose a unified end-to-end unsupervised deep learning model, for the first\ntime, that can effectively remove multiple types of noise, including salt \\&\npepper noise, blurred and/or faded text, as well as watermarks from documents\nat various levels of intensity. We demonstrate that the proposed model\nsignificantly improves the quality of scanned images and the OCR of the pages\non several test datasets.",
          "link": "http://arxiv.org/abs/2105.09437",
          "publishedOn": "2021-05-23T06:08:17.320Z",
          "wordCount": 551,
          "title": "End-to-End Unsupervised Document Image Blind Denoising. (arXiv:2105.09437v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yada Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>",
          "description": "With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and characterized with multiple labels,\nthus exhibiting the co-existence of multiple types of heterogeneity. Although\nstate-of-the-art techniques are good at modeling the complex heterogeneity with\nsufficient label information, such label information can be quite expensive to\nobtain in real applications, leading to sub-optimal performance using these\ntechniques. Inspired by the capability of contrastive learning to utilize rich\nunlabeled data for improving performance, in this paper, we propose a unified\nheterogeneous learning framework, which combines both weighted unsupervised\ncontrastive loss and weighted supervised contrastive loss to model multiple\ntypes of heterogeneity. We also provide theoretical analyses showing that the\nproposed weighted supervised contrastive loss is the lower bound of the mutual\ninformation of two samples from the same class and the weighted unsupervised\ncontrastive loss is the lower bound of the mutual information between the\nhidden representation of two views of the same sample. Experimental results on\nreal-world data sets demonstrate the effectiveness and the efficiency of the\nproposed method modeling multiple types of heterogeneity.",
          "link": "http://arxiv.org/abs/2105.09401",
          "publishedOn": "2021-05-23T06:08:17.301Z",
          "wordCount": 613,
          "title": "Heterogeneous Contrastive Learning. (arXiv:2105.09401v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolotouros_N/0/1/0/all/0/1\">Nikos Kolotouros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badger_M/0/1/0/all/0/1\">Marc Badger</a>",
          "description": "Animals are diverse in shape, but building a deformable shape model for a new\nspecies is not always possible due to the lack of 3D data. We present a method\nto capture new species using an articulated template and images of that\nspecies. In this work, we focus mainly on birds. Although birds represent\nalmost twice the number of species as mammals, no accurate shape model is\navailable. To capture a novel species, we first fit the articulated template to\neach training sample. By disentangling pose and shape, we learn a shape space\nthat captures variation both among species and within each species from image\nevidence. We learn models of multiple species from the CUB dataset, and\ncontribute new species-specific and multi-species shape models that are useful\nfor downstream reconstruction tasks. Using a low-dimensional embedding, we show\nthat our learned 3D shape space better reflects the phylogenetic relationships\namong birds than learned perceptual features.",
          "link": "http://arxiv.org/abs/2105.09396",
          "publishedOn": "2021-05-23T06:08:17.274Z",
          "wordCount": 596,
          "title": "Birds of a Feather: Capturing Avian Shape Models from Images. (arXiv:2105.09396v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09378",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gadjimuradov_F/0/1/0/all/0/1\">Fasil Gadjimuradov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benkert_T/0/1/0/all/0/1\">Thomas Benkert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nickel_M/0/1/0/all/0/1\">Marcel Dominik Nickel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>",
          "description": "Purpose: To develop an algorithm for robust partial Fourier (PF)\nreconstruction applicable to diffusion-weighted (DW) images with non-smooth\nphase variations.\n\nMethods: Based on an unrolled proximal splitting algorithm, a neural network\narchitecture is derived which alternates between data consistency operations\nand regularization implemented by recurrent convolutions. In order to exploit\ncorrelations, multiple repetitions of the same slice are jointly reconstructed\nunder consideration of permutation-equivariance. The proposed method is trained\non DW liver data of 60 volunteers and evaluated on retrospectively and\nprospectively sub-sampled data of different anatomies and resolutions. In\naddition, the benefits of using a recurrent network over other unrolling\nstrategies is investigated.\n\nResults: Conventional PF techniques can be significantly outperformed in\nterms of quantitative measures as well as perceptual image quality. The\nproposed method is able to generalize well to brain data with contrasts and\nresolution not present in the training set. The reduction in echo time (TE)\nassociated with prospective PF-sampling enables DW imaging with higher signal.\nAlso, the TE increase in acquisitions with higher resolution can be compensated\nfor. It can be shown that unrolling by means of a recurrent network produced\nbetter results than using a weight-shared network or a cascade of networks.\n\nConclusion: This work demonstrates that robust PF reconstruction of DW data\nis feasible even at strong PF factors in applications with severe phase\nvariations. Since the proposed method does not rely on smoothness priors of the\nphase but uses learned recurrent convolutions instead, artifacts of\nconventional PF methods can be avoided.",
          "link": "http://arxiv.org/abs/2105.09378",
          "publishedOn": "2021-05-23T06:08:17.262Z",
          "wordCount": 705,
          "title": "Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network. (arXiv:2105.09378v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halperin_T/0/1/0/all/0/1\">Tavi Halperin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakim_H/0/1/0/all/0/1\">Hanit Hakim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vantzos_O/0/1/0/all/0/1\">Orestis Vantzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochman_G/0/1/0/all/0/1\">Gershon Hochman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_N/0/1/0/all/0/1\">Netai Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassy_L/0/1/0/all/0/1\">Lior Sassy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupchik_M/0/1/0/all/0/1\">Michael Kupchik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_O/0/1/0/all/0/1\">Ofir Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>",
          "description": "We present an algorithm for producing a seamless animated loop from a single\nimage. The algorithm detects periodic structures, such as the windows of a\nbuilding or the steps of a staircase, and generates a non-trivial displacement\nvector field that maps each segment of the structure onto a neighboring segment\nalong a user- or auto-selected main direction of motion. This displacement\nfield is used, together with suitable temporal and spatial smoothing, to warp\nthe image and produce the frames of a continuous animation loop. Our\ncinemagraphs are created in under a second on a mobile device. Over 140,000\nusers downloaded our app and exported over 350,000 cinemagraphs. Moreover, we\nconducted two user studies that show that users prefer our method for creating\nsurreal and structured cinemagraphs compared to more manual approaches and\ncompared to previous methods.",
          "link": "http://arxiv.org/abs/2105.09374",
          "publishedOn": "2021-05-23T06:08:17.231Z",
          "wordCount": 608,
          "title": "Endless Loops: Detecting and Animating Periodic Patterns in Still Images. (arXiv:2105.09374v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09365",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Uysal_E/0/1/0/all/0/1\">Enes Sadi Uysal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilici_M/0/1/0/all/0/1\">M.&#x15e;afak Bilici</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaza_B/0/1/0/all/0/1\">B. Selin Zaza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozgenc_M/0/1/0/all/0/1\">M. Yi&#x11f;it &#xd6;zgen&#xe7;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyar_O/0/1/0/all/0/1\">Onur Boyar</a>",
          "description": "Retinal Vessel Segmentation is important for diagnosis of various diseases.\nThe research on retinal vessel segmentation focuses mainly on improvement of\nthe segmentation model which is usually based on U-Net architecture. In our\nstudy we use the U-Net architecture and we rely on heavy data augmentation in\norder to achieve better performance. The success of the data augmentation\nrelies on successfully addressing the problem of input images. By analyzing\ninput images and performing the augmentation accordingly we show that the\nperformance of the U-Net model can be increased dramatically. Results are\nreported using the most widely used retina dataset, DRIVE.",
          "link": "http://arxiv.org/abs/2105.09365",
          "publishedOn": "2021-05-23T06:08:17.203Z",
          "wordCount": 564,
          "title": "Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation. (arXiv:2105.09365v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuesu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>",
          "description": "While imitation learning for vision based autonomous mobile robot navigation\nhas recently received a great deal of attention in the research community,\nexisting approaches typically require state action demonstrations that were\ngathered using the deployment platform. However, what if one cannot easily\noutfit their platform to record these demonstration signals or worse yet the\ndemonstrator does not have access to the platform at all? Is imitation learning\nfor vision based autonomous navigation even possible in such scenarios? In this\nwork, we hypothesize that the answer is yes and that recent ideas from the\nImitation from Observation (IfO) literature can be brought to bear such that a\nrobot can learn to navigate using only ego centric video collected by a\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\nintroduce a new algorithm, Visual Observation only Imitation Learning for\nAutonomous navigation (VOILA), that can successfully learn navigation policies\nfrom a single video demonstration collected from a physically different agent.\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\nnot only successfully imitates the expert, but that it also learns navigation\npolicies that can generalize to novel environments. Further, we demonstrate the\neffectiveness of VOILA in a real world setting by showing that it allows a\nwheeled Jackal robot to successfully imitate a human walking in an environment\nusing a video recorded using a mobile phone camera.",
          "link": "http://arxiv.org/abs/2105.09371",
          "publishedOn": "2021-05-23T06:08:17.176Z",
          "wordCount": 671,
          "title": "VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.",
          "link": "http://arxiv.org/abs/2105.09356",
          "publishedOn": "2021-05-23T06:08:17.161Z",
          "wordCount": 621,
          "title": "Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v1 [cs.LG])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2007.13693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pares_F/0/1/0/all/0/1\">Ferran Par&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_Duart_A/0/1/0/all/0/1\">Anna Arias-Duart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1\">Dario Garcia-Gasulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campo_Frances_G/0/1/0/all/0/1\">Gema Campo-Franc&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viladrich_N/0/1/0/all/0/1\">Nina Viladrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1\">Eduard Ayguad&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1\">Jes&#xfa;s Labarta</a>",
          "description": "In the image classification task, the most common approach is to resize all\nimages in a dataset to a unique shape, while reducing their precision to a size\nwhich facilitates experimentation at scale. This practice has benefits from a\ncomputational perspective, but it entails negative side-effects on performance\ndue to loss of information and image deformation. In this work we introduce the\nMAMe dataset, an image classification dataset with remarkable high resolution\nand variable shape properties. The goal of MAMe is to provide a tool for\nstudying the impact of such properties in image classification, while\nmotivating research in the field. The MAMe dataset contains thousands of\nartworks from three different museums, and proposes a classification task\nconsisting on differentiating between 29 mediums (i.e. materials and\ntechniques) supervised by art experts. After reviewing the singularity of MAMe\nin the context of current image classification tasks, a thorough description of\nthe task is provided, together with dataset statistics. Experiments are\nconducted to evaluate the impact of using high resolution images, variable\nshape inputs and both properties at the same time. Results illustrate the\npositive impact in performance when using high resolution images, while\nhighlighting the lack of solutions to exploit variable shapes. An additional\nexperiment exposes the distinctiveness between the MAMe dataset and the\nprototypical ImageNet dataset. Finally, the baselines are inspected using\nexplainability methods and expert knowledge, to gain insights on the challenges\nthat remain ahead.",
          "link": "http://arxiv.org/abs/2007.13693",
          "publishedOn": "2021-05-23T06:10:40.919Z",
          "wordCount": 725,
          "title": "The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties. (arXiv:2007.13693v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09872",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yoon_J/0/1/0/all/0/1\">Jun Ho Yoon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kim_S/0/1/0/all/0/1\">Seyoung Kim</a>",
          "description": "In many real-world problems, complex dependencies are present both among\nsamples and among features. The Kronecker sum or the Cartesian product of two\ngraphs, each modeling dependencies across features and across samples, has been\nused as an inverse covariance matrix for a matrix-variate Gaussian\ndistribution, as an alternative to a Kronecker-product inverse covariance\nmatrix, due to its more intuitive sparse structure. However, the existing\nmethods for sparse Kronecker-sum inverse covariance estimation are limited in\nthat they do not scale to more than a few hundred features and samples and that\nthe unidentifiable parameters pose challenges in estimation. In this paper, we\nintroduce EiGLasso, a highly scalable method for sparse Kronecker-sum inverse\ncovariance estimation, based on Newton's method combined with\neigendecomposition of the two graphs for exploiting the structure of Kronecker\nsum. EiGLasso further reduces computation time by approximating the Hessian\nbased on the eigendecomposition of the sample and feature graphs. EiGLasso\nachieves quadratic convergence with the exact Hessian and linear convergence\nwith the approximate Hessian. We describe a simple new approach to estimating\nthe unidentifiable parameters that generalizes the existing methods. On\nsimulated and real-world data, we demonstrate that EiGLasso achieves two to\nthree orders-of-magnitude speed-up compared to the existing methods.",
          "link": "http://arxiv.org/abs/2105.09872",
          "publishedOn": "2021-05-23T06:10:40.912Z",
          "wordCount": 626,
          "title": "EiGLasso for Scalable Sparse Kronecker-Sum Inverse Covariance Estimation. (arXiv:2105.09872v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walton_N/0/1/0/all/0/1\">Neil Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kuang Xu</a>",
          "description": "We review the role of information and learning in the stability and\noptimization of queueing systems. In recent years, techniques from supervised\nlearning, bandit learning and reinforcement learning have been applied to\nqueueing systems supported by increasing role of information in decision\nmaking. We present observations and new results that help rationalize the\napplication of these areas to queueing systems.\n\nWe prove that the MaxWeight and BackPressure policies are an application of\nBlackwell's Approachability Theorem. This connects queueing theoretic results\nwith adversarial learning. We then discuss the requirements of statistical\nlearning for service parameter estimation. As an example, we show how queue\nsize regret can be bounded when applying a perceptron algorithm to classify\nservice. Next, we discuss the role of state information in improved decision\nmaking. Here we contrast the roles of epistemic information (information on\nuncertain parameters) and aleatoric information (information on an uncertain\nstate). Finally we review recent advances in the theory of reinforcement\nlearning and queueing, as well as, provide discussion on current research\nchallenges.",
          "link": "http://arxiv.org/abs/2105.08769",
          "publishedOn": "2021-05-23T06:10:40.895Z",
          "wordCount": 609,
          "title": "Learning and Information in Stochastic Networks and Queues. (arXiv:2105.08769v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kefato_Z/0/1/0/all/0/1\">Zekarias T. Kefato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdzijauskas_S/0/1/0/all/0/1\">Sarunas Girdzijauskas</a>",
          "description": "Real world data is mostly unlabeled or only few instances are labeled.\nManually labeling data is a very expensive and daunting task. This calls for\nunsupervised learning techniques that are powerful enough to achieve comparable\nresults as semi-supervised/supervised techniques. Contrastive self-supervised\nlearning has emerged as a powerful direction, in some cases outperforming\nsupervised techniques. In this study, we propose, SelfGNN, a novel contrastive\nself-supervised graph neural network (GNN) without relying on explicit\ncontrastive terms. We leverage Batch Normalization, which introduces implicit\ncontrastive terms, without sacrificing performance. Furthermore, as data\naugmentation is key in contrastive learning, we introduce four feature\naugmentation (FA) techniques for graphs. Though graph topological augmentation\n(TA) is commonly used, our empirical findings show that FA perform as good as\nTA. Moreover, FA incurs no computational overhead, unlike TA, which often has\nO(N^3) time complexity, N-number of nodes. Our empirical evaluation on seven\npublicly available real-world data shows that, SelfGNN is powerful and leads to\na performance comparable with SOTA supervised GNNs and always better than SOTA\nsemi-supervised and unsupervised GNNs. The source code is available at\nhttps://github.com/zekarias-tilahun/SelfGNN.",
          "link": "http://arxiv.org/abs/2103.14958",
          "publishedOn": "2021-05-23T06:10:40.871Z",
          "wordCount": 666,
          "title": "Self-supervised Graph Neural Networks without explicit negative sampling. (arXiv:2103.14958v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>",
          "description": "Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of available\nEnglish datasets by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in low-resource languages. We project\npredictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,\nSpanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in\nTRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in\nOffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513\nF1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our\napproach compares favourably to the best systems submitted to recent shared\ntasks on these three languages. Additionally, we report competitive performance\non Arabic, and Turkish using the training and development sets of OffensEval\n2020 shared task. The results for all languages confirm the robustness of\ncross-lingual contextual embeddings and transfer learning for this task.",
          "link": "http://arxiv.org/abs/2105.05996",
          "publishedOn": "2021-05-23T06:10:40.860Z",
          "wordCount": 695,
          "title": "Multilingual Offensive Language Identification for Low-resource Languages. (arXiv:2105.05996v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingsheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>",
          "description": "Federated learning enables multiple parties to collaboratively learn a model\nwithout exchanging their data. While most existing federated learning\nalgorithms need many rounds to converge, one-shot federated learning (i.e.,\nfederated learning with a single communication round) is a promising approach\nto make federated learning applicable in cross-silo setting in practice.\nHowever, existing one-shot algorithms only support specific models and do not\nprovide any privacy guarantees, which significantly limit the applications in\npractice. In this paper, we propose a practical one-shot federated learning\nalgorithm named FedKT. By utilizing the knowledge transfer technique, FedKT can\nbe applied to any classification models and can flexibly achieve differential\nprivacy guarantees. Our experiments on various tasks show that FedKT can\nsignificantly outperform the other state-of-the-art federated learning\nalgorithms with a single communication round.",
          "link": "http://arxiv.org/abs/2010.01017",
          "publishedOn": "2021-05-23T06:10:40.852Z",
          "wordCount": 581,
          "title": "Practical One-Shot Federated Learning for Cross-Silo Setting. (arXiv:2010.01017v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuqing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1\">Olivia Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>",
          "description": "Policies trained in simulation often fail when transferred to the real world\ndue to the `reality gap' where the simulator is unable to accurately capture\nthe dynamics and visual properties of the real world. Current approaches to\ntackle this problem, such as domain randomization, require prior knowledge and\nengineering to determine how much to randomize system parameters in order to\nlearn a policy that is robust to sim-to-real transfer while also not being too\nconservative. We propose a method for automatically tuning simulator system\nparameters to match the real world using only raw RGB images of the real world\nwithout the need to define rewards or estimate state. Our key insight is to\nreframe the auto-tuning of parameters as a search problem where we iteratively\nshift the simulation system parameters to approach the real-world system\nparameters. We propose a Search Param Model (SPM) that, given a sequence of\nobservations and actions and a set of system parameters, predicts whether the\ngiven parameters are higher or lower than the true parameters used to generate\nthe observations. We evaluate our method on multiple robotic control tasks in\nboth sim-to-sim and sim-to-real transfer, demonstrating significant improvement\nover naive domain randomization. Project videos and code at\nhttps://yuqingd.github.io/autotuned-sim2real/",
          "link": "http://arxiv.org/abs/2104.07662",
          "publishedOn": "2021-05-23T06:10:40.837Z",
          "wordCount": 681,
          "title": "Auto-Tuned Sim-to-Real Transfer. (arXiv:2104.07662v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yueyao Yu</a>",
          "description": "What makes an artificial neural network easier to train and more likely to\nproduce desirable solutions than other comparable networks? In this paper, we\nprovide a new angle to study such issues under the setting of a fixed number of\nmodel parameters which in general is the most dominant cost factor. We\nintroduce a notion of variability and show that it correlates positively to the\nactivation ratio and negatively to a phenomenon called {Collapse to Constants}\n(or C2C), which is closely related but not identical to the phenomenon commonly\nknown as vanishing gradient. Experiments on a styled model problem empirically\nverify that variability is indeed a key performance indicator for fully\nconnected neural networks. The insights gained from this variability study will\nhelp the design of new and effective neural network architectures.",
          "link": "http://arxiv.org/abs/2105.08911",
          "publishedOn": "2021-05-23T06:10:40.822Z",
          "wordCount": 564,
          "title": "Variability of Artificial Neural Networks. (arXiv:2105.08911v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_L/0/1/0/all/0/1\">Lekshmi Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_C/0/1/0/all/0/1\">Chandra R. Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1\">Himanshu Tyagi</a>",
          "description": "In the problem of multiple support recovery, we are given access to linear\nmeasurements of multiple sparse samples in $\\mathbb{R}^{d}$. These samples can\nbe partitioned into $\\ell$ groups, with samples having the same support\nbelonging to the same group. For a given budget of $m$ measurements per sample,\nthe goal is to recover the $\\ell$ underlying supports, in the absence of the\nknowledge of group labels. We study this problem with a focus on the\nmeasurement-constrained regime where $m$ is smaller than the support size $k$\nof each sample. We design a two-step procedure that estimates the union of the\nunderlying supports first, and then uses a spectral algorithm to estimate the\nindividual supports. Our proposed estimator can recover the supports with $m<k$\nmeasurements per sample, from $\\tilde{O}(k^{4}\\ell^{4}/m^{4})$ samples. Our\nguarantees hold for a general, generative model assumption on the samples and\nmeasurement matrices. We also provide results from experiments conducted on\nsynthetic data and on the MNIST dataset.",
          "link": "http://arxiv.org/abs/2105.09855",
          "publishedOn": "2021-05-23T06:10:40.816Z",
          "wordCount": 591,
          "title": "Multiple Support Recovery Using Very Few Measurements Per Sample. (arXiv:2105.09855v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08506",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1\">Sara Atito Ali Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1\">Mehmet Can Yavuz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sen_M/0/1/0/all/0/1\">Mehmet Umut Sen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulsen_F/0/1/0/all/0/1\">Fatih Gulsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tutar_O/0/1/0/all/0/1\">Onur Tutar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korkmazer_B/0/1/0/all/0/1\">Bora Korkmazer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samanci_C/0/1/0/all/0/1\">Cesur Samanci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirolu_S/0/1/0/all/0/1\">Sabri Sirolu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamid_R/0/1/0/all/0/1\">Rauf Hamid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eryurekli_A/0/1/0/all/0/1\">Ali Ergun Eryurekli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mammadov_T/0/1/0/all/0/1\">Toghrul Mammadov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yanikoglu_B/0/1/0/all/0/1\">Berrin Yanikoglu</a>",
          "description": "Detecting COVID-19 in computed tomography (CT) or radiography images has been\nproposed as a supplement to the definitive RT-PCR test. We present a deep\nlearning ensemble for detecting COVID-19 infection, combining slice-based (2D)\nand volume-based (3D) approaches. The 2D system detects the infection on each\nCT slice independently, combining them to obtain the patient-level decision via\ndifferent methods (averaging and long-short term memory networks). The 3D\nsystem takes the whole CT volume to arrive to the patient-level decision in one\nstep. A new high resolution chest CT scan dataset, called the IST-C dataset, is\nalso collected in this work. The proposed ensemble, called IST-CovNet, obtains\n90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting\nCOVID-19 among normal controls and other types of lung pathologies; and 93.69%\naccuracy and 0.99 AUC score on the publicly available MosMed dataset that\nconsists of COVID-19 scans and normal controls only. The system is deployed at\nIstanbul University Cerrahpasa School of Medicine.",
          "link": "http://arxiv.org/abs/2105.08506",
          "publishedOn": "2021-05-23T06:10:40.796Z",
          "wordCount": 693,
          "title": "COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches. (arXiv:2105.08506v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retrain. Our key innovation is to redefine\nthe gradient to a new synaptic parameter, allowing better exploration of\nnetwork structures by taking full advantage of the competition between pruning\nand regrowth of connections. The experimental results show that the proposed\nmethod achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset\nso far. Moreover, it reaches a $\\sim$3.5% accuracy loss under unprecedented\n0.73% connectivity, which reveals remarkable structure refining capability in\nSNNs. Our work suggests that there exists extremely high redundancy in deep\nSNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring .",
          "link": "http://arxiv.org/abs/2105.04916",
          "publishedOn": "2021-05-23T06:10:40.788Z",
          "wordCount": 696,
          "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhiwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dingyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bingchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yongxin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>",
          "description": "Large ride-hailing platforms, such as DiDi, Uber and Lyft, connect tens of\nthousands of vehicles in a city to millions of ride demands throughout the day,\nproviding great promises for improving transportation efficiency through the\ntasks of order dispatching and vehicle repositioning. Existing studies,\nhowever, usually consider the two tasks in simplified settings that hardly\naddress the complex interactions between the two, the real-time fluctuations\nbetween supply and demand, and the necessary coordinations due to the\nlarge-scale nature of the problem. In this paper we propose a unified\nvalue-based dynamic learning framework (V1D3) for tackling both tasks. At the\ncenter of the framework is a globally shared value function that is updated\ncontinuously using online experiences generated from real-time platform\ntransactions. To improve the sample-efficiency and the robustness, we further\npropose a novel periodic ensemble method combining the fast online learning\nwith a large-scale offline training scheme that leverages the abundant\nhistorical driver trajectory data. This allows the proposed framework to adapt\nquickly to the highly dynamic environment, to generalize robustly to recurrent\npatterns and to drive implicit coordinations among the population of managed\nvehicles. Extensive experiments based on real-world datasets show considerably\nimprovements over other recently proposed methods on both tasks. Particularly,\nV1D3 outperforms the first prize winners of both dispatching and repositioning\ntracks in the KDD Cup 2020 RL competition, achieving state-of-the-art results\non improving both total driver income and user experience related metrics.",
          "link": "http://arxiv.org/abs/2105.08791",
          "publishedOn": "2021-05-23T06:10:40.781Z",
          "wordCount": 705,
          "title": "Value Function is All You Need: A Unified Learning Framework for Ride Hailing Platforms. (arXiv:2105.08791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08147",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rister_B/0/1/0/all/0/1\">Blaine Rister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L. Rubin</a>",
          "description": "Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently\nobtained to determine the extent of lung disease and are a valuable source of\ndata for creating artificial intelligence models. Most work to date assessing\ndisease severity on chest imaging has focused on segmenting computed tomography\n(CT) images; however, given that CTs are performed much less frequently than\nchest X-rays for COVID-19 patients, automated lung lesion segmentation on chest\nX-rays could be clinically valuable. There currently exists a universal\nshortage of chest X-rays with ground truth COVID-19 lung lesion annotations,\nand manually contouring lung opacities is a tedious, labor-intensive task. To\naccelerate severity detection and augment the amount of publicly available\nchest X-ray training data for supervised deep learning (DL) models, we leverage\nexisting annotated CT images to generate frontal projection \"chest X-ray\"\nimages for training COVID-19 chest X-ray models. In this paper, we propose an\nautomated pipeline for segmentation of COVID-19 lung lesions on chest X-rays\ncomprised of a Mask R-CNN trained on a mixed dataset of open-source chest\nX-rays and coronal X-ray projections computed from annotated volumetric CTs. On\na test set containing 40 chest X-rays of COVID-19 positive patients, our model\nachieved IoU scores of 0.81 $\\pm$ 0.03 and 0.79 $\\pm$ 0.03 when trained on a\ndataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50\nprojections from CTs, respectively. Our model far outperforms current baselines\nwith limited supervised training and may assist in automated COVID-19 severity\nquantification on chest X-rays.",
          "link": "http://arxiv.org/abs/2105.08147",
          "publishedOn": "2021-05-23T06:10:40.774Z",
          "wordCount": 779,
          "title": "COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs. (arXiv:2105.08147v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Akul Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1\">Ethan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puranik_S/0/1/0/all/0/1\">Samir Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Horace He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "While programming is one of the most broadly applicable skills in modern\nsociety, modern machine learning models still cannot code solutions to basic\nproblems. It can be difficult to accurately assess code generation performance,\nand there has been surprisingly little work on evaluating code generation in a\nway that is both flexible and rigorous. To meet this challenge, we introduce\nAPPS, a benchmark for code generation. Unlike prior work in more restricted\nsettings, our benchmark measures the ability of models to take an arbitrary\nnatural language specification and generate Python code fulfilling this\nspecification. Similar to how companies assess candidate software developers,\nwe then evaluate models by checking their generated code on test cases. Our\nbenchmark includes 10,000 problems, which range from having simple one-line\nsolutions to being substantial algorithmic challenges. We fine-tune large\nlanguage models on both GitHub and our training set, and we find that the\nprevalence of syntax errors is decreasing exponentially. Recent models such as\nGPT-Neo can pass approximately 15% of the test cases of introductory problems,\nso we find that machine learning models are beginning to learn how to code. As\nthe social significance of automatic code generation increases over the coming\nyears, our benchmark can provide an important measure for tracking\nadvancements.",
          "link": "http://arxiv.org/abs/2105.09938",
          "publishedOn": "2021-05-23T06:10:40.767Z",
          "wordCount": 662,
          "title": "Measuring Coding Challenge Competence With APPS. (arXiv:2105.09938v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guandong Xu</a>",
          "description": "In recommendation systems, the existence of the missing-not-at-random (MNAR)\nproblem results in the selection bias issue, degrading the recommendation\nperformance ultimately. A common practice to address MNAR is to treat missing\nentries from the so-called \"exposure\" perspective, i.e., modeling how an item\nis exposed (provided) to a user. Most of the existing approaches use heuristic\nmodels or re-weighting strategy on observed ratings to mimic the\nmissing-at-random setting. However, little research has been done to reveal how\nthe ratings are missing from a causal perspective. To bridge the gap, we\npropose an unbiased and robust method called DENC (De-bias Network Confounding\nin Recommendation) inspired by confounder analysis in causal inference. In\ngeneral, DENC provides a causal analysis on MNAR from both the inherent factors\n(e.g., latent user or item factors) and auxiliary network's perspective.\nParticularly, the proposed exposure model in DENC can control the social\nnetwork confounder meanwhile preserves the observed exposure information. We\nalso develop a deconfounding model through the balanced representation learning\nto retain the primary user and item features, which enables DENC generalize\nwell on the rating prediction. Extensive experiments on three datasets validate\nthat our proposed model outperforms the state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2105.07775",
          "publishedOn": "2021-05-23T06:10:40.748Z",
          "wordCount": 632,
          "title": "Be Causal: De-biasing Social Network Confounding in Recommendation. (arXiv:2105.07775v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1\">Lan V. Truong</a>",
          "description": "We establish exact asymptotic expressions for the normalized mutual\ninformation and minimum mean-square-error (MMSE) of sparse linear regression in\nthe sub-linear sparsity regime. Our result is achieved by a generalization of\nthe adaptive interpolation method in Bayesian inference for linear regimes to\nsub-linear ones. A modification of the well-known approximate message passing\nalgorithm to approach the MMSE fundamental limit is also proposed, and its\nstate evolution is rigorously analysed. Our results show that the traditional\nlinear assumption between the signal dimension and number of observations in\nthe replica and adaptive interpolation methods is not necessary for sparse\nsignals. They also show how to modify the existing well-known AMP algorithms\nfor linear regimes to sub-linear ones.",
          "link": "http://arxiv.org/abs/2101.11156",
          "publishedOn": "2021-05-23T06:10:40.742Z",
          "wordCount": 596,
          "title": "Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1\">Sukhdeep S. Sodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1\">Ellie Ka-In Chio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1\">Ambarish Jash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apte_A/0/1/0/all/0/1\">Ajit Apte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeje_A/0/1/0/all/0/1\">Ayooluwakunmi Jeje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1\">Dima Kuzmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_H/0/1/0/all/0/1\">Harry Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Effrat_J/0/1/0/all/0/1\">Jon Effrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_T/0/1/0/all/0/1\">Tarush Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1\">Nitin Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sarvjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Senqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tameen Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wankhede_A/0/1/0/all/0/1\">Amol Wankhede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1\">Moustafa Alzantot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Allen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1\">Tushar Chandra</a>",
          "description": "As more and more online search queries come from voice, automatic speech\nrecognition becomes a key component to deliver relevant search results. Errors\nintroduced by automatic speech recognition (ASR) lead to irrelevant search\nresults returned to the user, thus causing user dissatisfaction. In this paper,\nwe introduce an approach, Mondegreen, to correct voice queries in text space\nwithout depending on audio signals, which may not always be available due to\nsystem constraints or privacy or bandwidth (for example, some ASR systems run\non-device) considerations. We focus on voice queries transcribed via several\nproprietary commercial ASR systems. These queries come from users making\ninternet, or online service search queries. We first present an analysis\nshowing how different the language distribution coming from user voice queries\nis from that in traditional text corpora used to train off-the-shelf ASR\nsystems. We then demonstrate that Mondegreen can achieve significant\nimprovements in increased user interaction by correcting user voice queries in\none of the largest search systems in Google. Finally, we see Mondegreen as\ncomplementing existing highly-optimized production ASR systems, which may not\nbe frequently retrained and thus lag behind due to vocabulary drifts.",
          "link": "http://arxiv.org/abs/2105.09930",
          "publishedOn": "2021-05-23T06:10:40.735Z",
          "wordCount": 678,
          "title": "Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries. (arXiv:2105.09930v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenwen Yu</a>",
          "description": "Intrusion detection has been a key topic in the field of cyber security, and\nthe common network threats nowadays have the characteristics of varieties and\nvariation. Considering the serious imbalance of intrusion detection datasets\nwill result in low classification performance on attack behaviors of small\nsample size and difficulty to detect network attacks accurately and\nefficiently, using Adaptive Synthetic Sampling (ADASYN) method to balance\ndatasets was proposed in this paper. In addition, Random Forest algorithm was\nused to train intrusion detection classifiers. Through the comparative\nexperiment of Intrusion detection on CICIDS 2017 dataset, it is found that\nADASYN with Random Forest performs better. Based on the experimental results,\nthe improvement of precision, recall, F1 scores and AUC values after ADASYN is\nthen analyzed. Experiments show that the proposed method can be applied to\nintrusion detection with large data, and can effectively improve the\nclassification accuracy of network attack behaviors. Compared with traditional\nmachine learning models, it has better performance, generalization ability and\nrobustness.",
          "link": "http://arxiv.org/abs/2105.04301",
          "publishedOn": "2021-05-23T06:10:40.728Z",
          "wordCount": 615,
          "title": "ADASYN-Random Forest Based Intrusion Detection Model. (arXiv:2105.04301v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianchen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chaosheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingyuan Deng</a>",
          "description": "In this paper, we investigate a new multi-armed bandit (MAB) online learning\nmodel that considers real-world phenomena in many recommender systems: (i) the\nlearning agent cannot pull the arms by itself and thus has to offer rewards to\nusers to incentivize arm-pulling indirectly; and (ii) if users with specific\narm preferences are well rewarded, they induce a \"self-reinforcing\" effect in\nthe sense that they will attract more users of similar arm preferences. Besides\naddressing the tradeoff of exploration and exploitation, another key feature of\nthis new MAB model is to balance reward and incentivizing payment. The goal of\nthe agent is to maximize the total reward over a fixed time horizon $T$ with a\nlow total payment. Our contributions in this paper are two-fold: (i) We propose\na new MAB model with random arm selection that considers the relationship of\nusers' self-reinforcing preferences and incentives; and (ii) We leverage the\nproperties of a multi-color Polya urn with nonlinear feedback model to propose\ntwo MAB policies termed \"At-Least-$n$ Explore-Then-Commit\" and \"UCB-List\". We\nprove that both policies achieve $O(log T)$ expected regret with $O(log T)$\nexpected payment over a time horizon $T$. We conduct numerical simulations to\ndemonstrate and verify the performances of these two policies and study their\nrobustness under various settings.",
          "link": "http://arxiv.org/abs/2105.08869",
          "publishedOn": "2021-05-23T06:10:40.722Z",
          "wordCount": 672,
          "title": "Incentivized Bandit Learning with Self-Reinforcing User Preferences. (arXiv:2105.08869v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majdabadi_M/0/1/0/all/0/1\">Mahdiyar Molahasani Majdabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Younhee Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deivalakshmi_S/0/1/0/all/0/1\">S. Deivalakshmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Seokbum Ko</a>",
          "description": "Prostate cancer is a very common disease among adult men. One in seven\nCanadian men is diagnosed with this cancer in their lifetime. Super-Resolution\n(SR) can facilitate early diagnosis and potentially save many lives. In this\npaper, a robust and accurate model is proposed for prostate MRI SR. The model\nis trained on the Prostate-Diagnosis and PROSTATEx datasets. The proposed model\noutperformed the state-of-the-art prostate SR model in all similarity metrics\nwith notable margins. A new task-specific similarity assessment is introduced\nas well. A classifier is trained for severe cancer detection and the drop in\nthe accuracy of this model when dealing with super-resolved images is used for\nevaluating the ability of medical detail reconstruction of the SR models. The\nproposed SR model is a step towards an efficient and accurate general medical\nSR platform.",
          "link": "http://arxiv.org/abs/2105.07495",
          "publishedOn": "2021-05-23T06:10:40.703Z",
          "wordCount": 573,
          "title": "Capsule GAN for Prostate MRI Super-Resolution. (arXiv:2105.07495v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutellier_T/0/1/0/all/0/1\">Thibaud Lutellier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lin Tan</a>",
          "description": "Automatic program repair (APR) is crucial to improve software reliability.\nRecently, neural machine translation (NMT) techniques have been used to fix\nsoftware bugs automatically. While promising, these approaches have two major\nlimitations. Their search space often does not contain the correct fix, and\ntheir search strategy ignores software knowledge such as strict code syntax.\nDue to these limitations, existing NMT-based techniques underperform the best\ntemplate-based approaches.\n\nWe propose CURE, a new NMT-based APR technique with three major novelties.\nFirst, CURE pre-trains a programming language (PL) model on a large software\ncodebase to learn developer-like source code before the APR task. Second, CURE\ndesigns a new code-aware search strategy that finds more correct fixes by\nfocusing on compilable patches and patches that are close in length to the\nbuggy code. Finally, CURE uses a subword tokenization technique to generate a\nsmaller search space that contains more correct fixes.\n\nOur evaluation on two widely-used benchmarks shows that CURE correctly fixes\n57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR\ntechniques on both benchmarks.",
          "link": "http://arxiv.org/abs/2103.00073",
          "publishedOn": "2021-05-23T06:10:40.696Z",
          "wordCount": 659,
          "title": "CURE: Code-Aware Neural Machine Translation for Automatic Program Repair. (arXiv:2103.00073v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Greenberg_I/0/1/0/all/0/1\">Ido Greenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannay_N/0/1/0/all/0/1\">Netanel Yannay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>",
          "description": "Determining the noise parameters of a Kalman Filter (KF) has been studied for\ndecades. A huge body of research focuses on the task of estimation of the noise\nunder various conditions, since precise noise estimation is considered\nequivalent to minimization of the filtering errors. However, we show that even\na small violation of the KF assumptions can significantly modify the effective\nnoise, breaking the equivalence between the tasks and making noise estimation\nan inferior strategy. We show that such violations are very common, and are\noften not trivial to handle or even notice. Consequentially, we argue that a\nrobust solution is needed - rather than choosing a dedicated model per problem.\nTo that end, we apply gradient-based optimization to the filtering errors\ndirectly, with relation to a simple and efficient parameterization of the\nsymmetric and positive-definite parameters of KF. In radar tracking and video\ntracking, we show that the optimization improves both the accuracy of KF and\nits robustness to design decisions. In addition, we demonstrate how an\noptimized neural network model can seem to reduce the errors significantly\ncompared to a KF - and how this reduction vanishes once the KF is optimized\nsimilarly. This indicates how complicated models can be wrongly identified as\nsuperior to KF, while in fact they were merely more optimized.",
          "link": "http://arxiv.org/abs/2104.02372",
          "publishedOn": "2021-05-23T06:10:40.667Z",
          "wordCount": 689,
          "title": "Noise Estimation Is Not Optimal: How to Use Kalman Filter the Right Way. (arXiv:2104.02372v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00685",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Marvin_D/0/1/0/all/0/1\">Dario Marvin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nespoli_L/0/1/0/all/0/1\">Lorenzo Nespoli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Strepparava_D/0/1/0/all/0/1\">Davide Strepparava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Medici_V/0/1/0/all/0/1\">Vasco Medici</a>",
          "description": "The ability to forecast the concentration of air pollutants in an urban\nregion is crucial for decision-makers wishing to reduce the impact of pollution\non public health through active measures (e.g. temporary traffic closures). In\nthis study, we present a machine learning approach applied to the forecast of\nthe day-ahead maximum value of the ozone concentration for several geographical\nlocations in southern Switzerland. Due to the low density of measurement\nstations and to the complex orography of the use case terrain, we adopted\nfeature selection methods instead of explicitly restricting relevant features\nto a neighbourhood of the prediction sites, as common in spatio-temporal\nforecasting methods. We then used Shapley values to assess the explainability\nof the learned models in terms of feature importance and feature interactions\nin relation to ozone predictions; our analysis suggests that the trained models\neffectively learned explanatory cross-dependencies among atmospheric variables.\nFinally, we show how weighting observations helps in increasing the accuracy of\nthe forecasts for specific ranges of ozone's daily peak values.",
          "link": "http://arxiv.org/abs/2012.00685",
          "publishedOn": "2021-05-23T06:10:40.661Z",
          "wordCount": 629,
          "title": "A data-driven approach to the forecasting of ground-level ozone concentration. (arXiv:2012.00685v3 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hingane_S/0/1/0/all/0/1\">Shreeshail Hingane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Artistic style transfer aims to transfer the style characteristics of one\nimage onto another image while retaining its content. Existing approaches\ncommonly leverage various normalization techniques, although these face\nlimitations in adequately transferring diverse textures to different spatial\nlocations. Self-Attention-based approaches have tackled this issue with partial\nsuccess but suffer from unwanted artifacts. Motivated by these observations,\nthis paper aims to combine the best of both worlds: self-attention and\nnormalization. That yields a new plug-and-play module that we name\nSelf-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially\na spatially adaptive normalization module whose parameters are inferred through\nattention on the content and style image. We demonstrate that plugging SAFIN\ninto the base network of another state-of-the-art method results in enhanced\nstylization. We also develop a novel base network composed of Wavelet Transform\nfor multi-scale style transfer, which when combined with SAFIN, produces\nvisually appealing results with lesser unwanted textures.",
          "link": "http://arxiv.org/abs/2105.06129",
          "publishedOn": "2021-05-23T06:10:40.654Z",
          "wordCount": 616,
          "title": "SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization. (arXiv:2105.06129v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guofeng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1\">Yanguang Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>",
          "description": "The study of multi-type Protein-Protein Interaction (PPI) is fundamental for\nunderstanding biological processes from a systematic perspective and revealing\ndisease mechanisms. Existing methods suffer from significant performance\ndegradation when tested in unseen dataset. In this paper, we investigate the\nproblem and find that it is mainly attributed to the poor performance for\ninter-novel-protein interaction prediction. However, current evaluations\noverlook the inter-novel-protein interactions, and thus fail to give an\ninstructive assessment. As a result, we propose to address the problem from\nboth the evaluation and the methodology. Firstly, we design a new evaluation\nframework that fully respects the inter-novel-protein interactions and gives\nconsistent assessment across datasets. Secondly, we argue that correlations\nbetween proteins must provide useful information for analysis of novel\nproteins, and based on this, we propose a graph neural network based method\n(GNN-PPI) for better inter-novel-protein interaction prediction. Experimental\nresults on real-world datasets of different scales demonstrate that GNN-PPI\nsignificantly outperforms state-of-the-art PPI prediction methods, especially\nfor the inter-novel-protein interaction prediction.",
          "link": "http://arxiv.org/abs/2105.06709",
          "publishedOn": "2021-05-23T06:10:40.635Z",
          "wordCount": 631,
          "title": "Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction. (arXiv:2105.06709v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>",
          "description": "This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge\nTransfer), a novel method for the automatic transfer of explanatory knowledge\nthrough neural encoding mechanisms. We demonstrate that N-XKT is able to\nimprove accuracy and generalization on science Question Answering (QA).\nSpecifically, by leveraging facts from background explanatory knowledge\ncorpora, the N-XKT model shows a clear improvement on zero-shot QA.\nFurthermore, we show that N-XKT can be fine-tuned on a target QA dataset,\nenabling faster convergence and more accurate results. A systematic analysis is\nconducted to quantitatively analyze the performance of the N-XKT model and the\nimpact of different categories of knowledge on the zero-shot generalization\ntask.",
          "link": "http://arxiv.org/abs/2105.05737",
          "publishedOn": "2021-05-23T06:10:40.486Z",
          "wordCount": 554,
          "title": "Encoding Explanatory Knowledge for Zero-shot Science Question Answering. (arXiv:2105.05737v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leshchinskiy_B/0/1/0/all/0/1\">Brandon Leshchinskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Requena_Mesa_C/0/1/0/all/0/1\">Christian Requena-Mesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chishtie_F/0/1/0/all/0/1\">Farrukh Chishtie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1\">Natalia D&#xed;az-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1\">Oc&#xe9;ane Boulais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aruna Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pina_A/0/1/0/all/0/1\">Aaron Pi&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raissi_C/0/1/0/all/0/1\">Chedy Ra&#xef;ssi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1\">Alexander Lavin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1\">Dava Newman</a>",
          "description": "As climate change increases the intensity of natural disasters, society needs\nbetter tools for adaptation. Floods, for example, are the most frequent natural\ndisaster, and better tools for flood risk communication could increase the\nsupport for flood-resilient infrastructure development. Our work aims to enable\nmore visual communication of large-scale climate impacts via visualizing the\noutput of coastal flood models as satellite imagery. We propose the first deep\nlearning pipeline to ensure physical-consistency in synthetic visual satellite\nimagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it\nproduces imagery that is physically-consistent with the output of an\nexpert-validated storm surge model (NOAA SLOSH). By evaluating the imagery\nrelative to physics-based flood maps, we find that our proposed framework\noutperforms baseline models in both physical-consistency and photorealism. We\nenvision our work to be the first step towards a global visualization of how\nclimate change shapes our landscape. Continuing on this path, we show that the\nproposed pipeline generalizes to visualize arctic sea ice melt. We also publish\na dataset of over 25k labelled image-pairs to study image-to-image translation\nin Earth observation.",
          "link": "http://arxiv.org/abs/2104.04785",
          "publishedOn": "2021-05-23T06:10:40.479Z",
          "wordCount": 685,
          "title": "Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization. (arXiv:2104.04785v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01187",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Qi_Z/0/1/0/all/0/1\">Zhengling Qi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Miao_R/0/1/0/all/0/1\">Rui Miao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoke Zhang</a>",
          "description": "Data-driven individualized decision making has recently received increasing\nresearch interests. Most existing methods rely on the assumption of no\nunmeasured confounding, which unfortunately cannot be ensured in practice\nespecially in observational studies. Motivated by the recent proposed proximal\ncausal inference, we develop several proximal learning approaches to estimating\noptimal individualized treatment regimes (ITRs) in the presence of unmeasured\nconfounding. In particular, we establish several identification results for\ndifferent classes of ITRs, exhibiting the trade-off between the risk of making\nuntestable assumptions and the value function improvement in decision making.\nBased on these results, we propose several classification-based approaches to\nfinding a variety of restricted in-class optimal ITRs and develop their\ntheoretical properties. The appealing numerical performance of our proposed\nmethods is demonstrated via an extensive simulation study and one real data\napplication.",
          "link": "http://arxiv.org/abs/2105.01187",
          "publishedOn": "2021-05-23T06:10:40.465Z",
          "wordCount": 580,
          "title": "Proximal Learning for Individualized Treatment Regimes Under Unmeasured Confounding. (arXiv:2105.01187v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Ed Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the accuracy of popular NAR models adopted in neural machine\ntranslation by a large margin.",
          "link": "http://arxiv.org/abs/2105.03842",
          "publishedOn": "2021-05-23T06:10:40.453Z",
          "wordCount": 751,
          "title": "FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1\">Arnd Koeppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1\">Franz Bamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1\">Michael Selzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1\">Britta Nestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1\">Bernd Markert</a>",
          "description": "(Artificial) neural networks have become increasingly popular in mechanics as\nmeans to accelerate computations with model order reduction techniques and as\nuniversal models for a wide variety of materials. However, the major\ndisadvantage of neural networks remains: their numerous parameters are\nchallenging to interpret and explain. Thus, neural networks are often labeled\nas black boxes, and their results often elude human interpretation. In\nmechanics, the new and active field of physics-informed neural networks\nattempts to mitigate this disadvantage by designing deep neural networks on the\nbasis of mechanical knowledge. By using this a priori knowledge, deeper and\nmore complex neural networks became feasible, since the mechanical assumptions\ncould be explained. However, the internal reasoning and explanation of neural\nnetwork parameters remain mysterious.\n\nComplementary to the physics-informed approach, we propose a first step\ntowards a physics-informing approach, which explains neural networks trained on\nmechanical data a posteriori. This novel explainable artificial intelligence\napproach aims at elucidating the black box of neural networks and their\nhigh-dimensional representations. Therein, the principal component analysis\ndecorrelates the distributed representations in cell states of RNNs and allows\nthe comparison to known and fundamental functions. The novel approach is\nsupported by a systematic hyperparameter search strategy that identifies the\nbest neural network architectures and training parameters. The findings of\nthree case studies on fundamental constitutive models (hyperelasticity,\nelastoplasticity, and viscoelasticity) imply that the proposed strategy can\nhelp identify numerical and analytical closed-form solutions to characterize\nnew materials.",
          "link": "http://arxiv.org/abs/2104.10683",
          "publishedOn": "2021-05-23T06:10:40.434Z",
          "wordCount": 713,
          "title": "Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1\">Quentin Paletta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>",
          "description": "Improving irradiance forecasting is critical to further increase the share of\nsolar in the energy mix. On a short time scale, fish-eye cameras on the ground\nare used to capture cloud displacements causing the local variability of the\nelectricity production. As most of the solar radiation comes directly from the\nSun, current forecasting approaches use its position in the image as a\nreference to interpret the cloud cover dynamics. However, existing Sun tracking\nmethods rely on external data and a calibration of the camera, which requires\naccess to the device. To address these limitations, this study introduces an\nimage-based Sun tracking algorithm to localise the Sun in the image when it is\nvisible and interpolate its daily trajectory from past observations. We\nvalidate the method on a set of sky images collected over a year at SIRTA's\nlab. Experimental results show that the proposed method provides robust smooth\nSun trajectories with a mean absolute error below 1% of the image size.",
          "link": "http://arxiv.org/abs/2012.01059",
          "publishedOn": "2021-05-23T06:10:40.406Z",
          "wordCount": 634,
          "title": "A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wang-Zhou Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muggleton_S/0/1/0/all/0/1\">Stephen H. Muggleton</a>",
          "description": "For many reasoning-heavy tasks involving raw inputs, it is challenging to\ndesign an appropriate end-to-end learning pipeline. Neuro-Symbolic Learning,\ndivide the process into sub-symbolic perception and symbolic reasoning, trying\nto utilise data-driven machine learning and knowledge-driven reasoning\nsimultaneously. However, they suffer from the exponential computational\ncomplexity within the interface between these two components, where the\nsub-symbolic learning model lacks direct supervision, and the symbolic model\nlacks accurate input facts. Hence, most of them assume the existence of a\nstrong symbolic knowledge base and only learn the perception model while\navoiding a crucial problem: where does the knowledge come from? In this paper,\nwe present Abductive Meta-Interpretive Learning ($Meta_{Abd}$) that unites\nabduction and induction to learn neural networks and induce logic theories\njointly from raw data. Experimental results demonstrate that $Meta_{Abd}$ not\nonly outperforms the compared systems in predictive accuracy and data\nefficiency but also induces logic programs that can be re-used as background\nknowledge in subsequent learning tasks. To the best of our knowledge,\n$Meta_{Abd}$ is the first system that can jointly learn neural networks from\nscratch and induce recursive first-order logic theories with predicate\ninvention.",
          "link": "http://arxiv.org/abs/2010.03514",
          "publishedOn": "2021-05-23T06:10:40.399Z",
          "wordCount": 639,
          "title": "Abductive Knowledge Induction From Raw Data. (arXiv:2010.03514v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>",
          "description": "Machine learning on graphs has been extensively studied in both academic and\nindustry. However, as the literature on graph learning booms with a vast number\nof emerging methods and techniques, it becomes increasingly difficult to\nmanually design the optimal machine learning algorithm for different\ngraph-related tasks. To solve this critical challenge, automated machine\nlearning (AutoML) on graphs which combines the strength of graph machine\nlearning and AutoML together, is gaining attention from the research community.\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\n(NAS) for graph machine learning. We further overview libraries related to\nautomated graph machine learning and in-depth discuss AutoGL, the first\ndedicated open-source library for AutoML on graphs. In the end, we share our\ninsights on future research directions for automated graph machine learning.\nThis paper is the first systematic and comprehensive review of automated\nmachine learning on graphs to the best of our knowledge.",
          "link": "http://arxiv.org/abs/2103.00742",
          "publishedOn": "2021-05-23T06:10:40.390Z",
          "wordCount": 625,
          "title": "Automated Machine Learning on Graphs: A Survey. (arXiv:2103.00742v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaohang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1\">Md Zakirul Alam Bhuiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lianzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "Depression is one of the most common mental illness problems, and the\nsymptoms shown by patients are not consistent, making it difficult to diagnose\nin the process of clinical practice and pathological research. Although\nresearchers hope that artificial intelligence can contribute to the diagnosis\nand treatment of depression, the traditional centralized machine learning needs\nto aggregate patient data, and the data privacy of patients with mental illness\nneeds to be strictly confidential, which hinders machine learning algorithms\nclinical application. To solve the problem of privacy of the medical history of\npatients with depression, we implement federated learning to analyze and\ndiagnose depression. First, we propose a general multi-view federated learning\nframework using multi-source data, which can extend any traditional machine\nlearning model to support federated learning across different institutions or\nparties. Secondly, we adopt late fusion methods to solve the problem of\ninconsistent time series of multi-view data. Finally, we compare the federated\nframework with other cooperative learning frameworks in performance and discuss\nthe related results.",
          "link": "http://arxiv.org/abs/2102.09342",
          "publishedOn": "2021-05-23T06:10:40.383Z",
          "wordCount": 672,
          "title": "FedMood: Federated Learning on Mobile Health Data for Mood Detection. (arXiv:2102.09342v6 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dapeng Zhao</a>",
          "description": "Thesis document of the degree of Master of Science in Robotics of Carnegie\nMellon University School of Computer Science.",
          "link": "http://arxiv.org/abs/2104.10241",
          "publishedOn": "2021-05-23T06:10:40.362Z",
          "wordCount": 465,
          "title": "Predicting Human Trajectories by Learning and Matching Patterns. (arXiv:2104.10241v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santu_S/0/1/0/all/0/1\">Shubhra Kanti Karmaker Santu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1\">Md. Mahadi Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1\">Micah J. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1\">Kalyan Veeramachaneni</a>",
          "description": "As big data becomes ubiquitous across domains, and more and more stakeholders\naspire to make the most of their data, demand for machine learning tools has\nspurred researchers to explore the possibilities of automated machine learning\n(AutoML). AutoML tools aim to make machine learning accessible for non-machine\nlearning experts (domain experts), to improve the efficiency of machine\nlearning, and to accelerate machine learning research. But although automation\nand efficiency are among AutoML's main selling points, the process still\nrequires human involvement at a number of vital steps, including understanding\nthe attributes of domain-specific data, defining prediction problems, creating\na suitable training data set, and selecting a promising machine learning\ntechnique. These steps often require a prolonged back-and-forth that makes this\nprocess inefficient for domain experts and data scientists alike, and keeps\nso-called AutoML systems from being truly automatic. In this review article, we\nintroduce a new classification system for AutoML systems, using a seven-tiered\nschematic to distinguish these systems based on their level of autonomy. We\nbegin by describing what an end-to-end machine learning pipeline actually looks\nlike, and which subtasks of the machine learning pipeline have been automated\nso far. We highlight those subtasks which are still done manually - generally\nby a data scientist - and explain how this limits domain experts' access to\nmachine learning. Next, we introduce our novel level-based taxonomy for AutoML\nsystems and define each level according to the scope of automation support\nprovided. Finally, we lay out a roadmap for the future, pinpointing the\nresearch required to further automate the end-to-end machine learning pipeline\nand discussing important challenges that stand in the way of this ambitious\ngoal.",
          "link": "http://arxiv.org/abs/2010.10777",
          "publishedOn": "2021-05-23T06:10:40.354Z",
          "wordCount": 775,
          "title": "AutoML to Date and Beyond: Challenges and Opportunities. (arXiv:2010.10777v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1\">Joel Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1\">Alex Gu</a>",
          "description": "The Continual Learning (CL) problem involves performing well on a sequence of\ntasks under limited compute. Current algorithms in the domain are either slow,\noffline or sensitive to hyper-parameters. La-MAML, an optimization-based\nmeta-learning algorithm claims to be better than other replay-based,\nprior-based and meta-learning based approaches. According to the MER paper [1],\nmetrics to measure performance in the continual learning arena are Retained\nAccuracy (RA) and Backward Transfer-Interference (BTI). La-MAML claims to\nperform better in these values when compared to the SOTA in the domain. This is\nthe main claim of the paper, which we shall be verifying in this report.",
          "link": "http://arxiv.org/abs/2102.05824",
          "publishedOn": "2021-05-23T06:10:40.345Z",
          "wordCount": 550,
          "title": "Reproducibility Report: La-MAML: Look-ahead Meta Learning for Continual Learning. (arXiv:2102.05824v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12365",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "This article addresses several fundamental issues associated with the\napproximation theory of neural networks, including the characterization of\napproximation spaces, the determination of the metric entropy of these spaces,\nand approximation rates of neural networks. For any activation function\n$\\sigma$, we show that the largest Banach space of functions which can be\nefficiently approximated by the corresponding shallow neural networks is the\nspace whose norm is given by the gauge of the closed convex hull of the set\n$\\{\\pm\\sigma(\\omega\\cdot x + b)\\}$. We characterize this space for the ReLU$^k$\nand cosine activation functions and, in particular, show that the resulting\ngauge space is equivalent to the spectral Barron space if $\\sigma=\\cos$ and is\nequivalent to the Barron space when $\\sigma={\\rm ReLU}$. Our main result\nestablishes the precise asymptotics of the $L^2$-metric entropy of the unit\nball of these guage spaces and, as a consequence, the optimal approximation\nrates for shallow ReLU$^k$ networks. The sharpest previous results hold only in\nthe special case that $k=0$ and $d=2$, where the metric entropy has been\ndetermined up to logarithmic factors. When $k > 0$ or $d > 2$, there is a\nsignificant gap between the previous best upper and lower bounds. We close all\nof these gaps and determine the precise asymptotics of the metric entropy for\nall $k \\geq 0$ and $d\\geq 2$, including removing the logarithmic factors\npreviously mentioned. Finally, we use these results to quantify how much is\nlost by Barron's spectral condition relative to the convex hull of\n$\\{\\pm\\sigma(\\omega\\cdot x + b)\\}$ when $\\sigma={\\rm ReLU}^k$. Finally, we also\nshow that the orthogonal greedy algorithm can algorithmically realize the\nimproved approximation rates which have been derived.",
          "link": "http://arxiv.org/abs/2101.12365",
          "publishedOn": "2021-05-23T06:10:40.334Z",
          "wordCount": 762,
          "title": "Optimal Approximation Rates and Metric Entropy of ReLU$^k$ and Cosine Networks. (arXiv:2101.12365v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04350",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zheng_L/0/1/0/all/0/1\">Liangzhen Zheng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lan_H/0/1/0/all/0/1\">Haidong Lan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>",
          "description": "Proteins structure prediction has long been a grand challenge over the past\n50 years, owing to its board scientific and application interests. There are\ntwo major types of modelling algorithm, template-free modelling and\ntemplate-based modelling, which is suitable for easy prediction tasks, and is\nwidely adopted in computer aided drug discoveries for drug design and\nscreening. Although it has been several decades since its first edition, the\ncurrent template-based modeling approach suffers from two important problems:\n1) there are many missing regions in the template-query sequence alignment, and\n2) the accuracy of the distance pairs from different regions of the template\nvaries, and this information is not well introduced into the modeling. To solve\nthe two problems, we propose a structural optimization process based on\ntemplate modelling, introducing two neural network models predict the distance\ninformation of the missing regions and the accuracy of the distance pairs of\ndifferent regions in the template modeling structure. The predicted distances\nand residue pairwise specific accuracy information are incorporated into the\npotential energy function for structural optimization, which significantly\nimproves the qualities of the original template modelling decoys.",
          "link": "http://arxiv.org/abs/2105.04350",
          "publishedOn": "2021-05-23T06:10:40.318Z",
          "wordCount": 649,
          "title": "tFold-TR: Combining Deep Learning Enhanced Hybrid Potential Energy for Template-Based Modelling Structure Refinement. (arXiv:2105.04350v2 [physics.bio-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08796",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaojun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdollahi_A/0/1/0/all/0/1\">Ali Abdollahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_T/0/1/0/all/0/1\">Trevor Jones</a>",
          "description": "For electric vehicles (EV) and energy storage (ES) batteries, thermal runaway\nis a critical issue as it can lead to uncontrollable fires or even explosions.\nThermal anomaly detection can identify problematic battery packs that may\neventually undergo thermal runaway. However, there are common challenges like\ndata unavailability, environment and configuration variations, and battery\naging. We propose a data-driven method to detect battery thermal anomaly based\non comparing shape-similarity between thermal measurements. Based on their\nshapes, the measurements are continuously being grouped into different\nclusters. Anomaly is detected by monitoring deviations within the clusters.\nUnlike model-based or other data-driven methods, the proposed method is robust\nto data loss and requires minimal reference data for different pack\nconfigurations. As the initial experimental results show, the method not only\ncan be more accurate than the onboard BMS and but also can detect unforeseen\nanomalies at the early stage.",
          "link": "http://arxiv.org/abs/2103.08796",
          "publishedOn": "2021-05-23T06:10:40.311Z",
          "wordCount": 607,
          "title": "Data-driven Thermal Anomaly Detection for Batteries using Unsupervised Shape Clustering. (arXiv:2103.08796v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cachay_S/0/1/0/all/0/1\">Salva R&#xfc;hling Cachay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erickson_E/0/1/0/all/0/1\">Emma Erickson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucker_A/0/1/0/all/0/1\">Arthur Fender C. Bucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokropek_E/0/1/0/all/0/1\">Ernest Pokropek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potosnak_W/0/1/0/all/0/1\">Willa Potosnak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bire_S/0/1/0/all/0/1\">Suyash Bire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>",
          "description": "Deep learning-based models have recently outperformed state-of-the-art\nseasonal forecasting models, such as for predicting El Ni\\~no-Southern\nOscillation (ENSO). However, current deep learning models are based on\nconvolutional neural networks which are difficult to interpret and can fail to\nmodel large-scale atmospheric patterns. In comparison, graph neural networks\n(GNNs) are capable of modeling large-scale spatial dependencies and are more\ninterpretable due to the explicit modeling of information flow through edge\nconnections. We propose the first application of graph neural networks to\nseasonal forecasting. We design a novel graph connectivity learning module that\nenables our GNN model to learn large-scale spatial interactions jointly with\nthe actual ENSO forecasting task. Our model, \\graphino, outperforms\nstate-of-the-art deep learning-based models for forecasts up to six months\nahead. Additionally, we show that our model is more interpretable as it learns\nsensible connectivity structures that correlate with the ENSO anomaly pattern.",
          "link": "http://arxiv.org/abs/2104.05089",
          "publishedOn": "2021-05-23T06:10:40.304Z",
          "wordCount": 632,
          "title": "The World as a Graph: Improving El Ni\\~no Forecasts with Graph Neural Networks. (arXiv:2104.05089v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1\">Dylan R. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1\">Sina Ghiassian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "Catastrophic forgetting remains a severe hindrance to the broad application\nof artificial neural networks (ANNs), however, it continues to be a poorly\nunderstood phenomenon. Despite the extensive amount of work on catastrophic\nforgetting, we argue that it is still unclear how exactly the phenomenon should\nbe quantified, and, moreover, to what degree all of the choices we make when\ndesigning learning systems affect the amount of catastrophic forgetting. We use\nvarious testbeds from the reinforcement learning and supervised learning\nliterature to (1) provide evidence that the choice of which modern\ngradient-based optimization algorithm is used to train an ANN has a significant\nimpact on the amount of catastrophic forgetting and show that-surprisingly-in\nmany instances classical algorithms such as vanilla SGD experience less\ncatastrophic forgetting than the more modern algorithms such as Adam. We\nempirically compare four different existing metrics for quantifying\ncatastrophic forgetting and (2) show that the degree to which the learning\nsystems experience catastrophic forgetting is sufficiently sensitive to the\nmetric used that a change from one principled metric to another is enough to\nchange the conclusions of a study dramatically. Our results suggest that a much\nmore rigorous experimental methodology is required when looking at catastrophic\nforgetting. Based on our results, we recommend inter-task forgetting in\nsupervised learning must be measured with both retention and relearning metrics\nconcurrently, and intra-task forgetting in reinforcement learning must-at the\nvery least-be measured with pairwise interference.",
          "link": "http://arxiv.org/abs/2102.07686",
          "publishedOn": "2021-05-23T06:10:40.297Z",
          "wordCount": 752,
          "title": "Does Standard Backpropagation Forget Less Catastrophically Than Adam?. (arXiv:2102.07686v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02694",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Perugachi_Diaz_Y/0/1/0/all/0/1\">Yura Perugachi-Diaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bhulai_S/0/1/0/all/0/1\">Sandjai Bhulai</a>",
          "description": "We introduce Invertible Dense Networks (i-DenseNets), a more parameter\nefficient extension of Residual Flows. The method relies on an analysis of the\nLipschitz continuity of the concatenation in DenseNets, where we enforce\ninvertibility of the network by satisfying the Lipschitz constant. Furthermore,\nwe propose a learnable weighted concatenation, which not only improves the\nmodel performance but also indicates the importance of the concatenated\nweighted representation. Additionally, we introduce the Concatenated LipSwish\nas activation function, for which we show how to enforce the Lipschitz\ncondition and which boosts performance. The new architecture, i-DenseNet,\nout-performs Residual Flow and other flow-based models on density estimation\nevaluated in bits per dimension, where we utilize an equal parameter budget.\nMoreover, we show that the proposed model out-performs Residual Flows when\ntrained as a hybrid model where the model is both a generative and a\ndiscriminative model.",
          "link": "http://arxiv.org/abs/2102.02694",
          "publishedOn": "2021-05-23T06:10:40.286Z",
          "wordCount": 588,
          "title": "Invertible DenseNets with Concatenated LipSwish. (arXiv:2102.02694v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frerix_T/0/1/0/all/0/1\">Thomas Frerix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkov_D/0/1/0/all/0/1\">Dmitrii Kochkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jamie A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyer_S/0/1/0/all/0/1\">Stephan Hoyer</a>",
          "description": "Variational data assimilation optimizes for an initial state of a dynamical\nsystem such that its evolution fits observational data. The physical model can\nsubsequently be evolved into the future to make predictions. This principle is\na cornerstone of large scale forecasting applications such as numerical weather\nprediction. As such, it is implemented in current operational systems of\nweather forecasting agencies across the globe. However, finding a good initial\nstate poses a difficult optimization problem in part due to the non-invertible\nrelationship between physical states and their corresponding observations. We\nlearn a mapping from observational data to physical states and show how it can\nbe used to improve optimizability. We employ this mapping in two ways: to\nbetter initialize the non-convex optimization problem, and to reformulate the\nobjective function in better behaved physics space instead of observation\nspace. Our experimental results for the Lorenz96 model and a two-dimensional\nturbulent fluid flow demonstrate that this procedure significantly improves\nforecast quality for chaotic systems.",
          "link": "http://arxiv.org/abs/2102.11192",
          "publishedOn": "2021-05-23T06:10:40.278Z",
          "wordCount": 643,
          "title": "Variational Data Assimilation with a Learned Inverse Observation Operator. (arXiv:2102.11192v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chenjian Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongjin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Liqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwei Xu</a>",
          "description": "Tensor completion refers to the task of estimating the missing data from an\nincomplete measurement or observation, which is a core problem frequently\narising from the areas of big data analysis, computer vision, and network\nengineering. Due to the multidimensional nature of high-order tensors, the\nmatrix approaches, e.g., matrix factorization and direct matricization of\ntensors, are often not ideal for tensor completion and recovery. In this paper,\nwe introduce a unified low-rank and sparse enhanced Tucker decomposition model\nfor tensor completion. Our model possesses a sparse regularization term to\npromote a sparse core tensor of the Tucker decomposition, which is beneficial\nfor tensor data compression. Moreover, we enforce low-rank regularization terms\non factor matrices of the Tucker decomposition for inducing the low-rankness of\nthe tensor with a cheap computational cost. Numerically, we propose a\ncustomized ADMM with enough easy subproblems to solve the underlying model. It\nis remarkable that our model is able to deal with different types of real-world\ndata sets, since it exploits the potential periodicity and inherent correlation\nproperties appeared in tensors. A series of computational experiments on\nreal-world data sets, including internet traffic data sets, color images, and\nface recognition, demonstrate that our model performs better than many existing\nstate-of-the-art matricization and tensorization approaches in terms of\nachieving higher recovery accuracy.",
          "link": "http://arxiv.org/abs/2010.00359",
          "publishedOn": "2021-05-23T06:10:40.269Z",
          "wordCount": 699,
          "title": "Low-Rank and Sparse Enhanced Tucker Decomposition for Tensor Completion. (arXiv:2010.00359v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1\">Max Schwarzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Ankesh Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rishab Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1\">R Devon Hjelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1\">Philip Bachman</a>",
          "description": "While deep reinforcement learning excels at solving tasks where large amounts\nof data can be collected through virtually unlimited interaction with the\nenvironment, learning from limited interaction remains a key challenge. We\nposit that an agent can learn more efficiently if we augment reward\nmaximization with self-supervised objectives based on structure in its visual\ninput and sequential interaction with the environment. Our method,\nSelf-Predictive Representations(SPR), trains an agent to predict its own latent\nstate representations multiple steps into the future. We compute target\nrepresentations for future states using an encoder which is an exponential\nmoving average of the agent's parameters and we make predictions using a\nlearned transition model. On its own, this future prediction objective\noutperforms prior methods for sample-efficient deep RL from pixels. We further\nimprove performance by adding data augmentation to the future prediction loss,\nwhich forces the agent's representations to be consistent across multiple views\nof an observation. Our full self-supervised objective, which combines future\nprediction and data augmentation, achieves a median human-normalized score of\n0.415 on Atari in a setting limited to 100k steps of environment interaction,\nwhich represents a 55% relative improvement over the previous state-of-the-art.\nNotably, even in this limited data regime, SPR exceeds expert human scores on 7\nout of 26 games. The code associated with this work is available at\nhttps://github.com/mila-iqia/spr",
          "link": "http://arxiv.org/abs/2007.05929",
          "publishedOn": "2021-05-23T06:10:40.251Z",
          "wordCount": 723,
          "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations. (arXiv:2007.05929v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02995",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhong_W/0/1/0/all/0/1\">Wenxuan Zhong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_P/0/1/0/all/0/1\">Ping Ma</a>",
          "description": "Optimal transport has been one of the most exciting subjects in mathematics,\nstarting from the 18th century. As a powerful tool to transport between two\nprobability measures, optimal transport methods have been reinvigorated\nnowadays in a remarkable proliferation of modern data science applications. To\nmeet the big data challenges, various computational tools have been developed\nin the recent decade to accelerate the computation for optimal transport\nmethods. In this review, we present some cutting-edge computational optimal\ntransport methods with a focus on the regularization-based methods and the\nprojection-based methods. We discuss their real-world applications in\nbiomedical research.",
          "link": "http://arxiv.org/abs/2008.02995",
          "publishedOn": "2021-05-23T06:10:40.243Z",
          "wordCount": 566,
          "title": "A Review on Modern Computational Optimal Transport Methods with Applications in Biomedical Research. (arXiv:2008.02995v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10718",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rangi_A/0/1/0/all/0/1\">Anshuka Rangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khojasteh_M/0/1/0/all/0/1\">Mohammad Javad Khojasteh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Franceschetti_M/0/1/0/all/0/1\">Massimo Franceschetti</a>",
          "description": "We study the problem of learning-based attacks in linear systems, where the\ncommunication channel between the controller and the plant can be hijacked by a\nmalicious attacker. We assume the attacker learns the dynamics of the system\nfrom observations, then overrides the controller's actuation signal, while\nmimicking legitimate operation by providing fictitious sensor readings to the\ncontroller. On the other hand, the controller is on a lookout to detect the\npresence of the attacker and tries to enhance the detection performance by\ncarefully crafting its control signals. We study the trade-offs between the\ninformation acquired by the attacker from observations, the detection\ncapabilities of the controller, and the control cost. Specifically, we provide\ntight upper and lower bounds on the expected $\\epsilon$-deception time, namely\nthe time required by the controller to make a decision regarding the presence\nof an attacker with confidence at least $(1-\\epsilon\\log(1/\\epsilon))$. We then\nshow a probabilistic lower bound on the time that must be spent by the attacker\nlearning the system, in order for the controller to have a given expected\n$\\epsilon$-deception time. We show that this bound is also order optimal, in\nthe sense that if the attacker satisfies it, then there exists a learning\nalgorithm with the given order expected deception time. Finally, we show a\nlower bound on the expected energy expenditure required to guarantee detection\nwith confidence at least $1-\\epsilon \\log(1/\\epsilon)$.",
          "link": "http://arxiv.org/abs/2011.10718",
          "publishedOn": "2021-05-23T06:10:40.226Z",
          "wordCount": 720,
          "title": "Learning-based attacks in Cyber-Physical Systems: Exploration, Detection, and Control Cost trade-offs. (arXiv:2011.10718v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1\">George Michalopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaka_H/0/1/0/all/0/1\">Hussam Kaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Helen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have\nachieved state-of-the-art results in biomedical natural language processing\ntasks by focusing their pre-training process on domain-specific corpora.\nHowever, such models do not take into consideration expert domain knowledge.\n\nIn this work, we introduced UmlsBERT, a contextual embedding model that\nintegrates domain knowledge during the pre-training process via a novel\nknowledge augmentation strategy. More specifically, the augmentation on\nUmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was\nperformed in two ways: i) connecting words that have the same underlying\n`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to\ncreate clinically meaningful input embeddings. By applying these two\nstrategies, UmlsBERT can encode clinical domain knowledge into word embeddings\nand outperform existing domain-specific models on common named-entity\nrecognition (NER) and clinical natural language inference clinical NLP tasks.",
          "link": "http://arxiv.org/abs/2010.10391",
          "publishedOn": "2021-05-23T06:10:40.219Z",
          "wordCount": 642,
          "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus. (arXiv:2010.10391v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03814",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1\">Phairot Autthasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1\">Rattanaphon Chaisaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1\">Thapanun Sudhawiyangkul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1\">Phurin Rangpong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1\">Suktipol Kiatthaveephong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1\">Nat Dilokthanakul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1\">Gun Bhakdisongkhram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>",
          "description": "Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)\nallow control of several applications by decoding neurophysiological phenomena,\nwhich are usually recorded by electroencephalography (EEG) using a non-invasive\ntechnique. Despite great advances in MI-based BCI, EEG rhythms are specific to\na subject and various changes over time. These issues point to significant\nchallenges to enhance the classification performance, especially in a\nsubject-independent manner. To overcome these challenges, we propose MIN2Net, a\nnovel end-to-end multi-task learning to tackle this task. We integrate deep\nmetric learning into a multi-task autoencoder to learn a compact and\ndiscriminative latent representation from EEG and perform classification\nsimultaneously. This approach reduces the complexity in pre-processing, results\nin significant performance improvement on EEG classification. Experimental\nresults in a subject-independent manner show that MIN2Net outperforms the\nstate-of-the-art techniques, achieving an F1-score improvement of 6.72%, and\n2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that\nMIN2Net improves discriminative information in the latent representation. This\nstudy indicates the possibility and practicality of using this model to develop\nMI-based BCI applications for new users without the need for calibration.",
          "link": "http://arxiv.org/abs/2102.03814",
          "publishedOn": "2021-05-23T06:10:40.212Z",
          "wordCount": 664,
          "title": "MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruimin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiayi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baofeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunlei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jie Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongjiang Wei</a>",
          "description": "Quantitative susceptibility mapping (QSM) has demonstrated great potential in\nquantifying tissue susceptibility in various brain diseases. However, the\nintrinsic ill-posed inverse problem relating the tissue phase to the underlying\nsusceptibility distribution affects the accuracy for quantifying tissue\nsusceptibility. Recently, deep learning has shown promising results to improve\naccuracy by reducing the streaking artifacts. However, there exists a mismatch\nbetween the observed phase and the theoretical forward phase estimated by the\nsusceptibility label. In this study, we proposed a model-based deep learning\narchitecture that followed the STI (susceptibility tensor imaging) physical\nmodel, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the\nrelationship between STI-derived phase contrast induced by the susceptibility\ntensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The\nconvolution neural networks are embedded into the physical model to learn a\nregularization term containing prior information. ki33 and phase induced by\nki13 and ki23 terms were used as the labels for network training. Quantitative\nevaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed\ndeep learning QSM methods. The results showed that MoDL-QSM achieved superior\nperformance, demonstrating its potential for future applications.",
          "link": "http://arxiv.org/abs/2101.08413",
          "publishedOn": "2021-05-23T06:10:40.206Z",
          "wordCount": 660,
          "title": "MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility Mapping. (arXiv:2101.08413v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Basat_R/0/1/0/all/0/1\">Ran Ben-Basat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitzenmacher_M/0/1/0/all/0/1\">Michael Mitzenmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1\">Shay Vargaftik</a>",
          "description": "We consider the fundamental problem of communicating an estimate of a real\nnumber $x\\in[0,1]$ using a single bit. A sender that knows $x$ chooses a value\n$X\\in\\set{0,1}$ to transmit. In turn, a receiver estimates $x$ based on the\nvalue of $X$. We consider both the biased and unbiased estimation problems and\naim to minimize the cost. For the biased case, the cost is the worst-case (over\nthe choice of $x$) expected squared error, which coincides with the variance if\nthe algorithm is required to be unbiased.\n\nWe first overview common biased and unbiased estimation approaches and prove\ntheir optimality when no shared randomness is allowed. We then show how a small\namount of shared randomness, which can be as low as a single bit, reduces the\ncost in both cases. Specifically, we derive lower bounds on the cost attainable\nby any algorithm with unrestricted use of shared randomness and propose\nnear-optimal solutions that use a small number of shared random bits. Finally,\nwe discuss open problems and future directions.",
          "link": "http://arxiv.org/abs/2010.02331",
          "publishedOn": "2021-05-23T06:10:40.199Z",
          "wordCount": 664,
          "title": "How to send a real number using a single bit (and some shared randomness). (arXiv:2010.02331v4 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06284",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>",
          "description": "This paper addresses the issue of long-scale correlations that is\ncharacteristic for symbolic music and is a challenge for modern generative\nalgorithms. It suggests a very simple workaround for this challenge, namely,\ngeneration of a drum pattern that could be further used as a foundation for\nmelody generation. The paper presents a large dataset of drum patterns\nalongside with corresponding melodies. It explores two possible methods for\ndrum pattern generation. Exploring a latent space of drum patterns one could\ngenerate new drum patterns with a given music style. Finally, the paper\ndemonstrates that a simple artificial neural network could be trained to\ngenerate melodies corresponding with these drum patters used as inputs.\nResulting system could be used for end-to-end generation of symbolic music with\nsong-like structure and higher long-scale correlations between the notes.",
          "link": "http://arxiv.org/abs/2007.06284",
          "publishedOn": "2021-05-23T06:10:40.190Z",
          "wordCount": 604,
          "title": "Artificial Neural Networks Jamming on the Beat. (arXiv:2007.06284v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishad_S/0/1/0/all/0/1\">Sunil Nishad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shubhangi Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranu_S/0/1/0/all/0/1\">Sayan Ranu</a>",
          "description": "Majority of the existing graph neural networks (GNN) learn node embeddings\nthat encode their local neighborhoods but not their positions. Consequently,\ntwo nodes that are vastly distant but located in similar local neighborhoods\nmap to similar embeddings in those networks. This limitation prevents accurate\nperformance in predictive tasks that rely on position information. In this\npaper,we develop GraphReach, a position-aware inductive GNN that captures the\nglobal positions of nodes through reachability estimations with respect to a\nset of anchor nodes. The anchors are strategically selected so that\nreachability estimations across all the nodes are maximized. We show that this\ncombinatorial anchor selection problem is NP-hard and, consequently, develop a\ngreedy (1-1/e) approximation heuristic. Empirical evaluation against\nstate-of-the-art GNN architectures reveal that GraphReach provides up to 40%\nrelative improvement in accuracy. In addition, it is more robust to adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2008.09657",
          "publishedOn": "2021-05-23T06:10:40.181Z",
          "wordCount": 616,
          "title": "GraphReach: Position-Aware Graph Neural Network using Reachability Estimations. (arXiv:2008.09657v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bowen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "With the explosive growth of e-commerce, online transaction fraud has become\none of the biggest challenges for e-commerce platforms. The historical\nbehaviors of users provide rich information for digging into the users' fraud\nrisk. While considerable efforts have been made in this direction, a\nlong-standing challenge is how to effectively exploit internal user information\nand provide explainable prediction results. In fact, the value variations of\nsame field from different events and the interactions of different fields\ninside one event have proven to be strong indicators for fraudulent behaviors.\nIn this paper, we propose the Dual Importance-aware Factorization Machines\n(DIFM), which exploits the internal field information among users' behavior\nsequence from dual perspectives, i.e., field value variations and field\ninteractions simultaneously for fraud detection. The proposed model is deployed\nin the risk management system of one of the world's largest e-commerce\nplatforms, which utilize it to provide real-time transaction fraud detection.\nExperimental results on real industrial data from different regions in the\nplatform clearly demonstrate that our model achieves significant improvements\ncompared with various state-of-the-art baseline models. Moreover, the DIFM\ncould also give an insight into the explanation of the prediction results from\ndual perspectives.",
          "link": "http://arxiv.org/abs/2008.05600",
          "publishedOn": "2021-05-23T06:10:40.174Z",
          "wordCount": 673,
          "title": "Modeling the Field Value Variations and Field Interactions Simultaneously for Fraud Detection. (arXiv:2008.05600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cordonnier_J/0/1/0/all/0/1\">Jean-Baptiste Cordonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1\">Andreas Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Attention layers are widely used in natural language processing (NLP) and are\nbeginning to influence computer vision architectures. Training very large\ntransformer models allowed significant improvement in both fields, but once\ntrained, these networks show symptoms of over-parameterization. For instance,\nit is known that many attention heads can be pruned without impacting accuracy.\nThis work aims to enhance current understanding on how multiple heads interact.\nMotivated by the observation that attention heads learn redundant key/query\nprojections, we propose a collaborative multi-head attention layer that enables\nheads to learn shared projections. Our scheme decreases the number of\nparameters in an attention layer and can be used as a drop-in replacement in\nany transformer architecture. Our experiments confirm that sharing key/query\ndimensions can be exploited in language understanding, machine translation and\nvision. We also show that it is possible to re-parametrize a pre-trained\nmulti-head attention layer into our collaborative attention layer.\nCollaborative multi-head attention reduces the size of the key and query\nprojections by 4 for same accuracy and speed. Our code is public.",
          "link": "http://arxiv.org/abs/2006.16362",
          "publishedOn": "2021-05-23T06:10:40.021Z",
          "wordCount": 628,
          "title": "Multi-Head Attention: Collaborate Instead of Concatenate. (arXiv:2006.16362v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gou_J/0/1/0/all/0/1\">Jianping Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1\">Stephen John Maybank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "In recent years, deep neural networks have been successful in both industry\nand academia, especially for computer vision tasks. The great success of deep\nlearning is mainly due to its scalability to encode large-scale data and to\nmaneuver billions of model parameters. However, it is a challenge to deploy\nthese cumbersome deep models on devices with limited resources, e.g., mobile\nphones and embedded devices, not only because of the high computational\ncomplexity but also the large storage requirements. To this end, a variety of\nmodel compression and acceleration techniques have been developed. As a\nrepresentative type of model compression and acceleration, knowledge\ndistillation effectively learns a small student model from a large teacher\nmodel. It has received rapid increasing attention from the community. This\npaper provides a comprehensive survey of knowledge distillation from the\nperspectives of knowledge categories, training schemes, teacher-student\narchitecture, distillation algorithms, performance comparison and applications.\nFurthermore, challenges in knowledge distillation are briefly reviewed and\ncomments on future research are discussed and forwarded.",
          "link": "http://arxiv.org/abs/2006.05525",
          "publishedOn": "2021-05-23T06:10:39.966Z",
          "wordCount": 677,
          "title": "Knowledge Distillation: A Survey. (arXiv:2006.05525v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cramer_B/0/1/0/all/0/1\">Benjamin Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Billaudelle_S/0/1/0/all/0/1\">Sebastian Billaudelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanya_S/0/1/0/all/0/1\">Simeon Kanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibfried_A/0/1/0/all/0/1\">Aron Leibfried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubl_A/0/1/0/all/0/1\">Andreas Gr&#xfc;bl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karasenko_V/0/1/0/all/0/1\">Vitali Karasenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pehle_C/0/1/0/all/0/1\">Christian Pehle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreiber_K/0/1/0/all/0/1\">Korbinian Schreiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stradmann_Y/0/1/0/all/0/1\">Yannik Stradmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weis_J/0/1/0/all/0/1\">Johannes Weis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schemmel_J/0/1/0/all/0/1\">Johannes Schemmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenke_F/0/1/0/all/0/1\">Friedemann Zenke</a>",
          "description": "To rapidly process temporal information at a low metabolic cost, biological\nneurons integrate inputs as an analog sum but communicate with spikes, binary\nevents in time. Analog neuromorphic hardware uses the same principles to\nemulate spiking neural networks with exceptional energy-efficiency. However,\ninstantiating high-performing spiking networks on such hardware remains a\nsignificant challenge due to device mismatch and the lack of efficient training\nalgorithms. Here, we introduce a general in-the-loop learning framework based\non surrogate gradients that resolves these issues. Using the BrainScaleS-2\nneuromorphic system, we show that learning self-corrects for device mismatch\nresulting in competitive spiking network performance on both vision and speech\nbenchmarks. Our networks display sparse spiking activity with, on average, far\nless than one spike per hidden neuron and input, perform inference at rates of\nup to 85 k frames/second, and consume less than 200 mW. In summary, our work\nsets several new benchmarks for low-energy spiking network processing on analog\nneuromorphic hardware and paves the way for future on-chip learning algorithms.",
          "link": "http://arxiv.org/abs/2006.07239",
          "publishedOn": "2021-05-23T06:10:39.959Z",
          "wordCount": 664,
          "title": "Surrogate gradients for analog neuromorphic computing. (arXiv:2006.07239v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09917",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1\">Aleksandr Beknazaryan</a>",
          "description": "An example of an activation function $\\sigma$ is given such that networks\nwith activations $\\{\\sigma, \\lfloor\\cdot\\rfloor\\}$, integer weights and a fixed\narchitecture depending on $d$ approximate continuous functions on $[0,1]^d$.\nThe range of integer weights required for $\\varepsilon$-approximation of\nH\\\"older continuous functions is derived, which leads to a convergence rate of\norder $n^{\\frac{-2\\beta}{2\\beta+d}}\\log_2n$ for neural network regression\nestimation of unknown $\\beta$-H\\\"older continuous function with given $n$\nsamples.",
          "link": "http://arxiv.org/abs/2105.09917",
          "publishedOn": "2021-05-23T06:10:39.952Z",
          "wordCount": 489,
          "title": "Neural networks with superexpressive activations and integer weights. (arXiv:2105.09917v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1912.07913",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Grelier_E/0/1/0/all/0/1\">Erwan Grelier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nouy_A/0/1/0/all/0/1\">Anthony Nouy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lebrun_R/0/1/0/all/0/1\">R&#xe9;gis Lebrun</a>",
          "description": "We consider the problem of the estimation of a high-dimensional probability\ndistribution from i.i.d. samples of the distribution using model classes of\nfunctions in tree-based tensor formats, a particular case of tensor networks\nassociated with a dimension partition tree. The distribution is assumed to\nadmit a density with respect to a product measure, possibly discrete for\nhandling the case of discrete random variables.\n\nAfter discussing the representation of classical model classes in tree-based\ntensor formats, we present learning algorithms based on empirical risk\nminimization using a $L^2$ contrast.\n\nThese algorithms exploit the multilinear parametrization of the formats to\nrecast the nonlinear minimization problem into a sequence of empirical risk\nminimization problems with linear models. A suitable parametrization of the\ntensor in tree-based tensor format allows to obtain a linear model with\northogonal bases, so that each problem admits an explicit expression of the\nsolution and cross-validation risk estimates. These estimations of the risk\nenable the model selection, for instance when exploiting sparsity in the\ncoefficients of the representation.\n\nA strategy for the adaptation of the tensor format (dimension tree and\ntree-based ranks) is provided, which allows to discover and exploit some\nspecific structures of high-dimensional probability distributions such as\nindependence or conditional independence.\n\nWe illustrate the performances of the proposed algorithms for the\napproximation of classical probabilistic models (such as Gaussian distribution,\ngraphical models, Markov chain).",
          "link": "http://arxiv.org/abs/1912.07913",
          "publishedOn": "2021-05-23T06:10:39.944Z",
          "wordCount": 685,
          "title": "Learning high-dimensional probability distributions using tree tensor networks. (arXiv:1912.07913v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.01383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Jan Paul Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangrong Xu</a>",
          "description": "This paper proposes a novel automatically generating image masks method for\nthe state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method\nachieves the best results in object detection until now, however, it is very\ntime-consuming and laborious to get the object Masks for training, the proposed\nmethod is composed by a two-stage design, to automatically generating image\nmasks, the first stage implements a fully convolutional networks (FCN) based\nsegmentation network, the second stage network, a Mask R-CNN based object\ndetection network, which is trained on the object image masks from FCN output,\nthe original input image, and additional label information. Through\nexperimentation, our proposed method can obtain the image masks automatically\nto train Mask R-CNN, and it can achieve very high classification accuracy with\nan over 90% mean of average precision (mAP) for segmentation",
          "link": "http://arxiv.org/abs/2003.01383",
          "publishedOn": "2021-05-23T06:10:39.936Z",
          "wordCount": 603,
          "title": "Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN. (arXiv:2003.01383v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.08797",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1\">Soufiane Hayou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1\">Jean-Francois Ton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Overparameterized Neural Networks (NN) display state-of-the-art performance.\nHowever, there is a growing need for smaller, energy-efficient, neural networks\ntobe able to use machine learning applications on devices with limited\ncomputational resources. A popular approach consists of using pruning\ntechniques. While these techniques have traditionally focused on pruning\npre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et\nal. (2018) has shown promising results when pruning at initialization. However,\nfor Deep NNs, such procedures remain unsatisfactory as the resulting pruned\nnetworks can be difficult to train and, for instance, they do not prevent one\nlayer from being fully pruned. In this paper, we provide a comprehensive\ntheoretical analysis of Magnitude and Gradient based pruning at initialization\nand training of sparse architectures. This allows us to propose novel\nprincipled approaches which we validate experimentally on a variety of NN\narchitectures.",
          "link": "http://arxiv.org/abs/2002.08797",
          "publishedOn": "2021-05-23T06:10:39.916Z",
          "wordCount": 619,
          "title": "Robust Pruning at Initialization. (arXiv:2002.08797v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1\">Luiz Giovanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceschin_F/0/1/0/all/0/1\">Fabr&#xed;cio Ceschin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Mirela Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aokun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1\">Ramchandra Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_S/0/1/0/all/0/1\">Sanjay Banda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lysaght_M/0/1/0/all/0/1\">Madison Lysaght</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1\">Heng Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapountzis_N/0/1/0/all/0/1\">Nikolaos Sapountzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruimin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthews_B/0/1/0/all/0/1\">Brandon Matthews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dapeng Oliver Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregio_A/0/1/0/all/0/1\">Andr&#xe9; Gr&#xe9;gio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniela Oliveira</a>",
          "description": "This paper investigates whether computer usage profiles comprised of\nprocess-, network-, mouse- and keystroke-related events are unique and\ntemporally consistent in a naturalistic setting, discussing challenges and\nopportunities of using such profiles in applications of continuous\nauthentication. We collected ecologically-valid computer usage profiles from 28\nMS Windows 10 computer users over 8 weeks and submitted this data to\ncomprehensive machine learning analysis involving a diverse set of online and\noffline classifiers. We found that (i) computer usage profiles have the\npotential to uniquely characterize computer users (with a maximum F-score of\n99.94%); (ii) network-related events were the most useful features to properly\nrecognize profiles (95.14% of the top features distinguishing users being\nnetwork-related); (iii) user profiles were mostly inconsistent over the 8-week\ndata collection period, with 92.86% of users exhibiting drifts in terms of time\nand usage habits; and (iv) online models are better suited to handle computer\nusage profiles compared to offline models (maximum F-score for each approach\nwas 95.99% and 99.94%, respectively).",
          "link": "http://arxiv.org/abs/2105.09900",
          "publishedOn": "2021-05-23T06:10:39.909Z",
          "wordCount": 613,
          "title": "Computer Users Have Unique Yet Temporally Inconsistent Computer Usage Profiles. (arXiv:2105.09900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1812.00002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1\">Sen Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Congzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>",
          "description": "Interactive news recommendation has been launched and attracted much\nattention recently. In this scenario, user's behavior evolves from single click\nbehavior to multiple behaviors including like, comment, share etc. However,\nmost of the existing methods still use single click behavior as the unique\ncriterion of judging user's preferences. Further, although heterogeneous graphs\nhave been applied in different areas, a proper way to construct a heterogeneous\ngraph for interactive news data with an appropriate learning mechanism on it is\nstill desired. To address the above concerns, we propose a graph-based\nbehavior-aware network, which simultaneously considers six different types of\nbehaviors as well as user's demand on the news diversity. We have three main\nsteps. First, we build an interaction behavior graph for multi-level and\nmulti-category data. Second, we apply DeepWalk on the behavior graph to obtain\nentity semantics, then build a graph-based convolutional neural network called\nG-CNN to learn news representations, and an attention-based LSTM to learn\nbehavior sequence representations. Third, we introduce core and coritivity\nfeatures for the behavior graph, which measure the concentration degree of\nuser's interests. These features affect the trade-off between accuracy and\ndiversity of our personalized recommendation system. Taking these features into\naccount, our system finally achieves recommending news to different users at\ntheir different levels of concentration degrees.",
          "link": "http://arxiv.org/abs/1812.00002",
          "publishedOn": "2021-05-23T06:10:39.898Z",
          "wordCount": 678,
          "title": "The Graph-Based Behavior-Aware Recommendation for Interactive News. (arXiv:1812.00002v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wangyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from human's visual and intuitive perspective. We take\nthe first step to bridge the gap by proposing a deep learning-based technique\nto automatically classify road networks into four classes on a visual basis.\nThe method is implemented by generating an image of the street network (Colored\nRoad Hierarchy Diagram), which we introduce in this paper, and classifying it\nusing a deep convolutional neural network (ResNet-34). The model achieves an\noverall classification accuracy of 0.875. Nine cities around the world are\nselected as the study areas and their road networks are acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through a\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: the\neffectiveness of our human perception augmentation is examined by a case study\nof urban vitality prediction. An advanced tree-based regression model is for\nthe first time designated to establish the relationship between morphological\nindices and vitality indicators. A positive effect of human perception\naugmentation is detected in the comparative experiment of baseline model and\naugmented model. This work expands the toolkit of quantitative urban morphology\nstudy with new techniques, supporting further studies in the future.",
          "link": "http://arxiv.org/abs/2105.09908",
          "publishedOn": "2021-05-23T06:10:39.891Z",
          "wordCount": 703,
          "title": "Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1\">Manav Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1\">Peter Jakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1\">Tobias Schmid-Schirling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Multi-view classification is inspired by the behavior of humans, especially\nwhen fine-grained features or in our case rarely occurring anomalies are to be\ndetected. Current contributions point to the problem of how high-dimensional\ndata can be fused. In this work, we build upon the deep support vector data\ndescription algorithm and address multi-perspective anomaly detection using\nthree different fusion techniques i.e. early fusion, late fusion, and late\nfusion with multiple decoders. We employ different augmentation techniques with\na denoising process to deal with scarce one-class data, which further improves\nthe performance (ROC AUC = 80\\%). Furthermore, we introduce the dices dataset\nthat consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g. drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed approach\nexceeds the state-of-the-art on both the MNIST and dices datasets. To the best\nof our knowledge, this is the first work that focuses on addressing\nmulti-perspective anomaly detection in images by jointly using different\nperspectives together with one single objective function for anomaly detection.",
          "link": "http://arxiv.org/abs/2105.09903",
          "publishedOn": "2021-05-23T06:10:39.885Z",
          "wordCount": 633,
          "title": "Multi-Perspective Anomaly Detection. (arXiv:2105.09903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.13934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>",
          "description": "Methods to quantify the complexity of trajectory datasets are still a missing\npiece in benchmarking human trajectory prediction models. In order to gain a\nbetter understanding of the complexity of trajectory prediction tasks and\nfollowing the intuition, that more complex datasets contain more information,\nan approach for quantifying the amount of information contained in a dataset\nfrom a prototype-based dataset representation is proposed. The dataset\nrepresentation is obtained by first employing a non-trivial spatial sequence\nalignment, which enables a subsequent learning vector quantization (LVQ) stage.\nA large-scale complexity analysis is conducted on several human trajectory\nprediction benchmarking datasets, followed by a brief discussion on indications\nfor human trajectory prediction and benchmarking.",
          "link": "http://arxiv.org/abs/2005.13934",
          "publishedOn": "2021-05-23T06:10:39.867Z",
          "wordCount": 600,
          "title": "Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction. (arXiv:2005.13934v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanli Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden M. Lake</a>",
          "description": "Humans are highly efficient learners, with the ability to grasp the meaning\nof a new concept from just a few examples. Unlike popular computer vision\nsystems, humans can flexibly leverage the compositional structure of the visual\nworld, understanding new concepts as combinations of existing concepts. In the\ncurrent paper, we study how people learn different types of visual\ncompositions, using abstract visual forms with rich relational structure. We\nfind that people can make meaningful compositional generalizations from just a\nfew examples in a variety of scenarios, and we develop a Bayesian program\ninduction model that provides a close fit to the behavioral data. Unlike past\nwork examining special cases of compositionality, our work shows how a single\ncomputational approach can account for many distinct types of compositional\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09848",
          "publishedOn": "2021-05-23T06:10:39.860Z",
          "wordCount": 587,
          "title": "Flexible Compositional Learning of Structured Visual Concepts. (arXiv:2105.09848v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhuangwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanbu Guo</a>",
          "description": "Chinese word segmentation (CWS) is the basic of Chinese natural language\nprocessing (NLP). The quality of word segmentation will directly affect the\nrest of NLP tasks. Recently, with the artificial intelligence tide rising\nagain, Long Short-Term Memory (LSTM) neural network, as one of easily modeling\nin sequence, has been widely utilized in various kinds of NLP tasks, and\nfunctions well. Attention mechanism is an ingenious method to solve the memory\ncompression problem on LSTM. Furthermore, inspired by the powerful abilities of\nbidirectional LSTM models for modeling sequence and CRF model for decoding, we\npropose a Bidirectional LSTM-CRF Attention-based Model in this paper.\nExperiments on PKU and MSRA benchmark datasets show that our model performs\nbetter than the baseline methods modeling by other neural networks.",
          "link": "http://arxiv.org/abs/2105.09681",
          "publishedOn": "2021-05-23T06:10:39.853Z",
          "wordCount": 546,
          "title": "Bidirectional LSTM-CRF Attention-based Model for Chinese Word Segmentation. (arXiv:2105.09681v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$~GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.",
          "link": "http://arxiv.org/abs/2105.09858",
          "publishedOn": "2021-05-23T06:10:39.847Z",
          "wordCount": 646,
          "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>",
          "description": "Recommender systems are gaining increasing and critical impacts on human and\nsociety since a growing number of users use them for information seeking and\ndecision making. Therefore, it is crucial to address the potential unfairness\nproblems in recommendations. Just like users have personalized preferences on\nitems, users' demands for fairness are also personalized in many scenarios.\nTherefore, it is important to provide personalized fair recommendations for\nusers to satisfy their personalized fairness demands. Besides, previous works\non fair recommendation mainly focus on association-based fairness. However, it\nis important to advance from associative fairness notions to causal fairness\nnotions for assessing fairness more properly in recommender systems. Based on\nthe above considerations, this paper focuses on achieving personalized\ncounterfactual fairness for users in recommender systems. To this end, we\nintroduce a framework for achieving counterfactually fair recommendations\nthrough adversary learning by generating feature-independent user embeddings\nfor recommendation. The framework allows recommender systems to achieve\npersonalized fairness for users while also covering non-personalized\nsituations. Experiments on two real-world datasets with shallow and deep\nrecommendation algorithms show that our method can generate fairer\nrecommendations for users with a desirable recommendation performance.",
          "link": "http://arxiv.org/abs/2105.09829",
          "publishedOn": "2021-05-23T06:10:39.840Z",
          "wordCount": 630,
          "title": "Towards Personalized Fairness based on Causal Notion. (arXiv:2105.09829v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09866",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Guet_C/0/1/0/all/0/1\">Claude Guet</a>",
          "description": "Solving physics problems for which we know the equations, boundary conditions\nand symmetries can be done by deep learning. The constraints can be either\nimposed as terms in a loss function or used to formulate a neural ansatz. In\nthe present case study, we calculate the induced field inside and outside a\ndielectric cube placed in a uniform electric field, wherein the dielectric\nmismatch at edges and corners of the cube makes accurate calculations\nnumerically challenging. The electric potential is expressed as an ansatz\nincorporating neural networks with known leading order behaviors and symmetries\nand the Laplace's equation is then solved with boundary conditions at the\ndielectric interface by minimizing a loss function. The loss function ensures\nthat both Laplace's equation and boundary conditions are satisfied everywhere\ninside a large solution domain. We study how the electric potential inside and\noutside a quasi-cubic particle evolves through a sequence of shapes from a\nsphere to a cube. The neural network being differentiable, it is\nstraightforward to calculate the electric field over the whole domain, the\ninduced surface charge distribution and the polarizability. The neural network\nbeing retentive, one can efficiently follow how the field changes upon\nparticle's shape or dielectric constant by iterating from any previously\nconverged solution. The present work's objective is two-fold, first to show how\nan a priori knowledge can be incorporated into neural networks to achieve\nefficient learning and second to apply the method and study how the induced\nfield and polarizability change when a dielectric particle progressively\nchanges its shape from a sphere to a cube.",
          "link": "http://arxiv.org/abs/2105.09866",
          "publishedOn": "2021-05-23T06:10:39.834Z",
          "wordCount": 707,
          "title": "Deep learning in physics: a study of dielectric quasi-cubic particles in a uniform electric field. (arXiv:2105.09866v1 [physics.class-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7~GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.",
          "link": "http://arxiv.org/abs/2105.09856",
          "publishedOn": "2021-05-23T06:10:39.814Z",
          "wordCount": 635,
          "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awad_N/0/1/0/all/0/1\">Noor Awad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1\">Neeratyoy Mallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "Modern machine learning algorithms crucially rely on several design decisions\nto achieve strong performance, making the problem of Hyperparameter\nOptimization (HPO) more important than ever. Here, we combine the advantages of\nthe popular bandit-based HPO method Hyperband (HB) and the evolutionary search\napproach of Differential Evolution (DE) to yield a new HPO method which we call\nDEHB. Comprehensive results on a very broad range of HPO problems, as well as a\nwide range of tabular benchmarks from neural architecture search, demonstrate\nthat DEHB achieves strong performance far more robustly than all previous HPO\nmethods we are aware of, especially for high-dimensional problems with discrete\ninput dimensions. For example, DEHB is up to 1000x faster than random search.\nIt is also efficient in computational time, conceptually simple and easy to\nimplement, positioning it well to become a new default HPO method.",
          "link": "http://arxiv.org/abs/2105.09821",
          "publishedOn": "2021-05-23T06:10:39.793Z",
          "wordCount": 571,
          "title": "DEHB: Evolutionary Hyberband for Scalable, Robust and Efficient Hyperparameter Optimization. (arXiv:2105.09821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuangshuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karayiannidis_Y/0/1/0/all/0/1\">Yiannis Karayiannidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bjorkman_M/0/1/0/all/0/1\">M&#xe5;rten Bj&#xf6;rkman</a>",
          "description": "Learning generative models and inferring latent trajectories have shown to be\nchallenging for time series due to the intractable marginal likelihoods of\nflexible generative models. It can be addressed by surrogate objectives for\noptimization. We propose Monte Carlo filtering objectives (MCFOs), a family of\nvariational objectives for jointly learning parametric generative models and\namortized adaptive importance proposals of time series. MCFOs extend the\nchoices of likelihood estimators beyond Sequential Monte Carlo in\nstate-of-the-art objectives, possess important properties revealing the factors\nfor the tightness of objectives, and allow for less biased and variant gradient\nestimates. We demonstrate that the proposed MCFOs and gradient estimations lead\nto efficient and stable model learning, and learned generative models well\nexplain data and importance proposals are more sample efficient on various\nkinds of time series data.",
          "link": "http://arxiv.org/abs/2105.09801",
          "publishedOn": "2021-05-23T06:10:39.786Z",
          "wordCount": 595,
          "title": "Monte Carlo Filtering Objectives: A New Family of Variational Objectives to Learn Generative Model and Neural Adaptive Proposal for Time Series. (arXiv:2105.09801v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ran Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rujun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhuoling Xiao</a>",
          "description": "The technology for Visual Odometry (VO) that estimates the position and\norientation of the moving object through analyzing the image sequences captured\nby on-board cameras, has been well investigated with the rising interest in\nautonomous driving. This paper studies monocular VO from the perspective of\nDeep Learning (DL). Unlike most current learning-based methods, our approach,\ncalled DeepAVO, is established on the intuition that features contribute\ndiscriminately to different motion patterns. Specifically, we present a novel\nfour-branch network to learn the rotation and translation by leveraging\nConvolutional Neural Networks (CNNs) to focus on different quadrants of optical\nflow input. To enhance the ability of feature selection, we further introduce\nan effective channel-spatial attention mechanism to force each branch to\nexplicitly distill related information for specific Frame to Frame (F2F) motion\nestimation. Experiments on various datasets involving outdoor driving and\nindoor walking scenarios show that the proposed DeepAVO outperforms the\nstate-of-the-art monocular methods by a large margin, demonstrating competitive\nperformance to the stereo VO algorithm and verifying promising potential for\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09899",
          "publishedOn": "2021-05-23T06:10:39.777Z",
          "wordCount": 619,
          "title": "DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry. (arXiv:2105.09899v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Cong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jingwen Leng</a>",
          "description": "Leveraging sparsity in deep neural network (DNN) models is promising for\naccelerating model inference. Yet existing GPUs can only leverage the sparsity\nfrom weights but not activations, which are dynamic, unpredictable, and hence\nchallenging to exploit. In this work, we propose a novel architecture to\nefficiently harness the dual-side sparsity (i.e., weight and activation\nsparsity). We take a systematic approach to understand the (dis)advantages of\nprevious sparsity-related architectures and propose a novel, unexplored\nparadigm that combines outer-product computation primitive and bitmap-based\nencoding format. We demonstrate the feasibility of our design with minimal\nchanges to the existing production-scale inner-product-based Tensor Core. We\npropose a set of novel ISA extensions and co-design the matrix-matrix\nmultiplication and convolution algorithms, which are the two dominant\ncomputation patterns in today's DNN models, to exploit our new dual-side sparse\nTensor Core. Our evaluation shows that our design can fully unleash the\ndual-side DNN sparsity and improve the performance by up to one order of\nmagnitude with \\hl{small} hardware overhead.",
          "link": "http://arxiv.org/abs/2105.09564",
          "publishedOn": "2021-05-23T06:10:39.770Z",
          "wordCount": 586,
          "title": "Dual-side Sparse Tensor Core. (arXiv:2105.09564v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09720",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mudiyanselage_T/0/1/0/all/0/1\">Thosini Bamunu Mudiyanselage</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Senanayake_N/0/1/0/all/0/1\">Nipuna Senanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1\">Chunyan Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yi Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanqing Zhang</a>",
          "description": "The novel corona virus (Covid-19) has introduced significant challenges due\nto its rapid spreading nature through respiratory transmission. As a result,\nthere is a huge demand for Artificial Intelligence (AI) based quick disease\ndiagnosis methods as an alternative to high demand tests such as Polymerase\nChain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective\nradiography technique due to resource availability and quick screening. But, a\nsufficient and systematic data collection that is required by complex deep\nleaning (DL) models is more difficult and hence there are recent efforts that\nutilize transfer learning to address this issue. Still these transfer learnt\nmodels suffer from lack of generalization and increased bias to the training\ndataset resulting poor performance for unseen data. Limited correlation of the\ntransferred features from the pre-trained model to a specific medical imaging\ndomain like X-ray and overfitting on fewer data can be reasons for this\ncircumstance. In this work, we propose a novel Graph Convolution Neural Network\n(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR\nimages and meta information about patients. The proposed method exploits\nimportant relational knowledge between data instances and their features using\ngraph representation and applies convolution to learn the graph data which is\nnot possible with conventional convolution on Euclidean domain. The results of\nextensive experiments of proposed model on binary (Covid vs normal) and three\nclass (Covid, normal, other pneumonia) classification problems outperform\ndifferent benchmark transfer learnt models, hence overcoming the aforementioned\ndrawbacks.",
          "link": "http://arxiv.org/abs/2105.09720",
          "publishedOn": "2021-05-23T06:10:39.750Z",
          "wordCount": 747,
          "title": "Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks. (arXiv:2105.09720v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1\">Kasra Arnavaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1\">Jelena M. Krivokapic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1\">Silja Heilmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1\">Jakob Andreas B&#xe6;rentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1\">Pia Nyeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>",
          "description": "Motivated by a challenging tubular network segmentation task, this paper\ntackles two commonly encountered problems in biomedical imaging: Topological\nconsistency of the segmentation, and limited annotations. We propose a\ntopological score which measures both topological and geometric consistency\nbetween the predicted and ground truth segmentations, applied for model\nselection and validation. We apply our topological score in three scenarios: i.\na U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised\nU-net architecture, which offers a straightforward approach to jointly training\nthe network both as an autoencoder and a segmentation algorithm. This allows us\nto utilize un-annotated data for training a representation that generalizes\nacross test data variability, in spite of our annotated training data having\nvery limited variation. Our contributions are validated on a challenging\nsegmentation task, locating tubular structures in the fetal pancreas from noisy\nlive imaging confocal microscopy.",
          "link": "http://arxiv.org/abs/2105.09737",
          "publishedOn": "2021-05-23T06:10:39.743Z",
          "wordCount": 590,
          "title": "Semi-supervised, Topology-Aware Segmentation of Tubular Structures from Live Imaging 3D Microscopy. (arXiv:2105.09737v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yue Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>",
          "description": "The Graph Augmented Multi-layer Perceptron (GA-MLP) model is an attractive\nalternative to Graph Neural Networks (GNNs). This is because it is resistant to\nthe over-smoothing problem, and deeper GA-MLP models yield better performance.\nGA-MLP models are traditionally optimized by the Stochastic Gradient Descent\n(SGD). However, SGD suffers from the layer dependency problem, which prevents\nthe gradients of different layers of GA-MLP models from being calculated in\nparallel. In this paper, we propose a parallel deep learning Alternating\nDirection Method of Multipliers (pdADMM) framework to achieve model\nparallelism: parameters in each layer of GA-MLP models can be updated in\nparallel. The extended pdADMM-Q algorithm reduces communication cost by\nutilizing the quantization technique. Theoretical convergence to a critical\npoint of the pdADMM algorithm and the pdADMM-Q algorithm is provided with a\nsublinear convergence rate $o(1/k)$. Extensive experiments in six benchmark\ndatasets demonstrate that the pdADMM can lead to high speedup, and outperforms\nall the existing state-of-the-art comparison methods.",
          "link": "http://arxiv.org/abs/2105.09837",
          "publishedOn": "2021-05-23T06:10:39.688Z",
          "wordCount": 638,
          "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on Gradient-Free ADMM framework. (arXiv:2105.09837v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Thai_B/0/1/0/all/0/1\">Binh Nguyen-Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1\">Catherine Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawi_N/0/1/0/all/0/1\">Nadia Badawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "The absence or abnormality of fidgety movements of joints or limbs is\nstrongly indicative of cerebral palsy in infants. Developing computer-based\nmethods for assessing infant movements in videos is pivotal for improved\ncerebral palsy screening. Most existing methods use appearance-based features\nand are thus sensitive to strong but irrelevant signals caused by background\nclutter or a moving camera. Moreover, these features are computed over the\nwhole frame, thus they measure gross whole body movements rather than specific\njoint/limb motion.\n\nAddressing these challenges, we develop and validate a new method for fidgety\nmovement assessment from consumer-grade videos using human poses extracted from\nshort clips. Human poses capture only relevant motion profiles of joints and\nlimbs and are thus free from irrelevant appearance artifacts. The dynamics and\ncoordination between joints are modeled using spatio-temporal graph\nconvolutional networks. Frames and body parts that contain discriminative\ninformation about fidgety movements are selected through a spatio-temporal\nattention mechanism. We validate the proposed model on the cerebral palsy\nscreening task using a real-life consumer-grade video dataset collected at an\nAustralian hospital through the Cerebral Palsy Alliance, Australia. Our\nexperiments show that the proposed method achieves the ROC-AUC score of 81.87%,\nsignificantly outperforming existing competing methods with better\ninterpretability.",
          "link": "http://arxiv.org/abs/2105.09783",
          "publishedOn": "2021-05-23T06:10:39.663Z",
          "wordCount": 657,
          "title": "A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos. (arXiv:2105.09783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Devleena Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_Y/0/1/0/all/0/1\">Yasutaka Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivek_R/0/1/0/all/0/1\">Rajan P. Vivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_N/0/1/0/all/0/1\">Naoto Takeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fish_S/0/1/0/all/0/1\">Sean T. Fish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploetz_T/0/1/0/all/0/1\">Thomas Ploetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1\">Sonia Chernova</a>",
          "description": "Smart home environments are designed to provide services that help improve\nthe quality of life for the occupant via a variety of sensors and actuators\ninstalled throughout the space. Many automated actions taken by a smart home\nare governed by the output of an underlying activity recognition system.\nHowever, activity recognition systems may not be perfectly accurate and\ntherefore inconsistencies in smart home operations can lead a user to wonder\n\"why did the smart home do that?\" In this work, we build on insights from\nExplainable Artificial Intelligence (XAI) techniques to contribute\ncomputational methods for explainable activity recognition. Specifically, we\ngenerate explanations for smart home activity recognition systems that explain\nwhat about an activity led to the given classification. To do so, we introduce\nfour computational techniques for generating natural language explanations of\nsmart home data and compare their effectiveness at generating meaningful\nexplanations. Through a study with everyday users, we evaluate user preferences\ntowards the four explanation types. Our results show that the leading approach,\nSHAP, has a 92% success rate in generating accurate explanations. Moreover, 84%\nof sampled scenarios users preferred natural language explanations over a\nsimple activity label, underscoring the need for explainable activity\nrecognition systems. Finally, we show that explanations generated by some XAI\nmethods can lead users to lose confidence in the accuracy of the underlying\nactivity recognition model, while others lead users to gain confidence. Taking\nall studied factors into consideration, we make a recommendation regarding\nwhich existing XAI method leads to the best performance in the domain of smart\nhome automation, and discuss a range of topics for future work in this area.",
          "link": "http://arxiv.org/abs/2105.09787",
          "publishedOn": "2021-05-23T06:10:39.642Z",
          "wordCount": 704,
          "title": "Explainable Activity Recognition for Smart Home Systems. (arXiv:2105.09787v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09788",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1\">Ruiqi Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_G/0/1/0/all/0/1\">Ganggang Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shang_Z/0/1/0/all/0/1\">Zuofeng Shang</a>",
          "description": "When data is of an extraordinarily large size or physically stored in\ndifferent locations, the distributed nearest neighbor (NN) classifier is an\nattractive tool for classification. We propose a novel distributed adaptive NN\nclassifier for which the number of nearest neighbors is a tuning parameter\nstochastically chosen by a data-driven criterion. An early stopping rule is\nproposed when searching for the optimal tuning parameter, which not only speeds\nup the computation but also improves the finite sample performance of the\nproposed Algorithm. Convergence rate of excess risk of the distributed adaptive\nNN classifier is investigated under various sub-sample size compositions. In\nparticular, we show that when the sub-sample sizes are sufficiently large, the\nproposed classifier achieves the nearly optimal convergence rate. Effectiveness\nof the proposed approach is demonstrated through simulation studies as well as\nan empirical application to a real-world dataset.",
          "link": "http://arxiv.org/abs/2105.09788",
          "publishedOn": "2021-05-23T06:10:39.622Z",
          "wordCount": 571,
          "title": "Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory. (arXiv:2105.09788v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borkar_J/0/1/0/all/0/1\">Jaydeep Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "There has been a rise in the use of Machine Learning as a Service (MLaaS)\nVision APIs as they offer multiple services including pre-built models and\nalgorithms, which otherwise take a huge amount of resources if built from\nscratch. As these APIs get deployed for high-stakes applications, it's very\nimportant that they are robust to different manipulations. Recent works have\nonly focused on typical adversarial attacks when evaluating the robustness of\nvision APIs. We propose two new aspects of adversarial image generation methods\nand evaluate them on the robustness of Google Cloud Vision API's optical\ncharacter recognition service and object detection APIs deployed in real-world\nsettings such as sightengine.com, picpurify.com, Google Cloud Vision API, and\nMicrosoft Azure's Computer Vision API. Specifically, we go beyond the\nconventional small-noise adversarial attacks and introduce secret embedding and\ntransparent adversarial examples as a simpler way to evaluate robustness. These\nmethods are so straightforward that even non-specialists can craft such\nattacks. As a result, they pose a serious threat where APIs are used for\nhigh-stakes applications. Our transparent adversarial examples successfully\nevade state-of-the art object detections APIs such as Azure Cloud Vision\n(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).\n90% of the images have a secret embedded text that successfully fools the\nvision of time-limited humans but is detected by Google Cloud Vision API's\noptical character recognition. Complementing to current research, our results\nprovide simple but unconventional methods on robustness evaluation.",
          "link": "http://arxiv.org/abs/2105.09685",
          "publishedOn": "2021-05-23T06:10:39.616Z",
          "wordCount": 694,
          "title": "Simple Transparent Adversarial Examples. (arXiv:2105.09685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09716",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">Yongfeng Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhao_M/0/1/0/all/0/1\">Mingming Zhao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wen_Z/0/1/0/all/0/1\">Zaiwen Wen</a>",
          "description": "In this paper, we consider the linear programming (LP) formulation for deep\nreinforcement learning. The number of the constraints depends on the size of\nstate and action spaces, which makes the problem intractable in large or\ncontinuous environments. The general augmented Lagrangian method suffers the\ndouble-sampling obstacle in solving the LP. Namely, the conditional\nexpectations originated from the constraint functions and the quadratic\npenalties in the augmented Lagrangian function impose difficulties in sampling\nand evaluation. Motivated from the updates of the multipliers, we overcome the\nobstacles in minimizing the augmented Lagrangian function by replacing the\nintractable conditional expectations with the multipliers. Therefore, a deep\nparameterized augment Lagrangian method is proposed. Furthermore, the\nreplacement provides a promising breakthrough to integrate the two steps in the\naugmented Lagrangian method into a single constrained problem. A general\ntheoretical analysis shows that the solutions generated from a sequence of the\nconstrained optimizations converge to the optimal solution of the LP if the\nerror is controlled properly. A theoretical analysis on the quadratic penalty\nalgorithm under neural tangent kernel setting shows the residual can be\narbitrarily small if the parameter in network and optimization algorithm is\nchosen suitably. Preliminary experiments illustrate that our method is\ncompetitive to other state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2105.09716",
          "publishedOn": "2021-05-23T06:10:39.608Z",
          "wordCount": 642,
          "title": "A Stochastic Composite Augmented Lagrangian Method For Reinforcement Learning. (arXiv:2105.09716v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pryor_W/0/1/0/all/0/1\">Will Pryor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnoy_Y/0/1/0/all/0/1\">Yotam Barnoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raval_S/0/1/0/all/0/1\">Suraj Raval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mair_L/0/1/0/all/0/1\">Lamar Mair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerner_D/0/1/0/all/0/1\">Daniel Lerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erin_O/0/1/0/all/0/1\">Onder Erin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory D. Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Mercado_Y/0/1/0/all/0/1\">Yancy Diaz-Mercado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krieger_A/0/1/0/all/0/1\">Axel Krieger</a>",
          "description": "Real-time visual localization of needles is necessary for various surgical\napplications, including surgical automation and visual feedback. In this study\nwe investigate localization and autonomous robotic control of needles in the\ncontext of our magneto-suturing system. Our system holds the potential for\nsurgical manipulation with the benefit of minimal invasiveness and reduced\npatient side effects. However, the non-linear magnetic fields produce\nunintuitive forces and demand delicate position-based control that exceeds the\ncapabilities of direct human manipulation. This makes automatic needle\nlocalization a necessity. Our localization method combines neural network-based\nsegmentation and classical techniques, and we are able to consistently locate\nour needle with 0.73 mm RMS error in clean environments and 2.72 mm RMS error\nin challenging environments with blood and occlusion. The average localization\nRMS error is 2.16 mm for all environments we used in the experiments. We\ncombine this localization method with our closed-loop feedback control system\nto demonstrate the further applicability of localization to autonomous control.\nOur needle is able to follow a running suture path in (1) no blood, no tissue;\n(2) heavy blood, no tissue; (3) no blood, with tissue; and (4) heavy blood,\nwith tissue environments. The tip position tracking error ranges from 2.6 mm to\n3.7 mm RMS, opening the door towards autonomous suturing tasks.",
          "link": "http://arxiv.org/abs/2105.09481",
          "publishedOn": "2021-05-23T06:10:39.583Z",
          "wordCount": 664,
          "title": "Localization and Control of Magnetic Suture Needles in Cluttered Surgical Site with Blood and Tissue. (arXiv:2105.09481v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1\">Keyue Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuzhuo Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Distantly supervised (DS) relation extraction (RE) has attracted much\nattention in the past few years as it can utilize large-scale auto-labeled\ndata. However, its evaluation has long been a problem: previous works either\ntook costly and inconsistent methods to manually examine a small sample of\nmodel predictions, or directly test models on auto-labeled data -- which, by\nour check, produce as much as 53% wrong labels at the entity pair level in the\npopular NYT10 dataset. This problem has not only led to inaccurate evaluation,\nbut also made it hard to understand where we are and what's left to improve in\nthe research of DS-RE. To evaluate DS-RE models in a more credible way, we\nbuild manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,\nand thoroughly evaluate several competitive models, especially the latest\npre-trained ones. The experimental results show that the manual evaluation can\nindicate very different conclusions from automatic ones, especially some\nunexpected observations, e.g., pre-trained models can achieve dominating\nperformance while being more susceptible to false-positives compared to\nprevious methods. We hope that both our manual test sets and novel observations\ncan help advance future DS-RE research.",
          "link": "http://arxiv.org/abs/2105.09543",
          "publishedOn": "2021-05-23T06:10:39.576Z",
          "wordCount": 643,
          "title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction. (arXiv:2105.09543v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09679",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Kimura_S/0/1/0/all/0/1\">Shun Kimura</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ota_K/0/1/0/all/0/1\">Keisuke Ota</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Takeda_K/0/1/0/all/0/1\">Koujin Takeda</a>",
          "description": "Neuronal ensemble inference is a significant problem in the study of\nbiological neural networks. Various methods have been proposed for ensemble\ninference from experimental data of neuronal activity. Among them, Bayesian\ninference approach with generative model was proposed recently. However, this\nmethod requires large computational cost for appropriate inference. In this\nwork, we give an improved Bayesian inference algorithm by modifying update rule\nin Markov chain Monte Carlo method and introducing the idea of simulated\nannealing for hyperparameter control. We compare the performance of ensemble\ninference between our algorithm and the original one, and discuss the advantage\nof our method.",
          "link": "http://arxiv.org/abs/2105.09679",
          "publishedOn": "2021-05-23T06:10:39.539Z",
          "wordCount": 556,
          "title": "Improved Neuronal Ensemble Inference with Generative Model and MCMC. (arXiv:2105.09679v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gruttemeier_N/0/1/0/all/0/1\">Niels Gr&#xfc;ttemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1\">Christian Komusiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morawietz_N/0/1/0/all/0/1\">Nils Morawietz</a>",
          "description": "A Bayesian network is a directed acyclic graph that represents statistical\ndependencies between variables of a joint probability distribution. A\nfundamental task in data science is to learn a Bayesian network from observed\ndata. \\textsc{Polytree Learning} is the problem of learning an optimal Bayesian\nnetwork that fulfills the additional property that its underlying undirected\ngraph is a forest. In this work, we revisit the complexity of \\textsc{Polytree\nLearning}. We show that \\textsc{Polytree Learning} can be solved in $3^n \\cdot\n|I|^{\\mathcal{O}(1)}$ time where $n$ is the number of variables and $|I|$ is\nthe total instance size. Moreover, we consider the influence of the number of\nvariables $d$ that might receive a nonempty parent set in the final DAG on the\ncomplexity of \\textsc{Polytree Learning}. We show that \\textsc{Polytree\nLearning} has no $f(d)\\cdot |I|^{\\mathcal{O}(1)}$-time algorithm, unlike\nBayesian network learning which can be solved in $2^d \\cdot\n|I|^{\\mathcal{O}(1)}$ time. We show that, in contrast, if $d$ and the maximum\nparent set size are bounded, then we can obtain efficient algorithms.",
          "link": "http://arxiv.org/abs/2105.09675",
          "publishedOn": "2021-05-23T06:10:39.519Z",
          "wordCount": 602,
          "title": "On the Parameterized Complexity of Polytree Learning. (arXiv:2105.09675v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09670",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_H/0/1/0/all/0/1\">Huolan Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yongkai Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_C/0/1/0/all/0/1\">Chenguang Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cheng_H/0/1/0/all/0/1\">Huimin Cheng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhong_W/0/1/0/all/0/1\">Wenxuan Zhong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>",
          "description": "Background: Extensive clinical evidence suggests that a preventive screening\nof coronary heart disease (CHD) at an earlier stage can greatly reduce the\nmortality rate. We use 64 two-dimensional speckle tracking echocardiography\n(2D-STE) features and seven clinical features to predict whether one has CHD.\nMethods: We develop a machine learning approach that integrates a number of\npopular classification methods together by model stacking, and generalize the\ntraditional stacking method to a two-step stacking method to improve the\ndiagnostic performance. Results: By borrowing strengths from multiple\nclassification models through the proposed method, we improve the CHD\nclassification accuracy from around 70% to 87.7% on the testing set. The\nsensitivity of the proposed method is 0.903 and the specificity is 0.843, with\nan AUC of 0.904, which is significantly higher than those of the individual\nclassification models. Conclusions: Our work lays a foundation for the\ndeployment of speckle tracking echocardiography-based screening tools for\ncoronary heart disease.",
          "link": "http://arxiv.org/abs/2105.09670",
          "publishedOn": "2021-05-23T06:10:39.511Z",
          "wordCount": 612,
          "title": "Ensemble machine learning approach for screening of coronary heart disease based on echocardiography and risk factors. (arXiv:2105.09670v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1\">Danilo Dessi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1\">Rim Helaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1\">Daniele Riboni</a>",
          "description": "Today, we are seeing an ever-increasing number of clinical notes that contain\nclinical results, images, and textual descriptions of patient's health state.\nAll these data can be analyzed and employed to cater novel services that can\nhelp people and domain experts with their common healthcare tasks. However,\nmany technologies such as Deep Learning and tools like Word Embeddings have\nstarted to be investigated only recently, and many challenges remain open when\nit comes to healthcare domain applications. To address these challenges, we\npropose the use of Deep Learning and Word Embeddings for identifying sixteen\nmorbidity types within textual descriptions of clinical records. For this\npurpose, we have used a Deep Learning model based on Bidirectional Long-Short\nTerm Memory (LSTM) layers which can exploit state-of-the-art vector\nrepresentations of data such as Word Embeddings. We have employed pre-trained\nWord Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained\non the target domain. Furthermore, we have compared the performances of the\ndeep learning approaches against the traditional tf-idf using Support Vector\nMachine and Multilayer perceptron (our baselines). From the obtained results it\nseems that the latter outperforms the combination of Deep Learning approaches\nusing any word embeddings. Our preliminary results indicate that there are\nspecific features that make the dataset biased in favour of traditional machine\nlearning approaches.",
          "link": "http://arxiv.org/abs/2105.09632",
          "publishedOn": "2021-05-23T06:10:39.502Z",
          "wordCount": 680,
          "title": "TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1\">Amit Daniely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granot_E/0/1/0/all/0/1\">Elad Granot</a>",
          "description": "As machine learning increasingly becomes more prevalent in our everyday life,\nmany organizations offer neural-networks based services as a black-box. The\nreasons for hiding a learning model may vary: e.g., preventing copying of its\nbehavior or keeping back an adversarial from reverse-engineering its mechanism\nand revealing sensitive information about its training data.\n\nHowever, even as a black-box, some information can still be discovered by\nspecific queries. In this work, we show a polynomial-time algorithm that uses a\npolynomial number of queries to mimic precisely the behavior of a three-layer\nneural network that uses ReLU activation.",
          "link": "http://arxiv.org/abs/2105.09673",
          "publishedOn": "2021-05-23T06:10:39.448Z",
          "wordCount": 521,
          "title": "An Exact Poly-Time Membership-Queries Algorithm for Extraction a three-Layer ReLU Network. (arXiv:2105.09673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09618",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Malem_Shinitski_N/0/1/0/all/0/1\">Noa Malem-Shinitski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ojeda_C/0/1/0/all/0/1\">Cesar Ojeda</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Opper_M/0/1/0/all/0/1\">Manfred Opper</a>",
          "description": "Traditionally, Hawkes processes are used to model time--continuous point\nprocesses with history dependence. Here we propose an extended model where the\nself--effects are of both excitatory and inhibitory type and follow a Gaussian\nProcess. Whereas previous work either relies on a less flexible\nparameterization of the model, or requires a large amount of data, our\nformulation allows for both a flexible model and learning when data are scarce.\nWe continue the line of work of Bayesian inference for Hawkes processes, and\nour approach dispenses with the necessity of estimating a branching structure\nfor the posterior, as we perform inference on an aggregated sum of Gaussian\nProcesses. Efficient approximate Bayesian inference is achieved via data\naugmentation, and we describe a mean--field variational inference approach to\nlearn the model parameters. To demonstrate the flexibility of the model we\napply our methodology on data from three different domains and compare it to\npreviously reported results.",
          "link": "http://arxiv.org/abs/2105.09618",
          "publishedOn": "2021-05-23T06:10:39.438Z",
          "wordCount": 584,
          "title": "Nonlinear Hawkes Process with Gaussian Process Self Effects. (arXiv:2105.09618v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1\">Raluca Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepecki_J/0/1/0/all/0/1\">Jaroslaw Rzepecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuniga_E/0/1/0/all/0/1\">Evelyn Zuniga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costello_G/0/1/0/all/0/1\">Gavin Costello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1\">Guy Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1\">Ali Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>",
          "description": "A key challenge on the path to developing agents that learn complex\nhuman-like behavior is the need to quickly and accurately quantify\nhuman-likeness. While human assessments of such behavior can be highly\naccurate, speed and scalability are limited. We address these limitations\nthrough a novel automated Navigation Turing Test (ANTT) that learns to predict\nhuman judgments of human-likeness. We demonstrate the effectiveness of our\nautomated NTT on a navigation task in a complex 3D environment. We investigate\nsix classification models to shed light on the types of architectures best\nsuited to this task, and validate them against data collected through a human\nNTT. Our best models achieve high accuracy when distinguishing true human and\nagent behavior. At the same time, we show that predicting finer-grained human\nassessment of agents' progress towards human-like behavior remains unsolved.\nOur work takes an important step towards agents that more effectively learn\ncomplex human-like behavior.",
          "link": "http://arxiv.org/abs/2105.09637",
          "publishedOn": "2021-05-23T06:10:39.418Z",
          "wordCount": 589,
          "title": "Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation. (arXiv:2105.09637v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose {\\name}, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. {\\name} utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that {\\name} achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.",
          "link": "http://arxiv.org/abs/2105.09601",
          "publishedOn": "2021-05-23T06:10:39.393Z",
          "wordCount": 667,
          "title": "See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bountrogiannis_K/0/1/0/all/0/1\">Konstantinos Bountrogiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzagkarakis_G/0/1/0/all/0/1\">George Tzagkarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalides_P/0/1/0/all/0/1\">Panagiotis Tsakalides</a>",
          "description": "Due to the importance of the lower bounding distances and the attractiveness\nof symbolic representations, the family of symbolic aggregate approximations\n(SAX) has been used extensively for encoding time series data. However, typical\nSAX-based methods rely on two restrictive assumptions; the Gaussian\ndistribution and equiprobable symbols. This paper proposes two novel\ndata-driven SAX-based symbolic representations, distinguished by their\ndiscretization steps. The first representation, oriented for general data\ncompaction and indexing scenarios, is based on the combination of kernel\ndensity estimation and Lloyd-Max quantization to minimize the information loss\nand mean squared error in the discretization step. The second method, oriented\nfor high-level mining tasks, employs the Mean-Shift clustering method and is\nshown to enhance anomaly detection in the lower-dimensional space. Besides, we\nverify on a theoretical basis a previously observed phenomenon of the intrinsic\nprocess that results in a lower than the expected variance of the intermediate\npiecewise aggregate approximation. This phenomenon causes an additional\ninformation loss but can be avoided with a simple modification. The proposed\nrepresentations possess all the attractive properties of the conventional SAX\nmethod. Furthermore, experimental evaluation on real-world datasets\ndemonstrates their superiority compared to the traditional SAX and an\nalternative data-driven SAX variant.",
          "link": "http://arxiv.org/abs/2105.09592",
          "publishedOn": "2021-05-23T06:10:39.382Z",
          "wordCount": 634,
          "title": "Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zejin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>",
          "description": "The increasing concerns about data privacy and security drives the emergence\nof a new field of studying privacy-preserving machine learning from isolated\ndata sources, i.e., \\textit{federated learning}. Vertical federated learning,\nwhere different parties hold different features for common users, has a great\npotential of driving a more variety of business cooperation among enterprises\nin different fields. Decision tree models especially decision tree ensembles\nare a class of widely applied powerful machine learning models with high\ninterpretability and modeling efficiency. However, the interpretability are\ncompromised in these works such as SecureBoost since the feature names are not\nexposed to avoid possible data breaches due to the unprotected decision path.\nIn this paper, we shall propose Fed-EINI, an efficient and interpretable\ninference framework for federated decision tree models with only one round of\nmulti-party communication. We shall compute the candidate sets of leaf nodes\nbased on the local data at each party in parallel, followed by securely\ncomputing the weight of the only leaf node in the intersection of the candidate\nsets. We propose to protect the decision path by the efficient additively\nhomomorphic encryption method, which allows the disclosure of feature names and\nthus makes the federated decision trees interpretable. The advantages of\nFed-EINI will be demonstrated through theoretical analysis and extensive\nnumerical results. Experiments show that the inference efficiency is improved\nby over $50\\%$ in average.",
          "link": "http://arxiv.org/abs/2105.09540",
          "publishedOn": "2021-05-23T06:10:39.363Z",
          "wordCount": 684,
          "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voiculescu_I/0/1/0/all/0/1\">Irina Voiculescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>",
          "description": "Entanglement is a physical phenomenon, which has fueled recent successes of\nquantum algorithms. Although quantum neural networks (QNNs) have shown\npromising results in solving simple machine learning tasks recently, for the\ntime being, the effect of entanglement in QNNs and the behavior of QNNs in\nbinary pattern classification are still underexplored. In this work, we provide\nsome theoretical insight into the properties of QNNs by presenting and\nanalyzing a new form of invariance embedded in QNNs for both quantum binary\nclassification and quantum representation learning, which we term negational\nsymmetry. Given a quantum binary signal and its negational counterpart where a\nbitwise NOT operation is applied to each quantum bit of the binary signal, a\nQNN outputs the same logits. That is to say, QNNs cannot differentiate a\nquantum binary signal and its negational counterpart in a binary classification\ntask. We further empirically evaluate the negational symmetry of QNNs in binary\npattern classification tasks using Google's quantum computing framework. The\ntheoretical and experimental results suggest that negational symmetry is a\nfundamental property of QNNs, which is not shared by classical models. Our\nfindings also imply that negational symmetry is a double-edged sword in\npractical quantum applications.",
          "link": "http://arxiv.org/abs/2105.09580",
          "publishedOn": "2021-05-23T06:10:39.355Z",
          "wordCount": 634,
          "title": "Negational Symmetry of Quantum Neural Networks for Binary Pattern Classification. (arXiv:2105.09580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1\">Takashi Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1\">Liu Ziyin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masahito Ueda</a>",
          "description": "Stochastic gradient descent (SGD) undergoes complicated multiplicative noise\nfor the mean-square loss. We use this property of the SGD noise to derive a\nstochastic differential equation (SDE) with simpler additive noise by\nperforming a non-uniform transformation of the time variable. In the SDE, the\ngradient of the loss is replaced by that of the logarithmized loss.\nConsequently, we show that, near a local or global minimum, the stationary\ndistribution $P_\\mathrm{ss}(\\theta)$ of the network parameters $\\theta$ follows\na power-law with respect to the loss function $L(\\theta)$, i.e.\n$P_\\mathrm{ss}(\\theta)\\propto L(\\theta)^{-\\phi}$ with the exponent $\\phi$\nspecified by the mini-batch size, the learning rate, and the Hessian at the\nminimum. We obtain the escape rate formula from a local minimum, which is\ndetermined not by the loss barrier height $\\Delta L=L(\\theta^s)-L(\\theta^*)$\nbetween a minimum $\\theta^*$ and a saddle $\\theta^s$ but by the logarithmized\nloss barrier height $\\Delta\\log L=\\log[L(\\theta^s)/L(\\theta^*)]$. Our\nescape-rate formula explains an empirical fact that SGD prefers flat minima\nwith low effective dimensions.",
          "link": "http://arxiv.org/abs/2105.09557",
          "publishedOn": "2021-05-23T06:10:39.348Z",
          "wordCount": 604,
          "title": "Logarithmic landscape and power-law escape rate of SGD. (arXiv:2105.09557v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagtap_A/0/1/0/all/0/1\">Ameya D. Jagtap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yeonjong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "We propose a new type of neural networks, Kronecker neural networks (KNNs),\nthat form a general framework for neural networks with adaptive activation\nfunctions. KNNs employ the Kronecker product, which provides an efficient way\nof constructing a very wide network while keeping the number of parameters low.\nOur theoretical analysis reveals that under suitable conditions, KNNs induce a\nfaster decay of the loss than that by the feed-forward networks. This is also\nempirically verified through a set of computational examples. Furthermore,\nunder certain technical assumptions, we establish global convergence of\ngradient descent for KNNs. As a specific case, we propose the Rowdy activation\nfunction that is designed to get rid of any saturation region by injecting\nsinusoidal fluctuations, which include trainable parameters. The proposed Rowdy\nactivation function can be employed in any neural network architecture like\nfeed-forward neural networks, Recurrent neural networks, Convolutional neural\nnetworks etc. The effectiveness of KNNs with Rowdy activation is demonstrated\nthrough various computational experiments including function approximation\nusing feed-forward neural networks, solution inference of partial differential\nequations using the physics-informed neural networks, and standard deep\nlearning benchmark problems using convolutional and fully-connected neural\nnetworks.",
          "link": "http://arxiv.org/abs/2105.09513",
          "publishedOn": "2021-05-23T06:10:39.334Z",
          "wordCount": 632,
          "title": "Deep Kronecker neural networks: A general framework for neural networks with adaptive activation functions. (arXiv:2105.09513v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09536",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Fried_S/0/1/0/all/0/1\">Sela Fried</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wolfer_G/0/1/0/all/0/1\">Geoffrey Wolfer</a>",
          "description": "We formulate extendibility of the minimax one-trajectory length of several\nstatistical Markov chains inference problems and give sufficient conditions for\nboth the possibility and impossibility of such extensions. We follow up and\napply this framework to recently published results on learning and identity\ntesting of ergodic Markov chains. In particular, we show that for some of the\naforementioned results, we can omit the aperiodicity requirement by simulating\nan $\\alpha$-lazy version of the original process, and quantify the incurred\ncost of removing this assumption.",
          "link": "http://arxiv.org/abs/2105.09536",
          "publishedOn": "2021-05-23T06:10:39.325Z",
          "wordCount": 516,
          "title": "On the $\\alpha$-lazy version of Markov chains in estimation and testing problems. (arXiv:2105.09536v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Takamichi Toda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriwaki_D/0/1/0/all/0/1\">Daisuke Moriwaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1\">Kazuhiro Ota</a>",
          "description": "Large and acute economic shocks such as the 2007-2009 financial crisis and\nthe current COVID-19 infections rapidly change the economic environment. In\nsuch a situation, the importance of real-time economic analysis using\nalternative datais emerging. Alternative data such as search query and location\ndata are closer to real-time and richer than official statistics that are\ntypically released once a month in an aggregated form. We take advantage of\nspatio-temporal granularity of alternative data and propose a\nmixed-FrequencyAggregate Learning (MF-AGL)model that predicts economic\nindicators for the smaller areas in real-time. We apply the model for the\nreal-world problem; prediction of the number of job applicants which is closely\nrelated to the unemployment rates. We find that the proposed model predicts (i)\nthe regional heterogeneity of the labor market condition and (ii) the rapidly\nchanging economic status. The model can be applied to various tasks, especially\neconomic analysis",
          "link": "http://arxiv.org/abs/2105.09579",
          "publishedOn": "2021-05-23T06:10:39.318Z",
          "wordCount": 613,
          "title": "Aggregate Learning for Mixed Frequency Data. (arXiv:2105.09579v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09506",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Cai_S/0/1/0/all/0/1\">Shengze Cai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiping Mao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yin_M/0/1/0/all/0/1\">Minglang Yin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "Despite the significant progress over the last 50 years in simulating flow\nproblems using numerical discretization of the Navier-Stokes equations (NSE),\nwe still cannot incorporate seamlessly noisy data into existing algorithms,\nmesh-generation is complex, and we cannot tackle high-dimensional problems\ngoverned by parametrized NSE. Moreover, solving inverse flow problems is often\nprohibitively expensive and requires complex and expensive formulations and new\ncomputer codes. Here, we review flow physics-informed learning, integrating\nseamlessly data and mathematical models, and implementing them using\nphysics-informed neural networks (PINNs). We demonstrate the effectiveness of\nPINNs for inverse problems related to three-dimensional wake flows, supersonic\nflows, and biomedical flows.",
          "link": "http://arxiv.org/abs/2105.09506",
          "publishedOn": "2021-05-23T06:10:39.309Z",
          "wordCount": 535,
          "title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review. (arXiv:2105.09506v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berahmand_K/0/1/0/all/0/1\">Kamal Berahmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasiri_E/0/1/0/all/0/1\">Elahe Nasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forouzandeh_S/0/1/0/all/0/1\">Saman Forouzandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuefeng Li</a>",
          "description": "Predicting links in complex networks has been one of the essential topics\nwithin the realm of data mining and science discovery over the past few years.\nThis problem remains an attempt to identify future, deleted, and redundant\nlinks using the existing links in a graph. Local random walk is considered to\nbe one of the most well-known algorithms in the category of quasi-local\nmethods. It traverses the network using the traditional random walk with a\nlimited number of steps, randomly selecting one adjacent node in each step\namong the nodes which have equal importance. Then this method uses the\ntransition probability between node pairs to calculate the similarity between\nthem. However, in most datasets, this method is not able to perform accurately\nin scoring remarkably similar nodes. In the present article, an efficient\nmethod is proposed for improving local random walk by encouraging random walk\nto move, in every step, towards the node which has a stronger influence.\nTherefore, the next node is selected according to the influence of the source\nnode. To do so, using mutual information, the concept of the asymmetric mutual\ninfluence of nodes is presented. A comparison between the proposed method and\nother similarity-based methods (local, quasi-local, and global) has been\nperformed, and results have been reported for 11 real-world networks. It had a\nhigher prediction accuracy compared with other link prediction approaches.",
          "link": "http://arxiv.org/abs/2105.09494",
          "publishedOn": "2021-05-23T06:10:39.302Z",
          "wordCount": 671,
          "title": "A Preference Random Walk Algorithm for Link Prediction through Mutual Influence Nodes in Complex Networks. (arXiv:2105.09494v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose \\method, a\ntraining method to obtain a single unified multilingual translation model.\nmCOLT is empowered by two techniques: (i) a contrastive learning scheme to\nclose the gap among representations of different languages, and (ii) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mCOLT achieves\ncompetitive or even better performance than a strong pre-trained model mBART on\ntens of WMT benchmarks. For non-English directions, mCOLT achieves an\nimprovement of average 10+ BLEU compared with the multilingual baseline.",
          "link": "http://arxiv.org/abs/2105.09501",
          "publishedOn": "2021-05-23T06:10:39.262Z",
          "wordCount": 576,
          "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frostig_R/0/1/0/all/0/1\">Roy Frostig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Matthew J. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maclaurin_D/0/1/0/all/0/1\">Dougal Maclaurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paszke_A/0/1/0/all/0/1\">Adam Paszke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radul_A/0/1/0/all/0/1\">Alexey Radul</a>",
          "description": "We decompose reverse-mode automatic differentiation into (forward-mode)\nlinearization followed by transposition. Doing so isolates the essential\ndifference between forward- and reverse-mode AD, and simplifies their joint\nimplementation. In particular, once forward-mode AD rules are defined for every\nprimitive operation in a source language, only linear primitives require an\nadditional transposition rule in order to arrive at a complete reverse-mode AD\nimplementation. This is how reverse-mode AD is written in JAX and Dex.",
          "link": "http://arxiv.org/abs/2105.09469",
          "publishedOn": "2021-05-23T06:10:39.240Z",
          "wordCount": 507,
          "title": "Decomposing reverse-mode automatic differentiation. (arXiv:2105.09469v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haghighat_E/0/1/0/all/0/1\">Ehsan Haghighat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekar_A/0/1/0/all/0/1\">Ali Can Bekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madenci_E/0/1/0/all/0/1\">Erdogan Madenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juanes_R/0/1/0/all/0/1\">Ruben Juanes</a>",
          "description": "Deep learning has been the most popular machine learning method in the last\nfew years. In this chapter, we present the application of deep learning and\nphysics-informed neural networks concerning structural mechanics and vibration\nproblems. Demonstration problems involve de-noising data, solution to\ntime-dependent ordinary and partial differential equations, and characterizing\nthe system's response for a given data.",
          "link": "http://arxiv.org/abs/2105.09477",
          "publishedOn": "2021-05-23T06:10:39.180Z",
          "wordCount": 491,
          "title": "Deep learning for solution and inversion of structural mechanics and vibrations. (arXiv:2105.09477v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leblebici_A/0/1/0/all/0/1\">Asim Leblebici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gesoglu_O/0/1/0/all/0/1\">Omer Gesoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basbinar_Y/0/1/0/all/0/1\">Yasemin Basbinar</a>",
          "description": "Until the beginning of 2021, lung cancer is known to be the most common\ncancer in the world. The disease is common due to factors such as occupational\nexposure, smoking and environmental pollution. The early diagnosis and\ntreatment of the disease is of great importance as well as the prevention of\nthe causes that cause the disease. The study was planned to create a web\ninterface that works with machine learning algorithms to predict prognosis\nusing lung cancer clinical and gene expression in the GDC data portal.",
          "link": "http://arxiv.org/abs/2105.09471",
          "publishedOn": "2021-05-23T06:10:39.153Z",
          "wordCount": 515,
          "title": "AI-Decision Support System Interface Using Cancer Related Data for Lung Cancer Prognosis. (arXiv:2105.09471v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lazic_S/0/1/0/all/0/1\">Stanley E. Lazic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_D/0/1/0/all/0/1\">Dominic P. Williams</a>",
          "description": "Knowing the uncertainty in a prediction is critical when making expensive\ninvestment decisions and when patient safety is paramount, but machine learning\n(ML) models in drug discovery typically provide only a single best estimate and\nignore all sources of uncertainty. Predictions from these models may therefore\nbe over-confident, which can put patients at risk and waste resources when\ncompounds that are destined to fail are further developed. Probabilistic\npredictive models (PPMs) can incorporate uncertainty in both the data and\nmodel, and return a distribution of predicted values that represents the\nuncertainty in the prediction. PPMs not only let users know when predictions\nare uncertain, but the intuitive output from these models makes communicating\nrisk easier and decision making better. Many popular machine learning methods\nhave a PPM or Bayesian analogue, making PPMs easy to fit into current\nworkflows. We use toxicity prediction as a running example, but the same\nprinciples apply for all prediction models used in drug discovery. The\nconsequences of ignoring uncertainty and how PPMs account for uncertainty are\nalso described. We aim to make the discussion accessible to a broad\nnon-mathematical audience. Equations are provided to make ideas concrete for\nmathematical readers (but can be skipped without loss of understanding) and\ncode is available for computational researchers\n(https://github.com/stanlazic/ML_uncertainty_quantification).",
          "link": "http://arxiv.org/abs/2105.09474",
          "publishedOn": "2021-05-23T06:10:39.144Z",
          "wordCount": 649,
          "title": "Quantifying sources of uncertainty in drug discovery predictions with probabilistic models. (arXiv:2105.09474v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09467",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Yan_B/0/1/0/all/0/1\">Bicheng Yan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Harp_D/0/1/0/all/0/1\">Dylan Robert Harp</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_B/0/1/0/all/0/1\">Bailian Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pawar_R/0/1/0/all/0/1\">Rajesh Pawar</a>",
          "description": "In this work, an efficient physics-constrained deep learning model is\ndeveloped for solving multiphase flow in 3D heterogeneous porous media. The\nmodel fully leverages the spatial topology predictive capability of\nconvolutional neural networks, and is coupled with an efficient\ncontinuity-based smoother to predict flow responses that need spatial\ncontinuity. Furthermore, the transient regions are penalized to steer the\ntraining process such that the model can accurately capture flow in these\nregions. The model takes inputs including properties of porous media, fluid\nproperties and well controls, and predicts the temporal-spatial evolution of\nthe state variables (pressure and saturation). While maintaining the continuity\nof fluid flow, the 3D spatial domain is decomposed into 2D images for reducing\ntraining cost, and the decomposition results in an increased number of training\ndata samples and better training efficiency. Additionally, a surrogate model is\nseparately constructed as a postprocessor to calculate well flow rate based on\nthe predictions of state variables from the deep learning model. We use the\nexample of CO2 injection into saline aquifers, and apply the\nphysics-constrained deep learning model that is trained from physics-based\nsimulation data and emulates the physics process. The model performs prediction\nwith a speedup of ~1400 times compared to physics-based simulations, and the\naverage temporal errors of predicted pressure and saturation plumes are 0.27%\nand 0.099% respectively. Furthermore, water production rate is efficiently\npredicted by a surrogate model for well flow rate, with a mean error less than\n5%. Therefore, with its unique scheme to cope with the fidelity in fluid flow\nin porous media, the physics-constrained deep learning model can become an\nefficient predictive model for computationally demanding inverse problems or\nother coupled processes.",
          "link": "http://arxiv.org/abs/2105.09467",
          "publishedOn": "2021-05-23T06:10:39.136Z",
          "wordCount": 727,
          "title": "A Physics-Constrained Deep Learning Model for Simulating Multiphase Flow in 3D Heterogeneous Porous Media. (arXiv:2105.09467v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09468",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Tang_H/0/1/0/all/0/1\">Hewei Tang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fu_P/0/1/0/all/0/1\">Pengcheng Fu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sherman_C/0/1/0/all/0/1\">Christopher S. Sherman</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_J/0/1/0/all/0/1\">Jize Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ju_X/0/1/0/all/0/1\">Xin Ju</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hamon_F/0/1/0/all/0/1\">Fran&#xe7;ois Hamon</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Azzolina_N/0/1/0/all/0/1\">Nicholas A. Azzolina</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Burton_Kelly_M/0/1/0/all/0/1\">Matthew Burton-Kelly</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Morris_J/0/1/0/all/0/1\">Joseph P. Morris</a>",
          "description": "Fast assimilation of monitoring data to update forecasts of pressure buildup\nand carbon dioxide (CO2) plume migration under geologic uncertainties is a\nchallenging problem in geologic carbon storage. The high computational cost of\ndata assimilation with a high-dimensional parameter space impedes fast\ndecision-making for commercial-scale reservoir management. We propose to\nleverage physical understandings of porous medium flow behavior with deep\nlearning techniques to develop a fast history matching-reservoir response\nforecasting workflow. Applying an Ensemble Smoother Multiple Data Assimilation\nframework, the workflow updates geologic properties and predicts reservoir\nperformance with quantified uncertainty from pressure history and CO2 plumes\ninterpreted through seismic inversion. As the most computationally expensive\ncomponent in such a workflow is reservoir simulation, we developed surrogate\nmodels to predict dynamic pressure and CO2 plume extents under multi-well\ninjection. The surrogate models employ deep convolutional neural networks,\nspecifically, a wide residual network and a residual U-Net. The workflow is\nvalidated against a flat three-dimensional reservoir model representative of a\nclastic shelf depositional environment. Intelligent treatments are applied to\nbridge between quantities in a true-3D reservoir model and those in a\nsingle-layer reservoir model. The workflow can complete history matching and\nreservoir forecasting with uncertainty quantification in less than one hour on\na mainstream personal workstation.",
          "link": "http://arxiv.org/abs/2105.09468",
          "publishedOn": "2021-05-23T06:10:39.125Z",
          "wordCount": 658,
          "title": "A Deep Learning-Accelerated Data Assimilation and Forecasting Workflow for Commercial-Scale Geologic Carbon Storage. (arXiv:2105.09468v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Kababji_A/0/1/0/all/0/1\">Ayman Al-Kababji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amira_A/0/1/0/all/0/1\">Abbes Amira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensaali_F/0/1/0/all/0/1\">Faycal Bensaali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarouf_A/0/1/0/all/0/1\">Abdulah Jarouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shidqi_L/0/1/0/all/0/1\">Lisan Shidqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djelouat_H/0/1/0/all/0/1\">Hamza Djelouat</a>",
          "description": "Fall detection is a serious healthcare issue that needs to be solved. Falling\nwithout quick medical intervention would lower the chances of survival for the\nelderly, especially if living alone. Hence, the need is there for developing\nfall detection algorithms with high accuracy. This paper presents a novel\nIoT-based system for fall detection that includes a sensing device transmitting\ndata to a mobile application through a cloud-connected gateway device. Then,\nthe focus is shifted to the algorithmic aspect where multiple features are\nextracted from 3-axis accelerometer data taken from existing datasets. The\nresults emphasize on the significance of Continuous Wavelet Transform (CWT) as\nan influential feature for determining falls. CWT, Signal Energy (SE), Signal\nMagnitude Area (SMA), and Signal Vector Magnitude (SVM) features have shown\npromising classification results using K-Nearest Neighbors (KNN) and E-Nearest\nNeighbors (ENN). For all performance metrics (accuracy, recall, precision,\nspecificity, and F1 Score), the achieved results are higher than 95% for a\ndataset of small size, while more than 98.47% score is achieved in the\naforementioned criteria over the UniMiB-SHAR dataset by the same algorithms,\nwhere the classification time for a single test record is extremely efficient\nand is real-time",
          "link": "http://arxiv.org/abs/2105.09461",
          "publishedOn": "2021-05-23T06:10:39.102Z",
          "wordCount": 658,
          "title": "An IoT-Based Framework for Remote Fall Monitoring. (arXiv:2105.09461v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rundi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changxi Zheng</a>",
          "description": "Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation -- describing a shape as\na sequence of computer-aided design (CAD) operations. Unlike meshes and point\nclouds, CAD models encode the user creation process of 3D shapes, widely used\nin numerous industrial and engineering design tasks. However, the sequential\nand irregular structure of CAD operations poses significant challenges for\nexisting 3D generative models. Drawing an analogy between CAD operations and\nnatural language, we propose a CAD generative network based on the Transformer.\nWe demonstrate the performance of our model for both shape autoencoding and\nrandom shape generation. To train our network, we create a new CAD dataset\nconsisting of 179,133 models and their CAD construction sequences. We have made\nthis dataset publicly available to promote future research on this topic.",
          "link": "http://arxiv.org/abs/2105.09492",
          "publishedOn": "2021-05-23T06:10:39.091Z",
          "wordCount": 608,
          "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "Superpixels are higher-order perceptual groups of pixels in an image, often\ncarrying much more information than raw pixels. There is an inherent relational\nstructure to the relationship among different superpixels of an image. This\nrelational information can convey some form of domain information about the\nimage, e.g. relationship between superpixels representing two eyes in a cat\nimage. Our interest in this paper is to construct computer vision models,\nspecifically those based on Deep Neural Networks (DNNs) to incorporate these\nsuperpixels information. We propose a methodology to construct a hybrid model\nthat leverages (a) Convolutional Neural Network (CNN) to deal with spatial\ninformation in an image, and (b) Graph Neural Network (GNN) to deal with\nrelational superpixel information in the image. The proposed deep model is\nlearned using a generic hybrid loss function that we call a `hybrid' loss. We\nevaluate the predictive performance of our proposed hybrid vision model on four\npopular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.\nMoreover, we evaluate our method on three real-world classification tasks:\nCOVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint\nIdentification. The results demonstrate that the relational superpixel\ninformation provided via a GNN could improve the performance of standard\nCNN-based vision systems.",
          "link": "http://arxiv.org/abs/2105.09448",
          "publishedOn": "2021-05-23T06:10:39.076Z",
          "wordCount": 693,
          "title": "Superpixel-based Domain-Knowledge Infusion in Computer Vision. (arXiv:2105.09448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alegre_L/0/1/0/all/0/1\">Lucas N. Alegre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzan_A/0/1/0/all/0/1\">Ana L. C. Bazzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1\">Bruno C. da Silva</a>",
          "description": "Non-stationary environments are challenging for reinforcement learning\nalgorithms. If the state transition and/or reward functions change based on\nlatent factors, the agent is effectively tasked with optimizing a behavior that\nmaximizes performance over a possibly infinite random sequence of Markov\nDecision Processes (MDPs), each of which drawn from some unknown distribution.\nWe call each such MDP a context. Most related works make strong assumptions\nsuch as knowledge about the distribution over contexts, the existence of\npre-training phases, or a priori knowledge about the number, sequence, or\nboundaries between contexts. We introduce an algorithm that efficiently learns\npolicies in non-stationary environments. It analyzes a possibly infinite stream\nof data and computes, in real-time, high-confidence change-point detection\nstatistics that reflect whether novel, specialized policies need to be created\nand deployed to tackle novel contexts, or whether previously-optimized ones\nmight be reused. We show that (i) this algorithm minimizes the delay until\nunforeseen changes to a context are detected, thereby allowing for rapid\nresponses; and (ii) it bounds the rate of false alarm, which is important in\norder to minimize regret. Our method constructs a mixture model composed of a\n(possibly infinite) ensemble of probabilistic dynamics predictors that model\nthe different modes of the distribution over underlying latent MDPs. We\nevaluate our algorithm on high-dimensional continuous reinforcement learning\nproblems and show that it outperforms state-of-the-art (model-free and\nmodel-based) RL algorithms, as well as state-of-the-art meta-learning methods\nspecially designed to deal with non-stationarity.",
          "link": "http://arxiv.org/abs/2105.09452",
          "publishedOn": "2021-05-23T06:10:39.068Z",
          "wordCount": 713,
          "title": "Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection. (arXiv:2105.09452v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "The past decades have witnessed the prosperity of graph mining, with a\nmultitude of sophisticated models and algorithms designed for various mining\ntasks, such as ranking, classification, clustering and anomaly detection.\nGenerally speaking, the vast majority of the existing works aim to answer the\nfollowing question, that is, given a graph, what is the best way to mine it? In\nthis paper, we introduce the graph sanitation problem, to answer an orthogonal\nquestion. That is, given a mining task and an initial graph, what is the best\nway to improve the initially provided graph? By learning a better graph as part\nof the input of the mining model, it is expected to benefit graph mining in a\nvariety of settings, ranging from denoising, imputation to defense. We\nformulate the graph sanitation problem as a bilevel optimization problem, and\nfurther instantiate it by semi-supervised node classification, together with an\neffective solver named GaSoliNe. Extensive experimental results demonstrate\nthat the proposed method is (1) broadly applicable with respect to different\ngraph neural network models and flexible graph modification strategies, (2)\neffective in improving the node classification accuracy on both the original\nand contaminated graphs in various perturbation scenarios. In particular, it\nbrings up to 25% performance improvement over the existing robust graph neural\nnetwork methods.",
          "link": "http://arxiv.org/abs/2105.09384",
          "publishedOn": "2021-05-23T06:10:38.908Z",
          "wordCount": 631,
          "title": "Graph Sanitation with Application to Node Classification. (arXiv:2105.09384v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09365",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Uysal_E/0/1/0/all/0/1\">Enes Sadi Uysal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilici_M/0/1/0/all/0/1\">M.&#x15e;afak Bilici</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaza_B/0/1/0/all/0/1\">B. Selin Zaza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozgenc_M/0/1/0/all/0/1\">M. Yi&#x11f;it &#xd6;zgen&#xe7;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyar_O/0/1/0/all/0/1\">Onur Boyar</a>",
          "description": "Retinal Vessel Segmentation is important for diagnosis of various diseases.\nThe research on retinal vessel segmentation focuses mainly on improvement of\nthe segmentation model which is usually based on U-Net architecture. In our\nstudy we use the U-Net architecture and we rely on heavy data augmentation in\norder to achieve better performance. The success of the data augmentation\nrelies on successfully addressing the problem of input images. By analyzing\ninput images and performing the augmentation accordingly we show that the\nperformance of the U-Net model can be increased dramatically. Results are\nreported using the most widely used retina dataset, DRIVE.",
          "link": "http://arxiv.org/abs/2105.09365",
          "publishedOn": "2021-05-23T06:10:38.879Z",
          "wordCount": 564,
          "title": "Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation. (arXiv:2105.09365v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_E/0/1/0/all/0/1\">Evan Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guvenc_L/0/1/0/all/0/1\">Levent Guven&#xe7;</a>",
          "description": "As passenger vehicle technologies have advanced, so have their capabilities\nto avoid obstacles, especially with developments in tires, suspensions,\nsteering, as well as safety technologies like ABS, ESC, and more recently, ADAS\nsystems. However, environments around passenger vehicles have also become more\ncomplex, and dangerous. There have previously been studies that outline driver\ntendencies and performance capabilities when attempting to avoid obstacles\nwhile driving passenger vehicles. Now that autonomous vehicles are being\ndeveloped with obstacle avoidance capabilities, it is important to target\nperformance that meets or exceeds that of human drivers. This manuscript\nhighlights systems that are crucial for an emergency obstacle avoidance\nmaneuver (EOAM) and identifies the state-of-the-art for each of the related\nsystems, while considering the nuances of traveling at highway speeds. Some of\nthe primary EOAM-related systems/areas that are discussed in this review are:\ngeneral path planning methods, system hierarchies, decision-making, trajectory\ngeneration, and trajectory-tracking control methods. After concluding remarks,\nsuggestions for future work which could lead to an ideal EOAM development, are\ndiscussed.",
          "link": "http://arxiv.org/abs/2105.09446",
          "publishedOn": "2021-05-23T06:10:38.865Z",
          "wordCount": 619,
          "title": "A Review of Autonomous Road Vehicle Integrated Approaches to an Emergency Obstacle Avoidance Maneuver. (arXiv:2105.09446v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Ming-Chang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jia-Chun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gran_E/0/1/0/all/0/1\">Ernst Gunnar Gran</a>",
          "description": "Over the past decade, many approaches have been introduced for traffic speed\nprediction. However, providing fine-grained, accurate, time-efficient, and\nadaptive traffic speed prediction for a growing transportation network where\nthe size of the network keeps increasing and new traffic detectors are\nconstantly deployed has not been well studied. To address this issue, this\npaper presents DistTune based on Long Short-Term Memory (LSTM) and the\nNelder-Mead method. Whenever encountering an unprocessed detector, DistTune\ndecides if it should customize an LSTM model for this detector by comparing the\ndetector with other processed detectors in terms of the normalized traffic\nspeed patterns they have observed. If similarity is found, DistTune directly\nshares an existing LSTM model with this detector to achieve time-efficient\nprocessing. Otherwise, DistTune customizes an LSTM model for the detector to\nachieve fine-grained prediction. To make DistTune even more time-efficient,\nDistTune performs on a cluster of computing nodes in parallel. To achieve\nadaptive traffic speed prediction, DistTune also provides LSTM re-customization\nfor detectors that suffer from unsatisfactory prediction accuracy due to for\ninstance traffic speed pattern change. Extensive experiments based on traffic\ndata collected from freeway I5-N in California are conducted to evaluate the\nperformance of DistTune. The results demonstrate that DistTune provides\nfine-grained, accurate, time-efficient, and adaptive traffic speed prediction\nfor a growing transportation network.",
          "link": "http://arxiv.org/abs/2105.09421",
          "publishedOn": "2021-05-23T06:10:38.850Z",
          "wordCount": 658,
          "title": "DistTune: Distributed Fine-Grained Adaptive Traffic Speed Prediction for Growing Transportation Networks. (arXiv:2105.09421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pau-Chen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eykholt_K/0/1/0/all/0/1\">Kevin Eykholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhongshu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamjoom_H/0/1/0/all/0/1\">Hani Jamjoom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaram_K/0/1/0/all/0/1\">K. R. Jayaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdez_E/0/1/0/all/0/1\">Enriquillo Valdez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Ashish Verma</a>",
          "description": "Federated Learning (FL) enables collaborative training among mutually\ndistrusting parties. Model updates, rather than training data, are concentrated\nand fused in a central aggregation server. A key security challenge in FL is\nthat an untrustworthy or compromised aggregation process might lead to\nunforeseeable information leakage. This challenge is especially acute due to\nrecently demonstrated attacks that have reconstructed large fractions of\ntraining data from ostensibly \"sanitized\" model updates.\n\nIn this paper, we introduce TRUDA, a new cross-silo FL system, employing a\ntrustworthy and decentralized aggregation architecture to break down\ninformation concentration with regard to a single aggregator. Based on the\nunique computational properties of model-fusion algorithms, all exchanged model\nupdates in TRUDA are disassembled at the parameter-granularity and re-stitched\nto random partitions designated for multiple TEE-protected aggregators. Thus,\neach aggregator only has a fragmentary and shuffled view of model updates and\nis oblivious to the model architecture. Our new security mechanisms can\nfundamentally mitigate training reconstruction attacks, while still preserving\nthe final accuracy of trained models and keeping performance overheads low.",
          "link": "http://arxiv.org/abs/2105.09400",
          "publishedOn": "2021-05-23T06:10:38.835Z",
          "wordCount": 604,
          "title": "Separation of Powers in Federated Learning. (arXiv:2105.09400v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Javaheri_B/0/1/0/all/0/1\">Behzad Javaheri</a>",
          "description": "Herein, we have compared the performance of SVM and MLP in emotion\nrecognition using speech and song channels of the RAVDESS dataset. We have\nundertaken a journey to extract various audio features, identify optimal\nscaling strategy and hyperparameter for our models. To increase sample size, we\nhave performed audio data augmentation and addressed data imbalance using\nSMOTE. Our data indicate that optimised SVM outperforms MLP with an accuracy of\n82 compared to 75%. Following data augmentation, the performance of both\nalgorithms was identical at ~79%, however, overfitting was evident for the SVM.\nOur final exploration indicated that the performance of both SVM and MLP were\nsimilar in which both resulted in lower accuracy for the speech channel\ncompared to the song channel. Our findings suggest that both SVM and MLP are\npowerful classifiers for emotion recognition in a vocal-dependent manner.",
          "link": "http://arxiv.org/abs/2105.09406",
          "publishedOn": "2021-05-23T06:10:38.820Z",
          "wordCount": 576,
          "title": "Speech & Song Emotion Recognition Using Multilayer Perceptron and Standard Vector Machine. (arXiv:2105.09406v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahlou_C/0/1/0/all/0/1\">Chuhong Lahlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crayton_A/0/1/0/all/0/1\">Ancil Crayton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1\">Caroline Trier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willett_E/0/1/0/all/0/1\">Evan Willett</a>",
          "description": "In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an\nArtificial Intelligence (AI) Health Outcomes Challenge seeking solutions to\npredict risk in value-based care for incorporation into CMS Innovation Center\npayment and service delivery models. Recently, modern language models have\nplayed key roles in a number of health related tasks. This paper presents, to\nthe best of our knowledge, the first application of these models to patient\nreadmission prediction. To facilitate this, we create a dataset of 1.2 million\nmedical history samples derived from the Limited Dataset (LDS) issued by CMS.\nMoreover, we propose a comprehensive modeling solution centered on a deep\nlearning framework for this data. To demonstrate the framework, we train an\nattention-based Transformer to learn Medicare semantics in support of\nperforming downstream prediction tasks thereby achieving 0.91 AUC and 0.91\nrecall on readmission classification. We also introduce a novel data\npre-processing pipeline and discuss pertinent deployment considerations\nsurrounding model explainability and bias.",
          "link": "http://arxiv.org/abs/2105.09428",
          "publishedOn": "2021-05-23T06:10:38.807Z",
          "wordCount": 592,
          "title": "Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder. (arXiv:2105.09428v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1\">Aditya Parulekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1\">Advait Parulekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "We consider the problem of finding an approximate solution to $\\ell_1$\nregression while only observing a small number of labels. Given an $n \\times d$\nunlabeled data matrix $X$, we must choose a small set of $m \\ll n$ rows to\nobserve the labels of, then output an estimate $\\widehat{\\beta}$ whose error on\nthe original problem is within a $1 + \\varepsilon$ factor of optimal. We show\nthat sampling from $X$ according to its Lewis weights and outputting the\nempirical minimizer succeeds with probability $1-\\delta$ for $m >\nO(\\frac{1}{\\varepsilon^2} d \\log \\frac{d}{\\varepsilon \\delta})$. This is\nanalogous to the performance of sampling according to leverage scores for\n$\\ell_2$ regression, but with exponentially better dependence on $\\delta$. We\nalso give a corresponding lower bound of $\\Omega(\\frac{d}{\\varepsilon^2} + (d +\n\\frac{1}{\\varepsilon^2}) \\log\\frac{1}{\\delta})$.",
          "link": "http://arxiv.org/abs/2105.09433",
          "publishedOn": "2021-05-23T06:10:38.786Z",
          "wordCount": 552,
          "title": "L1 Regression with Lewis Weights Subsampling. (arXiv:2105.09433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yada Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>",
          "description": "With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and characterized with multiple labels,\nthus exhibiting the co-existence of multiple types of heterogeneity. Although\nstate-of-the-art techniques are good at modeling the complex heterogeneity with\nsufficient label information, such label information can be quite expensive to\nobtain in real applications, leading to sub-optimal performance using these\ntechniques. Inspired by the capability of contrastive learning to utilize rich\nunlabeled data for improving performance, in this paper, we propose a unified\nheterogeneous learning framework, which combines both weighted unsupervised\ncontrastive loss and weighted supervised contrastive loss to model multiple\ntypes of heterogeneity. We also provide theoretical analyses showing that the\nproposed weighted supervised contrastive loss is the lower bound of the mutual\ninformation of two samples from the same class and the weighted unsupervised\ncontrastive loss is the lower bound of the mutual information between the\nhidden representation of two views of the same sample. Experimental results on\nreal-world data sets demonstrate the effectiveness and the efficiency of the\nproposed method modeling multiple types of heterogeneity.",
          "link": "http://arxiv.org/abs/2105.09401",
          "publishedOn": "2021-05-23T06:10:38.751Z",
          "wordCount": 613,
          "title": "Heterogeneous Contrastive Learning. (arXiv:2105.09401v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1\">Daniel Glasner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1\">Srikumar Ramalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papineni_K/0/1/0/all/0/1\">Kishore Papineni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>",
          "description": "It is generally believed that robust training of extremely large networks is\ncritical to their success in real-world applications. However, when taken to\nthe extreme, methods that promote robustness can hurt the model's sensitivity\nto rare or underrepresented patterns. In this paper, we discuss this trade-off\nbetween sensitivity and robustness to natural (non-adversarial) perturbations\nby introducing two notions: contextual feature utility and contextual feature\nsensitivity. We propose Feature Contrastive Learning (FCL) that encourages a\nmodel to be more sensitive to the features that have higher contextual utility.\nEmpirical results demonstrate that models trained with FCL achieve a better\nbalance of robustness and sensitivity, leading to improved generalization in\nthe presence of noise on both vision and NLP datasets.",
          "link": "http://arxiv.org/abs/2105.09394",
          "publishedOn": "2021-05-23T06:10:38.598Z",
          "wordCount": 552,
          "title": "Balancing Robustness and Sensitivity using Feature Contrastive Learning. (arXiv:2105.09394v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Avraham Adler</a>",
          "description": "This paper reviews a wide selection of machine learning models built to\npredict both the presence of diabetes and the presence of undiagnosed diabetes\nusing eight years of National Health and Nutrition Examination Survey (NHANES)\ndata. Models are tuned and compared via their Brier Scores. The most relevant\nvariables of the best performing models are then compared. A Support Vector\nMachine with a linear kernel performed best for predicting diabetes, returning\na Brier score of 0.0654 and an AUROC of 0.9235 on the test set. An elastic net\nregression performed best for predicting undiagnosed diabetes with a Brier\nscore of 0.0294 and an AUROC of 0.9439 on the test set. Similar features appear\nprominently in the models for both sets of models. Blood osmolality, family\nhistory, the prevalance of various compounds, and hypertension are key\nindicators for all diabetes risk. For undiagnosed diabetes in particular, there\nare ethnicity or genetic components which arise as strong correlates as well.",
          "link": "http://arxiv.org/abs/2105.09379",
          "publishedOn": "2021-05-23T06:10:38.585Z",
          "wordCount": 588,
          "title": "Using Machine Learning Techniques to Identify Key Risk Factors for Diabetes and Undiagnosed Diabetes. (arXiv:2105.09379v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuesu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>",
          "description": "While imitation learning for vision based autonomous mobile robot navigation\nhas recently received a great deal of attention in the research community,\nexisting approaches typically require state action demonstrations that were\ngathered using the deployment platform. However, what if one cannot easily\noutfit their platform to record these demonstration signals or worse yet the\ndemonstrator does not have access to the platform at all? Is imitation learning\nfor vision based autonomous navigation even possible in such scenarios? In this\nwork, we hypothesize that the answer is yes and that recent ideas from the\nImitation from Observation (IfO) literature can be brought to bear such that a\nrobot can learn to navigate using only ego centric video collected by a\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\nintroduce a new algorithm, Visual Observation only Imitation Learning for\nAutonomous navigation (VOILA), that can successfully learn navigation policies\nfrom a single video demonstration collected from a physically different agent.\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\nnot only successfully imitates the expert, but that it also learns navigation\npolicies that can generalize to novel environments. Further, we demonstrate the\neffectiveness of VOILA in a real world setting by showing that it allows a\nwheeled Jackal robot to successfully imitate a human walking in an environment\nusing a video recorded using a mobile phone camera.",
          "link": "http://arxiv.org/abs/2105.09371",
          "publishedOn": "2021-05-23T06:10:38.567Z",
          "wordCount": 671,
          "title": "VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.",
          "link": "http://arxiv.org/abs/2105.09356",
          "publishedOn": "2021-05-23T06:10:38.549Z",
          "wordCount": 621,
          "title": "Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wainakh_A/0/1/0/all/0/1\">Aidmar Wainakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1\">Fabrizio Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mussig_T/0/1/0/all/0/1\">Till M&#xfc;&#xdf;ig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keim_J/0/1/0/all/0/1\">Jens Keim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordero_C/0/1/0/all/0/1\">Carlos Garcia Cordero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_E/0/1/0/all/0/1\">Ephraim Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grube_T/0/1/0/all/0/1\">Tim Grube</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhlhauser_M/0/1/0/all/0/1\">Max M&#xfc;hlh&#xe4;user</a>",
          "description": "Federated learning enables multiple users to build a joint model by sharing\ntheir model updates (gradients), while their raw data remains local on their\ndevices. In contrast to the common belief that this provides privacy benefits,\nwe here add to the very recent results on privacy risks when sharing gradients.\nSpecifically, we propose Label Leakage from Gradients (LLG), a novel attack to\nextract the labels of the users' training data from their shared gradients. The\nattack exploits the direction and magnitude of gradients to determine the\npresence or absence of any label. LLG is simple yet effective, capable of\nleaking potential sensitive information represented by labels, and scales well\nto arbitrary batch sizes and multiple classes. We empirically and\nmathematically demonstrate the validity of our attack under different settings.\nMoreover, empirical results show that LLG successfully extracts labels with\nhigh accuracy at the early stages of model training. We also discuss different\ndefense mechanisms against such leakage. Our findings suggest that gradient\ncompression is a practical technique to prevent our attack.",
          "link": "http://arxiv.org/abs/2105.09369",
          "publishedOn": "2021-05-23T06:10:38.516Z",
          "wordCount": 611,
          "title": "User Label Leakage from Gradients in Federated Learning. (arXiv:2105.09369v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin B. Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrato_G/0/1/0/all/0/1\">Guillermo Serrato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>",
          "description": "The joint task of bug localization and program repair is an integral part of\nthe software development process. In this work we present DeepDebug, an\napproach to automated debugging using large, pretrained transformers. We begin\nby training a bug-creation model on reversed commit data for the purpose of\ngenerating synthetic bugs. We apply these synthetic bugs toward two ends.\nFirst, we directly train a backtranslation model on all functions from 200K\nrepositories. Next, we focus on 10K repositories for which we can execute\ntests, and create buggy versions of all functions in those repositories that\nare covered by passing tests. This provides us with rich debugging information\nsuch as stack traces and print statements, which we use to finetune our model\nwhich was pretrained on raw source code. Finally, we strengthen all our models\nby expanding the context window beyond the buggy function itself, and adding a\nskeleton consisting of that function's parent class, imports, signatures,\ndocstrings, and method bodies, in order of priority. On the QuixBugs benchmark,\nwe increase the total number of fixes found by over 50%, while also decreasing\nthe false positive rate from 35% to 5% and decreasing the timeout from six\nhours to one minute. On our own benchmark of executable tests, our model fixes\n68% of all bugs on its first attempt without using traces, and after adding\ntraces it fixes 75% on first attempt. We will open-source our framework and\nvalidation set for evaluating on executable tests.",
          "link": "http://arxiv.org/abs/2105.09352",
          "publishedOn": "2021-05-23T06:10:38.347Z",
          "wordCount": 678,
          "title": "DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and Code Skeletons. (arXiv:2105.09352v1 [cs.SE])"
        }
      ]
    }
  ],
  "cliVersion": "1.8.1"
}