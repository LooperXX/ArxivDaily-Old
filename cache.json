{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.08759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sundriyal_M/0/1/0/all/0/1\">Megha Sundriyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Parantak Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Shubhashis Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "The formulation of a claim rests at the core of argument mining. To demarcate\nbetween a claim and a non-claim is arduous for both humans and machines, owing\nto latent linguistic variance between the two and the inadequacy of extensive\ndefinition-based formalization. Furthermore, the increase in the usage of\nonline social media has resulted in an explosion of unsolicited information on\nthe web presented as informal text. To account for the aforementioned, in this\npaper, we proposed DESYR. It is a framework that intends on annulling the said\nissues for informal web-based text by leveraging a combination of hierarchical\nrepresentation learning (dependency-inspired Poincare embedding),\ndefinition-based alignment, and feature projection. We do away with fine-tuning\ncomputer-heavy language models in favor of fabricating a more domain-centric\nbut lighter approach. Experimental results indicate that DESYR builds upon the\nstate-of-the-art system across four benchmark claim datasets, most of which\nwere constructed with informal texts. We see an increase of 3 claim-F1 points\non the LESA-Twitter dataset, an increase of 1 claim-F1 point and 9 macro-F1\npoints on the Online Comments(OC) dataset, an increase of 24 claim-F1 points\nand 17 macro-F1 points on the Web Discourse(WD) dataset, and an increase of 8\nclaim-F1 points and 5 macro-F1 points on the Micro Texts(MT) dataset. We also\nperform an extensive analysis of the results. We make a 100-D pre-trained\nversion of our Poincare-variant along with the source code.",
          "link": "http://arxiv.org/abs/2108.08759",
          "publishedOn": "2021-08-20T01:53:50.948Z",
          "wordCount": 678,
          "title": "DESYR: Definition and Syntactic Representation Based Claim Detection on the Web. (arXiv:2108.08759v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pisoni_R/0/1/0/all/0/1\">Raphael Pisoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmi_S/0/1/0/all/0/1\">Sri Lakshmi</a>",
          "description": "CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal\nmodel that jointly learns representations of images and texts. The model is\ntrained on a massive amount of English data and shows impressive performance on\nzero-shot classification tasks. Training the same model on a different language\nis not trivial, since data in other languages might be not enough and the model\nneeds high-quality translations of the texts to guarantee a good performance.\nIn this paper, we present the first CLIP model for the Italian Language\n(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show\nthat CLIP-Italian outperforms the multilingual CLIP model on the tasks of image\nretrieval and zero-shot classification.",
          "link": "http://arxiv.org/abs/2108.08688",
          "publishedOn": "2021-08-20T01:53:50.916Z",
          "wordCount": 552,
          "title": "Contrastive Language-Image Pre-training for the Italian Language. (arXiv:2108.08688v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.03744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "The recently proposed SNLI-VE corpus for recognising visual-textual\nentailment is a large, real-world dataset for fine-grained multimodal\nreasoning. However, the automatic way in which SNLI-VE has been assembled (via\ncombining parts of two related datasets) gives rise to a large number of errors\nin the labels of this corpus. In this paper, we first present a data collection\neffort to correct the class with the highest error rate in SNLI-VE. Secondly,\nwe re-evaluate an existing model on the corrected corpus, which we call\nSNLI-VE-2.0, and provide a quantitative comparison with its performance on the\nnon-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends\nhuman-written natural language explanations to SNLI-VE-2.0. Finally, we train\nmodels that learn from these explanations at training time, and output such\nexplanations at testing time.",
          "link": "http://arxiv.org/abs/2004.03744",
          "publishedOn": "2021-08-20T01:53:50.788Z",
          "wordCount": 621,
          "title": "e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations. (arXiv:2004.03744v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Encheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongru Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yongping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Meiling Hu</a>",
          "description": "Medical Dialogue Generation (MDG) is intended to build a medical dialogue\nsystem for intelligent consultation, which can communicate with patients in\nreal-time, thereby improving the efficiency of clinical diagnosis with broad\napplication prospects. This paper presents our proposed framework for the\nChinese MDG organized by the 2021 China conference on knowledge graph and\nsemantic computing (CCKS) competition, which requires generating\ncontext-consistent and medically meaningful responses conditioned on the\ndialogue history. In our framework, we propose a pipeline system composed of\nentity prediction and entity-aware dialogue generation, by adding predicted\nentities to the dialogue model with a fusion mechanism, thereby utilizing\ninformation from different sources. At the decoding stage, we propose a new\ndecoding mechanism named Entity-revised Diverse Beam Search (EDBS) to improve\nentity correctness and promote the length and quality of the final response.\nThe proposed method wins both the CCKS and the International Conference on\nLearning Representations (ICLR) 2021 Workshop Machine Learning for Preventing\nand Combating Pandemics (MLPCP) Track 1 Entity-aware MED competitions, which\ndemonstrate the practicality and effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.01266",
          "publishedOn": "2021-08-20T01:53:50.756Z",
          "wordCount": 643,
          "title": "More but Correct: Generating Diversified and Entity-revised Medical Response. (arXiv:2108.01266v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>",
          "description": "In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% baseline is based on BERT training on the corrected dataset, which is\nhard to surpass.",
          "link": "http://arxiv.org/abs/2102.00225",
          "publishedOn": "2021-08-20T01:53:50.663Z",
          "wordCount": 581,
          "title": "Learning From How Human Correct. (arXiv:2102.00225v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1\">Rohit Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara Lee Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "Most existing video-and-language (VidL) research focuses on a single dataset,\nor multiple datasets of a single task. In reality, a truly useful VidL system\nis expected to be easily generalizable to diverse tasks, domains, and datasets.\nTo facilitate the evaluation of such systems, we introduce Video-And-Language\nUnderstanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets\nover 3 popular tasks: (i) text-to-video retrieval; (ii) video question\nanswering; and (iii) video captioning. VALUE benchmark aims to cover a broad\nrange of video genres, video lengths, data volumes, and task difficulty levels.\nRather than focusing on single-channel videos with visual information only,\nVALUE promotes models that leverage information from both video frames and\ntheir associated subtitles, as well as models that share knowledge across\nmultiple tasks. We evaluate various baseline methods with and without\nlarge-scale VidL pre-training, and systematically investigate the impact of\nvideo input channels, fusion methods, and different video representations. We\nalso study the transferability between tasks, and conduct multi-task learning\nunder different settings. The significant gap between our best model and human\nperformance calls for future study for advanced VidL models. VALUE is available\nat https://value-benchmark.github.io/.",
          "link": "http://arxiv.org/abs/2106.04632",
          "publishedOn": "2021-08-20T01:53:50.648Z",
          "wordCount": 703,
          "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique is to apply named entity disambiguation (NED)\nsystems to the question, and retrieve KB facts for the disambiguated entities.\nThis work presents ECQA, an efficient method that prunes irrelevant parts of\nthe search space using KB-aware signals. ECQA is based on top-k query\nprocessing over score-ordered lists of KB items that combine signals about\nlexical matching, relevance to the question, coherence among candidate items,\nand connectivity in the KB graph. Experiments with two recent QA benchmarks\ndemonstrate the superiority of ECQA over state-of-the-art baselines with\nrespect to answer presence, size of the search space, and runtimes.",
          "link": "http://arxiv.org/abs/2108.08597",
          "publishedOn": "2021-08-20T01:53:50.622Z",
          "wordCount": 590,
          "title": "Efficient Contextualization using Top-k Operators for Question Answering over Knowledge Graphs. (arXiv:2108.08597v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-08-20T01:53:50.615Z",
          "wordCount": 751,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but barely utilize semantic\ndata and knowledge. This paper presents the first QA system that can seamlessly\noperate over RDF datasets and text corpora, or both together, in a unified\nframework. Our method, called UNIQORN, builds a context graph on the fly, by\nretrieving question-relevant triples from the RDF data and/or the text corpus,\nwhere the latter case is handled by automatic information extraction. The\nresulting graph is typically rich but highly noisy. UNIQORN copes with this\ninput by advanced graph algorithms for Group Steiner Trees, that identify the\nbest answer candidates in the context graph. Experimental results on several\nbenchmarks of complex questions with multiple entities and relations, show that\nUNIQORN, an unsupervised method with only five parameters, produces results\ncomparable to the state-of-the-art on KGs, text corpora, and heterogeneous\nsources. The graph-based methodology provides user-interpretable evidence for\nthe complete answering process.",
          "link": "http://arxiv.org/abs/2108.08614",
          "publishedOn": "2021-08-20T01:53:50.459Z",
          "wordCount": 654,
          "title": "UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongyin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingwu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Beihong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>",
          "description": "Recently, the retrieval models based on dense representations have been\ngradually applied in the first stage of the document retrieval tasks, showing\nbetter performance than traditional sparse vector space models. To obtain high\nefficiency, the basic structure of these models is Bi-encoder in most cases.\nHowever, this simple structure may cause serious information loss during the\nencoding of documents since the queries are agnostic. To address this problem,\nwe design a method to mimic the queries on each of the documents by an\niterative clustering process and represent the documents by multiple pseudo\nqueries (i.e., the cluster centroids). To boost the retrieval process using\napproximate nearest neighbor search library, we also optimize the matching\nfunction with a two-step score calculation procedure. Experimental results on\nseveral popular ranking and QA datasets show that our model can achieve\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.03599",
          "publishedOn": "2021-08-20T01:53:50.338Z",
          "wordCount": 619,
          "title": "Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval. (arXiv:2105.03599v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1\">Michal Sej&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1\">V&#xe1;clav Moravec</a>",
          "description": "This paper describes a novel dataset consisting of sentences with semantic\nsimilarity annotations. The data originate from the journalistic domain in the\nCzech language. We describe the process of collecting and annotating the data\nin detail. The dataset contains 138,556 human annotations divided into train\nand test sets. In total, 485 journalism students participated in the creation\nprocess. To increase the reliability of the test set, we compute the annotation\nas an average of 9 individual annotations. We evaluate the quality of the\ndataset by measuring inter and intra annotation annotators' agreements. Beside\nagreement numbers, we provide detailed statistics of the collected dataset. We\nconclude our paper with a baseline experiment of building a system for\npredicting the semantic similarity of sentences. Due to the massive number of\ntraining annotations (116 956), the model can perform significantly better than\nan average annotator (0,92 versus 0,86 of Person's correlation coefficients).",
          "link": "http://arxiv.org/abs/2108.08708",
          "publishedOn": "2021-08-20T01:53:50.331Z",
          "wordCount": 594,
          "title": "Czech News Dataset for Semanic Textual Similarity. (arXiv:2108.08708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingchao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Heng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liaosa Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weiqiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Existing system dealing with online complaint provides a final decision\nwithout explanations. We propose to analyse the complaint text of internet\nfraud in a fine-grained manner. Considering the complaint text includes\nmultiple clauses with various functions, we propose to identify the role of\neach clause and classify them into different types of fraud element. We\nconstruct a large labeled dataset originated from a real finance service\nplatform. We build an element identification model on top of BERT and propose\nadditional two modules to utilize the context of complaint text for better\nelement label classification, namely, global context encoder and label refiner.\nExperimental results show the effectiveness of our model.",
          "link": "http://arxiv.org/abs/2108.08676",
          "publishedOn": "2021-08-20T01:53:50.324Z",
          "wordCount": 569,
          "title": "Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>",
          "description": "Spoken Language Understanding (SLU) is one essential step in building a\ndialogue system. Due to the expensive cost of obtaining the labeled data, SLU\nsuffers from the data scarcity problem. Therefore, in this paper, we focus on\ndata augmentation for slot filling task in SLU. To achieve that, we aim at\ngenerating more diverse data based on existing data. Specifically, we try to\nexploit the latent language knowledge from pretrained language models by\nfinetuning them. We propose two strategies for finetuning process: value-based\nand context-based augmentation. Experimental results on two public SLU datasets\nhave shown that compared with existing data augmentation methods, our proposed\nmethod can generate more diverse sentences and significantly improve the\nperformance on SLU.",
          "link": "http://arxiv.org/abs/2108.08451",
          "publishedOn": "2021-08-20T01:53:50.243Z",
          "wordCount": 563,
          "title": "Augmenting Slot Values and Contexts for Spoken Language Understanding with Pretrained Models. (arXiv:2108.08451v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "Although automated metrics are commonly used to evaluate NLG systems, they\noften correlate poorly with human judgements. Newer metrics such as BERTScore\nhave addressed many weaknesses in prior metrics such as BLEU and ROUGE, which\nrely on n-gram matching. These newer methods, however, are still limited in\nthat they do not consider the generation context, so they cannot properly\nreward generated text that is correct but deviates from the given reference.\n\nIn this paper, we propose Language Model Augmented Relevance Score (MARS), a\nnew context-aware metric for NLG evaluation. MARS leverages off-the-shelf\nlanguage models, guided by reinforcement learning, to create augmented\nreferences that consider both the generation context and available human\nreferences, which are then used as additional references to score generated\ntext. Compared with seven existing metrics in three common NLG tasks, MARS not\nonly achieves higher correlation with human reference judgements, but also\ndifferentiates well-formed candidates from adversarial samples to a larger\ndegree.",
          "link": "http://arxiv.org/abs/2108.08485",
          "publishedOn": "2021-08-20T01:53:50.215Z",
          "wordCount": 589,
          "title": "Language Model Augmented Relevance Score. (arXiv:2108.08485v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangzhe Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jialiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_Z/0/1/0/all/0/1\">Ziquan Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>",
          "description": "Current storytelling systems focus more ongenerating stories with coherent\nplots regard-less of the narration style, which is impor-tant for controllable\ntext generation. There-fore, we propose a new task, stylized story gen-eration,\nnamely generating stories with speci-fied style given a leading context. To\ntacklethe problem, we propose a novel generationmodel that first plans the\nstylized keywordsand then generates the whole story with theguidance of the\nkeywords. Besides, we pro-pose two automatic metrics to evaluate theconsistency\nbetween the generated story andthe specified style. Experiments\ndemonstratesthat our model can controllably generateemo-tion-driven\norevent-driven stories based onthe ROCStories dataset (Mostafazadeh et\nal.,2016). Our study presents insights for stylizedstory generation in further\nresearch.",
          "link": "http://arxiv.org/abs/2105.08625",
          "publishedOn": "2021-08-20T01:53:50.200Z",
          "wordCount": 588,
          "title": "Stylized Story Generation with Style-Guided Planning. (arXiv:2105.08625v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Megan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1\">Bryan Wilson</a>",
          "description": "Legal interpretation is a linguistic venture. In judicial opinions, for\nexample, courts are often asked to interpret the text of statutes and\nlegislation. As time has shown, this is not always as easy as it sounds.\nMatters can hinge on vague or inconsistent language and, under the surface,\nhuman biases can impact the decision-making of judges. This raises an important\nquestion: what if there was a method of extracting the meaning of statutes\nconsistently? That is, what if it were possible to use machines to encode\nlegislation in a mathematically precise form that would permit clearer\nresponses to legal questions? This article attempts to unpack the notion of\nmachine-readability, providing an overview of both its historical and recent\ndevelopments. The paper will reflect on logic syntax and symbolic language to\nassess the capacity and limits of representing legal knowledge. In doing so,\nthe paper seeks to move beyond existing literature to discuss the implications\nof various approaches to machine-readable legislation. Importantly, this study\nhopes to highlight the challenges encountered in this burgeoning ecosystem of\nmachine-readable legislation against existing human-readable counterparts.",
          "link": "http://arxiv.org/abs/2108.08678",
          "publishedOn": "2021-08-20T01:53:50.192Z",
          "wordCount": 604,
          "title": "The Legislative Recipe: Syntax for Machine-Readable Legislation. (arXiv:2108.08678v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>",
          "description": "We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual\nretrieval in eleven typologically diverse languages, designed to evaluate\nranking with learned dense representations. The goal of this resource is to\nspur research in dense retrieval techniques in non-English languages, motivated\nby recent observations that existing techniques for representation learning\nperform poorly when applied to out-of-distribution data. As a starting point,\nwe provide zero-shot baselines for this new dataset based on a multi-lingual\nadaptation of DPR that we call \"mDPR\". Experiments show that although the\neffectiveness of mDPR is much lower than BM25, dense representations\nnevertheless appear to provide valuable relevance signals, improving BM25\nresults in sparse-dense hybrids. In addition to analyses of our results, we\nalso discuss future challenges and present a research agenda in multi-lingual\ndense retrieval. Mr. TyDi can be downloaded at\nhttps://github.com/castorini/mr.tydi.",
          "link": "http://arxiv.org/abs/2108.08787",
          "publishedOn": "2021-08-20T01:53:50.169Z",
          "wordCount": 574,
          "title": "Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval. (arXiv:2108.08787v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puranik_K/0/1/0/all/0/1\">Karthik Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durairaj_T/0/1/0/all/0/1\">Thenmozi Durairaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Anbukkarasi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1\">Kingston Pal Thamburaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>",
          "description": "This paper reports the Machine Translation (MT) systems submitted by the\nIIITT team for the English->Marathi and English->Irish language pairs LoResMT\n2021 shared task. The task focuses on getting exceptional translations for\nrather low-resourced languages like Irish and Marathi. We fine-tune IndicTrans,\na pretrained multilingual NMT model for English->Marathi, using external\nparallel corpus as input for additional training. We have used a pretrained\nHelsinki-NLP Opus MT English->Irish model for the latter language pair. Our\napproaches yield relatively promising results on the BLEU metrics. Under the\nteam name IIITT, our systems ranked 1, 1, and 2 in English->Marathi,\nIrish->English, and English->Irish, respectively.",
          "link": "http://arxiv.org/abs/2108.08556",
          "publishedOn": "2021-08-20T01:53:50.131Z",
          "wordCount": 549,
          "title": "Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021. (arXiv:2108.08556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weicheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "This paper studies the relative importance of attention heads in\nTransformer-based models to aid their interpretability in cross-lingual and\nmulti-lingual tasks. Prior research has found that only a few attention heads\nare important in each mono-lingual Natural Language Processing (NLP) task and\npruning the remaining heads leads to comparable or improved performance of the\nmodel. However, the impact of pruning attention heads is not yet clear in\ncross-lingual and multi-lingual tasks. Through extensive experiments, we show\nthat (1) pruning a number of attention heads in a multi-lingual\nTransformer-based model has, in general, positive effects on its performance in\ncross-lingual and multi-lingual tasks and (2) the attention heads to be pruned\ncan be ranked using gradients and identified with a few trial experiments. Our\nexperiments focus on sequence labeling tasks, with potential applicability on\nother cross-lingual and multi-lingual tasks. For comprehensiveness, we examine\ntwo pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and\nXLM-R, on three tasks across 9 languages each. We also discuss the validity of\nour findings and their extensibility to truly resource-scarce languages and\nother task settings.",
          "link": "http://arxiv.org/abs/2108.08375",
          "publishedOn": "2021-08-20T01:53:50.110Z",
          "wordCount": 626,
          "title": "Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks. (arXiv:2108.08375v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dolin_P/0/1/0/all/0/1\">Pavel Dolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dHauthuille_L/0/1/0/all/0/1\">Luc d&#x27;Hauthuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vattani_A/0/1/0/all/0/1\">Andrea Vattani</a>",
          "description": "Twitch chats pose a unique problem in natural language understanding due to a\nlarge presence of neologisms, specifically emotes. There are a total of 8.06\nmillion emotes, over 400k of which were used in the week studied. There is\nvirtually no information on the meaning or sentiment of emotes, and with a\nconstant influx of new emotes and drift in their frequencies, it becomes\nimpossible to maintain an updated manually-labeled dataset. Our paper makes a\ntwo fold contribution. First we establish a new baseline for sentiment analysis\non Twitch data, outperforming the previous supervised benchmark by 7.9% points.\nSecondly, we introduce a simple but powerful unsupervised framework based on\nword embeddings and k-NN to enrich existing models with out-of-vocabulary\nknowledge. This framework allows us to auto-generate a pseudo-dictionary of\nemotes and we show that we can nearly match the supervised benchmark above even\nwhen injecting such emote knowledge into sentiment classifiers trained on\nextraneous datasets such as movie reviews or Twitter.",
          "link": "http://arxiv.org/abs/2108.08411",
          "publishedOn": "2021-08-20T01:53:50.069Z",
          "wordCount": 591,
          "title": "FeelsGoodMan: Inferring Semantics of Twitch Neologisms. (arXiv:2108.08411v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zexian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohui Hu</a>",
          "description": "Conditional masked language models (CMLM) have shown impressive progress in\nnon-autoregressive machine translation (NAT). They learn the conditional\ntranslation model by predicting the random masked subset in the target\nsentence. Based on the CMLM framework, we introduce Multi-view Subset\nRegularization (MvSR), a novel regularization method to improve the performance\nof the NAT model. Specifically, MvSR consists of two parts: (1) \\textit{shared\nmask consistency}: we forward the same target with different mask strategies,\nand encourage the predictions of shared mask positions to be consistent with\neach other. (2) \\textit{model consistency}, we maintain an exponential moving\naverage of the model weights, and enforce the predictions to be consistent\nbetween the average model and the online model. Without changing the CMLM-based\narchitecture, our approach achieves remarkable performance on three public\nbenchmarks with 0.36-1.14 BLEU gains over previous NAT models. Moreover,\ncompared with the stronger Transformer baseline, we reduce the gap to 0.01-0.44\nBLEU scores on small datasets (WMT16 RO$\\leftrightarrow$EN and IWSLT\nDE$\\rightarrow$EN).",
          "link": "http://arxiv.org/abs/2108.08447",
          "publishedOn": "2021-08-20T01:53:50.056Z",
          "wordCount": 588,
          "title": "MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation. (arXiv:2108.08447v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganhotra_J/0/1/0/all/0/1\">Jatin Ganhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hong-Kwang J. Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuske_Z/0/1/0/all/0/1\">Zolt&#xe1;n T&#xfc;ske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>",
          "description": "End-to-end spoken language understanding (SLU) systems that process\nhuman-human or human-computer interactions are often context independent and\nprocess each turn of a conversation independently. Spoken conversations on the\nother hand, are very much context dependent, and dialog history contains useful\ninformation that can improve the processing of each conversational turn. In\nthis paper, we investigate the importance of dialog history and how it can be\neffectively integrated into end-to-end SLU systems. While processing a spoken\nutterance, our proposed RNN transducer (RNN-T) based SLU model has access to\nits dialog history in the form of decoded transcripts and SLU labels of\nprevious turns. We encode the dialog history as BERT embeddings, and use them\nas an additional input to the SLU model along with the speech features for the\ncurrent utterance. We evaluate our approach on a recently released spoken\ndialog data set, the HarperValleyBank corpus. We observe significant\nimprovements: 8% for dialog action and 30% for caller intent recognition tasks,\nin comparison to a competitive context independent end-to-end baseline system.",
          "link": "http://arxiv.org/abs/2108.08405",
          "publishedOn": "2021-08-20T01:53:50.003Z",
          "wordCount": 624,
          "title": "Integrating Dialog History into End-to-End Spoken Language Understanding Systems. (arXiv:2108.08405v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.",
          "link": "http://arxiv.org/abs/2108.08468",
          "publishedOn": "2021-08-20T01:53:49.979Z",
          "wordCount": 701,
          "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junjie H. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinden_K/0/1/0/all/0/1\">Kohei Shinden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Makoto P. Kato</a>",
          "description": "This paper addresses the problem of generating table captions for scholarly\ndocuments, which often require additional information outside the table. To\nthis end, we propose a method of retrieving relevant sentences from the paper\nbody, and feeding the table content as well as the retrieved sentences into\npre-trained language models (e.g. T5 and GPT-2) for generating table captions.\nThe contributions of this paper are: (1) discussion on the challenges in table\ncaptioning for scholarly documents; (2) development of a dataset DocBank-TB,\nwhich is publicly available; and (3) comparison of caption generation methods\nfor scholarly documents with different strategies to retrieve relevant\nsentences from the paper body. Our experimental results showed that T5 is the\nbetter generation model for this task, as it outperformed GPT-2 in BLEU and\nMETEOR implying that the generated text are clearer and more precise. Moreover,\ninputting relevant sentences matching the row header or whole table is\neffective.",
          "link": "http://arxiv.org/abs/2108.08111",
          "publishedOn": "2021-08-19T01:35:02.750Z",
          "wordCount": 599,
          "title": "Table Caption Generation in Scholarly Documents Leveraging Pre-trained Language Models. (arXiv:2108.08111v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fitzpatrick_B/0/1/0/all/0/1\">Benjamin Fitzpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinyu &quot;Sherwin&quot; Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straub_J/0/1/0/all/0/1\">Jeremy Straub</a>",
          "description": "Expert systems have been used to enable computers to make recommendations and\ndecisions. This paper presents the use of a machine learning trained expert\nsystem (MLES) for phishing site detection and fake news detection. Both topics\nshare a similar goal: to design a rule-fact network that allows a computer to\nmake explainable decisions like domain experts in each respective area. The\nphishing website detection study uses a MLES to detect potential phishing\nwebsites by analyzing site properties (like URL length and expiration time).\nThe fake news detection study uses a MLES rule-fact network to gauge news story\ntruthfulness based on factors such as emotion, the speaker's political\naffiliation status, and job. The two studies use different MLES network\nimplementations, which are presented and compared herein. The fake news study\nutilized a more linear design while the phishing project utilized a more\ncomplex connection structure. Both networks' inputs are based on commonly\navailable data sets.",
          "link": "http://arxiv.org/abs/2108.08264",
          "publishedOn": "2021-08-19T01:35:00.438Z",
          "wordCount": 604,
          "title": "Fake News and Phishing Detection Using a Machine Learning Trained Expert System. (arXiv:2108.08264v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "While pretrained encoders have achieved success in various natural language\nunderstanding (NLU) tasks, there is a gap between these pretrained encoders and\nnatural language generation (NLG). NLG tasks are often based on the\nencoder-decoder framework, where the pretrained encoders can only benefit part\nof it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual\nencoder-decoder model that regards the decoder as the task layer of\noff-the-shelf pretrained encoders. Specifically, we augment the pretrained\nmultilingual encoder with a decoder and pre-train it in a self-supervised way.\nTo take advantage of both the large-scale monolingual data and bilingual data,\nwe adopt the span corruption and translation span corruption as the\npre-training tasks. Experiments show that DeltaLM outperforms various strong\nbaselines on both natural language generation and translation tasks, including\nmachine translation, abstractive text summarization, data-to-text, and question\ngeneration. The code and pretrained models are available at\n\\url{https://aka.ms/deltalm}.",
          "link": "http://arxiv.org/abs/2106.13736",
          "publishedOn": "2021-08-19T01:35:00.347Z",
          "wordCount": 637,
          "title": "DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders. (arXiv:2106.13736v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>",
          "description": "Product summarization aims to automatically generate product descriptions,\nwhich is of great commercial potential. Considering the customer preferences on\ndifferent product aspects, it would benefit from generating aspect-oriented\ncustomized summaries. However, conventional systems typically focus on\nproviding general product summaries, which may miss the opportunity to match\nproducts with customer interests. To address the problem, we propose CUSTOM,\naspect-oriented product summarization for e-commerce, which generates diverse\nand controllable summaries towards different product aspects. To support the\nstudy of CUSTOM and further this line of research, we construct two Chinese\ndatasets, i.e., SMARTPHONE and COMPUTER, including 76,279 / 49,280 short\nsummaries for 12,118 / 11,497 real-world commercial products, respectively.\nFurthermore, we introduce EXT, an extraction-enhanced generation framework for\nCUSTOM, where two famous sequence-to-sequence models are implemented in this\npaper. We conduct extensive experiments on the two proposed datasets for CUSTOM\nand show results of two famous baseline models and EXT, which indicates that\nEXT can generate diverse, high-quality, and consistent summaries.",
          "link": "http://arxiv.org/abs/2108.08010",
          "publishedOn": "2021-08-19T01:35:00.336Z",
          "wordCount": 600,
          "title": "CUSTOM: Aspect-Oriented Product Summarization for E-Commerce. (arXiv:2108.08010v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>",
          "description": "We propose UniT, a Unified Transformer model to simultaneously learn the most\nprominent tasks across different domains, ranging from object detection to\nnatural language understanding and multimodal reasoning. Based on the\ntransformer encoder-decoder architecture, our UniT model encodes each input\nmodality with an encoder and makes predictions on each task with a shared\ndecoder over the encoded input representations, followed by task-specific\noutput heads. The entire model is jointly trained end-to-end with losses from\neach task. Compared to previous efforts on multi-task learning with\ntransformers, we share the same model parameters across all tasks instead of\nseparately fine-tuning task-specific models and handle a much higher variety of\ntasks across different domains. In our experiments, we learn 7 tasks jointly\nover 8 datasets, achieving strong performance on each task with significantly\nfewer parameters. Our code is available in MMF at https://mmf.sh.",
          "link": "http://arxiv.org/abs/2102.10772",
          "publishedOn": "2021-08-19T01:35:00.258Z",
          "wordCount": 617,
          "title": "UniT: Multimodal Multitask Learning with a Unified Transformer. (arXiv:2102.10772v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohlender_B/0/1/0/all/0/1\">Bela Bohlender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viehmann_C/0/1/0/all/0/1\">Christina Viehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hane_V/0/1/0/all/0/1\">Vincent Hane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamson_Y/0/1/0/all/0/1\">Yanik Adamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khuri_J/0/1/0/all/0/1\">Jaber Khuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossmann_J/0/1/0/all/0/1\">Jonas Brossmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "The open-access dissemination of pretrained language models through online\nrepositories has led to a democratization of state-of-the-art natural language\nprocessing (NLP) research. This also allows people outside of NLP to use such\nmodels and adapt them to specific use-cases. However, a certain amount of\ntechnical proficiency is still required which is an entry barrier for users who\nwant to apply these models to a certain task but lack the necessary knowledge\nor resources. In this work, we aim to overcome this gap by providing a tool\nwhich allows researchers to leverage pretrained models without writing a single\nline of code. Built upon the parameter-efficient adapter modules for transfer\nlearning, our AdapterHub Playground provides an intuitive interface, allowing\nthe usage of adapters for prediction, training and analysis of textual data for\na variety of NLP tasks. We present the tool's architecture and demonstrate its\nadvantages with prototypical use-cases, where we show that predictive\nperformance can easily be increased in a few-shot learning scenario. Finally,\nwe evaluate its usability in a user study. We provide the code and a live\ninterface at https://adapter-hub.github.io/playground.",
          "link": "http://arxiv.org/abs/2108.08103",
          "publishedOn": "2021-08-19T01:35:00.243Z",
          "wordCount": 627,
          "title": "AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters. (arXiv:2108.08103v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1\">Maxime Kayser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1\">Cornelius Emde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Recently, there has been an increasing number of efforts to introduce models\ncapable of generating natural language explanations (NLEs) for their\npredictions on vision-language (VL) tasks. Such models are appealing, because\nthey can provide human-friendly and comprehensive explanations. However, there\nis a lack of comparison between existing methods, which is due to a lack of\nre-usable evaluation frameworks and a scarcity of datasets. In this work, we\nintroduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable\nvision-language tasks that establishes a unified evaluation framework and\nprovides the first comprehensive comparison of existing approaches that\ngenerate NLEs for VL tasks. It spans four models and three datasets and both\nautomatic metrics and human evaluation are used to assess model-generated\nexplanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs\n(over 430k instances). We also propose a new model that combines UNITER, which\nlearns joint embeddings of images and text, and GPT-2, a pre-trained language\nmodel that is well-suited for text generation. It surpasses the previous state\nof the art by a large margin across all datasets. Code and data are available\nhere: https://github.com/maximek3/e-ViL.",
          "link": "http://arxiv.org/abs/2105.03761",
          "publishedOn": "2021-08-19T01:35:00.217Z",
          "wordCount": 681,
          "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chengkun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>",
          "description": "Understanding speaker's feelings and producing appropriate responses with\nemotion connection is a key communicative skill for empathetic dialogue\nsystems. In this paper, we propose a simple technique called Affective Decoding\nfor empathetic response generation. Our method can effectively incorporate\nemotion signals during each decoding step, and can additionally be augmented\nwith an auxiliary dual emotion encoder, which learns separate embeddings for\nthe speaker and listener given the emotion base of the dialogue. Extensive\nempirical studies show that our models are perceived to be more empathetic by\nhuman evaluations, in comparison to several strong mainstream methods for\nempathetic responding.",
          "link": "http://arxiv.org/abs/2108.08102",
          "publishedOn": "2021-08-19T01:35:00.209Z",
          "wordCount": 534,
          "title": "Affective Decoding for Empathetic Response Generation. (arXiv:2108.08102v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>",
          "description": "The number of biomedical literature on new biomedical concepts is rapidly\nincreasing, which necessitates a reliable biomedical named entity recognition\n(BioNER) model for identifying new and unseen entity mentions. However, it is\nquestionable whether existing BioNER models can effectively handle them. In\nthis work, we systematically analyze the three types of recognition abilities\nof BioNER models: memorization, synonym generalization, and concept\ngeneralization. We find that although BioNER models achieve state-of-the-art\nperformance on BioNER benchmarks based on overall performance, they have\nlimitations in identifying synonyms and new biomedical concepts such as\nCOVID-19. From this observation, we conclude that existing BioNER models are\noverestimated in terms of their generalization abilities. Also, we identify\nseveral difficulties in recognizing unseen mentions in BioNER and make the\nfollowing conclusions: (1) BioNER models tend to exploit dataset biases, which\nhinders the models' abilities to generalize, and (2) several biomedical names\nhave novel morphological patterns with little name regularity such as COVID-19,\nand models fail to recognize them. We apply a current statistics-based\ndebiasing method to our problem as a simple remedy and show the improvement in\ngeneralization to unseen mentions. We hope that our analyses and findings would\nbe able to facilitate further research into the generalization capabilities of\nNER models in a domain where their reliability is of utmost importance.",
          "link": "http://arxiv.org/abs/2101.00160",
          "publishedOn": "2021-08-19T01:35:00.202Z",
          "wordCount": 721,
          "title": "How Do Your Biomedical Named Entity Models Generalize to Novel Entities?. (arXiv:2101.00160v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1\">Md Monowar Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Noman Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>",
          "description": "In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.",
          "link": "http://arxiv.org/abs/2108.07971",
          "publishedOn": "2021-08-19T01:35:00.180Z",
          "wordCount": 531,
          "title": "De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Medical imaging technologies, including computed tomography (CT) or chest\nX-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.\nSince manual report writing is usually too time-consuming, a more intelligent\nauxiliary medical system that could generate medical reports automatically and\nimmediately is urgently needed. In this article, we propose to use the medical\nvisual language BERT (Medical-VLBERT) model to identify the abnormality on the\nCOVID-19 scans and generate the medical report automatically based on the\ndetected lesion regions. To produce more accurate medical reports and minimize\nthe visual-and-linguistic differences, this model adopts an alternate learning\nstrategy with two procedures that are knowledge pretraining and transferring.\nTo be more precise, the knowledge pretraining procedure is to memorize the\nknowledge from medical texts, while the transferring procedure is to utilize\nthe acquired knowledge for professional medical sentences generations through\nobservations of medical images. In practice, for automatic medical report\ngeneration on the COVID-19 cases, we constructed a dataset of 368 medical\nfindings in Chinese and 1104 chest CT scans from The First Affiliated Hospital\nof Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun\nYat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of\nthe COVID-19 training samples, our model was first trained on the large-scale\nChinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for\nfurther fine-tuning. The experimental results showed that Medical-VLBERT\nachieved state-of-the-art performances on terminology prediction and report\ngeneration with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The\nChinese COVID-19 CT dataset is available at https://covid19ct.github.io/.",
          "link": "http://arxiv.org/abs/2108.05067",
          "publishedOn": "2021-08-19T01:35:00.171Z",
          "wordCount": 798,
          "title": "Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>",
          "description": "Reasoning machine reading comprehension (R-MRC) aims to answer complex\nquestions that require discrete reasoning based on text. To support discrete\nreasoning, evidence, typically the concise textual fragments that describe\nquestion-related facts, including topic entities and attribute values, are\ncrucial clues from question to answer. However, previous end-to-end methods\nthat achieve state-of-the-art performance rarely solve the problem by paying\nenough emphasis on the modeling of evidence, missing the opportunity to further\nimprove the model's reasoning ability for R-MRC. To alleviate the above issue,\nin this paper, we propose an evidence-emphasized discrete reasoning approach\n(EviDR), in which sentence and clause level evidence is first detected based on\ndistant supervision, and then used to drive a reasoning module implemented with\na relational heterogeneous graph convolutional network to derive answers.\nExtensive experiments are conducted on DROP (discrete reasoning over\nparagraphs) dataset, and the results demonstrate the effectiveness of our\nproposed approach. In addition, qualitative analysis verifies the capability of\nthe proposed evidence-emphasized discrete reasoning for R-MRC.",
          "link": "http://arxiv.org/abs/2108.07994",
          "publishedOn": "2021-08-19T01:35:00.146Z",
          "wordCount": 612,
          "title": "EviDR: Evidence-Emphasized Discrete Reasoning for Reasoning Machine Reading Comprehension. (arXiv:2108.07994v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>",
          "description": "Existing data-driven methods can well handle short text generation. However,\nwhen applied to the long-text generation scenarios such as story generation or\nadvertising text generation in the commercial scenario, these methods may\ngenerate illogical and uncontrollable texts. To address these aforementioned\nissues, we propose a graph-based grouping planner(GGP) following the idea of\nfirst-plan-then-generate. Specifically, given a collection of key phrases, GGP\nfirstly encodes these phrases into an instance-level sequential representation\nand a corpus-level graph-based representation separately. With these two\nsynergic representations, we then regroup these phrases into a fine-grained\nplan, based on which we generate the final long text. We conduct our\nexperiments on three long text generation datasets and the experimental results\nreveal that GGP significantly outperforms baselines, which proves that GGP can\ncontrol the long text generation by knowing how to say and in what order.",
          "link": "http://arxiv.org/abs/2108.07998",
          "publishedOn": "2021-08-19T01:35:00.095Z",
          "wordCount": 582,
          "title": "GGP: A Graph-based Grouping Planner for Explicit Control of Long Text Generation. (arXiv:2108.07998v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07909",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_D/0/1/0/all/0/1\">D.-S. Wang</a>",
          "description": "Quantum computing has been a fascinating research field in quantum physics.\nRecent progresses motivate us to study in depth the universal quantum computing\nmodels (UQCM), which lie at the foundation of quantum computing and have tight\nconnections with fundamental physics. Although being developed decades ago, a\nphysically concise principle or picture to formalize and understand UQCM is\nstill lacking. This is challenging given the diversity of still-emerging\nmodels, but important to understand the difference between classical and\nquantum computing. In this work, we carried out a primary attempt to unify UQCM\nby classifying a few of them as two categories, hence making a table of models.\nWith such a table, some known models or schemes appear as hybridization or\ncombination of models, and more importantly, it leads to new schemes that have\nnot been explored yet. Our study of UQCM also leads to some insights into\nquantum algorithms. This work reveals the importance and feasibility of\nsystematic study of computing models.",
          "link": "http://arxiv.org/abs/2108.07909",
          "publishedOn": "2021-08-19T01:34:59.845Z",
          "wordCount": 602,
          "title": "A comparative study of universal quantum computing models: towards a physical unification. (arXiv:2108.07909v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyun Zhang</a>",
          "description": "Sequence-to-sequence models provide a viable new approach to generative\nsummarization, allowing models that are no longer limited to simply selecting\nand recombining sentences from the original text. However, these models have\nthree drawbacks: their grasp of the details of the original text is often\ninaccurate, and the text generated by such models often has repetitions, while\nit is difficult to handle words that are beyond the word list. In this paper,\nwe propose a new architecture that combines reinforcement learning and\nadversarial generative networks to enhance the sequence-to-sequence attention\nmodel. First, we use a hybrid pointer-generator network that copies words\ndirectly from the source text, contributing to accurate reproduction of\ninformation without sacrificing the ability of generators to generate new\nwords. Second, we use both intra-temporal and intra-decoder attention to\npenalize summarized content and thus discourage repetition. We apply our model\nto our own proposed COVID-19 paper title summarization task and achieve close\napproximations to the current model on ROUEG, while bringing better\nreadability.",
          "link": "http://arxiv.org/abs/2105.15176",
          "publishedOn": "2021-08-19T01:34:59.788Z",
          "wordCount": 668,
          "title": "Reinforced Generative Adversarial Network for Abstractive Text Summarization. (arXiv:2105.15176v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shaunak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Changwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Manisha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1\">Maxim Sviridenko</a>",
          "description": "Coming up with effective ad text is a time consuming process, and\nparticularly challenging for small businesses with limited advertising\nexperience. When an inexperienced advertiser onboards with a poorly written ad\ntext, the ad platform has the opportunity to detect low performing ad text, and\nprovide improvement suggestions. To realize this opportunity, we propose an ad\ntext strength indicator (TSI) which: (i) predicts the click-through-rate (CTR)\nfor an input ad text, (ii) fetches similar existing ads to create a\nneighborhood around the input ad, (iii) and compares the predicted CTRs in the\nneighborhood to declare whether the input ad is strong or weak. In addition, as\nsuggestions for ad text improvement, TSI shows anonymized versions of superior\nads (higher predicted CTR) in the neighborhood. For (i), we propose a BERT\nbased text-to-CTR model trained on impressions and clicks associated with an ad\ntext. For (ii), we propose a sentence-BERT based semantic-ad-similarity model\ntrained using weak labels from ad campaign setup data. Offline experiments\ndemonstrate that our BERT based text-to-CTR model achieves a significant lift\nin CTR prediction AUC for cold start (new) advertisers compared to bag-of-words\nbased baselines. In addition, our semantic-textual-similarity model for similar\nads retrieval achieves a precision@1 of 0.93 (for retrieving ads from the same\nproduct category); this is significantly higher compared to unsupervised\nTF-IDF, word2vec, and sentence-BERT baselines. Finally, we share promising\nonline results from advertisers in the Yahoo (Verizon Media) ad platform where\na variant of TSI was implemented with sub-second end-to-end latency.",
          "link": "http://arxiv.org/abs/2108.08226",
          "publishedOn": "2021-08-19T01:34:59.714Z",
          "wordCount": 700,
          "title": "TSI: an Ad Text Strength Indicator using Text-to-CTR and Semantic-Ad-Similarity. (arXiv:2108.08226v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weiwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaowei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazi_M/0/1/0/all/0/1\">Michaeel Kazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhoutong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Huiji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Many search systems work with large amounts of natural language data, e.g.,\nsearch queries, user profiles and documents, where deep learning based natural\nlanguage processing techniques (deep NLP) can be of great help. In this paper,\nwe introduce a comprehensive study of applying deep NLP techniques to five\nrepresentative tasks in search engines. Through the model design and\nexperiments of the five tasks, readers can find answers to three important\nquestions: (1) When is deep NLP helpful/not helpful in search systems? (2) How\nto address latency challenges? (3) How to ensure model robustness? This work\nbuilds on existing efforts of LinkedIn search, and is tested at scale on a\ncommercial search engine. We believe our experiences can provide useful\ninsights for the industry and research communities.",
          "link": "http://arxiv.org/abs/2108.08252",
          "publishedOn": "2021-08-19T01:34:59.696Z",
          "wordCount": 571,
          "title": "Deep Natural Language Processing for LinkedIn Search Systems. (arXiv:2108.08252v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bharwani_N/0/1/0/all/0/1\">Nashwin Bharwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushner_W/0/1/0/all/0/1\">Warren Kushner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandona_S/0/1/0/all/0/1\">Sangeet Dandona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreiber_B/0/1/0/all/0/1\">Ben Schreiber</a>",
          "description": "Natural Language Processing research has recently been dominated by large\nscale transformer models. Although they achieve state of the art on many\nimportant language tasks, transformers often require expensive compute\nresources, and days spanning to weeks to train. This is feasible for\nresearchers at big tech companies and leading research universities, but not\nfor scrappy start-up founders, students, and independent researchers. Stephen\nMerity's SHA-RNN, a compact, hybrid attention-RNN model, is designed for\nconsumer-grade modeling as it requires significantly fewer parameters and less\ntraining time to reach near state of the art results. We analyze Merity's model\nhere through an exploratory model analysis over several units of the\narchitecture considering both training time and overall quality in our\nassessment. Ultimately, we combine these findings into a new architecture which\nwe call SHAQ: Single Headed Attention Quasi-recurrent Neural Network. With our\nnew architecture we achieved similar accuracy results as the SHA-RNN while\naccomplishing a 4x speed boost in training.",
          "link": "http://arxiv.org/abs/2108.08207",
          "publishedOn": "2021-08-19T01:34:59.675Z",
          "wordCount": 594,
          "title": "SHAQ: Single Headed Attention with Quasi-Recurrence. (arXiv:2108.08207v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "Generating context-aware language that embodies diverse emotions is an\nimportant step towards building empathetic NLP systems. In this paper, we\npropose a formulation of modulated layer normalization -- a technique inspired\nby computer vision -- that allows us to use large-scale language models for\nemotional response generation. In automatic and human evaluation on the\nMojiTalk dataset, our proposed modulated layer normalization method outperforms\nprior baseline methods while maintaining diversity, fluency, and coherence. Our\nmethod also obtains competitive performance even when using only 10% of the\navailable training data.",
          "link": "http://arxiv.org/abs/2108.07886",
          "publishedOn": "2021-08-19T01:34:59.642Z",
          "wordCount": 525,
          "title": "Modulating Language Models with Emotions. (arXiv:2108.07886v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suri_H/0/1/0/all/0/1\">Huqun Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_W/0/1/0/all/0/1\">Wenhua Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Chunsheng Guan</a>",
          "description": "In this paper, we introduce MeDiaQA, a novel question answering(QA) dataset,\nwhich constructed on real online Medical Dialogues. It contains 22k\nmultiple-choice questions annotated by human for over 11k dialogues with 120k\nutterances between patients and doctors, covering 150 specialties of diseases,\nwhich are collected from haodf.com and dxy.com. MeDiaQA is the first QA dataset\nwhere reasoning over medical dialogues, especially their quantitative contents.\nThe dataset has the potential to test the computing, reasoning and\nunderstanding ability of models across multi-turn dialogues, which is\nchallenging compared with the existing datasets. To address the challenges, we\ndesign MeDia-BERT, and it achieves 64.3% accuracy, while human performance of\n93% accuracy, which indicates that there still remains a large room for\nimprovement.",
          "link": "http://arxiv.org/abs/2108.08074",
          "publishedOn": "2021-08-19T01:34:59.635Z",
          "wordCount": 560,
          "title": "MeDiaQA: A Question Answering Dataset on Medical Dialogues. (arXiv:2108.08074v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lisong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Intent detection and slot filling are two main tasks in natural language\nunderstanding (NLU) for identifying users' needs from their utterances. These\ntwo tasks are highly related and often trained jointly. However, most previous\nworks assume that each utterance only corresponds to one intent, ignoring the\nfact that a user utterance in many cases could include multiple intents. In\nthis paper, we propose a novel Self-Distillation Joint NLU model (SDJN) for\nmulti-intent NLU. First, we formulate multiple intent detection as a weakly\nsupervised problem and approach with multiple instance learning (MIL). Then, we\ndesign an auxiliary loop via self-distillation with three orderly arranged\ndecoders: Initial Slot Decoder, MIL Intent Decoder, and Final Slot Decoder. The\noutput of each decoder will serve as auxiliary information for the next\ndecoder. With the auxiliary knowledge provided by the MIL Intent Decoder, we\nset Final Slot Decoder as the teacher model that imparts knowledge back to\nInitial Slot Decoder to complete the loop. The auxiliary loop enables intents\nand slots to guide mutually in-depth and further boost the overall NLU\nperformance. Experimental results on two public multi-intent datasets indicate\nthat our model achieves strong performance compared to others.",
          "link": "http://arxiv.org/abs/2108.08042",
          "publishedOn": "2021-08-19T01:34:59.627Z",
          "wordCount": 638,
          "title": "Joint Multiple Intent Detection and Slot Filling via Self-distillation. (arXiv:2108.08042v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1\">Ankan Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Animesh Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1\">Tapas Nayak</a>",
          "description": "In this work, we present a Web-based annotation tool `Relation Triplets\nExtractor' \\footnote{https://abera87.github.io/annotate/} (RTE) for annotating\nrelation triplets from the text. Relation extraction is an important task for\nextracting structured information about real-world entities from the\nunstructured text available on the Web. In relation extraction, we focus on\nbinary relation that refers to relations between two entities. Recently, many\nsupervised models are proposed to solve this task, but they mostly use noisy\ntraining data obtained using the distant supervision method. In many cases,\nevaluation of the models is also done based on a noisy test dataset. The lack\nof annotated clean dataset is a key challenge in this area of research. In this\nwork, we built a web-based tool where researchers can annotate datasets for\nrelation extraction on their own very easily. We use a server-less architecture\nfor this tool, and the entire annotation operation is processed using\nclient-side code. Thus it does not suffer from any network latency, and the\nprivacy of the user's data is also maintained. We hope that this tool will be\nbeneficial for the researchers to advance the field of relation extraction.",
          "link": "http://arxiv.org/abs/2108.08184",
          "publishedOn": "2021-08-19T01:34:59.618Z",
          "wordCount": 620,
          "title": "RTE: A Tool for Annotating Relation Triplets from Text. (arXiv:2108.08184v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.",
          "link": "http://arxiv.org/abs/2108.08217",
          "publishedOn": "2021-08-19T01:34:59.554Z",
          "wordCount": 678,
          "title": "X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hongjin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yueyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "In this paper, we explore the problem of developing personalized chatbots. A\npersonalized chatbot is designed as a digital chatting assistant for a user.\nThe key characteristic of a personalized chatbot is that it should have a\nconsistent personality with the corresponding user. It can talk the same way as\nthe user when it is delegated to respond to others' messages. We present a\nretrieval-based personalized chatbot model, namely IMPChat, to learn an\nimplicit user profile from the user's dialogue history. We argue that the\nimplicit user profile is superior to the explicit user profile regarding\naccessibility and flexibility. IMPChat aims to learn an implicit user profile\nthrough modeling user's personalized language style and personalized\npreferences separately. To learn a user's personalized language style, we\nelaborately build language models from shallow to deep using the user's\nhistorical responses; To model a user's personalized preferences, we explore\nthe conditional relations underneath each post-response pair of the user. The\npersonalized preferences are dynamic and context-aware: we assign higher\nweights to those historical pairs that are topically related to the current\nquery when aggregating the personalized preferences. We match each response\ncandidate with the personalized language style and personalized preference,\nrespectively, and fuse the two matching signals to determine the final ranking\nscore. Comprehensive experiments on two large datasets show that our method\noutperforms all baseline models.",
          "link": "http://arxiv.org/abs/2108.07935",
          "publishedOn": "2021-08-19T01:34:59.499Z",
          "wordCount": 682,
          "title": "Learning Implicit User Profiles for Personalized Retrieval-Based Chatbot. (arXiv:2108.07935v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hurriyetoglu_A/0/1/0/all/0/1\">Ali H&#xfc;rriyeto&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanev_H/0/1/0/all/0/1\">Hristo Tanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavarella_V/0/1/0/all/0/1\">Vanni Zavarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piskorski_J/0/1/0/all/0/1\">Jakub Piskorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeniterzi_R/0/1/0/all/0/1\">Reyyan Yeniterzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoruk_E/0/1/0/all/0/1\">Erdem Y&#xf6;r&#xfc;k</a>",
          "description": "This workshop is the fourth issue of a series of workshops on automatic\nextraction of socio-political events from news, organized by the Emerging\nMarket Welfare Project, with the support of the Joint Research Centre of the\nEuropean Commission and with contributions from many other prominent scholars\nin this field. The purpose of this series of workshops is to foster research\nand development of reliable, valid, robust, and practical solutions for\nautomatically detecting descriptions of socio-political events, such as\nprotests, riots, wars and armed conflicts, in text streams. This year workshop\ncontributors make use of the state-of-the-art NLP technologies, such as Deep\nLearning, Word Embeddings and Transformers and cover a wide range of topics\nfrom text classification to news bias detection. Around 40 teams have\nregistered and 15 teams contributed to three tasks that are i) multilingual\nprotest news detection, ii) fine-grained classification of socio-political\nevents, and iii) discovering Black Lives Matter protest events. The workshop\nalso highlights two keynote and four invited talks about various aspects of\ncreating event data sets and multi- and cross-lingual machine learning in few-\nand zero-shot settings.",
          "link": "http://arxiv.org/abs/2108.07865",
          "publishedOn": "2021-08-19T01:34:59.161Z",
          "wordCount": 659,
          "title": "Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021): Workshop and Shared Task Report. (arXiv:2108.07865v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schoch_S/0/1/0/all/0/1\">Stephanie Schoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>",
          "description": "Text style transfer involves rewriting the content of a source sentence in a\ntarget style. Despite there being a number of style tasks with available data,\nthere has been limited systematic discussion of how text style datasets relate\nto each other. This understanding, however, is likely to have implications for\nselecting multiple data sources for model training. While it is prudent to\nconsider inherent stylistic properties when determining these relationships, we\nalso must consider how a style is realized in a particular dataset. In this\npaper, we conduct several empirical analyses of existing text style datasets.\nBased on our results, we propose a categorization of stylistic and dataset\nproperties to consider when utilizing or comparing text style datasets.",
          "link": "http://arxiv.org/abs/2108.07871",
          "publishedOn": "2021-08-19T01:34:59.124Z",
          "wordCount": 551,
          "title": "Contextualizing Variation in Text Style Transfer Datasets. (arXiv:2108.07871v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Abhiroop Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krook_R/0/1/0/all/0/1\">Robert Krook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_B/0/1/0/all/0/1\">Bo Joel Svensson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheeran_M/0/1/0/all/0/1\">Mary Sheeran</a>",
          "description": "Programming microcontrollers involves low-level interfacing with hardware and\nperipherals that are concurrent and reactive. Such programs are typically\nwritten in a mixture of C and assembly using concurrent language extensions\n(like $\\texttt{FreeRTOS tasks}$ and $\\texttt{semaphores}$), resulting in\nunsafe, callback-driven, error-prone and difficult-to-maintain code.\n\nWe address this challenge by introducing $\\texttt{SenseVM}$ - a\nbytecode-interpreted virtual machine that provides a message-passing based\n$\\textit{higher-order concurrency}$ model, originally introduced by Reppy, for\nmicrocontroller programming. This model treats synchronous operations as\nfirst-class values (called $\\texttt{Events}$) akin to the treatment of\nfirst-class functions in functional languages. This primarily allows the\nprogrammer to compose and tailor their own concurrency abstractions and,\nadditionally, abstracts away unsafe memory operations, common in shared-memory\nconcurrency models, thereby making microcontroller programs safer, composable\nand easier-to-maintain.\n\nOur VM is made portable via a low-level $\\textit{bridge}$ interface, built\natop the embedded OS - Zephyr. The bridge is implemented by all drivers and\ndesigned such that programming in response to a software message or a hardware\ninterrupt remains uniform and indistinguishable. In this paper we demonstrate\nthe features of our VM through an example, written in a Caml-like functional\nlanguage, running on the $\\texttt{nRF52840}$ and $\\texttt{STM32F4}$\nmicrocontrollers.",
          "link": "http://arxiv.org/abs/2108.07805",
          "publishedOn": "2021-08-19T01:34:59.072Z",
          "wordCount": 620,
          "title": "Higher-Order Concurrency for Microcontrollers. (arXiv:2108.07805v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Neural text matching models have been widely used in community question\nanswering, information retrieval, and dialogue. However, these models designed\nfor short texts cannot well address the long-form text matching problem,\nbecause there are many contexts in long-form texts can not be directly aligned\nwith each other, and it is difficult for existing models to capture the key\nmatching signals from such noisy data. Besides, these models are\ncomputationally expensive for simply use all textual data indiscriminately. To\ntackle the effectiveness and efficiency problem, we propose a novel\nhierarchical noise filtering model, namely Match-Ignition. The main idea is to\nplug the well-known PageRank algorithm into the Transformer, to identify and\nfilter both sentence and word level noisy information in the matching process.\nNoisy sentences are usually easy to detect because previous work has shown that\ntheir similarity can be explicitly evaluated by the word overlapping, so we\ndirectly use PageRank to filter such information based on a sentence similarity\ngraph. Unlike sentences, words rely on their contexts to express concrete\nmeanings, so we propose to jointly learn the filtering and matching process, to\nwell capture the critical word-level matching signals. Specifically, a word\ngraph is first built based on the attention scores in each self-attention block\nof Transformer, and key words are then selected by applying PageRank on this\ngraph. In this way, noisy words will be filtered out layer by layer in the\nmatching process. Experimental results show that Match-Ignition outperforms\nboth SOTA short text matching models and recent long-form text matching models.\nWe also conduct detailed analysis to show that Match-Ignition efficiently\ncaptures important sentences and words, to facilitate the long-form text\nmatching process.",
          "link": "http://arxiv.org/abs/2101.06423",
          "publishedOn": "2021-08-18T01:54:59.339Z",
          "wordCount": 749,
          "title": "Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching. (arXiv:2101.06423v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkhasov_A/0/1/0/all/0/1\">Alex Malkhasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoshin_A/0/1/0/all/0/1\">Andrey Manoshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dima_G/0/1/0/all/0/1\">George Dima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cserhati_R/0/1/0/all/0/1\">R&#xe9;ka Cserh&#xe1;ti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">Md.Sadek Hossain Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardi_M/0/1/0/all/0/1\">Matt S&#xe1;rdi</a>",
          "description": "This report presents the results of the EENLP project, done as a part of EEML\n2021 summer school.\n\nIt presents a broad index of NLP resources for Eastern European languages,\nwhich, we hope, could be helpful for the NLP community; several new\nhand-crafted cross-lingual datasets focused on Eastern European languages, and\na sketch evaluation of cross-lingual transfer learning abilities of several\nmodern multilingual Transformer-based models.",
          "link": "http://arxiv.org/abs/2108.02605",
          "publishedOn": "2021-08-18T01:54:59.299Z",
          "wordCount": 537,
          "title": "EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>",
          "description": "Social norms -- the unspoken commonsense rules about acceptable social\nbehavior -- are crucial in understanding the underlying causes and intents of\npeople's actions in narratives. For example, underlying an action such as\n\"wanting to call cops on my neighbors\" are social norms that inform our\nconduct, such as \"It is expected that you report crimes.\"\n\nWe present Social Chemistry, a new conceptual formalism to study people's\neveryday social norms and moral judgments over a rich spectrum of real life\nsituations described in natural language. We introduce Social-Chem-101, a\nlarge-scale corpus that catalogs 292k rules-of-thumb such as \"it is rude to run\na blender at 5am\" as the basic conceptual units. Each rule-of-thumb is further\nbroken down with 12 different dimensions of people's judgments, including\nsocial judgments of good and bad, moral foundations, expected cultural\npressure, and assumed legality, which together amount to over 4.5 million\nannotations of categorical labels and free-text descriptions.\n\nComprehensive empirical results based on state-of-the-art neural models\ndemonstrate that computational modeling of social norms is a promising research\ndirection. Our model framework, Neural Norm Transformer, learns and generalizes\nSocial-Chem-101 to successfully reason about previously unseen situations,\ngenerating relevant (and potentially novel) attribute-aware social\nrules-of-thumb.",
          "link": "http://arxiv.org/abs/2011.00620",
          "publishedOn": "2021-08-18T01:54:59.278Z",
          "wordCount": 688,
          "title": "Social Chemistry 101: Learning to Reason about Social and Moral Norms. (arXiv:2011.00620v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Yuqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>",
          "description": "We present a task and benchmark dataset for person-centric visual grounding,\nthe problem of linking between people named in a caption and people pictured in\nan image. In contrast to prior work in visual grounding, which is predominantly\nobject-based, our new task masks out the names of people in captions in order\nto encourage methods trained on such image-caption pairs to focus on contextual\ncues (such as rich interactions between multiple people), rather than learning\nassociations between names and appearances. To facilitate this task, we\nintroduce a new dataset, Who's Waldo, mined automatically from image-caption\ndata on Wikimedia Commons. We propose a Transformer-based method that\noutperforms several strong baselines on this task, and are releasing our data\nto the research community to spur work on contextual models that consider both\nvision and language.",
          "link": "http://arxiv.org/abs/2108.07253",
          "publishedOn": "2021-08-18T01:54:59.270Z",
          "wordCount": 609,
          "title": "Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1\">Stefan Heid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.",
          "link": "http://arxiv.org/abs/2008.01377",
          "publishedOn": "2021-08-18T01:54:59.244Z",
          "wordCount": 718,
          "title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mundotiya_R/0/1/0/all/0/1\">Rajesh Kumar Mundotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Manish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapur_R/0/1/0/all/0/1\">Rahul Kapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swasti Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anil Kumar Singh</a>",
          "description": "Corpus preparation for low-resource languages and for development of human\nlanguage technology to analyze or computationally process them is a laborious\ntask, primarily due to the unavailability of expert linguists who are native\nspeakers of these languages and also due to the time and resources required.\nBhojpuri, Magahi, and Maithili, languages of the Purvanchal region of India (in\nthe north-eastern parts), are low-resource languages belonging to the\nIndo-Aryan (or Indic) family. They are closely related to Hindi, which is a\nrelatively high-resource language, which is why we compare with Hindi. We\ncollected corpora for these three languages from various sources and cleaned\nthem to the extent possible, without changing the data in them. The text\nbelongs to different domains and genres. We calculated some basic statistical\nmeasures for these corpora at character, word, syllable, and morpheme levels.\nThese corpora were also annotated with parts-of-speech (POS) and chunk tags.\nThe basic statistical measures were both absolute and relative and were\nexptected to indicate of linguistic properties such as morphological, lexical,\nphonological, and syntactic complexities (or richness). The results were\ncompared with a standard Hindi corpus. For most of the measures, we tried to\nthe corpus size the same across the languages to avoid the effect of corpus\nsize, but in some cases it turned out that using the full corpus was better,\neven if sizes were very different. Although the results are not very clear, we\ntry to draw some conclusions about the languages and the corpora. For POS\ntagging and chunking, the BIS tagset was used to manually annotate the data.\nThe POS tagged data sizes are 16067, 14669 and 12310 sentences, respectively,\nfor Bhojpuri, Magahi and Maithili. The sizes for chunking are 9695 and 1954\nsentences for Bhojpuri and Maithili, respectively.",
          "link": "http://arxiv.org/abs/2004.13945",
          "publishedOn": "2021-08-18T01:54:59.234Z",
          "wordCount": 787,
          "title": "Linguistic Resources for Bhojpuri, Magahi and Maithili: Statistics about them, their Similarity Estimates, and Baselines for Three Applications. (arXiv:2004.13945v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sack_H/0/1/0/all/0/1\">Harald Sack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>",
          "description": "With the increasing trend in the topic of migration in Europe, the public is\nnow more engaged in expressing their opinions through various platforms such as\nTwitter. Understanding the online discourses is therefore essential to capture\nthe public opinion. The goal of this study is the analysis of social media\nplatform to quantify public attitudes towards migrations and the identification\nof different factors causing these attitudes. The tweets spanning from 2013 to\nJul-2021 in the European countries which are hosts to immigrants are collected,\npre-processed, and filtered using advanced topic modeling technique. BERT-based\nentity linking and sentiment analysis, and attention-based hate speech\ndetection are performed to annotate the curated tweets. Moreover, the external\ndatabases are used to identify the potential social and economic factors\ncausing negative attitudes of the people about migration. To further promote\nresearch in the interdisciplinary fields of social science and computer\nscience, the outcomes are integrated into a Knowledge Base (KB), i.e.,\nMigrationsKB which significantly extends the existing models to take into\naccount the public attitudes towards migrations and the economic indicators.\nThis KB is made public using FAIR principles, which can be queried through\nSPARQL endpoint. Data dumps are made available on Zenodo.",
          "link": "http://arxiv.org/abs/2108.07593",
          "publishedOn": "2021-08-18T01:54:59.141Z",
          "wordCount": 650,
          "title": "MigrationsKB: A Knowledge Base of Public Attitudes towards Migrations and their Driving Factors. (arXiv:2108.07593v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daiber_J/0/1/0/all/0/1\">Joachim Daiber</a>",
          "description": "Progress in cross-lingual modeling depends on challenging, realistic, and\ndiverse evaluation sets. We introduce Multilingual Knowledge Questions and\nAnswers (MKQA), an open-domain question answering evaluation set comprising 10k\nquestion-answer pairs aligned across 26 typologically diverse languages (260k\nquestion-answer pairs in total). Answers are based on a heavily curated,\nlanguage-independent data representation, making results comparable across\nlanguages and independent of language-specific passages. With 26 languages,\nthis dataset supplies the widest range of languages to-date for evaluating\nquestion answering. We benchmark a variety of state-of-the-art methods and\nbaselines for generative and extractive question answering, trained on Natural\nQuestions, in zero shot and translation settings. Results indicate this dataset\nis challenging even in English, but especially in low-resource languages",
          "link": "http://arxiv.org/abs/2007.15207",
          "publishedOn": "2021-08-18T01:54:59.131Z",
          "wordCount": 580,
          "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering. (arXiv:2007.15207v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mickus_T/0/1/0/all/0/1\">Timothee Mickus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_M/0/1/0/all/0/1\">Mathieu Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>",
          "description": "Can language models learn grounded representations from text distribution\nalone? This question is both central and recurrent in natural language\nprocessing; authors generally agree that grounding requires more than textual\ndistribution. We propose to experimentally test this claim: if any two words\nhave different meanings and yet cannot be distinguished from distribution\nalone, then grounding is out of the reach of text-based models. To that end, we\npresent early work on an online game for the collection of human judgments on\nthe distributional similarity of word pairs in five languages. We further\nreport early results of our data collection campaign.",
          "link": "http://arxiv.org/abs/2108.07708",
          "publishedOn": "2021-08-18T01:54:59.118Z",
          "wordCount": 534,
          "title": "A Game Interface to Study Semantic Grounding in Text-Based Models. (arXiv:2108.07708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1\">Yash Kumar Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1\">Nathanael Chambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond Mooney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>",
          "description": "Answering questions about why characters perform certain actions is central\nto understanding and reasoning about narratives. Despite recent progress in QA,\nit is not clear if existing models have the ability to answer \"why\" questions\nthat may require commonsense knowledge external to the input narrative. In this\nwork, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more\nthan 30k questions and free-form answers concerning why characters in short\nnarratives perform the actions described. For a third of this dataset, the\nanswers are not present within the narrative. Given the limitations of\nautomated evaluation for this task, we also present a systematized human\nevaluation interface for this dataset. Our evaluation of state-of-the-art\nmodels show that they are far below human performance on answering such\nquestions. They are especially worse on questions whose answers are external to\nthe narrative, thus providing a challenge for future QA and narrative\nunderstanding research.",
          "link": "http://arxiv.org/abs/2106.06132",
          "publishedOn": "2021-08-18T01:54:58.990Z",
          "wordCount": 639,
          "title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cortiz_D/0/1/0/all/0/1\">Diogo Cortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jefferson O. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calegari_N/0/1/0/all/0/1\">Newton Calegari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Ana Lu&#xed;sa Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Ana Ang&#xe9;lica Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botelho_C/0/1/0/all/0/1\">Carolina Botelho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rego_G/0/1/0/all/0/1\">Gabriel Gaudencio R&#xea;go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampaio_W/0/1/0/all/0/1\">Waldir Sampaio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggio_P/0/1/0/all/0/1\">Paulo Sergio Boggio</a>",
          "description": "Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task in NLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weak supervised corpus for fine-grained emotion in Portuguese. We\nevaluate our dataset by fine-tuning a transformer-based language model (BERT)\nand validating it on a Golden Standard annotated validation set. Our results\n(F1-score= .64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resources environment.",
          "link": "http://arxiv.org/abs/2108.07638",
          "publishedOn": "2021-08-18T01:54:58.969Z",
          "wordCount": 555,
          "title": "A Weak Supervised Dataset of Fine-Grained Emotions in Portuguese. (arXiv:2108.07638v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1\">William Rudman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillman_N/0/1/0/all/0/1\">Nate Gillman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayne_T/0/1/0/all/0/1\">Taylor Rayne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "The recent success of distributed word representations has led to an\nincreased interest in analyzing the properties of their spatial distribution.\nCurrent metrics suggest that contextualized word embedding models do not\nuniformly utilize all dimensions when embedding tokens in vector space. Here we\nargue that existing metrics are fragile and tend to obfuscate the true spatial\ndistribution of point clouds. To ameliorate this issue, we propose IsoScore: a\nnovel metric which quantifies the degree to which a point cloud uniformly\nutilizes the ambient vector space. We demonstrate that IsoScore has several\ndesirable properties such as mean invariance and direct correspondence to the\nnumber of dimensions used, which are properties that existing scores do not\npossess. Furthermore, IsoScore is conceptually intuitive and computationally\nefficient, making it well suited for analyzing the distribution of point clouds\nin arbitrary vector spaces, not necessarily limited to those of word embeddings\nalone. Additionally, we use IsoScore to demonstrate that a number of recent\nconclusions in the NLP literature that have been derived using brittle metrics\nof spatial distribution, such as average cosine similarity, may be incomplete\nor altogether inaccurate.",
          "link": "http://arxiv.org/abs/2108.07344",
          "publishedOn": "2021-08-18T01:54:58.946Z",
          "wordCount": 620,
          "title": "IsoScore: Measuring the Uniformity of Vector Space Utilization. (arXiv:2108.07344v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1\">Juan Diego Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanamanchi_P/0/1/0/all/0/1\">Pawan Sasanka Ammanamanchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>",
          "description": "Developing documentation guidelines and easy-to-use templates for datasets\nand models is a challenging task, especially given the variety of backgrounds,\nskills, and incentives of the people involved in the building of natural\nlanguage processing (NLP) tools. Nevertheless, the adoption of standard\ndocumentation practices across the field of NLP promotes more accessible and\ndetailed descriptions of NLP datasets and models, while supporting researchers\nand developers in reflecting on their work. To help with the standardization of\ndocumentation, we present two case studies of efforts that aim to develop\nreusable documentation templates -- the HuggingFace data card, a general\npurpose card for datasets in NLP, and the GEM benchmark data and model cards\nwith a focus on natural language generation. We describe our process for\ndeveloping these templates, including the identification of relevant\nstakeholder groups, the definition of a set of guiding principles, the use of\nexisting templates as our foundation, and iterative revisions based on\nfeedback.",
          "link": "http://arxiv.org/abs/2108.07374",
          "publishedOn": "2021-08-18T01:54:58.938Z",
          "wordCount": 652,
          "title": "Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards. (arXiv:2108.07374v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boudin_F/0/1/0/all/0/1\">Florian Boudin</a>",
          "description": "Citation recommendation is intended to assist researchers in the process of\nsearching for relevant papers to cite by recommending appropriate citations for\na given input text. Existing test collections for this task are noisy and\nunreliable since they are built automatically from parsed PDF papers. In this\npaper, we present our ongoing effort at creating a publicly available, manually\nannotated test collection for citation recommendation. We also conduct a series\nof experiments to evaluate the effectiveness of content-based baseline models\non the test collection, providing results for future work to improve upon. Our\ntest collection and code to replicate experiments are available at\nhttps://github.com/boudinfl/acm-cr",
          "link": "http://arxiv.org/abs/2108.07571",
          "publishedOn": "2021-08-18T01:54:58.918Z",
          "wordCount": 545,
          "title": "ACM-CR: A Manually Annotated Test Collection for Citation Recommendation. (arXiv:2108.07571v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xintong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>",
          "description": "Many generation tasks follow a one-to-many mapping relationship: each input\ncould be associated with multiple outputs. Existing methods like Conditional\nVariational AutoEncoder(CVAE) employ a latent variable to model this\none-to-many relationship. However, this high-dimensional and dense latent\nvariable lacks explainability and usually leads to poor and uncontrollable\ngenerations. In this paper, we innovatively introduce the linguistic concept of\npattern to decompose the one-to-many mapping into multiple one-to-one mappings\nand further propose a model named Sparse Pattern Mixture of Experts(SPMoE).\nEach one-to-one mapping is associated with a conditional generation pattern and\nis modeled with an expert in SPMoE. To ensure each language pattern can be\nexclusively handled with an expert model for better explainability and\ndiversity, a sparse mechanism is employed to coordinate all the expert models\nin SPMoE. We assess the performance of our SPMoE on the paraphrase generation\ntask and the experiment results prove that SPMoE can achieve a good balance in\nterms of quality, pattern-level diversity, and corpus-level diversity.",
          "link": "http://arxiv.org/abs/2108.07535",
          "publishedOn": "2021-08-18T01:54:58.884Z",
          "wordCount": 605,
          "title": "SPMoE: Generate Multiple Pattern-Aware Outputs with Sparse Pattern Mixture of Expert. (arXiv:2108.07535v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>",
          "description": "It's challenging to customize transducer-based automatic speech recognition\n(ASR) system with context information which is dynamic and unavailable during\nmodel training. In this work, we introduce a light-weight contextual spelling\ncorrection model to correct context-related recognition errors in\ntransducer-based ASR systems. We incorporate the context information into the\nspelling correction model with a shared context encoder and use a filtering\nalgorithm to handle large-size context lists. Experiments show that the model\nimproves baseline ASR model performance with about 50% relative word error rate\nreduction, which also significantly outperforms the baseline method such as\ncontextual LM biasing. The model also shows excellent performance for\nout-of-vocabulary terms not seen during training.",
          "link": "http://arxiv.org/abs/2108.07493",
          "publishedOn": "2021-08-18T01:54:58.876Z",
          "wordCount": 566,
          "title": "A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems. (arXiv:2108.07493v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bornea_M/0/1/0/all/0/1\">Mihaela Bornea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1\">Pavan Kapanipathi</a>",
          "description": "Relation linking is essential to enable question answering over knowledge\nbases. Although there are various efforts to improve relation linking\nperformance, the current state-of-the-art methods do not achieve optimal\nresults, therefore, negatively impacting the overall end-to-end question\nanswering performance. In this work, we propose a novel approach for relation\nlinking framing it as a generative problem facilitating the use of pre-trained\nsequence-to-sequence models. We extend such sequence-to-sequence models with\nthe idea of infusing structured data from the target knowledge base, primarily\nto enable these models to handle the nuances of the knowledge base. Moreover,\nwe train the model with the aim to generate a structured output consisting of a\nlist of argument-relation pairs, enabling a knowledge validation step. We\ncompared our method against the existing relation linking systems on four\ndifferent datasets derived from DBpedia and Wikidata. Our method reports large\nimprovements over the state-of-the-art while using a much simpler model that\ncan be easily adapted to different knowledge bases.",
          "link": "http://arxiv.org/abs/2108.07337",
          "publishedOn": "2021-08-18T01:54:58.866Z",
          "wordCount": 615,
          "title": "Generative Relation Linking for Question Answering over Knowledge Bases. (arXiv:2108.07337v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amitoj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasekh_A/0/1/0/all/0/1\">Amin Rasekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golbin_I/0/1/0/all/0/1\">Ilana Golbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anand Rao</a>",
          "description": "An independent ethical assessment of an artificial intelligence system is an\nimpartial examination of the system's development, deployment, and use in\nalignment with ethical values. System-level qualitative frameworks that\ndescribe high-level requirements and component-level quantitative metrics that\nmeasure individual ethical dimensions have been developed over the past few\nyears. However, there exists a gap between the two, which hinders the execution\nof independent ethical assessments in practice. This study bridges this gap and\ndesigns a holistic independent ethical assessment process for a text\nclassification model with a special focus on the task of hate speech detection.\nThe assessment is further augmented with protected attributes mining and\ncounterfactual-based analysis to enhance bias assessment. It covers assessments\nof technical performance, data bias, embedding bias, classification bias, and\ninterpretability. The proposed process is demonstrated through an assessment of\na deep hate speech detection model.",
          "link": "http://arxiv.org/abs/2108.07627",
          "publishedOn": "2021-08-18T01:54:58.811Z",
          "wordCount": 614,
          "title": "Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study. (arXiv:2108.07627v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tianlong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shouyi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_W/0/1/0/all/0/1\">Wang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dandan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Huiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>",
          "description": "Time Delay Neural Networks (TDNN)-based methods are widely used in dialect\nidentification. However, in previous work with TDNN application, subtle variant\nis being neglected in different feature scales. To address this issue, we\npropose a new architecture, named dynamic multi-scale convolution, which\nconsists of dynamic kernel convolution, local multi-scale learning, and global\nmulti-scale pooling. Dynamic kernel convolution captures features between\nshort-term and long-term context adaptively. Local multi-scale learning, which\nrepresents multi-scale features at a granular level, is able to increase the\nrange of receptive fields for convolution operation. Besides, global\nmulti-scale pooling is applied to aggregate features from different bottleneck\nlayers in order to collect information from multiple aspects. The proposed\narchitecture significantly outperforms state-of-the-art system on the\nAP20-OLR-dialect-task of oriental language recognition (OLR) challenge 2020,\nwith the best average cost performance (Cavg) of 0.067 and the best equal error\nrate (EER) of 6.52%. Compared with the known best results, our method achieves\n9% of Cavg and 45% of EER relative improvement, respectively. Furthermore, the\nparameters of proposed model are 91% fewer than the best known model.",
          "link": "http://arxiv.org/abs/2108.07787",
          "publishedOn": "2021-08-18T01:54:58.802Z",
          "wordCount": 615,
          "title": "Dynamic Multi-scale Convolution for Dialect Identification. (arXiv:2108.07787v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Ortiz_A/0/1/0/all/0/1\">Alberto Mu&#xf1;oz-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strzyz_M/0/1/0/all/0/1\">Michalina Strzyz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1\">David Vilares</a>",
          "description": "Different linearizations have been proposed to cast dependency parsing as\nsequence labeling and solve the task as: (i) a head selection problem, (ii)\nfinding a representation of the token arcs as bracket strings, or (iii)\nassociating partial transition sequences of a transition-based parser to words.\nYet, there is little understanding about how these linearizations behave in\nlow-resource setups. Here, we first study their data efficiency, simulating\ndata-restricted setups from a diverse set of rich-resource treebanks. Second,\nwe test whether such differences manifest in truly low-resource setups. The\nresults show that head selection encodings are more data-efficient and perform\nbetter in an ideal (gold) framework, but that such advantage greatly vanishes\nin favour of bracketing formats when the running setup resembles a real-world\nlow-resource configuration.",
          "link": "http://arxiv.org/abs/2108.07556",
          "publishedOn": "2021-08-18T01:54:58.795Z",
          "wordCount": 565,
          "title": "Not All Linearizations Are Equally Data-Hungry in Sequence Labeling Parsing. (arXiv:2108.07556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Latorre_J/0/1/0/all/0/1\">Javier Latorre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailleul_C/0/1/0/all/0/1\">Charlotte Bailleul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrill_T/0/1/0/all/0/1\">Tuuli Morrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conkie_A/0/1/0/all/0/1\">Alistair Conkie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_Y/0/1/0/all/0/1\">Yannis Stylianou</a>",
          "description": "In this work, we explore multiple architectures and training procedures for\ndeveloping a multi-speaker and multi-lingual neural TTS system with the goals\nof a) improving the quality when the available data in the target language is\nlimited and b) enabling cross-lingual synthesis. We report results from a large\nexperiment using 30 speakers in 8 different languages across 15 different\nlocales. The system is trained on the same amount of data per speaker. Compared\nto a single-speaker model, when the suggested system is fine tuned to a\nspeaker, it produces significantly better quality in most of the cases while it\nonly uses less than $40\\%$ of the speaker's data used to build the\nsingle-speaker model. In cross-lingual synthesis, on average, the generated\nquality is within $80\\%$ of native single-speaker models, in terms of Mean\nOpinion Score.",
          "link": "http://arxiv.org/abs/2108.07737",
          "publishedOn": "2021-08-18T01:54:58.787Z",
          "wordCount": 600,
          "title": "Combining speakers of multiple languages to improve quality of neural voices. (arXiv:2108.07737v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanerva_J/0/1/0/all/0/1\">Jenna Kanerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1\">Li-Hsin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastas_I/0/1/0/all/0/1\">Iiro Rastas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantsi_V/0/1/0/all/0/1\">Valtteri Skantsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilpelainen_J/0/1/0/all/0/1\">Jemina Kilpel&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupari_H/0/1/0/all/0/1\">Hanna-Mari Kupari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piirto_A/0/1/0/all/0/1\">Aurora Piirto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saarni_J/0/1/0/all/0/1\">Jenna Saarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevon_M/0/1/0/all/0/1\">Maija Sev&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarkka_O/0/1/0/all/0/1\">Otto Tarkka</a>",
          "description": "This document describes the annotation guidelines used to construct the Turku\nParaphrase Corpus. These guidelines were developed together with the corpus\nannotation, revising and extending the guidelines regularly during the\nannotation work. Our paraphrase annotation scheme uses the base scale 1-4,\nwhere labels 1 and 2 are used for negative candidates (not paraphrases), while\nlabels 3 and 4 are paraphrases at least in the given context if not everywhere.\nIn addition to base labeling, the scheme is enriched with additional\nsubcategories (flags) for categorizing different types of paraphrases inside\nthe two positive labels, making the annotation scheme suitable for more\nfine-grained paraphrase categorization. The annotation scheme is used to\nannotate over 100,000 Finnish paraphrase pairs.",
          "link": "http://arxiv.org/abs/2108.07499",
          "publishedOn": "2021-08-18T01:54:58.776Z",
          "wordCount": 570,
          "title": "Annotation Guidelines for the Turku Paraphrase Corpus. (arXiv:2108.07499v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bock_A/0/1/0/all/0/1\">A. Bock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palladino_A/0/1/0/all/0/1\">A. Palladino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_Heisters_S/0/1/0/all/0/1\">S. Smith-Heisters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boardman_I/0/1/0/all/0/1\">I. Boardman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellegrini_E/0/1/0/all/0/1\">E. Pellegrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bienenstock_E/0/1/0/all/0/1\">E.J. Bienenstock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenti_A/0/1/0/all/0/1\">A. Valenti</a>",
          "description": "The proliferation of news media available online simultaneously presents a\nvaluable resource and significant challenge to analysts aiming to profile and\nunderstand social and cultural trends in a geographic location of interest.\nWhile an abundance of news reports documenting significant events, trends, and\nresponses provides a more democratized picture of the social characteristics of\na location, making sense of an entire corpus to extract significant trends is a\nsteep challenge for any one analyst or team. Here, we present an approach using\nnatural language processing techniques that seeks to quantify how a set of\npre-defined topics of interest change over time across a large corpus of text.\nWe found that, given a predefined topic, we can identify and rank sets of\nterms, or n-grams, that map to those topics and have usage patterns that\ndeviate from a normal baseline. Emergence, disappearance, or significant\nvariations in n-gram usage present a ground-up picture of a topic's dynamic\nsalience within a corpus of interest.",
          "link": "http://arxiv.org/abs/2108.07345",
          "publishedOn": "2021-08-18T01:54:58.736Z",
          "wordCount": 638,
          "title": "An NLP approach to quantify dynamic salience of predefined topics in a text corpus. (arXiv:2108.07345v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yijia Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chang-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Protein is linked to almost every life process. Therefore, analyzing the\nbiological structure and property of protein sequences is critical to the\nexploration of life, as well as disease detection and drug discovery.\nTraditional protein analysis methods tend to be labor-intensive and\ntime-consuming. The emergence of deep learning models makes modeling data\npatterns in large quantities of data possible. Interdisciplinary researchers\nhave begun to leverage deep learning methods to model large biological\ndatasets, e.g. using long short-term memory and convolutional neural network\nfor protein sequence classification. After millions of years of evolution,\nevolutionary information is encoded in protein sequences. Inspired by the\nsimilarity between natural language and protein sequences, we use large-scale\nlanguage models to model evolutionary-scale protein sequences, encoding protein\nbiology information in representation. Significant improvements are observed in\nboth token-level and sequence-level tasks, demonstrating that our large-scale\nmodel can accurately capture evolution information from pretraining on\nevolutionary-scale individual sequences. Our code and model are available at\nhttps://github.com/THUDM/ProteinLM.",
          "link": "http://arxiv.org/abs/2108.07435",
          "publishedOn": "2021-08-18T01:54:58.726Z",
          "wordCount": 615,
          "title": "Modeling Protein Using Large-scale Pretrain Language Model. (arXiv:2108.07435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amorim_A/0/1/0/all/0/1\">Arthur Azevedo de Amorim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>",
          "description": "Kleene algebra with tests (KAT) is a foundational equational framework for\nreasoning about programs, which has found applications in program\ntransformations, networking and compiler optimizations, among many other areas.\nIn his seminal work, Kozen proved that KAT subsumes propositional Hoare logic,\nshowing that one can reason about the (partial) correctness of while programs\nby means of the equational theory of KAT.\n\nIn this work, we investigate the support that KAT provides for reasoning\nabout \\emph{incorrectness}, instead, as embodied by Ohearn's recently proposed\nincorrectness logic. We show that KAT cannot directly express incorrectness\nlogic. The main reason for this limitation can be traced to the fact that KAT\ncannot express explicitly the notion of codomain, which is essential to express\nincorrectness triples. To address this issue, we study Kleene algebra with Top\nand Tests (TopKAT), an extension of KAT with a top element. We show that TopKAT\nis powerful enough to express a codomain operation, to express incorrectness\ntriples, and to prove all the rules of incorrectness logic sound. This shows\nthat one can reason about the incorrectness of while-like programs by means of\nthe equational theory of TopKAT.",
          "link": "http://arxiv.org/abs/2108.07707",
          "publishedOn": "2021-08-18T01:54:58.717Z",
          "wordCount": 633,
          "title": "On Incorrectness Logic and Kleene Algebra With Top and Tests. (arXiv:2108.07707v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Helen Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1\">Cooper Raterink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Ivan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1\">Nicholas Frosst</a>",
          "description": "Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.",
          "link": "http://arxiv.org/abs/2108.07790",
          "publishedOn": "2021-08-18T01:54:58.657Z",
          "wordCount": 597,
          "title": "Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_S/0/1/0/all/0/1\">Sijie Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haifeng Hu</a>",
          "description": "Humans express their opinions and emotions through multiple modalities which\nmainly consist of textual, acoustic and visual modalities. Prior works on\nmultimodal sentiment analysis mostly apply Recurrent Neural Network (RNN) to\nmodel aligned multimodal sequences. However, it is unpractical to align\nmultimodal sequences due to different sample rates for different modalities.\nMoreover, RNN is prone to the issues of gradient vanishing or exploding and it\nhas limited capacity of learning long-range dependency which is the major\nobstacle to model unaligned multimodal sequences. In this paper, we introduce\nGraph Capsule Aggregation (GraphCAGE) to model unaligned multimodal sequences\nwith graph-based neural model and Capsule Network. By converting sequence data\ninto graph, the previously mentioned problems of RNN are avoided. In addition,\nthe aggregation capability of Capsule Network and the graph-based structure\nenable our model to be interpretable and better solve the problem of long-range\ndependency. Experimental results suggest that GraphCAGE achieves\nstate-of-the-art performance on two benchmark datasets with representations\nrefined by Capsule Network and interpretation provided.",
          "link": "http://arxiv.org/abs/2108.07543",
          "publishedOn": "2021-08-18T01:54:58.644Z",
          "wordCount": 593,
          "title": "Graph Capsule Aggregation for Unaligned Multimodal Sequences. (arXiv:2108.07543v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xianrui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "Language models (LMs) pre-trained on massive amounts of text, in particular\nbidirectional encoder representations from Transformers (BERT), generative\npre-training (GPT), and GPT-2, have become a key technology for many natural\nlanguage processing tasks. In this paper, we present results using fine-tuned\nGPT, GPT-2, and their combination for automatic speech recognition (ASR).\nUnlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct\nproduct of the output probabilities is no longer a valid language prior\nprobability. A conversion method is proposed to compute the correct language\nprior probability based on bidirectional LM outputs in a mathematically exact\nway. Experimental results on the widely used AMI and Switchboard ASR tasks\nshowed that the combination of the fine-tuned GPT and GPT-2 outperformed the\ncombination of three neural LMs with different architectures trained from\nscratch on the in-domain text by up to a 12% relative word error rate reduction\n(WERR). Furthermore, the proposed conversion for language prior probabilities\nenables BERT to receive an extra 3% relative WERR, and the combination of BERT,\nGPT and GPT-2 results in further improvements.",
          "link": "http://arxiv.org/abs/2108.07789",
          "publishedOn": "2021-08-18T01:54:58.598Z",
          "wordCount": 620,
          "title": "Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition. (arXiv:2108.07789v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "We propose Future Discriminators for Generation (FUDGE), a flexible and\nmodular method for controlled text generation. Given a pre-existing model G for\ngenerating text from a distribution of interest, FUDGE enables conditioning on\na desired attribute a (for example, formality) while requiring access only to\nG's output logits. FUDGE learns an attribute predictor operating on a partial\nsequence, and uses this predictor's outputs to adjust G's original\nprobabilities. We show that FUDGE models terms corresponding to a Bayesian\ndecomposition of the conditional distribution of G given attribute a. Moreover,\nFUDGE can easily compose predictors for multiple desired attributes. We\nevaluate FUDGE on three tasks -- couplet completion in poetry, topic control in\nlanguage generation, and formality change in machine translation -- and observe\ngains in all three tasks.",
          "link": "http://arxiv.org/abs/2104.05218",
          "publishedOn": "2021-08-17T01:54:46.340Z",
          "wordCount": 590,
          "title": "FUDGE: Controlled Text Generation With Future Discriminators. (arXiv:2104.05218v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We introduce a noisy channel approach for language model prompting in\nfew-shot text classification. Instead of computing the likelihood of the label\ngiven the input (referred as direct models), channel models compute the\nconditional probability of the input given the label, and are thereby required\nto explain every word in the input. We use channel models for recently proposed\nfew-shot learning methods with no or very limited updates to the language model\nparameters, via either in-context demonstration or prompt tuning. Our\nexperiments show that, for both methods, channel models significantly\noutperform their direct counterparts, which we attribute to their stability,\ni.e., lower variance and higher worst-case accuracy. We also present extensive\nablations that provide recommendations for when to use channel prompt tuning\ninstead of other competitive models (e.g., direct head tuning): channel prompt\ntuning is preferred when the number of training examples is small, labels in\nthe training data are imbalanced, or generalization to unseen labels is\nrequired.",
          "link": "http://arxiv.org/abs/2108.04106",
          "publishedOn": "2021-08-17T01:54:46.279Z",
          "wordCount": 617,
          "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification. (arXiv:2108.04106v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "The newly emerged transformer technology has a tremendous impact on NLP\nresearch. In the general English domain, transformer-based models have achieved\nstate-of-the-art performances on various NLP benchmarks. In the clinical\ndomain, researchers also have investigated transformer models for clinical\napplications. The goal of this study is to systematically explore three widely\nused transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical\nrelation extraction and develop an open-source package with clinical\npre-trained transformer-based models to facilitate information extraction in\nthe clinical domain. We developed a series of clinical RE models based on three\ntransformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these\nmodels using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2\nchallenges. We compared two classification strategies (binary vs. multi-class\nclassification) and investigated two approaches to generate candidate relations\nin different experimental settings. In this study, we compared three\ntransformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We\ndemonstrated that the RoBERTa-clinical RE model achieved the best performance\non the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2\ndataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our\nresults indicated that the binary classification strategy consistently\noutperformed the multi-class classification strategy for clinical relation\nextraction. Our methods and models are publicly available at\nhttps://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction.\nWe believe this work will improve current practice on clinical relation\nextraction and other related NLP tasks in the biomedical domain.",
          "link": "http://arxiv.org/abs/2107.08957",
          "publishedOn": "2021-08-17T01:54:46.271Z",
          "wordCount": 708,
          "title": "Clinical Relation Extraction Using Transformer-based Models. (arXiv:2107.08957v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>",
          "description": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.",
          "link": "http://arxiv.org/abs/2108.03857",
          "publishedOn": "2021-08-17T01:54:46.244Z",
          "wordCount": 613,
          "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>",
          "description": "In this paper, we describe our end-to-end multilingual speech translation\nsystem submitted to the IWSLT 2021 evaluation campaign on the Multilingual\nSpeech Translation shared task. Our system is built by leveraging transfer\nlearning across modalities, tasks and languages. First, we leverage\ngeneral-purpose multilingual modules pretrained with large amounts of\nunlabelled and labelled data. We further enable knowledge transfer from the\ntext task to the speech task by training two tasks jointly. Finally, our\nmultilingual model is finetuned on speech translation task-specific data to\nachieve the best translation results. Experimental results show our system\noutperforms the reported systems, including both end-to-end and cascaded based\napproaches, by a large margin.\n\nIn some translation directions, our speech translation results evaluated on\nthe public Multilingual TEDx test set are even comparable with the ones from a\nstrong text-to-text translation system, which uses the oracle speech\ntranscripts as input.",
          "link": "http://arxiv.org/abs/2107.06959",
          "publishedOn": "2021-08-17T01:54:46.238Z",
          "wordCount": 641,
          "title": "FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task. (arXiv:2107.06959v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zuoyun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "Subword units are commonly used for end-to-end automatic speech recognition\n(ASR), while a fully acoustic-oriented subword modeling approach is somewhat\nmissing. We propose an acoustic data-driven subword modeling (ADSM) approach\nthat adapts the advantages of several text-based and acoustic-based subword\nmethods into one pipeline. With a fully acoustic-oriented label design and\nlearning process, ADSM produces acoustic-structured subword units and\nacoustic-matched target sequence for further ASR training. The obtained ADSM\nlabels are evaluated with different end-to-end ASR approaches including CTC,\nRNN-Transducer and attention models. Experiments on the LibriSpeech corpus show\nthat ADSM clearly outperforms both byte pair encoding (BPE) and\npronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis\nshows that ADSM achieves acoustically more logical word segmentation and more\nbalanced sequence length, and thus, is suitable for both time-synchronous and\nlabel-synchronous models. We also briefly describe how to apply acoustic-based\nsubword regularization and unseen text segmentation using ADSM.",
          "link": "http://arxiv.org/abs/2104.09106",
          "publishedOn": "2021-08-17T01:54:46.213Z",
          "wordCount": 624,
          "title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1\">Deepak Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1\">Jared Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGresley_P/0/1/0/all/0/1\">Patrick LeGresley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Anand Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainbrand_D/0/1/0/all/0/1\">Dmitri Vainbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashinkunti_P/0/1/0/all/0/1\">Prethvi Kashinkunti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1\">Julie Bernauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1\">Amar Phanishayee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Large language models have led to state-of-the-art accuracies across a range\nof tasks. However, training these models efficiently is challenging for two\nreasons: a) GPU memory capacity is limited, making it impossible to fit large\nmodels on even a multi-GPU server, and b) the number of compute operations\nrequired to train these models can result in unrealistically long training\ntimes. Consequently, new methods of model parallelism such as tensor and\npipeline parallelism have been proposed. Unfortunately, naive usage of these\nmethods leads to fundamental scaling issues at thousands of GPUs, e.g., due to\nexpensive cross-node communication or devices spending significant time waiting\non other devices to make progress.\n\nIn this paper, we show how different types of parallelism methods (tensor,\npipeline, and data parallelism) can be composed to scale to thousands of GPUs\nand models with trillions of parameters. We survey techniques for pipeline\nparallelism and propose a novel interleaved pipeline parallelism schedule that\ncan improve throughput by 10+% with memory footprint comparable to existing\napproaches. We quantitatively study the trade-offs between tensor, pipeline,\nand data parallelism, and provide intuition as to how to configure distributed\ntraining of a large model. Our approach allows us to perform training\niterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs\nwith achieved per-GPU throughput of 52% of theoretical peak. Our code is open\nsourced at https://github.com/nvidia/megatron-lm.",
          "link": "http://arxiv.org/abs/2104.04473",
          "publishedOn": "2021-08-17T01:54:46.207Z",
          "wordCount": 744,
          "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. (arXiv:2104.04473v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "The existence of multiple datasets for sarcasm detection prompts us to apply\ntransfer learning to exploit their commonality. The adversarial neural transfer\n(ANT) framework utilizes multiple loss terms that encourage the source-domain\nand the target-domain feature distributions to be similar while optimizing for\ndomain-specific performance. However, these objectives may be in conflict,\nwhich can lead to optimization difficulties and sometimes diminished transfer.\nWe propose a generalized latent optimization strategy that allows different\nlosses to accommodate each other and improves training dynamics. The proposed\nmethod outperforms transfer learning and meta-learning baselines. In\nparticular, we achieve 10.02% absolute performance gain over the previous state\nof the art on the iSarcasm dataset.",
          "link": "http://arxiv.org/abs/2104.09261",
          "publishedOn": "2021-08-17T01:54:46.188Z",
          "wordCount": 594,
          "title": "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection. (arXiv:2104.09261v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jia Tracy Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_M/0/1/0/all/0/1\">Michiharu Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prihar_E/0/1/0/all/0/1\">Ethan Prihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_B/0/1/0/all/0/1\">Ben Graff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>",
          "description": "Since the introduction of the original BERT (i.e., BASE BERT), researchers\nhave developed various customized BERT models with improved performance for\nspecific domains and tasks by exploiting the benefits of {\\em transfer\nlearning}. Due to the nature of mathematical texts, which often use domain\nspecific vocabulary along with equations and math symbols, we posit that the\ndevelopment of a new BERT model for mathematics would be useful for many\nmathematical downstream tasks. In this resource paper, we introduce our\nmulti-institutional effort (i.e., two learning platforms and three academic\ninstitutions in the US) toward this need: MathBERT, a model created by\npre-training the BASE BERT model on a large mathematical corpus ranging from\npre-kindergarten (pre-k), to high-school, to college graduate level\nmathematical content. In addition, we select three general NLP tasks that are\noften used in mathematics education: prediction of knowledge component,\nauto-grading open-ended Q\\&A, and knowledge tracing, to demonstrate the\nsuperiority of MathBERT over BASE BERT. Our experiments show that MathBERT\noutperforms prior best methods by 1.2-22\\% and BASE BERT by 2-8\\% on these\ntasks. In addition, we build a mathematics specific vocabulary `mathVocab' to\ntrain with MathBERT. We discover that MathBERT pre-trained with `mathVocab'\noutperforms MathBERT trained with the BASE BERT vocabulary (i.e., `origVocab').\nMathBERT is currently being adopted at the participated leaning platforms:\nStride, Inc, a commercial educational resource provider, and ASSISTments.org, a\nfree online educational platform. We release MathBERT for public usage at:\nhttps://github.com/tbs17/MathBERT.",
          "link": "http://arxiv.org/abs/2106.07340",
          "publishedOn": "2021-08-17T01:54:46.170Z",
          "wordCount": 718,
          "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kahyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas J. Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McInnes_B/0/1/0/all/0/1\">Bridget McInnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">&#xd6;zlem Uzuner</a>",
          "description": "Methods and Materials: We investigated transferability of neural\nnetwork-based de-identification sys-tems with and without domain\ngeneralization. We used two domain generalization approaches: a novel approach\nJoint-Domain Learning (JDL) as developed in this paper, and a state-of-the-art\ndomain general-ization approach Common-Specific Decomposition (CSD) from the\nliterature. First, we measured trans-ferability from a single external source.\nSecond, we used two external sources and evaluated whether domain\ngeneralization can improve transferability of de-identification models across\ndomains which rep-resent different note types from the same institution. Third,\nusing two external sources with in-domain training data, we studied whether\nexternal source data are useful even in cases where sufficient in-domain\ntraining data are available. Finally, we investigated transferability of the\nde-identification mod-els across institutions. Results and Conclusions: We\nfound transferability from a single external source gave inconsistent re-sults.\nUsing additional external sources consistently yielded an F1-score of\napproximately 80%, but domain generalization was not always helpful to improve\ntransferability. We also found that external sources were useful even in cases\nwhere in-domain training data were available by reducing the amount of needed\nin-domain training data or by improving performance. Transferability across\ninstitutions was differed by note type and annotation label. External sources\nfrom a different institution were also useful to further improve performance.",
          "link": "http://arxiv.org/abs/2102.08517",
          "publishedOn": "2021-08-17T01:54:46.163Z",
          "wordCount": 684,
          "title": "Transferability of Neural Network-based De-identification Systems. (arXiv:2102.08517v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David I. Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adebonojo_D/0/1/0/all/0/1\">Damilola Adebonojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayeni_A/0/1/0/all/0/1\">Adesina Ayeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofe Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1\">Ayodele Awokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>",
          "description": "Massively multilingual machine translation (MT) has shown impressive\ncapabilities, including zero and few-shot translation between low-resource\nlanguage pairs. However, these models are often evaluated on high-resource\nlanguages with the assumption that they generalize to low-resource ones. The\ndifficulty of evaluating MT models on low-resource pairs is often due to lack\nof standardized evaluation datasets. In this paper, we present MENYO-20k, the\nfirst multi-domain parallel corpus with a special focus on clean orthography\nfor Yor\\`ub\\'a--English with standardized train-test splits for benchmarking.\nWe provide several neural MT benchmarks and compare them to the performance of\npopular pre-trained (massively multilingual) MT models both for the\nheterogeneous test set and its subdomains. Since these pre-trained models use\nhuge amounts of data with uncertain quality, we also analyze the effect of\ndiacritics, a major characteristic of Yor\\`ub\\'a, in the training data. We\ninvestigate how and when this training condition affects the final quality and\nintelligibility of a translation. Our models outperform massively multilingual\nmodels such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$ BLEU) when\ntranslating to Yor\\`ub\\'a, setting a high quality benchmark for future\nresearch.",
          "link": "http://arxiv.org/abs/2103.08647",
          "publishedOn": "2021-08-17T01:54:46.140Z",
          "wordCount": 673,
          "title": "The Effect of Domain and Diacritics in Yor\\`ub\\'a-English Neural Machine Translation. (arXiv:2103.08647v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong Q. Nguyen</a>",
          "description": "The mainstream image captioning models rely on Convolutional Neural Network\n(CNN) image features to generate captions via recurrent models. Recently, image\nscene graphs have been used to augment captioning models so as to leverage\ntheir structural semantics, such as object entities, relationships and\nattributes. Several studies have noted that the naive use of scene graphs from\na black-box scene graph generator harms image captioning performance and that\nscene graph-based captioning models have to incur the overhead of explicit use\nof image features to generate decent captions. Addressing these challenges, we\npropose \\textbf{SG2Caps}, a framework that utilizes only the scene graph labels\nfor competitive image captioning performance. The basic idea is to close the\nsemantic gap between the two scene graphs - one derived from the input image\nand the other from its caption. In order to achieve this, we leverage the\nspatial location of objects and the Human-Object-Interaction (HOI) labels as an\nadditional HOI graph. SG2Caps outperforms existing scene graph-only captioning\nmodels by a large margin, indicating scene graphs as a promising representation\nfor image captioning. Direct utilization of scene graph labels avoids expensive\ngraph convolutions over high-dimensional CNN features resulting in 49% fewer\ntrainable parameters. Our code is available at:\nhttps://github.com/Kien085/SG2Caps",
          "link": "http://arxiv.org/abs/2102.04990",
          "publishedOn": "2021-08-17T01:54:46.113Z",
          "wordCount": 683,
          "title": "In Defense of Scene Graphs for Image Captioning. (arXiv:2102.04990v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soldan_M/0/1/0/all/0/1\">Mattia Soldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Sisi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1\">Jesper Tegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Grounding language queries in videos aims at identifying the time interval\n(or moment) semantically relevant to a language query. The solution to this\nchallenging task demands understanding videos' and queries' semantic content\nand the fine-grained reasoning about their multi-modal interactions. Our key\nidea is to recast this challenge into an algorithmic graph matching problem.\nFueled by recent advances in Graph Neural Networks, we propose to leverage\nGraph Convolutional Networks to model video and textual information as well as\ntheir semantic alignment. To enable the mutual exchange of information across\nthe modalities, we design a novel Video-Language Graph Matching Network\n(VLG-Net) to match video and query graphs. Core ingredients include\nrepresentation graphs built atop video snippets and query tokens separately and\nused to model intra-modality relationships. A Graph Matching layer is adopted\nfor cross-modal context modeling and multi-modal fusion. Finally, moment\ncandidates are created using masked moment attention pooling by fusing the\nmoment's enriched snippet features. We demonstrate superior performance over\nstate-of-the-art grounding methods on three widely used datasets for temporal\nlocalization of moments in videos with language queries: ActivityNet-Captions,\nTACoS, and DiDeMo.",
          "link": "http://arxiv.org/abs/2011.10132",
          "publishedOn": "2021-08-17T01:54:46.097Z",
          "wordCount": 672,
          "title": "VLG-Net: Video-Language Graph Matching Network for Video Grounding. (arXiv:2011.10132v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chun Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zaixiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "The choice of token vocabulary affects the performance of machine\ntranslation. This paper aims to figure out what is a good vocabulary and\nwhether one can find the optimal vocabulary without trial training. To answer\nthese questions, we first provide an alternative understanding of the role of\nvocabulary from the perspective of information theory. Motivated by this, we\nformulate the quest of vocabularization -- finding the best token dictionary\nwith a proper size -- as an optimal transport (OT) problem. We propose VOLT, a\nsimple and efficient solution without trial training. Empirical results show\nthat VOLT outperforms widely-used vocabularies in diverse scenarios, including\nWMT-14 English-German and TED's 52 translation directions. For example, VOLT\nachieves almost 70% vocabulary size reduction and 0.5 BLEU gain on\nEnglish-German translation. Also, compared to BPE-search, VOLT reduces the\nsearch time from 384 GPU hours to 30 GPU hours on English-German translation.\nCodes are available at https://github.com/Jingjing-NLP/VOLT .",
          "link": "http://arxiv.org/abs/2012.15671",
          "publishedOn": "2021-08-17T01:54:46.090Z",
          "wordCount": 642,
          "title": "Vocabulary Learning via Optimal Transport for Neural Machine Translation. (arXiv:2012.15671v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>",
          "description": "Referring expression generation (REG) algorithms offer computational models\nof the production of referring expressions. In earlier work, a corpus of\nreferring expressions (REs) in Mandarin was introduced. In the present paper,\nwe annotate this corpus, evaluate classic REG algorithms on it, and compare the\nresults with earlier results on the evaluation of REG for English referring\nexpressions. Next, we offer an in-depth analysis of the corpus, focusing on\nissues that arise from the grammar of Mandarin. We discuss shortcomings of\nprevious REG evaluations that came to light during our investigation and we\nhighlight some surprising results. Perhaps most strikingly, we found a much\nhigher proportion of under-specified expressions than previous studies had\nsuggested, not just in Mandarin but in English as well.",
          "link": "http://arxiv.org/abs/2011.07398",
          "publishedOn": "2021-08-17T01:54:46.023Z",
          "wordCount": 591,
          "title": "Lessons from Computational Modelling of Reference Production in Mandarin and English. (arXiv:2011.07398v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Soumyadeep Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sural_S/0/1/0/all/0/1\">Shamik Sural</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1\">Niyati Chhaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_A/0/1/0/all/0/1\">Anandhavelu Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>",
          "description": "A consumer-dependent (business-to-consumer) organization tends to present\nitself as possessing a set of human qualities, which is termed as the brand\npersonality of the company. The perception is impressed upon the consumer\nthrough the content, be it in the form of advertisement, blogs or magazines,\nproduced by the organization. A consistent brand will generate trust and retain\ncustomers over time as they develop an affinity towards regularity and common\npatterns. However, maintaining a consistent messaging tone for a brand has\nbecome more challenging with the virtual explosion in the amount of content\nwhich needs to be authored and pushed to the Internet to maintain an edge in\nthe era of digital marketing. To understand the depth of the problem, we\ncollect around 300K web page content from around 650 companies. We develop\ntrait-specific classification models by considering the linguistic features of\nthe content. The classifier automatically identifies the web articles which are\nnot consistent with the mission and vision of a company and further helps us to\ndiscover the conditions under which the consistency cannot be maintained. To\naddress the brand inconsistency issue, we then develop a sentence ranking\nsystem that outputs the top three sentences that need to be changed for making\na web article more consistent with the company's brand personality.",
          "link": "http://arxiv.org/abs/2011.09754",
          "publishedOn": "2021-08-17T01:54:46.012Z",
          "wordCount": 718,
          "title": "An Integrated Approach for Improving Brand Consistency of Web Content: Modeling, Analysis and Recommendation. (arXiv:2011.09754v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_V/0/1/0/all/0/1\">Violet Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeNero_J/0/1/0/all/0/1\">John DeNero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "We propose an efficient batching strategy for variable-length decoding on GPU\narchitectures. During decoding, when candidates terminate or are pruned\naccording to heuristics, our streaming approach periodically \"refills\" the\nbatch before proceeding with a selected subset of candidates. We apply our\nmethod to variable-width beam search on a state-of-the-art machine translation\nmodel. Our method decreases runtime by up to 71% compared to a fixed-width beam\nsearch baseline and 17% compared to a variable-width baseline, while matching\nbaselines' BLEU. Finally, experiments show that our method can speed up\ndecoding in other domains, such as semantic and syntactic parsing.",
          "link": "http://arxiv.org/abs/2010.02164",
          "publishedOn": "2021-08-17T01:54:45.988Z",
          "wordCount": 589,
          "title": "A Streaming Approach For Efficient Batched Beam Search. (arXiv:2010.02164v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>",
          "description": "Vision-and-language pretraining (VLP) aims to learn generic multimodal\nrepresentations from massive image-text pairs. While various successful\nattempts have been proposed, learning fine-grained semantic alignments between\nimage-text pairs plays a key role in their approaches. Nevertheless, most\nexisting VLP approaches have not fully utilized the intrinsic knowledge within\nthe image-text pairs, which limits the effectiveness of the learned alignments\nand further restricts the performance of their models. To this end, we\nintroduce a new VLP method called ROSITA, which integrates the cross- and\nintra-modal knowledge in a unified scene graph to enhance the semantic\nalignments. Specifically, we introduce a novel structural knowledge masking\n(SKM) strategy to use the scene graph structure as a priori to perform masked\nlanguage (region) modeling, which enhances the semantic alignments by\neliminating the interference information within and across modalities.\nExtensive ablation studies and comprehensive analysis verifies the\neffectiveness of ROSITA in semantic alignments. Pretrained with both in-domain\nand out-of-domain datasets, ROSITA significantly outperforms existing\nstate-of-the-art VLP methods on three typical vision-and-language tasks over\nsix benchmark datasets.",
          "link": "http://arxiv.org/abs/2108.07073",
          "publishedOn": "2021-08-17T01:54:45.979Z",
          "wordCount": 634,
          "title": "ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration. (arXiv:2108.07073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1\">Houcemeddine Turki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taieb_M/0/1/0/all/0/1\">Mohamed Ali Hadj Taieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouicha_M/0/1/0/all/0/1\">Mohamed Ben Aouicha</a>",
          "description": "So far, multi-label classification algorithms have been evaluated using\nstatistical methods that do not consider the semantics of the considered\nclasses and that fully depend on abstract computations such as Bayesian\nReasoning. Currently, there are several attempts to develop ontology-based\nmethods for a better assessment of supervised classification algorithms. In\nthis research paper, we define a novel approach that aligns expected labels\nwith predicted labels in multi-label classification using ontology-driven\nfeature-based semantic similarity measures and we use it to develop a method\nfor creating precise confusion matrices for a more effective evaluation of\nmulti-label classification algorithms.",
          "link": "http://arxiv.org/abs/2011.00109",
          "publishedOn": "2021-08-17T01:54:45.972Z",
          "wordCount": 579,
          "title": "Knowledge-Based Construction of Confusion Matrices for Multi-Label Classification Algorithms using Semantic Similarity Measures. (arXiv:2011.00109v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velankar_M/0/1/0/all/0/1\">Makarand Velankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotian_R/0/1/0/all/0/1\">Rachita Kotian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Parag Kulkarni</a>",
          "description": "Lyrics play a significant role in conveying the song's mood and are\ninformation to understand and interpret music communication. Conventional\nnatural language processing approaches use translation of the Hindi text into\nEnglish for analysis. This approach is not suitable for lyrics as it is likely\nto lose the inherent intended contextual meaning. Thus, the need was identified\nto develop a system for Devanagari text analysis. The data set of 300 song\nlyrics with equal distribution in five different moods is used for the\nexperimentation. The proposed system performs contextual mood analysis of Hindi\nsong lyrics in Devanagari text format. The contextual analysis is stored as a\nknowledge base, updated using an incremental learning approach with new data.\nContextual knowledge graph with moods and associated important contextual terms\nprovides the graphical representation of the lyric data set used. The testing\nresults show 64% accuracy for the mood prediction. This work can be easily\nextended to applications related to Hindi literary work such as summarization,\nindexing, contextual retrieval, context-based classification and grouping of\ndocuments.",
          "link": "http://arxiv.org/abs/2108.06947",
          "publishedOn": "2021-08-17T01:54:45.954Z",
          "wordCount": 616,
          "title": "Contextual Mood Analysis with Knowledge Graph Representation for Hindi Song Lyrics in Devanagari Script. (arXiv:2108.06947v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burnyshev_P/0/1/0/all/0/1\">Pavel Burnyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malykh_V/0/1/0/all/0/1\">Valentin Malykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bout_A/0/1/0/all/0/1\">Andrey Bout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>",
          "description": "Sub-tasks of intent classification, such as robustness to distribution shift,\nadaptation to specific user groups and personalization, out-of-domain\ndetection, require extensive and flexible datasets for experiments and\nevaluation. As collecting such datasets is time- and labor-consuming, we\npropose to use text generation methods to gather datasets. The generator should\nbe trained to generate utterances that belong to the given intent. We explore\ntwo approaches to generating task-oriented utterances. In the zero-shot\napproach, the model is trained to generate utterances from seen intents and is\nfurther used to generate utterances for intents unseen during training. In the\none-shot approach, the model is presented with a single utterance from a test\nintent. We perform a thorough automatic, and human evaluation of the dataset\ngenerated utilizing two proposed approaches. Our results reveal that the\nattributes of the generated data are close to original test sets, collected via\ncrowd-sourcing.",
          "link": "http://arxiv.org/abs/2108.06991",
          "publishedOn": "2021-08-17T01:54:45.932Z",
          "wordCount": 584,
          "title": "A Single Example Can Improve Zero-Shot Data Generation. (arXiv:2108.06991v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1906.04225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>",
          "description": "Retrieve-and-edit based approaches to structured prediction, where structures\nassociated with retrieved neighbors are edited to form new structures, have\nrecently attracted increased interest. However, much recent work merely\nconditions on retrieved structures (e.g., in a sequence-to-sequence framework),\nrather than explicitly manipulating them. We show we can perform accurate\nsequence labeling by explicitly (and only) copying labels from retrieved\nneighbors. Moreover, because this copying is label-agnostic, we can achieve\nimpressive performance in zero-shot sequence-labeling tasks. We additionally\nconsider a dynamic programming approach to sequence labeling in the presence of\nretrieved neighbors, which allows for controlling the number of distinct\n(copied) segments used to form a prediction, and leads to both more\ninterpretable and accurate predictions.",
          "link": "http://arxiv.org/abs/1906.04225",
          "publishedOn": "2021-08-17T01:54:45.922Z",
          "wordCount": 577,
          "title": "Label-Agnostic Sequence Labeling by Copying Nearest Neighbors. (arXiv:1906.04225v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dat Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Stephen S. Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>",
          "description": "We propose a novel interpretable deep neural network for text classification,\ncalled ProtoryNet, based on a new concept of prototype trajectories. Motivated\nby the prototype theory in modern linguistics, ProtoryNet makes a prediction by\nfinding the most similar prototype for each sentence in a text sequence and\nfeeding an RNN backbone with the proximity of each sentence to the\ncorresponding active prototype. The RNN backbone then captures the temporal\npattern of the prototypes, which we refer to as prototype trajectories.\nPrototype trajectories enable intuitive and fine-grained interpretation of the\nreasoning process of the RNN model, in resemblance to how humans analyze texts.\nWe also design a prototype pruning procedure to reduce the total number of\nprototypes used by the model for better interpretability. Experiments on\nmultiple public data sets show that ProtoryNet is more accurate than the\nbaseline prototype-based deep neural net and reduces the performance gap\ncompared to state-of-the-art black-box models. In addition, after prototype\npruning, the resulting ProtoryNet models only need less than or around 20\nprototypes for all datasets, which significantly benefits interpretability.\nFurthermore, we report a survey result indicating that human users find\nProtoryNet more intuitive and easier to understand than other prototype-based\nmethods.",
          "link": "http://arxiv.org/abs/2007.01777",
          "publishedOn": "2021-08-17T01:54:45.913Z",
          "wordCount": 658,
          "title": "Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>",
          "description": "We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.",
          "link": "http://arxiv.org/abs/2108.07127",
          "publishedOn": "2021-08-17T01:54:45.905Z",
          "wordCount": 638,
          "title": "Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages. (arXiv:2108.07127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lizhi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Weijia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenmian Yang</a>",
          "description": "Spoken Language Understanding (SLU), a core component of the task-oriented\ndialogue system, expects a shorter inference latency due to the impatience of\nhumans. Non-autoregressive SLU models clearly increase the inference speed but\nsuffer uncoordinated-slot problems caused by the lack of sequential dependency\ninformation among each slot chunk. To gap this shortcoming, in this paper, we\npropose a novel non-autoregressive SLU model named Layered-Refine Transformer,\nwhich contains a Slot Label Generation (SLG) task and a Layered Refine\nMechanism (LRM). SLG is defined as generating the next slot label with the\ntoken sequence and generated slot labels. With SLG, the non-autoregressive\nmodel can efficiently obtain dependency information during training and spend\nno extra time in inference. LRM predicts the preliminary SLU results from\nTransformer's middle states and utilizes them to guide the final prediction.\nExperiments on two public datasets indicate that our model significantly\nimproves SLU performance (1.5\\% on Overall accuracy) while substantially speed\nup (more than 10 times) the inference process over the state-of-the-art\nbaseline.",
          "link": "http://arxiv.org/abs/2108.07005",
          "publishedOn": "2021-08-17T01:54:45.898Z",
          "wordCount": 599,
          "title": "An Effective Non-Autoregressive Model for Spoken Language Understanding. (arXiv:2108.07005v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaduo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shujuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>",
          "description": "The multi-format information extraction task in the 2021 Language and\nIntelligence Challenge is designed to comprehensively evaluate information\nextraction from different dimensions. It consists of an multiple slots relation\nextraction subtask and two event extraction subtasks that extract events from\nboth sentence-level and document-level. Here we describe our system for this\nmulti-format information extraction competition task. Specifically, for the\nrelation extraction subtask, we convert it to a traditional triple extraction\ntask and design a voting based method that makes full use of existing models.\nFor the sentence-level event extraction subtask, we convert it to a NER task\nand use a pointer labeling based method for extraction. Furthermore,\nconsidering the annotated trigger information may be helpful for event\nextraction, we design an auxiliary trigger recognition model and use the\nmulti-task learning mechanism to integrate the trigger features into the event\nextraction model. For the document-level event extraction subtask, we design an\nEncoder-Decoder based method and propose a Transformer-alike decoder.\nFinally,our system ranks No.4 on the test set leader-board of this multi-format\ninformation extraction task, and its F1 scores for the subtasks of relation\nextraction, event extractions of sentence-level and document-level are 79.887%,\n85.179%, and 70.828% respectively. The codes of our model are available at\n{https://github.com/neukg/MultiIE}.",
          "link": "http://arxiv.org/abs/2108.06957",
          "publishedOn": "2021-08-17T01:54:45.871Z",
          "wordCount": 644,
          "title": "An Effective System for Multi-format Information Extraction. (arXiv:2108.06957v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiawen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_J/0/1/0/all/0/1\">Jinye Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1\">Kenny Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>",
          "description": "The analytical description of charts is an exciting and important research\narea with many applications in academia and industry. Yet, this challenging\ntask has received limited attention from the computational linguistics research\ncommunity. This paper proposes \\textsf{AutoChart}, a large dataset for the\nanalytical description of charts, which aims to encourage more research into\nthis important area. Specifically, we offer a novel framework that generates\nthe charts and their analytical description automatically. We conducted\nextensive human and machine evaluations on the generated charts and\ndescriptions and demonstrate that the generated texts are informative,\ncoherent, and relevant to the corresponding charts.",
          "link": "http://arxiv.org/abs/2108.06897",
          "publishedOn": "2021-08-17T01:54:45.838Z",
          "wordCount": 543,
          "title": "AutoChart: A Dataset for Chart-to-Text Generation Task. (arXiv:2108.06897v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1\">Phuc Tran Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabryszak_A/0/1/0/all/0/1\">Aleksandra Gabryszak</a>",
          "description": "We present MobIE, a German-language dataset, which is human-annotated with 20\ncoarse- and fine-grained entity types and entity linking information for\ngeographically linkable entities. The dataset consists of 3,232 social media\ntexts and traffic reports with 91K tokens, and contains 20.5K annotated\nentities, 13.1K of which are linked to a knowledge base. A subset of the\ndataset is human-annotated with seven mobility-related, n-ary relation types,\nwhile the remaining documents are annotated using a weakly-supervised labeling\napproach implemented with the Snorkel framework. To the best of our knowledge,\nthis is the first German-language dataset that combines annotations for NER, EL\nand RE, and thus can be used for joint and multi-task learning of these\nfundamental information extraction tasks. We make MobIE public at\nhttps://github.com/dfki-nlp/mobie.",
          "link": "http://arxiv.org/abs/2108.06955",
          "publishedOn": "2021-08-17T01:54:45.829Z",
          "wordCount": 585,
          "title": "MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain. (arXiv:2108.06955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barba_O/0/1/0/all/0/1\">Ocean M. Barba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calbay_F/0/1/0/all/0/1\">Franz Arvin T. Calbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francisco_A/0/1/0/all/0/1\">Angelica Jane S. Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1\">Angel Luis D. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponay_C/0/1/0/all/0/1\">Charmaine S. Ponay</a>",
          "description": "Social media has played a huge part on how people get informed and\ncommunicate with one another. It has helped people express their needs due to\ndistress especially during disasters. Because posts made through it are\npublicly accessible by default, Twitter is among the most helpful social media\nsites in times of disaster. With this, the study aims to assess the needs\nexpressed during calamities by Filipinos on Twitter. Data were gathered and\nclassified as either disaster-related or unrelated with the use of Na\\\"ive\nBayes classifier. After this, the disaster-related tweets were clustered per\ndisaster type using Incremental Clustering Algorithm, and then sub-clustered\nbased on the location and time of the tweet using Density-based Spatiotemporal\nClustering Algorithm. Lastly, using Support Vector Machines, the tweets were\nclassified according to the expressed need, such as shelter, rescue, relief,\ncash, prayer, and others. After conducting the study, results showed that the\nIncremental Clustering Algorithm and Density-Based Spatiotemporal Clustering\nAlgorithm were able to cluster the tweets with f-measure scores of 47.20% and\n82.28% respectively. Also, the Na\\\"ive Bayes and Support Vector Machines were\nable to classify with an average f-measure score of 97% and an average accuracy\nof 77.57% respectively.",
          "link": "http://arxiv.org/abs/2108.06853",
          "publishedOn": "2021-08-17T01:54:45.753Z",
          "wordCount": 661,
          "title": "Clustering Filipino Disaster-Related Tweets Using Incremental and Density-Based Spatiotemporal Algorithm with Support Vector Machines for Needs Assessment 2. (arXiv:2108.06853v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Tables are often created with hierarchies, but existing works on table\nreasoning mainly focus on flat tables and neglect hierarchical tables.\nHierarchical tables challenge existing methods by hierarchical indexing, as\nwell as implicit relationships of calculation and semantics. This work presents\nHiTab, a free and open dataset for the research community to study question\nanswering (QA) and natural language generation (NLG) over hierarchical tables.\nHiTab is a cross-domain dataset constructed from a wealth of statistical\nreports and Wikipedia pages, and has unique characteristics: (1) nearly all\ntables are hierarchical, and (2) both target sentences for NLG and questions\nfor QA are revised from high-quality descriptions in statistical reports that\nare meaningful and diverse. (3) HiTab provides fine-grained annotations on both\nentity and quantity alignment. Targeting hierarchical structure, we devise a\nnovel hierarchy-aware logical form for symbolic reasoning over tables, which\nshows high effectiveness. Then given annotations of entity and quantity\nalignment, we propose partially supervised training, which helps models to\nlargely reduce spurious predictions in the QA task. In the NLG task, we find\nthat entity and quantity alignment also helps NLG models to generate better\nresults in a conditional generation setting. Experiment results of\nstate-of-the-art baselines suggest that this dataset presents a strong\nchallenge and a valuable benchmark for future research.",
          "link": "http://arxiv.org/abs/2108.06712",
          "publishedOn": "2021-08-17T01:54:45.681Z",
          "wordCount": 668,
          "title": "HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. (arXiv:2108.06712v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yuchen Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>",
          "description": "To quantitatively and intuitively explore the generalization ability of\npre-trained language models (PLMs), we have designed several tasks of\narithmetic and logical reasoning. We both analyse how well PLMs generalize when\nthe test data is in the same distribution as the train data and when it is\ndifferent, for the latter analysis, we have also designed a cross-distribution\ntest set other than the in-distribution test set. We conduct experiments on one\nof the most advanced and publicly released generative PLM - BART. Our research\nfinds that the PLMs can easily generalize when the distribution is the same,\nhowever, it is still difficult for them to generalize out of the distribution.",
          "link": "http://arxiv.org/abs/2108.06743",
          "publishedOn": "2021-08-17T01:54:45.655Z",
          "wordCount": 557,
          "title": "Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning. (arXiv:2108.06743v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Same_F/0/1/0/all/0/1\">Fahime Same</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>",
          "description": "Despite achieving encouraging results, neural Referring Expression Generation\nmodels are often thought to lack transparency. We probed neural Referential\nForm Selection (RFS) models to find out to what extent the linguistic features\ninfluencing the RE form are learnt and captured by state-of-the-art RFS models.\nThe results of 8 probing tasks show that all the defined features were learnt\nto some extent. The probing tasks pertaining to referential status and\nsyntactic position exhibited the highest performance. The lowest performance\nwas achieved by the probing models designed to predict discourse structure\nproperties beyond the sentence level.",
          "link": "http://arxiv.org/abs/2108.06806",
          "publishedOn": "2021-08-17T01:54:45.642Z",
          "wordCount": 529,
          "title": "What can Neural Referential Form Selectors Learn?. (arXiv:2108.06806v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yutong Li</a>",
          "description": "Building an independent misspelling detector and serve it before correction\ncan bring multiple benefits to speller and other search components, which is\nparticularly true for the most commonly deployed noisy-channel based speller\nsystems. With rapid development of deep learning and substantial advancement in\ncontextual representation learning such as BERTology, building a decent\nmisspelling detector without having to rely on hand-crafted features associated\nwith noisy-channel architecture becomes more-than-ever accessible. However\nBERTolgy models are trained with natural language corpus but Maps Search is\nhighly domain specific, would BERTology continue its success. In this paper we\ndesign 4 stages of models for misspeling detection ranging from the most basic\nLSTM to single-domain augmented fine-tuned BERT. We found for Maps Search in\nour case, other advanced BERTology family model such as RoBERTa does not\nnecessarily outperform BERT, and a classic cross-domain fine-tuned full BERT\neven underperforms a smaller single-domain fine-tuned BERT. We share more\nfindings through comprehensive modeling experiments and analysis, we also\nbriefly cover the data generation algorithm breakthrough.",
          "link": "http://arxiv.org/abs/2108.06842",
          "publishedOn": "2021-08-17T01:54:45.630Z",
          "wordCount": 596,
          "title": "Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations. (arXiv:2108.06842v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_M/0/1/0/all/0/1\">Megan Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jingjing Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acero_A/0/1/0/all/0/1\">Alex Acero</a>",
          "description": "Named entity recognition (NER) is usually developed and tested on text from\nwell-written sources. However, in intelligent voice assistants, where NER is an\nimportant component, input to NER may be noisy because of user or speech\nrecognition error. In applications, entity labels may change frequently, and\nnon-textual properties like topicality or popularity may be needed to choose\namong alternatives.\n\nWe describe a NER system intended to address these problems. We test and\ntrain this system on a proprietary user-derived dataset. We compare with a\nbaseline text-only NER system; the baseline enhanced with external gazetteers;\nand the baseline enhanced with the search and indirect labelling techniques we\ndescribe below. The final configuration gives around 6% reduction in NER error\nrate. We also show that this technique improves related tasks, such as semantic\nparsing, with an improvement of up to 5% in error rate.",
          "link": "http://arxiv.org/abs/2108.06633",
          "publishedOn": "2021-08-17T01:54:45.617Z",
          "wordCount": 609,
          "title": "DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants. (arXiv:2108.06633v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Gaole He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Knowledge base question answering (KBQA) aims to answer a question over a\nknowledge base (KB). Early studies mainly focused on answering simple questions\nover KBs and achieved great success. However, their performance on complex\nquestions is still far from satisfactory. Therefore, in recent years,\nresearchers propose a large number of novel methods, which looked into the\nchallenges of answering complex questions. In this survey, we review recent\nadvances on KBQA with the focus on solving complex questions, which usually\ncontain multiple subjects, express compound relations, or involve numerical\noperations. In detail, we begin with introducing the complex KBQA task and\nrelevant background. Then, we describe benchmark datasets for complex KBQA task\nand introduce the construction process of these datasets. Next, we present two\nmainstream categories of methods for complex KBQA, namely semantic\nparsing-based (SP-based) methods and information retrieval-based (IR-based)\nmethods. Specifically, we illustrate their procedures with flow designs and\ndiscuss their major differences and similarities. After that, we summarize the\nchallenges that these two categories of methods encounter when answering\ncomplex questions, and explicate advanced solutions and techniques used in\nexisting work. Finally, we conclude and discuss several promising directions\nrelated to complex KBQA for future research.",
          "link": "http://arxiv.org/abs/2108.06688",
          "publishedOn": "2021-08-17T01:54:45.603Z",
          "wordCount": 639,
          "title": "Complex Knowledge Base Question Answering: A Survey. (arXiv:2108.06688v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_J/0/1/0/all/0/1\">Jessica Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narisetty_C/0/1/0/all/0/1\">Chaitanya Narisetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>",
          "description": "We motivate and propose a suite of simple but effective improvements for\nconcept-to-text generation called SAPPHIRE: Set Augmentation and Post-hoc\nPHrase Infilling and REcombination. We demonstrate their effectiveness on\ngenerative commonsense reasoning, a.k.a. the CommonGen task, through\nexperiments using both BART and T5 models. Through extensive automatic and\nhuman evaluation, we show that SAPPHIRE noticeably improves model performance.\nAn in-depth qualitative analysis illustrates that SAPPHIRE effectively\naddresses many issues of the baseline model generations, including lack of\ncommonsense, insufficient specificity, and poor fluency.",
          "link": "http://arxiv.org/abs/2108.06643",
          "publishedOn": "2021-08-17T01:54:45.566Z",
          "wordCount": 532,
          "title": "SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation. (arXiv:2108.06643v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanqun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dineen_S/0/1/0/all/0/1\">Shay Dineen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhipeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueqing Liu</a>",
          "description": "Public security vulnerability reports (e.g., CVE reports) play an important\nrole in the maintenance of computer and network systems. Security companies and\nadministrators rely on information from these reports to prioritize tasks on\ndeveloping and deploying patches to their customers. Since these reports are\nunstructured texts, automatic information extraction (IE) can help scale up the\nprocessing by converting the unstructured reports to structured forms, e.g.,\nsoftware names and versions and vulnerability types. Existing works on\nautomated IE for security vulnerability reports often rely on a large number of\nlabeled training samples. However, creating massive labeled training set is\nboth expensive and time consuming. In this work, for the first time, we propose\nto investigate this problem where only a small number of labeled training\nsamples are available. In particular, we investigate the performance of\nfine-tuning several state-of-the-art pre-trained language models on our small\ntraining dataset. The results show that with pre-trained language models and\ncarefully tuned hyperparameters, we have reached or slightly outperformed the\nstate-of-the-art system on this task. Consistent with previous two-step process\nof first fine-tuning on main category and then transfer learning to others as\nin [7], if otherwise following our proposed approach, the number of required\nlabeled samples substantially decrease in both stages: 90% reduction in\nfine-tuning from 5758 to 576,and 88.8% reduction in transfer learning with 64\nlabeled samples per category. Our experiments thus demonstrate the\neffectiveness of few-sample learning on NER for security vulnerability report.\nThis result opens up multiple research opportunities for few-sample learning\nfor security vulnerability reports, which is discussed in the paper. Code:\nhttps://github.com/guanqun-yang/FewVulnerability.",
          "link": "http://arxiv.org/abs/2108.06590",
          "publishedOn": "2021-08-17T01:54:45.498Z",
          "wordCount": 721,
          "title": "Few-Sample Named Entity Recognition for Security Vulnerability Reports by Fine-Tuning Pre-Trained Language Models. (arXiv:2108.06590v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ernie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_A/0/1/0/all/0/1\">Alex Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>",
          "description": "We propose a shared task on training instance selection for few-shot neural\ntext generation. Large-scale pretrained language models have led to dramatic\nimprovements in few-shot text generation. Nonetheless, almost all previous work\nsimply applies random sampling to select the few-shot training instances.\nLittle to no attention has been paid to the selection strategies and how they\nwould affect model performance. The study of the selection strategy can help us\nto (1) make the most use of our annotation budget in downstream tasks and (2)\nbetter benchmark few-shot text generative models. We welcome submissions that\npresent their selection strategies and the effects on the generation quality.",
          "link": "http://arxiv.org/abs/2108.06614",
          "publishedOn": "2021-08-17T01:54:45.387Z",
          "wordCount": 553,
          "title": "The SelectGen Challenge: Finding the Best Training Samples for Few-Shot Neural Text Generation. (arXiv:2108.06614v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1\">Deuk Sin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Consistency, which refers to the capability of generating the same\npredictions for semantically similar contexts, is a highly desirable property\nfor a sound language understanding model. Although recent pretrained language\nmodels (PLMs) deliver outstanding performance in various downstream tasks, they\nshould exhibit consistent behaviour provided the models truly understand\nlanguage. In this paper, we propose a simple framework named consistency\nanalysis on language understanding models (CALUM)} to evaluate the model's\nlower-bound consistency ability. Through experiments, we confirmed that current\nPLMs are prone to generate inconsistent predictions even for semantically\nidentical inputs. We also observed that multi-task training with paraphrase\nidentification tasks is of benefit to improve consistency, increasing the\nconsistency by 13% on average.",
          "link": "http://arxiv.org/abs/2108.06665",
          "publishedOn": "2021-08-17T01:54:45.340Z",
          "wordCount": 546,
          "title": "Accurate, yet inconsistent? Consistency Analysis on Language Understanding Models. (arXiv:2108.06665v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Atul Kr. Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao-Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shatam_S/0/1/0/all/0/1\">Sheetal Shatam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fransen_T/0/1/0/all/0/1\">Theodorus Fransen</a>",
          "description": "We present the findings of the LoResMT 2021 shared task which focuses on\nmachine translation (MT) of COVID-19 data for both low-resource spoken and sign\nlanguages. The organization of this task was conducted as part of the fourth\nworkshop on technologies for machine translation of low resource languages\n(LoResMT). Parallel corpora is presented and publicly available which includes\nthe following directions: English$\\leftrightarrow$Irish,\nEnglish$\\leftrightarrow$Marathi, and Taiwanese Sign\nlanguage$\\leftrightarrow$Traditional Chinese. Training data consists of 8112,\n20933 and 128608 segments, respectively. There are additional monolingual data\nsets for Marathi and English that consist of 21901 segments. The results\npresented here are based on entries from a total of eight teams. Three teams\nsubmitted systems for English$\\leftrightarrow$Irish while five teams submitted\nsystems for English$\\leftrightarrow$Marathi. Unfortunately, there were no\nsystems submissions for the Taiwanese Sign language$\\leftrightarrow$Traditional\nChinese task. Maximum system performance was computed using BLEU and follow as\n36.0 for English--Irish, 34.6 for Irish--English, 24.2 for English--Marathi,\nand 31.3 for Marathi--English.",
          "link": "http://arxiv.org/abs/2108.06598",
          "publishedOn": "2021-08-17T01:54:45.014Z",
          "wordCount": 656,
          "title": "Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-resource Languages. (arXiv:2108.06598v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiajun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guangyu Yan</a>",
          "description": "Entity extraction is a key technology for obtaining information from massive\ntexts in natural language processing. The further interaction between them does\nnot meet the standards of human reading comprehension, thus limiting the\nunderstanding of the model, and also the omission or misjudgment of the answer\n(ie the target entity) due to the reasoning question. An effective MRC-based\nentity extraction model-MRC-I2DP, which uses the proposed gated\nattention-attracting mechanism to adjust the restoration of each part of the\ntext pair, creating problems and thinking for multi-level interactive attention\ncalculations to increase the target entity It also uses the proposed 2D\nprobability coding module, TALU function and mask mechanism to strengthen the\ndetection of all possible targets of the target, thereby improving the\nprobability and accuracy of prediction. Experiments have proved that MRC-I2DP\nrepresents an overall state-of-the-art model in 7 from the scientific and\npublic domains, achieving a performance improvement of 2.1% ~ 10.4% compared to\nthe model model in F1.",
          "link": "http://arxiv.org/abs/2108.06444",
          "publishedOn": "2021-08-17T01:54:44.992Z",
          "wordCount": 598,
          "title": "A New Entity Extraction Method Based on Machine Reading Comprehension. (arXiv:2108.06444v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratik Kumar</a>",
          "description": "With surge in online platforms, there has been an upsurge in the user\nengagement on these platforms via comments and reactions. A large portion of\nsuch textual comments are abusive, rude and offensive to the audience. With\nmachine learning systems in-place to check such comments coming onto platform,\nbiases present in the training data gets passed onto the classifier leading to\ndiscrimination against a set of classes, religion and gender. In this work, we\nevaluate different classifiers and feature to estimate the bias in these\nclassifiers along with their performance on downstream task of toxicity\nclassification. Results show that improvement in performance of automatic toxic\ncomment detection models is positively correlated to mitigating biases in these\nmodels. In our work, LSTM with attention mechanism proved to be a better\nmodelling strategy than a CNN model. Further analysis shows that fasttext\nembeddings is marginally preferable than glove embeddings on training models\nfor toxicity comment detection. Deeper analysis reveals the findings that such\nautomatic models are particularly biased to specific identity groups even\nthough the model has a high AUC score. Finally, in effort to mitigate bias in\ntoxicity detection models, a multi-task setup trained with auxiliary task of\ntoxicity sub-types proved to be useful leading to upto 0.26% (6% relative) gain\nin AUC scores.",
          "link": "http://arxiv.org/abs/2108.06487",
          "publishedOn": "2021-08-17T01:54:44.936Z",
          "wordCount": 659,
          "title": "Investigating Bias In Automatic Toxic Comment Detection: An Empirical Study. (arXiv:2108.06487v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walter_T/0/1/0/all/0/1\">Tobias Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirschner_C/0/1/0/all/0/1\">Celina Kirschner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>",
          "description": "We analyze bias in historical corpora as encoded in diachronic distributional\nsemantic models by focusing on two specific forms of bias, namely a political\n(i.e., anti-communism) and racist (i.e., antisemitism) one. For this, we use a\nnew corpus of German parliamentary proceedings, DeuPARL, spanning the period\n1867--2020. We complement this analysis of historical biases in diachronic word\nembeddings with a novel measure of bias on the basis of term co-occurrences and\ngraph-based label propagation. The results of our bias measurements align with\ncommonly perceived historical trends of antisemitic and anti-communist biases\nin German politics in different time periods, thus indicating the viability of\nanalyzing historical bias trends using semantic spaces induced from historical\ncorpora.",
          "link": "http://arxiv.org/abs/2108.06295",
          "publishedOn": "2021-08-16T00:47:33.181Z",
          "wordCount": 570,
          "title": "Diachronic Analysis of German Parliamentary Proceedings: Ideological Shifts through the Lens of Political Biases. (arXiv:2108.06295v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.01107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1\">Luis Enrico Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1\">Diane Kathryn Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Question generation (QG) is a natural language generation task where a model\nis trained to ask questions corresponding to some input text. Most recent\napproaches frame QG as a sequence-to-sequence problem and rely on additional\nfeatures and mechanisms to increase performance; however, these often increase\nmodel complexity, and can rely on auxiliary data unavailable in practical use.\nA single Transformer-based unidirectional language model leveraging transfer\nlearning can be used to produce high quality questions while disposing of\nadditional task-specific complexity. Our QG model, finetuned from GPT-2 Small,\noutperforms several paragraph-level QG baselines on the SQuAD dataset by 0.95\nMETEOR points. Human evaluators rated questions as easy to answer, relevant to\ntheir context paragraph, and corresponding well to natural human speech. Also\nintroduced is a new set of baseline scores on the RACE dataset, which has not\npreviously been used for QG tasks. Further experimentation with varying model\ncapacities and datasets with non-identification type questions is recommended\nin order to further verify the robustness of pretrained Transformer-based LMs\nas question generators.",
          "link": "http://arxiv.org/abs/2005.01107",
          "publishedOn": "2021-08-16T00:47:32.091Z",
          "wordCount": 666,
          "title": "Simplifying Paragraph-level Question Generation via Transformer Language Models. (arXiv:2005.01107v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Kelvin Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cachola_I/0/1/0/all/0/1\">Isabel Cachola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "We address the task of explaining relationships between two scientific\ndocuments using natural language text. This task requires modeling the complex\ncontent of long technical documents, deducing a relationship between these\ndocuments, and expressing the details of that relationship in text. In addition\nto the theoretical interest of this task, successful solutions can help improve\nresearcher efficiency in search and review. In this paper we establish a\ndataset of 622K examples from 154K documents. We pretrain a large language\nmodel to serve as the foundation for autoregressive approaches to the task. We\nexplore the impact of taking different views on the two documents, including\nthe use of dense representations extracted with scientific IE systems. We\nprovide extensive automatic and human evaluations which show the promise of\nsuch models, but make clear challenges for future work.",
          "link": "http://arxiv.org/abs/2002.00317",
          "publishedOn": "2021-08-16T00:47:32.080Z",
          "wordCount": 612,
          "title": "Explaining Relationships Between Scientific Documents. (arXiv:2002.00317v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>",
          "description": "Most previous methods for text data augmentation are limited to simple tasks\nand weak baselines. We explore data augmentation on hard tasks (i.e., few-shot\nnatural language understanding) and strong baselines (i.e., pretrained models\nwith over one billion parameters). Under this setting, we reproduced a large\nnumber of previous augmentation methods and found that these methods bring\nmarginal gains at best and sometimes degrade the performance much. To address\nthis challenge, we propose a novel data augmentation method FlipDA that jointly\nuses a generative model and a classifier to generate label-flipped data.\nCentral to the idea of FlipDA is the discovery that generating label-flipped\ndata is more crucial to the performance than generating label-preserved data.\nExperiments show that FlipDA achieves a good tradeoff between effectiveness and\nrobustness---it substantially improves many tasks while not negatively\naffecting the others.",
          "link": "http://arxiv.org/abs/2108.06332",
          "publishedOn": "2021-08-16T00:47:32.049Z",
          "wordCount": 571,
          "title": "FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning. (arXiv:2108.06332v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1\">Joaqu&#xed;n Silveira-Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>",
          "description": "This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as\nwell as the corresponding performance evaluations. Both models were pre-trained\nusing the largest Spanish corpus known to date, with a total of 570GB of clean\nand deduplicated text processed for this work, compiled from the web crawlings\nperformed by the National Library of Spain from 2009 to 2019. We extended the\ncurrent evaluation datasets with an extractive Question Answering dataset and\nour models outperform the existing Spanish models across tasks and settings.",
          "link": "http://arxiv.org/abs/2107.07253",
          "publishedOn": "2021-08-16T00:47:32.041Z",
          "wordCount": 547,
          "title": "Spanish Language Models. (arXiv:2107.07253v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Benefiting from large-scale pre-training, we have witnessed significant\nperformance boost on the popular Visual Question Answering (VQA) task. Despite\nrapid progress, it remains unclear whether these state-of-the-art (SOTA) models\nare robust when encountering examples in the wild. To study this, we introduce\nAdversarial VQA, a new large-scale VQA benchmark, collected iteratively via an\nadversarial human-and-model-in-the-loop procedure. Through this new benchmark,\nwe discover several interesting findings. (i) Surprisingly, we find that during\ndataset collection, non-expert annotators can easily attack SOTA VQA models\nsuccessfully. (ii) Both large-scale pre-trained models and adversarial training\nmethods achieve far worse performance on the new benchmark than over standard\nVQA v2 dataset, revealing the fragility of these models while demonstrating the\neffectiveness of our adversarial dataset. (iii) When used for data\naugmentation, our dataset can effectively boost model performance on other\nrobust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light\non robustness study in the community and serve as a valuable benchmark for\nfuture work.",
          "link": "http://arxiv.org/abs/2106.00245",
          "publishedOn": "2021-08-16T00:47:32.034Z",
          "wordCount": 649,
          "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models. (arXiv:2106.00245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resabal_J/0/1/0/all/0/1\">Jose Kristian Resabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">James Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1\">Dan John Velasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Transformers represent the state-of-the-art in Natural Language Processing\n(NLP) in recent years, proving effective even in tasks done in low-resource\nlanguages. While pretrained transformers for these languages can be made, it is\nchallenging to measure their true performance and capacity due to the lack of\nhard benchmark datasets, as well as the difficulty and cost of producing them.\nIn this paper, we present three contributions: First, we propose a methodology\nfor automatically producing Natural Language Inference (NLI) benchmark datasets\nfor low-resource languages using published news articles. Through this, we\ncreate and release NewsPH-NLI, the first sentence entailment benchmark dataset\nin the low-resource Filipino language. Second, we produce new pretrained\ntransformers based on the ELECTRA technique to further alleviate the resource\nscarcity in Filipino, benchmarking them on our dataset against other\ncommonly-used transfer learning techniques. Lastly, we perform analyses on\ntransfer learning techniques to shed light on their true performance when\noperating in low-data domains through the use of degradation tests.",
          "link": "http://arxiv.org/abs/2010.11574",
          "publishedOn": "2021-08-16T00:47:31.996Z",
          "wordCount": 675,
          "title": "Exploiting News Article Structure for Automatic Corpus Generation of Entailment Datasets. (arXiv:2010.11574v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Hannah Lei</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiqi Lu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ji_A/0/1/0/all/0/1\">Alan Ji</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bertram_E/0/1/0/all/0/1\">Emmett Bertram</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Paul Gao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Barman_A/0/1/0/all/0/1\">Arko Barman</a> (1) ((1) Rice University, Houston, United States, (2) The University of Texas Health Science Center at Houston, United States)",
          "description": "Many COVID-19 patients developed prolonged symptoms after the infection,\nincluding fatigue, delirium, and headache. The long-term health impact of these\nconditions is still not clear. It is necessary to develop a way to follow up\nwith these patients for monitoring their health status to support timely\nintervention and treatment. In the lack of sufficient human resources to follow\nup with patients, we propose a novel smart chatbot solution backed with machine\nlearning to collect information (i.e., generating digital diary) in a\npersonalized manner. In this article, we describe the design framework and\ncomponents of our prototype.",
          "link": "http://arxiv.org/abs/2103.06816",
          "publishedOn": "2021-08-16T00:47:31.979Z",
          "wordCount": 636,
          "title": "COVID-19 Smart Chatbot Prototype for Patient Monitoring. (arXiv:2103.06816v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>",
          "description": "Time is an important dimension in our physical world. Lots of facts can\nevolve with respect to time. For example, the U.S. President might change every\nfour years. Therefore, it is important to consider the time dimension and\nempower the existing QA models to reason over time. However, the existing QA\ndatasets contain rather few time-sensitive questions, hence not suitable for\ndiagnosing or benchmarking the model's temporal reasoning capability. In order\nto promote research in this direction, we propose to construct a time-sensitive\nQA dataset. The dataset is constructed by 1) mining time-evolving facts from\nWikiData and align them to their corresponding Wikipedia page, 2) employing\ncrowd workers to verify and calibrate these noisy facts, 3) generating\nquestion-answer pairs based on the annotated time-sensitive facts. Our dataset\nposes two novel challenges: 1) the model needs to understand both explicit and\nimplicit mention of time information in the long document, 2) the model needs\nto perform temporal reasoning like comparison, addition, subtraction. We\nevaluate different SoTA long-document QA systems like BigBird and FiD on our\ndataset. The best-performing model FiD can only achieve 46\\% accuracy, still\nfar behind the human performance of 87\\%. We demonstrate that these models are\nstill lacking the ability to perform robust temporal understanding and\nreasoning. Therefore, we believe that our dataset could serve as a benchmark to\nempower future studies in temporal reasoning. The dataset and code are released\nin~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.",
          "link": "http://arxiv.org/abs/2108.06314",
          "publishedOn": "2021-08-16T00:47:31.968Z",
          "wordCount": 673,
          "title": "A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadri_N/0/1/0/all/0/1\">Nima Sadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bihan Liu</a>",
          "description": "Creating abstractive summaries from meeting transcripts has proven to be\nchallenging due to the limited amount of labeled data available for training\nneural network models. Moreover, Transformer-based architectures have proven to\nbeat state-of-the-art models in summarizing news data. In this paper, we\nutilize a Transformer-based Pointer Generator Network to generate abstract\nsummaries for meeting transcripts. This model uses 2 LSTMs as an encoder and a\ndecoder, a Pointer network which copies words from the inputted text, and a\nGenerator network to produce out-of-vocabulary words (hence making the summary\nabstractive). Moreover, a coverage mechanism is used to avoid repetition of\nwords in the generated summary. First, we show that training the model on a\nnews summary dataset and using zero-shot learning to test it on the meeting\ndataset proves to produce better results than training it on the AMI meeting\ndataset. Second, we show that training this model first on out-of-domain data,\nsuch as the CNN-Dailymail dataset, followed by a fine-tuning stage on the AMI\nmeeting dataset is able to improve the performance of the model significantly.\nWe test our model on a testing set from the AMI dataset and report the ROUGE-2\nscore of the generated summary to compare with previous literature. We also\nreport the Factual score of our summaries since it is a better benchmark for\nabstractive summaries since the ROUGE-2 score is limited to measuring\nword-overlaps. We show that our improved model is able to improve on previous\nmodels by at least 5 ROUGE-2 scores, which is a substantial improvement. Also,\na qualitative analysis of the summaries generated by our model shows that these\nsummaries and human-readable and indeed capture most of the important\ninformation from the transcripts.",
          "link": "http://arxiv.org/abs/2108.06310",
          "publishedOn": "2021-08-16T00:47:31.945Z",
          "wordCount": 712,
          "title": "MeetSum: Transforming Meeting Transcript Summarization using Transformers!. (arXiv:2108.06310v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dietrich_A/0/1/0/all/0/1\">Anastasia Dietrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gressmann_F/0/1/0/all/0/1\">Frithjof Gressmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_D/0/1/0/all/0/1\">Douglas Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelombiev_I/0/1/0/all/0/1\">Ivan Chelombiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justus_D/0/1/0/all/0/1\">Daniel Justus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>",
          "description": "Identifying algorithms for computational efficient unsupervised training of\nlarge language models is an important and active area of research. In this\nwork, we develop and study a straightforward, dynamic always-sparse\npre-training approach for BERT language modeling task, which leverages periodic\ncompression steps based on magnitude pruning followed by random parameter\nre-allocation. This approach enables us to achieve Pareto improvements in terms\nof the number of floating-point operations (FLOPs) over statically sparse and\ndense models across a broad spectrum of network sizes. Furthermore, we\ndemonstrate that training remains FLOP-efficient when using coarse-grained\nblock sparsity, making it particularly promising for efficient execution on\nmodern hardware accelerators.",
          "link": "http://arxiv.org/abs/2108.06277",
          "publishedOn": "2021-08-16T00:47:31.934Z",
          "wordCount": 541,
          "title": "Towards Structured Dynamic Sparse Pre-Training of BERT. (arXiv:2108.06277v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gizinski_S/0/1/0/all/0/1\">Stanis&#x142;aw Gizinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzba_M/0/1/0/all/0/1\">Micha&#x142; Kuzba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielinski_B/0/1/0/all/0/1\">Bartosz Pielinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sienkiewicz_J/0/1/0/all/0/1\">Julian Sienkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laniewski_S/0/1/0/all/0/1\">Stanis&#x142;aw &#x141;aniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1\">Przemys&#x142;aw Biecek</a>",
          "description": "The growing number of AI applications, also for high-stake decisions,\nincreases the interest in Explainable and Interpretable Machine Learning\n(XI-ML). This trend can be seen both in the increasing number of regulations\nand strategies for developing trustworthy AI and the growing number of\nscientific papers dedicated to this topic. To ensure the sustainable\ndevelopment of AI, it is essential to understand the dynamics of the impact of\nregulation on research papers as well as the impact of scientific discourse on\nAI-related policies. This paper introduces a novel framework for joint analysis\nof AI-related policy documents and eXplainable Artificial Intelligence (XAI)\nresearch papers. The collected documents are enriched with metadata and\ninterconnections, using various NLP methods combined with a methodology\ninspired by Institutional Grammar. Based on the information extracted from\ncollected documents, we showcase a series of analyses that help understand\ninteractions, similarities, and differences between documents at different\nstages of institutionalization. To the best of our knowledge, this is the first\nwork to use automatic language analysis tools to understand the dynamics\nbetween XI-ML methods and regulations. We believe that such a system\ncontributes to better cooperation between XAI researchers and AI policymakers.",
          "link": "http://arxiv.org/abs/2108.06216",
          "publishedOn": "2021-08-16T00:47:31.920Z",
          "wordCount": 662,
          "title": "MAIR: Framework for mining relationships between research articles, strategies, and regulations in the field of explainable artificial intelligence. (arXiv:2108.06216v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gerhard_Young_G/0/1/0/all/0/1\">Greyson Gerhard-Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantha_R/0/1/0/all/0/1\">Raviteja Anantha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chappidi_S/0/1/0/all/0/1\">Srinivas Chappidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmeister_B/0/1/0/all/0/1\">Bj&#xf6;rn Hoffmeister</a>",
          "description": "Recent work building open-domain chatbots has demonstrated that increasing\nmodel size improves performance. On the other hand, latency and connectivity\nconsiderations dictate the move of digital assistants on the device. Giving a\ndigital assistant like Siri, Alexa, or Google Assistant the ability to discuss\njust about anything leads to the need for reducing the chatbot model size such\nthat it fits on the user's device. We demonstrate that low parameter models can\nsimultaneously retain their general knowledge conversational abilities while\nimproving in a specific domain. Additionally, we propose a generic framework\nthat accounts for variety in question types, tracks reference throughout\nmulti-turn conversations, and removes inconsistent and potentially toxic\nresponses. Our framework seamlessly transitions between chatting and performing\ntransactional tasks, which will ultimately make interactions with digital\nassistants more human-like. We evaluate our framework on 1 internal and 4\npublic benchmark datasets using both automatic (Perplexity) and human (SSA -\nSensibleness and Specificity Average) evaluation metrics and establish\ncomparable performance while reducing model parameters by 90%.",
          "link": "http://arxiv.org/abs/2108.06329",
          "publishedOn": "2021-08-16T00:47:31.901Z",
          "wordCount": 601,
          "title": "Low-Resource Adaptation of Open-Domain Generative Chatbots. (arXiv:2108.06329v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>",
          "description": "The advent of contextualised language models has brought gains in search\neffectiveness, not just when applied for re-ranking the output of classical\nweighting models such as BM25, but also when used directly for passage indexing\nand retrieval, a technique which is called dense retrieval. In the existing\nliterature in neural ranking, two dense retrieval families have become\napparent: single representation, where entire passages are represented by a\nsingle embedding (usually BERT's [CLS] token, as exemplified by the recent ANCE\napproach), or multiple representations, where each token in a passage is\nrepresented by its own embedding (as exemplified by the recent ColBERT\napproach). These two families have not been directly compared. However, because\nof the likely importance of dense retrieval moving forward, a clear\nunderstanding of their advantages and disadvantages is paramount. To this end,\nthis paper contributes a direct study on their comparative effectiveness,\nnoting situations where each method under/over performs w.r.t. each other, and\nw.r.t. a BM25 baseline. We observe that, while ANCE is more efficient than\nColBERT in terms of response time and memory usage, multiple representations\nare statistically more effective than the single representations for MAP and\nMRR@10. We also show that multiple representations obtain better improvements\nthan single representations for queries that are the hardest for BM25, as well\nas for definitional queries, and those with complex information needs.",
          "link": "http://arxiv.org/abs/2108.06279",
          "publishedOn": "2021-08-16T00:47:31.842Z",
          "wordCount": 668,
          "title": "On Single and Multiple Representations in Dense Passage Retrieval. (arXiv:2108.06279v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolova_M/0/1/0/all/0/1\">Marina Sokolova</a>",
          "description": "Reddit.com is a popular social media platform among young people. Reddit\nusers share their stories to seek support from other users, especially during\nthe Covid-19 pandemic. Messages posted on Reddit and their content have\nprovided researchers with opportunity to analyze public concerns. In this\nstudy, we analyzed sentiments of COVID-related messages posted on r/Depression.\nOur study poses the following questions: a) What are the common topics that the\nReddit users discuss? b) Can we use these topics to classify sentiments of the\nposts? c) What matters concern people more during the pandemic?\n\nKey Words: Sentiment Classification, Depression, COVID-19, Reddit, LDA, BERT",
          "link": "http://arxiv.org/abs/2108.06215",
          "publishedOn": "2021-08-16T00:47:31.787Z",
          "wordCount": 598,
          "title": "Sentiment Analysis of the COVID-related r/Depression Posts. (arXiv:2108.06215v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_P/0/1/0/all/0/1\">Prithiviraj Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhkaran Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achankuju_J/0/1/0/all/0/1\">Josemon Achankuju</a>",
          "description": "We present VESPA, an intentionally simple yet novel zero-shot system for\nlayout, locale, and domain agnostic document extraction. In spite of the\navailability of large corpora of documents, the lack of labeled and validated\ndatasets makes it a challenge to discriminatively train document extraction\nmodels for enterprises. We show that this problem can be addressed by simply\ntransferring the information extraction (IE) task to a natural language\nQuestion-Answering (QA) task without engineering task-specific architectures.\nWe demonstrate the effectiveness of our system by evaluating on a closed corpus\nof real-world retail and tax invoices with multiple complex layouts, domains,\nand geographies. The empirical evaluation shows that our system outperforms 4\nprominent commercial invoice solutions that use discriminatively trained models\nwith architectures specifically crafted for invoice extraction. We extracted 6\nfields with zero upfront human annotation or training with an Avg. F1 of 87.50.",
          "link": "http://arxiv.org/abs/2108.06069",
          "publishedOn": "2021-08-16T00:47:31.761Z",
          "wordCount": 585,
          "title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA Ensemble. (arXiv:2108.06069v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1\">Tapas Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting triplets\nof aspect terms, their associated sentiments, and the opinion terms that\nprovide evidence for the expressed sentiments. Previous approaches to ASTE\nusually simultaneously extract all three components or first identify the\naspect and opinion terms, then pair them up to predict their sentiment\npolarities. In this work, we present a novel paradigm, ASTE-RL, by regarding\nthe aspect and opinion terms as arguments of the expressed sentiment in a\nhierarchical reinforcement learning (RL) framework. We first focus on\nsentiments expressed in a sentence, then identify the target aspect and opinion\nterms for that sentiment. This takes into account the mutual interactions among\nthe triplet's components while improving exploration and sample efficiency.\nFurthermore, this hierarchical RLsetup enables us to deal with multiple and\noverlapping triplets. In our experiments, we evaluate our model on existing\ndatasets from laptop and restaurant domains and show that it achieves\nstate-of-the-art performance. The implementation of this work is publicly\navailable at https://github.com/declare-lab/ASTE-RL.",
          "link": "http://arxiv.org/abs/2108.06107",
          "publishedOn": "2021-08-16T00:47:31.734Z",
          "wordCount": 606,
          "title": "Aspect Sentiment Triplet Extraction Using Reinforcement Learning. (arXiv:2108.06107v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>",
          "description": "Both latent semantic analysis (LSA) and correspondence analysis (CA) use a\nsingular value decomposition (SVD) for dimensionality reduction. In this\narticle, LSA and CA are compared from a theoretical point of view and applied\nin both a toy example and an authorship attribution example. In text mining\ninterest goes out to the relationships among documents and terms: for example,\nwhat terms are more often used in what documents. However, the LSA solution\ndisplays a mix of marginal effects and these relationships. It appears that CA\nhas more attractive properties than LSA. One such property is that, in CA, the\neffect of the margins is effectively eliminated, so that the CA solution is\noptimally suited to focus on the relationships among documents and terms. Three\nmechanisms are distinguished to weight documents and terms, and a unifying\nframework is proposed that includes these three mechanisms and includes both CA\nand LSA as special cases. In the authorship attribution example, the national\nanthem of the Netherlands, the application of the discussed methods is\nillustrated.",
          "link": "http://arxiv.org/abs/2108.06197",
          "publishedOn": "2021-08-16T00:47:31.709Z",
          "wordCount": 621,
          "title": "A Comparison of Latent Semantic Analysis and Correspondence Analysis for Text Mining. (arXiv:2108.06197v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutsch_J/0/1/0/all/0/1\">Julian Gutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietsch_M/0/1/0/all/0/1\">Malte Pietsch</a>",
          "description": "The evaluation of question answering models compares ground-truth annotations\nwith model predictions. However, as of today, this comparison is mostly\nlexical-based and therefore misses out on answers that have no lexical overlap\nbut are still semantically similar, thus treating correct answers as false.\nThis underestimation of the true performance of models hinders user acceptance\nin applications and complicates a fair comparison of different models.\nTherefore, there is a need for an evaluation metric that is based on semantics\ninstead of pure string similarity. In this short paper, we present SAS, a\ncross-encoder-based metric for the estimation of semantic answer similarity,\nand compare it to seven existing metrics. To this end, we create an English and\na German three-way annotated evaluation dataset containing pairs of answers\nalong with human judgment of their semantic similarity, which we release along\nwith an implementation of the SAS metric and the experiments. We find that\nsemantic similarity metrics based on recent transformer models correlate much\nbetter with human judgment than traditional lexical similarity metrics on our\ntwo newly created datasets and one dataset from related work.",
          "link": "http://arxiv.org/abs/2108.06130",
          "publishedOn": "2021-08-16T00:47:31.701Z",
          "wordCount": 622,
          "title": "Semantic Answer Similarity for Evaluating Question Answering Models. (arXiv:2108.06130v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>",
          "description": "Hateful and offensive content detection has been extensively explored in a\nsingle modality such as text. However, such toxic information could also be\ncommunicated via multimodal content such as online memes. Therefore, detecting\nmultimodal hateful content has recently garnered much attention in academic and\nindustry research communities. This paper aims to contribute to this emerging\nresearch topic by proposing DisMultiHate, which is a novel framework that\nperformed the classification of multimodal hateful content. Specifically,\nDisMultiHate is designed to disentangle target entities in multimodal memes to\nimprove hateful content classification and explainability. We conduct extensive\nexperiments on two publicly available hateful and offensive memes datasets. Our\nexperiment results show that DisMultiHate is able to outperform\nstate-of-the-art unimodal and multimodal baselines in the hateful meme\nclassification task. Empirical case studies were also conducted to demonstrate\nDisMultiHate's ability to disentangle target entities in memes and ultimately\nshowcase DisMultiHate's explainability of the multimodal hateful content\nclassification task.",
          "link": "http://arxiv.org/abs/2108.06207",
          "publishedOn": "2021-08-16T00:47:31.670Z",
          "wordCount": 597,
          "title": "Disentangling Hate in Online Memes. (arXiv:2108.06207v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shangwen Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">QiaoQiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Recently, dense passage retrieval has become a mainstream approach to finding\nrelevant information in various natural language processing tasks. A number of\nstudies have been devoted to improving the widely adopted dual-encoder\narchitecture. However, most of the previous studies only consider query-centric\nsimilarity relation when learning the dual-encoder retriever. In order to\ncapture more comprehensive similarity relations, we propose a novel approach\nthat leverages both query-centric and PAssage-centric sImilarity Relations\n(called PAIR) for dense passage retrieval. To implement our approach, we make\nthree major technical contributions by introducing formal formulations of the\ntwo kinds of similarity relations, generating high-quality pseudo labeled data\nvia knowledge distillation, and designing an effective two-stage training\nprocedure that incorporates passage-centric similarity relation constraint.\nExtensive experiments show that our approach significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions datasets.",
          "link": "http://arxiv.org/abs/2108.06027",
          "publishedOn": "2021-08-16T00:47:31.565Z",
          "wordCount": 595,
          "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>",
          "description": "Reasoning on the knowledge graph (KG) aims to infer new facts from existing\nones. Methods based on the relational path in the literature have shown strong,\ninterpretable, and inductive reasoning ability. However, the paths are\nnaturally limited in capturing complex topology in KG. In this paper, we\nintroduce a novel relational structure, i.e., relational directed graph\n(r-digraph), which is composed of overlapped relational paths, to capture the\nKG's structural information. Since the digraph exhibits more complex structure\nthan paths, constructing and learning on the r-digraph are challenging. Here,\nwe propose a variant of graph neural network, i.e., RED-GNN, to address the\nabove challenges by learning the RElational Digraph with a variant of GNN.\nSpecifically, RED-GNN recursively encodes multiple r-digraphs with shared edges\nand selects the strongly correlated edges through query-dependent attention\nweights. We demonstrate the significant gains on reasoning both KG with unseen\nentities and incompletion KG benchmarks by the r-digraph, the efficiency of\nRED-GNN, and the interpretable dependencies learned on the r-digraph.",
          "link": "http://arxiv.org/abs/2108.06040",
          "publishedOn": "2021-08-16T00:47:31.552Z",
          "wordCount": 591,
          "title": "Knowledge Graph Reasoning with Relational Directed Graph. (arXiv:2108.06040v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>",
          "description": "Ranking models have achieved promising results, but it remains challenging to\ndesign personalized ranking systems to leverage user profiles and semantic\nrepresentations between queries and documents. In this paper, we propose a\ntopic-based personalized ranking model (TPRM) that integrates user topical\nprofile with pretrained contextualized term representations to tailor the\ngeneral document ranking list. Experiments on the real-world dataset\ndemonstrate that TPRM outperforms state-of-the-art ad-hoc ranking models and\npersonalized ranking models significantly.",
          "link": "http://arxiv.org/abs/2108.06014",
          "publishedOn": "2021-08-16T00:47:31.535Z",
          "wordCount": 511,
          "title": "TPRM: A Topic-based Personalized Ranking Model for Web Search. (arXiv:2108.06014v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandla_T/0/1/0/all/0/1\">Thomas Mandla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modha_S/0/1/0/all/0/1\">Sandip Modha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Amit Kumar Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandini_D/0/1/0/all/0/1\">Durgesh Nandini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Daksh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schafer_J/0/1/0/all/0/1\">Johannes Sch&#xe4;fer</a>",
          "description": "With the growth of social media, the spread of hate speech is also increasing\nrapidly. Social media are widely used in many countries. Also Hate Speech is\nspreading in these countries. This brings a need for multilingual Hate Speech\ndetection algorithms. Much research in this area is dedicated to English at the\nmoment. The HASOC track intends to provide a platform to develop and optimize\nHate Speech detection algorithms for Hindi, German and English. The dataset is\ncollected from a Twitter archive and pre-classified by a machine learning\nsystem. HASOC has two sub-task for all three languages: task A is a binary\nclassification problem (Hate and Not Offensive) while task B is a fine-grained\nclassification problem for three classes (HATE) Hate speech, OFFENSIVE and\nPROFANITY. Overall, 252 runs were submitted by 40 teams. The performance of the\nbest classification algorithms for task A are F1 measures of 0.51, 0.53 and\n0.52 for English, Hindi, and German, respectively. For task B, the best\nclassification algorithms achieved F1 measures of 0.26, 0.33 and 0.29 for\nEnglish, Hindi, and German, respectively. This article presents the tasks and\nthe data development as well as the results. The best performing algorithms\nwere mainly variants of the transformer architecture BERT. However, also other\nsystems were applied with good success",
          "link": "http://arxiv.org/abs/2108.05927",
          "publishedOn": "2021-08-16T00:47:31.505Z",
          "wordCount": 680,
          "title": "Overview of the HASOC track at FIRE 2020: Hate Speech and Offensive Content Identification in Indo-European Languages. (arXiv:2108.05927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Meizhen Ding</a>",
          "description": "Query expansion with pseudo-relevance feedback (PRF) is a powerful approach\nto enhance the effectiveness in information retrieval. Recently, with the rapid\nadvance of deep learning techniques, neural text generation has achieved\npromising success in many natural language tasks. To leverage the strength of\ntext generation for information retrieval, in this article, we propose a novel\napproach which effectively integrates text generation models into PRF-based\nquery expansion. In particular, our approach generates augmented query terms\nvia neural text generation models conditioned on both the initial query and\npseudo-relevance feedback. Moreover, in order to train the generative model, we\nadopt the conditional generative adversarial nets (CGANs) and propose the\nPRF-CGAN method in which both the generator and the discriminator are\nconditioned on the pseudo-relevance feedback. We evaluate the performance of\nour approach on information retrieval tasks using two benchmark datasets. The\nexperimental results show that our approach achieves comparable performance or\noutperforms traditional query expansion methods on both the retrieval and\nreranking tasks.",
          "link": "http://arxiv.org/abs/2108.06010",
          "publishedOn": "2021-08-16T00:47:31.397Z",
          "wordCount": 598,
          "title": "GQE-PRF: Generative Query Expansion with Pseudo-Relevance Feedback. (arXiv:2108.06010v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertram Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>",
          "description": "Detecting online hate is a complex task, and low-performing detection models\nhave harmful consequences when used for sensitive applications such as content\nmoderation. Emoji-based hate is a key emerging challenge for online hate\ndetection. We present HatemojiCheck, a test suite of 3,930 short-form\nstatements that allows us to evaluate how detection models perform on hateful\nlanguage expressed with emoji. Using the test suite, we expose weaknesses in\nexisting hate detection models. To address these weaknesses, we create the\nHatemojiTrain dataset using an innovative human-and-model-in-the-loop approach.\nModels trained on these 5,912 adversarial examples perform substantially better\nat detecting emoji-based hate, while retaining strong performance on text-only\nhate. Both HatemojiCheck and HatemojiTrain are made publicly available.",
          "link": "http://arxiv.org/abs/2108.05921",
          "publishedOn": "2021-08-16T00:47:31.347Z",
          "wordCount": 564,
          "title": "Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (arXiv:2108.05921v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.03599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongyin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingwu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Beihong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>",
          "description": "Recently, the retrieval models based on dense representations have been\ngradually applied in the first stage of the document retrieval tasks, showing\nbetter performance than traditional sparse vector space models. To obtain high\nefficiency, the basic structure of these models is Bi-encoder in most cases.\nHowever, this simple structure may cause serious information loss during the\nencoding of documents since the queries are agnostic. To address this problem,\nwe design a method to mimic the queries on each of the documents by an\niterative clustering process and represent the documents by multiple pseudo\nqueries (i.e., the cluster centroids). To boost the retrieval process using\napproximate nearest neighbor search library, we also optimize the matching\nfunction with a two-step score calculation procedure. Experimental results on\nseveral popular ranking and QA datasets show that our model can achieve\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.03599",
          "publishedOn": "2021-08-20T01:53:49.470Z",
          "wordCount": 619,
          "title": "Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval. (arXiv:2105.03599v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>",
          "description": "We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual\nretrieval in eleven typologically diverse languages, designed to evaluate\nranking with learned dense representations. The goal of this resource is to\nspur research in dense retrieval techniques in non-English languages, motivated\nby recent observations that existing techniques for representation learning\nperform poorly when applied to out-of-distribution data. As a starting point,\nwe provide zero-shot baselines for this new dataset based on a multi-lingual\nadaptation of DPR that we call \"mDPR\". Experiments show that although the\neffectiveness of mDPR is much lower than BM25, dense representations\nnevertheless appear to provide valuable relevance signals, improving BM25\nresults in sparse-dense hybrids. In addition to analyses of our results, we\nalso discuss future challenges and present a research agenda in multi-lingual\ndense retrieval. Mr. TyDi can be downloaded at\nhttps://github.com/castorini/mr.tydi.",
          "link": "http://arxiv.org/abs/2108.08787",
          "publishedOn": "2021-08-20T01:53:49.443Z",
          "wordCount": 574,
          "title": "Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval. (arXiv:2108.08787v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1\">Changwon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_K/0/1/0/all/0/1\">Kyeong-Joong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sungsu Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Won-Yong Shin</a>",
          "description": "In recent years, many recommender systems using network embedding (NE) such\nas graph neural networks (GNNs) have been extensively studied in the sense of\nimproving recommendation accuracy. However, such attempts have focused mostly\non utilizing only the information of positive user-item interactions with high\nratings. Thus, there is a challenge on how to make use of low rating scores for\nrepresenting users' preferences since low ratings can be still informative in\ndesigning NE-based recommender systems. In this study, we present SiReN, a new\nsign-aware recommender system based on GNN models. Specifically, SiReN has\nthree key components: 1) constructing a signed bipartite graph for more\nprecisely representing users' preferences, which is split into two\nedge-disjoint graphs with positive and negative edges each, 2) generating two\nembeddings for the partitioned graphs with positive and negative edges via a\nGNN model and a multi-layer perceptron (MLP), respectively, and then using an\nattention model to obtain the final embeddings, and 3) establishing a\nsign-aware Bayesian personalized ranking (BPR) loss function in the process of\noptimization. Through comprehensive experiments, we empirically demonstrate\nthat SiReN consistently outperforms state-of-the-art NE-aided recommendation\nmethods.",
          "link": "http://arxiv.org/abs/2108.08735",
          "publishedOn": "2021-08-20T01:53:49.411Z",
          "wordCount": 640,
          "title": "SiReN: Sign-Aware Recommendation Using Graph Neural Networks. (arXiv:2108.08735v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-08-20T01:53:49.398Z",
          "wordCount": 751,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but barely utilize semantic\ndata and knowledge. This paper presents the first QA system that can seamlessly\noperate over RDF datasets and text corpora, or both together, in a unified\nframework. Our method, called UNIQORN, builds a context graph on the fly, by\nretrieving question-relevant triples from the RDF data and/or the text corpus,\nwhere the latter case is handled by automatic information extraction. The\nresulting graph is typically rich but highly noisy. UNIQORN copes with this\ninput by advanced graph algorithms for Group Steiner Trees, that identify the\nbest answer candidates in the context graph. Experimental results on several\nbenchmarks of complex questions with multiple entities and relations, show that\nUNIQORN, an unsupervised method with only five parameters, produces results\ncomparable to the state-of-the-art on KGs, text corpora, and heterogeneous\nsources. The graph-based methodology provides user-interpretable evidence for\nthe complete answering process.",
          "link": "http://arxiv.org/abs/2108.08614",
          "publishedOn": "2021-08-20T01:53:49.380Z",
          "wordCount": 654,
          "title": "UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vardasbi_A/0/1/0/all/0/1\">Ali Vardasbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1\">Ilya Markov</a>",
          "description": "In counterfactual learning to rank (CLTR) user interactions are used as a\nsource of supervision. Since user interactions come with bias, an important\nfocus of research in this field lies in developing methods to correct for the\nbias of interactions. Inverse propensity scoring (IPS) is a popular method\nsuitable for correcting position bias. Affine correction (AC) is a\ngeneralization of IPS that corrects for position bias and trust bias. IPS and\nAC provably remove bias, conditioned on an accurate estimation of the bias\nparameters. Estimating the bias parameters, in turn, requires an accurate\nestimation of the relevance probabilities. This cyclic dependency introduces\npractical limitations in terms of sensitivity, convergence and efficiency.\n\nWe propose a new correction method for position and trust bias in CLTR in\nwhich, unlike the existing methods, the correction does not rely on relevance\nestimation. Our proposed method, mixture-based correction (MBC), is based on\nthe assumption that the distribution of the CTRs over the items being ranked is\na mixture of two distributions: the distribution of CTRs for relevant items and\nthe distribution of CTRs for non-relevant items. We prove that our method is\nunbiased. The validity of our proof is not conditioned on accurate bias\nparameter estimation. Our experiments show that MBC, when used in different\nbias settings and accompanied by different LTR algorithms, outperforms AC, the\nstate-of-the-art method for correcting position and trust bias, in some\nsettings, while performing on par in other settings. Furthermore, MBC is orders\nof magnitude more efficient than AC in terms of the training time.",
          "link": "http://arxiv.org/abs/2108.08538",
          "publishedOn": "2021-08-20T01:53:49.360Z",
          "wordCount": 697,
          "title": "Mixture-Based Correction for Position and Trust Bias in Counterfactual Learning to Rank. (arXiv:2108.08538v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.",
          "link": "http://arxiv.org/abs/2108.08468",
          "publishedOn": "2021-08-20T01:53:49.335Z",
          "wordCount": 701,
          "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique is to apply named entity disambiguation (NED)\nsystems to the question, and retrieve KB facts for the disambiguated entities.\nThis work presents ECQA, an efficient method that prunes irrelevant parts of\nthe search space using KB-aware signals. ECQA is based on top-k query\nprocessing over score-ordered lists of KB items that combine signals about\nlexical matching, relevance to the question, coherence among candidate items,\nand connectivity in the KB graph. Experiments with two recent QA benchmarks\ndemonstrate the superiority of ECQA over state-of-the-art baselines with\nrespect to answer presence, size of the search space, and runtimes.",
          "link": "http://arxiv.org/abs/2108.08597",
          "publishedOn": "2021-08-20T01:53:49.296Z",
          "wordCount": 590,
          "title": "Efficient Contextualization using Top-k Operators for Question Answering over Knowledge Graphs. (arXiv:2108.08597v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>",
          "description": "BERT-based information retrieval models are expensive, in both time (query\nlatency) and computational resources (energy, hardware cost), making many of\nthese models impractical especially under resource constraints. The reliance on\na query encoder that only performs tokenization and on the pre-processing of\npassage representations at indexing, has allowed the recently proposed TILDE\nmethod to overcome the high query latency issue typical of BERT-based models.\nThis however is at the expense of a lower effectiveness compared to other\nBERT-based re-rankers and dense retrievers. In addition, the original TILDE\nmethod is characterised by indexes with a very high memory footprint, as it\nexpands each passage into the size of the BERT vocabulary. In this paper, we\npropose TILDEv2, a new model that stems from the original TILDE but that\naddresses its limitations. TILDEv2 relies on contextualized exact term matching\nwith expanded passages. This requires to only store in the index the score of\ntokens that appear in the expanded passages (rather than all the vocabulary),\nthus producing indexes that are 99% smaller than those of TILDE. This matching\nmechanism also improves ranking effectiveness by 24%, without adding to the\nquery latency. This makes TILDEv2 the state-of-the-art passage re-ranking\nmethod for CPU-only environments, capable of maintaining query latency below\n100ms on commodity hardware.",
          "link": "http://arxiv.org/abs/2108.08513",
          "publishedOn": "2021-08-20T01:53:49.239Z",
          "wordCount": 646,
          "title": "Fast Passage Re-ranking with Contextualized Exact Term Matching and Efficient Passage Expansion. (arXiv:2108.08513v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper presents a three-tier modality alignment approach to learning\ntext-image joint embedding, coined as JEMA, for cross-modal retrieval of\ncooking recipes and food images. The first tier improves recipe text embedding\nby optimizing the LSTM networks with term extraction and ranking enhanced\nsequence patterns, and optimizes the image embedding by combining the\nResNeXt-101 image encoder with the category embedding using wideResNet-50 with\nword2vec. The second tier modality alignment optimizes the textual-visual joint\nembedding loss function using a double batch-hard triplet loss with soft-margin\noptimization. The third modality alignment incorporates two types of\ncross-modality alignments as the auxiliary loss regularizations to further\nreduce the alignment errors in the joint learning of the two modality-specific\nembedding functions. The category-based cross-modal alignment aims to align the\nimage category with the recipe category as a loss regularization to the joint\nembedding. The cross-modal discriminator-based alignment aims to add the\nvisual-textual embedding distribution alignment to further regularize the joint\nembedding loss. Extensive experiments with the one-million recipes benchmark\ndataset Recipe1M demonstrate that the proposed JEMA approach outperforms the\nstate-of-the-art cross-modal embedding methods for both image-to-recipe and\nrecipe-to-image retrievals.",
          "link": "http://arxiv.org/abs/2108.03788",
          "publishedOn": "2021-08-19T01:34:59.170Z",
          "wordCount": 668,
          "title": "Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parisi_L/0/1/0/all/0/1\">Luca Parisi</a>",
          "description": "This study presents the m-arcsinh Kernel ('m-ar-K') Fast Independent\nComponent Analysis ('FastICA') method ('m-ar-K-FastICA') for feature\nextraction. The kernel trick has enabled dimensionality reduction techniques to\ncapture a higher extent of non-linearity in the data; however, reproducible,\nopen-source kernels to aid with feature extraction are still limited and may\nnot be reliable when projecting features from entropic data. The m-ar-K\nfunction, freely available in Python and compatible with its open-source\nlibrary 'scikit-learn', is hereby coupled with FastICA to achieve more reliable\nfeature extraction in presence of a high extent of randomness in the data,\nreducing the need for pre-whitening. Different classification tasks were\nconsidered, as related to five (N = 5) open access datasets of various degrees\nof information entropy, available from scikit-learn and the University\nCalifornia Irvine (UCI) Machine Learning repository. Experimental results\ndemonstrate improvements in the classification performance brought by the\nproposed feature extraction. The novel m-ar-K-FastICA dimensionality reduction\napproach is compared to the 'FastICA' gold standard method, supporting its\nhigher reliability and computational efficiency, regardless of the underlying\nuncertainty in the data.",
          "link": "http://arxiv.org/abs/2108.07908",
          "publishedOn": "2021-08-19T01:34:59.148Z",
          "wordCount": 654,
          "title": "M-ar-K-Fast Independent Component Analysis. (arXiv:2108.07908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shaunak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Changwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Manisha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1\">Maxim Sviridenko</a>",
          "description": "Coming up with effective ad text is a time consuming process, and\nparticularly challenging for small businesses with limited advertising\nexperience. When an inexperienced advertiser onboards with a poorly written ad\ntext, the ad platform has the opportunity to detect low performing ad text, and\nprovide improvement suggestions. To realize this opportunity, we propose an ad\ntext strength indicator (TSI) which: (i) predicts the click-through-rate (CTR)\nfor an input ad text, (ii) fetches similar existing ads to create a\nneighborhood around the input ad, (iii) and compares the predicted CTRs in the\nneighborhood to declare whether the input ad is strong or weak. In addition, as\nsuggestions for ad text improvement, TSI shows anonymized versions of superior\nads (higher predicted CTR) in the neighborhood. For (i), we propose a BERT\nbased text-to-CTR model trained on impressions and clicks associated with an ad\ntext. For (ii), we propose a sentence-BERT based semantic-ad-similarity model\ntrained using weak labels from ad campaign setup data. Offline experiments\ndemonstrate that our BERT based text-to-CTR model achieves a significant lift\nin CTR prediction AUC for cold start (new) advertisers compared to bag-of-words\nbased baselines. In addition, our semantic-textual-similarity model for similar\nads retrieval achieves a precision@1 of 0.93 (for retrieving ads from the same\nproduct category); this is significantly higher compared to unsupervised\nTF-IDF, word2vec, and sentence-BERT baselines. Finally, we share promising\nonline results from advertisers in the Yahoo (Verizon Media) ad platform where\na variant of TSI was implemented with sub-second end-to-end latency.",
          "link": "http://arxiv.org/abs/2108.08226",
          "publishedOn": "2021-08-19T01:34:59.096Z",
          "wordCount": 700,
          "title": "TSI: an Ad Text Strength Indicator using Text-to-CTR and Semantic-Ad-Similarity. (arXiv:2108.08226v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rusnak_L/0/1/0/all/0/1\">Lucas Rusnak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tesic_J/0/1/0/all/0/1\">Jelena Te&#x161;i&#x107;</a>",
          "description": "Attitudinal Network Graphs are signed graphs where edges capture an expressed\nopinion; two vertices connected by an edge can be agreeable (positive) or\nantagonistic (negative). A signed graph is called balanced if each of its\ncycles includes an even number of negative edges. Balance is often\ncharacterized by the frustration index or by finding a single convergent\nbalanced state of network consensus. In this paper, we propose to expand the\nmeasures of consensus from a single balanced state associated with the\nfrustration index to the set of nearest balanced states. We introduce the\nfrustration cloud as a set of all nearest balanced states and use a\ngraph-balancing algorithm to find all nearest balanced states in a\ndeterministic way. Computational concerns are addressed by measuring consensus\nprobabilistically, and we introduce new vertex and edge metrics to quantify\nstatus, agreement, and influence. We also introduce a new global measure of\ncontroversy for a given signed graph and show that vertex status is a zero-sum\ngame in the signed network. We propose an efficient scalable algorithm for\ncalculating frustration cloud-based measures in social network and survey data\nof up to 80,000 vertices and half-a-million edges. We also demonstrate the\npower of the proposed approach to provide discriminant features for community\ndiscovery when compared to spectral clustering and to automatically identify\ndominant vertices and anomalous decisions in the network.",
          "link": "http://arxiv.org/abs/2009.07776",
          "publishedOn": "2021-08-19T01:34:59.058Z",
          "wordCount": 706,
          "title": "Characterizing Attitudinal Network Graphs through Frustration Cloud. (arXiv:2009.07776v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Badita_A/0/1/0/all/0/1\">Ajay Badita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jinan_R/0/1/0/all/0/1\">Rooji Jinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamanan_B/0/1/0/all/0/1\">Balajee Vamanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parag_P/0/1/0/all/0/1\">Parimal Parag</a>",
          "description": "We consider energy minimization for data-intensive applications run on large\nnumber of servers, for given performance guarantees. We consider a system,\nwhere each incoming application is sent to a set of servers, and is considered\nto be completed if a subset of them finish serving it. We consider a simple\ncase when each server core has two speed levels, where the higher speed can be\nachieved by higher power for each core independently. The core selects one of\nthe two speeds probabilistically for each incoming application request. We\nmodel arrival of application requests by a Poisson process, and random service\ntime at the server with independent exponential random variables. Our model and\nanalysis generalizes to today's state-of-the-art in CPU energy management where\neach core can independently select a speed level from a set of supported speeds\nand corresponding voltages. The performance metrics under consideration are the\nmean number of applications in the system and the average energy expenditure.\nWe first provide a tight approximation to study this previously intractable\nproblem and derive closed form approximate expressions for the performance\nmetrics when service times are exponentially distributed. Next, we study the\ntrade-off between the approximate mean number of applications and energy\nexpenditure in terms of the switching probability.",
          "link": "http://arxiv.org/abs/2108.08199",
          "publishedOn": "2021-08-19T01:34:59.025Z",
          "wordCount": 648,
          "title": "Modeling Performance and Energy trade-offs in Online Data-Intensive Applications. (arXiv:2108.08199v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hongjin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yueyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "In this paper, we explore the problem of developing personalized chatbots. A\npersonalized chatbot is designed as a digital chatting assistant for a user.\nThe key characteristic of a personalized chatbot is that it should have a\nconsistent personality with the corresponding user. It can talk the same way as\nthe user when it is delegated to respond to others' messages. We present a\nretrieval-based personalized chatbot model, namely IMPChat, to learn an\nimplicit user profile from the user's dialogue history. We argue that the\nimplicit user profile is superior to the explicit user profile regarding\naccessibility and flexibility. IMPChat aims to learn an implicit user profile\nthrough modeling user's personalized language style and personalized\npreferences separately. To learn a user's personalized language style, we\nelaborately build language models from shallow to deep using the user's\nhistorical responses; To model a user's personalized preferences, we explore\nthe conditional relations underneath each post-response pair of the user. The\npersonalized preferences are dynamic and context-aware: we assign higher\nweights to those historical pairs that are topically related to the current\nquery when aggregating the personalized preferences. We match each response\ncandidate with the personalized language style and personalized preference,\nrespectively, and fuse the two matching signals to determine the final ranking\nscore. Comprehensive experiments on two large datasets show that our method\noutperforms all baseline models.",
          "link": "http://arxiv.org/abs/2108.07935",
          "publishedOn": "2021-08-19T01:34:58.680Z",
          "wordCount": 682,
          "title": "Learning Implicit User Profiles for Personalized Retrieval-Based Chatbot. (arXiv:2108.07935v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaochao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Longfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanfeng Liu</a>",
          "description": "Cross-Domain Recommendation (CDR) and Cross-System Recommendation (CSR) have\nbeen proposed to improve the recommendation accuracy in a target dataset\n(domain/system) with the help of a source one with relatively richer\ninformation. However, most existing CDR and CSR approaches are single-target,\nnamely, there is a single target dataset, which can only help the target\ndataset and thus cannot benefit the source dataset. In this paper, we focus on\nthree new scenarios, i.e., Dual-Target CDR (DTCDR), Multi-Target CDR (MTCDR),\nand CDR+CSR, and aim to improve the recommendation accuracy in all datasets\nsimultaneously for all scenarios. To do this, we propose a unified framework,\ncalled GA (based on Graph embedding and Attention techniques), for all three\nscenarios. In GA, we first construct separate heterogeneous graphs to generate\nmore representative user and item embeddings. Then, we propose an element-wise\nattention mechanism to effectively combine the embeddings of common entities\n(users/items) learned from different datasets. Moreover, to avoid negative\ntransfer, we further propose a Personalized training strategy to minimize the\nembedding difference of common entities between a richer dataset and a sparser\ndataset, deriving three new models, i.e., GA-DTCDR-P, GA-MTCDR-P, and\nGA-CDR+CSR-P, for the three scenarios respectively. Extensive experiments\nconducted on four real-world datasets demonstrate that our proposed GA models\nsignificantly outperform the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.07976",
          "publishedOn": "2021-08-19T01:34:58.612Z",
          "wordCount": 676,
          "title": "A Unified Framework for Cross-Domain and Cross-System Recommendations. (arXiv:2108.07976v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianhui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>",
          "description": "Recent studies in recommender systems have managed to achieve significantly\nimproved performance by leveraging reviews for rating prediction. However,\ndespite being extensively studied, these methods still suffer from some\nlimitations. First, previous studies either encode the document or extract\nlatent sentiment via neural networks, which are difficult to interpret the\nsentiment of reviewers intuitively. Second, they neglect the personalized\ninteraction of reviews with user/item, i.e., each review has different\ncontributions when modeling the sentiment preference of user/item. To remedy\nthese issues, we propose a Sentiment-aware Interactive Fusion Network (SIFN)\nfor review-based item recommendation. Specifically, we first encode user/item\nreviews via BERT and propose a light-weighted sentiment learner to extract\nsemantic features of each review. Then, we propose a sentiment prediction task\nthat guides the sentiment learner to extract sentiment-aware features via\nexplicit sentiment labels. Finally, we design a rating prediction task that\ncontains a rating learner with an interactive and fusion module to fuse the\nidentity (i.e., user and item ID) and each review representation so that\nvarious interactive features can synergistically influence the final rating\nscore. Experimental results on five real-world datasets demonstrate that the\nproposed model is superior to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.08022",
          "publishedOn": "2021-08-19T01:34:58.596Z",
          "wordCount": 643,
          "title": "SIFN: A Sentiment-aware Interactive Fusion Network for Review-based Item Recommendation. (arXiv:2108.08022v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Neural text matching models have been widely used in community question\nanswering, information retrieval, and dialogue. However, these models designed\nfor short texts cannot well address the long-form text matching problem,\nbecause there are many contexts in long-form texts can not be directly aligned\nwith each other, and it is difficult for existing models to capture the key\nmatching signals from such noisy data. Besides, these models are\ncomputationally expensive for simply use all textual data indiscriminately. To\ntackle the effectiveness and efficiency problem, we propose a novel\nhierarchical noise filtering model, namely Match-Ignition. The main idea is to\nplug the well-known PageRank algorithm into the Transformer, to identify and\nfilter both sentence and word level noisy information in the matching process.\nNoisy sentences are usually easy to detect because previous work has shown that\ntheir similarity can be explicitly evaluated by the word overlapping, so we\ndirectly use PageRank to filter such information based on a sentence similarity\ngraph. Unlike sentences, words rely on their contexts to express concrete\nmeanings, so we propose to jointly learn the filtering and matching process, to\nwell capture the critical word-level matching signals. Specifically, a word\ngraph is first built based on the attention scores in each self-attention block\nof Transformer, and key words are then selected by applying PageRank on this\ngraph. In this way, noisy words will be filtered out layer by layer in the\nmatching process. Experimental results show that Match-Ignition outperforms\nboth SOTA short text matching models and recent long-form text matching models.\nWe also conduct detailed analysis to show that Match-Ignition efficiently\ncaptures important sentences and words, to facilitate the long-form text\nmatching process.",
          "link": "http://arxiv.org/abs/2101.06423",
          "publishedOn": "2021-08-18T01:54:58.691Z",
          "wordCount": 749,
          "title": "Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching. (arXiv:2101.06423v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Feng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>",
          "description": "Click-Through Rate (CTR) prediction, whose aim is to predict the probability\nof whether a user will click on an item, is an essential task for many online\napplications. Due to the nature of data sparsity and high dimensionality of CTR\nprediction, a key to making effective prediction is to model high-order feature\ninteraction. An efficient way to do this is to perform inner product of feature\nembeddings with self-attentive neural networks. To better model complex feature\ninteraction, in this paper we propose a novel DisentanglEd Self-atTentIve\nNEtwork (DESTINE) framework for CTR prediction that explicitly decouples the\ncomputation of unary feature importance from pairwise interaction.\nSpecifically, the unary term models the general importance of one feature on\nall other features, whereas the pairwise interaction term contributes to\nlearning the pure impact for each feature pair. We conduct extensive\nexperiments using two real-world benchmark datasets. The results show that\nDESTINE not only maintains computational efficiency but achieves consistent\nimprovements over state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2101.03654",
          "publishedOn": "2021-08-18T01:54:58.574Z",
          "wordCount": 640,
          "title": "Disentangled Self-Attentive Neural Networks for Click-Through Rate Prediction. (arXiv:2101.03654v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1\">Stefan Heid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.",
          "link": "http://arxiv.org/abs/2008.01377",
          "publishedOn": "2021-08-18T01:54:58.386Z",
          "wordCount": 718,
          "title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Saurav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayant_R/0/1/0/all/0/1\">Rushil Jayant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charagulla_N/0/1/0/all/0/1\">Nihaar Charagulla</a>",
          "description": "The popularization of the internet created a revitalized digital media. With\nmonetization driven by clicks, journalists have reprioritized their content for\nthe highly competitive atmosphere of online news. The resulting negativity bias\nis harmful and can lead to anxiety and mood disturbance. We utilized a pipeline\nof 4 sentiment analysis models trained on various datasets - using Sequential,\nLSTM, BERT, and SVM models. When combined, the application, a mobile app,\nsolely displays uplifting and inspiring stories for users to read. Results have\nbeen successful - 1,300 users rate the app at 4.9 stars, and 85% report\nimproved mental health by using it.",
          "link": "http://arxiv.org/abs/2108.07706",
          "publishedOn": "2021-08-18T01:54:58.353Z",
          "wordCount": 546,
          "title": "Sentiment Analysis on the News to Improve Mental Health. (arXiv:2108.07706v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongyoon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghwan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Successful sequential recommendation systems rely on accurately capturing the\nuser's short-term and long-term interest. Although Transformer-based models\nachieved state-of-the-art performance in the sequential recommendation task,\nthey generally require quadratic memory and time complexity to the sequence\nlength, making it difficult to extract the long-term interest of users. On the\nother hand, Multi-Layer Perceptrons (MLP)-based models, renowned for their\nlinear memory and time complexity, have recently shown competitive results\ncompared to Transformer in various tasks. Given the availability of a massive\namount of the user's behavior history, the linear memory and time complexity of\nMLP-based models make them a promising alternative to explore in the sequential\nrecommendation task. To this end, we adopted MLP-based models in sequential\nrecommendation but consistently observed that MLP-based methods obtain lower\nperformance than those of Transformer despite their computational benefits.\nFrom experiments, we observed that introducing explicit high-order interactions\nto MLP layers mitigates such performance gap. In response, we propose the\nMulti-Order Interaction (MOI) layer, which is capable of expressing an\narbitrary order of interactions within the inputs while maintaining the memory\nand time complexity of the MLP layer. By replacing the MLP layer with the MOI\nlayer, our model was able to achieve comparable performance with\nTransformer-based models while retaining the MLP-based models' computational\nbenefits.",
          "link": "http://arxiv.org/abs/2108.07505",
          "publishedOn": "2021-08-18T01:54:58.337Z",
          "wordCount": 658,
          "title": "MOI-Mixer: Improving MLP-Mixer with Multi Order Interactions in Sequential Recommendation. (arXiv:2108.07505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yifei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongji Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caihua Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1\">Khaled B. Letaief</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>",
          "description": "Graph convolutional networks (GCNs) have recently enabled a popular class of\nalgorithms for collaborative filtering (CF). Nevertheless, the theoretical\nunderpinnings of their empirical successes remain elusive. In this paper, we\nendeavor to obtain a better understanding of GCN-based CF methods via the lens\nof graph signal processing. By identifying the critical role of smoothness, a\nkey concept in graph signal processing, we develop a unified graph\nconvolution-based framework for CF. We prove that many existing CF methods are\nspecial cases of this framework, including the neighborhood-based methods,\nlow-rank matrix factorization, linear auto-encoders, and LightGCN,\ncorresponding to different low-pass filters. Based on our framework, we then\npresent a simple and computationally efficient CF baseline, which we shall\nrefer to as Graph Filter based Collaborative Filtering (GF-CF). Given an\nimplicit feedback matrix, GF-CF can be obtained in a closed form instead of\nexpensive training with back-propagation. Experiments will show that GF-CF\nachieves competitive or better performance against deep learning-based methods\non three well-known datasets, notably with a $70\\%$ performance gain over\nLightGCN on the Amazon-book dataset.",
          "link": "http://arxiv.org/abs/2108.07567",
          "publishedOn": "2021-08-18T01:54:58.324Z",
          "wordCount": 619,
          "title": "How Powerful is Graph Convolution for Recommendation?. (arXiv:2108.07567v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiangkun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>",
          "description": "Personalization lies at the core of boosting the product search system\nperformance. Prior studies mainly resorted to the semantic matching between\ntextual queries and user/product related documents, leaving the user\ncollaborative behaviors untapped. In fact, the collaborative filtering signals\nbetween users intuitively offer a complementary information for the semantic\nmatching. To close the gap between collaborative filtering and product search,\nwe propose a Hierarchical Heterogeneous Graph Neural Network (HHGNN) approach\nin this paper. Specifically, we organize HHGNN with a hierarchical graph\nstructure according to the three edge types. The sequence edge accounts for the\nsyntax formulation from word nodes to sentence nodes; the composition edge\naggregates the semantic features to the user and product nodes; and the\ninteraction edge on the top performs graph convolutional operation between user\nand product nodes. At last, we integrate the higher-order neighboring\ncollaborative features and the semantic features for better representation\nlearning. We conduct extensive experiments on six Amazon review datasets. The\nresults show that our proposed method can outperform the state-of-the-art\nbaselines with a large margin. In addition, we empirically prove that\ncollaborative filtering and semantic matching are complementary to each other\nin product search performance enhancement.",
          "link": "http://arxiv.org/abs/2108.07574",
          "publishedOn": "2021-08-18T01:54:58.296Z",
          "wordCount": 634,
          "title": "When Product Search Meets Collaborative Filtering: A Hierarchical Heterogeneous Graph Neural Network Approach. (arXiv:2108.07574v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pastor_E/0/1/0/all/0/1\">Eliana Pastor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfaro_L/0/1/0/all/0/1\">Luca de Alfaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baralis_E/0/1/0/all/0/1\">Elena Baralis</a>",
          "description": "When analyzing the behavior of machine learning algorithms, it is important\nto identify specific data subgroups for which the considered algorithm shows\ndifferent performance with respect to the entire dataset. The intervention of\ndomain experts is normally required to identify relevant attributes that define\nthese subgroups.\n\nWe introduce the notion of divergence to measure this performance difference\nand we exploit it in the context of (i) classification models and (ii) ranking\napplications to automatically detect data subgroups showing a significant\ndeviation in their behavior. Furthermore, we quantify the contribution of all\nattributes in the data subgroup to the divergent behavior by means of Shapley\nvalues, thus allowing the identification of the most impacting attributes.",
          "link": "http://arxiv.org/abs/2108.07450",
          "publishedOn": "2021-08-18T01:54:58.270Z",
          "wordCount": 564,
          "title": "Identifying Biased Subgroups in Ranking and Classification. (arXiv:2108.07450v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boudin_F/0/1/0/all/0/1\">Florian Boudin</a>",
          "description": "Citation recommendation is intended to assist researchers in the process of\nsearching for relevant papers to cite by recommending appropriate citations for\na given input text. Existing test collections for this task are noisy and\nunreliable since they are built automatically from parsed PDF papers. In this\npaper, we present our ongoing effort at creating a publicly available, manually\nannotated test collection for citation recommendation. We also conduct a series\nof experiments to evaluate the effectiveness of content-based baseline models\non the test collection, providing results for future work to improve upon. Our\ntest collection and code to replicate experiments are available at\nhttps://github.com/boudinfl/acm-cr",
          "link": "http://arxiv.org/abs/2108.07571",
          "publishedOn": "2021-08-18T01:54:58.253Z",
          "wordCount": 545,
          "title": "ACM-CR: A Manually Annotated Test Collection for Citation Recommendation. (arXiv:2108.07571v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amitoj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasekh_A/0/1/0/all/0/1\">Amin Rasekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golbin_I/0/1/0/all/0/1\">Ilana Golbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anand Rao</a>",
          "description": "An independent ethical assessment of an artificial intelligence system is an\nimpartial examination of the system's development, deployment, and use in\nalignment with ethical values. System-level qualitative frameworks that\ndescribe high-level requirements and component-level quantitative metrics that\nmeasure individual ethical dimensions have been developed over the past few\nyears. However, there exists a gap between the two, which hinders the execution\nof independent ethical assessments in practice. This study bridges this gap and\ndesigns a holistic independent ethical assessment process for a text\nclassification model with a special focus on the task of hate speech detection.\nThe assessment is further augmented with protected attributes mining and\ncounterfactual-based analysis to enhance bias assessment. It covers assessments\nof technical performance, data bias, embedding bias, classification bias, and\ninterpretability. The proposed process is demonstrated through an assessment of\na deep hate speech detection model.",
          "link": "http://arxiv.org/abs/2108.07627",
          "publishedOn": "2021-08-18T01:54:58.209Z",
          "wordCount": 614,
          "title": "Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study. (arXiv:2108.07627v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lesota_O/0/1/0/all/0/1\">Oleg Lesota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melchiorre_A/0/1/0/all/0/1\">Alessandro B. Melchiorre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stefan Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowald_D/0/1/0/all/0/1\">Dominik Kowald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1\">Elisabeth Lex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>",
          "description": "Several studies have identified discrepancies between the popularity of items\nin user profiles and the corresponding recommendation lists. Such behavior,\nwhich concerns a variety of recommendation algorithms, is referred to as\npopularity bias. Existing work predominantly adopts simple statistical\nmeasures, such as the difference of mean or median popularity, to quantify\npopularity bias. Moreover, it does so irrespective of user characteristics\nother than the inclination to popular content. In this work, in contrast, we\npropose to investigate popularity differences (between the user profile and\nrecommendation list) in terms of median, a variety of statistical moments, as\nwell as similarity measures that consider the entire popularity distributions\n(Kullback-Leibler divergence and Kendall's tau rank-order correlation). This\nresults in a more detailed picture of the characteristics of popularity bias.\nFurthermore, we investigate whether such algorithmic popularity bias affects\nusers of different genders in the same way. We focus on music recommendation\nand conduct experiments on the recently released standardized LFM-2b dataset,\ncontaining listening profiles of Last.fm users. We investigate the algorithmic\npopularity bias of seven common recommendation algorithms (five collaborative\nfiltering and two baselines). Our experiments show that (1) the studied metrics\nprovide novel insights into popularity bias in comparison with only using\naverage differences, (2) algorithms less inclined towards popularity bias\namplification do not necessarily perform worse in terms of utility (NDCG), (3)\nthe majority of the investigated recommenders intensify the popularity bias of\nthe female users.",
          "link": "http://arxiv.org/abs/2108.06973",
          "publishedOn": "2021-08-17T01:54:46.579Z",
          "wordCount": 691,
          "title": "Analyzing Item Popularity Bias of Music Recommender Systems: Are Different Genders Equally Affected?. (arXiv:2108.06973v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinghong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shangxuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yudong Chen</a>",
          "description": "Hashing technology has been widely used in image retrieval due to its\ncomputational and storage efficiency. Recently, deep unsupervised hashing\nmethods have attracted increasing attention due to the high cost of human\nannotations in the real world and the superiority of deep learning technology.\nHowever, most deep unsupervised hashing methods usually pre-compute a\nsimilarity matrix to model the pairwise relationship in the pre-trained feature\nspace. Then this similarity matrix would be used to guide hash learning, in\nwhich most of the data pairs are treated equivalently. The above process is\nconfronted with the following defects: 1) The pre-computed similarity matrix is\ninalterable and disconnected from the hash learning process, which cannot\nexplore the underlying semantic information. 2) The informative data pairs may\nbe buried by the large number of less-informative data pairs. To solve the\naforementioned problems, we propose a \\textbf{Deep Self-Adaptive\nHashing~(DSAH)} model to adaptively capture the semantic information with two\nspecial designs: \\textbf{Adaptive Neighbor Discovery~(AND)} and\n\\textbf{Pairwise Information Content~(PIC)}. Firstly, we adopt the AND to\ninitially construct a neighborhood-based similarity matrix, and then refine\nthis initial similarity matrix with a novel update strategy to further\ninvestigate the semantic structure behind the learned representation. Secondly,\nwe measure the priorities of data pairs with PIC and assign adaptive weights to\nthem, which is relies on the assumption that more dissimilar data pairs contain\nmore discriminative information for hash learning. Extensive experiments on\nseveral benchmark datasets demonstrate that the above two technologies\nfacilitate the deep hashing model to achieve superior performance in a\nself-adaptive manner.",
          "link": "http://arxiv.org/abs/2108.07094",
          "publishedOn": "2021-08-17T01:54:46.573Z",
          "wordCount": 698,
          "title": "Deep Self-Adaptive Hashing for Image Retrieval. (arXiv:2108.07094v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Depeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "These years much effort has been devoted to improving the accuracy or\nrelevance of the recommendation system. Diversity, a crucial factor which\nmeasures the dissimilarity among the recommended items, received rather little\nscrutiny. Directly related to user satisfaction, diversification is usually\ntaken into consideration after generating the candidate items. However, this\ndecoupled design of diversification and candidate generation makes the whole\nsystem suboptimal. In this paper, we aim at pushing the diversification to the\nupstream candidate generation stage, with the help of Graph Convolutional\nNetworks (GCN). Although GCN based recommendation algorithms have shown great\npower in modeling complex collaborative filtering effect to improve the\naccuracy of recommendation, how diversity changes is ignored in those advanced\nworks. We propose to perform rebalanced neighbor discovering, category-boosted\nnegative sampling and adversarial learning on top of GCN. We conduct extensive\nexperiments on real-world datasets. Experimental results verify the\neffectiveness of our proposed method on diversification. Further ablation\nstudies validate that our proposed method significantly alleviates the\naccuracy-diversity dilemma.",
          "link": "http://arxiv.org/abs/2108.06952",
          "publishedOn": "2021-08-17T01:54:46.550Z",
          "wordCount": 598,
          "title": "DGCN: Diversified Recommendation with Graph Convolutional Networks. (arXiv:2108.06952v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noor_K/0/1/0/all/0/1\">Kawsar Noor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roguski_L/0/1/0/all/0/1\">Lukasz Roguski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handy_A/0/1/0/all/0/1\">Alex Handy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klapaukh_R/0/1/0/all/0/1\">Roman Klapaukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folarin_A/0/1/0/all/0/1\">Amos Folarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romao_L/0/1/0/all/0/1\">Luis Romao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteson_J/0/1/0/all/0/1\">Joshua Matteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lea_N/0/1/0/all/0/1\">Nathan Lea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Leilei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Wai Keong Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Anoop Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1\">Richard J Dobson</a>",
          "description": "As more healthcare organisations transition to using electronic health record\n(EHR) systems it is important for these organisations to maximise the secondary\nuse of their data to support service improvement and clinical research. These\norganisations will find it challenging to have systems which can mine\ninformation from the unstructured data fields in the record (clinical notes,\nletters etc) and more practically have such systems interact with all of the\nhospitals data systems (legacy and current). To tackle this problem at\nUniversity College London Hospitals, we have deployed an enhanced version of\nthe CogStack platform; an information retrieval platform with natural language\nprocessing capabilities which we have configured to process the hospital's\nexisting and legacy records. The platform has improved data ingestion\ncapabilities as well as better tools for natural language processing. To date\nwe have processed over 18 million records and the insights produced from\nCogStack have informed a number of clinical research use cases at the\nhospitals.",
          "link": "http://arxiv.org/abs/2108.06835",
          "publishedOn": "2021-08-17T01:54:46.542Z",
          "wordCount": 628,
          "title": "Deployment of a Free-Text Analytics Platform at a UK National Health Service Research Hospital: CogStack at University College London Hospitals. (arXiv:2108.06835v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lijuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Semantic text matching is a critical problem in information retrieval.\nRecently, deep learning techniques have been widely used in this area and\nobtained significant performance improvements. However, most models are black\nboxes and it is hard to understand what happened in the matching process, due\nto the poor interpretability of deep learning. This paper aims at tackling this\nproblem. The key idea is to test whether existing deep text matching methods\nsatisfy some fundamental heuristics in information retrieval. Specifically,\nfour heuristics are used in our study, i.e., term frequency constraint, term\ndiscrimination constraint, length normalization constraints, and TF-length\nconstraint. Since deep matching models usually contain many parameters, it is\ndifficult to conduct a theoretical study for these complicated functions. In\nthis paper, We propose an empirical testing method. Specifically, We first\nconstruct some queries and documents to make them satisfy the assumption in a\nconstraint, and then test to which extend a deep text matching model trained on\nthe original dataset satisfies the corresponding constraint. Besides, a famous\nattribution based interpretation method, namely integrated gradient, is adopted\nto conduct detailed analysis and guide for feasible improvement. Experimental\nresults on LETOR 4.0 and MS Marco show that all the investigated deep text\nmatching methods, both representation and interaction based methods, satisfy\nthe above constraints with high probabilities in statistics. We further extend\nthese constraints to the semantic settings, which are shown to be better\nsatisfied for all the deep text matching models. These empirical findings give\nclear understandings on why deep text matching models usually perform well in\ninformation retrieval. We believe the proposed evaluation methodology will be\nuseful for testing future deep text matching models.",
          "link": "http://arxiv.org/abs/2108.07081",
          "publishedOn": "2021-08-17T01:54:46.459Z",
          "wordCount": 716,
          "title": "Toward the Understanding of Deep Text Matching Models for Information Retrieval. (arXiv:2108.07081v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1\">Mengting Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yusan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>",
          "description": "Modeling inter-dependencies between time-series is the key to achieve high\nperformance in anomaly detection for multivariate time-series data. The\nde-facto solution to model the dependencies is to feed the data into a\nrecurrent neural network (RNN). However, the fully connected network structure\nunderneath the RNN (either GRU or LSTM) assumes a static and complete\ndependency graph between time-series, which may not hold in many real-world\napplications. To alleviate this assumption, we propose a dynamic bipartite\ngraph structure to encode the inter-dependencies between time-series. More\nconcretely, we model time series as one type of nodes, and the time series\nsegments (regarded as event) as another type of nodes, where the edge between\ntwo types of nodes describe a temporal pattern occurred on a specific time\nseries at a certain time. Based on this design, relations between time series\ncan be explicitly modelled via dynamic connections to event nodes, and the\nmultivariate time-series anomaly detection problem can be formulated as a\nself-supervised, edge stream prediction problem in dynamic graphs. We conducted\nextensive experiments to demonstrate the effectiveness of the design.",
          "link": "http://arxiv.org/abs/2108.06783",
          "publishedOn": "2021-08-17T01:54:46.433Z",
          "wordCount": 629,
          "title": "Event2Graph: Event-driven Bipartite Graph for Multivariate Time-series Anomaly Detection. (arXiv:2108.06783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "The newly emerged transformer technology has a tremendous impact on NLP\nresearch. In the general English domain, transformer-based models have achieved\nstate-of-the-art performances on various NLP benchmarks. In the clinical\ndomain, researchers also have investigated transformer models for clinical\napplications. The goal of this study is to systematically explore three widely\nused transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical\nrelation extraction and develop an open-source package with clinical\npre-trained transformer-based models to facilitate information extraction in\nthe clinical domain. We developed a series of clinical RE models based on three\ntransformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these\nmodels using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2\nchallenges. We compared two classification strategies (binary vs. multi-class\nclassification) and investigated two approaches to generate candidate relations\nin different experimental settings. In this study, we compared three\ntransformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We\ndemonstrated that the RoBERTa-clinical RE model achieved the best performance\non the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2\ndataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our\nresults indicated that the binary classification strategy consistently\noutperformed the multi-class classification strategy for clinical relation\nextraction. Our methods and models are publicly available at\nhttps://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction.\nWe believe this work will improve current practice on clinical relation\nextraction and other related NLP tasks in the biomedical domain.",
          "link": "http://arxiv.org/abs/2107.08957",
          "publishedOn": "2021-08-17T01:54:46.403Z",
          "wordCount": 708,
          "title": "Clinical Relation Extraction Using Transformer-based Models. (arXiv:2107.08957v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>",
          "description": "Session-based recommendation systems suggest relevant items to users by\nmodeling user behavior and preferences using short-term anonymous sessions.\nExisting methods leverage Graph Neural Networks (GNNs) that propagate and\naggregate information from neighboring nodes i.e., local message passing. Such\ngraph-based architectures have representational limits, as a single sub-graph\nis susceptible to overfit the sequential dependencies instead of accounting for\ncomplex transitions between items in different sessions. We propose using a\nTransformer in combination with a target attentive GNN, which allows richer\nRepresentation Learning. Our experimental results and ablation show that our\nproposed method is competitive with the existing methods on real-world\nbenchmark datasets, improving on graph-based hypotheses.",
          "link": "http://arxiv.org/abs/2107.01516",
          "publishedOn": "2021-08-17T01:54:46.393Z",
          "wordCount": 570,
          "title": "Improved Representation Learning for Session-based Recommendation. (arXiv:2107.01516v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziwei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "In order to model the evolution of user preference, we should learn user/item\nembeddings based on time-ordered item purchasing sequences, which is defined as\nSequential Recommendation (SR) problem. Existing methods leverage sequential\npatterns to model item transitions. However, most of them ignore crucial\ntemporal collaborative signals, which are latent in evolving user-item\ninteractions and coexist with sequential patterns. Therefore, we propose to\nunify sequential patterns and temporal collaborative signals to improve the\nquality of recommendation, which is rather challenging. Firstly, it is hard to\nsimultaneously encode sequential patterns and collaborative signals. Secondly,\nit is non-trivial to express the temporal effects of collaborative signals.\n\nHence, we design a new framework Temporal Graph Sequential Recommender\n(TGSRec) upon our defined continuous-time bi-partite graph. We propose a novel\nTemporal Collaborative Trans-former (TCT) layer in TGSRec, which advances the\nself-attention mechanism by adopting a novel collaborative attention. TCT layer\ncan simultaneously capture collaborative signals from both users and items, as\nwell as considering temporal dynamics inside sequential patterns. We propagate\nthe information learned fromTCTlayerover the temporal graph to unify sequential\npatterns and temporal collaborative signals. Empirical results on five datasets\nshow that TGSRec significantly outperforms other baselines, in average up to\n22.5% and 22.1%absolute improvements in Recall@10and MRR, respectively.",
          "link": "http://arxiv.org/abs/2108.06625",
          "publishedOn": "2021-08-17T01:54:46.387Z",
          "wordCount": 650,
          "title": "Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer. (arXiv:2108.06625v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yankai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Menglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Ziqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jian Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "Aiming to alleviate data sparsity and cold-start problems of traditional\nrecommender systems, incorporating knowledge graphs (KGs) to supplement\nauxiliary information has recently gained considerable attention. Via unifying\nthe KG with user-item interactions into a tripartite graph, recent works\nexplore the graph topologies to learn the low-dimensional representations of\nusers and items with rich semantics. However, these real-world tripartite\ngraphs are usually scale-free, the intrinsic hierarchical graph structures of\nwhich are underemphasized in existing works, consequently, leading to\nsuboptimal recommendation performance.\n\nTo address this issue and provide more accurate recommendation, we propose a\nknowledge-aware recommendation method with the hyperbolic geometry, namely\nLorentzian Knowledge-enhanced Graph convolutional networks for Recommendation\n(LKGR). LKGR facilitates better modeling of scale-free tripartite graphs after\nthe data unification. Specifically, we employ different information propagation\nstrategies in the hyperbolic space to explicitly encode heterogeneous\ninformation from historical interactions and KGs. Our proposed knowledge-aware\nattention mechanism enables the model to automatically measure the information\ncontribution, producing the coherent information aggregation in the hyperbolic\nspace. Extensive experiments on three real-world benchmarks demonstrate that\nLKGR outperforms state-of-the-art methods by 2.2-29.9% of Recall@20 on Top-K\nrecommendation.",
          "link": "http://arxiv.org/abs/2108.06468",
          "publishedOn": "2021-08-17T01:54:46.379Z",
          "wordCount": 616,
          "title": "Modeling Scale-free Graphs for Knowledge-aware Recommendation. (arXiv:2108.06468v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Tables are often created with hierarchies, but existing works on table\nreasoning mainly focus on flat tables and neglect hierarchical tables.\nHierarchical tables challenge existing methods by hierarchical indexing, as\nwell as implicit relationships of calculation and semantics. This work presents\nHiTab, a free and open dataset for the research community to study question\nanswering (QA) and natural language generation (NLG) over hierarchical tables.\nHiTab is a cross-domain dataset constructed from a wealth of statistical\nreports and Wikipedia pages, and has unique characteristics: (1) nearly all\ntables are hierarchical, and (2) both target sentences for NLG and questions\nfor QA are revised from high-quality descriptions in statistical reports that\nare meaningful and diverse. (3) HiTab provides fine-grained annotations on both\nentity and quantity alignment. Targeting hierarchical structure, we devise a\nnovel hierarchy-aware logical form for symbolic reasoning over tables, which\nshows high effectiveness. Then given annotations of entity and quantity\nalignment, we propose partially supervised training, which helps models to\nlargely reduce spurious predictions in the QA task. In the NLG task, we find\nthat entity and quantity alignment also helps NLG models to generate better\nresults in a conditional generation setting. Experiment results of\nstate-of-the-art baselines suggest that this dataset presents a strong\nchallenge and a valuable benchmark for future research.",
          "link": "http://arxiv.org/abs/2108.06712",
          "publishedOn": "2021-08-17T01:54:46.355Z",
          "wordCount": 668,
          "title": "HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. (arXiv:2108.06712v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongjun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>",
          "description": "Sequential Recommendationdescribes a set of techniques to model dynamic user\nbehavior in order to predict future interactions in sequential user data. At\ntheir core, such approaches model transition probabilities between items in a\nsequence, whether through Markov chains, recurrent networks, or more recently,\nTransformers. However both old and new issues remain, including data-sparsity\nand noisy data; such issues can impair the performance, especially in complex,\nparameter-hungry models. In this paper, we investigate the application of\ncontrastive Self-Supervised Learning (SSL) to the sequential recommendation, as\na way to alleviate some of these issues. Contrastive SSL constructs\naugmentations from unlabelled instances, where agreements among positive pairs\nare maximized. It is challenging to devise a contrastive SSL framework for a\nsequential recommendation, due to its discrete nature, correlations among\nitems, and skewness of length distributions. To this end, we propose a novel\nframework, Contrastive Self-supervised Learning for sequential Recommendation\n(CoSeRec). We introduce two informative augmentation operators leveraging item\ncorrelations to create high-quality views for contrastive learning.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof the proposed method on improving model performance and the robustness\nagainst sparse and noisy data. Our implementation is available online at\n\\url{https://github.com/YChen1993/CoSeRec}",
          "link": "http://arxiv.org/abs/2108.06479",
          "publishedOn": "2021-08-17T01:54:46.349Z",
          "wordCount": 643,
          "title": "Contrastive Self-supervised Sequential Recommendation with Robust Augmentation. (arXiv:2108.06479v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hillmann_P/0/1/0/all/0/1\">Peter Hillmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heiland_E/0/1/0/all/0/1\">Erik Heiland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karcher_A/0/1/0/all/0/1\">Andreas Karcher</a>",
          "description": "Metadata are like the steam engine of the 21st century, driving businesses\nand offer multiple enhancements. Nevertheless, many companies are unaware that\nthese data can be used efficiently to improve their own operation. This is\nwhere the Enterprise Architecture Framework comes in. It empowers an\norganisation to get a clear view of their business, application, technical and\nphysical layer. This modelling approach is an established method for\norganizations to take a deeper look into their structure and processes. The\ndevelopment of such models requires a great deal of effort, is carried out\nmanually by interviewing stakeholders and requires continuous maintenance. Our\nnew approach enables the automated mining of Enterprise Architecture models.\nThe system uses common technologies to collect the metadata based on network\ntraffic, log files and other information in an organisation. Based on this, the\nnew approach generates EA models with the desired views points. Furthermore, a\nrule and knowledge-based reasoning is used to obtain a holistic overview. This\noffers a strategic decision support from business structure over process design\nup to planning the appropriate support technology. Therefore, it forms the base\nfor organisations to act in an agile way. The modelling can be performed in\ndifferent modelling languages, including ArchiMate and the Nato Architecture\nFramework (NAF). The designed approach is already evaluated on a small company\nwith multiple services and an infrastructure with several nodes.",
          "link": "http://arxiv.org/abs/2108.06696",
          "publishedOn": "2021-08-17T01:54:46.333Z",
          "wordCount": 670,
          "title": "Automated Enterprise Architecture Model Mining. (arXiv:2108.06696v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David/0/1/0/all/0/1\">David</a> (Xuejun) <a href=\"http://arxiv.org/find/cs/1/au:+Wang/0/1/0/all/0/1\">Wang</a>",
          "description": "Recommender systems (RecSys) have been well developed to assist user decision\nmaking. Traditional RecSys usually optimize a single objective (e.g., rating\nprediction errors or ranking quality) in the model. There is an emerging demand\nin multi-objective optimization recently in RecSys, especially in the area of\nmulti-stakeholder and multi-task recommender systems. This article provides an\noverview of multi-objective recommendations, followed by the discussions with\ncase studies. The document is considered as a supplementary material for our\ntutorial on multi-objective recommendations at ACM SIGKDD 2021.",
          "link": "http://arxiv.org/abs/2108.06367",
          "publishedOn": "2021-08-17T01:54:46.298Z",
          "wordCount": 499,
          "title": "Multi-Objective Recommendations: A Tutorial. (arXiv:2108.06367v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_R/0/1/0/all/0/1\">Rohan Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Maheep Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurya_C/0/1/0/all/0/1\">Chandresh Kumar Maurya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1\">Shitala Prasad</a>",
          "description": "Intelligent recommendation and reminder systems are the need of the\nfast-pacing life. Current intelligent systems such as Siri, Google Assistant,\nMicrosoft Cortona, etc., have limited capability. For example, if you want to\nwake up at 6 am because you have an upcoming trip, you have to set the alarm\nmanually. Besides, these systems do not recommend or remind what else to carry,\nsuch as carrying an umbrella during a likely rain. The present work proposes a\nsystem that takes an email as input and returns a recommendation-cumreminder\nlist. As a first step, we parse the emails, recognize the entities using named\nentity recognition (NER). In the second step, information retrieval over the\nweb is done to identify nearby places, climatic conditions, etc. Imperative\nsentences from the reviews of all places are extracted and passed to the object\nextraction module. The main challenge lies in extracting the objects (items) of\ninterest from the review. To solve it, a modified Machine Reading\nComprehension-NER (MRC-NER) model is trained to tag objects of interest by\nformulating annotation rules as a query. The objects so found are recommended\nto the user one day in advance. The final reminder list of objects is pruned by\nour proposed model for tracking objects kept during the \"packing activity.\"\nEventually, when the user leaves for the event/trip, an alert is sent\ncontaining the reminding list items. Our approach achieves superior performance\ncompared to several baselines by as much as 30% on recall and 10% on precision.",
          "link": "http://arxiv.org/abs/2108.06206",
          "publishedOn": "2021-08-16T00:47:33.990Z",
          "wordCount": 676,
          "title": "An Intelligent Recommendation-cum-Reminder System. (arXiv:2108.06206v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shangwen Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">QiaoQiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Recently, dense passage retrieval has become a mainstream approach to finding\nrelevant information in various natural language processing tasks. A number of\nstudies have been devoted to improving the widely adopted dual-encoder\narchitecture. However, most of the previous studies only consider query-centric\nsimilarity relation when learning the dual-encoder retriever. In order to\ncapture more comprehensive similarity relations, we propose a novel approach\nthat leverages both query-centric and PAssage-centric sImilarity Relations\n(called PAIR) for dense passage retrieval. To implement our approach, we make\nthree major technical contributions by introducing formal formulations of the\ntwo kinds of similarity relations, generating high-quality pseudo labeled data\nvia knowledge distillation, and designing an effective two-stage training\nprocedure that incorporates passage-centric similarity relation constraint.\nExtensive experiments show that our approach significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions datasets.",
          "link": "http://arxiv.org/abs/2108.06027",
          "publishedOn": "2021-08-16T00:47:33.945Z",
          "wordCount": 595,
          "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolova_M/0/1/0/all/0/1\">Marina Sokolova</a>",
          "description": "Reddit.com is a popular social media platform among young people. Reddit\nusers share their stories to seek support from other users, especially during\nthe Covid-19 pandemic. Messages posted on Reddit and their content have\nprovided researchers with opportunity to analyze public concerns. In this\nstudy, we analyzed sentiments of COVID-related messages posted on r/Depression.\nOur study poses the following questions: a) What are the common topics that the\nReddit users discuss? b) Can we use these topics to classify sentiments of the\nposts? c) What matters concern people more during the pandemic?\n\nKey Words: Sentiment Classification, Depression, COVID-19, Reddit, LDA, BERT",
          "link": "http://arxiv.org/abs/2108.06215",
          "publishedOn": "2021-08-16T00:47:33.249Z",
          "wordCount": 598,
          "title": "Sentiment Analysis of the COVID-related r/Depression Posts. (arXiv:2108.06215v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1\">Pavan Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1\">Alexander Lerch</a>",
          "description": "Several automatic approaches for objective music performance assessment (MPA)\nhave been proposed in the past, however, existing systems are not yet capable\nof reliably predicting ratings with the same accuracy as professional judges.\nThis study investigates contrastive learning as a potential method to improve\nexisting MPA systems. Contrastive learning is a widely used technique in\nrepresentation learning to learn a structured latent space capable of\nseparately clustering multiple classes. It has been shown to produce state of\nthe art results for image-based classification problems. We introduce a\nweighted contrastive loss suitable for regression tasks applied to a\nconvolutional neural network and show that contrastive loss results in\nperformance gains in regression tasks for MPA. Our results show that\ncontrastive-based methods are able to match and exceed SoTA performance for MPA\nregression tasks by creating better class clusters within the latent space of\nthe neural networks.",
          "link": "http://arxiv.org/abs/2108.01711",
          "publishedOn": "2021-08-16T00:47:32.398Z",
          "wordCount": 595,
          "title": "Improving Music Performance Assessment with Contrastive Learning. (arXiv:2108.01711v1 [cs.SD] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jeongwhan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jinsung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>",
          "description": "Collaborative filtering (CF) is a long-standing problem of recommender\nsystems. Many novel methods have been proposed, ranging from classical matrix\nfactorization to recent graph convolutional network-based approaches. After\nrecent fierce debates, researchers started to focus on linear graph\nconvolutional networks (GCNs) with a layer combination, which show\nstate-of-the-art accuracy in many datasets. In this work, we extend them based\non neural ordinary differential equations (NODEs), because the linear GCN\nconcept can be interpreted as a differential equation, and present the method\nof Learnable-Time ODE-based Collaborative Filtering (LT-OCF). The main novelty\nin our method is that after redesigning linear GCNs on top of the NODE regime,\ni) we learn the optimal architecture rather than relying on manually designed\nones, ii) we learn smooth ODE solutions that are considered suitable for CF,\nand iii) we test with various ODE solvers that internally build a diverse set\nof neural network connections. We also present a novel training method\nspecialized to our method. In our experiments with three benchmark datasets,\nGowalla, Yelp2018, and Amazon-Book, our method consistently shows better\naccuracy than existing methods, e.g., a recall of 0.0411 by LightGCN vs. 0.0442\nby LT-OCF and an NDCG of 0.0315 by LightGCN vs. 0.0341 by LT-OCF in\nAmazon-Book. One more important discovery in our experiments that is worth\nmentioning is that our best accuracy was achieved by dense connections rather\nthan linear connections.",
          "link": "http://arxiv.org/abs/2108.06208",
          "publishedOn": "2021-08-16T00:47:32.305Z",
          "wordCount": 673,
          "title": "LT-OCF: Learnable-Time ODE-based Collaborative Filtering. (arXiv:2108.06208v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>",
          "description": "The advent of contextualised language models has brought gains in search\neffectiveness, not just when applied for re-ranking the output of classical\nweighting models such as BM25, but also when used directly for passage indexing\nand retrieval, a technique which is called dense retrieval. In the existing\nliterature in neural ranking, two dense retrieval families have become\napparent: single representation, where entire passages are represented by a\nsingle embedding (usually BERT's [CLS] token, as exemplified by the recent ANCE\napproach), or multiple representations, where each token in a passage is\nrepresented by its own embedding (as exemplified by the recent ColBERT\napproach). These two families have not been directly compared. However, because\nof the likely importance of dense retrieval moving forward, a clear\nunderstanding of their advantages and disadvantages is paramount. To this end,\nthis paper contributes a direct study on their comparative effectiveness,\nnoting situations where each method under/over performs w.r.t. each other, and\nw.r.t. a BM25 baseline. We observe that, while ANCE is more efficient than\nColBERT in terms of response time and memory usage, multiple representations\nare statistically more effective than the single representations for MAP and\nMRR@10. We also show that multiple representations obtain better improvements\nthan single representations for queries that are the hardest for BM25, as well\nas for definitional queries, and those with complex information needs.",
          "link": "http://arxiv.org/abs/2108.06279",
          "publishedOn": "2021-08-16T00:47:32.256Z",
          "wordCount": 668,
          "title": "On Single and Multiple Representations in Dense Passage Retrieval. (arXiv:2108.06279v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hansi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "Modern E-commerce websites contain heterogeneous sources of information, such\nas numerical ratings, textual reviews and images. These information can be\nutilized to assist recommendation. Through textual reviews, a user explicitly\nexpress her affinity towards the item. Previous researchers found that by using\nthe information extracted from these reviews, we can better profile the users'\nexplicit preferences as well as the item features, leading to the improvement\nof recommendation performance. However, most of the previous algorithms were\nonly utilizing the review information for explicit-feedback problem i.e. rating\nprediction, and when it comes to implicit-feedback ranking problem such as\ntop-N recommendation, the usage of review information has not been fully\nexplored. Seeing this gap, in this work, we investigate the effectiveness of\ntextual review information for top-N recommendation under E-commerce settings.\nWe adapt several SOTA review-based rating prediction models for top-N\nrecommendation tasks and compare them to existing top-N recommendation models\nfrom both performance and efficiency. We find that models utilizing only review\ninformation can not achieve better performances than vanilla implicit-feedback\nmatrix factorization method. When utilizing review information as a regularizer\nor auxiliary information, the performance of implicit-feedback matrix\nfactorization method can be further improved. However, the optimal model\nstructure to utilize textual reviews for E-commerce top-N recommendation is yet\nto be determined.",
          "link": "http://arxiv.org/abs/2106.09665",
          "publishedOn": "2021-08-16T00:47:32.209Z",
          "wordCount": 695,
          "title": "Understanding the Effectiveness of Reviews in E-commerce Top-N Recommendation. (arXiv:2106.09665v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parasrampuria_R/0/1/0/all/0/1\">Rohan Parasrampuria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Suchandra Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Dhrubasish Sarkar</a>",
          "description": "In today's tech-savvy world every industry is trying to formulate methods for\nrecommending products by combining several techniques and algorithms to form a\npool that would bring forward the most enhanced models for making the\npredictions. Building on these lines is our paper focused on the application of\nsentiment analysis for recommendation in the insurance domain. We tried\nbuilding the following Machine Learning models namely, Logistic Regression,\nMultinomial Naive Bayes, and the mighty Random Forest for analyzing the\npolarity of a given feedback line given by a customer. Then we used this\npolarity along with other attributes like Age, Gender, Locality, Income, and\nthe list of other products already purchased by our existing customers as input\nfor our recommendation model. Then we matched the polarity score along with the\nuser's profiles and generated the list of insurance products to be recommended\nin descending order. Despite our model's simplicity and the lack of the key\ndata sets, the results seemed very logical and realistic. So, by developing the\nmodel with more enhanced methods and with access to better and true data\ngathered from an insurance industry may be the sector could be very well\nbenefitted from the amalgamation of sentiment analysis with a recommendation.",
          "link": "http://arxiv.org/abs/2108.06210",
          "publishedOn": "2021-08-16T00:47:32.176Z",
          "wordCount": 642,
          "title": "Recommending Insurance products by using Users' Sentiments. (arXiv:2108.06210v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gizinski_S/0/1/0/all/0/1\">Stanis&#x142;aw Gizinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzba_M/0/1/0/all/0/1\">Micha&#x142; Kuzba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielinski_B/0/1/0/all/0/1\">Bartosz Pielinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sienkiewicz_J/0/1/0/all/0/1\">Julian Sienkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laniewski_S/0/1/0/all/0/1\">Stanis&#x142;aw &#x141;aniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1\">Przemys&#x142;aw Biecek</a>",
          "description": "The growing number of AI applications, also for high-stake decisions,\nincreases the interest in Explainable and Interpretable Machine Learning\n(XI-ML). This trend can be seen both in the increasing number of regulations\nand strategies for developing trustworthy AI and the growing number of\nscientific papers dedicated to this topic. To ensure the sustainable\ndevelopment of AI, it is essential to understand the dynamics of the impact of\nregulation on research papers as well as the impact of scientific discourse on\nAI-related policies. This paper introduces a novel framework for joint analysis\nof AI-related policy documents and eXplainable Artificial Intelligence (XAI)\nresearch papers. The collected documents are enriched with metadata and\ninterconnections, using various NLP methods combined with a methodology\ninspired by Institutional Grammar. Based on the information extracted from\ncollected documents, we showcase a series of analyses that help understand\ninteractions, similarities, and differences between documents at different\nstages of institutionalization. To the best of our knowledge, this is the first\nwork to use automatic language analysis tools to understand the dynamics\nbetween XI-ML methods and regulations. We believe that such a system\ncontributes to better cooperation between XAI researchers and AI policymakers.",
          "link": "http://arxiv.org/abs/2108.06216",
          "publishedOn": "2021-08-16T00:47:32.150Z",
          "wordCount": 662,
          "title": "MAIR: Framework for mining relationships between research articles, strategies, and regulations in the field of explainable artificial intelligence. (arXiv:2108.06216v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhuoyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xingzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1\">Rui Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee Keong Kwoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chi Xu</a>",
          "description": "Latent factor models play a dominant role among recommendation techniques.\nHowever, most of the existing latent factor models assume both historical\ninteractions and embedding dimensions are independent of each other, and thus\nregrettably ignore the high-order interaction information among historical\ninteractions and embedding dimensions. In this paper, we propose a novel latent\nfactor model called COMET (COnvolutional diMEnsion inTeraction), which\nsimultaneously model the high-order interaction patterns among historical\ninteractions and embedding dimensions. To be specific, COMET stacks the\nembeddings of historical interactions horizontally at first, which results in\ntwo \"embedding maps\". In this way, internal interactions and dimensional\ninteractions can be exploited by convolutional neural networks with kernels of\ndifferent sizes simultaneously. A fully-connected multi-layer perceptron is\nthen applied to obtain two interaction vectors. Lastly, the representations of\nusers and items are enriched by the learnt interaction vectors, which can\nfurther be used to produce the final prediction. Extensive experiments and\nablation studies on various public implicit feedback datasets clearly\ndemonstrate the effectiveness and the rationality of our proposed method.",
          "link": "http://arxiv.org/abs/2007.14129",
          "publishedOn": "2021-08-16T00:47:32.116Z",
          "wordCount": 646,
          "title": "COMET: Convolutional Dimension Interaction for Collaborative Filtering. (arXiv:2007.14129v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>",
          "description": "Hateful and offensive content detection has been extensively explored in a\nsingle modality such as text. However, such toxic information could also be\ncommunicated via multimodal content such as online memes. Therefore, detecting\nmultimodal hateful content has recently garnered much attention in academic and\nindustry research communities. This paper aims to contribute to this emerging\nresearch topic by proposing DisMultiHate, which is a novel framework that\nperformed the classification of multimodal hateful content. Specifically,\nDisMultiHate is designed to disentangle target entities in multimodal memes to\nimprove hateful content classification and explainability. We conduct extensive\nexperiments on two publicly available hateful and offensive memes datasets. Our\nexperiment results show that DisMultiHate is able to outperform\nstate-of-the-art unimodal and multimodal baselines in the hateful meme\nclassification task. Empirical case studies were also conducted to demonstrate\nDisMultiHate's ability to disentangle target entities in memes and ultimately\nshowcase DisMultiHate's explainability of the multimodal hateful content\nclassification task.",
          "link": "http://arxiv.org/abs/2108.06207",
          "publishedOn": "2021-08-16T00:47:32.109Z",
          "wordCount": 597,
          "title": "Disentangling Hate in Online Memes. (arXiv:2108.06207v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>",
          "description": "Both latent semantic analysis (LSA) and correspondence analysis (CA) use a\nsingular value decomposition (SVD) for dimensionality reduction. In this\narticle, LSA and CA are compared from a theoretical point of view and applied\nin both a toy example and an authorship attribution example. In text mining\ninterest goes out to the relationships among documents and terms: for example,\nwhat terms are more often used in what documents. However, the LSA solution\ndisplays a mix of marginal effects and these relationships. It appears that CA\nhas more attractive properties than LSA. One such property is that, in CA, the\neffect of the margins is effectively eliminated, so that the CA solution is\noptimally suited to focus on the relationships among documents and terms. Three\nmechanisms are distinguished to weight documents and terms, and a unifying\nframework is proposed that includes these three mechanisms and includes both CA\nand LSA as special cases. In the authorship attribution example, the national\nanthem of the Netherlands, the application of the discussed methods is\nillustrated.",
          "link": "http://arxiv.org/abs/2108.06197",
          "publishedOn": "2021-08-16T00:47:32.058Z",
          "wordCount": 621,
          "title": "A Comparison of Latent Semantic Analysis and Correspondence Analysis for Text Mining. (arXiv:2108.06197v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>",
          "description": "Ranking models have achieved promising results, but it remains challenging to\ndesign personalized ranking systems to leverage user profiles and semantic\nrepresentations between queries and documents. In this paper, we propose a\ntopic-based personalized ranking model (TPRM) that integrates user topical\nprofile with pretrained contextualized term representations to tailor the\ngeneral document ranking list. Experiments on the real-world dataset\ndemonstrate that TPRM outperforms state-of-the-art ad-hoc ranking models and\npersonalized ranking models significantly.",
          "link": "http://arxiv.org/abs/2108.06014",
          "publishedOn": "2021-08-16T00:47:32.006Z",
          "wordCount": 511,
          "title": "TPRM: A Topic-based Personalized Ranking Model for Web Search. (arXiv:2108.06014v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutsch_J/0/1/0/all/0/1\">Julian Gutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietsch_M/0/1/0/all/0/1\">Malte Pietsch</a>",
          "description": "The evaluation of question answering models compares ground-truth annotations\nwith model predictions. However, as of today, this comparison is mostly\nlexical-based and therefore misses out on answers that have no lexical overlap\nbut are still semantically similar, thus treating correct answers as false.\nThis underestimation of the true performance of models hinders user acceptance\nin applications and complicates a fair comparison of different models.\nTherefore, there is a need for an evaluation metric that is based on semantics\ninstead of pure string similarity. In this short paper, we present SAS, a\ncross-encoder-based metric for the estimation of semantic answer similarity,\nand compare it to seven existing metrics. To this end, we create an English and\na German three-way annotated evaluation dataset containing pairs of answers\nalong with human judgment of their semantic similarity, which we release along\nwith an implementation of the SAS metric and the experiments. We find that\nsemantic similarity metrics based on recent transformer models correlate much\nbetter with human judgment than traditional lexical similarity metrics on our\ntwo newly created datasets and one dataset from related work.",
          "link": "http://arxiv.org/abs/2108.06130",
          "publishedOn": "2021-08-16T00:47:31.878Z",
          "wordCount": 622,
          "title": "Semantic Answer Similarity for Evaluating Question Answering Models. (arXiv:2108.06130v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Meizhen Ding</a>",
          "description": "Query expansion with pseudo-relevance feedback (PRF) is a powerful approach\nto enhance the effectiveness in information retrieval. Recently, with the rapid\nadvance of deep learning techniques, neural text generation has achieved\npromising success in many natural language tasks. To leverage the strength of\ntext generation for information retrieval, in this article, we propose a novel\napproach which effectively integrates text generation models into PRF-based\nquery expansion. In particular, our approach generates augmented query terms\nvia neural text generation models conditioned on both the initial query and\npseudo-relevance feedback. Moreover, in order to train the generative model, we\nadopt the conditional generative adversarial nets (CGANs) and propose the\nPRF-CGAN method in which both the generator and the discriminator are\nconditioned on the pseudo-relevance feedback. We evaluate the performance of\nour approach on information retrieval tasks using two benchmark datasets. The\nexperimental results show that our approach achieves comparable performance or\noutperforms traditional query expansion methods on both the retrieval and\nreranking tasks.",
          "link": "http://arxiv.org/abs/2108.06010",
          "publishedOn": "2021-08-16T00:47:31.869Z",
          "wordCount": 598,
          "title": "GQE-PRF: Generative Query Expansion with Pseudo-Relevance Feedback. (arXiv:2108.06010v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_P/0/1/0/all/0/1\">Prithiviraj Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhkaran Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achankuju_J/0/1/0/all/0/1\">Josemon Achankuju</a>",
          "description": "We present VESPA, an intentionally simple yet novel zero-shot system for\nlayout, locale, and domain agnostic document extraction. In spite of the\navailability of large corpora of documents, the lack of labeled and validated\ndatasets makes it a challenge to discriminatively train document extraction\nmodels for enterprises. We show that this problem can be addressed by simply\ntransferring the information extraction (IE) task to a natural language\nQuestion-Answering (QA) task without engineering task-specific architectures.\nWe demonstrate the effectiveness of our system by evaluating on a closed corpus\nof real-world retail and tax invoices with multiple complex layouts, domains,\nand geographies. The empirical evaluation shows that our system outperforms 4\nprominent commercial invoice solutions that use discriminatively trained models\nwith architectures specifically crafted for invoice extraction. We extracted 6\nfields with zero upfront human annotation or training with an Avg. F1 of 87.50.",
          "link": "http://arxiv.org/abs/2108.06069",
          "publishedOn": "2021-08-16T00:47:31.816Z",
          "wordCount": 585,
          "title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA Ensemble. (arXiv:2108.06069v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.08762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1\">Tobias Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1\">Silvan Mertes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangelova_S/0/1/0/all/0/1\">Stanislava Rangelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flutura_S/0/1/0/all/0/1\">Simon Flutura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>",
          "description": "Virtual Reality (VR) games that feature physical activities have been shown\nto increase players' motivation to do physical exercise. However, for such\nexercises to have a positive healthcare effect, they have to be repeated\nseveral times a week. To maintain player motivation over longer periods of\ntime, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the\ngame's challenge according to the player's capabilities. For exercise games,\nthis is mostly done by tuning specific in-game parameters like the speed of\nobjects. In this work, we propose to use experience-driven Procedural Content\nGeneration for DDA in VR exercise games by procedurally generating levels that\nmatch the player's current capabilities. Not only finetuning specific\nparameters but creating completely new levels has the potential to decrease\nrepetition over longer time periods and allows for the simultaneous adaptation\nof the cognitive and physical challenge of the exergame. As a proof-of-concept,\nwe implement an initial prototype in which the player must traverse a maze that\nincludes several exercise rooms, whereby the generation of the maze is realized\nby a neural network. Passing those exercise rooms requires the player to\nperform physical activities. To match the player's capabilities, we use Deep\nReinforcement Learning to adjust the structure of the maze and to decide which\nexercise rooms to include in the maze. We evaluate our prototype in an\nexploratory user study utilizing both biodata and subjective questionnaires.",
          "link": "http://arxiv.org/abs/2108.08762",
          "publishedOn": "2021-08-20T01:53:50.120Z",
          "wordCount": 681,
          "title": "Dynamic Difficulty Adjustment in Virtual Reality Exergames through Experience-driven Procedural Content Generation. (arXiv:2108.08762v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_N/0/1/0/all/0/1\">Neel Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thatipelli_A/0/1/0/all/0/1\">Anirudh Thatipelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "The lack of fine-grained joints (facial joints, hand fingers) is a\nfundamental performance bottleneck for state of the art skeleton action\nrecognition models. Despite this bottleneck, community's efforts seem to be\ninvested only in coming up with novel architectures. To specifically address\nthis bottleneck, we introduce two new pose based human action datasets -\nNTU60-X and NTU120-X. Our datasets extend the largest existing action\nrecognition dataset, NTU-RGBD. In addition to the 25 body joints for each\nskeleton as in NTU-RGBD, NTU60-X and NTU120-X dataset includes finger and\nfacial joints, enabling a richer skeleton representation. We appropriately\nmodify the state of the art approaches to enable training using the introduced\ndatasets. Our results demonstrate the effectiveness of these NTU-X datasets in\novercoming the aforementioned bottleneck and improve state of the art\nperformance, overall and on previously worst performing action categories.",
          "link": "http://arxiv.org/abs/2101.11529",
          "publishedOn": "2021-08-20T01:53:50.100Z",
          "wordCount": 643,
          "title": "NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions. (arXiv:2101.11529v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shivapuja_S/0/1/0/all/0/1\">Sravya Vardhani Shivapuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamkar_M/0/1/0/all/0/1\">Mansi Pradeep Khamkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_D/0/1/0/all/0/1\">Divij Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "Datasets for training crowd counting deep networks are typically heavy-tailed\nin count distribution and exhibit discontinuities across the count range. As a\nresult, the de facto statistical measures (MSE, MAE) exhibit large variance and\ntend to be unreliable indicators of performance across the count range. To\naddress these concerns in a holistic manner, we revise processes at various\nstages of the standard crowd counting pipeline. To enable principled and\nbalanced minibatch sampling, we propose a novel smoothed Bayesian sample\nstratification approach. We propose a novel cost function which can be readily\nincorporated into existing crowd counting deep networks to encourage\nstrata-aware optimization. We analyze the performance of representative crowd\ncounting approaches across standard datasets at per strata level and in\naggregate. We analyze the performance of crowd counting approaches across\nstandard datasets and demonstrate that our proposed modifications noticeably\nreduce error standard deviation. Our contributions represent a nuanced,\nstatistically balanced and fine-grained characterization of performance for\ncrowd counting approaches. Code, pretrained models and interactive\nvisualizations can be viewed at our project page https://deepcount.iiit.ac.in/",
          "link": "http://arxiv.org/abs/2108.08784",
          "publishedOn": "2021-08-20T01:53:50.079Z",
          "wordCount": 652,
          "title": "Wisdom of (Binned) Crowds: A Bayesian Stratification Paradigm for Crowd Counting. (arXiv:2108.08784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data are available at https://github.com/skelemoa/tal-hmo.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-08-20T01:53:50.013Z",
          "wordCount": 631,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1\">Cong Hua</a>",
          "description": "For a given video-based Human-Object Interaction scene, modeling the\nspatio-temporal relationship between humans and objects are the important cue\nto understand the contextual information presented in the video. With the\neffective spatio-temporal relationship modeling, it is possible not only to\nuncover contextual information in each frame but also to directly capture\ninter-time dependencies. It is more critical to capture the position changes of\nhuman and objects over the spatio-temporal dimension when their appearance\nfeatures may not show up significant changes over time. The full use of\nappearance features, the spatial location and the semantic information are also\nthe key to improve the video-based Human-Object Interaction recognition\nperformance. In this paper, Spatio-Temporal Interaction Graph Parsing Networks\n(STIGPN) are constructed, which encode the videos with a graph composed of\nhuman and object nodes. These nodes are connected by two types of relations:\n(i) spatial relations modeling the interactions between human and the\ninteracted objects within each frame. (ii) inter-time relations capturing the\nlong range dependencies between human and the interacted objects across frame.\nWith the graph, STIGPN learn spatio-temporal features directly from the whole\nvideo-based Human-Object Interaction scenes. Multi-modal features and a\nmulti-stream fusion strategy are used to enhance the reasoning capability of\nSTIGPN. Two Human-Object Interaction video datasets, including CAD-120 and\nSomething-Else, are used to evaluate the proposed architectures, and the\nstate-of-the-art performance demonstrates the superiority of STIGPN.",
          "link": "http://arxiv.org/abs/2108.08633",
          "publishedOn": "2021-08-20T01:53:49.832Z",
          "wordCount": 677,
          "title": "Spatio-Temporal Interaction Graph Parsing Networks for Human-Object Interaction Recognition. (arXiv:2108.08633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08551",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Heming Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katto_J/0/1/0/all/0/1\">Jiro Katto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyang Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yibo Fan</a>",
          "description": "In this paper, we propose a learned video codec with a residual prediction\nnetwork (RP-Net) and a feature-aided loop filter (LF-Net). For the RP-Net, we\nexploit the residual of previous multiple frames to further eliminate the\nredundancy of the current frame residual. For the LF-Net, the features from\nresidual decoding network and the motion compensation network are used to aid\nthe reconstruction quality. To reduce the complexity, a light ResNet structure\nis used as the backbone for both RP-Net and LF-Net. Experimental results\nillustrate that we can save about 10% BD-rate compared with previous learned\nvideo compression frameworks. Moreover, we can achieve faster coding speed due\nto the ResNet backbone. This project is available at\nhttps://github.com/chaoliu18/RPLVC.",
          "link": "http://arxiv.org/abs/2108.08551",
          "publishedOn": "2021-08-20T01:53:49.822Z",
          "wordCount": 570,
          "title": "Learned Video Compression with Residual Prediction and Loop Filter. (arXiv:2108.08551v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "The performance of a computer vision model depends on the size and quality of\nits training data. Recent studies have unveiled previously-unknown composition\nbiases in common image datasets which then lead to skewed model outputs, and\nhave proposed methods to mitigate these biases. However, most existing works\nassume that human-generated annotations can be considered gold-standard and\nunbiased. In this paper, we reveal that this assumption can be problematic, and\nthat special care should be taken to prevent models from learning such\nannotation biases. We focus on facial expression recognition and compare the\nlabel biases between lab-controlled and in-the-wild datasets. We demonstrate\nthat many expression datasets contain significant annotation biases between\ngenders, especially when it comes to the happy and angry expressions, and that\ntraditional methods cannot fully mitigate such biases in trained models. To\nremove expression annotation bias, we propose an AU-Calibrated Facial\nExpression Recognition (AUC-FER) framework that utilizes facial action units\n(AUs) and incorporates the triplet loss into the objective function.\nExperimental results suggest that the proposed method is more effective in\nremoving expression annotation bias than existing techniques.",
          "link": "http://arxiv.org/abs/2108.08504",
          "publishedOn": "2021-08-20T01:53:49.791Z",
          "wordCount": 631,
          "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition. (arXiv:2108.08504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zirui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Ziyi Ye</a>",
          "description": "Federated Learning (FL) is a privacy-protected machine learning paradigm that\nallows model to be trained directly at the edge without uploading data. One of\nthe biggest challenges faced by FL in practical applications is the\nheterogeneity of edge node data, which will slow down the convergence speed and\ndegrade the performance of the model. For the above problems, a representative\nsolution is to add additional constraints in the local training, such as\nFedProx, FedCurv and FedCL. However, the above algorithms still have room for\nimprovement. We propose to use the aggregation of all models obtained in the\npast as new constraint target to further improve the performance of such\nalgorithms. Experiments in various settings demonstrate that our method\nsignificantly improves the convergence speed and performance of the model.",
          "link": "http://arxiv.org/abs/2108.08577",
          "publishedOn": "2021-08-20T01:53:49.781Z",
          "wordCount": 563,
          "title": "Towards More Efficient Federated Learning with Better Optimization Objects. (arXiv:2108.08577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08470",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gong_X/0/1/0/all/0/1\">Xia Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuxiang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Haidi Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1\">Haoran Wei</a>",
          "description": "Musical instruments recognition is a widely used application for music\ninformation retrieval. As most of previous musical instruments recognition\ndataset focus on western musical instruments, it is difficult for researcher to\nstudy and evaluate the area of traditional Chinese musical instrument\nrecognition. This paper propose a traditional Chinese music dataset for\ntraining model and performance evaluation, named ChMusic. This dataset is free\nand publicly available, 11 traditional Chinese musical instruments and 55\ntraditional Chinese music excerpts are recorded in this dataset. Then an\nevaluation standard is proposed based on ChMusic dataset. With this standard,\nresearchers can compare their results following the same rule, and results from\ndifferent researchers will become comparable.",
          "link": "http://arxiv.org/abs/2108.08470",
          "publishedOn": "2021-08-20T01:53:49.431Z",
          "wordCount": 566,
          "title": "ChMusic: A Traditional Chinese Music Dataset for Evaluation of Instrument Recognition. (arXiv:2108.08470v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.",
          "link": "http://arxiv.org/abs/2108.08217",
          "publishedOn": "2021-08-19T01:34:58.952Z",
          "wordCount": 678,
          "title": "X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junjie H. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhou Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qihang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohno_S/0/1/0/all/0/1\">Satoru Ohno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliyawan_P/0/1/0/all/0/1\">Pujana Paliyawan</a>",
          "description": "This paper presents a commentator for providing real-time game commentary in\na fighting game. The commentary takes into account highlight cues, obtained by\nanalyzing scenes during gameplay, as input to adjust the pitch and loudness of\ncommentary to be spoken by using a Text-to-Speech (TTS) technology. We\ninvestigate different designs for pitch and loudness adjustment. The proposed\nAI consists of two parts: a dynamic adjuster for controlling pitch and loudness\nof the TTS and a real-time game commentary generator. We conduct a pilot study\non a fighting game, and our result shows that by adjusting the loudness\nsignificantly according to the level of game highlight, the entertainment of\nthe gameplay can be enhanced.",
          "link": "http://arxiv.org/abs/2108.08112",
          "publishedOn": "2021-08-19T01:34:58.636Z",
          "wordCount": 565,
          "title": "Fighting Game Commentator with Pitch and Loudness Adjustment Utilizing Highlight Cues. (arXiv:2108.08112v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junjie H. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yulin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhou Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliyawan_P/0/1/0/all/0/1\">Pujana Paliyawan</a>",
          "description": "This paper proposes a method for generating bullet comments for\nlive-streaming games based on highlights (i.e., the exciting parts of video\nclips) extracted from the game content and evaluate the effect of mental health\npromotion. Game live streaming is becoming a popular theme for academic\nresearch. Compared to traditional online video sharing platforms, such as\nYoutube and Vimeo, video live streaming platform has the benefits of\ncommunicating with other viewers in real-time. In sports broadcasting, the\ncommentator plays an essential role as mood maker by making matches more\nexciting. The enjoyment emerged while watching game live streaming also\nbenefits the audience's mental health. However, many e-sports live streaming\nchannels do not have a commentator for entertaining viewers. Therefore, this\npaper presents a design of an AI commentator that can be embedded in live\nstreaming games. To generate bullet comments for real-time game live streaming,\nthe system employs highlight evaluation to detect the highlights, and generate\nthe bullet comments. An experiment is conducted and the effectiveness of\ngenerated bullet comments in a live-streaming fighting game channel is\nevaluated.",
          "link": "http://arxiv.org/abs/2108.08083",
          "publishedOn": "2021-08-19T01:34:58.562Z",
          "wordCount": 628,
          "title": "Promoting Mental Well-Being for Audiences in a Live-Streaming Game by Highlight-Based Bullet Comments. (arXiv:2108.08083v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zichao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>",
          "description": "Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes are available at\nhttps://github.com/whwu95/DSANet.",
          "link": "http://arxiv.org/abs/2105.12085",
          "publishedOn": "2021-08-18T01:54:58.836Z",
          "wordCount": 684,
          "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the test (validation)\nset of the Aff-Wild2 database, the achieved CCC is 0.463 (0.469) for valence\nand 0.492 (0.649) for arousal, which significantly outperforms the baseline\nmethod with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for\nvalence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-08-18T01:54:58.621Z",
          "wordCount": 678,
          "title": "Continuous Emotion Recognition with Audio-visual Leader-follower Attentive Fusion. (arXiv:2107.01175v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lingzhi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Ying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sheng Yang</a>",
          "description": "This paper investigates adaptive streaming of one or multiple tiled 360\nvideos from a multi-antenna base station (BS) to one or multiple single-antenna\nusers, respectively, in a multi-carrier wireless system. We aim to maximize the\nvideo quality while keeping rebuffering time small via encoding rate adaptation\nat each group of pictures (GOP) and transmission adaptation at each\n(transmission) slot. To capture the impact of field-of-view (FoV) prediction,\nwe consider three cases of FoV viewing probability distributions, i.e.,\nperfect, imperfect, and unknown FoV viewing probability distributions, and use\nthe average total utility, worst average total utility, and worst total utility\nas the respective performance metrics. In the single-user scenario, we optimize\nthe encoding rates of the tiles, encoding rates of the FoVs, and transmission\nbeamforming vectors for all subcarriers to maximize the total utility in each\ncase. In the multi-user scenario, we adopt rate splitting with successive\ndecoding and optimize the encoding rates of the tiles, encoding rates of the\nFoVs, rates of the common and private messages, and transmission beamforming\nvectors for all subcarriers to maximize the total utility in each case. Then,\nwe separate the challenging optimization problem into multiple tractable\nproblems in each scenario. In the single-user scenario, we obtain a globally\noptimal solution of each problem using transformation techniques and the\nKarush-Kuhn-Tucker (KKT) conditions. In the multi-user scenario, we obtain a\nKKT point of each problem using the concave-convex procedure (CCCP). Finally,\nnumerical results demonstrate that the proposed solutions achieve notable gains\nover existing schemes in all three cases. To the best of our knowledge, this is\nthe first work revealing the impact of FoV prediction on the performance of\nadaptive streaming of tiled 360 videos.",
          "link": "http://arxiv.org/abs/2107.09491",
          "publishedOn": "2021-08-17T01:54:46.306Z",
          "wordCount": 773,
          "title": "Adaptive Streaming of 360 Videos with Perfect, Imperfect, and Unknown FoV Viewing Probabilities in Wireless Networks. (arXiv:2107.09491v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiawen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_J/0/1/0/all/0/1\">Jinye Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1\">Kenny Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>",
          "description": "The analytical description of charts is an exciting and important research\narea with many applications in academia and industry. Yet, this challenging\ntask has received limited attention from the computational linguistics research\ncommunity. This paper proposes \\textsf{AutoChart}, a large dataset for the\nanalytical description of charts, which aims to encourage more research into\nthis important area. Specifically, we offer a novel framework that generates\nthe charts and their analytical description automatically. We conducted\nextensive human and machine evaluations on the generated charts and\ndescriptions and demonstrate that the generated texts are informative,\ncoherent, and relevant to the corresponding charts.",
          "link": "http://arxiv.org/abs/2108.06897",
          "publishedOn": "2021-08-17T01:54:46.251Z",
          "wordCount": 543,
          "title": "AutoChart: A Dataset for Chart-to-Text Generation Task. (arXiv:2108.06897v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1\">Maksim Siniukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1\">Dmitriy Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>",
          "description": "Video-quality measurement plays a critical role in the development of\nvideo-processing applications. In this paper, we show how video preprocessing\ncan artificially increase the popular quality metric VMAF and its\ntuning-resistant version, VMAF NEG. We propose a pipeline that tunes\nprocessing-algorithm parameters to increase VMAF by up to 218.8%. A subjective\ncomparison revealed that for most preprocessing methods, a video's visual\nquality drops or stays unchanged. We also show that some preprocessing methods\ncan increase VMAF NEG scores by up to 23.6%.",
          "link": "http://arxiv.org/abs/2107.04510",
          "publishedOn": "2021-08-17T01:54:46.221Z",
          "wordCount": 550,
          "title": "Hacking VMAF and VMAF NEG: vulnerability to different preprocessing methods. (arXiv:2107.04510v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06515",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhonglan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Dongqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingfan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qilong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingyu Liu</a>",
          "description": "In industry, there exist plenty of scenarios where old gray photos need to be\nautomatically colored, such as video sites and archives. In this paper, we\npresent the HistoryNet focusing on historical person's diverse high fidelity\nclothing colorization based on fine grained semantic understanding and prior.\nColorization of historical persons is realistic and practical, however,\nexisting methods do not perform well in the regards. In this paper, a\nHistoryNet including three parts, namely, classification, fine grained semantic\nparsing and colorization, is proposed. Classification sub-module supplies\nclassifying of images according to the eras, nationalities and garment types;\nParsing sub-network supplies the semantic for person contours, clothing and\nbackground in the image to achieve more accurate colorization of clothes and\npersons and prevent color overflow. In the training process, we integrate\nclassification and semantic parsing features into the coloring generation\nnetwork to improve colorization. Through the design of classification and\nparsing subnetwork, the accuracy of image colorization can be improved and the\nboundary of each part of image can be more clearly. Moreover, we also propose a\nnovel Modern Historical Movies Dataset (MHMD) containing 1,353,166 images and\n42 labels of eras, nationalities, and garment types for automatic colorization\nfrom 147 historical movies or TV series made in modern time. Various\nquantitative and qualitative comparisons demonstrate that our method\noutperforms the state-of-the-art colorization methods, especially on military\nuniforms, which has correct colors according to the historical literatures.",
          "link": "http://arxiv.org/abs/2108.06515",
          "publishedOn": "2021-08-17T01:54:46.195Z",
          "wordCount": 694,
          "title": "Focusing on Persons: Colorizing Old Images Learning from Modern Historical Movies. (arXiv:2108.06515v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1\">Hassan Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_W/0/1/0/all/0/1\">Wei-Hong Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>",
          "description": "We present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Specifically, our\nVideo-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts\nmultimodal representations that are rich enough to benefit a variety of\ndownstream tasks. We train VATT end-to-end from scratch using multimodal\ncontrastive losses and evaluate its performance by the downstream tasks of\nvideo action recognition, audio event classification, image classification, and\ntext-to-video retrieval. Furthermore, we study a modality-agnostic\nsingle-backbone Transformer by sharing weights among the three modalities. We\nshow that the convolution-free VATT outperforms state-of-the-art ConvNet-based\narchitectures in the downstream tasks. Especially, VATT's vision Transformer\nachieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and\n41.1% on Moments in Time, new records while avoiding supervised pre-training.\nTransferring to image classification leads to 78.7% top-1 accuracy on ImageNet\ncompared to 64.7% by training the same Transformer from scratch, showing the\ngeneralizability of our model despite the domain gap between videos and images.\nVATT's audio Transformer also sets a new record on waveform-based audio event\nrecognition by achieving the mAP of 39.4% on AudioSet without any supervised\npre-training. VATT's source code is publicly available.",
          "link": "http://arxiv.org/abs/2104.11178",
          "publishedOn": "2021-08-16T00:47:31.743Z",
          "wordCount": 689,
          "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text. (arXiv:2104.11178v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>",
          "description": "Hateful and offensive content detection has been extensively explored in a\nsingle modality such as text. However, such toxic information could also be\ncommunicated via multimodal content such as online memes. Therefore, detecting\nmultimodal hateful content has recently garnered much attention in academic and\nindustry research communities. This paper aims to contribute to this emerging\nresearch topic by proposing DisMultiHate, which is a novel framework that\nperformed the classification of multimodal hateful content. Specifically,\nDisMultiHate is designed to disentangle target entities in multimodal memes to\nimprove hateful content classification and explainability. We conduct extensive\nexperiments on two publicly available hateful and offensive memes datasets. Our\nexperiment results show that DisMultiHate is able to outperform\nstate-of-the-art unimodal and multimodal baselines in the hateful meme\nclassification task. Empirical case studies were also conducted to demonstrate\nDisMultiHate's ability to disentangle target entities in memes and ultimately\nshowcase DisMultiHate's explainability of the multimodal hateful content\nclassification task.",
          "link": "http://arxiv.org/abs/2108.06207",
          "publishedOn": "2021-08-16T00:47:31.602Z",
          "wordCount": 597,
          "title": "Disentangling Hate in Online Memes. (arXiv:2108.06207v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthula_P/0/1/0/all/0/1\">Praneeth Chakravarthula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tursun_O/0/1/0/all/0/1\">Okan Tursun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Didyk_P/0/1/0/all/0/1\">Piotr Didyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_H/0/1/0/all/0/1\">Henry Fuchs</a>",
          "description": "Computer-generated holographic (CGH) displays show great potential and are\nemerging as the next-generation displays for augmented and virtual reality, and\nautomotive heads-up displays. One of the critical problems harming the wide\nadoption of such displays is the presence of speckle noise inherent to\nholography, that compromises its quality by introducing perceptible artifacts.\nAlthough speckle noise suppression has been an active research area, the\nprevious works have not considered the perceptual characteristics of the Human\nVisual System (HVS), which receives the final displayed imagery. However, it is\nwell studied that the sensitivity of the HVS is not uniform across the visual\nfield, which has led to gaze-contingent rendering schemes for maximizing the\nperceptual quality in various computer-generated imagery. Inspired by this, we\npresent the first method that reduces the \"perceived speckle noise\" by\nintegrating foveal and peripheral vision characteristics of the HVS, along with\nthe retinal point spread function, into the phase hologram computation.\nSpecifically, we introduce the anatomical and statistical retinal receptor\ndistribution into our computational hologram optimization, which places a\nhigher priority on reducing the perceived foveal speckle noise while being\nadaptable to any individual's optical aberration on the retina. Our method\ndemonstrates superior perceptual quality on our emulated holographic display.\nOur evaluations with objective measurements and subjective studies demonstrate\na significant reduction of the human perceived noise.",
          "link": "http://arxiv.org/abs/2108.06192",
          "publishedOn": "2021-08-16T00:47:31.385Z",
          "wordCount": 669,
          "title": "Gaze-Contingent Retinal Speckle Suppression for Perceptually-Matched Foveated Holographic Displays. (arXiv:2108.06192v1 [cs.HC])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.02456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Ke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Multi-label image recognition is a challenging computer vision task of\npractical use. Progresses in this area, however, are often characterized by\ncomplicated methods, heavy computations, and lack of intuitive explanations. To\neffectively capture different spatial regions occupied by objects from\ndifferent categories, we propose an embarrassingly simple module, named\nclass-specific residual attention (CSRA). CSRA generates class-specific\nfeatures for every category by proposing a simple spatial attention score, and\nthen combines it with the class-agnostic average pooling feature. CSRA achieves\nstate-of-the-art results on multilabel recognition, and at the same time is\nmuch simpler than them. Furthermore, with only 4 lines of code, CSRA also leads\nto consistent improvement across many diverse pretrained models and datasets\nwithout any extra training. CSRA is both easy to implement and light in\ncomputations, which also enjoys intuitive explanations and visualizations.",
          "link": "http://arxiv.org/abs/2108.02456",
          "publishedOn": "2021-08-20T01:53:52.288Z",
          "wordCount": 595,
          "title": "Residual Attention: A Simple but Effective Method for Multi-Label Recognition. (arXiv:2108.02456v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1\">Rohit Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara Lee Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "Most existing video-and-language (VidL) research focuses on a single dataset,\nor multiple datasets of a single task. In reality, a truly useful VidL system\nis expected to be easily generalizable to diverse tasks, domains, and datasets.\nTo facilitate the evaluation of such systems, we introduce Video-And-Language\nUnderstanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets\nover 3 popular tasks: (i) text-to-video retrieval; (ii) video question\nanswering; and (iii) video captioning. VALUE benchmark aims to cover a broad\nrange of video genres, video lengths, data volumes, and task difficulty levels.\nRather than focusing on single-channel videos with visual information only,\nVALUE promotes models that leverage information from both video frames and\ntheir associated subtitles, as well as models that share knowledge across\nmultiple tasks. We evaluate various baseline methods with and without\nlarge-scale VidL pre-training, and systematically investigate the impact of\nvideo input channels, fusion methods, and different video representations. We\nalso study the transferability between tasks, and conduct multi-task learning\nunder different settings. The significant gap between our best model and human\nperformance calls for future study for advanced VidL models. VALUE is available\nat https://value-benchmark.github.io/.",
          "link": "http://arxiv.org/abs/2106.04632",
          "publishedOn": "2021-08-20T01:53:52.257Z",
          "wordCount": 703,
          "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nalaie_K/0/1/0/all/0/1\">Keivan Nalaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rong Zheng</a>",
          "description": "In surveillance and search and rescue applications, it is important to\nperform multi-target tracking (MOT) in real-time on low-end devices. Today's\nMOT solutions employ deep neural networks, which tend to have high computation\ncomplexity. Recognizing the effects of frame sizes on tracking performance, we\npropose DeepScale, a model agnostic frame size selection approach that operates\non top of existing fully convolutional network-based trackers to accelerate\ntracking throughput. In the training stage, we incorporate detectability scores\ninto a one-shot tracker architecture so that DeepScale can learn representation\nestimations for different frame sizes in a self-supervised manner. {During\ninference, it can adapt frame sizes according to the complexity of visual\ncontents based on user-controlled parameters.} Extensive experiments and\nbenchmark tests on MOT datasets demonstrate the effectiveness and flexibility\nof DeepScale. Compared to a state-of-the-art tracker, DeepScale++, a variant of\nDeepScale achieves 1.57X accelerated with only moderate degradation (~ 2.3%) in\ntracking accuracy on the MOT15 dataset in one configuration.",
          "link": "http://arxiv.org/abs/2107.10404",
          "publishedOn": "2021-08-20T01:53:52.208Z",
          "wordCount": 623,
          "title": "DeepScale: An Online Frame Size Adaptation Approach to Accelerate Visual Multi-object Tracking. (arXiv:2107.10404v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sanghun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwak_D/0/1/0/all/0/1\">Daehoon Gwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungha Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Identifying unexpected objects on roads in semantic segmentation (e.g.,\nidentifying dogs on roads) is crucial in safety-critical applications. Existing\napproaches use images of unexpected objects from external datasets or require\nadditional training (e.g., retraining segmentation networks or training an\nextra network), which necessitate a non-trivial amount of labor intensity or\nlengthy inference time. One possible alternative is to use prediction scores of\na pre-trained network such as the max logits (i.e., maximum values among\nclasses before the final softmax layer) for detecting such objects. However,\nthe distribution of max logits of each predicted class is significantly\ndifferent from each other, which degrades the performance of identifying\nunexpected objects in urban-scene segmentation. To address this issue, we\npropose a simple yet effective approach that standardizes the max logits in\norder to align the different distributions and reflect the relative meanings of\nmax logits within each predicted class. Moreover, we consider the local regions\nfrom two different perspectives based on the intuition that neighboring pixels\nshare similar semantic information. In contrast to previous approaches, our\nmethod does not utilize any external datasets or require additional training,\nwhich makes our method widely applicable to existing pre-trained segmentation\nmodels. Such a straightforward approach achieves a new state-of-the-art\nperformance on the publicly available Fishyscapes Lost & Found leaderboard with\na large margin. Our code is publicly available at this\n$\\href{https://github.com/shjung13/Standardized-max-logits}{link}$.",
          "link": "http://arxiv.org/abs/2107.11264",
          "publishedOn": "2021-08-20T01:53:52.193Z",
          "wordCount": 723,
          "title": "Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation. (arXiv:2107.11264v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benkner_M/0/1/0/all/0/1\">Marcel Seelbach Benkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1\">Zorah L&#xe4;hner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wunderlich_C/0/1/0/all/0/1\">Christof Wunderlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>",
          "description": "Finding shape correspondences can be formulated as an NP-hard quadratic\nassignment problem (QAP) that becomes infeasible for shapes with high sampling\ndensity. A promising research direction is to tackle such quadratic\noptimization problems over binary variables with quantum annealing, which\nallows for some problems a more efficient search in the solution space.\nUnfortunately, enforcing the linear equality constraints in QAPs via a penalty\nsignificantly limits the success probability of such methods on currently\navailable quantum hardware. To address this limitation, this paper proposes\nQ-Match, i.e., a new iterative quantum method for QAPs inspired by the\nalpha-expansion algorithm, which allows solving problems of an order of\nmagnitude larger than current quantum methods. It implicitly enforces the QAP\nconstraints by updating the current estimates in a cyclic fashion. Further,\nQ-Match can be applied iteratively, on a subset of well-chosen correspondences,\nallowing us to scale to real-world problems. Using the latest quantum annealer,\nthe D-Wave Advantage, we evaluate the proposed method on a subset of QAPLIB as\nwell as on isometric shape matching problems from the FAUST dataset.",
          "link": "http://arxiv.org/abs/2105.02878",
          "publishedOn": "2021-08-20T01:53:52.153Z",
          "wordCount": 666,
          "title": "Q-Match: Iterative Shape Matching via Quantum Annealing. (arXiv:2105.02878v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1\">Kalun Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfreundt_F/0/1/0/all/0/1\">Franz-Josef Pfreundt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>",
          "description": "Over the last decade, the development of deep image classification networks\nhas mostly been driven by the search for the best performance in terms of\nclassification accuracy on standardized benchmarks like ImageNet. More\nrecently, this focus has been expanded by the notion of model robustness, \\ie\nthe generalization abilities of models towards previously unseen changes in the\ndata distribution. While new benchmarks, like ImageNet-C, have been introduced\nto measure robustness properties, we argue that fixed testsets are only able to\ncapture a small portion of possible data variations and are thus limited and\nprone to generate new overfitted solutions. To overcome these drawbacks, we\nsuggest to estimate the robustness of a model directly from the structure of\nits learned feature-space. We introduce robustness indicators which are\nobtained via unsupervised clustering of latent representations from a trained\nclassifier and show very high correlations to the model performance on\ncorrupted test data.",
          "link": "http://arxiv.org/abs/2106.12303",
          "publishedOn": "2021-08-20T01:53:52.118Z",
          "wordCount": 623,
          "title": "Estimating the Robustness of Classification Models by the Structure of the Learned Feature-Space. (arXiv:2106.12303v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1\">Gyri Reiersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1\">David Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Forest carbon offsets are increasingly popular and can play a significant\nrole in financing climate mitigation, forest conservation, and reforestation.\nMeasuring how much carbon is stored in forests is, however, still largely done\nvia expensive, time-consuming, and sometimes unaccountable field measurements.\nTo overcome these limitations, many verification bodies are leveraging machine\nlearning (ML) algorithms to estimate forest carbon from satellite or aerial\nimagery. Aerial imagery allows for tree species or family classification, which\nimproves the satellite imagery-based forest type classification. However,\naerial imagery is significantly more expensive to collect and it is unclear by\nhow much the higher resolution improves the forest carbon estimation. This\nproposal paper describes the first systematic comparison of forest carbon\nestimation from aerial imagery, satellite imagery, and ground-truth field\nmeasurements via deep learning-based algorithms for a tropical reforestation\nproject. Our initial results show that forest carbon estimates from satellite\nimagery can overestimate above-ground biomass by up to 10-times for tropical\nreforestation projects. The significant difference between aerial and\nsatellite-derived forest carbon measurements shows the potential for aerial\nimagery-based ML algorithms and raises the importance to extend this study to a\nglobal benchmark between options for carbon measurements.",
          "link": "http://arxiv.org/abs/2107.11320",
          "publishedOn": "2021-08-20T01:53:52.099Z",
          "wordCount": 693,
          "title": "Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhengmi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyazaki_T/0/1/0/all/0/1\">Tomo Miyazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugaya_Y/0/1/0/all/0/1\">Yoshihiro Sugaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omachi_S/0/1/0/all/0/1\">Shinichiro Omachi</a>",
          "description": "Scene text erasing, which replaces text regions with reasonable content in\nnatural images, has drawn significant attention in the computer vision\ncommunity in recent years. There are two potential subtasks in scene text\nerasing: text detection and image inpainting. Either subtask requires\nconsiderable data to achieve better performance; however, the lack of a\nlarge-scale real-world scene-text removal dataset does not allow existing\nmethods to work according to their potential. To avoid the limitation of the\nlack of pairwise real-world data, we enhance and make considerable use of the\nsynthetic text and subsequently train our model only on the dataset generated\nby the improved synthetic text engine. Our proposed network contains a stroke\nmask prediction module and background inpainting module that can extract the\ntext stroke as a relatively small hole from the cropped text image to maintain\nmore background content for better inpainting results. This model can partially\nerase text instances in a scene image with a bounding box or work with an\nexisting scene-text detector for automatic scene text erasing. The experimental\nresults from the qualitative and quantitative evaluation of the SCUT-Syn,\nICDAR2013, and SCUT-EnsText datasets demonstrate that our method significantly\noutperforms existing state-of-the-art methods even when they were trained on\nreal-world data.",
          "link": "http://arxiv.org/abs/2104.11493",
          "publishedOn": "2021-08-20T01:53:52.053Z",
          "wordCount": 670,
          "title": "Stroke-Based Scene Text Erasing Using Synthetic Data for Training. (arXiv:2104.11493v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1\">Chi-Wei Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>",
          "description": "We present a novel pyramidal output representation to ensure parsimony with\nour \"specialize and fuse\" process for semantic segmentation. A pyramidal\n\"output\" representation consists of coarse-to-fine levels, where each level is\n\"specialize\" in a different class distribution (e.g., more stuff than things\nclasses at coarser levels). Two types of pyramidal outputs (i.e., unity and\nsemantic pyramid) are \"fused\" into the final semantic output, where the unity\npyramid indicates unity-cells (i.e., all pixels in such cell share the same\nsemantic label). The process ensures parsimony by predicting a relatively small\nnumber of labels for unity-cells (e.g., a large cell of grass) to build the\nfinal semantic output. In addition to the \"output\" representation, we design a\ncoarse-to-fine contextual module to aggregate the \"features\" representation\nfrom different levels. We validate the effectiveness of each key module in our\nmethod through comprehensive ablation studies. Finally, our approach achieves\nstate-of-the-art performance on three widely-used semantic segmentation\ndatasets -- ADE20K, COCO-Stuff, and Pascal-Context.",
          "link": "http://arxiv.org/abs/2108.01866",
          "publishedOn": "2021-08-20T01:53:52.047Z",
          "wordCount": 616,
          "title": "Specialize and Fuse: Pyramidal Output Representation for Semantic Segmentation. (arXiv:2108.01866v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seiskari_O/0/1/0/all/0/1\">Otto Seiskari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rantalankila_P/0/1/0/all/0/1\">Pekka Rantalankila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ylilammi_J/0/1/0/all/0/1\">Jerry Ylilammi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>",
          "description": "We present HybVIO, a novel hybrid approach for combining filtering-based\nvisual-inertial odometry (VIO) with optimization-based SLAM. The core of our\nmethod is highly robust, independent VIO with improved IMU bias modeling,\noutlier rejection, stationarity detection, and feature track selection, which\nis adjustable to run on embedded hardware. Long-term consistency is achieved\nwith a loosely-coupled SLAM module. In academic benchmarks, our solution yields\nexcellent performance in all categories, especially in the real-time use case,\nwhere we outperform the current state-of-the-art. We also demonstrate the\nfeasibility of VIO for vehicular tracking on consumer-grade hardware using a\ncustom dataset, and show good performance in comparison to current commercial\nVISLAM alternatives.",
          "link": "http://arxiv.org/abs/2106.11857",
          "publishedOn": "2021-08-20T01:53:52.029Z",
          "wordCount": 575,
          "title": "HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry. (arXiv:2106.11857v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Appearance and motion are two important sources of information in video\nobject segmentation (VOS). Previous methods mainly focus on using simplex\nsolutions, lowering the upper bound of feature collaboration among and across\nthese two cues. In this paper, we study a novel framework, termed the FSNet\n(Full-duplex Strategy Network), which designs a relational cross-attention\nmodule (RCAM) to achieve the bidirectional message propagation across embedding\nsubspaces. Furthermore, the bidirectional purification module (BPM) is\nintroduced to update the inconsistent features between the spatial-temporal\nembeddings, effectively improving the model robustness. By considering the\nmutual restraint within the full-duplex strategy, our FSNet performs the\ncross-modal feature-passing (i.e., transmission and receiving) simultaneously\nbefore the fusion and decoding stage, making it robust to various challenging\nscenarios (e.g., motion blur, occlusion) in VOS. Extensive experiments on five\npopular benchmarks (i.e., DAVIS$_{16}$, FBMS, MCL, SegTrack-V2, and\nDAVSOD$_{19}$) show that our FSNet outperforms other state-of-the-arts for both\nthe VOS and video salient object detection tasks.",
          "link": "http://arxiv.org/abs/2108.03151",
          "publishedOn": "2021-08-20T01:53:51.990Z",
          "wordCount": 617,
          "title": "Full-Duplex Strategy for Video Object Segmentation. (arXiv:2108.03151v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianjun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Point cloud registration is a fundamental problem in 3D computer vision. In\nthis paper, we cast point cloud registration into a planning problem in\nreinforcement learning, which can seek the transformation between the source\nand target point clouds through trial and error. By modeling the point cloud\nregistration process as a Markov decision process (MDP), we develop a latent\ndynamic model of point clouds, consisting of a transformation network and\nevaluation network. The transformation network aims to predict the new\ntransformed feature of the point cloud after performing a rigid transformation\n(i.e., action) on it while the evaluation network aims to predict the alignment\nprecision between the transformed source point cloud and target point cloud as\nthe reward signal. Once the dynamic model of the point cloud is trained, we\nemploy the cross-entropy method (CEM) to iteratively update the planning policy\nby maximizing the rewards in the point cloud registration process. Thus, the\noptimal policy, i.e., the transformation between the source and target point\nclouds, can be obtained via gradually narrowing the search space of the\ntransformation. Experimental results on ModelNet40 and 7Scene benchmark\ndatasets demonstrate that our method can yield good registration performance in\nan unsupervised manner.",
          "link": "http://arxiv.org/abs/2108.02613",
          "publishedOn": "2021-08-20T01:53:51.962Z",
          "wordCount": 663,
          "title": "Planning with Learned Dynamic Model for Unsupervised Point Cloud Registration. (arXiv:2108.02613v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1\">Raviteja Vemulapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1\">Philip Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Bradley Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_L/0/1/0/all/0/1\">Lior Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Wu</a>",
          "description": "Collecting labeled data for the task of semantic segmentation is expensive\nand time-consuming, as it requires dense pixel-level annotations. While recent\nConvolutional Neural Network (CNN) based semantic segmentation approaches have\nachieved impressive results by using large amounts of labeled training data,\ntheir performance drops significantly as the amount of labeled data decreases.\nThis happens because deep CNNs trained with the de facto cross-entropy loss can\neasily overfit to small amounts of labeled data. To address this issue, we\npropose a simple and effective contrastive learning-based training strategy in\nwhich we first pretrain the network using a pixel-wise, label-based contrastive\nloss, and then fine-tune it using the cross-entropy loss. This approach\nincreases intra-class compactness and inter-class separability, thereby\nresulting in a better pixel classifier. We demonstrate the effectiveness of the\nproposed training strategy using the Cityscapes and PASCAL VOC 2012\nsegmentation datasets. Our results show that pretraining with the proposed\ncontrastive loss results in large performance gains (more than 20% absolute\nimprovement in some settings) when the amount of labeled data is limited. In\nmany settings, the proposed contrastive pretraining strategy, which does not\nuse any additional data, is able to match or outperform the widely-used\nImageNet pretraining strategy that uses more than a million additional labeled\nimages.",
          "link": "http://arxiv.org/abs/2012.06985",
          "publishedOn": "2021-08-20T01:53:51.794Z",
          "wordCount": 710,
          "title": "Contrastive Learning for Label-Efficient Semantic Segmentation. (arXiv:2012.06985v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1\">Deepti Ghadiyaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>",
          "description": "This paper presents a novel task together with a new benchmark for detecting\ngeneric, taxonomy-free event boundaries that segment a whole video into chunks.\nConventional work in temporal video segmentation and action detection focuses\non localizing pre-defined action categories and thus does not scale to generic\nvideos. Cognitive Science has known since last century that humans consistently\nsegment videos into meaningful temporal chunks. This segmentation happens\nnaturally, without pre-defined event categories and without being explicitly\nasked to do so. Here, we repeat these cognitive experiments on mainstream CV\ndatasets; with our novel annotation guideline which addresses the complexities\nof taxonomy-free event boundary annotation, we introduce the task of Generic\nEvent Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our\nKinetics-GEBD has the largest number of boundaries (e.g. 32 of ActivityNet, 8\nof EPIC-Kitchens-100) which are in-the-wild, taxonomy-free, cover generic event\nchange, and respect human perception diversity. We view GEBD as an important\nstepping stone towards understanding the video as a whole, and believe it has\nbeen previously neglected due to a lack of proper task definition and\nannotations. Through experiment and human study we demonstrate the value of the\nannotations. Further, we benchmark supervised and un-supervised GEBD approaches\non the TAPOS dataset and our Kinetics-GEBD. We release our annotations and\nbaseline codes at CVPR'21 LOVEU Challenge:\nhttps://sites.google.com/view/loveucvpr21.",
          "link": "http://arxiv.org/abs/2101.10511",
          "publishedOn": "2021-08-20T01:53:51.786Z",
          "wordCount": 719,
          "title": "Generic Event Boundary Detection: A Benchmark for Event Segmentation. (arXiv:2101.10511v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Peixi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Open set recognition (OSR), aiming to simultaneously classify the seen\nclasses and identify the unseen classes as 'unknown', is essential for reliable\nmachine learning.The key challenge of OSR is how to reduce the empirical\nclassification risk on the labeled known data and the open space risk on the\npotential unknown data simultaneously. To handle the challenge, we formulate\nthe open space risk problem from the perspective of multi-class integration,\nand model the unexploited extra-class space with a novel concept Reciprocal\nPoint. Follow this, a novel learning framework, termed Adversarial Reciprocal\nPoint Learning (ARPL), is proposed to minimize the overlap of known\ndistribution and unknown distributions without loss of known classification\naccuracy. Specifically, each reciprocal point is learned by the extra-class\nspace with the corresponding known category, and the confrontation among\nmultiple known categories are employed to reduce the empirical classification\nrisk. Then, an adversarial margin constraint is proposed to reduce the open\nspace risk by limiting the latent open space constructed by reciprocal points.\nTo further estimate the unknown distribution from open space, an instantiated\nadversarial enhancement method is designed to generate diverse and confusing\ntraining samples, based on the adversarial mechanism between the reciprocal\npoints and known classes. This can effectively enhance the model\ndistinguishability to the unknown classes. Extensive experimental results on\nvarious benchmark datasets indicate that the proposed method is significantly\nsuperior to other existing approaches and achieves state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2103.00953",
          "publishedOn": "2021-08-20T01:53:51.768Z",
          "wordCount": 707,
          "title": "Adversarial Reciprocal Points Learning for Open Set Recognition. (arXiv:2103.00953v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>",
          "description": "A myriad of recent breakthroughs in hand-crafted neural architectures for\nvisual recognition have highlighted the urgent need to explore hybrid\narchitectures consisting of diversified building blocks. Meanwhile, neural\narchitecture search methods are surging with an expectation to reduce human\nefforts. However, whether NAS methods can efficiently and effectively handle\ndiversified search spaces with disparate candidates (e.g. CNNs and\ntransformers) is still an open question. In this work, we present Block-wisely\nSelf-supervised Neural Architecture Search (BossNAS), an unsupervised NAS\nmethod that addresses the problem of inaccurate architecture rating caused by\nlarge weight-sharing space and biased supervision in previous methods. More\nspecifically, we factorize the search space into blocks and utilize a novel\nself-supervised training scheme, named ensemble bootstrapping, to train each\nblock separately before searching them as a whole towards the population\ncenter. Additionally, we present HyTra search space, a fabric-like hybrid\nCNN-transformer search space with searchable down-sampling positions. On this\nchallenging search space, our searched model, BossNet-T, achieves up to 82.5%\naccuracy on ImageNet, surpassing EfficientNet by 2.4% with comparable compute\ntime. Moreover, our method achieves superior architecture rating accuracy with\n0.78 and 0.76 Spearman correlation on the canonical MBConv search space with\nImageNet and on NATS-Bench size search space with CIFAR-100, respectively,\nsurpassing state-of-the-art NAS methods. Code:\nhttps://github.com/changlin31/BossNAS",
          "link": "http://arxiv.org/abs/2103.12424",
          "publishedOn": "2021-08-20T01:53:51.761Z",
          "wordCount": 705,
          "title": "BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search. (arXiv:2103.12424v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen-Hsuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei-Chiu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "Neural Radiance Fields (NeRF) have recently gained a surge of interest within\nthe computer vision community for its power to synthesize photorealistic novel\nviews of real-world scenes. One limitation of NeRF, however, is its requirement\nof accurate camera poses to learn the scene representations. In this paper, we\npropose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from\nimperfect (or even unknown) camera poses -- the joint problem of learning\nneural 3D representations and registering camera frames. We establish a\ntheoretical connection to classical image alignment and show that\ncoarse-to-fine registration is also applicable to NeRF. Furthermore, we show\nthat na\\\"ively applying positional encoding in NeRF has a negative impact on\nregistration with a synthesis-based objective. Experiments on synthetic and\nreal-world data show that BARF can effectively optimize the neural scene\nrepresentations and resolve large camera pose misalignment at the same time.\nThis enables view synthesis and localization of video sequences from unknown\ncamera poses, opening up new avenues for visual localization systems (e.g.\nSLAM) and potential applications for dense 3D mapping and reconstruction.",
          "link": "http://arxiv.org/abs/2104.06405",
          "publishedOn": "2021-08-20T01:53:51.690Z",
          "wordCount": 657,
          "title": "BARF: Bundle-Adjusting Neural Radiance Fields. (arXiv:2104.06405v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cazenavette_G/0/1/0/all/0/1\">George Cazenavette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guevara_M/0/1/0/all/0/1\">Manuel Ladron De Guevara</a>",
          "description": "While attention-based transformer networks achieve unparalleled success in\nnearly all language tasks, the large number of tokens (pixels) found in images\ncoupled with the quadratic activation memory usage makes them prohibitive for\nproblems in computer vision. As such, while language-to-language translation\nhas been revolutionized by the transformer model, convolutional networks remain\nthe de facto solution for image-to-image translation. The recently proposed\nMLP-Mixer architecture alleviates some of the computational issues associated\nwith attention-based networks while still retaining the long-range connections\nthat make transformer models desirable. Leveraging this memory-efficient\nalternative to self-attention, we propose a new exploratory model in unpaired\nimage-to-image translation called MixerGAN: a simpler MLP-based architecture\nthat considers long-distance relationships between pixels without the need for\nexpensive attention mechanisms. Quantitative and qualitative analysis shows\nthat MixerGAN achieves competitive results when compared to prior\nconvolutional-based methods.",
          "link": "http://arxiv.org/abs/2105.14110",
          "publishedOn": "2021-08-20T01:53:51.683Z",
          "wordCount": 600,
          "title": "MixerGAN: An MLP-Based Architecture for Unpaired Image-to-Image Translation. (arXiv:2105.14110v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-08-20T01:53:51.664Z",
          "wordCount": 613,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1\">Jonathan Balloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "Modern computer vision applications suffer from catastrophic forgetting when\nincrementally learning new concepts over time. The most successful approaches\nto alleviate this forgetting require extensive replay of previously seen data,\nwhich is problematic when memory constraints or data legality concerns exist.\nIn this work, we consider the high-impact problem of Data-Free\nClass-Incremental Learning (DFCIL), where an incremental learning agent must\nlearn new concepts over time without storing generators or training data from\npast tasks. One approach for DFCIL is to replay synthetic images produced by\ninverting a frozen copy of the learner's classification model, but we show this\napproach fails for common class-incremental benchmarks when using standard\ndistillation strategies. We diagnose the cause of this failure and propose a\nnovel incremental distillation strategy for DFCIL, contributing a modified\ncross-entropy training and importance-weighted feature distillation, and show\nthat our method results in up to a 25.1% increase in final task accuracy\n(absolute difference) compared to SOTA DFCIL methods for common\nclass-incremental benchmarks. Our method even outperforms several standard\nreplay based methods which store a coreset of images.",
          "link": "http://arxiv.org/abs/2106.09701",
          "publishedOn": "2021-08-20T01:53:51.658Z",
          "wordCount": 666,
          "title": "Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Massoli_F/0/1/0/all/0/1\">Fabio Valerio Massoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_A/0/1/0/all/0/1\">Alperen Kantarci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akti_S/0/1/0/all/0/1\">&#x15e;eymanur Akti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Hazim Kemal Ekenel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1\">Giuseppe Amato</a>",
          "description": "Anomalies are ubiquitous in all scientific fields and can express an\nunexpected event due to incomplete knowledge about the data distribution or an\nunknown process that suddenly comes into play and distorts observations. Due to\nsuch events' rarity, to train deep learning models on the Anomaly Detection\n(AD) task, scientists only rely on \"normal\" data, i.e., non-anomalous samples.\nThus, letting the neural network infer the distribution beneath the input data.\nIn such a context, we propose a novel framework, named Multi-layer One-Class\nClassificAtion (MOCCA),to train and test deep learning models on the AD task.\nSpecifically, we applied it to autoencoders. A key novelty in our work stems\nfrom the explicit optimization of intermediate representations for the AD task.\nIndeed, differently from commonly used approaches that consider a neural\nnetwork as a single computational block, i.e., using the output of the last\nlayer only, MOCCA explicitly leverages the multi-layer structure of deep\narchitectures. Each layer's feature space is optimized for AD during training,\nwhile in the test phase, the deep representations extracted from the trained\nlayers are combined to detect anomalies. With MOCCA, we split the training\nprocess into two steps. First, the autoencoder is trained on the reconstruction\ntask only. Then, we only retain the encoder tasked with minimizing the L_2\ndistance between the output representation and a reference point, the\nanomaly-free training data centroid, at each considered layer. Subsequently, we\ncombine the deep features extracted at the various trained layers of the\nencoder model to detect anomalies at inference time. To assess the performance\nof the models trained with MOCCA, we conduct extensive experiments on publicly\navailable datasets. We show that our proposed method reaches comparable or\nsuperior performance to state-of-the-art approaches available in the\nliterature.",
          "link": "http://arxiv.org/abs/2012.12111",
          "publishedOn": "2021-08-20T01:53:51.651Z",
          "wordCount": 775,
          "title": "MOCCA: Multi-Layer One-Class ClassificAtion for Anomaly Detection. (arXiv:2012.12111v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03078",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1\">Jinli Suo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weihang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>",
          "description": "High quality imaging usually requires bulky and expensive lenses to\ncompensate geometric and chromatic aberrations. This poses high constraints on\nthe optical hash or low cost applications. Although one can utilize algorithmic\nreconstruction to remove the artifacts of low-end lenses, the degeneration from\noptical aberrations is spatially varying and the computation has to trade off\nefficiency for performance. For example, we need to conduct patch-wise\noptimization or train a large set of local deep neural networks to achieve high\nreconstruction performance across the whole image. In this paper, we propose a\nPSF aware plug-and-play deep network, which takes the aberrant image and PSF\nmap as input and produces the latent high quality version via incorporating\nlens-specific deep priors, thus leading to a universal and flexible optical\naberration correction method. Specifically, we pre-train a base model from a\nset of diverse lenses and then adapt it to a given lens by quickly refining the\nparameters, which largely alleviates the time and memory consumption of model\nlearning. The approach is of high efficiency in both training and testing\nstages. Extensive results verify the promising applications of our proposed\napproach for compact low-end cameras.",
          "link": "http://arxiv.org/abs/2104.03078",
          "publishedOn": "2021-08-20T01:53:51.644Z",
          "wordCount": 660,
          "title": "Universal and Flexible Optical Aberration Correction Using Deep-Prior Based Deconvolution. (arXiv:2104.03078v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>",
          "description": "Convolutional neural networks have allowed remarkable advances in single\nimage super-resolution (SISR) over the last decade. Among recent advances in\nSISR, attention mechanisms are crucial for high-performance SR models. However,\nthe attention mechanism remains unclear on why it works and how it works in\nSISR. In this work, we attempt to quantify and visualize attention mechanisms\nin SISR and show that not all attention modules are equally beneficial. We then\npropose attention in attention network (A$^2$N) for more efficient and accurate\nSISR. Specifically, A$^2$N consists of a non-attention branch and a coupling\nattention branch. A dynamic attention module is proposed to generate weights\nfor these two branches to suppress unwanted attention adjustments dynamically,\nwhere the weights change adaptively according to the input features. This\nallows attention modules to specialize to beneficial examples without otherwise\npenalties and thus greatly improve the capacity of the attention network with\nfew parameters overhead. Experimental results demonstrate that our final model\nA$^2$N could achieve superior trade-off performances comparing with\nstate-of-the-art networks of similar sizes. Codes are available at\nhttps://github.com/haoyuc/A2N.",
          "link": "http://arxiv.org/abs/2104.09497",
          "publishedOn": "2021-08-20T01:53:51.637Z",
          "wordCount": 650,
          "title": "Attention in Attention Network for Image Super-Resolution. (arXiv:2104.09497v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tete Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Colorado J Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>",
          "description": "We present Region Similarity Representation Learning (ReSim), a new approach\nto self-supervised representation learning for localization-based tasks such as\nobject detection and segmentation. While existing work has largely focused on\nsolely learning global representations for an entire image, ReSim learns both\nregional representations for localization as well as semantic image-level\nrepresentations. ReSim operates by sliding a fixed-sized window across the\noverlapping area between two views (e.g., image crops), aligning these areas\nwith their corresponding convolutional feature map regions, and then maximizing\nthe feature similarity across views. As a result, ReSim learns spatially and\nsemantically consistent feature representation throughout the convolutional\nfeature maps of a neural network. A shift or scale of an image region, e.g., a\nshift or scale of an object, has a corresponding change in the feature maps;\nthis allows downstream tasks to leverage these representations for\nlocalization. Through object detection, instance segmentation, and dense pose\nestimation experiments, we illustrate how ReSim learns representations which\nsignificantly improve the localization and classification performance compared\nto a competitive MoCo-v2 baseline: $+2.7$ AP$^{\\text{bb}}_{75}$ VOC, $+1.1$\nAP$^{\\text{bb}}_{75}$ COCO, and $+1.9$ AP$^{\\text{mk}}$ Cityscapes. Code and\npre-trained models are released at: \\url{https://github.com/Tete-Xiao/ReSim}",
          "link": "http://arxiv.org/abs/2103.12902",
          "publishedOn": "2021-08-20T01:53:51.617Z",
          "wordCount": 652,
          "title": "Region Similarity Representation Learning. (arXiv:2103.12902v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okerinde_A/0/1/0/all/0/1\">Ademola Okerinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">William Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theis_T/0/1/0/all/0/1\">Tom Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nafi_N/0/1/0/all/0/1\">Nasik Nafi</a>",
          "description": "Class imbalance is an inherent problem in many machine learning\nclassification tasks. This often leads to trained models that are unusable for\nany practical purpose. In this study we explore an unsupervised approach to\naddress these imbalances by leveraging transfer learning from pre-trained image\nclassification models to encoder-based Generative Adversarial Network (eGAN).\nTo the best of our knowledge, this is the first work to tackle this problem\nusing GAN without needing to augment with synthesized fake images.\n\nIn the proposed approach we use the discriminator network to output a\nnegative or positive score. We classify as minority, test samples with negative\nscores and as majority those with positive scores. Our approach eliminates\nepistemic uncertainty in model predictions, as the P(minority) + P(majority)\nneed not sum up to 1. The impact of transfer learning and combinations of\ndifferent pre-trained image classification models at the generator and\ndiscriminator is also explored. Best result of 0.69 F1-score was obtained on\nCIFAR-10 classification task with imbalance ratio of 1:2500.\n\nOur approach also provides a mechanism of thresholding the specificity or\nsensitivity of our machine learning system. Keywords: Class imbalance, Transfer\nLearning, GAN, nash equilibrium",
          "link": "http://arxiv.org/abs/2104.04162",
          "publishedOn": "2021-08-20T01:53:51.610Z",
          "wordCount": 675,
          "title": "eGAN: Unsupervised approach to class imbalance using transfer learning. (arXiv:2104.04162v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daliparthi_V/0/1/0/all/0/1\">Venkata Satya Sai Ajay Daliparthi</a>",
          "description": "In recent years, deep neural networks (DNNs) achieved state-of-the-art\nperformance on several computer vision tasks. However, the one typical drawback\nof these DNNs is the requirement of massive labeled data. Even though few-shot\nlearning methods address this problem, they often use techniques such as\nmeta-learning and metric-learning on top of the existing methods. In this work,\nwe address this problem from a neuroscience perspective by proposing a\nhypothesis named Ikshana, which is supported by several findings in\nneuroscience. Our hypothesis approximates the refining process of conceptual\ngist in the human brain while understanding a natural scene/image. While our\nhypothesis holds no particular novelty in neuroscience, it provides a novel\nperspective for designing DNNs for vision tasks. By following the Ikshana\nhypothesis, we propose a novel neural-inspired CNN architecture named\nIkshanaNet. The empirical results demonstrate the effectiveness of our method\nby outperforming several baselines on the entire and subsets of the Cityscapes\nand the CamVid semantic segmentation benchmarks.",
          "link": "http://arxiv.org/abs/2101.10837",
          "publishedOn": "2021-08-20T01:53:51.603Z",
          "wordCount": 632,
          "title": "The Ikshana Hypothesis of Human Scene Understanding Mechanism. (arXiv:2101.10837v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.",
          "link": "http://arxiv.org/abs/2108.04351",
          "publishedOn": "2021-08-20T01:53:51.595Z",
          "wordCount": 664,
          "title": "Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "The growing number of action classes has posed a new challenge for video\nunderstanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction.\nThe ZSAR task aims to recognize target (unseen) actions without training\nexamples by leveraging semantic representations to bridge seen and unseen\nactions. However, due to the complexity and diversity of actions, it remains\nchallenging to semantically represent action classes and transfer knowledge\nfrom seen data. In this work, we propose an ER-enhanced ZSAR model inspired by\nan effective human memory technique Elaborative Rehearsal (ER), which involves\nelaborating a new concept and relating it to known concepts. Specifically, we\nexpand each action class as an Elaborative Description (ED) sentence, which is\nmore discriminative than a class name and less costly than manual-defined\nattributes. Besides directly aligning class semantics with videos, we\nincorporate objects from the video as Elaborative Concepts (EC) to improve\nvideo semantics and generalization from seen actions to unseen actions. Our\nER-enhanced ZSAR model achieves state-of-the-art results on three existing\nbenchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics\ndataset to overcome limitations of current benchmarks and demonstrate the first\ncase where ZSAR performance is comparable to few-shot learning baselines on\nthis more realistic setting. We will release our codes and collected EDs at\nhttps://github.com/DeLightCMU/ElaborativeRehearsal.",
          "link": "http://arxiv.org/abs/2108.02833",
          "publishedOn": "2021-08-20T01:53:51.589Z",
          "wordCount": 662,
          "title": "Elaborative Rehearsal for Zero-shot Action Recognition. (arXiv:2108.02833v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_N/0/1/0/all/0/1\">Neel Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thatipelli_A/0/1/0/all/0/1\">Anirudh Thatipelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "The lack of fine-grained joints (facial joints, hand fingers) is a\nfundamental performance bottleneck for state of the art skeleton action\nrecognition models. Despite this bottleneck, community's efforts seem to be\ninvested only in coming up with novel architectures. To specifically address\nthis bottleneck, we introduce two new pose based human action datasets -\nNTU60-X and NTU120-X. Our datasets extend the largest existing action\nrecognition dataset, NTU-RGBD. In addition to the 25 body joints for each\nskeleton as in NTU-RGBD, NTU60-X and NTU120-X dataset includes finger and\nfacial joints, enabling a richer skeleton representation. We appropriately\nmodify the state of the art approaches to enable training using the introduced\ndatasets. Our results demonstrate the effectiveness of these NTU-X datasets in\novercoming the aforementioned bottleneck and improve state of the art\nperformance, overall and on previously worst performing action categories.",
          "link": "http://arxiv.org/abs/2101.11529",
          "publishedOn": "2021-08-20T01:53:51.571Z",
          "wordCount": 643,
          "title": "NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions. (arXiv:2101.11529v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1\">Megh Shukla</a>",
          "description": "Active learning algorithms select a subset of data for annotation to maximize\nthe model performance on a budget. One such algorithm is Expected Gradient\nLength, which as the name suggests uses the approximate gradient induced per\nexample in the sampling process. While Expected Gradient Length has been\nsuccessfully used for classification and regression, the formulation for\nregression remains intuitively driven. Hence, our theoretical contribution\ninvolves deriving this formulation, thereby supporting the experimental\nevidence. Subsequently, we show that expected gradient length in regression is\nequivalent to Bayesian uncertainty. If certain assumptions are infeasible, our\nalgorithmic contribution (EGL++) approximates the effect of ensembles with a\nsingle deterministic network. Instead of computing multiple possible inferences\nper input, we leverage previously annotated samples to quantify the probability\nof previous labels being the true label. Such an approach allows us to extend\nexpected gradient length to a new task: human pose estimation. We perform\nexperimental validation on two human pose datasets (MPII and LSP/LSPET),\nhighlighting the interpretability and competitiveness of EGL++ with different\nactive learning algorithms for human pose estimation.",
          "link": "http://arxiv.org/abs/2104.09493",
          "publishedOn": "2021-08-20T01:53:51.564Z",
          "wordCount": 649,
          "title": "Bayesian Uncertainty and Expected Gradient Length - Regression: Two Sides Of The Same Coin?. (arXiv:2104.09493v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arkabandhu Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mingchao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Chris Jermaine</a>",
          "description": "Recent papers have suggested that transfer learning can outperform\nsophisticated meta-learning methods for few-shot image classification. We take\nthis hypothesis to its logical conclusion, and suggest the use of an ensemble\nof high-quality, pre-trained feature extractors for few-shot image\nclassification. We show experimentally that a library of pre-trained feature\nextractors combined with a simple feed-forward network learned with an\nL2-regularizer can be an excellent option for solving cross-domain few-shot\nimage classification. Our experimental results suggest that this simpler\nsample-efficient approach far outperforms several well-established\nmeta-learning algorithms on a variety of few-shot tasks.",
          "link": "http://arxiv.org/abs/2101.00562",
          "publishedOn": "2021-08-20T01:53:51.558Z",
          "wordCount": 596,
          "title": "Few-shot Image Classification: Just Use a Library of Pre-trained Feature Extractors and a Simple Classifier. (arXiv:2101.00562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1\">Taewon Kang</a>",
          "description": "Existing state-of-the-art techniques in exemplar-based image-to-image\ntranslation hold several critical concerns. Existing methods related to\nexemplar-based image-to-image translation are impossible to translate on an\nimage tuple input (source, target) that is not aligned. Additionally, we can\nconfirm that the existing method exhibits limited generalization ability to\nunseen images. In order to overcome this limitation, we propose Multiple GAN\nInversion for Exemplar-based Image-to-Image Translation. Our novel Multiple GAN\nInversion avoids human intervention by using a self-deciding algorithm to\nchoose the number of layers using Fr\\'echet Inception Distance(FID), which\nselects more plausible image reconstruction results among multiple hypotheses\nwithout any training or supervision. Experimental results have in fact, shown\nthe advantage of the proposed method compared to existing state-of-the-art\nexemplar-based image-to-image translation methods.",
          "link": "http://arxiv.org/abs/2103.14471",
          "publishedOn": "2021-08-20T01:53:51.551Z",
          "wordCount": 616,
          "title": "Multiple GAN Inversion for Exemplar-based Image-to-Image Translation. (arXiv:2103.14471v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data are available at https://github.com/skelemoa/tal-hmo.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-08-20T01:53:51.544Z",
          "wordCount": 631,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yihong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yutong Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delorme_G/0/1/0/all/0/1\">Guillaume Delorme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>",
          "description": "Transformer networks have proven extremely powerful for a wide variety of\ntasks since they were introduced. Computer vision is not an exception, as the\nuse of transformers has become very popular in the vision community in recent\nyears. Despite this wave, multiple-object tracking (MOT) exhibits for now some\nsort of incompatibility with transformers. We argue that the standard\nrepresentation - bounding boxes with insufficient sparse queries - is not\noptimal to learning transformers for MOT. Inspired by recent research, we\npropose TransCenter, the first transformer-based MOT architecture for dense\nheatmap predictions. Methodologically, we propose the use of dense pixel-level\nmulti-scale queries in a transformer dual-decoder network, to be able to\nglobally and robustly infer the heatmap of targets' centers and associate them\nthrough time. TransCenter outperforms the current state-of-the-art in standard\nbenchmarks both in MOT17 and MOT20. Our ablation study demonstrates the\nadvantage in the proposed architecture compared to more naive alternatives. The\ncode will be made publicly available at\nhttps://github.com/yihongxu/transcenter.",
          "link": "http://arxiv.org/abs/2103.15145",
          "publishedOn": "2021-08-20T01:53:51.538Z",
          "wordCount": 640,
          "title": "TransCenter: Transformers with Dense Queries for Multiple-Object Tracking. (arXiv:2103.15145v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rivoir_D/0/1/0/all/0/1\">Dominik Rivoir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_M/0/1/0/all/0/1\">Micha Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Docea_R/0/1/0/all/0/1\">Reuben Docea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolbinger_F/0/1/0/all/0/1\">Fiona Kolbinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riediger_C/0/1/0/all/0/1\">Carina Riediger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weitz_J/0/1/0/all/0/1\">J&#xfc;rgen Weitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speidel_S/0/1/0/all/0/1\">Stefanie Speidel</a>",
          "description": "Research in unpaired video translation has mainly focused on short-term\ntemporal consistency by conditioning on neighboring frames. However for\ntransfer from simulated to photorealistic sequences, available information on\nthe underlying geometry offers potential for achieving global consistency\nacross views. We propose a novel approach which combines unpaired image\ntranslation with neural rendering to transfer simulated to photorealistic\nsurgical abdominal scenes. By introducing global learnable textures and a\nlighting-invariant view-consistency loss, our method produces consistent\ntranslations of arbitrary views and thus enables long-term consistent video\nsynthesis. We design and test our model to generate video sequences from\nminimally-invasive surgical abdominal scenes. Because labeled data is often\nlimited in this domain, photorealistic data where ground truth information from\nthe simulated domain is preserved is especially relevant. By extending existing\nimage-based methods to view-consistent videos, we aim to impact the\napplicability of simulated training and evaluation environments for surgical\napplications. Code and data: this http URL",
          "link": "http://arxiv.org/abs/2103.17204",
          "publishedOn": "2021-08-20T01:53:51.518Z",
          "wordCount": 641,
          "title": "Long-Term Temporally Consistent Unpaired Video Translation from Simulated Surgical 3D Data. (arXiv:2103.17204v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Engin_D/0/1/0/all/0/1\">Deniz Engin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnitzler_F/0/1/0/all/0/1\">Fran&#xe7;ois Schnitzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_N/0/1/0/all/0/1\">Ngoc Q. K. Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>",
          "description": "High-level understanding of stories in video such as movies and TV shows from\nraw data is extremely challenging. Modern video question answering (VideoQA)\nsystems often use additional human-made sources like plot synopses, scripts,\nvideo descriptions or knowledge bases. In this work, we present a new approach\nto understand the whole story without such external sources. The secret lies in\nthe dialog: unlike any prior work, we treat dialog as a noisy source to be\nconverted into text description via dialog summarization, much like recent\nmethods treat video. The input of each modality is encoded by transformers\nindependently, and a simple fusion method combines all modalities, using soft\ntemporal attention for localization over long inputs. Our model outperforms the\nstate of the art on the KnowIT VQA dataset by a large margin, without using\nquestion-specific human annotation or human-made plot summaries. It even\noutperforms human evaluators who have never watched any whole episode before.\nCode is available at https://engindeniz.github.io/dialogsummary-videoqa",
          "link": "http://arxiv.org/abs/2103.14517",
          "publishedOn": "2021-08-20T01:53:51.511Z",
          "wordCount": 635,
          "title": "On the hidden treasure of dialog in video question answering. (arXiv:2103.14517v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1\">Klemen Kotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>",
          "description": "In the past few years, we have witnessed remarkable breakthroughs in\nself-supervised representation learning. Despite the success and adoption of\nrepresentations learned through this paradigm, much is yet to be understood\nabout how different training methods and datasets influence performance on\ndownstream tasks. In this paper, we analyze contrastive approaches as one of\nthe most successful and popular variants of self-supervised representation\nlearning. We perform this analysis from the perspective of the training\nalgorithms, pre-training datasets and end tasks. We examine over 700 training\nexperiments including 30 encoders, 4 pre-training datasets and 20 diverse\ndownstream tasks. Our experiments address various questions regarding the\nperformance of self-supervised models compared to their supervised\ncounterparts, current benchmarks used for evaluation, and the effect of the\npre-training data on end task performance. Our Visual Representation Benchmark\n(ViRB) is available at: https://github.com/allenai/virb.",
          "link": "http://arxiv.org/abs/2103.14005",
          "publishedOn": "2021-08-20T01:53:51.504Z",
          "wordCount": 608,
          "title": "Contrasting Contrastive Self-Supervised Representation Learning Pipelines. (arXiv:2103.14005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heeseung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Manjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>",
          "description": "Spatio-temporal convolution often fails to learn motion dynamics in videos\nand thus an effective motion representation is required for video understanding\nin the wild. In this paper, we propose a rich and robust motion representation\nbased on spatio-temporal self-similarity (STSS). Given a sequence of frames,\nSTSS represents each local region as similarities to its neighbors in space and\ntime. By converting appearance features into relational values, it enables the\nlearner to better recognize structural patterns in space and time. We leverage\nthe whole volume of STSS and let our model learn to extract an effective motion\nrepresentation from it. The proposed neural block, dubbed SELFY, can be easily\ninserted into neural architectures and trained end-to-end without additional\nsupervision. With a sufficient volume of the neighborhood in space and time, it\neffectively captures long-term interaction and fast motion in the video,\nleading to robust action recognition. Our experimental analysis demonstrates\nits superiority over previous methods for motion modeling as well as its\ncomplementarity to spatio-temporal features from direct convolution. On the\nstandard action recognition benchmarks, Something-Something-V1 & V2, Diving-48,\nand FineGym, the proposed method achieves the state-of-the-art results.",
          "link": "http://arxiv.org/abs/2102.07092",
          "publishedOn": "2021-08-20T01:53:51.497Z",
          "wordCount": 663,
          "title": "Learning Self-Similarity in Space and Time as Generalized Motion for Action Recognition. (arXiv:2102.07092v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1\">Muhammet Bastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinliang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_D/0/1/0/all/0/1\">Doug Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>",
          "description": "Proxy-based metric learning losses are superior to pair-based losses due to\ntheir fast convergence and low training complexity. However, existing\nproxy-based losses focus on learning class-discriminative features while\noverlooking the commonalities shared across classes which are potentially\nuseful in describing and matching samples. Moreover, they ignore the implicit\nhierarchy of categories in real-world datasets, where similar subordinate\nclasses can be grouped together. In this paper, we present a framework that\nleverages this implicit hierarchy by imposing a hierarchical structure on the\nproxies and can be used with any existing proxy-based loss. This allows our\nmodel to capture both class-discriminative features and class-shared\ncharacteristics without breaking the implicit data hierarchy. We evaluate our\nmethod on five established image retrieval datasets such as In-Shop and SOP.\nResults demonstrate that our hierarchical proxy-based loss framework improves\nthe performance of existing proxy-based losses, especially on large datasets\nwhich exhibit strong hierarchical structure.",
          "link": "http://arxiv.org/abs/2103.13538",
          "publishedOn": "2021-08-20T01:53:51.490Z",
          "wordCount": 612,
          "title": "Hierarchical Proxy-based Loss for Deep Metric Learning. (arXiv:2103.13538v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iwase_S/0/1/0/all/0/1\">Shun Iwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1\">Rawal Khirodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokota_R/0/1/0/all/0/1\">Rio Yokota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>",
          "description": "We present RePOSE, a fast iterative refinement method for 6D object pose\nestimation. Prior methods perform refinement by feeding zoomed-in input and\nrendered RGB images into a CNN and directly regressing an update of a refined\npose. Their runtime is slow due to the computational cost of CNN, which is\nespecially prominent in multiple-object pose refinement. To overcome this\nproblem, RePOSE leverages image rendering for fast feature extraction using a\n3D model with a learnable texture. We call this deep texture rendering, which\nuses a shallow multi-layer perceptron to directly regress a view-invariant\nimage representation of an object. Furthermore, we utilize differentiable\nLevenberg-Marquardt (LM) optimization to refine a pose fast and accurately by\nminimizing the feature-metric error between the input and rendered image\nrepresentations without the need of zooming in. These image representations are\ntrained such that differentiable LM optimization converges within few\niterations. Consequently, RePOSE runs at 92 FPS and achieves state-of-the-art\naccuracy of 51.6% on the Occlusion LineMOD dataset - a 4.1% absolute\nimprovement over the prior art, and comparable result on the YCB-Video dataset\nwith a much faster runtime. The code is available at\nhttps://github.com/sh8/repose.",
          "link": "http://arxiv.org/abs/2104.00633",
          "publishedOn": "2021-08-20T01:53:51.471Z",
          "wordCount": 664,
          "title": "RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering. (arXiv:2104.00633v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "Temporal action proposal generation is an important and challenging task in\nvideo understanding, which aims at detecting all temporal segments containing\naction instances of interest. The existing proposal generation approaches are\ngenerally based on pre-defined anchor windows or heuristic bottom-up boundary\nmatching strategies. This paper presents a simple and efficient framework\n(RTD-Net) for direct action proposal generation, by re-purposing a\nTransformer-alike architecture. To tackle the essential visual difference\nbetween time and space, we make three important improvements over the original\ntransformer detection framework (DETR). First, to deal with slowness prior in\nvideos, we replace the original Transformer encoder with a boundary attentive\nmodule to better capture long-range temporal information. Second, due to the\nambiguous temporal boundary and relatively sparse annotations, we present a\nrelaxed matching scheme to relieve the strict criteria of single assignment to\neach groundtruth. Finally, we devise a three-branch head to further improve the\nproposal confidence estimation by explicitly predicting its completeness.\nExtensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate\nthe effectiveness of RTD-Net, on both tasks of temporal action proposal\ngeneration and temporal action detection. Moreover, due to its simplicity in\ndesign, our framework is more efficient than previous proposal generation\nmethods, without non-maximum suppression post-processing. The code and models\nare made available at https://github.com/MCG-NJU/RTD-Action.",
          "link": "http://arxiv.org/abs/2102.01894",
          "publishedOn": "2021-08-20T01:53:51.464Z",
          "wordCount": 689,
          "title": "Relaxed Transformer Decoders for Direct Action Proposal Generation. (arXiv:2102.01894v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1\">Ardhendu Shekhar Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "Learning in a low-data regime from only a few labeled examples is an\nimportant, but challenging problem. Recent advancements within meta-learning\nhave demonstrated encouraging performance, in particular, for the task of\nfew-shot classification. We propose a novel optimization-based meta-learning\napproach for few-shot classification. It consists of an embedding network,\nproviding a general representation of the image, and a base learner module. The\nlatter learns a linear classifier during the inference through an unrolled\noptimization procedure. We design an inner learning objective composed of (i) a\nrobust classification loss on the support set and (ii) an entropy loss,\nallowing transductive learning from unlabeled query samples. By employing an\nefficient initialization module and a Steepest Descent based optimization\nalgorithm, our base learner predicts a powerful classifier within only a few\niterations. Further, our strategy enables important aspects of the base learner\nobjective to be learned during meta-training. To the best of our knowledge,\nthis work is the first to integrate both induction and transduction into the\nbase learner in an optimization-based meta-learning framework. We perform a\ncomprehensive experimental analysis, demonstrating the effectiveness of our\napproach on four few-shot classification datasets.",
          "link": "http://arxiv.org/abs/2010.00511",
          "publishedOn": "2021-08-20T01:53:51.453Z",
          "wordCount": 656,
          "title": "Few-Shot Classification By Few-Iteration Meta-Learning. (arXiv:2010.00511v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1\">Kevin Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Fredrick Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yingyong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>",
          "description": "Convolutional neural networks (CNNs) have developed to become powerful models\nfor various computer vision tasks ranging from object detection to semantic\nsegmentation. However, most of the state-of-the-art CNNs cannot be deployed\ndirectly on edge devices such as smartphones and drones, which need low latency\nunder limited power and memory bandwidth. One popular, straightforward approach\nto compressing CNNs is network slimming, which imposes $\\ell_1$ regularization\non the channel-associated scaling factors via the batch normalization layers\nduring training. Network slimming thereby identifies insignificant channels\nthat can be pruned for inference. In this paper, we propose replacing the\n$\\ell_1$ penalty with an alternative nonconvex, sparsity-inducing penalty in\norder to yield a more compressed and/or accurate CNN architecture. We\ninvestigate $\\ell_p (0 < p < 1)$, transformed $\\ell_1$ (T$\\ell_1$), minimax\nconcave penalty (MCP), and smoothly clipped absolute deviation (SCAD) due to\ntheir recent successes and popularity in solving sparse optimization problems,\nsuch as compressed sensing and variable selection. We demonstrate the\neffectiveness of network slimming with nonconvex penalties on three neural\nnetwork architectures -- VGG-19, DenseNet-40, and ResNet-164 -- on standard\nimage classification datasets. Based on the numerical experiments, T$\\ell_1$\npreserves model accuracy against channel pruning, $\\ell_{1/2, 3/4}$ yield\nbetter compressed models with similar accuracies after retraining as $\\ell_1$,\nand MCP and SCAD provide more accurate models after retraining with similar\ncompression as $\\ell_1$. Network slimming with T$\\ell_1$ regularization also\noutperforms the latest Bayesian modification of network slimming in compressing\na CNN architecture in terms of memory storage while preserving its model\naccuracy after channel pruning.",
          "link": "http://arxiv.org/abs/2010.01242",
          "publishedOn": "2021-08-20T01:53:51.446Z",
          "wordCount": 763,
          "title": "Improving Network Slimming with Nonconvex Regularization. (arXiv:2010.01242v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>",
          "description": "Adversarial attack algorithms are dominated by penalty methods, which are\nslow in practice, or more efficient distance-customized methods, which are\nheavily tailored to the properties of the distance considered. We propose a\nwhite-box attack algorithm to generate minimally perturbed adversarial examples\nbased on Augmented Lagrangian principles. We bring several algorithmic\nmodifications, which have a crucial effect on performance. Our attack enjoys\nthe generality of penalty methods and the computational efficiency of\ndistance-customized algorithms, and can be readily used for a wide set of\ndistances. We compare our attack to state-of-the-art methods on three datasets\nand several models, and consistently obtain competitive performances with\nsimilar or lower computational complexity.",
          "link": "http://arxiv.org/abs/2011.11857",
          "publishedOn": "2021-08-20T01:53:51.438Z",
          "wordCount": 578,
          "title": "Augmented Lagrangian Adversarial Attacks. (arXiv:2011.11857v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1\">Arnav Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamba_U/0/1/0/all/0/1\">Udbhav Bamba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_R/0/1/0/all/0/1\">Rishabh Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>",
          "description": "Deeper and wider CNNs are known to provide improved performance for deep\nlearning tasks. However, most such networks have poor performance gain per\nparameter increase. In this paper, we investigate whether the gain observed in\ndeeper models is purely due to the addition of more optimization parameters or\nwhether the physical size of the network as well plays a role. Further, we\npresent a novel rescaling strategy for CNNs based on learnable repetition of\nits parameters. Based on this strategy, we rescale CNNs without changing their\nparameter count, and show that learnable sharing of weights itself can provide\nsignificant boost in the performance of any given model without changing its\nparameter count. We show that small base networks when rescaled, can provide\nperformance comparable to deeper networks with as low as 6% of optimization\nparameters of the deeper one.\n\nThe relevance of weight sharing is further highlighted through the example of\ngroup-equivariant CNNs. We show that the significant improvements obtained with\ngroup-equivariant CNNs over the regular CNNs on classification problems are\nonly partly due to the added equivariance property, and part of it comes from\nthe learnable repetition of network weights. For rot-MNIST dataset, we show\nthat up to 40% of the relative gain reported by state-of-the-art methods for\nrotation equivariance could actually be due to just the learnt repetition of\nweights.",
          "link": "http://arxiv.org/abs/2101.05650",
          "publishedOn": "2021-08-20T01:53:51.420Z",
          "wordCount": 684,
          "title": "Rescaling CNN through Learnable Repetition of Network Parameters. (arXiv:2101.05650v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aayush Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debnath_S/0/1/0/all/0/1\">Shoubhik Debnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafleche_J/0/1/0/all/0/1\">Jean-Francois Lafleche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cameracci_E/0/1/0/all/0/1\">Eric Cameracci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+State_G/0/1/0/all/0/1\">Gavriel State</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1\">Stan Birchfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Marc T. Law</a>",
          "description": "Synthetic data is emerging as a promising solution to the scalability issue\nof supervised deep learning, especially when real data are difficult to acquire\nor hard to annotate. Synthetic data generation, however, can itself be\nprohibitively expensive when domain experts have to manually and painstakingly\noversee the process. Moreover, neural networks trained on synthetic data often\ndo not perform well on real data because of the domain gap. To solve these\nchallenges, we propose Sim2SG, a self-supervised automatic scene generation\ntechnique for matching the distribution of real data. Importantly, Sim2SG does\nnot require supervision from the real-world dataset, thus making it applicable\nin situations for which such annotations are difficult to obtain. Sim2SG is\ndesigned to bridge both the content and appearance gaps, by matching the\ncontent of real data, and by matching the features in the source and target\ndomains. We select scene graph (SG) generation as the downstream task, due to\nthe limited availability of labeled datasets. Experiments demonstrate\nsignificant improvements over leading baselines in reducing the domain gap both\nqualitatively and quantitatively, on several synthetic datasets as well as the\nreal-world KITTI dataset.",
          "link": "http://arxiv.org/abs/2011.14488",
          "publishedOn": "2021-08-20T01:53:51.399Z",
          "wordCount": 662,
          "title": "Self-Supervised Real-to-Sim Scene Generation. (arXiv:2011.14488v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuru_N/0/1/0/all/0/1\">Nurullah Giray Kuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>",
          "description": "From CNNs to attention mechanisms, encoding inductive biases into neural\nnetworks has been a fruitful source of improvement in machine learning. Adding\nauxiliary losses to the main objective function is a general way of encoding\nbiases that can help networks learn better representations. However, since\nauxiliary losses are minimized only on training data, they suffer from the same\ngeneralization gap as regular task losses. Moreover, by adding a term to the\nloss function, the model optimizes a different objective than the one we care\nabout. In this work we address both problems: first, we take inspiration from\n\\textit{transductive learning} and note that after receiving an input but\nbefore making a prediction, we can fine-tune our networks on any unsupervised\nloss. We call this process {\\em tailoring}, because we customize the model to\neach input to ensure our prediction satisfies the inductive bias. Second, we\nformulate {\\em meta-tailoring}, a nested optimization similar to that in\nmeta-learning, and train our models to perform well on the task objective after\nadapting them using an unsupervised loss. The advantages of tailoring and\nmeta-tailoring are discussed theoretically and demonstrated empirically on a\ndiverse set of examples.",
          "link": "http://arxiv.org/abs/2009.10623",
          "publishedOn": "2021-08-20T01:53:51.392Z",
          "wordCount": 703,
          "title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yudong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>",
          "description": "Generating high-fidelity talking head video by fitting with the input audio\nsequence is a challenging problem that receives considerable attentions\nrecently. In this paper, we address this problem with the aid of neural scene\nrepresentation networks. Our method is completely different from existing\nmethods that rely on intermediate representations like 2D landmarks or 3D face\nmodels to bridge the gap between audio input and video output. Specifically,\nthe feature of input audio signal is directly fed into a conditional implicit\nfunction to generate a dynamic neural radiance field, from which a\nhigh-fidelity talking-head video corresponding to the audio signal is\nsynthesized using volume rendering. Another advantage of our framework is that\nnot only the head (with hair) region is synthesized as previous methods did,\nbut also the upper body is generated via two individual neural radiance fields.\nExperimental results demonstrate that our novel framework can (1) produce\nhigh-fidelity and natural results, and (2) support free adjustment of audio\nsignals, viewing directions, and background images. Code is available at\nhttps://github.com/YudongGuo/AD-NeRF.",
          "link": "http://arxiv.org/abs/2103.11078",
          "publishedOn": "2021-08-20T01:53:51.385Z",
          "wordCount": 664,
          "title": "AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis. (arXiv:2103.11078v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1007.3881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>",
          "description": "In this paper orthogonal multifilters for astronomical image processing are\npresented. We obtained new orthogonal multifilters based on the orthogonal\nwavelet of Haar and Daubechies. Recently, multiwavelets have been introduced as\na more powerful multiscale analysis tool. It adds several degrees of freedom in\nmultifilter design and makes it possible to have several useful properties such\nas symmetry, orthogonality, short support, and a higher number of vanishing\nmoments simultaneously. Multifilter decomposition of scanned photographic\nplates with astronomical images is made.",
          "link": "http://arxiv.org/abs/1007.3881",
          "publishedOn": "2021-08-20T01:53:51.378Z",
          "wordCount": 579,
          "title": "Orthogonal multifilters image processing of astronomical images from scanned photographic plates. (arXiv:1007.3881v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>",
          "description": "Human-Object Interaction (HOI) detection is a fundamental visual task aiming\nat localizing and recognizing interactions between humans and objects. Existing\nworks focus on the visual and linguistic features of humans and objects.\nHowever, they do not capitalise on the high-level and semantic relationships\npresent in the image, which provides crucial contextual and detailed relational\nknowledge for HOI inference. We propose a novel method to exploit this\ninformation, through the scene graph, for the Human-Object Interaction (SG2HOI)\ndetection task. Our method, SG2HOI, incorporates the SG information in two\nways: (1) we embed a scene graph into a global context clue, serving as the\nscene-specific environmental context; and (2) we build a relation-aware\nmessage-passing module to gather relationships from objects' neighborhood and\ntransfer them into interactions. Empirical evaluation shows that our SG2HOI\nmethod outperforms the state-of-the-art methods on two benchmark HOI datasets:\nV-COCO and HICO-DET. Code will be available at https://github.com/ht014/SG2HOI.",
          "link": "http://arxiv.org/abs/2108.08584",
          "publishedOn": "2021-08-20T01:53:51.358Z",
          "wordCount": 589,
          "title": "Exploiting Scene Graphs for Human-Object Interaction Detection. (arXiv:2108.08584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.07133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooklev_T/0/1/0/all/0/1\">Todor Cooklev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keinert_F/0/1/0/all/0/1\">Fritz Keinert</a>",
          "description": "We consider the design of an orthogonal symmetric/antisymmetric multiwavelet\nfrom its matrix product filter by matrix spectral factorization (MSF). As a\ntest problem, we construct a simple matrix product filter with desirable\nproperties, and factor it using Bauer's method, which in this case can be done\nin closed form. The corresponding orthogonal multiwavelet function is derived\nusing algebraic techniques which allow symmetry to be considered. This leads to\nthe known orthogonal multiwavelet SA1, which can also be derived directly. We\nalso give a lifting scheme for SA1, investigate the influence of the number of\nsignificant digits in the calculations, and show some numerical experiments.",
          "link": "http://arxiv.org/abs/1910.07133",
          "publishedOn": "2021-08-20T01:53:51.351Z",
          "wordCount": 647,
          "title": "Design of a Simple Orthogonal Multiwavelet Filter by Matrix Spectral Factorization. (arXiv:1910.07133v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>",
          "description": "This paper proposes a novel location-aware deep-learning-based single image\nreflection removal method. Our network has a reflection detection module to\nregress a probabilistic reflection confidence map, taking multi-scale Laplacian\nfeatures as inputs. This probabilistic map tells if a region is\nreflection-dominated or transmission-dominated, and it is used as a cue for the\nnetwork to control the feature flow when predicting the reflection and\ntransmission layers. We design our network as a recurrent network to\nprogressively refine reflection removal results at each iteration. The novelty\nis that we leverage Laplacian kernel parameters to emphasize the boundaries of\nstrong reflections. It is beneficial to strong reflection detection and\nsubstantially improves the quality of reflection removal results. Extensive\nexperiments verify the superior performance of the proposed method over\nstate-of-the-art approaches. Our code and the pre-trained model can be found at\nhttps://github.com/zdlarr/Location-aware-SIRR.",
          "link": "http://arxiv.org/abs/2012.07131",
          "publishedOn": "2021-08-20T01:53:51.339Z",
          "wordCount": 613,
          "title": "Location-aware Single Image Reflection Removal. (arXiv:2012.07131v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katircioglu_I/0/1/0/all/0/1\">Isinsu Katircioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sporri_J/0/1/0/all/0/1\">J&#xf6;rg Sp&#xf6;rri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Self-supervised detection and segmentation of foreground objects aims for\naccuracy without annotated training data. However, existing approaches\npredominantly rely on restrictive assumptions on appearance and motion. For\nscenes with dynamic activities and camera motion, we propose a multi-camera\nframework in which geometric constraints are embedded in the form of multi-view\nconsistency during training via coarse 3D localization in a voxel grid and\nfine-grained offset regression. In this manner, we learn a joint distribution\nof proposals over multiple views. At inference time, our method operates on\nsingle RGB images. We outperform state-of-the-art techniques both on images\nthat visually depart from those of standard benchmarks and on those of the\nclassical Human3.6M dataset.",
          "link": "http://arxiv.org/abs/2012.05119",
          "publishedOn": "2021-08-20T01:53:51.333Z",
          "wordCount": 577,
          "title": "Human Detection and Segmentation via Multi-view Consensus. (arXiv:2012.05119v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sudipta Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1\">Niluthpol Chowdhury Mithun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "Prior works on text-based video moment localization focus on temporally\ngrounding the textual query in an untrimmed video. These works assume that the\nrelevant video is already known and attempt to localize the moment on that\nrelevant video only. Different from such works, we relax this assumption and\naddress the task of localizing moments in a corpus of videos for a given\nsentence query. This task poses a unique challenge as the system is required to\nperform: (i) retrieval of the relevant video where only a segment of the video\ncorresponds with the queried sentence, and (ii) temporal localization of moment\nin the relevant video based on sentence query. Towards overcoming this\nchallenge, we propose Hierarchical Moment Alignment Network (HMAN) which learns\nan effective joint embedding space for moments and sentences. In addition to\nlearning subtle differences between intra-video moments, HMAN focuses on\ndistinguishing inter-video global semantic concepts based on sentence queries.\nQualitative and quantitative results on three benchmark text-based video moment\nretrieval datasets - Charades-STA, DiDeMo, and ActivityNet Captions -\ndemonstrate that our method achieves promising performance on the proposed task\nof temporal localization of moments in a corpus of videos.",
          "link": "http://arxiv.org/abs/2008.08716",
          "publishedOn": "2021-08-20T01:53:51.326Z",
          "wordCount": 657,
          "title": "Text-based Localization of Moments in a Video Corpus. (arXiv:2008.08716v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dabral_R/0/1/0/all/0/1\">Rishabh Dabral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Arjun Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>",
          "description": "This paper proposes GraviCap, i.e., a new approach for joint markerless 3D\nhuman motion capture and object trajectory estimation from monocular RGB\nvideos. We focus on scenes with objects partially observed during a free\nflight. In contrast to existing monocular methods, we can recover scale, object\ntrajectories as well as human bone lengths in meters and the ground plane's\norientation, thanks to the awareness of the gravity constraining object\nmotions. Our objective function is parametrised by the object's initial\nvelocity and position, gravity direction and focal length, and jointly\noptimised for one or several free flight episodes. The proposed human-object\ninteraction constraints ensure geometric consistency of the 3D reconstructions\nand improved physical plausibility of human poses compared to the unconstrained\ncase. We evaluate GraviCap on a new dataset with ground-truth annotations for\npersons and different objects undergoing free flights. In the experiments, our\napproach achieves state-of-the-art accuracy in 3D human motion capture on\nvarious metrics. We urge the reader to watch our supplementary video. Both the\nsource code and the dataset are released; see\nthis http URL",
          "link": "http://arxiv.org/abs/2108.08844",
          "publishedOn": "2021-08-20T01:53:51.309Z",
          "wordCount": 630,
          "title": "Gravity-Aware Monocular 3D Human-Object Reconstruction. (arXiv:2108.08844v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Point clouds captured in real-world applications are often incomplete due to\nthe limited sensor resolution, single viewpoint, and occlusion. Therefore,\nrecovering the complete point clouds from partial ones becomes an indispensable\ntask in many practical applications. In this paper, we present a new method\nthat reformulates point cloud completion as a set-to-set translation problem\nand design a new model, called PoinTr that adopts a transformer encoder-decoder\narchitecture for point cloud completion. By representing the point cloud as a\nset of unordered groups of points with position embeddings, we convert the\npoint cloud to a sequence of point proxies and employ the transformers for\npoint cloud generation. To facilitate transformers to better leverage the\ninductive bias about 3D geometric structures of point clouds, we further devise\na geometry-aware block that models the local geometric relationships\nexplicitly. The migration of transformers enables our model to better learn\nstructural knowledge and preserve detailed information for point cloud\ncompletion. Furthermore, we propose two more challenging benchmarks with more\ndiverse incomplete point clouds that can better reflect the real-world\nscenarios to promote future research. Experimental results show that our method\noutperforms state-of-the-art methods by a large margin on both the new\nbenchmarks and the existing ones. Code is available at\nhttps://github.com/yuxumin/PoinTr",
          "link": "http://arxiv.org/abs/2108.08839",
          "publishedOn": "2021-08-20T01:53:51.301Z",
          "wordCount": 663,
          "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers. (arXiv:2108.08839v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_U/0/1/0/all/0/1\">Utako Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakao_M/0/1/0/all/0/1\">Megumi Nakao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohzeki_M/0/1/0/all/0/1\">Masayuki Ohzeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokuno_J/0/1/0/all/0/1\">Junko Tokuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Yoshikawa_T/0/1/0/all/0/1\">Toyofumi Fengshi Chen-Yoshikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_T/0/1/0/all/0/1\">Tetsuya Matsuda</a>",
          "description": "In video-assisted thoracoscopic surgeries, successful procedures of nodule\nresection are highly dependent on the precise estimation of lung deformation\nbetween the inflated lung in the computed tomography (CT) images during\npreoperative planning and the deflated lung in the treatment views during\nsurgery. Lungs in the pneumothorax state during surgery have a large volume\nchange from normal lungs, making it difficult to build a mechanical model. The\npurpose of this study is to develop a deformation estimation method of the 3D\nsurface of a deflated lung from a few partial observations. To estimate\ndeformations for a largely deformed lung, a kernel regression-based solution\nwas introduced. The proposed method used a few landmarks to capture the partial\ndeformation between the 3D surface mesh obtained from preoperative CT and the\nintraoperative anatomical positions. The deformation for each vertex of the\nentire mesh model was estimated per-vertex as a relative position from the\nlandmarks. The landmarks were placed in the anatomical position of the lung's\nouter contour. The method was applied on nine datasets of the left lungs of\nlive Beagle dogs. Contrast-enhanced CT images of the lungs were acquired. The\nproposed method achieved a local positional error of vertices of 2.74 mm,\nHausdorff distance of 6.11 mm, and Dice similarity coefficient of 0.94.\nMoreover, the proposed method could estimate lung deformations from a small\nnumber of training cases and a small observation area. This study contributes\nto the data-driven modeling of pneumothorax deformation of the lung.",
          "link": "http://arxiv.org/abs/2102.12505",
          "publishedOn": "2021-08-20T01:53:51.294Z",
          "wordCount": 740,
          "title": "Kernel-based framework to estimate deformations of pneumothorax lung using relative position of anatomical landmarks. (arXiv:2102.12505v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McKee_D/0/1/0/all/0/1\">Daniel McKee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_B/0/1/0/all/0/1\">Bing Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berneshawi_A/0/1/0/all/0/1\">Andrew Berneshawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1\">Svetlana Lazebnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>",
          "description": "In this paper, we explore learning end-to-end deep neural trackers without\ntracking annotations. This is important as large-scale training data is\nessential for training deep neural trackers while tracking annotations are\nexpensive to acquire. In place of tracking annotations, we first hallucinate\nvideos from images with bounding box annotations using zoom-in/out motion\ntransformations to obtain free tracking labels. We add video simulation\naugmentations to create a diverse tracking dataset, albeit with simple motion.\nNext, to tackle harder tracking cases, we mine hard examples across an\nunlabeled pool of real videos with a tracker trained on our hallucinated video\ndata. For hard example mining, we propose an optimization-based connecting\nprocess to first identify and then rectify hard examples from the pool of\nunlabeled videos. Finally, we train our tracker jointly on hallucinated data\nand mined hard video examples. Our weakly supervised tracker achieves\nstate-of-the-art performance on the MOT17 and TAO-person datasets. On MOT17, we\nfurther demonstrate that the combination of our self-generated data and the\nexisting manually-annotated data leads to additional improvements.",
          "link": "http://arxiv.org/abs/2108.08836",
          "publishedOn": "2021-08-20T01:53:51.286Z",
          "wordCount": 611,
          "title": "Multi-Object Tracking with Hallucinated and Unlabeled Videos. (arXiv:2108.08836v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaman_A/0/1/0/all/0/1\">Anadi Chaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belius_D/0/1/0/all/0/1\">David Belius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>",
          "description": "Convolutional neural networks (CNNs) have been tremendously successful in\nsolving imaging inverse problems. To understand their success, an effective\nstrategy is to construct simpler and mathematically more tractable\nconvolutional sparse coding (CSC) models that share essential ingredients with\nCNNs. Existing CSC methods, however, underperform leading CNNs in challenging\ninverse problems. We hypothesize that the performance gap may be attributed in\npart to how they process images at different spatial scales: While many CNNs\nuse multiscale feature representations, existing CSC models mostly rely on\nsingle-scale dictionaries. To close the performance gap, we thus propose a\nmultiscale convolutional dictionary structure. The proposed dictionary\nstructure is derived from the U-Net, arguably the most versatile and widely\nused CNN for image-to-image learning problems. We show that incorporating the\nproposed multiscale dictionary in an otherwise standard CSC framework yields\nperformance competitive with state-of-the-art CNNs across a range of\nchallenging inverse problems including CT and MRI reconstruction. Our work thus\ndemonstrates the effectiveness and scalability of the multiscale CSC approach\nin solving challenging inverse problems.",
          "link": "http://arxiv.org/abs/2011.12815",
          "publishedOn": "2021-08-20T01:53:51.274Z",
          "wordCount": 641,
          "title": "Learning Multiscale Convolutional Dictionaries for Image Reconstruction. (arXiv:2011.12815v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kaifeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>",
          "description": "Video Visual Relation Detection (VidVRD), has received significant attention\nof our community over recent years. In this paper, we apply the\nstate-of-the-art video object tracklet detection pipeline MEGA and deepSORT to\ngenerate tracklet proposals. Then we perform VidVRD in a tracklet-based manner\nwithout any pre-cutting operations. Specifically, we design a tracklet-based\nvisual Transformer. It contains a temporal-aware decoder which performs feature\ninteractions between the tracklets and learnable predicate query embeddings,\nand finally predicts the relations. Experimental results strongly demonstrate\nthe superiority of our method, which outperforms other methods by a large\nmargin on the Video Relation Understanding (VRU) Grand Challenge in ACM\nMultimedia 2021. Codes are released at\nhttps://github.com/Dawn-LX/VidVRD-tracklets.",
          "link": "http://arxiv.org/abs/2108.08669",
          "publishedOn": "2021-08-20T01:53:51.267Z",
          "wordCount": 561,
          "title": "Video Relation Detection via Tracklet based Visual Transformer. (arXiv:2108.08669v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00081",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Barzekar_H/0/1/0/all/0/1\">Hosein Barzekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zeyun Yu</a>",
          "description": "Cancers are the leading cause of death in many countries. Early diagnosis\nplays a crucial role in having proper treatment for this debilitating disease.\nThe automated classification of the type of cancer is a challenging task since\npathologists must examine a huge number of histopathological images to detect\ninfinitesimal abnormalities. In this study, we propose a novel convolutional\nneural network (CNN) architecture composed of a Concatenation of multiple\nNetworks, called C-Net, to classify biomedical images. The model incorporates\nmultiple CNNs including Outer, Middle, and Inner. The first two parts of the\narchitecture contain six networks that serve as feature extractors to feed into\nthe Inner network to classify the images in terms of malignancy and benignancy.\nThe C-Net is applied for histopathological image classification on two public\ndatasets, including BreakHis and Osteosarcoma. To evaluate the performance, the\nmodel is tested using several evaluation metrics for its reliability. The C-Net\nmodel outperforms all other models on the individual metrics for both datasets\nand achieves zero misclassification. This approach has the potential to be\nextended to additional classification tasks, as experimental results\ndemonstrate utilizing extensive evaluation metrics.",
          "link": "http://arxiv.org/abs/2011.00081",
          "publishedOn": "2021-08-20T01:53:51.249Z",
          "wordCount": 647,
          "title": "C-Net: A Reliable Convolutional Neural Network for Biomedical Image Classification. (arXiv:2011.00081v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1310.1869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkova_K/0/1/0/all/0/1\">Katya Tsvetkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_M/0/1/0/all/0/1\">Milcho Tsvetkov</a>",
          "description": "We want to approximate the mxn image A from scanned astronomical photographic\nplates (from the Sofia Sky Archive Data Center) by using far fewer entries than\nin the original matrix. By using rank of a matrix, k we remove the redundant\ninformation or noise and use as Wiener filter, when rank k<m or k<n. With this\napproximation more than 98% compression ration of image of astronomical plate\nwithout that image details, is obtained. The SVD of images from scanned\nphotographic plates (SPP) is considered and its possible image compression.",
          "link": "http://arxiv.org/abs/1310.1869",
          "publishedOn": "2021-08-20T01:53:51.242Z",
          "wordCount": 616,
          "title": "Singular Value Decomposition of Images from Scanned Photographic Plates. (arXiv:1310.1869v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.04390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huijuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "Meta-learning has been the most common framework for few-shot learning in\nrecent years. It learns the model from collections of few-shot classification\ntasks, which is believed to have a key advantage of making the training\nobjective consistent with the testing objective. However, some recent works\nreport that by training for whole-classification, i.e. classification on the\nwhole label-set, it can get comparable or even better embedding than many\nmeta-learning algorithms. The edge between these two lines of works has yet\nbeen underexplored, and the effectiveness of meta-learning in few-shot learning\nremains unclear. In this paper, we explore a simple process: meta-learning over\na whole-classification pre-trained model on its evaluation metric. We observe\nthis simple method achieves competitive performance to state-of-the-art methods\non standard benchmarks. Our further analysis shed some light on understanding\nthe trade-offs between the meta-learning objective and the whole-classification\nobjective in few-shot learning.",
          "link": "http://arxiv.org/abs/2003.04390",
          "publishedOn": "2021-08-20T01:53:51.235Z",
          "wordCount": 639,
          "title": "Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning. (arXiv:2003.04390v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Jaesung Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1\">Sunghoon Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minjun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "To reconstruct a 3D scene from a set of calibrated views, traditional\nmulti-view stereo techniques rely on two distinct stages: local depth maps\ncomputation and global depth maps fusion. Recent studies concentrate on deep\nneural architectures for depth estimation by using conventional depth fusion\nmethod or direct 3D reconstruction network by regressing Truncated Signed\nDistance Function (TSDF). In this paper, we advocate that replicating the\ntraditional two stages framework with deep neural networks improves both the\ninterpretability and the accuracy of the results. As mentioned, our network\noperates in two steps: 1) the local computation of the local depth maps with a\ndeep MVS technique, and, 2) the depth maps and images' features fusion to build\na single TSDF volume. In order to improve the matching performance between\nimages acquired from very different viewpoints (e.g., large-baseline and\nrotations), we introduce a rotation-invariant 3D convolution kernel called\nPosedConv. The effectiveness of the proposed architecture is underlined via a\nlarge series of experiments conducted on the ScanNet dataset where our approach\ncompares favorably against both traditional and deep learning techniques.",
          "link": "http://arxiv.org/abs/2108.08623",
          "publishedOn": "2021-08-20T01:53:51.229Z",
          "wordCount": 621,
          "title": "VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction. (arXiv:2108.08623v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>",
          "description": "Category-level 6D pose estimation, aiming to predict the location and\norientation of unseen object instances, is fundamental to many scenarios such\nas robotic manipulation and augmented reality, yet still remains unsolved.\nPrecisely recovering instance 3D model in the canonical space and accurately\nmatching it with the observation is an essential point when estimating 6D pose\nfor unseen objects. In this paper, we achieve accurate category-level 6D pose\nestimation via cascaded relation and recurrent reconstruction networks.\nSpecifically, a novel cascaded relation network is dedicated for advanced\nrepresentation learning to explore the complex and informative relations among\ninstance RGB image, instance point cloud and category shape prior. Furthermore,\nwe design a recurrent reconstruction network for iterative residual refinement\nto progressively improve the reconstruction and correspondence estimations from\ncoarse to fine. Finally, the instance 6D pose is obtained leveraging the\nestimated dense correspondences between the instance point cloud and the\nreconstructed 3D model in the canonical space. We have conducted extensive\nexperiments on two well-acknowledged benchmarks of category-level 6D pose\nestimation, with significant performance improvement over existing approaches.\nOn the representatively strict evaluation metrics of $3D_{75}$ and $5^{\\circ}2\ncm$, our method exceeds the latest state-of-the-art SPD by $4.9\\%$ and $17.7\\%$\non the CAMERA25 dataset, and by $2.7\\%$ and $8.5\\%$ on the REAL275 dataset.\nCodes are available at https://wangjiaze.cn/projects/6DPoseEstimation.html.",
          "link": "http://arxiv.org/abs/2108.08755",
          "publishedOn": "2021-08-20T01:53:51.198Z",
          "wordCount": 663,
          "title": "Category-Level 6D Object Pose Estimation via Cascaded Relation and Recurrent Reconstruction Networks. (arXiv:2108.08755v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.03744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "The recently proposed SNLI-VE corpus for recognising visual-textual\nentailment is a large, real-world dataset for fine-grained multimodal\nreasoning. However, the automatic way in which SNLI-VE has been assembled (via\ncombining parts of two related datasets) gives rise to a large number of errors\nin the labels of this corpus. In this paper, we first present a data collection\neffort to correct the class with the highest error rate in SNLI-VE. Secondly,\nwe re-evaluate an existing model on the corrected corpus, which we call\nSNLI-VE-2.0, and provide a quantitative comparison with its performance on the\nnon-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends\nhuman-written natural language explanations to SNLI-VE-2.0. Finally, we train\nmodels that learn from these explanations at training time, and output such\nexplanations at testing time.",
          "link": "http://arxiv.org/abs/2004.03744",
          "publishedOn": "2021-08-20T01:53:51.191Z",
          "wordCount": 621,
          "title": "e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations. (arXiv:2004.03744v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.08878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to adapt existing models of the\nsource domain to a new target domain with only unlabeled data. Most existing\nmethods suffer from noticeable negative transfer resulting from either the\nerror-prone discriminator network or the unreasonable teacher model. Besides,\nthe local regional consistency in UDA has been largely neglected, and only\nextracting the global-level pattern information is not powerful enough for\nfeature alignment due to the abuse use of contexts. To this end, we propose an\nuncertainty-aware consistency regularization method for cross-domain semantic\nsegmentation. Firstly, we introduce an uncertainty-guided consistency loss with\na dynamic weighting scheme by exploiting the latent uncertainty information of\nthe target samples. As such, more meaningful and reliable knowledge from the\nteacher model can be transferred to the student model. We further reveal the\nreason why the current consistency regularization is often unstable in\nminimizing the domain discrepancy. Besides, we design a ClassDrop mask\ngeneration algorithm to produce strong class-wise perturbations. Guided by this\nmask, we propose a ClassOut strategy to realize effective regional consistency\nin a fine-grained manner. Experiments demonstrate that our method outperforms\nthe state-of-the-art methods on four domain adaptation benchmarks, i.e., GTAV\n$\\rightarrow $ Cityscapes and SYNTHIA $\\rightarrow $ Cityscapes, Virtual KITTI\n$\\rightarrow$ KITTI and Cityscapes $\\rightarrow$ KITTI.",
          "link": "http://arxiv.org/abs/2004.08878",
          "publishedOn": "2021-08-20T01:53:51.184Z",
          "wordCount": 696,
          "title": "Uncertainty-Aware Consistency Regularization for Cross-Domain Semantic Segmentation. (arXiv:2004.08878v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_A/0/1/0/all/0/1\">Ansheng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>",
          "description": "Adaptive and flexible image editing is a desirable function of modern\ngenerative models. In this work, we present a generative model with\nauto-encoder architecture for per-region style manipulation. We apply a code\nconsistency loss to enforce an explicit disentanglement between content and\nstyle latent representations, making the content and style of generated samples\nconsistent with their corresponding content and style references. The model is\nalso constrained by a content alignment loss to ensure the foreground editing\nwill not interfere background contents. As a result, given interested region\nmasks provided by users, our model supports foreground region-wise style\ntransfer. Specially, our model receives no extra annotations such as semantic\nlabels except for self-supervision. Extensive experiments show the\neffectiveness of the proposed method and exhibit the flexibility of the\nproposed model for various applications, including region-wise style editing,\nlatent space interpolation, cross-domain style transfer.",
          "link": "http://arxiv.org/abs/2108.08674",
          "publishedOn": "2021-08-20T01:53:51.177Z",
          "wordCount": 581,
          "title": "Towards Controllable and Photorealistic Region-wise Image Manipulation. (arXiv:2108.08674v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Garvita Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1\">Nikolaos Sarafianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll1_G/0/1/0/all/0/1\">Gerard Pons-Moll1</a>",
          "description": "We present Neural Generalized Implicit Functions(Neural-GIF), to animate\npeople in clothing as a function of the body pose. Given a sequence of scans of\na subject in various poses, we learn to animate the character for new poses.\nExisting methods have relied on template-based representations of the human\nbody (or clothing). However such models usually have fixed and limited\nresolutions, require difficult data pre-processing steps and cannot be used\nwith complex clothing. We draw inspiration from template-based methods, which\nfactorize motion into articulation and non-rigid deformation, but generalize\nthis concept for implicit shape learning to obtain a more flexible model. We\nlearn to map every point in the space to a canonical space, where a learned\ndeformation field is applied to model non-rigid effects, before evaluating the\nsigned distance field. Our formulation allows the learning of complex and\nnon-rigid deformations of clothing and soft tissue, without computing a\ntemplate registration as it is common with current approaches. Neural-GIF can\nbe trained on raw 3D scans and reconstructs detailed complex surface geometry\nand deformations. Moreover, the model can generalize to new poses. We evaluate\nour method on a variety of characters from different public datasets in diverse\nclothing styles and show significant improvements over baseline methods,\nquantitatively and qualitatively. We also extend our model to multiple shape\nsetting. To stimulate further research, we will make the model, code and data\npublicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/",
          "link": "http://arxiv.org/abs/2108.08807",
          "publishedOn": "2021-08-20T01:53:51.170Z",
          "wordCount": 675,
          "title": "Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing. (arXiv:2108.08807v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1\">Maithra Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1\">Thomas Unterthiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1\">Alexey Dosovitskiy</a>",
          "description": "Convolutional neural networks (CNNs) have so far been the de-facto model for\nvisual data. Recent work has shown that (Vision) Transformer models (ViT) can\nachieve comparable or even superior performance on image classification tasks.\nThis raises a central question: how are Vision Transformers solving these\ntasks? Are they acting like convolutional networks, or learning entirely\ndifferent visual representations? Analyzing the internal representation\nstructure of ViTs and CNNs on image classification benchmarks, we find striking\ndifferences between the two architectures, such as ViT having more uniform\nrepresentations across all layers. We explore how these differences arise,\nfinding crucial roles played by self-attention, which enables early aggregation\nof global information, and ViT residual connections, which strongly propagate\nfeatures from lower to higher layers. We study the ramifications for spatial\nlocalization, demonstrating ViTs successfully preserve input spatial\ninformation, with noticeable effects from different classification methods.\nFinally, we study the effect of (pretraining) dataset scale on intermediate\nfeatures and transfer learning, and conclude with a discussion on connections\nto new architectures such as the MLP-Mixer.",
          "link": "http://arxiv.org/abs/2108.08810",
          "publishedOn": "2021-08-20T01:53:51.164Z",
          "wordCount": 622,
          "title": "Do Vision Transformers See Like Convolutional Neural Networks?. (arXiv:2108.08810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zixin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zeyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chiew-Lan Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_L/0/1/0/all/0/1\">Long Quan</a>",
          "description": "Matching local features across images is a fundamental problem in computer\nvision. Targeting towards high accuracy and efficiency, we propose Seeded Graph\nMatching Network, a graph neural network with sparse structure to reduce\nredundant connectivity and learn compact representation. The network consists\nof 1) Seeding Module, which initializes the matching by generating a small set\nof reliable matches as seeds. 2) Seeded Graph Neural Network, which utilizes\nseed matches to pass messages within/across images and predicts assignment\ncosts. Three novel operations are proposed as basic elements for message\npassing: 1) Attentional Pooling, which aggregates keypoint features within the\nimage to seed matches. 2) Seed Filtering, which enhances seed features and\nexchanges messages across images. 3) Attentional Unpooling, which propagates\nseed features back to original keypoints. Experiments show that our method\nreduces computational and memory complexity significantly compared with typical\nattention-based networks while competitive or higher performance is achieved.",
          "link": "http://arxiv.org/abs/2108.08771",
          "publishedOn": "2021-08-20T01:53:51.157Z",
          "wordCount": 606,
          "title": "Learning to Match Features with Seeded Graph Matching Network. (arXiv:2108.08771v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>",
          "description": "Attention module does not always help deep models learn causal features that\nare robust in any confounding context, e.g., a foreground object feature is\ninvariant to different backgrounds. This is because the confounders trick the\nattention to capture spurious correlations that benefit the prediction when the\ntraining and testing data are IID (identical & independent distribution); while\nharm the prediction when the data are OOD (out-of-distribution). The sole\nfundamental solution to learn causal attention is by causal intervention, which\nrequires additional annotations of the confounders, e.g., a \"dog\" model is\nlearned within \"grass+dog\" and \"road+dog\" respectively, so the \"grass\" and\n\"road\" contexts will no longer confound the \"dog\" recognition. However, such\nannotation is not only prohibitively expensive, but also inherently\nproblematic, as the confounders are elusive in nature. In this paper, we\npropose a causal attention module (CaaM) that self-annotates the confounders in\nunsupervised fashion. In particular, multiple CaaMs can be stacked and\nintegrated in conventional attention CNN and self-attention Vision Transformer.\nIn OOD settings, deep models with CaaM outperform those without it\nsignificantly; even in IID settings, the attention localization is also\nimproved by CaaM, showing a great potential in applications that require robust\nvisual saliency. Codes are available at \\url{https://github.com/Wangt-CN/CaaM}.",
          "link": "http://arxiv.org/abs/2108.08782",
          "publishedOn": "2021-08-20T01:53:51.150Z",
          "wordCount": 640,
          "title": "Causal Attention for Unbiased Visual Recognition. (arXiv:2108.08782v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1\">Cong Hua</a>",
          "description": "For a given video-based Human-Object Interaction scene, modeling the\nspatio-temporal relationship between humans and objects are the important cue\nto understand the contextual information presented in the video. With the\neffective spatio-temporal relationship modeling, it is possible not only to\nuncover contextual information in each frame but also to directly capture\ninter-time dependencies. It is more critical to capture the position changes of\nhuman and objects over the spatio-temporal dimension when their appearance\nfeatures may not show up significant changes over time. The full use of\nappearance features, the spatial location and the semantic information are also\nthe key to improve the video-based Human-Object Interaction recognition\nperformance. In this paper, Spatio-Temporal Interaction Graph Parsing Networks\n(STIGPN) are constructed, which encode the videos with a graph composed of\nhuman and object nodes. These nodes are connected by two types of relations:\n(i) spatial relations modeling the interactions between human and the\ninteracted objects within each frame. (ii) inter-time relations capturing the\nlong range dependencies between human and the interacted objects across frame.\nWith the graph, STIGPN learn spatio-temporal features directly from the whole\nvideo-based Human-Object Interaction scenes. Multi-modal features and a\nmulti-stream fusion strategy are used to enhance the reasoning capability of\nSTIGPN. Two Human-Object Interaction video datasets, including CAD-120 and\nSomething-Else, are used to evaluate the proposed architectures, and the\nstate-of-the-art performance demonstrates the superiority of STIGPN.",
          "link": "http://arxiv.org/abs/2108.08633",
          "publishedOn": "2021-08-20T01:53:51.120Z",
          "wordCount": 677,
          "title": "Spatio-Temporal Interaction Graph Parsing Networks for Human-Object Interaction Recognition. (arXiv:2108.08633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hyunyoung Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1\">Eunhyeok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sungjoo Yoo</a>",
          "description": "Self-supervised monocular depth estimation has been widely studied, owing to\nits practical importance and recent promising improvements. However, most works\nsuffer from limited supervision of photometric consistency, especially in weak\ntexture regions and at object boundaries. To overcome this weakness, we propose\nnovel ideas to improve self-supervised monocular depth estimation by leveraging\ncross-domain information, especially scene semantics. We focus on incorporating\nimplicit semantic knowledge into geometric representation enhancement and\nsuggest two ideas: a metric learning approach that exploits the\nsemantics-guided local geometry to optimize intermediate depth representations\nand a novel feature fusion module that judiciously utilizes cross-modality\nbetween two heterogeneous feature representations. We comprehensively evaluate\nour methods on the KITTI dataset and demonstrate that our method outperforms\nstate-of-the-art methods. The source code is available at\nhttps://github.com/hyBlue/FSRE-Depth.",
          "link": "http://arxiv.org/abs/2108.08829",
          "publishedOn": "2021-08-20T01:53:51.103Z",
          "wordCount": 568,
          "title": "Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation. (arXiv:2108.08829v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1010.4059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>",
          "description": "This article is about the architecture of a lossless wavelet filter bank with\nreprogrammable logic. It is based on second generation of wavelets with a\nreduced of number of operations. A new basic structure for parallel\narchitecture and modules to forward and backward integer discrete wavelet\ntransform is proposed.",
          "link": "http://arxiv.org/abs/1010.4059",
          "publishedOn": "2021-08-20T01:53:51.083Z",
          "wordCount": 553,
          "title": "Multiplierless Modules for Forward and Backward Integer Wavelet Transform. (arXiv:1010.4059v3 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1\">Tzu-Yi Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>",
          "description": "We address the challenging task of few-shot segmentation in this work. It is\nessential for few-shot semantic segmentation to fully utilize the support\ninformation. Previous methods typically adapt masked average pooling over the\nsupport feature to extract the support clues as a global vector, usually\ndominated by the salient part and loses some important clues. In this work, we\nargue that every support pixel's information is desired to be transferred to\nall query pixels and propose a Correspondence Matching Network (CMNet) with an\nOptimal Transport Matching module to mine out the correspondence between the\nquery and support images. Besides, it is important to fully utilize both local\nand global information from the annotated support images. To this end, we\npropose a Message Flow module to propagate the message along the inner-flow\nwithin the same image and cross-flow between support and query images, which\ngreatly help enhance the local feature representations. We further address the\nfew-shot segmentation as a multi-task learning problem to alleviate the domain\ngap issue between different datasets. Experiments on PASCAL VOC 2012, MS COCO,\nand FSS-1000 datasets show that our network achieves new state-of-the-art\nfew-shot segmentation performance.",
          "link": "http://arxiv.org/abs/2108.08518",
          "publishedOn": "2021-08-20T01:53:51.064Z",
          "wordCount": 630,
          "title": "Few-shot Segmentation with Optimal Transport Matching and Message Flow. (arXiv:2108.08518v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.00618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie P. Kaelbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Alberto Rodriguez</a>",
          "description": "Pushing is a fundamental robotic skill. Existing work has shown how to\nexploit models of pushing to achieve a variety of tasks, including grasping\nunder uncertainty, in-hand manipulation and clearing clutter. Such models,\nhowever, are approximate, which limits their applicability. Learning-based\nmethods can reason directly from raw sensory data with accuracy, and have the\npotential to generalize to a wider diversity of scenarios. However, developing\nand testing such methods requires rich-enough datasets. In this paper we\nintroduce Omnipush, a dataset with high variety of planar pushing behavior. In\nparticular, we provide 250 pushes for each of 250 objects, all recorded with\nRGB-D and a high precision tracking system. The objects are constructed so as\nto systematically explore key factors that affect pushing -- the shape of the\nobject and its mass distribution -- which have not been broadly explored in\nprevious datasets, and allow to study generalization in model learning.\nOmnipush includes a benchmark for meta-learning dynamic models, which requires\nalgorithms that make good predictions and estimate their own uncertainty. We\nalso provide an RGB video prediction benchmark and propose other relevant tasks\nthat can be suited with this dataset.\n\nData and code are available at\n\\url{https://web.mit.edu/mcube/omnipush-dataset/}.",
          "link": "http://arxiv.org/abs/1910.00618",
          "publishedOn": "2021-08-20T01:53:51.058Z",
          "wordCount": 697,
          "title": "Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video. (arXiv:1910.00618v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1\">Juhi Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_L/0/1/0/all/0/1\">Lagan Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhiman_H/0/1/0/all/0/1\">Harsh S. Dhiman</a>",
          "description": "In this manuscript, an image analytics based deep learning framework for wind\nturbine blade surface damage detection is proposed. Turbine blade(s) which\ncarry approximately one-third of a turbine weight are susceptible to damage and\ncan cause sudden malfunction of a grid-connected wind energy conversion system.\nThe surface damage detection of wind turbine blade requires a large dataset so\nas to detect a type of damage at an early stage. Turbine blade images are\ncaptured via aerial imagery. Upon inspection, it is found that the image\ndataset was limited and hence image augmentation is applied to improve blade\nimage dataset. The approach is modeled as a multi-class supervised learning\nproblem and deep learning methods like Convolutional neural network (CNN),\nVGG16-RCNN and AlexNet are tested for determining the potential capability of\nturbine blade surface damage.",
          "link": "http://arxiv.org/abs/2108.08636",
          "publishedOn": "2021-08-20T01:53:51.051Z",
          "wordCount": 591,
          "title": "Wind Turbine Blade Surface Damage Detection based on Aerial Imagery and VGG16-RCNN Framework. (arXiv:2108.08636v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Huanhuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuchen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xudong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jijunnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>",
          "description": "Accurate visual re-localization is very critical to many artificial\nintelligence applications, such as augmented reality, virtual reality, robotics\nand autonomous driving. To accomplish this task, we propose an integrated\nvisual re-localization method called RLOCS by combining image retrieval,\nsemantic consistency and geometry verification to achieve accurate estimations.\nThe localization pipeline is designed as a coarse-to-fine paradigm. In the\nretrieval part, we cascade the architecture of ResNet101-GeM-ArcFace and employ\nDBSCAN followed by spatial verification to obtain a better initial coarse pose.\nWe design a module called observation constraints, which combines geometry\ninformation and semantic consistency for filtering outliers. Comprehensive\nexperiments are conducted on open datasets, including retrieval on R-Oxford5k\nand R-Paris6k, semantic segmentation on Cityscapes, localization on Aachen\nDay-Night and InLoc. By creatively modifying separate modules in the total\npipeline, our method achieves many performance improvements on the challenging\nlocalization benchmarks.",
          "link": "http://arxiv.org/abs/2108.08516",
          "publishedOn": "2021-08-20T01:53:51.044Z",
          "wordCount": 592,
          "title": "Retrieval and Localization with Observation Constraints. (arXiv:2108.08516v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>",
          "description": "Scene graphs provide valuable information to many downstream tasks. Many\nscene graph generation (SGG) models solely use the limited annotated relation\ntriples for training, leading to their underperformance on low-shot (few and\nzero) scenarios, especially on the rare predicates. To address this problem, we\npropose a novel semantic compositional learning strategy that makes it possible\nto construct additional, realistic relation triples with objects from different\nimages. Specifically, our strategy decomposes a relation triple by identifying\nand removing the unessential component and composes a new relation triple by\nfusing with a semantically or visually similar object from a visual components\ndictionary, whilst ensuring the realisticity of the newly composed triple.\nNotably, our strategy is generic and can be combined with existing SGG models\nto significantly improve their performance. We performed a comprehensive\nevaluation on the benchmark dataset Visual Genome. For three recent SGG models,\nadding our strategy improves their performance by close to 50\\%, and all of\nthem substantially exceed the current state-of-the-art.",
          "link": "http://arxiv.org/abs/2108.08600",
          "publishedOn": "2021-08-20T01:53:51.037Z",
          "wordCount": 599,
          "title": "Semantic Compositional Learning for Low-shot Scene Graph Generation. (arXiv:2108.08600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1\">Patrick Esser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1\">Robin Rombach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1\">Andreas Blattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Autoregressive models and their sequential factorization of the data\nlikelihood have recently demonstrated great potential for image representation\nand synthesis. Nevertheless, they incorporate image context in a linear 1D\norder by attending only to previously synthesized image patches above or to the\nleft. Not only is this unidirectional, sequential bias of attention unnatural\nfor images as it disregards large parts of a scene until synthesis is almost\ncomplete. It also processes the entire image on a single scale, thus ignoring\nmore global contextual information up to the gist of the entire scene. As a\nremedy we incorporate a coarse-to-fine hierarchy of context by combining the\nautoregressive formulation with a multinomial diffusion process: Whereas a\nmultistage diffusion process successively removes information to coarsen an\nimage, we train a (short) Markov chain to invert this process. In each stage,\nthe resulting autoregressive ImageBART model progressively incorporates context\nfrom previous stages in a coarse-to-fine manner. Experiments show greatly\nimproved image modification capabilities over autoregressive models while also\nproviding high-fidelity image generation, both of which are enabled through\nefficient training in a compressed latent space. Specifically, our approach can\ntake unrestricted, user-provided masks into account to perform local image\nediting. Thus, in contrast to pure autoregressive models, it can solve\nfree-form image inpainting and, in the case of conditional models, local,\ntext-guided image modification without requiring mask-specific training.",
          "link": "http://arxiv.org/abs/2108.08827",
          "publishedOn": "2021-08-20T01:53:51.030Z",
          "wordCount": 664,
          "title": "ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis. (arXiv:2108.08827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neill_D/0/1/0/all/0/1\">Daniel B. Neill</a>",
          "description": "Machine learning is gaining popularity in a broad range of areas working with\ngeographic data, such as ecology or atmospheric sciences. Here, data often\nexhibit spatial effects, which can be difficult to learn for neural networks.\nIn this study, we propose SXL, a method for embedding information on the\nautoregressive nature of spatial data directly into the learning process using\nauxiliary tasks. We utilize the local Moran's I, a popular measure of local\nspatial autocorrelation, to \"nudge\" the model to learn the direction and\nmagnitude of local spatial effects, complementing the learning of the primary\ntask. We further introduce a novel expansion of Moran's I to multiple\nresolutions, thus capturing spatial interactions over longer and shorter\ndistances simultaneously. The novel multi-resolution Moran's I can be\nconstructed easily and as a multi-dimensional tensor offers seamless\nintegration into existing machine learning frameworks. Throughout a range of\nexperiments using real-world data, we highlight how our method consistently\nimproves the training of neural networks in unsupervised and supervised\nlearning tasks. In generative spatial modeling experiments, we propose a novel\nloss for auxiliary task GANs utilizing task uncertainty weights. Our proposed\nmethod outperforms domain-specific spatial interpolation benchmarks,\nhighlighting its potential for downstream applications. This study bridges\nexpertise from geographic information science and machine learning, showing how\nthis integration of disciplines can help to address domain-specific challenges.\nThe code for our experiments is available on Github:\nhttps://github.com/konstantinklemmer/sxl.",
          "link": "http://arxiv.org/abs/2006.10461",
          "publishedOn": "2021-08-20T01:53:51.003Z",
          "wordCount": 712,
          "title": "Auxiliary-task learning for geographic data with autoregressive embeddings. (arXiv:2006.10461v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_B/0/1/0/all/0/1\">Banglei Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Ji Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>",
          "description": "We propose three novel solvers for estimating the relative pose of a\nmulti-camera system from affine correspondences (ACs). A new constraint is\nderived interpreting the relationship of ACs and the generalized camera model.\nUsing the constraint, we demonstrate efficient solvers for two types of motions\nassumed. Considering that the cameras undergo planar motion, we propose a\nminimal solution using a single AC and a solver with two ACs to overcome the\ndegenerate case. Also, we propose a minimal solution using two ACs with known\nvertical direction, e.g., from an IMU. Since the proposed methods require\nsignificantly fewer correspondences than state-of-the-art algorithms, they can\nbe efficiently used within RANSAC for outlier removal and initial motion\nestimation. The solvers are tested both on synthetic data and on real-world\nscenes from the KITTI odometry benchmark. It is shown that the accuracy of the\nestimated poses is superior to the state-of-the-art techniques.",
          "link": "http://arxiv.org/abs/2007.10700",
          "publishedOn": "2021-08-20T01:53:50.994Z",
          "wordCount": 624,
          "title": "Minimal Cases for Computing the Generalized Relative Pose using Affine Correspondences. (arXiv:2007.10700v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bateni_P/0/1/0/all/0/1\">Peyman Bateni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_J/0/1/0/all/0/1\">Jarred Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "We develop a transductive meta-learning method that uses unlabelled instances\nto improve few-shot image classification performance. Our approach combines a\nregularized Mahalanobis-distance-based soft k-means clustering procedure with a\nmodified state of the art neural adaptive feature extractor to achieve improved\ntest-time classification accuracy using unlabelled data. We evaluate our method\non transductive few-shot learning tasks, in which the goal is to jointly\npredict labels for query (test) examples given a set of support (training)\nexamples. We achieve state-of-the-art performance on the Meta-Dataset,\nmini-ImageNet and tiered-ImageNet benchmarks.",
          "link": "http://arxiv.org/abs/2006.12245",
          "publishedOn": "2021-08-20T01:53:50.988Z",
          "wordCount": 587,
          "title": "Enhancing Few-Shot Image Classification with Unlabelled Examples. (arXiv:2006.12245v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08505",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weixia Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_M/0/1/0/all/0/1\">Meng Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xianpei Wang</a>",
          "description": "Perceptual quality assessment of the videos acquired in the wilds is of vital\nimportance for quality assurance of video services. The inaccessibility of\nreference videos with pristine quality and the complexity of authentic\ndistortions pose great challenges for this kind of blind video quality\nassessment (BVQA) task. Although model-based transfer learning is an effective\nand efficient paradigm for the BVQA task, it remains to be a challenge to\nexplore what and how to bridge the domain shifts for better video\nrepresentation. In this work, we propose to transfer knowledge from image\nquality assessment (IQA) databases with authentic distortions and large-scale\naction recognition with rich motion patterns. We rely on both groups of data to\nlearn the feature extractor. We train the proposed model on the target VQA\ndatabases using a mixed list-wise ranking loss function. Extensive experiments\non six databases demonstrate that our method performs very competitively under\nboth individual database and mixed database training settings. We also verify\nthe rationality of each component of the proposed method and explore a simple\nmanner for further improvement.",
          "link": "http://arxiv.org/abs/2108.08505",
          "publishedOn": "2021-08-20T01:53:50.981Z",
          "wordCount": 630,
          "title": "Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception. (arXiv:2108.08505v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims\nat inferring novel object categories in an unlabeled set by leveraging from\nprior knowledge of a labeled set containing different, but related classes.\nExisting approaches tackle this problem by considering multiple objective\nfunctions, usually involving specialized loss terms for the labeled and the\nunlabeled samples respectively, and often requiring auxiliary regularization\nterms. In this paper, we depart from this traditional scheme and introduce a\nUNified Objective function (UNO) for discovering novel classes, with the\nexplicit purpose of favoring synergy between supervised and unsupervised\nlearning. Using a multi-view self-labeling strategy, we generate pseudo-labels\nthat can be treated homogeneously with ground truth labels. This leads to a\nsingle classification objective operating on both known and unknown classes.\nDespite its simplicity, UNO outperforms the state of the art by a significant\nmargin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The\nproject page is available at: \\url{https://ncd-uno.github.io}.",
          "link": "http://arxiv.org/abs/2108.08536",
          "publishedOn": "2021-08-20T01:53:50.975Z",
          "wordCount": 608,
          "title": "A Unified Objective for Novel Class Discovery. (arXiv:2108.08536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pochuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1\">Holger R. Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oda_M/0/1/0/all/0/1\">Masahiro Oda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weichung Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuh_C/0/1/0/all/0/1\">Chiou-Shann Fuh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Po-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kao-Lang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wei-Chih Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>",
          "description": "Federated learning (FL) for medical image segmentation becomes more\nchallenging in multi-task settings where clients might have different\ncategories of labels represented in their data. For example, one client might\nhave patient data with \"healthy'' pancreases only while datasets from other\nclients may contain cases with pancreatic tumors. The vanilla federated\naveraging algorithm makes it possible to obtain more generalizable deep\nlearning-based segmentation models representing the training data from multiple\ninstitutions without centralizing datasets. However, it might be sub-optimal\nfor the aforementioned multi-task scenarios. In this paper, we investigate\nheterogeneous optimization methods that show improvements for the automated\nsegmentation of pancreas and pancreatic tumors in abdominal CT images with FL\nsettings.",
          "link": "http://arxiv.org/abs/2108.08537",
          "publishedOn": "2021-08-20T01:53:50.957Z",
          "wordCount": 571,
          "title": "Multi-task Federated Learning for Heterogeneous Pancreas Segmentation. (arXiv:2108.08537v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shivapuja_S/0/1/0/all/0/1\">Sravya Vardhani Shivapuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamkar_M/0/1/0/all/0/1\">Mansi Pradeep Khamkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_D/0/1/0/all/0/1\">Divij Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "Datasets for training crowd counting deep networks are typically heavy-tailed\nin count distribution and exhibit discontinuities across the count range. As a\nresult, the de facto statistical measures (MSE, MAE) exhibit large variance and\ntend to be unreliable indicators of performance across the count range. To\naddress these concerns in a holistic manner, we revise processes at various\nstages of the standard crowd counting pipeline. To enable principled and\nbalanced minibatch sampling, we propose a novel smoothed Bayesian sample\nstratification approach. We propose a novel cost function which can be readily\nincorporated into existing crowd counting deep networks to encourage\nstrata-aware optimization. We analyze the performance of representative crowd\ncounting approaches across standard datasets at per strata level and in\naggregate. We analyze the performance of crowd counting approaches across\nstandard datasets and demonstrate that our proposed modifications noticeably\nreduce error standard deviation. Our contributions represent a nuanced,\nstatistically balanced and fine-grained characterization of performance for\ncrowd counting approaches. Code, pretrained models and interactive\nvisualizations can be viewed at our project page https://deepcount.iiit.ac.in/",
          "link": "http://arxiv.org/abs/2108.08784",
          "publishedOn": "2021-08-20T01:53:50.941Z",
          "wordCount": 652,
          "title": "Wisdom of (Binned) Crowds: A Bayesian Stratification Paradigm for Crowd Counting. (arXiv:2108.08784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08305",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1\">Lichuan Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1\">Royson Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mohamed S. Abdelfattah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_H/0/1/0/all/0/1\">Hongkai Wen</a>",
          "description": "Deep learning-based blind super-resolution (SR) methods have recently\nachieved unprecedented performance in upscaling frames with unknown\ndegradation. These models are able to accurately estimate the unknown\ndownscaling kernel from a given low-resolution (LR) image in order to leverage\nthe kernel during restoration. Although these approaches have largely been\nsuccessful, they are predominantly image-based and therefore do not exploit the\ntemporal properties of the kernels across multiple video frames. In this paper,\nwe investigated the temporal properties of the kernels and highlighted its\nimportance in the task of blind video super-resolution. Specifically, we\nmeasured the kernel temporal consistency of real-world videos and illustrated\nhow the estimated kernels might change per frame in videos of varying\ndynamicity of the scene and its objects. With this new insight, we revisited\nprevious popular video SR approaches, and showed that previous assumptions of\nusing a fixed kernel throughout the restoration process can lead to visual\nartifacts when upscaling real-world videos. In order to counteract this, we\ntailored existing single-image and video SR techniques to leverage kernel\nconsistency during both kernel estimation and video upscaling processes.\nExtensive experiments on synthetic and real-world videos show substantial\nrestoration gains quantitatively and qualitatively, achieving the new\nstate-of-the-art in blind video SR and underlining the potential of exploiting\nkernel temporal consistency.",
          "link": "http://arxiv.org/abs/2108.08305",
          "publishedOn": "2021-08-20T01:53:50.934Z",
          "wordCount": 665,
          "title": "Temporal Kernel Consistency for Blind Video Super-Resolution. (arXiv:2108.08305v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Attention mechanism has demonstrated great potential in fine-grained visual\nrecognition tasks. In this paper, we present a counterfactual attention\nlearning method to learn more effective attention based on causal inference.\nUnlike most existing methods that learn visual attention based on conventional\nlikelihood, we propose to learn the attention with counterfactual causality,\nwhich provides a tool to measure the attention quality and a powerful\nsupervisory signal to guide the learning process. Specifically, we analyze the\neffect of the learned visual attention on network prediction through\ncounterfactual intervention and maximize the effect to encourage the network to\nlearn more useful attention for fine-grained image recognition. Empirically, we\nevaluate our method on a wide range of fine-grained recognition tasks where\nattention plays a crucial role, including fine-grained image categorization,\nperson re-identification, and vehicle re-identification. The consistent\nimprovement on all benchmarks demonstrates the effectiveness of our method.\nCode is available at https://github.com/raoyongming/CAL",
          "link": "http://arxiv.org/abs/2108.08728",
          "publishedOn": "2021-08-20T01:53:50.925Z",
          "wordCount": 601,
          "title": "Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification. (arXiv:2108.08728v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1\">Aihua Zheng</a>",
          "description": "Recently, Batch DropBlock network (BDB) has demonstrated its effectiveness on\nperson image representation and re-identification task via feature erasing.\nHowever, BDB drops the features \\textbf{randomly} which may lead to sub-optimal\nresults. In this paper, we propose a novel Self-Thresholding attention guided\nAdaptive DropBlock network (STADB) for person re-ID which can\n\\textbf{adaptively} erase the most discriminative regions. Specifically, STADB\nfirst obtains an attention map by channel-wise pooling and returns a drop mask\nby thresholding the attention map. Then, the input features and\nself-thresholding attention guided drop mask are multiplied to generate the\ndropped feature maps. In addition, STADB utilizes the spatial and channel\nattention to learn a better feature map and iteratively trains the feature\ndropping module for person re-ID. Experiments on several benchmark datasets\ndemonstrate that the proposed STADB outperforms many other related methods for\nperson re-ID. The source code of this paper is released at:\n\\textcolor{red}{\\url{https://github.com/wangxiao5791509/STADB_ReID}}.",
          "link": "http://arxiv.org/abs/2007.03584",
          "publishedOn": "2021-08-20T01:53:50.898Z",
          "wordCount": 627,
          "title": "STADB: A Self-Thresholding Attention Guided ADB Network for Person Re-identification. (arXiv:2007.03584v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiawu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1\">Teng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Despite superior performance on many computer vision tasks, deep convolution\nneural networks are well known to be compressed on devices that have resource\nconstraints. Most existing network pruning methods require laborious human\nefforts and prohibitive computation resources, especially when the constraints\nare changed. This practically limits the application of model compression when\nthe model needs to be deployed on a wide range of devices. Besides, existing\nmethods are still challenged by the missing theoretical guidance. In this paper\nwe propose an information theory-inspired strategy for automatic model\ncompression. The principle behind our method is the information bottleneck\ntheory, i.e., the hidden representation should compress information with each\nother. We thus introduce the normalized Hilbert-Schmidt Independence Criterion\n(nHSIC) on network activations as a stable and generalized indicator of layer\nimportance. When a certain resource constraint is given, we integrate the HSIC\nindicator with the constraint to transform the architecture search problem into\na linear programming problem with quadratic constraints. Such a problem is\neasily solved by a convex optimization method with a few seconds. We also\nprovide a rigorous proof to reveal that optimizing the normalized HSIC\nsimultaneously minimizes the mutual information between different layers.\nWithout any search process, our method achieves better compression tradeoffs\ncomparing to the state-of-the-art compression algorithms. For instance, with\nResNet-50, we achieve a 45.3%-FLOPs reduction, with a 75.75 top-1 accuracy on\nImageNet. Codes are avaliable at\nhttps://github.com/MAC-AutoML/ITPruner/tree/master.",
          "link": "http://arxiv.org/abs/2108.08532",
          "publishedOn": "2021-08-20T01:53:50.891Z",
          "wordCount": 679,
          "title": "An Information Theory-inspired Strategy for Automatic Network Pruning. (arXiv:2108.08532v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yavartanoo_M/0/1/0/all/0/1\">Mohsen Yavartanoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">JaeYoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neshatavar_R/0/1/0/all/0/1\">Reyhaneh Neshatavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>",
          "description": "3D Shape representation has substantial effects on 3D shape reconstruction.\nPrimitive-based representations approximate a 3D shape mainly by a set of\nsimple implicit primitives, but the low geometrical complexity of the\nprimitives limits the shape resolution. Moreover, setting a sufficient number\nof primitives for an arbitrary shape is challenging. To overcome these issues,\nwe propose a constrained implicit algebraic surface as the primitive with few\nlearnable coefficients and higher geometrical complexities and a deep neural\nnetwork to produce these primitives. Our experiments demonstrate the\nsuperiorities of our method in terms of representation power compared to the\nstate-of-the-art methods in single RGB image 3D shape reconstruction.\nFurthermore, we show that our method can semantically learn segments of 3D\nshapes in an unsupervised manner. The code is publicly available from\nhttps://myavartanoo.github.io/3dias/ .",
          "link": "http://arxiv.org/abs/2108.08653",
          "publishedOn": "2021-08-20T01:53:50.883Z",
          "wordCount": 573,
          "title": "3DIAS: 3D Shape Reconstruction with Implicit Algebraic Surfaces. (arXiv:2108.08653v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ardino_P/0/1/0/all/0/1\">Pierfrancesco Ardino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1\">Marco De Nadai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>",
          "description": "This paper introduces Click to Move (C2M), a novel framework for video\ngeneration where the user can control the motion of the synthesized video\nthrough mouse clicks specifying simple object trajectories of the key objects\nin the scene. Our model receives as input an initial frame, its corresponding\nsegmentation map and the sparse motion vectors encoding the input provided by\nthe user. It outputs a plausible video sequence starting from the given frame\nand with a motion that is consistent with user input. Notably, our proposed\ndeep architecture incorporates a Graph Convolution Network (GCN) modelling the\nmovements of all the objects in the scene in a holistic manner and effectively\ncombining the sparse user motion information and image features. Experimental\nresults show that C2M outperforms existing methods on two publicly available\ndatasets, thus demonstrating the effectiveness of our GCN framework at\nmodelling object interactions. The source code is publicly available at\nhttps://github.com/PierfrancescoArdino/C2M.",
          "link": "http://arxiv.org/abs/2108.08815",
          "publishedOn": "2021-08-20T01:53:50.877Z",
          "wordCount": 610,
          "title": "Click to Move: Controlling Video Generation with Sparse Motion. (arXiv:2108.08815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Danping Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenxian Yu</a>",
          "description": "Self-supervised monocular depth estimation has achieved impressive\nperformance on outdoor datasets. Its performance however degrades notably in\nindoor environments because of the lack of textures. Without rich textures, the\nphotometric consistency is too weak to train a good depth network. Inspired by\nthe early works on indoor modeling, we leverage the structural regularities\nexhibited in indoor scenes, to train a better depth network. Specifically, we\nadopt two extra supervisory signals for self-supervised training: 1) the\nManhattan normal constraint and 2) the co-planar constraint. The Manhattan\nnormal constraint enforces the major surfaces (the floor, ceiling, and walls)\nto be aligned with dominant directions. The co-planar constraint states that\nthe 3D points be well fitted by a plane if they are located within the same\nplanar region. To generate the supervisory signals, we adopt two components to\nclassify the major surface normal into dominant directions and detect the\nplanar regions on the fly during training. As the predicted depth becomes more\naccurate after more training epochs, the supervisory signals also improve and\nin turn feedback to obtain a better depth model. Through extensive experiments\non indoor benchmark datasets, the results show that our network outperforms the\nstate-of-the-art methods. The source code is available at\nhttps://github.com/SJTU-ViSYS/StructDepth .",
          "link": "http://arxiv.org/abs/2108.08574",
          "publishedOn": "2021-08-20T01:53:50.869Z",
          "wordCount": 658,
          "title": "StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation. (arXiv:2108.08574v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Deep learning has achieved remarkable success in medicalimage segmentation,\nbut it usually requires a large numberof images labeled with fine-grained\nsegmentation masks, andthe annotation of these masks can be very expensive\nandtime-consuming. Therefore, recent methods try to use un-supervised domain\nadaptation (UDA) methods to borrow in-formation from labeled data from other\ndatasets (source do-mains) to a new dataset (target domain). However, due tothe\nabsence of labels in the target domain, the performance ofUDA methods is much\nworse than that of the fully supervisedmethod. In this paper, we propose a\nweakly supervised do-main adaptation setting, in which we can partially label\nnewdatasets with bounding boxes, which are easier and cheaperto obtain than\nsegmentation masks. Accordingly, we proposea new weakly-supervised domain\nadaptation method calledBox-Adapt, which fully explores the fine-grained\nsegmenta-tion mask in the source domain and the weak bounding boxin the target\ndomain. Our Box-Adapt is a two-stage methodthat first performs joint training\non the source and target do-mains, and then conducts self-training with the\npseudo-labelsof the target domain. We demonstrate the effectiveness of\nourmethod in the liver segmentation task. Weakly supervised do-main adaptation",
          "link": "http://arxiv.org/abs/2108.08432",
          "publishedOn": "2021-08-20T01:53:50.850Z",
          "wordCount": 625,
          "title": "Box-Adapt: Domain-Adaptive Medical Image Segmentation using Bounding BoxSupervision. (arXiv:2108.08432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yujun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>",
          "description": "Lane detection plays a key role in autonomous driving. While car cameras\nalways take streaming videos on the way, current lane detection works mainly\nfocus on individual images (frames) by ignoring dynamics along the video. In\nthis work, we collect a new video instance lane detection (VIL-100) dataset,\nwhich contains 100 videos with in total 10,000 frames, acquired from different\nreal traffic scenarios. All the frames in each video are manually annotated to\na high-quality instance-level lane annotation, and a set of frame-level and\nvideo-level metrics are included for quantitative performance evaluation.\nMoreover, we propose a new baseline model, named multi-level memory aggregation\nnetwork (MMA-Net), for video instance lane detection. In our approach, the\nrepresentation of current frame is enhanced by attentively aggregating both\nlocal and global memory features from other frames. Experiments on the new\ncollected dataset show that the proposed MMA-Net outperforms state-of-the-art\nlane detection methods and video object segmentation methods. We release our\ndataset and code at https://github.com/yujun0-0/MMA-Net.",
          "link": "http://arxiv.org/abs/2108.08482",
          "publishedOn": "2021-08-20T01:53:50.843Z",
          "wordCount": 616,
          "title": "VIL-100: A New Dataset and A Baseline Model for Video Instance Lane Detection. (arXiv:2108.08482v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Soumava Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_T/0/1/0/all/0/1\">Titir Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Soma Biswas</a>",
          "description": "In this work, for the first time, we address the problem of universal\ncross-domain retrieval, where the test data can belong to classes or domains\nwhich are unseen during training. Due to dynamically increasing number of\ncategories and practical constraint of training on every possible domain, which\nrequires large amounts of data, generalizing to both unseen classes and domains\nis important. Towards that goal, we propose SnMpNet (Semantic Neighbourhood and\nMixture Prediction Network), which incorporates two novel losses to account for\nthe unseen classes and domains encountered during testing. Specifically, we\nintroduce a novel Semantic Neighborhood loss to bridge the knowledge gap\nbetween seen and unseen classes and ensure that the latent space embedding of\nthe unseen classes is semantically meaningful with respect to its neighboring\nclasses. We also introduce a mix-up based supervision at image-level as well as\nsemantic-level of the data for training with the Mixture Prediction loss, which\nhelps in efficient retrieval when the query belongs to an unseen domain. These\nlosses are incorporated on the SE-ResNet50 backbone to obtain SnMpNet.\nExtensive experiments on two large-scale datasets, Sketchy Extended and\nDomainNet, and thorough comparisons with state-of-the-art justify the\neffectiveness of the proposed model.",
          "link": "http://arxiv.org/abs/2108.08356",
          "publishedOn": "2021-08-20T01:53:50.836Z",
          "wordCount": 638,
          "title": "Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains. (arXiv:2108.08356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Peixi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Li Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Recently, the generalization behavior of Convolutional Neural Networks (CNN)\nis gradually transparent through explanation techniques with the frequency\ncomponents decomposition. However, the importance of the phase spectrum of the\nimage for a robust vision system is still ignored. In this paper, we notice\nthat the CNN tends to converge at the local optimum which is closely related to\nthe high-frequency components of the training images, while the amplitude\nspectrum is easily disturbed such as noises or common corruptions. In contrast,\nmore empirical studies found that humans rely on more phase components to\nachieve robust recognition. This observation leads to more explanations of the\nCNN's generalization behaviors in both robustness to common perturbations and\nout-of-distribution detection, and motivates a new perspective on data\naugmentation designed by re-combing the phase spectrum of the current image and\nthe amplitude spectrum of the distracter image. That is, the generated samples\nforce the CNN to pay more attention to the structured information from phase\ncomponents and keep robust to the variation of the amplitude. Experiments on\nseveral image datasets indicate that the proposed method achieves\nstate-of-the-art performances on multiple generalizations and calibration\ntasks, including adaptability for common corruptions and surface variations,\nout-of-distribution detection, and adversarial attack.",
          "link": "http://arxiv.org/abs/2108.08487",
          "publishedOn": "2021-08-20T01:53:50.829Z",
          "wordCount": 654,
          "title": "Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain. (arXiv:2108.08487v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08775",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sankar_R/0/1/0/all/0/1\">Rahul Sankar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhudev_A/0/1/0/all/0/1\">Amithash M Prabhudev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahesh_P/0/1/0/all/0/1\">P A Mahesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prakashini_K/0/1/0/all/0/1\">K Prakashini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sudha Kiran Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "The world is going through a challenging phase due to the disastrous effect\ncaused by the COVID-19 pandemic on the healthcare system and the economy. The\nrate of spreading, post-COVID-19 symptoms, and the occurrence of new strands of\nCOVID-19 have put the healthcare systems in disruption across the globe. Due to\nthis, the task of accurately screening COVID-19 cases has become of utmost\npriority. Since the virus infects the respiratory system, Chest X-Ray is an\nimaging modality that is adopted extensively for the initial screening. We have\nperformed a comprehensive study that uses CXR images to identify COVID-19 cases\nand realized the necessity of having a more generalizable model. We utilize\nMobileNetV2 architecture as the feature extractor and integrate it into Capsule\nNetworks to construct a fully automated and lightweight model termed as\nMobileCaps. MobileCaps is trained and evaluated on the publicly available\ndataset with the model ensembling and Bayesian optimization strategies to\nefficiently classify CXR images of patients with COVID-19 from non-COVID-19\npneumonia and healthy cases. The proposed model is further evaluated on two\nadditional RT-PCR confirmed datasets to demonstrate the generalizability. We\nalso introduce MobileCaps-S and leverage it for performing severity assessment\nof CXR images of COVID-19 based on the Radiographic Assessment of Lung Edema\n(RALE) scoring technique. Our classification model achieved an overall recall\nof 91.60, 94.60, 92.20, and a precision of 98.50, 88.21, 92.62 for COVID-19,\nnon-COVID-19 pneumonia, and healthy cases, respectively. Further, the severity\nassessment model attained an R$^2$ coefficient of 70.51. Owing to the fact that\nthe proposed models have fewer trainable parameters than the state-of-the-art\nmodels reported in the literature, we believe our models will go a long way in\naiding healthcare systems in the battle against the pandemic.",
          "link": "http://arxiv.org/abs/2108.08775",
          "publishedOn": "2021-08-20T01:53:50.823Z",
          "wordCount": 814,
          "title": "MobileCaps: A Lightweight Model for Screening and Severity Analysis of COVID-19 Chest X-Ray Images. (arXiv:2108.08775v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Mingjun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zikui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1\">Srikanth V. Krishnamurthy</a>",
          "description": "Vision systems that deploy Deep Neural Networks (DNNs) are known to be\nvulnerable to adversarial examples. Recent research has shown that checking the\nintrinsic consistencies in the input data is a promising way to detect\nadversarial attacks (e.g., by checking the object co-occurrence relationships\nin complex scenes). However, existing approaches are tied to specific models\nand do not offer generalizability. Motivated by the observation that language\ndescriptions of natural scene images have already captured the object\nco-occurrence relationships that can be learned by a language model, we develop\na novel approach to perform context consistency checks using such language\nmodels. The distinguishing aspect of our approach is that it is independent of\nthe deployed object detector and yet offers very high accuracy in terms of\ndetecting adversarial examples in practical scenes with multiple objects.",
          "link": "http://arxiv.org/abs/2108.08421",
          "publishedOn": "2021-08-20T01:53:50.804Z",
          "wordCount": 590,
          "title": "Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes. (arXiv:2108.08421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>",
          "description": "Self-supervised learning has been successfully applied to pre-train video\nrepresentations, which aims at efficient adaptation from pre-training domain to\ndownstream tasks. Existing approaches merely leverage contrastive loss to learn\ninstance-level discrimination. However, lack of category information will lead\nto hard-positive problem that constrains the generalization ability of this\nkind of methods. We find that the multi-task process of meta learning can\nprovide a solution to this problem. In this paper, we propose a\nMeta-Contrastive Network (MCN), which combines the contrastive learning and\nmeta learning, to enhance the learning ability of existing self-supervised\napproaches. Our method contains two training stages based on model-agnostic\nmeta learning (MAML), each of which consists of a contrastive branch and a meta\nbranch. Extensive evaluations demonstrate the effectiveness of our method. For\ntwo downstream tasks, i.e., video action recognition and video retrieval, MCN\noutperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. To be\nmore specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8%\nand 54.5% for video action recognition, as well as 52.5% and 23.7% for video\nretrieval.",
          "link": "http://arxiv.org/abs/2108.08426",
          "publishedOn": "2021-08-20T01:53:50.796Z",
          "wordCount": 618,
          "title": "Self-Supervised Video Representation Learning with Meta-Contrastive Network. (arXiv:2108.08426v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1111.6276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>",
          "description": "A simple approach for orthogonal wavelets in compressed sensing (CS)\napplications is presented. We compare efficient algorithm for different\northogonal wavelet measurement matrices in CS for image processing from scanned\nphotographic plates (SPP). Some important characteristics were obtained for\nastronomical image processing of SPP. The best orthogonal wavelet choice for\nmeasurement matrix construction in CS for image compression of images of SPP is\ngiven. The image quality measure for linear and nonlinear image compression\nmethod is defined.",
          "link": "http://arxiv.org/abs/1111.6276",
          "publishedOn": "2021-08-20T01:53:50.781Z",
          "wordCount": 587,
          "title": "Compressed sensing of astronomical images:orthogonal wavelets domains. (arXiv:1111.6276v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wei Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>",
          "description": "Recent progress in stochastic motion prediction, i.e., predicting multiple\npossible future human motions given a single past pose sequence, has led to\nproducing truly diverse future motions and even providing control over the\nmotion of some body parts. However, to achieve this, the state-of-the-art\nmethod requires learning several mappings for diversity and a dedicated model\nfor controllable motion prediction. In this paper, we introduce a unified deep\ngenerative network for both diverse and controllable motion prediction. To this\nend, we leverage the intuition that realistic human motions consist of smooth\nsequences of valid poses, and that, given limited data, learning a pose prior\nis much more tractable than a motion one. We therefore design a generator that\npredicts the motion of different body parts sequentially, and introduce a\nnormalizing flow based pose prior, together with a joint angle loss, to achieve\nmotion realism.Our experiments on two standard benchmark datasets, Human3.6M\nand HumanEva-I, demonstrate that our approach outperforms the state-of-the-art\nbaselines in terms of both sample diversity and accuracy. The code is available\nat https://github.com/wei-mao-2019/gsps",
          "link": "http://arxiv.org/abs/2108.08422",
          "publishedOn": "2021-08-20T01:53:50.773Z",
          "wordCount": 614,
          "title": "Generating Smooth Pose Sequences for Diverse Human Motion Prediction. (arXiv:2108.08422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Razani_R/0/1/0/all/0/1\">Ryan Razani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Enxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taghavi_E/0/1/0/all/0/1\">Ehsan Taghavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingbing_L/0/1/0/all/0/1\">Liu Bingbing</a>",
          "description": "Panoptic segmentation as an integrated task of both static environmental\nunderstanding and dynamic object identification, has recently begun to receive\nbroad research interest. In this paper, we propose a new computationally\nefficient LiDAR based panoptic segmentation framework, called GP-S3Net.\nGP-S3Net is a proposal-free approach in which no object proposals are needed to\nidentify the objects in contrast to conventional two-stage panoptic systems,\nwhere a detection network is incorporated for capturing instance information.\nOur new design consists of a novel instance-level network to process the\nsemantic results by constructing a graph convolutional network to identify\nobjects (foreground), which later on are fused with the background classes.\nThrough the fine-grained clusters of the foreground objects from the semantic\nsegmentation backbone, over-segmentation priors are generated and subsequently\nprocessed by 3D sparse convolution to embed each cluster. Each cluster is\ntreated as a node in the graph and its corresponding embedding is used as its\nnode feature. Then a GCNN predicts whether edges exist between each cluster\npair. We utilize the instance label to generate ground truth edge labels for\neach constructed graph in order to supervise the learning. Extensive\nexperiments demonstrate that GP-S3Net outperforms the current state-of-the-art\napproaches, by a significant margin across available datasets such as, nuScenes\nand SemanticPOSS, ranking first on the competitive public SemanticKITTI\nleaderboard upon publication.",
          "link": "http://arxiv.org/abs/2108.08401",
          "publishedOn": "2021-08-20T01:53:50.720Z",
          "wordCount": 654,
          "title": "GP-S3Net: Graph-based Panoptic Sparse Semantic Segmentation Network. (arXiv:2108.08401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lennon_K/0/1/0/all/0/1\">Kyle Lennon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fransen_K/0/1/0/all/0/1\">Katharina Fransen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_A/0/1/0/all/0/1\">Alexander O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yumeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefeen_Y/0/1/0/all/0/1\">Yamin Arefeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Although LEGO sets have entertained generations of children and adults, the\nchallenge of designing customized builds matching the complexity of real-world\nor imagined scenes remains too great for the average enthusiast. In order to\nmake this feat possible, we implement a system that generates a LEGO brick\nmodel from 2D images. We design a novel solution to this problem that uses an\noctree-structured autoencoder trained on 3D voxelized models to obtain a\nfeasible latent representation for model reconstruction, and a separate network\ntrained to predict this latent representation from 2D images. LEGO models are\nobtained by algorithmic conversion of the 3D voxelized model to bricks. We\ndemonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An\noctree architecture enables the flexibility to produce multiple resolutions to\nbest fit a user's creative vision or design needs. In order to demonstrate the\nbroad applicability of our system, we generate step-by-step building\ninstructions and animations for LEGO models of objects and human faces.\nFinally, we test these automatically generated LEGO sets by constructing\nphysical builds using real LEGO bricks.",
          "link": "http://arxiv.org/abs/2108.08477",
          "publishedOn": "2021-08-20T01:53:50.713Z",
          "wordCount": 627,
          "title": "Image2Lego: Customized LEGO Set Generation from Images. (arXiv:2108.08477v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1\">Guohao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yufeng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danwei Wang</a>",
          "description": "Large-scale visual place recognition (VPR) is inherently challenging because\nnot all visual cues in the image are beneficial to the task. In order to\nhighlight the task-relevant visual cues in the feature embedding, the existing\nattention mechanisms are either based on artificial rules or trained in a\nthorough data-driven manner. To fill the gap between the two types, we propose\na novel Semantic Reinforced Attention Learning Network (SRALNet), in which the\ninferred attention can benefit from both semantic priors and data-driven\nfine-tuning. The contribution lies in two-folds. (1) To suppress misleading\nlocal features, an interpretable local weighting scheme is proposed based on\nhierarchical feature distribution. (2) By exploiting the interpretability of\nthe local weighting scheme, a semantic constrained initialization is proposed\nso that the local attention can be reinforced by semantic priors. Experiments\ndemonstrate that our method outperforms state-of-the-art techniques on\ncity-scale VPR benchmark datasets.",
          "link": "http://arxiv.org/abs/2108.08443",
          "publishedOn": "2021-08-20T01:53:50.705Z",
          "wordCount": 591,
          "title": "Semantic Reinforced Attention Learning for Visual Place Recognition. (arXiv:2108.08443v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_I/0/1/0/all/0/1\">Iago Su&#xe1;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1\">Jos&#xe9; M. Buenaposada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1\">Luis Baumela</a>",
          "description": "The advent of a panoply of resource limited devices opens up new challenges\nin the design of computer vision algorithms with a clear compromise between\naccuracy and computational requirements. In this paper we present new binary\nimage descriptors that emerge from the application of triplet ranking loss,\nhard negative mining and anchor swapping to traditional features based on pixel\ndifferences and image gradients. These descriptors, BAD (Box Average\nDifference) and HashSIFT, establish new operating points in the\nstate-of-the-art's accuracy vs.\\ resources trade-off curve. In our experiments\nwe evaluate the accuracy, execution time and energy consumption of the proposed\ndescriptors. We show that BAD bears the fastest descriptor implementation in\nthe literature while HashSIFT approaches in accuracy that of the top deep\nlearning-based descriptors, being computationally more efficient. We have made\nthe source code public.",
          "link": "http://arxiv.org/abs/2108.08380",
          "publishedOn": "2021-08-20T01:53:50.699Z",
          "wordCount": 570,
          "title": "Revisiting Binary Local Image Description for Resource Limited Devices. (arXiv:2108.08380v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_J/0/1/0/all/0/1\">Jenny Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stayt_J/0/1/0/all/0/1\">Jason Stayt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bienzle_D/0/1/0/all/0/1\">Dorothee Bienzle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welker_L/0/1/0/all/0/1\">Lutz Welker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voigt_J/0/1/0/all/0/1\">J&#xf6;rn Voigt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klopfleisch_R/0/1/0/all/0/1\">Robert Klopfleisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>",
          "description": "Pulmonary hemorrhage (P-Hem) occurs among multiple species and can have\nvarious causes. Cytology of bronchoalveolarlavage fluid (BALF) using a 5-tier\nscoring system of alveolar macrophages based on their hemosiderin content is\nconsidered the most sensitive diagnostic method. We introduce a novel, fully\nannotated multi-species P-Hem dataset which consists of 74 cytology whole slide\nimages (WSIs) with equine, feline and human samples. To create this\nhigh-quality and high-quantity dataset, we developed an annotation pipeline\ncombining human expertise with deep learning and data visualisation techniques.\nWe applied a deep learning-based object detection approach trained on 17\nexpertly annotated equine WSIs, to the remaining 39 equine, 12 human and 7\nfeline WSIs. The resulting annotations were semi-automatically screened for\nerrors on multiple types of specialised annotation maps and finally reviewed by\na trained pathologists. Our dataset contains a total of 297,383\nhemosiderophages classified into five grades. It is one of the largest publicly\navailableWSIs datasets with respect to the number of annotations, the scanned\narea and the number of species covered.",
          "link": "http://arxiv.org/abs/2108.08529",
          "publishedOn": "2021-08-20T01:53:50.679Z",
          "wordCount": 644,
          "title": "Inter-Species Cell Detection: Datasets on pulmonary hemosiderophages in equine, human and feline specimens. (arXiv:2108.08529v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobeil_Y/0/1/0/all/0/1\">Yan Gobeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercier_S/0/1/0/all/0/1\">Samuel Mercier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "Semi-supervised learning (SSL) has proven to be effective at leveraging\nlarge-scale unlabeled data to mitigate the dependency on labeled data in order\nto learn better models for visual recognition and classification tasks.\nHowever, recent SSL methods rely on unlabeled image data at a scale of billions\nto work well. This becomes infeasible for tasks with relatively fewer unlabeled\ndata in terms of runtime, memory and data acquisition. To address this issue,\nwe propose noisy semi-supervised transfer learning, an efficient SSL approach\nthat integrates transfer learning and self-training with noisy student into a\nsingle framework, which is tailored for tasks that can leverage unlabeled image\ndata on a scale of thousands. We evaluate our method on both binary and\nmulti-class classification tasks, where the objective is to identify whether an\nimage displays people practicing sports or the type of sport, as well as to\nidentify the pose from a pool of popular yoga poses. Extensive experiments and\nablation studies demonstrate that by leveraging unlabeled data, our proposed\nframework significantly improves visual classification, especially in\nmulti-class classification settings compared to state-of-the-art methods.\nMoreover, incorporating transfer learning not only improves classification\nperformance, but also requires 6x less compute time and 5x less memory. We also\nshow that our method boosts robustness of visual classification models, even\nwithout specifically optimizing for adversarial robustness.",
          "link": "http://arxiv.org/abs/2108.08362",
          "publishedOn": "2021-08-20T01:53:50.672Z",
          "wordCount": 656,
          "title": "STAR: Noisy Semi-Supervised Transfer Learning for Visual Classification. (arXiv:2108.08362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashrafee_A/0/1/0/all/0/1\">Alif Ashrafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Mohammed Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>",
          "description": "Automatic License Plate Recognition systems aim to provide an end-to-end\nsolution towards detecting, localizing, and recognizing license plate\ncharacters from vehicles appearing in video frames. However, deploying such\nsystems in the real world requires real-time performance in low-resource\nenvironments. In our paper, we propose a novel two-stage detection pipeline\npaired with Vision API that aims to provide real-time inference speed along\nwith consistently accurate detection and recognition performance. We used a\nhaar-cascade classifier as a filter on top of our backbone MobileNet SSDv2\ndetection model. This reduces inference time by only focusing on high\nconfidence detections and using them for recognition. We also impose a temporal\nframe separation strategy to identify multiple vehicle license plates in the\nsame clip. Furthermore, there are no publicly available Bangla license plate\ndatasets, for which we created an image dataset and a video dataset containing\nlicense plates in the wild. We trained our models on the image dataset and\nachieved an AP(0.5) score of 86% and tested our pipeline on the video dataset\nand observed reasonable detection and recognition performance (82.7% detection\nrate, and 60.8% OCR F1 score) with real-time processing speed (27.2 frames per\nsecond).",
          "link": "http://arxiv.org/abs/2108.08339",
          "publishedOn": "2021-08-20T01:53:50.655Z",
          "wordCount": 647,
          "title": "End-to-End License Plate Recognition Pipeline for Real-time Low Resource Video Based Applications. (arXiv:2108.08339v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruen_A/0/1/0/all/0/1\">Armin Gruen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_C/0/1/0/all/0/1\">Cive Fraser</a>",
          "description": "Recently developed automatic dense image matching algorithms are now being\nimplemented for DSM/DTM production, with their pixel-level surface generation\ncapability offering the prospect of partially alleviating the need for manual\nand semi-automatic stereoscopic measurements. In this paper, five\ncommercial/public software packages for 3D surface generation are evaluated,\nusing 5cm GSD imagery recorded from a UAV. Generated surface models are\nassessed against point clouds generated from mobile LiDAR and manual\nstereoscopic measurements. The software packages considered are APS, MICMAC,\nSURE, Pix4UAV and an SGM implementation from DLR.",
          "link": "http://arxiv.org/abs/2108.08369",
          "publishedOn": "2021-08-20T01:53:50.628Z",
          "wordCount": 542,
          "title": "Quality assessment of image matchers for DSM generation -- a comparative study based on UAV images. (arXiv:2108.08369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kin-man Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>",
          "description": "This paper considers the problem of generating an HDR image of a scene from\nits LDR images. Recent studies employ deep learning and solve the problem in an\nend-to-end fashion, leading to significant performance improvements. However,\nit is still hard to generate a good quality image from LDR images of a dynamic\nscene captured by a hand-held camera, e.g., occlusion due to the large motion\nof foreground objects, causing ghosting artifacts. The key to success relies on\nhow well we can fuse the input images in their feature space, where we wish to\nremove the factors leading to low-quality image generation while performing the\nfundamental computations for HDR image generation, e.g., selecting the\nbest-exposed image/region. We propose a novel method that can better fuse the\nfeatures based on two ideas. One is multi-step feature fusion; our network\ngradually fuses the features in a stack of blocks having the same structure.\nThe other is the design of the component block that effectively performs two\noperations essential to the problem, i.e., comparing and selecting appropriate\nimages/regions. Experimental results show that the proposed method outperforms\nthe previous state-of-the-art methods on the standard benchmark tests.",
          "link": "http://arxiv.org/abs/2108.08585",
          "publishedOn": "2021-08-20T01:53:50.607Z",
          "wordCount": 630,
          "title": "Progressive and Selective Fusion Network for High Dynamic Range Imaging. (arXiv:2108.08585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08473",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jonathan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_B/0/1/0/all/0/1\">Bowen Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ram_R/0/1/0/all/0/1\">Rahul Ram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">David Liang</a>",
          "description": "In this work, deep learning algorithms are used to classify fundus images in\nterms of diabetic retinopathy severity. Six different combinations of two model\narchitectures, the Dense Convolutional Network-121 and the Residual Neural\nNetwork-50 and three image types, RGB, Green, and High Contrast, were tested to\nfind the highest performing combination. We achieved an average validation loss\nof 0.17 and a max validation accuracy of 85 percent. By testing out multiple\ncombinations, certain combinations of parameters performed better than others,\nthough minimal variance was found overall. Green filtration was shown to\nperform the poorest, while amplified contrast appeared to have a negligible\neffect in comparison to RGB analysis. ResNet50 proved to be less of a robust\nmodel as opposed to DenseNet121.",
          "link": "http://arxiv.org/abs/2108.08473",
          "publishedOn": "2021-08-20T01:53:50.599Z",
          "wordCount": 603,
          "title": "Classification of Diabetic Retinopathy Severity in Fundus Images with DenseNet121 and ResNet50. (arXiv:2108.08473v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_Q/0/1/0/all/0/1\">Quazi Marufur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1\">Niko S&#xfc;nderhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corke_P/0/1/0/all/0/1\">Peter Corke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1\">Feras Dayoub</a>",
          "description": "Semantic segmentation is an important task that helps autonomous vehicles\nunderstand their surroundings and navigate safely. During deployment, even the\nmost mature segmentation models are vulnerable to various external factors that\ncan degrade the segmentation performance with potentially catastrophic\nconsequences for the vehicle and its surroundings. To address this issue, we\npropose a failure detection framework to identify pixel-level\nmisclassification. We do so by exploiting internal features of the segmentation\nmodel and training it simultaneously with a failure detection network. During\ndeployment, the failure detector can flag areas in the image where the\nsegmentation model have failed to segment correctly. We evaluate the proposed\napproach against state-of-the-art methods and achieve 12.30%, 9.46%, and 9.65%\nperformance improvement in the AUPR-Error metric for Cityscapes, BDD100K, and\nMapillary semantic segmentation datasets.",
          "link": "http://arxiv.org/abs/2108.08748",
          "publishedOn": "2021-08-20T01:53:50.580Z",
          "wordCount": 569,
          "title": "FSNet: A Failure Detection Framework for Semantic Segmentation. (arXiv:2108.08748v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_K/0/1/0/all/0/1\">Kushal Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1\">Pradeep Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_D/0/1/0/all/0/1\">Devarajan Sridharan</a>",
          "description": "Deep networks often make confident, yet incorrect, predictions when tested\nwith outlier data that is far removed from their training distributions.\nLikelihoods computed by deep generative models are a candidate metric for\noutlier detection with unlabeled data. Yet, previous studies have shown that\nsuch likelihoods are unreliable and can be easily biased by simple\ntransformations to input data. Here, we examine outlier detection with\nvariational autoencoders (VAEs), among the simplest class of deep generative\nmodels. First, we show that a theoretically-grounded correction readily\nameliorates a key bias with VAE likelihood estimates. The bias correction is\nmodel-free, sample-specific, and accurately computed with the Bernoulli and\ncontinuous Bernoulli visible distributions. Second, we show that a well-known\npreprocessing technique, contrast normalization, extends the effectiveness of\nbias correction to natural image datasets. Third, we show that the variance of\nthe likelihoods computed over an ensemble of VAEs also enables robust outlier\ndetection. We perform a comprehensive evaluation of our remedies with nine\n(grayscale and natural) image datasets, and demonstrate significant advantages,\nin terms of both speed and accuracy, over four other state-of-the-art methods.\nOur lightweight remedies are biologically inspired and may serve to achieve\nefficient outlier detection with many types of deep generative models.",
          "link": "http://arxiv.org/abs/2108.08760",
          "publishedOn": "2021-08-20T01:53:50.573Z",
          "wordCount": 645,
          "title": "Efficient remedies for outlier detection with variational autoencoders. (arXiv:2108.08760v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>",
          "description": "Existing self-supervised learning methods learn representation by means of\npretext tasks which are either (1) discriminating that explicitly specify which\nfeatures should be separated or (2) aligning that precisely indicate which\nfeatures should be closed together, but ignore the fact how to jointly and\nprincipally define which features to be repelled and which ones to be\nattracted. In this work, we combine the positive aspects of the discriminating\nand aligning methods, and design a hybrid method that addresses the above\nissue. Our method explicitly specifies the repulsion and attraction mechanism\nrespectively by discriminative predictive task and concurrently maximizing\nmutual information between paired views sharing redundant information. We\nqualitatively and quantitatively show that our proposed model learns better\nfeatures that are more effective for the diverse downstream tasks ranging from\nclassification to semantic segmentation. Our experiments on nine established\nbenchmarks show that the proposed model consistently outperforms the existing\nstate-of-the-art results of self-supervised and transfer learning protocol.",
          "link": "http://arxiv.org/abs/2108.08562",
          "publishedOn": "2021-08-20T01:53:50.566Z",
          "wordCount": 602,
          "title": "Concurrent Discrimination and Alignment for Self-Supervised Feature Learning. (arXiv:2108.08562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "We present a novel framework for mesh reconstruction from unstructured point\nclouds by taking advantage of the learned visibility of the 3D points in the\nvirtual views and traditional graph-cut based mesh generation. Specifically, we\nfirst propose a three-step network that explicitly employs depth completion for\nvisibility prediction. Then the visibility information of multiple views is\naggregated to generate a 3D mesh model by solving an optimization problem\nconsidering visibility in which a novel adaptive visibility weighting in\nsurface determination is also introduced to suppress line of sight with a large\nincident angle. Compared to other learning-based approaches, our pipeline only\nexercises the learning on a 2D binary classification task, \\ie, points visible\nor not in a view, which is much more generalizable and practically more\nefficient and capable to deal with a large number of points. Experiments\ndemonstrate that our method with favorable transferability and robustness, and\nachieve competing performances \\wrt state-of-the-art learning-based approaches\non small complex objects and outperforms on large indoor and outdoor scenes.\nCode is available at https://github.com/GDAOSU/vis2mesh.",
          "link": "http://arxiv.org/abs/2108.08378",
          "publishedOn": "2021-08-20T01:53:50.557Z",
          "wordCount": 626,
          "title": "Vis2Mesh: Efficient Mesh Reconstruction from Unstructured Point Clouds of Large Scenes with Learned Virtual View Visibility. (arXiv:2108.08378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Starmans_M/0/1/0/all/0/1\">Martijn P. A. Starmans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Voort_S/0/1/0/all/0/1\">Sebastian R. van der Voort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phil_T/0/1/0/all/0/1\">Thomas Phil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timbergen_M/0/1/0/all/0/1\">Milea J. M. Timbergen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vos_M/0/1/0/all/0/1\">Melissa Vos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padmos_G/0/1/0/all/0/1\">Guillaume A. Padmos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kessels_W/0/1/0/all/0/1\">Wouter Kessels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hanff_D/0/1/0/all/0/1\">David Hanff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grunhagen_D/0/1/0/all/0/1\">Dirk J. Grunhagen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verhoef_C/0/1/0/all/0/1\">Cornelis Verhoef</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sleijfer_S/0/1/0/all/0/1\">Stefan Sleijfer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bent_M/0/1/0/all/0/1\">Martin J. van den Bent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smits_M/0/1/0/all/0/1\">Marion Smits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dwarkasing_R/0/1/0/all/0/1\">Roy S. Dwarkasing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Els_C/0/1/0/all/0/1\">Christopher J. Els</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fiduzi_F/0/1/0/all/0/1\">Federico Fiduzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leenders_G/0/1/0/all/0/1\">Geert J. L. H. van Leenders</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blazevic_A/0/1/0/all/0/1\">Anela Blazevic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hofland_J/0/1/0/all/0/1\">Johannes Hofland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brabander_T/0/1/0/all/0/1\">Tessa Brabander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gils_R/0/1/0/all/0/1\">Renza A. H. van Gils</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Franssen_G/0/1/0/all/0/1\">Gaston J. H. Franssen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feelders_R/0/1/0/all/0/1\">Richard A. Feelders</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herder_W/0/1/0/all/0/1\">Wouter W. de Herder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buisman_F/0/1/0/all/0/1\">Florian E. Buisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willemssen_F/0/1/0/all/0/1\">Francois E. J. A. Willemssen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koerkamp_B/0/1/0/all/0/1\">Bas Groot Koerkamp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Angus_L/0/1/0/all/0/1\">Lindsay Angus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veldt_A/0/1/0/all/0/1\">Astrid A. M. van der Veldt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajicic_A/0/1/0/all/0/1\">Ana Rajicic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Odink_A/0/1/0/all/0/1\">Arlette E. Odink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deen_M/0/1/0/all/0/1\">Mitchell Deen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+T%2E_J/0/1/0/all/0/1\">Jose M. Castillo T.</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veenland_J/0/1/0/all/0/1\">Jifke Veenland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schoots_I/0/1/0/all/0/1\">Ivo Schoots</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Renckens_M/0/1/0/all/0/1\">Michel Renckens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doukas_M/0/1/0/all/0/1\">Michail Doukas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Man_R/0/1/0/all/0/1\">Rob A. de Man</a>, <a href=\"http://arxiv.org/find/eess/1/au:+IJzermans_J/0/1/0/all/0/1\">Jan N. M. IJzermans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miclea_R/0/1/0/all/0/1\">Razvan L. Miclea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vermeulen_P/0/1/0/all/0/1\">Peter B. Vermeulen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bron_E/0/1/0/all/0/1\">Esther E. Bron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomeer_M/0/1/0/all/0/1\">Maarten G. Thomeer</a>, et al. (3 additional authors not shown)",
          "description": "Radiomics uses quantitative medical imaging features to predict clinical\noutcomes. While many radiomics methods have been described in the literature,\nthese are generally designed for a single application. The aim of this study is\nto generalize radiomics across applications by proposing a framework to\nautomatically construct and optimize the radiomics workflow per application. To\nthis end, we formulate radiomics as a modular workflow, consisting of several\ncomponents: image and segmentation preprocessing, feature extraction, feature\nand sample preprocessing, and machine learning. For each component, a\ncollection of common algorithms is included. To optimize the workflow per\napplication, we employ automated machine learning using a random search and\nensembling. We evaluate our method in twelve different clinical applications,\nresulting in the following area under the curves: 1) liposarcoma (0.83); 2)\ndesmoid-type fibromatosis (0.82); 3) primary liver tumors (0.81); 4)\ngastrointestinal stromal tumors (0.77); 5) colorectal liver metastases (0.68);\n6) melanoma metastases (0.51); 7) hepatocellular carcinoma (0.75); 8)\nmesenteric fibrosis (0.81); 9) prostate cancer (0.72); 10) glioma (0.70); 11)\nAlzheimer's disease (0.87); and 12) head and neck cancer (0.84). Concluding,\nour method fully automatically constructs and optimizes the radiomics workflow,\nthereby streamlining the search for radiomics biomarkers in new applications.\nTo facilitate reproducibility and future research, we publicly release six\ndatasets, the software implementation of our framework (open-source), and the\ncode to reproduce this study.",
          "link": "http://arxiv.org/abs/2108.08618",
          "publishedOn": "2021-08-20T01:53:50.548Z",
          "wordCount": 830,
          "title": "Reproducible radiomics through automated machine learning validated on twelve clinical applications. (arXiv:2108.08618v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhamo_H/0/1/0/all/0/1\">Helisa Dhamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1\">Fabian Manhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Controllable scene synthesis consists of generating 3D information that\nsatisfy underlying specifications. Thereby, these specifications should be\nabstract, i.e. allowing easy user interaction, whilst providing enough\ninterface for detailed control. Scene graphs are representations of a scene,\ncomposed of objects (nodes) and inter-object relationships (edges), proven to\nbe particularly suited for this task, as they allow for semantic control on the\ngenerated content. Previous works tackling this task often rely on synthetic\ndata, and retrieve object meshes, which naturally limits the generation\ncapabilities. To circumvent this issue, we instead propose the first work that\ndirectly generates shapes from a scene graph in an end-to-end manner. In\naddition, we show that the same model supports scene modification, using the\nrespective scene graph as interface. Leveraging Graph Convolutional Networks\n(GCN) we train a variational Auto-Encoder on top of the object and edge\ncategories, as well as 3D shapes and scene layouts, allowing latter sampling of\nnew scenes and shapes.",
          "link": "http://arxiv.org/abs/2108.08841",
          "publishedOn": "2021-08-20T01:53:50.522Z",
          "wordCount": 604,
          "title": "Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs. (arXiv:2108.08841v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honglun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>",
          "description": "Colorization has attracted increasing interest in recent years. Classic\nreference-based methods usually rely on external color images for plausible\nresults. A large image database or online search engine is inevitably required\nfor retrieving such exemplars. Recent deep-learning-based methods could\nautomatically colorize images at a low cost. However, unsatisfactory artifacts\nand incoherent colors are always accompanied. In this work, we aim at\nrecovering vivid colors by leveraging the rich and diverse color priors\nencapsulated in a pretrained Generative Adversarial Networks (GAN).\nSpecifically, we first \"retrieve\" matched features (similar to exemplars) via a\nGAN encoder and then incorporate these features into the colorization process\nwith feature modulations. Thanks to the powerful generative color prior and\ndelicate designs, our method could produce vivid colors with a single forward\npass. Moreover, it is highly convenient to obtain diverse results by modifying\nGAN latent codes. Our method also inherits the merit of interpretable controls\nof GANs and could attain controllable and smooth transitions by walking through\nGAN latent space. Extensive experiments and user studies demonstrate that our\nmethod achieves superior performance than previous works.",
          "link": "http://arxiv.org/abs/2108.08826",
          "publishedOn": "2021-08-20T01:53:50.515Z",
          "wordCount": 626,
          "title": "Towards Vivid and Diverse Image Colorization with Generative Color Prior. (arXiv:2108.08826v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jingyang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yipeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Fenglong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>",
          "description": "Recently, deep learning-based image enhancement algorithms achieved\nstate-of-the-art (SOTA) performance on several publicly available datasets.\nHowever, most existing methods fail to meet practical requirements either for\nvisual perception or for computation efficiency, especially for high-resolution\nimages. In this paper, we propose a novel real-time image enhancer via\nlearnable spatial-aware 3-dimentional lookup tables(3D LUTs), which well\nconsiders global scenario and local spatial information. Specifically, we\nintroduce a light weight two-head weight predictor that has two outputs. One is\na 1D weight vector used for image-level scenario adaptation, the other is a 3D\nweight map aimed for pixel-wise category fusion. We learn the spatial-aware 3D\nLUTs and fuse them according to the aforementioned weights in an end-to-end\nmanner. The fused LUT is then used to transform the source image into the\ntarget tone in an efficient way. Extensive results show that our model\noutperforms SOTA image enhancement methods on public datasets both subjectively\nand objectively, and that our model only takes about 4ms to process a 4K\nresolution image on one NVIDIA V100 GPU.",
          "link": "http://arxiv.org/abs/2108.08697",
          "publishedOn": "2021-08-20T01:53:50.507Z",
          "wordCount": 627,
          "title": "Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables. (arXiv:2108.08697v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suin_M/0/1/0/all/0/1\">Maitreya Suin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>",
          "description": "We present a general learning-based solution for restoring images suffering\nfrom spatially-varying degradations. Prior approaches are typically\ndegradation-specific and employ the same processing across different images and\ndifferent pixels within. However, we hypothesize that such spatially rigid\nprocessing is suboptimal for simultaneously restoring the degraded pixels as\nwell as reconstructing the clean regions of the image. To overcome this\nlimitation, we propose SPAIR, a network design that harnesses\ndistortion-localization information and dynamically adjusts computation to\ndifficult regions in the image. SPAIR comprises of two components, (1) a\nlocalization network that identifies degraded pixels, and (2) a restoration\nnetwork that exploits knowledge from the localization network in filter and\nfeature domain to selectively and adaptively restore degraded pixels. Our key\nidea is to exploit the non-uniformity of heavy degradations in spatial-domain\nand suitably embed this knowledge within distortion-guided modules performing\nsparse normalization, feature extraction and attention. Our architecture is\nagnostic to physical formation model and generalizes across several types of\nspatially-varying degradations. We demonstrate the efficacy of SPAIR\nindividually on four restoration tasks-removal of rain-streaks, raindrops,\nshadows and motion blur. Extensive qualitative and quantitative comparisons\nwith prior art on 11 benchmark datasets demonstrate that our\ndegradation-agnostic network design offers significant performance gains over\nstate-of-the-art degradation-specific architectures. Code available at\nhttps://github.com/human-analysis/spatially-adaptive-image-restoration.",
          "link": "http://arxiv.org/abs/2108.08617",
          "publishedOn": "2021-08-20T01:53:50.499Z",
          "wordCount": 653,
          "title": "Spatially-Adaptive Image Restoration using Distortion-Guided Networks. (arXiv:2108.08617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Superpixel segmentation has recently seen important progress benefiting from\nthe advances in differentiable deep learning. However, the very high-resolution\nsuperpixel segmentation still remains challenging due to the expensive memory\nand computation cost, making the current advanced superpixel networks fail to\nprocess. In this paper, we devise Patch Calibration Networks (PCNet), aiming to\nefficiently and accurately implement high-resolution superpixel segmentation.\nPCNet follows the principle of producing high-resolution output from\nlow-resolution input for saving GPU memory and relieving computation cost. To\nrecall the fine details destroyed by the down-sampling operation, we propose a\nnovel Decoupled Patch Calibration (DPC) branch for collaboratively augment the\nmain superpixel generation branch. In particular, DPC takes a local patch from\nthe high-resolution images and dynamically generates a binary mask to impose\nthe network to focus on region boundaries. By sharing the parameters of DPC and\nmain branches, the fine-detailed knowledge learned from high-resolution patches\nwill be transferred to help calibrate the destroyed information. To the best of\nour knowledge, we make the first attempt to consider the deep-learning-based\nsuperpixel generation for high-resolution cases. To facilitate this research,\nwe build evaluation benchmarks from two public datasets and one new constructed\none, covering a wide range of diversities from fine-grained human parts to\ncityscapes. Extensive experiments demonstrate that our PCNet can not only\nperform favorably against the state-of-the-arts in the quantitative results but\nalso improve the resolution upper bound from 3K to 5K on 1080Ti GPUs.",
          "link": "http://arxiv.org/abs/2108.08607",
          "publishedOn": "2021-08-20T01:53:50.483Z",
          "wordCount": 684,
          "title": "Generating Superpixels for High-resolution Images with Decoupled Patch Calibration. (arXiv:2108.08607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eilertsen_G/0/1/0/all/0/1\">Gabriel Eilertsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajisharif_S/0/1/0/all/0/1\">Saghi Hajisharif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanji_P/0/1/0/all/0/1\">Param Hanji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsirikoglou_A/0/1/0/all/0/1\">Apostolia Tsirikoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantiuk_R/0/1/0/all/0/1\">Rafal K. Mantiuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unger_J/0/1/0/all/0/1\">Jonas Unger</a>",
          "description": "Single-image high dynamic range (SI-HDR) reconstruction has recently emerged\nas a problem well-suited for deep learning methods. Each successive technique\ndemonstrates an improvement over existing methods by reporting higher image\nquality scores. This paper, however, highlights that such improvements in\nobjective metrics do not necessarily translate to visually superior images. The\nfirst problem is the use of disparate evaluation conditions in terms of data\nand metric parameters, calling for a standardized protocol to make it possible\nto compare between papers. The second problem, which forms the main focus of\nthis paper, is the inherent difficulty in evaluating SI-HDR reconstructions\nsince certain aspects of the reconstruction problem dominate objective\ndifferences, thereby introducing a bias. Here, we reproduce a typical\nevaluation using existing as well as simulated SI-HDR methods to demonstrate\nhow different aspects of the problem affect objective quality metrics.\nSurprisingly, we found that methods that do not even reconstruct HDR\ninformation can compete with state-of-the-art deep learning methods. We show\nhow such results are not representative of the perceived quality and that\nSI-HDR reconstruction needs better evaluation protocols.",
          "link": "http://arxiv.org/abs/2108.08713",
          "publishedOn": "2021-08-20T01:53:50.475Z",
          "wordCount": 640,
          "title": "How to cheat with metrics in single-image HDR reconstruction. (arXiv:2108.08713v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Harsh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Amey Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahni_S/0/1/0/all/0/1\">Shivam Sahni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_U/0/1/0/all/0/1\">Udit Vyas</a>",
          "description": "Image Inpainting is one of the very popular tasks in the field of image\nprocessing with broad applications in computer vision. In various practical\napplications, images are often deteriorated by noise due to the presence of\ncorrupted, lost, or undesirable information. There have been various\nrestoration techniques used in the past with both classical and deep learning\napproaches for handling such issues. Some traditional methods include image\nrestoration by filling gap pixels using the nearby known pixels or using the\nmoving average over the same. The aim of this paper is to perform image\ninpainting using robust deep learning methods that use partial convolution\nlayers.",
          "link": "http://arxiv.org/abs/2108.08791",
          "publishedOn": "2021-08-20T01:53:50.465Z",
          "wordCount": 534,
          "title": "Image Inpainting using Partial Convolution. (arXiv:2108.08791v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Seogkyu Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kibeom Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Pilhyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jewook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>",
          "description": "Domain generalization aims to enhance the model robustness against domain\nshift without accessing the target domain. Since the available source domains\nfor training are limited, recent approaches focus on generating samples of\nnovel domains. Nevertheless, they either struggle with the optimization problem\nwhen synthesizing abundant domains or cause the distortion of class semantics.\nTo these ends, we propose a novel domain generalization framework where feature\nstatistics are utilized for stylizing original features to ones with novel\ndomain properties. To preserve class information during stylization, we first\ndecompose features into high and low frequency components. Afterward, we\nstylize the low frequency components with the novel domain styles sampled from\nthe manipulated statistics, while preserving the shape cues in high frequency\nones. As the final step, we re-merge both components to synthesize novel domain\nfeatures. To enhance domain robustness, we utilize the stylized features to\nmaintain the model consistency in terms of features as well as outputs. We\nachieve the feature consistency with the proposed domain-aware supervised\ncontrastive loss, which ensures domain invariance while increasing class\ndiscriminability. Experimental results demonstrate the effectiveness of the\nproposed feature stylization and the domain-aware contrastive loss. Through\nquantitative comparisons, we verify the lead of our method upon existing\nstate-of-the-art methods on two benchmarks, PACS and Office-Home.",
          "link": "http://arxiv.org/abs/2108.08596",
          "publishedOn": "2021-08-20T01:53:50.452Z",
          "wordCount": 657,
          "title": "Feature Stylization and Domain-aware Contrastive Learning for Domain Generalization. (arXiv:2108.08596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garau_N/0/1/0/all/0/1\">Nicola Garau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisagno_N/0/1/0/all/0/1\">Niccol&#xf2; Bisagno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brodka_P/0/1/0/all/0/1\">Piotr Br&#xf3;dka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conci_N/0/1/0/all/0/1\">Nicola Conci</a>",
          "description": "Human Pose Estimation (HPE) aims at retrieving the 3D position of human\njoints from images or videos. We show that current 3D HPE methods suffer a lack\nof viewpoint equivariance, namely they tend to fail or perform poorly when\ndealing with viewpoints unseen at training time. Deep learning methods often\nrely on either scale-invariant, translation-invariant, or rotation-invariant\noperations, such as max-pooling. However, the adoption of such procedures does\nnot necessarily improve viewpoint generalization, rather leading to more\ndata-dependent methods. To tackle this issue, we propose a novel capsule\nautoencoder network with fast Variational Bayes capsule routing, named DECA. By\nmodeling each joint as a capsule entity, combined with the routing algorithm,\nour approach can preserve the joints' hierarchical and geometrical structure in\nthe feature space, independently from the viewpoint. By achieving viewpoint\nequivariance, we drastically reduce the network data dependency at training\ntime, resulting in an improved ability to generalize for unseen viewpoints. In\nthe experimental validation, we outperform other methods on depth images from\nboth seen and unseen viewpoints, both top-view, and front-view. In the RGB\ndomain, the same network gives state-of-the-art results on the challenging\nviewpoint transfer task, also establishing a new framework for top-view HPE.\nThe code can be found at https://github.com/mmlab-cv/DECA.",
          "link": "http://arxiv.org/abs/2108.08557",
          "publishedOn": "2021-08-20T01:53:50.445Z",
          "wordCount": 668,
          "title": "DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders. (arXiv:2108.08557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08508",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Araki_K/0/1/0/all/0/1\">Kengo Araki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rokutan_Kurata_M/0/1/0/all/0/1\">Mariyo Rokutan-Kurata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Terada_K/0/1/0/all/0/1\">Kazuhiro Terada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshizawa_A/0/1/0/all/0/1\">Akihiko Yoshizawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bise_R/0/1/0/all/0/1\">Ryoma Bise</a>",
          "description": "Pathological diagnosis is used for examining cancer in detail, and its\nautomation is in demand. To automatically segment each cancer area, a\npatch-based approach is usually used since a Whole Slide Image (WSI) is huge.\nHowever, this approach loses the global information needed to distinguish\nbetween classes. In this paper, we utilized the Distance from the Boundary of\ntissue (DfB), which is global information that can be extracted from the\noriginal image. We experimentally applied our method to the three-class\nclassification of cervical cancer, and found that it improved the total\nperformance compared with the conventional method.",
          "link": "http://arxiv.org/abs/2108.08508",
          "publishedOn": "2021-08-20T01:53:50.438Z",
          "wordCount": 554,
          "title": "Patch-Based Cervical Cancer Segmentation using Distance from Boundary of Tissue. (arXiv:2108.08508v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pisoni_R/0/1/0/all/0/1\">Raphael Pisoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmi_S/0/1/0/all/0/1\">Sri Lakshmi</a>",
          "description": "CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal\nmodel that jointly learns representations of images and texts. The model is\ntrained on a massive amount of English data and shows impressive performance on\nzero-shot classification tasks. Training the same model on a different language\nis not trivial, since data in other languages might be not enough and the model\nneeds high-quality translations of the texts to guarantee a good performance.\nIn this paper, we present the first CLIP model for the Italian Language\n(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show\nthat CLIP-Italian outperforms the multilingual CLIP model on the tasks of image\nretrieval and zero-shot classification.",
          "link": "http://arxiv.org/abs/2108.08688",
          "publishedOn": "2021-08-20T01:53:50.419Z",
          "wordCount": 552,
          "title": "Contrastive Language-Image Pre-training for the Italian Language. (arXiv:2108.08688v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "The performance of a computer vision model depends on the size and quality of\nits training data. Recent studies have unveiled previously-unknown composition\nbiases in common image datasets which then lead to skewed model outputs, and\nhave proposed methods to mitigate these biases. However, most existing works\nassume that human-generated annotations can be considered gold-standard and\nunbiased. In this paper, we reveal that this assumption can be problematic, and\nthat special care should be taken to prevent models from learning such\nannotation biases. We focus on facial expression recognition and compare the\nlabel biases between lab-controlled and in-the-wild datasets. We demonstrate\nthat many expression datasets contain significant annotation biases between\ngenders, especially when it comes to the happy and angry expressions, and that\ntraditional methods cannot fully mitigate such biases in trained models. To\nremove expression annotation bias, we propose an AU-Calibrated Facial\nExpression Recognition (AUC-FER) framework that utilizes facial action units\n(AUs) and incorporates the triplet loss into the objective function.\nExperimental results suggest that the proposed method is more effective in\nremoving expression annotation bias than existing techniques.",
          "link": "http://arxiv.org/abs/2108.08504",
          "publishedOn": "2021-08-20T01:53:50.413Z",
          "wordCount": 631,
          "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition. (arXiv:2108.08504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yongmei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozerov_M/0/1/0/all/0/1\">Mikhail G. Mozerov</a>",
          "description": "A signed distance function (SDF) as the 3D shape description is one of the\nmost effective approaches to represent 3D geometry for rendering and\nreconstruction. Our work is inspired by the state-of-the-art method DeepSDF\nthat learns and analyzes the 3D shape as the iso-surface of its shell and this\nmethod has shown promising results especially in the 3D shape reconstruction\nand compression domain. In this paper, we consider the degeneration problem of\nreconstruction coming from the capacity decrease of the DeepSDF model, which\napproximates the SDF with a neural network and a single latent code. We propose\nLocal Geometry Code Learning (LGCL), a model that improves the original DeepSDF\nresults by learning from a local shape geometry of the full 3D shape. We add an\nextra graph neural network to split the single transmittable latent code into a\nset of local latent codes distributed on the 3D shape. Mentioned latent codes\nare used to approximate the SDF in their local regions, which will alleviate\nthe complexity of the approximation compared to the original DeepSDF.\nFurthermore, we introduce a new geometric loss function to facilitate the\ntraining of these local latent codes. Note that other local shape adjusting\nmethods use the 3D voxel representation, which in turn is a problem highly\ndifficult to solve or even is insolvable. In contrast, our architecture is\nbased on graph processing implicitly and performs the learning regression\nprocess directly in the latent code space, thus make the proposed architecture\nmore flexible and also simple for realization. Our experiments on 3D shape\nreconstruction demonstrate that our LGCL method can keep more details with a\nsignificantly smaller size of the SDF decoder and outperforms considerably the\noriginal DeepSDF method under the most important quantitative metrics.",
          "link": "http://arxiv.org/abs/2108.08593",
          "publishedOn": "2021-08-20T01:53:50.406Z",
          "wordCount": 731,
          "title": "3D Shapes Local Geometry Codes Learning with SDF. (arXiv:2108.08593v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08551",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Heming Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katto_J/0/1/0/all/0/1\">Jiro Katto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyang Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yibo Fan</a>",
          "description": "In this paper, we propose a learned video codec with a residual prediction\nnetwork (RP-Net) and a feature-aided loop filter (LF-Net). For the RP-Net, we\nexploit the residual of previous multiple frames to further eliminate the\nredundancy of the current frame residual. For the LF-Net, the features from\nresidual decoding network and the motion compensation network are used to aid\nthe reconstruction quality. To reduce the complexity, a light ResNet structure\nis used as the backbone for both RP-Net and LF-Net. Experimental results\nillustrate that we can save about 10% BD-rate compared with previous learned\nvideo compression frameworks. Moreover, we can achieve faster coding speed due\nto the ResNet backbone. This project is available at\nhttps://github.com/chaoliu18/RPLVC.",
          "link": "http://arxiv.org/abs/2108.08551",
          "publishedOn": "2021-08-20T01:53:50.389Z",
          "wordCount": 570,
          "title": "Learned Video Compression with Residual Prediction and Loop Filter. (arXiv:2108.08551v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1\">Yan Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1\">Fabian Manhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Directly regressing all 6 degrees-of-freedom (6DoF) for the object pose (e.g.\nthe 3D rotation and translation) in a cluttered environment from a single RGB\nimage is a challenging problem. While end-to-end methods have recently\ndemonstrated promising results at high efficiency, they are still inferior when\ncompared with elaborate P$n$P/RANSAC-based approaches in terms of pose\naccuracy. In this work, we address this shortcoming by means of a novel\nreasoning about self-occlusion, in order to establish a two-layer\nrepresentation for 3D objects which considerably enhances the accuracy of\nend-to-end 6D pose estimation. Our framework, named SO-Pose, takes a single RGB\nimage as input and respectively generates 2D-3D correspondences as well as\nself-occlusion information harnessing a shared encoder and two separate\ndecoders. Both outputs are then fused to directly regress the 6DoF pose\nparameters. Incorporating cross-layer consistencies that align correspondences,\nself-occlusion and 6D pose, we can further improve accuracy and robustness,\nsurpassing or rivaling all other state-of-the-art approaches on various\nchallenging datasets.",
          "link": "http://arxiv.org/abs/2108.08367",
          "publishedOn": "2021-08-20T01:53:50.382Z",
          "wordCount": 600,
          "title": "SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation. (arXiv:2108.08367v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hanbyul Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1\">Greg Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>",
          "description": "We introduce D3D-HOI: a dataset of monocular videos with ground truth\nannotations of 3D object pose, shape and part motion during human-object\ninteractions. Our dataset consists of several common articulated objects\ncaptured from diverse real-world scenes and camera viewpoints. Each manipulated\nobject (e.g., microwave oven) is represented with a matching 3D parametric\nmodel. This data allows us to evaluate the reconstruction quality of\narticulated objects and establish a benchmark for this challenging task. In\nparticular, we leverage the estimated 3D human pose for more accurate inference\nof the object spatial layout and dynamics. We evaluate this approach on our\ndataset, demonstrating that human-object relations can significantly reduce the\nambiguity of articulated object reconstructions from challenging real-world\nvideos. Code and dataset are available at\nhttps://github.com/facebookresearch/d3d-hoi.",
          "link": "http://arxiv.org/abs/2108.08420",
          "publishedOn": "2021-08-20T01:53:50.375Z",
          "wordCount": 559,
          "title": "D3D-HOI: Dynamic 3D Human-Object Interactions from Videos. (arXiv:2108.08420v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "While single-view 3D reconstruction has made significant progress benefiting\nfrom deep shape representations in recent years, garment reconstruction is\nstill not solved well due to open surfaces, diverse topologies and complex\ngeometric details. In this paper, we propose a novel learnable Anchored\nUnsigned Distance Function (AnchorUDF) representation for 3D garment\nreconstruction from a single image. AnchorUDF represents 3D shapes by\npredicting unsigned distance fields (UDFs) to enable open garment surface\nmodeling at arbitrary resolution. To capture diverse garment topologies,\nAnchorUDF not only computes pixel-aligned local image features of query points,\nbut also leverages a set of anchor points located around the surface to enrich\n3D position features for query points, which provides stronger 3D space context\nfor the distance function. Furthermore, in order to obtain more accurate point\nprojection direction at inference, we explicitly align the spatial gradient\ndirection of AnchorUDF with the ground-truth direction to the surface during\ntraining. Extensive experiments on two public 3D garment datasets, i.e., MGN\nand Deep Fashion3D, demonstrate that AnchorUDF achieves the state-of-the-art\nperformance on single-view garment reconstruction.",
          "link": "http://arxiv.org/abs/2108.08478",
          "publishedOn": "2021-08-20T01:53:50.368Z",
          "wordCount": 623,
          "title": "Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction. (arXiv:2108.08478v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08467",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Niyas_S/0/1/0/all/0/1\">S. Niyas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1\">M Anand Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "Computer-aided medical image analysis plays a significant role in assisting\nmedical practitioners for expert clinical diagnosis and deciding the optimal\ntreatment plan. At present, convolutional neural networks (CNN) are the\npreferred choice for medical image analysis. In addition, with the rapid\nadvancements in three-dimensional (3D) imaging systems and the availability of\nexcellent hardware and software support to process large volumes of data, 3D\ndeep learning methods are gaining popularity in medical image analysis. Here,\nwe present an extensive review of the recently evolved 3D deep learning methods\nin medical image segmentation. Furthermore, the research gaps and future\ndirections in 3D medical image segmentation are discussed.",
          "link": "http://arxiv.org/abs/2108.08467",
          "publishedOn": "2021-08-20T01:53:50.316Z",
          "wordCount": 574,
          "title": "Medical Image Segmentation using 3D Convolutional Neural Networks: A Review. (arXiv:2108.08467v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Haoran Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">He Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Li Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhicheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>",
          "description": "In this paper, we introduce the Multi-Modal Video Reasoning and Analyzing\nCompetition (MMVRAC) workshop in conjunction with ICCV 2021. This competition\nis composed of four different tracks, namely, video question answering,\nskeleton-based action recognition, fisheye video-based action recognition, and\nperson re-identification, which are based on two datasets: SUTD-TrafficQA and\nUAV-Human. We summarize the top-performing methods submitted by the\nparticipants in this competition and show their results achieved in the\ncompetition.",
          "link": "http://arxiv.org/abs/2108.08344",
          "publishedOn": "2021-08-20T01:53:50.258Z",
          "wordCount": 546,
          "title": "The Multi-Modal Video Reasoning and Analyzing Competition. (arXiv:2108.08344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08363",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zenglin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "This paper strives to classify and detect the relationship between object\ntubelets appearing within a video as a <subject-predicate-object> triplet.\nWhere existing works treat object proposals or tubelets as single entities and\nmodel their relations a posteriori, we propose to classify and detect\npredicates for pairs of object tubelets a priori. We also propose Social\nFabric: an encoding that represents a pair of object tubelets as a composition\nof interaction primitives. These primitives are learned over all relations,\nresulting in a compact representation able to localize and classify relations\nfrom the pool of co-occurring object tubelets across all timespans in a video.\nThe encoding enables our two-stage network. In the first stage, we train Social\nFabric to suggest proposals that are likely interacting. We use the Social\nFabric in the second stage to simultaneously fine-tune and predict predicate\nlabels for the tubelets. Experiments demonstrate the benefit of early video\nrelation modeling, our encoding and the two-stage architecture, leading to a\nnew state-of-the-art on two benchmarks. We also show how the encoding enables\nquery-by-primitive-example to search for spatio-temporal video relations. Code:\nhttps://github.com/shanshuo/Social-Fabric.",
          "link": "http://arxiv.org/abs/2108.08363",
          "publishedOn": "2021-08-20T01:53:50.207Z",
          "wordCount": 624,
          "title": "Social Fabric: Tubelet Compositions for Video Relation Detection. (arXiv:2108.08363v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodford_O/0/1/0/all/0/1\">Oliver J. Woodford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiazhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>",
          "description": "Generative Adversarial Networks (GANs) have achieved huge success in\ngenerating high-fidelity images, however, they suffer from low efficiency due\nto tremendous computational cost and bulky memory usage. Recent efforts on\ncompression GANs show noticeable progress in obtaining smaller generators by\nsacrificing image quality or involving a time-consuming searching process. In\nthis work, we aim to address these issues by introducing a teacher network that\nprovides a search space in which efficient network architectures can be found,\nin addition to performing knowledge distillation. First, we revisit the search\nspace of generative models, introducing an inception-based residual block into\ngenerators. Second, to achieve target computation cost, we propose a one-step\npruning algorithm that searches a student architecture from the teacher model\nand substantially reduces searching cost. It requires no l1 sparsity\nregularization and its associated hyper-parameters, simplifying the training\nprocedure. Finally, we propose to distill knowledge through maximizing feature\nsimilarity between teacher and student via an index named Global Kernel\nAlignment (GKA). Our compressed networks achieve similar or even better image\nfidelity (FID, mIoU) than the original models with much-reduced computational\ncost, e.g., MACs. Code will be released at\nhttps://github.com/snap-research/CAT.",
          "link": "http://arxiv.org/abs/2103.03467",
          "publishedOn": "2021-08-19T01:35:03.930Z",
          "wordCount": 673,
          "title": "Teachers Do More Than Teach: Compressing Image-to-Image Models. (arXiv:2103.03467v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1\">Maxime Kayser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1\">Cornelius Emde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Recently, there has been an increasing number of efforts to introduce models\ncapable of generating natural language explanations (NLEs) for their\npredictions on vision-language (VL) tasks. Such models are appealing, because\nthey can provide human-friendly and comprehensive explanations. However, there\nis a lack of comparison between existing methods, which is due to a lack of\nre-usable evaluation frameworks and a scarcity of datasets. In this work, we\nintroduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable\nvision-language tasks that establishes a unified evaluation framework and\nprovides the first comprehensive comparison of existing approaches that\ngenerate NLEs for VL tasks. It spans four models and three datasets and both\nautomatic metrics and human evaluation are used to assess model-generated\nexplanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs\n(over 430k instances). We also propose a new model that combines UNITER, which\nlearns joint embeddings of images and text, and GPT-2, a pre-trained language\nmodel that is well-suited for text generation. It surpasses the previous state\nof the art by a large margin across all datasets. Code and data are available\nhere: https://github.com/maximek3/e-ViL.",
          "link": "http://arxiv.org/abs/2105.03761",
          "publishedOn": "2021-08-19T01:35:03.923Z",
          "wordCount": 681,
          "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mingyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqiong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "3D ultrasound (US) is widely used for its rich diagnostic information.\nHowever, it is criticized for its limited field of view. 3D freehand US\nreconstruction is promising in addressing the problem by providing broad range\nand freeform scan. The existing deep learning based methods only focus on the\nbasic cases of skill sequences, and the model relies on the training data\nheavily. The sequences in real clinical practice are a mix of diverse skills\nand have complex scanning paths. Besides, deep models should adapt themselves\nto the testing cases with prior knowledge for better robustness, rather than\nonly fit to the training cases. In this paper, we propose a novel approach to\nsensorless freehand 3D US reconstruction considering the complex skill\nsequences. Our contribution is three-fold. First, we advance a novel online\nlearning framework by designing a differentiable reconstruction algorithm. It\nrealizes an end-to-end optimization from section sequences to the reconstructed\nvolume. Second, a self-supervised learning method is developed to explore the\ncontext information that reconstructed by the testing data itself, promoting\nthe perception of the model. Third, inspired by the effectiveness of shape\nprior, we also introduce adversarial training to strengthen the learning of\nanatomical shape prior in the reconstructed volume. By mining the context and\nstructural cues of the testing data, our online learning methods can drive the\nmodel to handle complex skill sequences. Experimental results on developmental\ndysplasia of the hip US and fetal US datasets show that, our proposed method\ncan outperform the start-of-the-art methods regarding the shift errors and path\nsimilarities.",
          "link": "http://arxiv.org/abs/2108.00274",
          "publishedOn": "2021-08-19T01:35:03.916Z",
          "wordCount": 737,
          "title": "Self Context and Shape Prior for Sensorless Freehand 3D Ultrasound Reconstruction. (arXiv:2108.00274v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rempe_D/0/1/0/all/0/1\">Davis Rempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1\">Aaron Hertzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>",
          "description": "We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal\npose and shape. Though substantial progress has been made in estimating 3D\nhuman motion and shape from dynamic observations, recovering plausible pose\nsequences in the presence of noise and occlusions remains a challenge. For this\npurpose, we propose an expressive generative model in the form of a conditional\nvariational autoencoder, which learns a distribution of the change in pose at\neach step of a motion sequence. Furthermore, we introduce a flexible\noptimization-based approach that leverages HuMoR as a motion prior to robustly\nestimate plausible pose and shape from ambiguous observations. Through\nextensive evaluations, we demonstrate that our model generalizes to diverse\nmotions and body shapes after training on a large motion capture dataset, and\nenables motion reconstruction from multiple input modalities including 3D\nkeypoints and RGB(-D) videos.",
          "link": "http://arxiv.org/abs/2105.04668",
          "publishedOn": "2021-08-19T01:35:03.909Z",
          "wordCount": 623,
          "title": "HuMoR: 3D Human Motion Model for Robust Pose Estimation. (arXiv:2105.04668v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Conditional image synthesis aims to create an image according to some\nmulti-modal guidance in the forms of textual descriptions, reference images,\nand image blocks to preserve, as well as their combinations. In this paper,\ninstead of investigating these control signals separately, we propose a new\ntwo-stage architecture, UFC-BERT, to unify any number of multi-modal controls.\nIn UFC-BERT, both the diverse control signals and the synthesized image are\nuniformly represented as a sequence of discrete tokens to be processed by\nTransformer. Different from existing two-stage autoregressive approaches such\nas DALL-E and VQGAN, UFC-BERT adopts non-autoregressive generation (NAR) at the\nsecond stage to enhance the holistic consistency of the synthesized image, to\nsupport preserving specified image blocks, and to improve the synthesis speed.\nFurther, we design a progressive algorithm that iteratively improves the\nnon-autoregressively generated image, with the help of two estimators developed\nfor evaluating the compliance with the controls and evaluating the fidelity of\nthe synthesized image, respectively. Extensive experiments on a newly collected\nlarge-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal\nCelebA-HQ verify that UFC-BERT can synthesize high-fidelity images that comply\nwith flexible multi-modal controls.",
          "link": "http://arxiv.org/abs/2105.14211",
          "publishedOn": "2021-08-19T01:35:03.900Z",
          "wordCount": 666,
          "title": "M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis. (arXiv:2105.14211v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Medical imaging technologies, including computed tomography (CT) or chest\nX-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.\nSince manual report writing is usually too time-consuming, a more intelligent\nauxiliary medical system that could generate medical reports automatically and\nimmediately is urgently needed. In this article, we propose to use the medical\nvisual language BERT (Medical-VLBERT) model to identify the abnormality on the\nCOVID-19 scans and generate the medical report automatically based on the\ndetected lesion regions. To produce more accurate medical reports and minimize\nthe visual-and-linguistic differences, this model adopts an alternate learning\nstrategy with two procedures that are knowledge pretraining and transferring.\nTo be more precise, the knowledge pretraining procedure is to memorize the\nknowledge from medical texts, while the transferring procedure is to utilize\nthe acquired knowledge for professional medical sentences generations through\nobservations of medical images. In practice, for automatic medical report\ngeneration on the COVID-19 cases, we constructed a dataset of 368 medical\nfindings in Chinese and 1104 chest CT scans from The First Affiliated Hospital\nof Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun\nYat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of\nthe COVID-19 training samples, our model was first trained on the large-scale\nChinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for\nfurther fine-tuning. The experimental results showed that Medical-VLBERT\nachieved state-of-the-art performances on terminology prediction and report\ngeneration with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The\nChinese COVID-19 CT dataset is available at https://covid19ct.github.io/.",
          "link": "http://arxiv.org/abs/2108.05067",
          "publishedOn": "2021-08-19T01:35:03.893Z",
          "wordCount": 798,
          "title": "Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagadec_B/0/1/0/all/0/1\">Benoit Lagadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>",
          "description": "Unsupervised person re-identification (ReID) aims at learning discriminative\nidentity features without annotations. Recently, self-supervised contrastive\nlearning has gained increasing attention for its effectiveness in unsupervised\nrepresentation learning. The main idea of instance contrastive learning is to\nmatch a same instance in different augmented views. However, the relationship\nbetween different instances has not been fully explored in previous contrastive\nmethods, especially for instance-level contrastive loss. To address this issue,\nwe propose Inter-instance Contrastive Encoding (ICE) that leverages\ninter-instance pairwise similarity scores to boost previous class-level\ncontrastive ReID methods. We first use pairwise similarity ranking as one-hot\nhard pseudo labels for hard instance contrast, which aims at reducing\nintra-class variance. Then, we use similarity scores as soft pseudo labels to\nenhance the consistency between augmented and original views, which makes our\nmodel more robust to augmentation perturbations. Experiments on several\nlarge-scale person ReID datasets validate the effectiveness of our proposed\nunsupervised method ICE, which is competitive with even supervised methods.\nCode is made available at https://github.com/chenhao2345/ICE.",
          "link": "http://arxiv.org/abs/2103.16364",
          "publishedOn": "2021-08-19T01:35:03.871Z",
          "wordCount": 630,
          "title": "ICE: Inter-instance Contrastive Encoding for Unsupervised Person Re-identification. (arXiv:2103.16364v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04930",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Teli_M/0/1/0/all/0/1\">Mohammad Nayeem Teli</a>",
          "description": "COVID-19 has led to hundreds of millions of cases and millions of deaths\nworldwide since its onset. The fight against this pandemic is on-going on\nmultiple fronts. While vaccinations are picking up speed, there are still\nbillions of unvaccinated people. In this fight against the virus, diagnosis of\nthe disease and isolation of the patients to prevent any spread play a huge\nrole. Machine Learning approaches have assisted in the diagnosis of COVID-19\ncases by analyzing chest X-rays and CT-scan images of patients. To push\nalgorithm development and research in this direction of radiological diagnosis,\na challenge to classify CT-scan series was organized in conjunction with ICCV,\n2021. In this research we present a simple and shallow Convolutional Neural\nNetwork based approach, TeliNet, to classify these CT-scan images of COVID-19\npatients presented as part of this competition. Our results outperform the F1\n`macro' score of the competition benchmark and VGGNet approaches. Our proposed\nsolution is also more lightweight in comparison to the other methods.",
          "link": "http://arxiv.org/abs/2107.04930",
          "publishedOn": "2021-08-19T01:35:03.843Z",
          "wordCount": 676,
          "title": "TeliNet: Classifying CT scan images for COVID-19 diagnosis. (arXiv:2107.04930v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1\">Shin&#x27;ya Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1\">Sekitoshi Kanai</a>",
          "description": "Generative adversarial networks built from deep convolutional neural networks\n(GANs) lack the ability to exactly replicate the high-frequency components of\nnatural images. To alleviate this issue, we introduce two novel training\ntechniques called frequency dropping (F-Drop) and frequency matching (F-Match).\nThe key idea of F-Drop is to filter out unnecessary high-frequency components\nfrom the input images of the discriminators. This simple modification prevents\nthe discriminators from being confused by perturbations of the high-frequency\ncomponents. In addition, F-Drop makes the GANs focus on fitting in the\nlow-frequency domain, in which there are the dominant components of natural\nimages. F-Match minimizes the difference between real and fake images in the\nfrequency domain for generating more realistic images. F-Match is implemented\nas a regularization term in the objective functions of the generators; it\npenalizes the batch mean error in the frequency domain. F-Match helps the\ngenerators to fit in the high-frequency domain filtered out by F-Drop to the\nreal image. We experimentally demonstrate that the combination of F-Drop and\nF-Match improves the generative performance of GANs in both the frequency and\nspatial domain on multiple image benchmarks.",
          "link": "http://arxiv.org/abs/2106.02343",
          "publishedOn": "2021-08-19T01:35:03.835Z",
          "wordCount": 666,
          "title": "F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07977",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Mengu_D/0/1/0/all/0/1\">Deniz Mengu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Veli_M/0/1/0/all/0/1\">Muhammed Veli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rivenson_Y/0/1/0/all/0/1\">Yair Rivenson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>",
          "description": "Diffractive optical networks unify wave optics and deep learning to\nall-optically compute a given machine learning or computational imaging task as\nthe light propagates from the input to the output plane. Here, we report the\ndesign of diffractive optical networks for the classification and\nreconstruction of spatially overlapping, phase-encoded objects. When two\ndifferent phase-only objects spatially overlap, the individual object functions\nare perturbed since their phase patterns are summed up. The retrieval of the\nunderlying phase images from solely the overlapping phase distribution presents\na challenging problem, the solution of which is generally not unique. We show\nthat through a task-specific training process, passive diffractive networks\ncomposed of successive transmissive layers can all-optically and simultaneously\nclassify two different randomly-selected, spatially overlapping phase images at\nthe input. After trained with ~550 million unique combinations of phase-encoded\nhandwritten digits from the MNIST dataset, our blind testing results reveal\nthat the diffractive network achieves an accuracy of >85.8% for all-optical\nclassification of two overlapping phase images of new handwritten digits. In\naddition to all-optical classification of overlapping phase objects, we also\ndemonstrate the reconstruction of these phase images based on a shallow\nelectronic neural network that uses the highly compressed output of the\ndiffractive network as its input (with e.g., ~20-65 times less number of\npixels) to rapidly reconstruct both of the phase images, despite their spatial\noverlap and related phase ambiguity. The presented phase image classification\nand reconstruction framework might find applications in e.g., computational\nimaging, microscopy and quantitative phase imaging fields.",
          "link": "http://arxiv.org/abs/2108.07977",
          "publishedOn": "2021-08-19T01:35:03.816Z",
          "wordCount": 709,
          "title": "Classification and reconstruction of spatially overlapping phase images using diffractive optical networks. (arXiv:2108.07977v1 [physics.optics])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhejun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "End-to-end approaches to autonomous driving commonly rely on expert\ndemonstrations. Although humans are good drivers, they are not good coaches for\nend-to-end algorithms that demand dense on-policy supervision. On the contrary,\nautomated experts that leverage privileged information can efficiently generate\nlarge scale on-policy and off-policy demonstrations. However, existing\nautomated experts for urban driving make heavy use of hand-crafted rules and\nperform suboptimally even on driving simulators, where ground-truth information\nis available. To address these issues, we train a reinforcement learning expert\nthat maps bird's-eye view images to continuous low-level actions. While setting\na new performance upper-bound on CARLA, our expert is also a better coach that\nprovides informative supervision signals for imitation learning agents to learn\nfrom. Supervised by our reinforcement learning coach, a baseline end-to-end\nagent with monocular camera-input achieves expert-level performance. Our\nend-to-end agent achieves a 78% success rate while generalizing to a new town\nand new weather on the NoCrash-dense benchmark and state-of-the-art performance\non the more challenging CARLA LeaderBoard.",
          "link": "http://arxiv.org/abs/2108.08265",
          "publishedOn": "2021-08-19T01:35:03.800Z",
          "wordCount": 610,
          "title": "End-to-End Urban Driving by Imitating a Reinforcement Learning Coach. (arXiv:2108.08265v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12310",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Miao_Y/0/1/0/all/0/1\">Yu-Chun Miao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xi-Le Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jian-Li Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu-Bang Zheng</a>",
          "description": "Image denoising is often empowered by accurate prior information. In recent\nyears, data-driven neural network priors have shown promising performance for\nRGB natural image denoising. Compared to classic handcrafted priors (e.g.,\nsparsity and total variation), the \"deep priors\" are learned using a large\nnumber of training samples -- which can accurately model the complex image\ngenerating process. However, data-driven priors are hard to acquire for\nhyperspectral images (HSIs) due to the lack of training data. A remedy is to\nuse the so-called unsupervised deep image prior (DIP). Under the unsupervised\nDIP framework, it is hypothesized and empirically demonstrated that proper\nneural network structures are reasonable priors of certain types of images, and\nthe network weights can be learned without training data. Nonetheless, the most\neffective unsupervised DIP structures were proposed for natural images instead\nof HSIs. The performance of unsupervised DIP-based HSI denoising is limited by\na couple of serious challenges, namely, network structure design and network\ncomplexity. This work puts forth an unsupervised DIP framework that is based on\nthe classic spatio-spectral decomposition of HSIs. Utilizing the so-called\nlinear mixture model of HSIs, two types of unsupervised DIPs, i.e., U-Net-like\nnetwork and fully-connected networks, are employed to model the abundance maps\nand endmembers contained in the HSIs, respectively. This way, empirically\nvalidated unsupervised DIP structures for natural images can be easily\nincorporated for HSI denoising. Besides, the decomposition also substantially\nreduces network complexity. An efficient alternating optimization algorithm is\nproposed to handle the formulated denoising problem. Semi-real and real data\nexperiments are employed to showcase the effectiveness of the proposed\napproach.",
          "link": "http://arxiv.org/abs/2102.12310",
          "publishedOn": "2021-08-19T01:35:03.793Z",
          "wordCount": 740,
          "title": "Hyperspectral Denoising Using Unsupervised Disentangled Spatio-Spectral Deep Priors. (arXiv:2102.12310v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Runyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>",
          "description": "Spatio-temporal action detection is an important and challenging problem in\nvideo understanding. The existing action detection benchmarks are limited in\naspects of small numbers of instances in a trimmed video or low-level atomic\nactions. This paper aims to present a new multi-person dataset of\nspatio-temporal localized sports actions, coined as MultiSports. We first\nanalyze the important ingredients of constructing a realistic and challenging\ndataset for spatio-temporal action detection by proposing three criteria: (1)\nmulti-person scenes and motion dependent identification, (2) with well-defined\nboundaries, (3) relatively fine-grained classes of high complexity. Based on\nthese guide-lines, we build the dataset of MultiSports v1.0 by selecting 4\nsports classes, collecting 3200 video clips, and annotating 37701 action\ninstances with 902k bounding boxes. Our datasets are characterized with\nimportant properties of high diversity, dense annotation, and high quality. Our\nMulti-Sports, with its realistic setting and detailed annotations, exposes the\nintrinsic challenges of spatio-temporal action detection. To benchmark this, we\nadapt several baseline methods to our dataset and give an in-depth analysis on\nthe action detection results in our dataset. We hope our MultiSports can serve\nas a standard benchmark for spatio-temporal action detection in the future. Our\ndataset website is at https://deeperaction.github.io/multisports/.",
          "link": "http://arxiv.org/abs/2105.07404",
          "publishedOn": "2021-08-19T01:35:03.774Z",
          "wordCount": 676,
          "title": "MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions. (arXiv:2105.07404v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiabo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>",
          "description": "Video activity localisation has recently attained increasing attention due to\nits practical values in automatically localising the most salient visual\nsegments corresponding to their language descriptions (sentences) from\nuntrimmed and unstructured videos. For supervised model training, a temporal\nannotation of both the start and end time index of each video segment for a\nsentence (a video moment) must be given. This is not only very expensive but\nalso sensitive to ambiguity and subjective annotation bias, a much harder task\nthan image labelling. In this work, we develop a more accurate\nweakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM)\nin video moment proposal generation and matching when only a paragraph\ndescription of activities without per-sentence temporal annotation is\navailable. Specifically, we explore two cross-sentence relational constraints:\n(1) Temporal ordering and (2) semantic consistency among sentences in a\nparagraph description of video activities. Existing weakly-supervised\ntechniques only consider within-sentence video segment correlations in training\nwithout considering cross-sentence paragraph context. This can mislead due to\nambiguous expressions of individual sentences with visually indiscriminate\nvideo moment proposals in isolation. Experiments on two publicly available\nactivity localisation datasets show the advantages of our approach over the\nstate-of-the-art weakly supervised methods, especially so when the video\nactivity descriptions become more complex.",
          "link": "http://arxiv.org/abs/2107.11443",
          "publishedOn": "2021-08-19T01:35:03.761Z",
          "wordCount": 679,
          "title": "Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation. (arXiv:2107.11443v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Schyler C. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhuangkun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsourdos_A/0/1/0/all/0/1\">Antonios Tsourdos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weisi Guo</a>",
          "description": "Increased drone proliferation in civilian and professional settings has\ncreated new threat vectors for airports and national infrastructures. The\neconomic damage for a single major airport from drone incursions is estimated\nto be millions per day. Due to the lack of diverse drone training data,\naccurate training of deep learning detection algorithms under scarce data is an\nopen challenge. Existing methods largely rely on collecting diverse and\ncomprehensive experimental drone footage data, artificially induced data\naugmentation, transfer and meta-learning, as well as physics-informed learning.\nHowever, these methods cannot guarantee capturing diverse drone designs and\nfully understanding the deep feature space of drones. Here, we show how\nunderstanding the general distribution of the drone data via a Generative\nAdversarial Network (GAN) and explaining the missing features using Topological\nData Analysis (TDA) - can allow us to acquire missing data to achieve rapid and\nmore accurate learning. We demonstrate our results on a drone image dataset,\nwhich contains both real drone images as well as simulated images from\ncomputer-aided design. When compared to random data collection (usual practice\n- discriminator accuracy of 94.67\\% after 200 epochs), our proposed GAN-TDA\ninformed data collection method offers a significant 4\\% improvement (99.42\\%\nafter 200 epochs). We believe that this approach of exploiting general data\ndistribution knowledge form neural networks can be applied to a wide range of\nscarce data open challenges.",
          "link": "http://arxiv.org/abs/2108.08244",
          "publishedOn": "2021-08-19T01:35:03.692Z",
          "wordCount": 679,
          "title": "Scarce Data Driven Deep Learning of Drones via Generalized Data Distribution Space. (arXiv:2108.08244v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Splietker_M/0/1/0/all/0/1\">Malte Splietker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "Dense real-time tracking and mapping from RGB-D images is an important tool\nfor many robotic applications, such as navigation or grasping. The recently\npresented Directional Truncated Signed Distance Function (DTSDF) is an\naugmentation of the regular TSDF and shows potential for more coherent maps and\nimproved tracking performance. In this work, we present methods for rendering\ndepth- and color maps from the DTSDF, making it a true drop-in replacement for\nthe regular TSDF in established trackers. We evaluate and show, that our method\nincreases re-usability of mapped scenes. Furthermore, we add color integration\nwhich notably improves color-correctness at adjacent surfaces.",
          "link": "http://arxiv.org/abs/2108.08115",
          "publishedOn": "2021-08-19T01:35:03.685Z",
          "wordCount": 556,
          "title": "Rendering and Tracking the Directional TSDF: Modeling Surface Orientation for Coherent Maps. (arXiv:2108.08115v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lindenberger_P/0/1/0/all/0/1\">Philipp Lindenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarlin_P/0/1/0/all/0/1\">Paul-Edouard Sarlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_V/0/1/0/all/0/1\">Viktor Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>",
          "description": "Finding local features that are repeatable across multiple views is a\ncornerstone of sparse 3D reconstruction. The classical image matching paradigm\ndetects keypoints per-image once and for all, which can yield poorly-localized\nfeatures and propagate large errors to the final geometry. In this paper, we\nrefine two key steps of structure-from-motion by a direct alignment of\nlow-level image information from multiple views: we first adjust the initial\nkeypoint locations prior to any geometric estimation, and subsequently refine\npoints and camera poses as a post-processing. This refinement is robust to\nlarge detection noise and appearance changes, as it optimizes a featuremetric\nerror based on dense features predicted by a neural network. This significantly\nimproves the accuracy of camera poses and scene geometry for a wide range of\nkeypoint detectors, challenging viewing conditions, and off-the-shelf deep\nfeatures. Our system easily scales to large image collections, enabling\npixel-perfect crowd-sourced localization at scale. Our code is publicly\navailable at https://github.com/cvg/pixel-perfect-sfm as an add-on to the\npopular SfM software COLMAP.",
          "link": "http://arxiv.org/abs/2108.08291",
          "publishedOn": "2021-08-19T01:35:03.613Z",
          "wordCount": 605,
          "title": "Pixel-Perfect Structure-from-Motion with Featuremetric Refinement. (arXiv:2108.08291v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.06592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>",
          "description": "Previous literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features. However, this was done\nwithout considering the description length of the learned representations which\nis a direct and unbiased measure of the model complexity. In this paper, first\nwe introduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined $\\varphi$. We lastly present Sparsely Activated Networks\n(SANs) that consist of kernels with shared weights that, during encoding, are\nconvolved with the input and then passed through a sparse activation function.\nDuring decoding, the same weights are convolved with the sparse activation map\nand subsequently the partial reconstructions from each weight are summed to\nreconstruct the input. We compare SANs using the five previously defined\nactivation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,\nFMNIST) and show that models that are selected using $\\varphi$ have small\ndescription representation length and consist of interpretable kernels.",
          "link": "http://arxiv.org/abs/1907.06592",
          "publishedOn": "2021-08-19T01:35:03.597Z",
          "wordCount": 700,
          "title": "Sparsely Activated Networks. (arXiv:1907.06592v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1\">David LeBauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1\">Max Burnette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1\">Noah Fahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1\">Rob Kooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1\">Kenton McHenry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "A core objective of the TERRA-REF project was to generate an open-access\nreference dataset for the evaluation of sensing technologies to study plants\nunder field conditions. The TERRA-REF program deployed a suite of\nhigh-resolution, cutting edge technology sensors on a gantry system with the\naim of scanning 1 hectare (10$^4$) at around 1 mm$^2$ spatial resolution\nmultiple times per week. The system contains co-located sensors including a\nstereo-pair RGB camera, a thermal imager, a laser scanner to capture 3D\nstructure, and two hyperspectral cameras covering wavelengths of 300-2500nm.\nThis sensor data is provided alongside over sixty types of traditional plant\nphenotype measurements that can be used to train new machine learning models.\nAssociated weather and environmental measurements, information about agronomic\nmanagement and experimental design, and the genomic sequences of hundreds of\nplant varieties have been collected and are available alongside the sensor and\nplant phenotype data.\n\nOver the course of four years and ten growing seasons, the TERRA-REF system\ngenerated over 1 PB of sensor data and almost 45 million files. The subset that\nhas been released to the public domain accounts for two seasons and about half\nof the total data volume. This provides an unprecedented opportunity for\ninvestigations far beyond the core biological scope of the project.\n\nThe focus of this paper is to provide the Computer Vision and Machine\nLearning communities an overview of the available data and some potential\napplications of this one of a kind data.",
          "link": "http://arxiv.org/abs/2107.14072",
          "publishedOn": "2021-08-19T01:35:03.579Z",
          "wordCount": 733,
          "title": "What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1\">Emilian Radoi</a>",
          "description": "The use of gait for person identification has important advantages such as\nbeing non-invasive, unobtrusive, not requiring cooperation and being less\nlikely to be obscured compared to other biometrics. Existing methods for gait\nrecognition require cooperative gait scenarios, in which a single person is\nwalking multiple times in a straight line in front of a camera. We aim to\naddress the challenges of real-world scenarios in which camera feeds capture\nmultiple people, who in most cases pass in front of the camera only once. We\naddress privacy concerns by using only motion information of walking\nindividuals, with no identifiable appearance-based information. As such, we\npropose a novel weakly supervised learning framework, WildGait, which consists\nof training a Spatio-Temporal Graph Convolutional Network on a large number of\nautomatically annotated skeleton sequences obtained from raw, real-world,\nsurveillance streams to learn useful gait signatures. We collected the training\ndata and compiled the largest dataset of walking skeletons called Uncooperative\nWild Gait, containing over 38k tracklets of anonymized walking 2D skeletons. We\nrelease the dataset for public use. Our results show that, with fine-tuning, we\nsurpass the current state-of-the-art pose-based gait recognition solutions. Our\nproposed method is reliable in training gait recognition methods in\nunconstrained environments, especially in settings with scarce amounts of\nannotated data.",
          "link": "http://arxiv.org/abs/2105.05528",
          "publishedOn": "2021-08-19T01:35:03.572Z",
          "wordCount": 712,
          "title": "WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Rui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zihan Zhou</a>",
          "description": "Human trajectory prediction has received increased attention lately due to\nits importance in applications such as autonomous vehicles and indoor robots.\nHowever, most existing methods make predictions based on human-labeled\ntrajectories and ignore the errors and noises in detection and tracking. In\nthis paper, we study the problem of human trajectory forecasting in raw videos,\nand show that the prediction accuracy can be severely affected by various types\nof tracking errors. Accordingly, we propose a simple yet effective strategy to\ncorrect the tracking failures by enforcing prediction consistency over time.\nThe proposed \"re-tracking\" algorithm can be applied to any existing tracking\nand prediction pipelines. Experiments on public benchmark datasets demonstrate\nthat the proposed method can improve both tracking and prediction performance\nin challenging real-world scenarios. The code and data are available at\nhttps://git.io/retracking-prediction.",
          "link": "http://arxiv.org/abs/2108.08259",
          "publishedOn": "2021-08-19T01:35:03.526Z",
          "wordCount": 588,
          "title": "Towards Robust Human Trajectory Prediction in Raw Videos. (arXiv:2108.08259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Te-Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahjoubfar_A/0/1/0/all/0/1\">Ata Mahjoubfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prusinski_D/0/1/0/all/0/1\">Daniel Prusinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_L/0/1/0/all/0/1\">Luis Stevens</a>",
          "description": "Neuromorphic computing mimics the neural activity of the brain through\nemulating spiking neural networks. In numerous machine learning tasks,\nneuromorphic chips are expected to provide superior solutions in terms of cost\nand power efficiency. Here, we explore the application of Loihi, a neuromorphic\ncomputing chip developed by Intel, for the computer vision task of image\nretrieval. We evaluated the functionalities and the performance metrics that\nare critical in content-based visual search and recommender systems using\ndeep-learning embeddings. Our results show that the neuromorphic solution is\nabout 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and\n12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a\nlightweight convolutional neural network without batching while maintaining the\nsame level of matching accuracy. The study validates the potential of\nneuromorphic computing in low-power image retrieval, as a complementary\nparadigm to the existing von Neumann architectures.",
          "link": "http://arxiv.org/abs/2008.01380",
          "publishedOn": "2021-08-19T01:35:03.498Z",
          "wordCount": 613,
          "title": "Neuromorphic Computing for Content-based Image Retrieval. (arXiv:2008.01380v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Prarthana Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1\">Krzysztof Czarnecki</a>",
          "description": "Existing point-cloud based 3D object detectors use convolution-like operators\nto process information in a local neighbourhood with fixed-weight kernels and\naggregate global context hierarchically. However, non-local neural networks and\nself-attention for 2D vision have shown that explicitly modeling long-range\ninteractions can lead to more robust and competitive models. In this paper, we\npropose two variants of self-attention for contextual modeling in 3D object\ndetection by augmenting convolutional features with self-attention features. We\nfirst incorporate the pairwise self-attention mechanism into the current\nstate-of-the-art BEV, voxel and point-based detectors and show consistent\nimprovement over strong baseline models of up to 1.5 3D AP while simultaneously\nreducing their parameter footprint and computational cost by 15-80% and 30-50%,\nrespectively, on the KITTI validation set. We next propose a self-attention\nvariant that samples a subset of the most representative features by learning\ndeformations over randomly sampled locations. This not only allows us to scale\nexplicit global contextual modeling to larger point-clouds, but also leads to\nmore discriminative and informative feature descriptors. Our method can be\nflexibly applied to most state-of-the-art detectors with increased accuracy and\nparameter and compute efficiency. We show our proposed method improves 3D\nobject detection performance on KITTI, nuScenes and Waymo Open datasets. Code\nis available at https://github.com/AutoVision-cloud/SA-Det3D.",
          "link": "http://arxiv.org/abs/2101.02672",
          "publishedOn": "2021-08-19T01:35:03.472Z",
          "wordCount": 694,
          "title": "SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection. (arXiv:2101.02672v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhongxi Zheng</a>",
          "description": "Learning from noisy labels is an important concern because of the lack of\naccurate ground-truth labels in plenty of real-world scenarios. In practice,\nvarious approaches for this concern first make some corrections corresponding\nto potentially noisy-labeled instances, and then update predictive model with\ninformation of the made corrections. However, in specific areas, such as\nmedical histopathology whole slide image analysis (MHWSIA), it is often\ndifficult or even impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. In this paper, we\nfocus on alleviating these two problems. For the problem 1), we present\none-step abductive multi-target learning (OSAMTL) that imposes a one-step\nlogical reasoning upon machine learning via a multi-target learning procedure\nto constrain the predictions of the learning model to be subject to our prior\nknowledge about the true target. For the problem 2), we propose a logical\nassessment formula (LAF) that evaluates the logical rationality of the outputs\nof an approach by estimating the consistencies between the predictions of the\nlearning model and the logical facts narrated from the results of the one-step\nlogical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori\n(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable\nthe machine learning model achieving logically more rational predictions, which\nis beyond various state-of-the-art approaches in handling complex noisy labels.",
          "link": "http://arxiv.org/abs/2011.14956",
          "publishedOn": "2021-08-19T01:35:03.464Z",
          "wordCount": 767,
          "title": "Handling Noisy Labels via One-Step Abductive Multi-Target Learning: An Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.",
          "link": "http://arxiv.org/abs/2108.08217",
          "publishedOn": "2021-08-19T01:35:03.456Z",
          "wordCount": 678,
          "title": "X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Song Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shengsheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenkui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>",
          "description": "Video-Text Retrieval has been a hot research topic with the growth of\nmultimedia data on the internet. Transformer for video-text learning has\nattracted increasing attention due to its promising performance. However,\nexisting cross-modal transformer approaches typically suffer from two major\nlimitations: 1) Exploitation of the transformer architecture where different\nlayers have different feature characteristics is limited; 2) End-to-end\ntraining mechanism limits negative sample interactions in a mini-batch. In this\npaper, we propose a novel approach named Hierarchical Transformer (HiT) for\nvideo-text retrieval. HiT performs Hierarchical Cross-modal Contrastive\nMatching in both feature-level and semantic-level, achieving multi-view and\ncomprehensive retrieval results. Moreover, inspired by MoCo, we propose\nMomentum Cross-modal Contrast for cross-modal learning to enable large-scale\nnegative sample interactions on-the-fly, which contributes to the generation of\nmore precise and discriminative representations. Experimental results on the\nthree major Video-Text Retrieval benchmark datasets demonstrate the advantages\nof our method.",
          "link": "http://arxiv.org/abs/2103.15049",
          "publishedOn": "2021-08-19T01:35:03.450Z",
          "wordCount": 620,
          "title": "HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval. (arXiv:2103.15049v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper presents a three-tier modality alignment approach to learning\ntext-image joint embedding, coined as JEMA, for cross-modal retrieval of\ncooking recipes and food images. The first tier improves recipe text embedding\nby optimizing the LSTM networks with term extraction and ranking enhanced\nsequence patterns, and optimizes the image embedding by combining the\nResNeXt-101 image encoder with the category embedding using wideResNet-50 with\nword2vec. The second tier modality alignment optimizes the textual-visual joint\nembedding loss function using a double batch-hard triplet loss with soft-margin\noptimization. The third modality alignment incorporates two types of\ncross-modality alignments as the auxiliary loss regularizations to further\nreduce the alignment errors in the joint learning of the two modality-specific\nembedding functions. The category-based cross-modal alignment aims to align the\nimage category with the recipe category as a loss regularization to the joint\nembedding. The cross-modal discriminator-based alignment aims to add the\nvisual-textual embedding distribution alignment to further regularize the joint\nembedding loss. Extensive experiments with the one-million recipes benchmark\ndataset Recipe1M demonstrate that the proposed JEMA approach outperforms the\nstate-of-the-art cross-modal embedding methods for both image-to-recipe and\nrecipe-to-image retrievals.",
          "link": "http://arxiv.org/abs/2108.03788",
          "publishedOn": "2021-08-19T01:35:03.433Z",
          "wordCount": 668,
          "title": "Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08202",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1\">Kaixin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shizun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoqing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>",
          "description": "Internet video delivery has undergone a tremendous explosion of growth over\nthe past few years. However, the quality of video delivery system greatly\ndepends on the Internet bandwidth. Deep Neural Networks (DNNs) are utilized to\nimprove the quality of video delivery recently. These methods divide a video\ninto chunks, and stream LR video chunks and corresponding content-aware models\nto the client. The client runs the inference of models to super-resolve the LR\nchunks. Consequently, a large number of models are streamed in order to deliver\na video. In this paper, we first carefully study the relation between models of\ndifferent chunks, then we tactfully design a joint training framework along\nwith the Content-aware Feature Modulation (CaFM) layer to compress these models\nfor neural video delivery. {\\bf With our method, each video chunk only requires\nless than $1\\% $ of original parameters to be streamed, achieving even better\nSR performance.} We conduct extensive experiments across various SR backbones,\nvideo time length, and scaling factors to demonstrate the advantages of our\nmethod. Besides, our method can be also viewed as a new approach of video\ncoding. Our primary experiments achieve better video quality compared with the\ncommercial H.264 and H.265 standard under the same storage cost, showing the\ngreat potential of the proposed method. Code is available\nat:\\url{https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021}",
          "link": "http://arxiv.org/abs/2108.08202",
          "publishedOn": "2021-08-19T01:35:03.426Z",
          "wordCount": 686,
          "title": "Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation. (arXiv:2108.08202v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08286",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bhat_G/0/1/0/all/0/1\">Goutam Bhat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "We propose a deep reparametrization of the maximum a posteriori formulation\ncommonly employed in multi-frame image restoration tasks. Our approach is\nderived by introducing a learned error metric and a latent representation of\nthe target image, which transforms the MAP objective to a deep feature space.\nThe deep reparametrization allows us to directly model the image formation\nprocess in the latent space, and to integrate learned image priors into the\nprediction. Our approach thereby leverages the advantages of deep learning,\nwhile also benefiting from the principled multi-frame fusion provided by the\nclassical MAP formulation. We validate our approach through comprehensive\nexperiments on burst denoising and burst super-resolution datasets. Our\napproach sets a new state-of-the-art for both tasks, demonstrating the\ngenerality and effectiveness of the proposed formulation.",
          "link": "http://arxiv.org/abs/2108.08286",
          "publishedOn": "2021-08-19T01:35:03.419Z",
          "wordCount": 576,
          "title": "Deep Reparametrization of Multi-Frame Super-Resolution and Denoising. (arXiv:2108.08286v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1805.12323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jimmy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peck_D/0/1/0/all/0/1\">Diondra Peck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_S/0/1/0/all/0/1\">Scott Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dialani_V/0/1/0/all/0/1\">Vandana Dialani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patterson_G/0/1/0/all/0/1\">Genevieve Patterson</a>",
          "description": "We propose DeepMiner, a framework to discover interpretable representations\nin deep neural networks and to build explanations for medical predictions. By\nprobing convolutional neural networks (CNNs) trained to classify cancer in\nmammograms, we show that many individual units in the final convolutional layer\nof a CNN respond strongly to diseased tissue concepts specified by the BI-RADS\nlexicon. After expert annotation of the interpretable units, our proposed\nmethod is able to generate explanations for CNN mammogram classification that\nare consistent with ground truth radiology reports on the Digital Database for\nScreening Mammography. We show that DeepMiner not only enables better\nunderstanding of the nuances of CNN classification decisions but also possibly\ndiscovers new visual knowledge relevant to medical diagnosis.",
          "link": "http://arxiv.org/abs/1805.12323",
          "publishedOn": "2021-08-19T01:35:03.413Z",
          "wordCount": 606,
          "title": "DeepMiner: Discovering Interpretable Representations for Mammogram Classification and Explanation. (arXiv:1805.12323v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yangdi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1\">Yang Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbo He</a>",
          "description": "Recent studies on the memorization effects of deep neural networks on noisy\nlabels show that the networks first fit the correctly-labeled training samples\nbefore memorizing the mislabeled samples. Motivated by this early-learning\nphenomenon, we propose a novel method to prevent memorization of the mislabeled\nsamples. Unlike the existing approaches which use the model output to identify\nor ignore the mislabeled samples, we introduce an indicator branch to the\noriginal model and enable the model to produce a confidence value for each\nsample. The confidence values are incorporated in our loss function which is\nlearned to assign large confidence values to correctly-labeled samples and\nsmall confidence values to mislabeled samples. We also propose an auxiliary\nregularization term to further improve the robustness of the model. To improve\nthe performance, we gradually correct the noisy labels with a well-designed\ntarget estimation strategy. We provide the theoretical analysis and conduct the\nexperiments on synthetic and real-world datasets, demonstrating that our\napproach achieves comparable results to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.08212",
          "publishedOn": "2021-08-19T01:35:03.404Z",
          "wordCount": 602,
          "title": "Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girase_H/0/1/0/all/0/1\">Harshayu Girase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1\">Haiming Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1\">Srikanth Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanehara_A/0/1/0/all/0/1\">Akira Kanehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>",
          "description": "Recent advances in trajectory prediction have shown that explicit reasoning\nabout agents' intent is important to accurately forecast their motion. However,\nthe current research activities are not directly applicable to intelligent and\nsafety critical systems. This is mainly because very few public datasets are\navailable, and they only consider pedestrian-specific intents for a short\ntemporal horizon from a restricted egocentric view. To this end, we propose\nLOKI (LOng term and Key Intentions), a novel large-scale dataset that is\ndesigned to tackle joint trajectory and intention prediction for heterogeneous\ntraffic agents (pedestrians and vehicles) in an autonomous driving setting. The\nLOKI dataset is created to discover several factors that may affect intention,\nincluding i) agent's own will, ii) social interactions, iii) environmental\nconstraints, and iv) contextual information. We also propose a model that\njointly performs trajectory and intention prediction, showing that recurrently\nreasoning about intention can assist with trajectory prediction. We show our\nmethod outperforms state-of-the-art trajectory prediction methods by upto\n$27\\%$ and also provide a baseline for frame-wise intention estimation.",
          "link": "http://arxiv.org/abs/2108.08236",
          "publishedOn": "2021-08-19T01:35:03.397Z",
          "wordCount": 640,
          "title": "LOKI: Long Term and Key Intentions for Trajectory Prediction. (arXiv:2108.08236v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baozhou_Z/0/1/0/all/0/1\">Zhu Baozhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofstee_P/0/1/0/all/0/1\">Peter Hofstee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ars_Z/0/1/0/all/0/1\">Zaid Al-Ars</a>",
          "description": "Attention mechanism has been regarded as an advanced technique to capture\nlong-range feature interactions and to boost the representation capability for\nconvolutional neural networks. However, we found two ignored problems in\ncurrent attentional activations-based models: the approximation problem and the\ninsufficient capacity problem of the attention maps. To solve the two problems\ntogether, we initially propose an attention module for convolutional neural\nnetworks by developing an AW-convolution, where the shape of attention maps\nmatches that of the weights rather than the activations. Our proposed attention\nmodule is a complementary method to previous attention-based schemes, such as\nthose that apply the attention mechanism to explore the relationship between\nchannel-wise and spatial features. Experiments on several datasets for image\nclassification and object detection tasks show the effectiveness of our\nproposed attention module. In particular, our proposed attention module\nachieves 1.00% Top-1 accuracy improvement on ImageNet classification over a\nResNet101 baseline and 0.63 COCO-style Average Precision improvement on the\nCOCO object detection on top of a Faster R-CNN baseline with the backbone of\nResNet101-FPN. When integrating with the previous attentional activations-based\nmodels, our proposed attention module can further increase their Top-1 accuracy\non ImageNet classification by up to 0.57% and COCO-style Average Precision on\nthe COCO object detection by up to 0.45. Code and pre-trained models will be\npublicly available.",
          "link": "http://arxiv.org/abs/2108.08205",
          "publishedOn": "2021-08-19T01:35:03.378Z",
          "wordCount": 650,
          "title": "An Attention Module for Convolutional Neural Networks. (arXiv:2108.08205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1812.02302",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Damelin_S/0/1/0/all/0/1\">Steven B. Damelin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ragozin_D/0/1/0/all/0/1\">David L. Ragozin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Werman_M/0/1/0/all/0/1\">Michael Werman</a>",
          "description": "We study Min-Max affine approximants of a continuous convex or concave\nfunction $f:\\Delta\\subset \\mathbb R^k\\xrightarrow{} \\mathbb R$ where $\\Delta$\nis a convex compact subset of $\\mathbb R^k$. In the case when $\\Delta$ is a\nsimplex we prove that there is a vertical translate of the supporting\nhyperplane in $\\mathbb R^{k+1}$ of the graph of $f$ at the vertices which is\nthe unique best affine approximant to $f$ on $\\Delta$. For $k=1$, this result\nprovides an extension of the Chebyshev equioscillation theorem for linear\napproximants. Our result has interesting connections to the computer graphics\nproblem of rapid rendering of projective transformations.",
          "link": "http://arxiv.org/abs/1812.02302",
          "publishedOn": "2021-08-19T01:35:03.372Z",
          "wordCount": 692,
          "title": "On Min-Max affine approximants of convex or concave real valued functions from $\\mathbb R^k$, Chebyshev equioscillation and graphics. (arXiv:1812.02302v10 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1\">Pourya Shamsolmoali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1\">Masoumeh Zareapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>",
          "description": "Object detection is a challenging task in remote sensing because objects only\noccupy a few pixels in the images, and the models are required to\nsimultaneously learn object locations and detection. Even though the\nestablished approaches well perform for the objects of regular sizes, they\nachieve weak performance when analyzing small ones or getting stuck in the\nlocal minima (e.g. false object parts). Two possible issues stand in their way.\nFirst, the existing methods struggle to perform stably on the detection of\nsmall objects because of the complicated background. Second, most of the\nstandard methods used hand-crafted features, and do not work well on the\ndetection of objects parts of which are missing. We here address the above\nissues and propose a new architecture with a multiple patch feature pyramid\nnetwork (MPFP-Net). Different from the current models that during training only\npursue the most discriminative patches, in MPFPNet the patches are divided into\nclass-affiliated subsets, in which the patches are related and based on the\nprimary loss function, a sequence of smooth loss functions are determined for\nthe subsets to improve the model for collecting small object parts. To enhance\nthe feature representation for patch selection, we introduce an effective\nmethod to regularize the residual values and make the fusion transition layers\nstrictly norm-preserving. The network contains bottom-up and crosswise\nconnections to fuse the features of different scales to achieve better\naccuracy, compared to several state-of-the-art object detection models. Also,\nthe developed architecture is more efficient than the baselines.",
          "link": "http://arxiv.org/abs/2108.08063",
          "publishedOn": "2021-08-19T01:35:03.365Z",
          "wordCount": 698,
          "title": "Multi-patch Feature Pyramid Network for Weakly Supervised Object Detection in Optical Remote Sensing Images. (arXiv:2108.08063v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huawei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiashu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Huaiguang Lei</a>",
          "description": "Fingerprint is an important biological feature of human body, which contains\nabundant gender information. At present, the academic research of fingerprint\ngender characteristics is generally at the level of understanding, while the\nstandardization research is quite limited. In this work, we propose a more\nrobust method, Dense Dilated Convolution ResNet (DDC-ResNet) to extract valid\ngender information from fingerprints. By replacing the normal convolution\noperations with the atrous convolution in the backbone, prior knowledge is\nprovided to keep the edge details and the global reception field can be\nextended. We explored the results in 3 ways: 1) The efficiency of the\nDDC-ResNet. 6 typical methods of automatic feature extraction coupling with 9\nmainstream classifiers are evaluated in our dataset with fair implementation\ndetails. Experimental results demonstrate that the combination of our approach\noutperforms other combinations in terms of average accuracy and separate-gender\naccuracy. It reaches 96.5% for average and 0.9752 (males)/0.9548 (females) for\nseparate-gender accuracy. 2) The effect of fingers. It is found that the best\nperformance of classifying gender with separate fingers is achieved by the\nright ring finger. 3) The effect of specific features. Based on the\nobservations of the concentrations of fingerprints visualized by our approach,\nit can be inferred that loops and whorls (level 1), bifurcations (level 2), as\nwell as line shapes (level 3) are connected with gender. Finally, we will open\nsource the dataset that contains 6000 fingerprint images",
          "link": "http://arxiv.org/abs/2108.08233",
          "publishedOn": "2021-08-19T01:35:03.358Z",
          "wordCount": 672,
          "title": "Research on Gender-related Fingerprint Features. (arXiv:2108.08233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Quan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anpei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haimin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "We introduce GNeRF, a framework to marry Generative Adversarial Networks\n(GAN) with Neural Radiance Field (NeRF) reconstruction for the complex\nscenarios with unknown and even randomly initialized camera poses. Recent\nNeRF-based advances have gained popularity for remarkable realistic novel view\nsynthesis. However, most of them heavily rely on accurate camera poses\nestimation, while few recent methods can only optimize the unknown camera poses\nin roughly forward-facing scenes with relatively short camera trajectories and\nrequire rough camera poses initialization. Differently, our GNeRF only utilizes\nrandomly initialized poses for complex outside-in scenarios. We propose a novel\ntwo-phases end-to-end framework. The first phase takes the use of GANs into the\nnew realm for optimizing coarse camera poses and radiance fields jointly, while\nthe second phase refines them with additional photometric loss. We overcome\nlocal minima using a hybrid and iterative optimization scheme. Extensive\nexperiments on a variety of synthetic and natural scenes demonstrate the\neffectiveness of GNeRF. More impressively, our approach outperforms the\nbaselines favorably in those scenes with repeated patterns or even low textures\nthat are regarded as extremely challenging before.",
          "link": "http://arxiv.org/abs/2103.15606",
          "publishedOn": "2021-08-19T01:35:03.342Z",
          "wordCount": 667,
          "title": "GNeRF: GAN-based Neural Radiance Field without Posed Camera. (arXiv:2103.15606v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>",
          "description": "Recently, FCNs have attracted widespread attention in the CD field. In\npursuit of better CD performance, it has become a tendency to design deeper and\nmore complicated FCNs, which inevitably brings about huge numbers of parameters\nand an unbearable computational burden. With the goal of designing a quite deep\narchitecture to obtain more precise CD results while simultaneously decreasing\nparameter numbers to improve efficiency, in this work, we present a very deep\nand efficient CD network, entitled EffCDNet. In EffCDNet, to reduce the\nnumerous parameters associated with deep architecture, an efficient convolution\nconsisting of depth-wise convolution and group convolution with a channel\nshuffle mechanism is introduced to replace standard convolutional layers. In\nterms of the specific network architecture, EffCDNet does not use mainstream\nUNet-like architecture, but rather adopts the architecture with a very deep\nencoder and a lightweight decoder. In the very deep encoder, two very deep\nsiamese streams stacked by efficient convolution first extract two highly\nrepresentative and informative feature maps from input image-pairs.\nSubsequently, an efficient ASPP module is designed to capture multi-scale\nchange information. In the lightweight decoder, a recurrent criss-cross\nself-attention (RCCA) module is applied to efficiently utilize non-local\nsimilar feature representations to enhance discriminability for each pixel,\nthus effectively separating the changed and unchanged regions. Moreover, to\ntackle the optimization problem in confused pixels, two novel loss functions\nbased on information entropy are presented. On two challenging CD datasets, our\napproach outperforms other SOTA FCN-based methods, with only benchmark-level\nparameter numbers and quite low computational overhead.",
          "link": "http://arxiv.org/abs/2108.08157",
          "publishedOn": "2021-08-19T01:35:03.335Z",
          "wordCount": 716,
          "title": "Towards Deep and Efficient: A Deep Siamese Self-Attention Fully Efficient Convolutional Network for Change Detection in VHR Images. (arXiv:2108.08157v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1\">Anastasiya Dykyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1\">Paulo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1\">Eric Jul</a>",
          "description": "Capillaries are the smallest vessels in the body responsible for the delivery\nof oxygen and nutrients to the surrounding cells. Various diseases have been\nshown to alter the density of nutritive capillaries and the flow velocity of\nerythrocytes. In previous studies, capillary density and flow velocity have\nbeen assessed manually by trained specialists. Manual analysis of a standard\n20-second long microvascular video takes on average 20 minutes and requires\nextensive training. Several studies have reported that manual analysis hinders\nthe application of microvascular microscopy in a clinical setting. In this\npaper, we present a fully automated state-of-the-art system, called\nCapillaryNet, that can quantify skin nutritive capillary density and red blood\ncell velocity from handheld microscopy videos. Moreover, CapillaryNet measures\nseveral novel microvascular parameters that researchers were previously unable\nto quantify, i.e. capillary hematocrit and Intra-capillary flow velocity\nheterogeneity. Our system has been used to analyze skin microcirculation videos\nfrom various patient groups (COVID-19, pancreatitis, and acute heart diseases).\nOur proposed system excels from existing capillary detection systems as it\ncombines the speed of traditional computer vision algorithms and the accuracy\nof convolutional neural networks.",
          "link": "http://arxiv.org/abs/2104.11574",
          "publishedOn": "2021-08-19T01:35:03.328Z",
          "wordCount": 740,
          "title": "CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaoshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "Stereo-based 3D detection aims at detecting 3D object bounding boxes from\nstereo images using intermediate depth maps or implicit 3D geometry\nrepresentations, which provides a low-cost solution for 3D perception. However,\nits performance is still inferior compared with LiDAR-based detection\nalgorithms. To detect and localize accurate 3D bounding boxes, LiDAR-based\nmodels can encode accurate object boundaries and surface normal directions from\nLiDAR point clouds. However, the detection results of stereo-based detectors\nare easily affected by the erroneous depth features due to the limitation of\nstereo matching. To solve the problem, we propose LIGA-Stereo (LiDAR Geometry\nAware Stereo Detector) to learn stereo-based 3D detectors under the guidance of\nhigh-level geometry-aware representations of LiDAR-based detection models. In\naddition, we found existing voxel-based stereo detectors failed to learn\nsemantic features effectively from indirect 3D supervisions. We attach an\nauxiliary 2D detection head to provide direct 2D semantic supervisions.\nExperiment results show that the above two strategies improved the geometric\nand semantic representation capabilities. Compared with the state-of-the-art\nstereo detector, our method has improved the 3D detection performance of cars,\npedestrians, cyclists by 10.44%, 5.69%, 5.97% mAP respectively on the official\nKITTI benchmark. The gap between stereo-based and LiDAR-based 3D detectors is\nfurther narrowed.",
          "link": "http://arxiv.org/abs/2108.08258",
          "publishedOn": "2021-08-19T01:35:03.318Z",
          "wordCount": 641,
          "title": "LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector. (arXiv:2108.08258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengsu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jianwei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuefeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "The past year has witnessed the rapid development of applying the Transformer\nmodule to vision problems. While some researchers have demonstrated that\nTransformer-based models enjoy a favorable ability of fitting data, there are\nstill growing number of evidences showing that these models suffer over-fitting\nespecially when the training data is limited. This paper offers an empirical\nstudy by performing step-by-step operations to gradually transit a\nTransformer-based model to a convolution-based model. The results we obtain\nduring the transition process deliver useful messages for improving visual\nrecognition. Based on these observations, we propose a new architecture named\nVisformer, which is abbreviated from the `Vision-friendly Transformer'. With\nthe same computational complexity, Visformer outperforms both the\nTransformer-based and convolution-based models in terms of ImageNet\nclassification accuracy, and the advantage becomes more significant when the\nmodel complexity is lower or the training set is smaller. The code is available\nat https://github.com/danczs/Visformer.",
          "link": "http://arxiv.org/abs/2104.12533",
          "publishedOn": "2021-08-19T01:35:03.285Z",
          "wordCount": 607,
          "title": "Visformer: The Vision-friendly Transformer. (arXiv:2104.12533v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chia-Hsiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yu-Shin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yuan-Yao Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hung-Yueh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kai-Chiang Wu</a>",
          "description": "Neural architecture search can discover neural networks with good\nperformance, and One-Shot approaches are prevalent. One-Shot approaches\ntypically require a supernet with weight sharing and predictors that predict\nthe performance of architecture. However, the previous methods take much time\nto generate performance predictors thus are inefficient. To this end, we\npropose FOX-NAS that consists of fast and explainable predictors based on\nsimulated annealing and multivariate regression. Our method is\nquantization-friendly and can be efficiently deployed to the edge. The\nexperiments on different hardware show that FOX-NAS models outperform some\nother popular neural network architectures. For example, FOX-NAS matches\nMobileNetV2 and EfficientNet-Lite0 accuracy with 240% and 40% less latency on\nthe edge CPU. FOX-NAS is the 3rd place winner of the 2020 Low-Power Computer\nVision Challenge (LPCVC), DSP classification track. See all evaluation results\nat https://lpcv.ai/competitions/2020. Search code and pre-trained models are\nreleased at https://github.com/great8nctu/FOX-NAS.",
          "link": "http://arxiv.org/abs/2108.08189",
          "publishedOn": "2021-08-19T01:35:03.279Z",
          "wordCount": 598,
          "title": "FOX-NAS: Fast, On-device and Explainable Neural Architecture Search. (arXiv:2108.08189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhaoyang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "Based on the powerful feature extraction ability of deep learning\narchitecture, recently, deep-learning based watermarking algorithms have been\nwidely studied. The basic framework of such algorithm is the auto-encoder like\nend-to-end architecture with an encoder, a noise layer and a decoder. The key\nto guarantee robustness is the adversarial training with the differential noise\nlayer. However, we found that none of the existing framework can well ensure\nthe robustness against JPEG compression, which is non-differential but is an\nessential and important image processing operation. To address such\nlimitations, we proposed a novel end-to-end training architecture, which\nutilizes Mini-Batch of Real and Simulated JPEG compression (MBRS) to enhance\nthe JPEG robustness. Precisely, for different mini-batches, we randomly choose\none of real JPEG, simulated JPEG and noise-free layer as the noise layer.\nBesides, we suggest to utilize the Squeeze-and-Excitation blocks which can\nlearn better feature in embedding and extracting stage, and propose a \"message\nprocessor\" to expand the message in a more appreciate way. Meanwhile, to\nimprove the robustness against crop attack, we propose an additive diffusion\nblock into the network. The extensive experimental results have demonstrated\nthe superior performance of the proposed scheme compared with the\nstate-of-the-art algorithms. Under the JPEG compression with quality factor\nQ=50, our models achieve a bit error rate less than 0.01% for extracted\nmessages, with PSNR larger than 36 for the encoded images, which shows the\nwell-enhanced robustness against JPEG attack. Besides, under many other\ndistortions such as Gaussian filter, crop, cropout and dropout, the proposed\nframework also obtains strong robustness. The code implemented by PyTorch\n\\cite{2011torch7} is avaiable in https://github.com/jzyustc/MBRS.",
          "link": "http://arxiv.org/abs/2108.08211",
          "publishedOn": "2021-08-19T01:35:03.270Z",
          "wordCount": 731,
          "title": "MBRS : Enhancing Robustness of DNN-based Watermarking by Mini-Batch of Real and Simulated JPEG Compression. (arXiv:2108.08211v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geneva_P/0/1/0/all/0/1\">Patrick Geneva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoquan Huang</a>",
          "description": "In this paper we present a consistent and distributed state estimator for\nmulti-robot cooperative localization (CL) which efficiently fuses environmental\nfeatures and loop-closure constraints across time and robots. In particular, we\nleverage covariance intersection (CI) to allow each robot to only estimate its\nown state and autocovariance and compensate for the unknown correlations\nbetween robots. Two novel multi-robot methods for utilizing common\nenvironmental SLAM features are introduced and evaluated in terms of accuracy\nand efficiency. Moreover, we adapt CI to enable drift-free estimation through\nthe use of loop-closure measurement constraints to other robots' historical\nposes without a significant increase in computational cost. The proposed\ndistributed CL estimator is validated against its non-realtime centralized\ncounterpart extensively in both simulations and real-world experiments.",
          "link": "http://arxiv.org/abs/2103.12770",
          "publishedOn": "2021-08-19T01:35:03.254Z",
          "wordCount": 576,
          "title": "Distributed Visual-Inertial Cooperative Localization. (arXiv:2103.12770v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_M/0/1/0/all/0/1\">Mike Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1\">Jason Ramapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1\">Anurag Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Atulit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1\">Miguel Angel Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paczan_N/0/1/0/all/0/1\">Nathan Paczan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_R/0/1/0/all/0/1\">Russ Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>",
          "description": "For many fundamental scene understanding tasks, it is difficult or impossible\nto obtain per-pixel ground truth labels from real images. We address this\nchallenge by introducing Hypersim, a photorealistic synthetic dataset for\nholistic indoor scene understanding. To create our dataset, we leverage a large\nrepository of synthetic scenes created by professional artists, and we generate\n77,400 images of 461 indoor scenes with detailed per-pixel labels and\ncorresponding ground truth geometry. Our dataset: (1) relies exclusively on\npublicly available 3D assets; (2) includes complete scene geometry, material\ninformation, and lighting information for every scene; (3) includes dense\nper-pixel semantic instance segmentations and complete camera information for\nevery image; and (4) factors every image into diffuse reflectance, diffuse\nillumination, and a non-diffuse residual term that captures view-dependent\nlighting effects.\n\nWe analyze our dataset at the level of scenes, objects, and pixels, and we\nanalyze costs in terms of money, computation time, and annotation effort.\nRemarkably, we find that it is possible to generate our entire dataset from\nscratch, for roughly half the cost of training a popular open-source natural\nlanguage processing model. We also evaluate sim-to-real transfer performance on\ntwo real-world scene understanding tasks - semantic segmentation and 3D shape\nprediction - where we find that pre-training on our dataset significantly\nimproves performance on both tasks, and achieves state-of-the-art performance\non the most challenging Pix3D test set. All of our rendered image data, as well\nas all the code we used to generate our dataset and perform our experiments, is\navailable online.",
          "link": "http://arxiv.org/abs/2011.02523",
          "publishedOn": "2021-08-19T01:35:03.246Z",
          "wordCount": 775,
          "title": "Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. (arXiv:2011.02523v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>",
          "description": "Vision Transformer (ViT) extends the application range of transformers from\nlanguage processing to computer vision tasks as being an alternative\narchitecture against the existing convolutional neural networks (CNN). Since\nthe transformer-based architecture has been innovative for computer vision\nmodeling, the design convention towards an effective architecture has been less\nstudied yet. From the successful design principles of CNN, we investigate the\nrole of spatial dimension conversion and its effectiveness on transformer-based\narchitecture. We particularly attend to the dimension reduction principle of\nCNNs; as the depth increases, a conventional CNN increases channel dimension\nand decreases spatial dimensions. We empirically show that such a spatial\ndimension reduction is beneficial to a transformer architecture as well, and\npropose a novel Pooling-based Vision Transformer (PiT) upon the original ViT\nmodel. We show that PiT achieves the improved model capability and\ngeneralization performance against ViT. Throughout the extensive experiments,\nwe further show PiT outperforms the baseline on several tasks such as image\nclassification, object detection, and robustness evaluation. Source codes and\nImageNet models are available at https://github.com/naver-ai/pit",
          "link": "http://arxiv.org/abs/2103.16302",
          "publishedOn": "2021-08-19T01:35:03.239Z",
          "wordCount": 646,
          "title": "Rethinking Spatial Dimensions of Vision Transformers. (arXiv:2103.16302v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhaohui Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>",
          "description": "Bicubic downscaling is a prevalent technique used to reduce the video storage\nburden or to accelerate the downstream processing speed. However, the inverse\nupscaling step is non-trivial, and the downscaled video may also deteriorate\nthe performance of downstream tasks. In this paper, we propose a\nself-conditioned probabilistic framework for video rescaling to learn the\npaired downscaling and upscaling procedures simultaneously. During the\ntraining, we decrease the entropy of the information lost in the downscaling by\nmaximizing its probability conditioned on the strong spatial-temporal prior\ninformation within the downscaled video. After optimization, the downscaled\nvideo by our framework preserves more meaningful information, which is\nbeneficial for both the upscaling step and the downstream tasks, e.g., video\naction recognition task. We further extend the framework to a lossy video\ncompression system, in which a gradient estimator for non-differential\nindustrial lossy codecs is proposed for the end-to-end training of the whole\nsystem. Extensive experimental results demonstrate the superiority of our\napproach on video rescaling, video compression, and efficient action\nrecognition tasks.",
          "link": "http://arxiv.org/abs/2107.11639",
          "publishedOn": "2021-08-19T01:35:03.222Z",
          "wordCount": 627,
          "title": "Self-Conditioned Probabilistic Learning of Video Rescaling. (arXiv:2107.11639v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1\">Man M. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjia Zhou</a>",
          "description": "Physical photographs now can be conveniently scanned by smartphones and\nstored forever as a digital version, yet the scanned photos are not restored\nwell. One solution is to train a supervised deep neural network on many digital\nphotos and the corresponding scanned photos. However, it requires a high labor\ncost, leading to limited training data. Previous works create training pairs by\nsimulating degradation using image processing techniques. Their synthetic\nimages are formed with perfectly scanned photos in latent space. Even so, the\nreal-world degradation in smartphone photo scanning remains unsolved since it\nis more complicated due to lens defocus, lighting conditions, losing details\nvia printing. Besides, locally structural misalignment still occurs in data due\nto distorted shapes captured in a 3-D world, reducing restoration performance\nand the reliability of the quantitative evaluation. To solve these problems, we\npropose a semi-supervised Deep Photo Scan (DPScan). First, we present a way of\nproducing real-world degradation and provide the DIV2K-SCAN dataset for\nsmartphone-scanned photo restoration. Also, Local Alignment is proposed to\nreduce the minor misalignment remaining in data. Second, we simulate many\ndifferent variants of the real-world degradation using low-level image\ntransformation to gain a generalization in smartphone-scanned image properties,\nthen train a degradation network to generalize all styles of degradation and\nprovide pseudo-scanned photos for unscanned images as if they were scanned by a\nsmartphone. Finally, we propose a Semi-Supervised Learning that allows our\nrestoration network to be trained on both scanned and unscanned images,\ndiversifying training image content. As a result, the proposed DPScan\nquantitatively and qualitatively outperforms its baseline architecture,\nstate-of-the-art academic research, and industrial products in smartphone photo\nscanning.",
          "link": "http://arxiv.org/abs/2102.06120",
          "publishedOn": "2021-08-19T01:35:03.215Z",
          "wordCount": 751,
          "title": "Deep Photo Scan: Semi-Supervised Learning for dealing with the real-world degradation in Smartphone Photo Scanning. (arXiv:2102.06120v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wentao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>",
          "description": "In a real-world scenario, human actions are typically out of the distribution\nfrom training data, which requires a model to both recognize the known actions\nand reject the unknown. Different from image data, video actions are more\nchallenging to be recognized in an open-set setting due to the uncertain\ntemporal dynamics and static bias of human actions. In this paper, we propose a\nDeep Evidential Action Recognition (DEAR) method to recognize actions in an\nopen testing set. Specifically, we formulate the action recognition problem\nfrom the evidential deep learning (EDL) perspective and propose a novel model\ncalibration method to regularize the EDL training. Besides, to mitigate the\nstatic bias of video representation, we propose a plug-and-play module to\ndebias the learned representation through contrastive learning. Experimental\nresults show that our DEAR method achieves consistent performance gain on\nmultiple mainstream action recognition models and benchmarks. Code and\npre-trained models are available at\n{\\small{\\url{https://www.rit.edu/actionlab/dear}}}.",
          "link": "http://arxiv.org/abs/2107.10161",
          "publishedOn": "2021-08-19T01:35:03.208Z",
          "wordCount": 619,
          "title": "Evidential Deep Learning for Open Set Action Recognition. (arXiv:2107.10161v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhuangwei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "3D LiDAR (light detection and ranging) semantic segmentation is important in\nscene understanding for many applications, such as auto-driving and robotics.\nFor example, for autonomous cars equipped with RGB cameras and LiDAR, it is\ncrucial to fuse complementary information from different sensors for robust and\naccurate segmentation. Existing fusion-based methods, however, may not achieve\npromising performance due to the vast difference between the two modalities. In\nthis work, we investigate a collaborative fusion scheme called perception-aware\nmulti-sensor fusion (PMF) to exploit perceptual information from two\nmodalities, namely, appearance information from RGB images and spatio-depth\ninformation from point clouds. To this end, we first project point clouds to\nthe camera coordinates to provide spatio-depth information for RGB images.\nThen, we propose a two-stream network to extract features from the two\nmodalities, separately, and fuse the features by effective residual-based\nfusion modules. Moreover, we propose additional perception-aware losses to\nmeasure the perceptual difference between the two modalities. Extensive\nexperiments on two benchmark data sets show the superiority of our method. For\nexample, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8 in\nmIoU.",
          "link": "http://arxiv.org/abs/2106.15277",
          "publishedOn": "2021-08-19T01:35:03.200Z",
          "wordCount": 656,
          "title": "Perception-Aware Multi-Sensor Fusion for 3D LiDAR Semantic Segmentation. (arXiv:2106.15277v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10437",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sieun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eunho Lee</a>",
          "description": "Recently, there has been discussions on the ill-posed nature of\nsuper-resolution that multiple possible reconstructions exist for a given\nlow-resolution image. Using normalizing flows, SRflow[23] achieves\nstate-of-the-art perceptual quality by learning the distribution of the output\ninstead of a deterministic output to one estimate. In this paper, we adapt the\nconcepts of SRFlow to improve GAN-based super-resolution by properly\nimplementing the one-to-many property. We modify the generator to estimate a\ndistribution as a mapping from random noise. We improve the content loss that\nhampers the perceptual training objectives. We also propose additional training\ntechniques to further enhance the perceptual quality of generated images. Using\nour proposed methods, we were able to improve the performance of ESRGAN[1] in\nx4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual\nextreme SR by applying our methods to RFB-ESRGAN[21].",
          "link": "http://arxiv.org/abs/2106.10437",
          "publishedOn": "2021-08-19T01:35:03.194Z",
          "wordCount": 601,
          "title": "One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (1) ((1) ETH Zurich, (2) Google)",
          "description": "Deep generative models can synthesize photorealistic images of human faces\nwith novel identities. However, a key challenge to the wide applicability of\nsuch techniques is to provide independent control over semantically meaningful\nparameters: appearance, head pose, face shape, and facial expressions. In this\npaper, we propose VariTex - to the best of our knowledge the first method that\nlearns a variational latent feature space of neural face textures, which allows\nsampling of novel identities. We combine this generative model with a\nparametric face model and gain explicit control over head pose and facial\nexpressions. To generate complete images of human heads, we propose an additive\ndecoder that adds plausible details such as hair. A novel training scheme\nenforces a pose-independent latent space and in consequence, allows learning a\none-to-many mapping between latent codes and pose-conditioned exterior regions.\nThe resulting method can generate geometrically consistent images of novel\nidentities under fine-grained control over head pose, face shape, and facial\nexpressions. This facilitates a broad range of downstream tasks, like sampling\nnovel identities, changing the head pose, expression transfer, and more. Code\nand models are available for research on https://mcbuehler.github.io/VariTex.",
          "link": "http://arxiv.org/abs/2104.05988",
          "publishedOn": "2021-08-19T01:35:03.155Z",
          "wordCount": 694,
          "title": "VariTex: Variational Neural Face Textures. (arXiv:2104.05988v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1\">Vighnesh Birodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathod_V/0/1/0/all/0/1\">Vivek Rathod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jonathan Huang</a>",
          "description": "Instance segmentation models today are very accurate when trained on large\nannotated datasets, but collecting mask annotations at scale is prohibitively\nexpensive. We address the partially supervised instance segmentation problem in\nwhich one can train on (significantly cheaper) bounding boxes for all\ncategories but use masks only for a subset of categories. In this work, we\nfocus on a popular family of models which apply differentiable cropping to a\nfeature map and predict a mask based on the resulting crop. Under this family,\nwe study Mask R-CNN and discover that instead of its default strategy of\ntraining the mask-head with a combination of proposals and groundtruth boxes,\ntraining the mask-head with only groundtruth boxes dramatically improves its\nperformance on novel classes. This training strategy also allows us to take\nadvantage of alternative mask-head architectures, which we exploit by replacing\nthe typical mask-head of 2-4 layers with significantly deeper off-the-shelf\narchitectures (e.g. ResNet, Hourglass models). While many of these\narchitectures perform similarly when trained in fully supervised mode, our main\nfinding is that they can generalize to novel classes in dramatically different\nways. We call this ability of mask-heads to generalize to unseen classes the\nstrong mask generalization effect and show that without any specialty modules\nor losses, we can achieve state-of-the-art results in the partially supervised\nCOCO instance segmentation benchmark. Finally, we demonstrate that our effect\nis general, holding across underlying detection methodologies (including\nanchor-based, anchor-free or no detector at all) and across different backbone\nnetworks. Code and pre-trained models are available at https://git.io/deepmac.",
          "link": "http://arxiv.org/abs/2104.00613",
          "publishedOn": "2021-08-19T01:35:03.148Z",
          "wordCount": 728,
          "title": "The surprising impact of mask-head architecture on novel class segmentation. (arXiv:2104.00613v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>",
          "description": "We propose UniT, a Unified Transformer model to simultaneously learn the most\nprominent tasks across different domains, ranging from object detection to\nnatural language understanding and multimodal reasoning. Based on the\ntransformer encoder-decoder architecture, our UniT model encodes each input\nmodality with an encoder and makes predictions on each task with a shared\ndecoder over the encoded input representations, followed by task-specific\noutput heads. The entire model is jointly trained end-to-end with losses from\neach task. Compared to previous efforts on multi-task learning with\ntransformers, we share the same model parameters across all tasks instead of\nseparately fine-tuning task-specific models and handle a much higher variety of\ntasks across different domains. In our experiments, we learn 7 tasks jointly\nover 8 datasets, achieving strong performance on each task with significantly\nfewer parameters. Our code is available in MMF at https://mmf.sh.",
          "link": "http://arxiv.org/abs/2102.10772",
          "publishedOn": "2021-08-19T01:35:03.140Z",
          "wordCount": 617,
          "title": "UniT: Multimodal Multitask Learning with a Unified Transformer. (arXiv:2102.10772v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "The recently proposed Visual image Transformers (ViT) with pure attention\nhave achieved promising performance on image recognition tasks, such as image\nclassification. However, the routine of the current ViT model is to maintain a\nfull-length patch sequence during inference, which is redundant and lacks\nhierarchical representation. To this end, we propose a Hierarchical Visual\nTransformer (HVT) which progressively pools visual tokens to shrink the\nsequence length and hence reduces the computational cost, analogous to the\nfeature maps downsampling in Convolutional Neural Networks (CNNs). It brings a\ngreat benefit that we can increase the model capacity by scaling dimensions of\ndepth/width/resolution/patch size without introducing extra computational\ncomplexity due to the reduced sequence length. Moreover, we empirically find\nthat the average pooled visual tokens contain more discriminative information\nthan the single class token. To demonstrate the improved scalability of our\nHVT, we conduct extensive experiments on the image classification task. With\ncomparable FLOPs, our HVT outperforms the competitive baselines on ImageNet and\nCIFAR-100 datasets. Code is available at https://github.com/MonashAI/HVT",
          "link": "http://arxiv.org/abs/2103.10619",
          "publishedOn": "2021-08-19T01:35:03.123Z",
          "wordCount": 640,
          "title": "Scalable Vision Transformers with Hierarchical Pooling. (arXiv:2103.10619v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noguchi_A/0/1/0/all/0/1\">Atsuhiro Noguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "We present Neural Articulated Radiance Field (NARF), a novel deformable 3D\nrepresentation for articulated objects learned from images. While recent\nadvances in 3D implicit representation have made it possible to learn models of\ncomplex objects, learning pose-controllable representations of articulated\nobjects remains a challenge, as current methods require 3D shape supervision\nand are unable to render appearance. In formulating an implicit representation\nof 3D articulated objects, our method considers only the rigid transformation\nof the most relevant object part in solving for the radiance field at each 3D\nlocation. In this way, the proposed method represents pose-dependent changes\nwithout significantly increasing the computational complexity. NARF is fully\ndifferentiable and can be trained from images with pose annotations. Moreover,\nthrough the use of an autoencoder, it can learn appearance variations over\nmultiple instances of an object class. Experiments show that the proposed\nmethod is efficient and can generalize well to novel poses. The code is\navailable for research purposes at https://github.com/nogu-atsu/NARF",
          "link": "http://arxiv.org/abs/2104.03110",
          "publishedOn": "2021-08-19T01:35:03.108Z",
          "wordCount": 623,
          "title": "Neural Articulated Radiance Field. (arXiv:2104.03110v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanran He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "Deep neural networks are known to be extremely vulnerable to adversarial\nexamples under white-box setting. Moreover, the malicious adversaries crafted\non the surrogate (source) model often exhibit black-box transferability on\nother models with the same learning task but having different architectures.\nRecently, various methods are proposed to boost the adversarial\ntransferability, among which the input transformation is one of the most\neffective approaches. We investigate in this direction and observe that\nexisting transformations are all applied on a single image, which might limit\nthe adversarial transferability. To this end, we propose a new input\ntransformation based attack method called Admix that considers the input image\nand a set of images randomly sampled from other categories. Instead of directly\ncalculating the gradient on the original input, Admix calculates the gradient\non the input image admixed with a small portion of each add-in image while\nusing the original label of the input to craft more transferable adversaries.\nEmpirical evaluations on standard ImageNet dataset demonstrate that Admix could\nachieve significantly better transferability than existing input transformation\nmethods under both single model setting and ensemble-model setting. By\nincorporating with existing input transformations, our method could further\nimprove the transferability and outperforms the state-of-the-art combination of\ninput transformations by a clear margin when attacking nine advanced defense\nmodels under ensemble-model setting. Code is available at\nhttps://github.com/JHL-HUST/Admix.",
          "link": "http://arxiv.org/abs/2102.00436",
          "publishedOn": "2021-08-19T01:35:03.100Z",
          "wordCount": 693,
          "title": "Admix: Enhancing the Transferability of Adversarial Attacks. (arXiv:2102.00436v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yingjie Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Multi-level feature fusion is a fundamental topic in computer vision. It has\nbeen exploited to detect, segment and classify objects at various scales. When\nmulti-level features meet multi-modal cues, the optimal feature aggregation and\nmulti-modal learning strategy become a hot potato. In this paper, we leverage\nthe inherent multi-modal and multi-level nature of RGB-D salient object\ndetection to devise a novel cascaded refinement network. In particular, first,\nwe propose to regroup the multi-level features into teacher and student\nfeatures using a bifurcated backbone strategy (BBS). Second, we introduce a\ndepth-enhanced module (DEM) to excavate informative depth cues from the channel\nand spatial views. Then, RGB and depth modalities are fused in a complementary\nway. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is\nsimple, efficient, and backbone-independent. Extensive experiments show that\nBBS-Net significantly outperforms eighteen SOTA models on eight challenging\ndatasets under five evaluation measures, demonstrating the superiority of our\napproach ($\\sim 4 \\%$ improvement in S-measure $vs.$ the top-ranked model:\nDMRA-iccv2019). In addition, we provide a comprehensive analysis on the\ngeneralization ability of different RGB-D datasets and provide a powerful\ntraining set for future research.",
          "link": "http://arxiv.org/abs/2007.02713",
          "publishedOn": "2021-08-19T01:35:03.094Z",
          "wordCount": 666,
          "title": "Bifurcated backbone strategy for RGB-D salient object detection. (arXiv:2007.02713v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rumberger_J/0/1/0/all/0/1\">Josef Lorenz Rumberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_P/0/1/0/all/0/1\">Peter Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohmen_M/0/1/0/all/0/1\">Melanie Dohmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guarino_V/0/1/0/all/0/1\">Vanessa Emanuela Guarino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokarian_A/0/1/0/all/0/1\">Ashkan Mokarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mais_L/0/1/0/all/0/1\">Lisa Mais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_J/0/1/0/all/0/1\">Jan Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1\">Dagmar Kainmueller</a>",
          "description": "Metric learning has received conflicting assessments concerning its\nsuitability for solving instance segmentation tasks. It has been dismissed as\ntheoretically flawed due to the shift equivariance of the employed CNNs and\ntheir respective inability to distinguish same-looking objects. Yet it has been\nshown to yield state of the art results for a variety of tasks, and practical\nissues have mainly been reported in the context of tile-and-stitch approaches,\nwhere discontinuities at tile boundaries have been observed. To date, neither\nof the reported issues have undergone thorough formal analysis. In our work, we\ncontribute a comprehensive formal analysis of the shift equivariance properties\nof encoder-decoder-style CNNs, which yields a clear picture of what can and\ncannot be achieved with metric learning in the face of same-looking objects. In\nparticular, we prove that a standard encoder-decoder network that takes\n$d$-dimensional images as input, with $l$ pooling layers and pooling factor\n$f$, has the capacity to distinguish at most $f^{dl}$ same-looking objects, and\nwe show that this upper limit can be reached. Furthermore, we show that to\navoid discontinuities in a tile-and-stitch approach, assuming standard batch\nsize 1, it is necessary to employ valid convolutions in combination with a\ntraining output window size strictly greater than $f^l$, while at test-time it\nis necessary to crop tiles to size $n\\cdot f^l$ before stitching, with $n\\geq\n1$. We complement these theoretical findings by discussing a number of\ninsightful special cases for which we show empirical results on synthetic data.",
          "link": "http://arxiv.org/abs/2101.05846",
          "publishedOn": "2021-08-19T01:35:03.087Z",
          "wordCount": 737,
          "title": "How Shift Equivariance Impacts Metric Learning for Instance Segmentation. (arXiv:2101.05846v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1\">Nikhila Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>",
          "description": "We present Worldsheet, a method for novel view synthesis using just a single\nRGB image as input. The main insight is that simply shrink-wrapping a planar\nmesh sheet onto the input image, consistent with the learned intermediate\ndepth, captures underlying geometry sufficient to generate photorealistic\nunseen views with large viewpoint changes. To operationalize this, we propose a\nnovel differentiable texture sampler that allows our wrapped mesh sheet to be\ntextured and rendered differentiably into an image from a target viewpoint. Our\napproach is category-agnostic, end-to-end trainable without using any 3D\nsupervision, and requires a single image at test time. We also explore a simple\nextension by stacking multiple layers of Worldsheets to better handle\nocclusions. Worldsheet consistently outperforms prior state-of-the-art methods\non single-image view synthesis across several datasets. Furthermore, this\nsimple idea captures novel views surprisingly well on a wide range of\nhigh-resolution in-the-wild images, converting them into navigable 3D pop-ups.\nVideo results and code are available at https://worldsheet.github.io.",
          "link": "http://arxiv.org/abs/2012.09854",
          "publishedOn": "2021-08-19T01:35:03.069Z",
          "wordCount": 658,
          "title": "Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image. (arXiv:2012.09854v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>",
          "description": "For all the ways convolutional neural nets have revolutionized computer\nvision in recent years, one important aspect has received surprisingly little\nattention: the effect of image size on the accuracy of tasks being trained for.\nTypically, to be efficient, the input images are resized to a relatively small\nspatial resolution (e.g. 224x224), and both training and inference are carried\nout at this resolution. The actual mechanism for this re-scaling has been an\nafterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic\nare commonly used in most machine learning software frameworks. But do these\nresizers limit the on task performance of the trained networks? The answer is\nyes. Indeed, we show that the typical linear resizer can be replaced with\nlearned resizers that can substantially improve performance. Importantly, while\nthe classical resizers typically result in better perceptual quality of the\ndownscaled images, our proposed learned resizers do not necessarily give better\nvisual quality, but instead improve task performance. Our learned image resizer\nis jointly trained with a baseline vision model. This learned CNN-based resizer\ncreates machine friendly visual manipulations that lead to a consistent\nimprovement of the end task metric over the baseline model. Specifically, here\nwe focus on the classification task with the ImageNet dataset, and experiment\nwith four different models to learn resizers adapted to each model. Moreover,\nwe show that the proposed resizer can also be useful for fine-tuning the\nclassification baselines for other vision tasks. To this end, we experiment\nwith three different baselines to develop image quality assessment (IQA) models\non the AVA dataset.",
          "link": "http://arxiv.org/abs/2103.09950",
          "publishedOn": "2021-08-19T01:35:03.058Z",
          "wordCount": 727,
          "title": "Learning to Resize Images for Computer Vision Tasks. (arXiv:2103.09950v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "The great success of Convolutional Neural Networks (CNN) for facial attribute\nprediction relies on a large amount of labeled images. Facial image datasets\nare usually annotated by some commonly used attributes (e.g., gender), while\nlabels for the other attributes (e.g., big nose) are limited which causes their\nprediction challenging. To address this problem, we use a new Multi-Task\nLearning (MTL) paradigm in which a facial attribute predictor uses the\nknowledge of other related attributes to obtain a better generalization\nperformance. Here, we leverage MLT paradigm in two problem settings. First, it\nis assumed that the structure of the tasks (e.g., grouping pattern of facial\nattributes) is known as a prior knowledge, and parameters of the tasks (i.e.,\npredictors) within the same group are represented by a linear combination of a\nlimited number of underlying basis tasks. Here, a sparsity constraint on the\ncoefficients of this linear combination is also considered such that each task\nis represented in a more structured and simpler manner. Second, it is assumed\nthat the structure of the tasks is unknown, and then structure and parameters\nof the tasks are learned jointly by using a Laplacian regularization framework.\nOur MTL methods are compared with competing methods for facial attribute\nprediction to show its effectiveness.",
          "link": "http://arxiv.org/abs/2108.04353",
          "publishedOn": "2021-08-19T01:35:03.048Z",
          "wordCount": 669,
          "title": "Tasks Structure Regularization in Multi-Task Learning for Improving Facial Attribute Prediction. (arXiv:2108.04353v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ci_Y/0/1/0/all/0/1\">Yuanzheng Ci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "The automation of neural architecture design has been a coveted alternative\nto human experts. Recent works have small search space, which is easier to\noptimize but has a limited upper bound of the optimal solution. Extra human\ndesign is needed for those methods to propose a more suitable space with\nrespect to the specific task and algorithm capacity. To further enhance the\ndegree of automation for neural architecture search, we present a Neural\nSearch-space Evolution (NSE) scheme that iteratively amplifies the results from\nthe previous effort by maintaining an optimized search space subset. This\ndesign minimizes the necessity of a well-designed search space. We further\nextend the flexibility of obtainable architectures by introducing a learnable\nmulti-branch setting. By employing the proposed method, a consistent\nperformance gain is achieved during a progressive search over upcoming search\nspaces. We achieve 77.3% top-1 retrain accuracy on ImageNet with 333M FLOPs,\nwhich yielded a state-of-the-art performance among previous auto-generated\narchitectures that do not involve knowledge distillation or weight pruning.\nWhen the latency constraint is adopted, our result also performs better than\nthe previous best-performing mobile models with a 77.9% Top-1 retrain accuracy.",
          "link": "http://arxiv.org/abs/2011.10904",
          "publishedOn": "2021-08-19T01:35:03.027Z",
          "wordCount": 671,
          "title": "Evolving Search Space for Neural Architecture Search. (arXiv:2011.10904v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayer_C/0/1/0/all/0/1\">Christoph Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "The presence of objects that are confusingly similar to the tracked target,\nposes a fundamental challenge in appearance-based visual tracking. Such\ndistractor objects are easily misclassified as the target itself, leading to\neventual tracking failure. While most methods strive to suppress distractors\nthrough more powerful appearance models, we take an alternative approach.\n\nWe propose to keep track of distractor objects in order to continue tracking\nthe target. To this end, we introduce a learned association network, allowing\nus to propagate the identities of all target candidates from frame-to-frame. To\ntackle the problem of lacking ground-truth correspondences between distractor\nobjects in visual tracking, we propose a training strategy that combines\npartial annotations with self-supervision. We conduct comprehensive\nexperimental validation and analysis of our approach on several challenging\ndatasets. Our tracker sets a new state-of-the-art on six benchmarks, achieving\nan AUC score of 67.1% on LaSOT and a +5.8% absolute gain on the OxUvA long-term\ndataset.",
          "link": "http://arxiv.org/abs/2103.16556",
          "publishedOn": "2021-08-19T01:35:03.019Z",
          "wordCount": 647,
          "title": "Learning Target Candidate Association to Keep Track of What Not to Track. (arXiv:2103.16556v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rongmei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "Due to the over-parameterization nature, neural networks are a powerful tool\nfor nonlinear function approximation. In order to achieve good generalization\non unseen data, a suitable inductive bias is of great importance for neural\nnetworks. One of the most straightforward ways is to regularize the neural\nnetwork with some additional objectives. L2 regularization serves as a standard\nregularization for neural networks. Despite its popularity, it essentially\nregularizes one dimension of the individual neuron, which is not strong enough\nto control the capacity of highly over-parameterized neural networks. Motivated\nby this, hyperspherical uniformity is proposed as a novel family of relational\nregularizations that impact the interaction among neurons. We consider several\ngeometrically distinct ways to achieve hyperspherical uniformity. The\neffectiveness of hyperspherical uniformity is justified by theoretical insights\nand empirical evaluations.",
          "link": "http://arxiv.org/abs/2103.01649",
          "publishedOn": "2021-08-19T01:35:02.970Z",
          "wordCount": 598,
          "title": "Learning with Hyperspherical Uniformity. (arXiv:2103.01649v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaoua_R/0/1/0/all/0/1\">Ryad Kaoua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durr_A/0/1/0/all/0/1\">Alexandra Durr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaris_S/0/1/0/all/0/1\">Stavros Lazaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1\">David Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>",
          "description": "Illustrations are an essential transmission instrument. For an historian, the\nfirst step in studying their evolution in a corpus of similar manuscripts is to\nidentify which ones correspond to each other. This image collation task is\ndaunting for manuscripts separated by many lost copies, spreading over\ncenturies, which might have been completely re-organized and greatly modified\nto adapt to novel knowledge or belief and include hundreds of illustrations.\nOur contributions in this paper are threefold. First, we introduce the task of\nillustration collation and a large annotated public dataset to evaluate\nsolutions, including 6 manuscripts of 2 different texts with more than 2 000\nillustrations and 1 200 annotated correspondences. Second, we analyze state of\nthe art similarity measures for this task and show that they succeed in simple\ncases but struggle for large manuscripts when the illustrations have undergone\nvery significant changes and are discriminated only by fine details. Finally,\nwe show clear evidence that significant performance boosts can be expected by\nexploiting cycle-consistent correspondences. Our code and data are available on\nthis http URL",
          "link": "http://arxiv.org/abs/2108.08109",
          "publishedOn": "2021-08-19T01:35:02.959Z",
          "wordCount": 616,
          "title": "Image Collation: Matching illustrations in manuscripts. (arXiv:2108.08109v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1\">Pengfei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>",
          "description": "Differentiable architecture search (DARTS) marks a milestone in Neural\nArchitecture Search (NAS), boasting simplicity and small search costs. However,\nDARTS still suffers from frequent performance collapse, which happens when some\noperations, such as skip connections, zeroes and poolings, dominate the\narchitecture. In this paper, we are the first to point out that the phenomenon\nis attributed to bi-level optimization. We propose Single-DARTS which merely\nuses single-level optimization, updating network weights and architecture\nparameters simultaneously with the same data batch. Even single-level\noptimization has been previously attempted, no literature provides a systematic\nexplanation on this essential point. Replacing the bi-level optimization,\nSingle-DARTS obviously alleviates performance collapse as well as enhances the\nstability of architecture search. Experiment results show that Single-DARTS\nachieves state-of-the-art performance on mainstream search spaces. For\ninstance, on NAS-Benchmark-201, the searched architectures are nearly optimal\nones. We also validate that the single-level optimization framework is much\nmore stable than the bi-level one. We hope that this simple yet effective\nmethod will give some insights on differential architecture search. The code is\navailable at https://github.com/PencilAndBike/Single-DARTS.git.",
          "link": "http://arxiv.org/abs/2108.08128",
          "publishedOn": "2021-08-19T01:35:02.920Z",
          "wordCount": 616,
          "title": "Single-DARTS: Towards Stable Architecture Search. (arXiv:2108.08128v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "RGB-D saliency detection has attracted increasing attention, due to its\neffectiveness and the fact that depth cues can now be conveniently captured.\nExisting works often focus on learning a shared representation through various\nfusion strategies, with few methods explicitly considering how to preserve\nmodality-specific characteristics. In this paper, taking a new perspective, we\npropose a specificity-preserving network (SP-Net) for RGB-D saliency detection,\nwhich benefits saliency detection performance by exploring both the shared\ninformation and modality-specific properties (e.g., specificity). Specifically,\ntwo modality-specific networks and a shared learning network are adopted to\ngenerate individual and shared saliency maps. A cross-enhanced integration\nmodule (CIM) is proposed to fuse cross-modal features in the shared learning\nnetwork, which are then propagated to the next layer for integrating\ncross-level information. Besides, we propose a multi-modal feature aggregation\n(MFA) module to integrate the modality-specific features from each individual\ndecoder into the shared decoder, which can provide rich complementary\nmulti-modal information to boost the saliency detection performance. Further, a\nskip connection is used to combine hierarchical features between the encoder\nand decoder layers. Experiments on six benchmark datasets demonstrate that our\nSP-Net outperforms other state-of-the-art methods. Code is available at:\nhttps://github.com/taozh2017/SPNet.",
          "link": "http://arxiv.org/abs/2108.08162",
          "publishedOn": "2021-08-19T01:35:02.912Z",
          "wordCount": 632,
          "title": "Specificity-preserving RGB-D Saliency Detection. (arXiv:2108.08162v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kukleva_A/0/1/0/all/0/1\">Anna Kukleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>",
          "description": "Both generalized and incremental few-shot learning have to deal with three\nmajor challenges: learning novel classes from only few samples per class,\npreventing catastrophic forgetting of base classes, and classifier calibration\nacross novel and base classes. In this work we propose a three-stage framework\nthat allows to explicitly and effectively address these challenges. While the\nfirst phase learns base classes with many samples, the second phase learns a\ncalibrated classifier for novel classes from few samples while also preventing\ncatastrophic forgetting. In the final phase, calibration is achieved across all\nclasses. We evaluate the proposed framework on four challenging benchmark\ndatasets for image and video few-shot classification and obtain\nstate-of-the-art results for both generalized and incremental few shot\nlearning.",
          "link": "http://arxiv.org/abs/2108.08165",
          "publishedOn": "2021-08-19T01:35:02.878Z",
          "wordCount": 564,
          "title": "Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting. (arXiv:2108.08165v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08158",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Okamoto_H/0/1/0/all/0/1\">Hideaki Okamoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nomura_T/0/1/0/all/0/1\">Takakiyo Nomura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nabeshima_K/0/1/0/all/0/1\">Kazuhito Nabeshima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_J/0/1/0/all/0/1\">Jun Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>",
          "description": "X-ray examination is suitable for screening of gastric cancer. Compared to\nendoscopy, which can only be performed by doctors, X-ray imaging can also be\nperformed by radiographers, and thus, can treat more patients. However, the\ndiagnostic accuracy of gastric radiographs is as low as 85%. To address this\nproblem, highly accurate and quantitative automated diagnosis using machine\nlearning needs to be performed. This paper proposes a diagnostic support method\nfor detecting gastric cancer sites from X-ray images with high accuracy. The\ntwo new technical proposal of the method are (1) stochastic functional gastric\nimage augmentation (sfGAIA), and (2) hard boundary box training (HBBT). The\nformer is a probabilistic enhancement of gastric folds in X-ray images based on\nmedical knowledge, whereas the latter is a recursive retraining technique to\nreduce false positives. We use 4,724 gastric radiographs of 145 patients in\nclinical practice and evaluate the cancer detection performance of the method\nin a patient-based five-group cross-validation. The proposed sfGAIA and HBBT\nsignificantly enhance the performance of the EfficientDet-D7 network by 5.9% in\nterms of the F1-score, and our screening method reaches a practical screening\ncapability for gastric cancer (F1: 57.8%, recall: 90.2%, precision: 42.5%).",
          "link": "http://arxiv.org/abs/2108.08158",
          "publishedOn": "2021-08-19T01:35:02.870Z",
          "wordCount": 659,
          "title": "Gastric Cancer Detection from X-ray Images Using Effective Data Augmentation and Hard Boundary Box Training. (arXiv:2108.08158v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhilu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruohao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>",
          "description": "Learning RAW-to-sRGB mapping has drawn increasing attention in recent years,\nwherein an input raw image is trained to imitate the target sRGB image captured\nby another camera. However, the severe color inconsistency makes it very\nchallenging to generate well-aligned training pairs of input raw and target\nsRGB images. While learning with inaccurately aligned supervision is prone to\ncausing pixel shift and producing blurry results. In this paper, we circumvent\nsuch issue by presenting a joint learning model for image alignment and\nRAW-to-sRGB mapping. To diminish the effect of color inconsistency in image\nalignment, we introduce to use a global color mapping (GCM) module to generate\nan initial sRGB image given the input raw image, which can keep the spatial\nlocation of the pixels unchanged, and the target sRGB image is utilized to\nguide GCM for converting the color towards it. Then a pre-trained optical flow\nestimation network (e.g., PWC-Net) is deployed to warp the target sRGB image to\nalign with the GCM output. To alleviate the effect of inaccurately aligned\nsupervision, the warped target sRGB image is leveraged to learn RAW-to-sRGB\nmapping. When training is done, the GCM module and optical flow network can be\ndetached, thereby bringing no extra computation cost for inference. Experiments\nshow that our method performs favorably against state-of-the-arts on ZRR and\nSR-RAW datasets. With our joint learning model, a light-weight backbone can\nachieve better quantitative and qualitative performance on ZRR dataset. Codes\nare available at https://github.com/cszhilu1998/RAW-to-sRGB.",
          "link": "http://arxiv.org/abs/2108.08119",
          "publishedOn": "2021-08-19T01:35:02.843Z",
          "wordCount": 686,
          "title": "Learning RAW-to-sRGB Mappings with Inaccurately Aligned Supervision. (arXiv:2108.08119v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solbach_M/0/1/0/all/0/1\">Markus D. Solbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>",
          "description": "The STAR architecture was designed to test the value of the full Selective\nTuning model of visual attention for complex real-world visuospatial tasks and\nbehaviors. However, knowledge of how humans solve such tasks in 3D as active\nobservers is lean. We thus devised a novel experimental setup and examined such\nbehavior. We discovered that humans exhibit a variety of problem-solving\nstrategies whose breadth and complexity are surprising and not easily handled\nby current methodologies. It is apparent that solution methods are dynamically\ncomposed by hypothesizing sequences of actions, testing them, and if they fail,\ntrying different ones. The importance of active observation is striking as is\nthe lack of any learning effect. These results inform our Cognitive Program\nrepresentation of STAR extending its relevance to real-world tasks.",
          "link": "http://arxiv.org/abs/2108.08145",
          "publishedOn": "2021-08-19T01:35:02.800Z",
          "wordCount": 580,
          "title": "Active Observer Visual Problem-Solving Methods are Dynamically Hypothesized, Deployed and Tested. (arXiv:2108.08145v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1\">Munan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenglang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shuang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Unsupervised domain adaption has proven to be an effective approach for\nalleviating the intensive workload of manual annotation by aligning the\nsynthetic source-domain data and the real-world target-domain samples.\nUnfortunately, mapping the target-domain distribution to the source-domain\nunconditionally may distort the essential structural information of the\ntarget-domain data. To this end, we firstly propose to introduce a novel\nmulti-anchor based active learning strategy to assist domain adaptation\nregarding the semantic segmentation task. By innovatively adopting multiple\nanchors instead of a single centroid, the source domain can be better\ncharacterized as a multimodal distribution, thus more representative and\ncomplimentary samples are selected from the target domain. With little workload\nto manually annotate these active samples, the distortion of the target-domain\ndistribution can be effectively alleviated, resulting in a large performance\ngain. The multi-anchor strategy is additionally employed to model the\ntarget-distribution. By regularizing the latent representation of the target\nsamples compact around multiple anchors through a novel soft alignment loss,\nmore precise segmentation can be achieved. Extensive experiments are conducted\non public datasets to demonstrate that the proposed approach outperforms\nstate-of-the-art methods significantly, along with thorough ablation study to\nverify the effectiveness of each component.",
          "link": "http://arxiv.org/abs/2108.08012",
          "publishedOn": "2021-08-19T01:35:02.794Z",
          "wordCount": 640,
          "title": "Multi-Anchor Active Domain Adaptation for Semantic Segmentation. (arXiv:2108.08012v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08095",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shenavarmasouleh_F/0/1/0/all/0/1\">Farzan Shenavarmasouleh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_F/0/1/0/all/0/1\">Farid Ghareh Mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amini_M/0/1/0/all/0/1\">M. Hadi Amini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taha_T/0/1/0/all/0/1\">Thiab Taha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>",
          "description": "Medical Imaging is one of the growing fields in the world of computer vision.\nIn this study, we aim to address the Diabetic Retinopathy (DR) problem as one\nof the open challenges in medical imaging. In this research, we propose a new\nlesion detection architecture, comprising of two sub-modules, which is an\noptimal solution to detect and find not only the type of lesions caused by DR,\ntheir corresponding bounding boxes, and their masks; but also the severity\nlevel of the overall case. Aside from traditional accuracy, we also use two\npopular evaluation criteria to evaluate the outputs of our models, which are\nintersection over union (IOU) and mean average precision (mAP). We hypothesize\nthat this new solution enables specialists to detect lesions with high\nconfidence and estimate the severity of the damage with high accuracy.",
          "link": "http://arxiv.org/abs/2108.08095",
          "publishedOn": "2021-08-19T01:35:02.758Z",
          "wordCount": 620,
          "title": "DRDrV3: Complete Lesion Detection in Fundus Images Using Mask R-CNN, Transfer Learning, and LSTM. (arXiv:2108.08095v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>",
          "description": "In this work, an extensive review of literature in the field of gesture\nrecognition carried out along with the implementation of a simple\nclassification system for hand hygiene stages based on deep learning solutions.\nA subset of robust dataset that consist of handwashing gestures with two hands\nas well as one-hand gestures such as linear hand movement utilized. A\npretrained neural network model, RES Net 50, with image net weights used for\nthe classification of 3 categories: Linear hand movement, rub hands palm to\npalm and rub hands with fingers interlaced movement. Correct predictions made\nfor the first two classes with > 60% accuracy. A complete dataset along with\nincreased number of classes and training steps will be explored as a future\nwork.",
          "link": "http://arxiv.org/abs/2108.08127",
          "publishedOn": "2021-08-19T01:35:02.742Z",
          "wordCount": 551,
          "title": "Hand Hygiene Video Classification Based on Deep Learning. (arXiv:2108.08127v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yao Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "This paper deals with a challenging task of video scene graph generation\n(VidSGG), which could serve as a structured video representation for high-level\nunderstanding tasks. We present a new {\\em detect-to-track} paradigm for this\ntask by decoupling the context modeling for relation prediction from the\ncomplicated low-level entity tracking. Specifically, we design an efficient\nmethod for frame-level VidSGG, termed as {\\em Target Adaptive Context\nAggregation Network} (TRACE), with a focus on capturing spatio-temporal context\ninformation for relation recognition. Our TRACE framework streamlines the\nVidSGG pipeline with a modular design, and presents two unique blocks of\nHierarchical Relation Tree (HRTree) construction and Target-adaptive Context\nAggregation. More specific, our HRTree first provides an adpative structure for\norganizing possible relation candidates efficiently, and guides context\naggregation module to effectively capture spatio-temporal structure\ninformation. Then, we obtain a contextualized feature representation for each\nrelation candidate and build a classification head to recognize its relation\ncategory. Finally, we provide a simple temporal association strategy to track\nTRACE detected results to yield the video-level VidSGG. We perform experiments\non two VidSGG benchmarks: ImageNet-VidVRD and Action Genome, and the results\ndemonstrate that our TRACE achieves the state-of-the-art performance. The code\nand models are made available at \\url{https://github.com/MCG-NJU/TRACE}.",
          "link": "http://arxiv.org/abs/2108.08121",
          "publishedOn": "2021-08-19T01:35:02.736Z",
          "wordCount": 644,
          "title": "Target Adaptive Context Aggregation for Video Scene Graph Generation. (arXiv:2108.08121v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tambwekar_A/0/1/0/all/0/1\">Anuj Tambwekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1\">Kshitij Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majee_A/0/1/0/all/0/1\">Anay Majee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Anbumani Subramanian</a>",
          "description": "Incremental few-shot learning has emerged as a new and challenging area in\ndeep learning, whose objective is to train deep learning models using very few\nsamples of new class data, and none of the old class data. In this work we\ntackle the problem of batch incremental few-shot road object detection using\ndata from the India Driving Dataset (IDD). Our approach, DualFusion, combines\nobject detectors in a manner that allows us to learn to detect rare objects\nwith very limited data, all without severely degrading the performance of the\ndetector on the abundant classes. In the IDD OpenSet incremental few-shot\ndetection task, we achieve a mAP50 score of 40.0 on the base classes and an\noverall mAP50 score of 38.8, both of which are the highest to date. In the COCO\nbatch incremental few-shot detection task, we achieve a novel AP score of 9.9,\nsurpassing the state-of-the-art novel class performance on the same by over 6.6\ntimes.",
          "link": "http://arxiv.org/abs/2108.08048",
          "publishedOn": "2021-08-19T01:35:02.729Z",
          "wordCount": 607,
          "title": "Few-Shot Batch Incremental Road Object Detection via Detector Fusion. (arXiv:2108.08048v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingkui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>",
          "description": "We present a deep learning pipeline that leverages network self-prior to\nrecover a full 3D model consisting of both a triangular mesh and a texture map\nfrom the colored 3D point cloud. Different from previous methods either\nexploiting 2D self-prior for image editing or 3D self-prior for pure surface\nreconstruction, we propose to exploit a novel hybrid 2D-3D self-prior in deep\nneural networks to significantly improve the geometry quality and produce a\nhigh-resolution texture map, which is typically missing from the output of\ncommodity-level 3D scanners. In particular, we first generate an initial mesh\nusing a 3D convolutional neural network with 3D self-prior, and then encode\nboth 3D information and color information in the 2D UV atlas, which is further\nrefined by 2D convolutional neural networks with the self-prior. In this way,\nboth 2D and 3D self-priors are utilized for the mesh and texture recovery.\nExperiments show that, without the need of any additional training data, our\nmethod recovers the 3D textured mesh model of high quality from sparse input,\nand outperforms the state-of-the-art methods in terms of both the geometry and\ntexture quality.",
          "link": "http://arxiv.org/abs/2108.08017",
          "publishedOn": "2021-08-19T01:35:02.704Z",
          "wordCount": 626,
          "title": "Deep Hybrid Self-Prior for Full 3D Mesh Generation. (arXiv:2108.08017v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nauata_N/0/1/0/all/0/1\">Nelson Nauata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1\">Yasutaka Furukawa</a>",
          "description": "This paper presents an explore-and-classify framework for structured\narchitectural reconstruction from an aerial image. Starting from a potentially\nimperfect building reconstruction by an existing algorithm, our approach 1)\nexplores the space of building models by modifying the reconstruction via\nheuristic actions; 2) learns to classify the correctness of building models\nwhile generating classification labels based on the ground-truth, and 3)\nrepeat. At test time, we iterate exploration and classification, seeking for a\nresult with the best classification score. We evaluate the approach using\ninitial reconstructions by two baselines and two state-of-the-art\nreconstruction algorithms. Qualitative and quantitative evaluations demonstrate\nthat our approach consistently improves the reconstruction quality from every\ninitial reconstruction.",
          "link": "http://arxiv.org/abs/2108.07990",
          "publishedOn": "2021-08-19T01:35:02.694Z",
          "wordCount": 555,
          "title": "Structured Outdoor Architecture Reconstruction by Exploration and Classification. (arXiv:2108.07990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yike Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bailan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenggang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_F/0/1/0/all/0/1\">Feng Dai</a>",
          "description": "As one of the most fundamental and challenging problems in computer vision,\nobject detection tries to locate object instances and find their categories in\nnatural images. The most important step in the evaluation of object detection\nalgorithm is calculating the intersection-over-union (IoU) between the\npredicted bounding box and the ground truth one. Although this procedure is\nwell-defined and solved for planar images, it is not easy for spherical image\nobject detection. Existing methods either compute the IoUs based on biased\nbounding box representations or make excessive approximations, thus would give\nincorrect results. In this paper, we first identify that spherical rectangles\nare unbiased bounding boxes for objects in spherical images, and then propose\nan analytical method for IoU calculation without any approximations. Based on\nthe unbiased representation and calculation, we also present an anchor free\nobject detection algorithm for spherical images. The experiments on two\nspherical object detection datasets show that the proposed method can achieve\nbetter performance than existing methods.",
          "link": "http://arxiv.org/abs/2108.08029",
          "publishedOn": "2021-08-19T01:35:02.684Z",
          "wordCount": 603,
          "title": "Unbiased IoU for Spherical Image Object Detection. (arXiv:2108.08029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Binghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhaoyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Biao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "In crowd counting, due to the problem of laborious labelling, it is perceived\nintractability of collecting a new large-scale dataset which has plentiful\nimages with large diversity in density, scene, etc. Thus, for learning a\ngeneral model, training with data from multiple different datasets might be a\nremedy and be of great value. In this paper, we resort to the multi-domain\njoint learning and propose a simple but effective Domain-specific Knowledge\nPropagating Network (DKPNet)1 for unbiasedly learning the knowledge from\nmultiple diverse data domains at the same time. It is mainly achieved by\nproposing the novel Variational Attention(VA) technique for explicitly modeling\nthe attention distributions for different domains. And as an extension to VA,\nIntrinsic Variational Attention(InVA) is proposed to handle the problems of\nover-lapped domains and sub-domains. Extensive experiments have been conducted\nto validate the superiority of our DKPNet over several popular datasets,\nincluding ShanghaiTech A/B, UCF-QNRF and NWPU.",
          "link": "http://arxiv.org/abs/2108.08023",
          "publishedOn": "2021-08-19T01:35:02.677Z",
          "wordCount": 601,
          "title": "Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting. (arXiv:2108.08023v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1\">Hui Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1\">Tianjia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>",
          "description": "Image generation has been heavily investigated in computer vision, where one\ncore research challenge is to generate images from arbitrarily complex\ndistributions with little supervision. Generative Adversarial Networks (GANs)\nas an implicit approach have achieved great successes in this direction and\ntherefore been employed widely. However, GANs are known to suffer from issues\nsuch as mode collapse, non-structured latent space, being unable to compute\nlikelihoods, etc. In this paper, we propose a new unsupervised non-parametric\nmethod named mixture of infinite conditional GANs or MIC-GANs, to tackle\nseveral GAN issues together, aiming for image generation with parsimonious\nprior knowledge. Through comprehensive evaluations across different datasets,\nwe show that MIC-GANs are effective in structuring the latent space and\navoiding mode collapse, and outperform state-of-the-art methods. MICGANs are\nadaptive, versatile, and robust. They offer a promising solution to several\nwell-known GAN issues. Code available: github.com/yinghdb/MICGANs.",
          "link": "http://arxiv.org/abs/2108.07975",
          "publishedOn": "2021-08-19T01:35:02.652Z",
          "wordCount": 587,
          "title": "Unsupervised Image Generation with Infinite Generative Adversarial Networks. (arXiv:2108.07975v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Keyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>",
          "description": "Depth estimation, as a necessary clue to convert 2D images into the 3D space,\nhas been applied in many machine vision areas. However, to achieve an entire\nsurrounding 360-degree geometric sensing, traditional stereo matching\nalgorithms for depth estimation are limited due to large noise, low accuracy,\nand strict requirements for multi-camera calibration. In this work, for a\nunified surrounding perception, we introduce panoramic images to obtain larger\nfield of view. We extend PADENet first appeared in our previous conference work\nfor outdoor scene understanding, to perform panoramic monocular depth\nestimation with a focus for indoor scenes. At the same time, we improve the\ntraining process of the neural network adapted to the characteristics of\npanoramic images. In addition, we fuse traditional stereo matching algorithm\nwith deep learning methods and further improve the accuracy of depth\npredictions. With a comprehensive variety of experiments, this research\ndemonstrates the effectiveness of our schemes aiming for indoor scene\nperception.",
          "link": "http://arxiv.org/abs/2108.08076",
          "publishedOn": "2021-08-19T01:35:02.631Z",
          "wordCount": 613,
          "title": "Panoramic Depth Estimation via Supervised and Unsupervised Learning in Indoor Scenes. (arXiv:2108.08076v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>",
          "description": "Unsupervised Domain Adaptation (UDA) can transfer knowledge from labeled\nsource data to unlabeled target data of the same categories. However, UDA for\nfirst-person action recognition is an under-explored problem, with lack of\ndatasets and limited consideration of first-person video characteristics. This\npaper focuses on addressing this problem. Firstly, we propose two small-scale\nfirst-person video domain adaptation datasets: ADL$_{small}$ and GTEA-KITCHEN.\nSecondly, we introduce channel-temporal attention blocks to capture the\nchannel-wise and temporal-wise relationships and model their inter-dependencies\nimportant to first-person vision. Finally, we propose a Channel-Temporal\nAttention Network (CTAN) to integrate these blocks into existing architectures.\nCTAN outperforms baselines on the two proposed datasets and one existing\ndataset EPIC$_{cvpr20}$.",
          "link": "http://arxiv.org/abs/2108.07846",
          "publishedOn": "2021-08-19T01:35:02.625Z",
          "wordCount": 547,
          "title": "Channel-Temporal Attention for First-Person Video Domain Adaptation. (arXiv:2108.07846v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haibo Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "With the recent success of deep neural networks, remarkable progress has been\nachieved on face recognition. However, collecting large-scale real-world\ntraining data for face recognition has turned out to be challenging, especially\ndue to the label noise and privacy issues. Meanwhile, existing face recognition\ndatasets are usually collected from web images, lacking detailed annotations on\nattributes (e.g., pose and expression), so the influences of different\nattributes on face recognition have been poorly investigated. In this paper, we\naddress the above-mentioned issues in face recognition using synthetic face\nimages, i.e., SynFace. Specifically, we first explore the performance gap\nbetween recent state-of-the-art face recognition models trained with synthetic\nand real face images. We then analyze the underlying causes behind the\nperformance gap, e.g., the poor intra-class variations and the domain gap\nbetween synthetic and real face images. Inspired by this, we devise the SynFace\nwith identity mixup (IM) and domain mixup (DM) to mitigate the above\nperformance gap, demonstrating the great potentials of synthetic data for face\nrecognition. Furthermore, with the controllable face synthesis model, we can\neasily manage different factors of synthetic face generation, including pose,\nexpression, illumination, the number of identities, and samples per identity.\nTherefore, we also perform a systematically empirical analysis on synthetic\nface images to provide some insights on how to effectively utilize synthetic\ndata for face recognition.",
          "link": "http://arxiv.org/abs/2108.07960",
          "publishedOn": "2021-08-19T01:35:02.601Z",
          "wordCount": 662,
          "title": "SynFace: Face Recognition with Synthetic Data. (arXiv:2108.07960v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongmei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>",
          "description": "Majority models of remote sensing image changing detection can only get great\neffect in a specific resolution data set. With the purpose of improving change\ndetection effectiveness of the model in the multi-resolution data set, a\nweighted rich-scale inception coder network (WRICNet) is proposed in this\narticle, which can make a great fusion of shallow multi-scale features, and\ndeep multi-scale features. The weighted rich-scale inception module of the\nproposed can obtain shallow multi-scale features, the weighted rich-scale coder\nmodule can obtain deep multi-scale features. The weighted scale block assigns\nappropriate weights to features of different scales, which can strengthen\nexpressive ability of the edge of the changing area. The performance\nexperiments on the multi-resolution data set demonstrate that, compared to the\ncomparative methods, the proposed can further reduce the false alarm outside\nthe change area, and the missed alarm in the change area, besides, the edge of\nthe change area is more accurate. The ablation study of the proposed shows that\nthe training strategy, and improvements of this article can improve the\neffectiveness of change detection.",
          "link": "http://arxiv.org/abs/2108.07955",
          "publishedOn": "2021-08-19T01:35:02.589Z",
          "wordCount": 620,
          "title": "WRICNet:A Weighted Rich-scale Inception Coder Network for Multi-Resolution Remote Sensing Image Change Detection. (arXiv:2108.07955v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zichen Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1\">Qiang Qiu</a>",
          "description": "Applying feature dependent network weights have been proved to be effective\nin many fields. However, in practice, restricted by the enormous size of model\nparameters and memory footprints, scalable and versatile dynamic convolutions\nwith per-pixel adapted filters are yet to be fully explored. In this paper, we\naddress this challenge by decomposing filters, adapted to each spatial\nposition, over dynamic filter atoms generated by a light-weight network from\nlocal features. Adaptive receptive fields can be supported by further\nrepresenting each filter atom over sets of pre-fixed multi-scale bases. As\nplug-and-play replacements to convolutional layers, the introduced adaptive\nconvolutions with per-pixel dynamic atoms enable explicit modeling of\nintra-image variance, while avoiding heavy computation, parameters, and memory\ncost. Our method preserves the appealing properties of conventional\nconvolutions as being translation-equivariant and parametrically efficient. We\npresent experiments to show that, the proposed method delivers comparable or\neven better performance across tasks, and are particularly effective on\nhandling tasks with significant intra-image variance.",
          "link": "http://arxiv.org/abs/2108.07895",
          "publishedOn": "2021-08-19T01:35:02.579Z",
          "wordCount": 593,
          "title": "Adaptive Convolutions with Per-pixel Dynamic Filter Atom. (arXiv:2108.07895v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yucheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>",
          "description": "Advanced self-supervised visual representation learning methods rely on the\ninstance discrimination (ID) pretext task. We point out that the ID task has an\nimplicit semantic consistency (SC) assumption, which may not hold in\nunconstrained datasets. In this paper, we propose a novel contrastive mask\nprediction (CMP) task for visual representation learning and design a mask\ncontrast (MaskCo) framework to implement the idea. MaskCo contrasts\nregion-level features instead of view-level features, which makes it possible\nto identify the positive sample without any assumptions. To solve the domain\ngap between masked and unmasked features, we design a dedicated mask prediction\nhead in MaskCo. This module is shown to be the key to the success of the CMP.\nWe evaluated MaskCo on training datasets beyond ImageNet and compare its\nperformance with MoCo V2. Results show that MaskCo achieves comparable\nperformance with MoCo V2 using ImageNet training dataset, but demonstrates a\nstronger performance across a range of downstream tasks when COCO or Conceptual\nCaptions are used for training. MaskCo provides a promising alternative to the\nID-based methods for self-supervised learning in the wild.",
          "link": "http://arxiv.org/abs/2108.07954",
          "publishedOn": "2021-08-19T01:35:02.549Z",
          "wordCount": 621,
          "title": "Self-Supervised Visual Representations Learning by Contrastive Mask Prediction. (arXiv:2108.07954v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07978",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy S. Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_L/0/1/0/all/0/1\">Lynhoo Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>",
          "description": "Nowadays modern displays are capable to render video content with high\ndynamic range (HDR) and wide color gamut (WCG). However, most available\nresources are still in standard dynamic range (SDR). Therefore, there is an\nurgent demand to transform existing SDR-TV contents into their HDR-TV versions.\nIn this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the\nformation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step\nsolution pipeline including adaptive global color mapping, local enhancement\nand highlight generation. Moreover, the above analysis inspires us to present a\nlightweight network that utilizes global statistics as guidance to conduct\nimage-adaptive color mapping. In addition, we construct a dataset using HDR\nvideos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate\nthe results of SDRTV-to-HDRTV algorithms. Furthermore, our final results\nachieve state-of-the-art performance in quantitative comparisons and visual\nquality. The code and dataset are available at\nhttps://github.com/chxy95/HDRTVNet.",
          "link": "http://arxiv.org/abs/2108.07978",
          "publishedOn": "2021-08-19T01:35:02.488Z",
          "wordCount": 604,
          "title": "A New Journey from SDRTV to HDRTV. (arXiv:2108.07978v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fani_M/0/1/0/all/0/1\">Mehrnaz Fani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clausi_D/0/1/0/all/0/1\">David A. Clausi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1\">John Zelek</a>",
          "description": "Identifying players in sports videos by recognizing their jersey numbers is a\nchallenging task in computer vision. We have designed and implemented a\nmulti-task learning network for jersey number recognition. In order to train a\nnetwork to recognize jersey numbers, two output label representations are used\n(1) Holistic - considers the entire jersey number as one class, and (2)\nDigit-wise - considers the two digits in a jersey number as two separate\nclasses. The proposed network learns both holistic and digit-wise\nrepresentations through a multi-task loss function. We determine the optimal\nweights to be assigned to holistic and digit-wise losses through an ablation\nstudy. Experimental results demonstrate that the proposed multi-task learning\nnetwork performs better than the constituent holistic and digit-wise\nsingle-task learning networks.",
          "link": "http://arxiv.org/abs/2108.07848",
          "publishedOn": "2021-08-19T01:35:02.466Z",
          "wordCount": 576,
          "title": "Multi-task learning for jersey number recognition in Ice Hockey. (arXiv:2108.07848v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07856",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fitzke_M/0/1/0/all/0/1\">Michael Fitzke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitley_D/0/1/0/all/0/1\">Derick Whitley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yau_W/0/1/0/all/0/1\">Wilson Yau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_F/0/1/0/all/0/1\">Fernando Rodrigues Jr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fadeev_V/0/1/0/all/0/1\">Vladimir Fadeev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bacmeister_C/0/1/0/all/0/1\">Cindy Bacmeister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carter_C/0/1/0/all/0/1\">Chris Carter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Edwards_J/0/1/0/all/0/1\">Jeffrey Edwards</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkinson_M/0/1/0/all/0/1\">Mark Parkinson</a>",
          "description": "Background: Histopathology is an important modality for the diagnosis and\nmanagement of many diseases in modern healthcare, and plays a critical role in\ncancer care. Pathology samples can be large and require multi-site sampling,\nleading to upwards of 20 slides for a single tumor, and the human-expert tasks\nof site selection and and quantitative assessment of mitotic figures are time\nconsuming and subjective. Automating these tasks in the setting of a digital\npathology service presents significant opportunities to improve workflow\nefficiency and augment human experts in practice. Approach: Multiple\nstate-of-the-art deep learning techniques for histopathology image\nclassification and mitotic figure detection were used in the development of\nOncoPetNet. Additionally, model-free approaches were used to increase speed and\naccuracy. The robust and scalable inference engine leverages Pytorch's\nperformance optimizations as well as specifically developed speed up techniques\nin inference. Results: The proposed system, demonstrated significantly improved\nmitotic counting performance for 41 cancer cases across 14 cancer types\ncompared to human expert baselines. In 21.9% of cases use of OncoPetNet led to\nchange in tumor grading compared to human expert evaluation. In deployment, an\neffective 0.27 min/slide inference was achieved in a high throughput veterinary\ndiagnostic pathology service across 2 centers processing 3,323 digital whole\nslide images daily. Conclusion: This work represents the first successful\nautomated deployment of deep learning systems for real-time expert-level\nperformance on important histopathology tasks at scale in a high volume\nclinical practice. The resulting impact outlines important considerations for\nmodel development, deployment, clinical decision making, and informs best\npractices for implementation of deep learning systems in digital histopathology\npractices.",
          "link": "http://arxiv.org/abs/2108.07856",
          "publishedOn": "2021-08-19T01:35:02.445Z",
          "wordCount": 758,
          "title": "OncoPetNet: A Deep Learning based AI system for mitotic figure counting on H&E stained whole slide digital images in a large veterinary diagnostic lab setting. (arXiv:2108.07856v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1\">Prune Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "The key challenge in learning dense correspondences lies in the lack of\nground-truth matches for real image pairs. While photometric consistency losses\nprovide unsupervised alternatives, they struggle with large appearance changes,\nwhich are ubiquitous in geometric and semantic matching tasks. Moreover,\nmethods relying on synthetic training pairs often suffer from poor\ngeneralisation to real data.\n\nWe propose Warp Consistency, an unsupervised learning objective for dense\ncorrespondence regression. Our objective is effective even in settings with\nlarge appearance and view-point changes. Given a pair of real images, we first\nconstruct an image triplet by applying a randomly sampled warp to one of the\noriginal images. We derive and analyze all flow-consistency constraints arising\nbetween the triplet. From our observations and empirical results, we design a\ngeneral unsupervised objective employing two of the derived constraints. We\nvalidate our warp consistency loss by training three recent dense\ncorrespondence networks for the geometric and semantic matching tasks. Our\napproach sets a new state-of-the-art on several challenging benchmarks,\nincluding MegaDepth, RobotCar and TSS. Code and models are at\ngithub.com/PruneTruong/DenseMatching.",
          "link": "http://arxiv.org/abs/2104.03308",
          "publishedOn": "2021-08-19T01:35:02.399Z",
          "wordCount": 669,
          "title": "Warp Consistency for Unsupervised Learning of Dense Correspondences. (arXiv:2104.03308v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "The unsupervised domain adaptation (UDA) has been widely adopted to alleviate\nthe data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is adapted for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vector that violates the poset constraints by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-08-19T01:35:02.392Z",
          "wordCount": 651,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yizeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>",
          "description": "In this paper, we explore the spatial redundancy in video recognition with\nthe aim to improve the computational efficiency. It is observed that the most\ninformative region in each frame of a video is usually a small image patch,\nwhich shifts smoothly across frames. Therefore, we model the patch localization\nproblem as a sequential decision task, and propose a reinforcement learning\nbased approach for efficient spatially adaptive video recognition (AdaFocus).\nIn specific, a light-weighted ConvNet is first adopted to quickly process the\nfull video sequence, whose features are used by a recurrent policy network to\nlocalize the most task-relevant regions. Then the selected patches are inferred\nby a high-capacity network for the final prediction. During offline inference,\nonce the informative patch sequence has been generated, the bulk of computation\ncan be done in parallel, and is efficient on modern GPU devices. In addition,\nwe demonstrate that the proposed method can be easily extended by further\nconsidering the temporal redundancy, e.g., dynamically skipping less valuable\nframes. Extensive experiments on five benchmark datasets, i.e., ActivityNet,\nFCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is\nsignificantly more efficient than the competitive baselines. Code is available\nat https://github.com/blackfeather-wang/AdaFocus.",
          "link": "http://arxiv.org/abs/2105.03245",
          "publishedOn": "2021-08-19T01:35:02.383Z",
          "wordCount": 675,
          "title": "Adaptive Focus for Efficient Video Recognition. (arXiv:2105.03245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Predictor-based algorithms have achieved remarkable performance in the Neural\nArchitecture Search (NAS) tasks. However, these methods suffer from high\ncomputation costs, as training the performance predictor usually requires\ntraining and evaluating hundreds of architectures from scratch. Previous works\nalong this line mainly focus on reducing the number of architectures required\nto fit the predictor. In this work, we tackle this challenge from a different\nperspective - improve search efficiency by cutting down the computation budget\nof architecture training. We propose NOn-uniform Successive Halving (NOSH), a\nhierarchical scheduling algorithm that terminates the training of\nunderperforming architectures early to avoid wasting budget. To effectively\nleverage the non-uniform supervision signals produced by NOSH, we formulate\npredictor-based architecture search as learning to rank with pairwise\ncomparisons. The resulting method - RANK-NOSH, reduces the search budget by ~5x\nwhile achieving competitive or even better performance than previous\nstate-of-the-art predictor-based methods on various spaces and datasets.",
          "link": "http://arxiv.org/abs/2108.08019",
          "publishedOn": "2021-08-19T01:35:02.376Z",
          "wordCount": 609,
          "title": "RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving. (arXiv:2108.08019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stacker_L/0/1/0/all/0/1\">Lukas St&#xe4;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Juncong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidenreich_P/0/1/0/all/0/1\">Philipp Heidenreich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonarens_F/0/1/0/all/0/1\">Frank Bonarens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>",
          "description": "Deep neural networks have proven increasingly important for automotive scene\nunderstanding with new algorithms offering constant improvements of the\ndetection performance. However, there is little emphasis on experiences and\nneeds for deployment in embedded environments. We therefore perform a case\nstudy of the deployment of two representative object detection networks on an\nedge AI platform. In particular, we consider RetinaNet for image-based 2D\nobject detection and PointPillars for LiDAR-based 3D object detection. We\ndescribe the modifications necessary to convert the algorithms from a PyTorch\ntraining environment to the deployment environment taking into account the\navailable tools. We evaluate the runtime of the deployed DNN using two\ndifferent libraries, TensorRT and TorchScript. In our experiments, we observe\nslight advantages of TensorRT for convolutional layers and TorchScript for\nfully connected layers. We also study the trade-off between runtime and\nperformance, when selecting an optimized setup for deployment, and observe that\nquantization significantly reduces the runtime while having only little impact\non the detection performance.",
          "link": "http://arxiv.org/abs/2108.08166",
          "publishedOn": "2021-08-19T01:35:02.369Z",
          "wordCount": 628,
          "title": "Deployment of Deep Neural Networks for Object Detection on Edge AI Devices with Runtime Optimization. (arXiv:2108.08166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Efe_U/0/1/0/all/0/1\">Ufuk Efe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1\">Kutalmis Gokalp Ince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1\">A. Aydin Alatan</a>",
          "description": "Deep learning-based image matching methods are improved significantly during\nthe recent years. Although these methods are reported to outperform the\nclassical techniques, the performance of the classical methods is not examined\nin detail. In this study, we compare classical and learning-based methods by\nemploying mutual nearest neighbor search with ratio test and optimizing the\nratio test threshold to achieve the best performance on two different\nperformance metrics. After a fair comparison, the experimental results on\nHPatches dataset reveal that the performance gap between classical and\nlearning-based methods is not that significant. Throughout the experiments, we\ndemonstrated that SuperGlue is the state-of-the-art technique for the image\nmatching problem on HPatches dataset. However, if a single parameter, namely\nratio test threshold, is carefully optimized, a well-known traditional method\nSIFT performs quite close to SuperGlue and even outperforms in terms of mean\nmatching accuracy (MMA) under 1 and 2 pixel thresholds. Moreover, a recent\napproach, DFM, which only uses pre-trained VGG features as descriptors and\nratio test, is shown to outperform most of the well-trained learning-based\nmethods. Therefore, we conclude that the parameters of any classical method\nshould be analyzed carefully before comparing against a learning-based\ntechnique.",
          "link": "http://arxiv.org/abs/2108.08179",
          "publishedOn": "2021-08-19T01:35:02.364Z",
          "wordCount": 648,
          "title": "Effect of Parameter Optimization on Classical and Learning-based Image Matching Methods. (arXiv:2108.08179v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lakkapragada_A/0/1/0/all/0/1\">Anish Lakkapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis Wall</a>",
          "description": "A formal autism diagnosis is an inefficient and lengthy process. Families\noften have to wait years before receiving a diagnosis for their child; some may\nnot receive one at all due to this delay. One approach to this problem is to\nuse digital technologies to detect the presence of behaviors related to autism,\nwhich in aggregate may lead to remote and automated diagnostics. One of the\nstrongest indicators of autism is stimming, which is a set of repetitive,\nself-stimulatory behaviors such as hand flapping, headbanging, and spinning.\nUsing computer vision to detect hand flapping is especially difficult due to\nthe sparsity of public training data in this space and excessive shakiness and\nmotion in such data. Our work demonstrates a novel method that overcomes these\nissues: we use hand landmark detection over time as a feature representation\nwhich is then fed into a Long Short-Term Memory (LSTM) model. We achieve a\nvalidation accuracy and F1 Score of about 72% on detecting whether videos from\nthe Self-Stimulatory Behaviour Dataset (SSBD) contain hand flapping or not. Our\nbest model also predicts accurately on external videos we recorded of ourselves\noutside of the dataset it was trained on. This model uses less than 26,000\nparameters, providing promise for fast deployment into ubiquitous and wearable\ndigital settings for a remote autism diagnosis.",
          "link": "http://arxiv.org/abs/2108.07917",
          "publishedOn": "2021-08-19T01:35:02.339Z",
          "wordCount": 645,
          "title": "Activity Recognition for Autism Diagnosis. (arXiv:2108.07917v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattapalli_S/0/1/0/all/0/1\">Sai Mattapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athavale_R/0/1/0/all/0/1\">Rishi Athavale</a>",
          "description": "Due to morphological similarity at the microscopic level, making an accurate\nand time-sensitive distinction between blood cells affected by Acute\nLymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage\nof machine learning architectures. However, three of the most common models,\nVGG, ResNet, and Inception, each come with their own set of flaws with room for\nimprovement which demands the need for a superior model. ALLNet, the proposed\nhybrid convolutional neural network architecture, consists of a combination of\nthe VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019\n(available here) contains 10,691 images of white blood cells which were used to\ntrain and test the models. 7,272 of the images in the dataset are of cells with\nALL and 3,419 of them are of healthy cells. Of the images, 60% were used to\ntrain the model, 20% were used for the cross-validation set, and 20% were used\nfor the test set. ALLNet outperformed the VGG, ResNet, and the Inception models\nacross the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%,\na specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803\nin the cross-validation set. In the test set, ALLNet achieved an accuracy of\n92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of\n0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the\nclinical workspace can better treat the thousands of people suffering from ALL\nacross the world, many of whom are children.",
          "link": "http://arxiv.org/abs/2108.08195",
          "publishedOn": "2021-08-19T01:35:02.322Z",
          "wordCount": 711,
          "title": "ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells. (arXiv:2108.08195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07973",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saragadam_V/0/1/0/all/0/1\">Vishwanath Saragadam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dave_A/0/1/0/all/0/1\">Akshat Dave</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_A/0/1/0/all/0/1\">Ashok Veeraraghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>",
          "description": "We introduce DeepIR, a new thermal image processing framework that combines\nphysically accurate sensor modeling with deep network-based image\nrepresentation. Our key enabling observations are that the images captured by\nthermal sensors can be factored into slowly changing, scene-independent sensor\nnon-uniformities (that can be accurately modeled using physics) and a\nscene-specific radiance flux (that is well-represented using a deep\nnetwork-based regularizer). DeepIR requires neither training data nor periodic\nground-truth calibration with a known black body target--making it well suited\nfor practical computer vision tasks. We demonstrate the power of going DeepIR\nby developing new denoising and super-resolution algorithms that exploit\nmultiple images of the scene captured with camera jitter. Simulated and real\ndata experiments demonstrate that DeepIR can perform high-quality\nnon-uniformity correction with as few as three images, achieving a 10dB PSNR\nimprovement over competing approaches.",
          "link": "http://arxiv.org/abs/2108.07973",
          "publishedOn": "2021-08-19T01:35:02.313Z",
          "wordCount": 592,
          "title": "Thermal Image Processing via Physics-Inspired Deep Networks. (arXiv:2108.07973v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Bingchen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yinyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiqun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Point completion refers to completing the missing geometries of an object\nfrom incomplete observations. Main-stream methods predict the missing shapes by\ndecoding a global feature learned from the input point cloud, which often leads\nto deficient results in preserving topology consistency and surface details. In\nthis work, we present ME-PCN, a point completion network that leverages\n`emptiness' in 3D shape space. Given a single depth scan, previous methods\noften encode the occupied partial shapes while ignoring the empty regions (e.g.\nholes) in depth maps. In contrast, we argue that these `emptiness' clues\nindicate shape boundaries that can be used to improve topology representation\nand detail granularity on surfaces. Specifically, our ME-PCN encodes both the\noccupied point cloud and the neighboring `empty points'. It estimates\ncoarse-grained but complete and reasonable surface points in the first stage,\nfollowed by a refinement stage to produce fine-grained surface details.\nComprehensive experiments verify that our ME-PCN presents better qualitative\nand quantitative performance against the state-of-the-art. Besides, we further\nprove that our `emptiness' design is lightweight and easy to embed in existing\nmethods, which shows consistent effectiveness in improving the CD and EMD\nscores.",
          "link": "http://arxiv.org/abs/2108.08187",
          "publishedOn": "2021-08-19T01:35:02.303Z",
          "wordCount": 630,
          "title": "ME-PCN: Point Completion Conditioned on Mask Emptiness. (arXiv:2108.08187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenxu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Saifeng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budagavi_M/0/1/0/all/0/1\">Madhukar Budagavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaohu Guo</a>",
          "description": "In this paper, we propose a talking face generation method that takes an\naudio signal as input and a short target video clip as reference, and\nsynthesizes a photo-realistic video of the target face with natural lip\nmotions, head poses, and eye blinks that are in-sync with the input audio\nsignal. We note that the synthetic face attributes include not only explicit\nones such as lip motions that have high correlations with speech, but also\nimplicit ones such as head poses and eye blinks that have only weak correlation\nwith the input audio. To model such complicated relationships among different\nface attributes with input audio, we propose a FACe Implicit Attribute Learning\nGenerative Adversarial Network (FACIAL-GAN), which integrates the\nphonetics-aware, context-aware, and identity-aware information to synthesize\nthe 3D face animation with realistic motions of lips, head poses, and eye\nblinks. Then, our Rendering-to-Video network takes the rendered face images and\nthe attention map of eye blinks as input to generate the photo-realistic output\nvideo frames. Experimental results and user studies show our method can\ngenerate realistic talking face videos with not only synchronized lip motions,\nbut also natural head movements and eye blinks, with better qualities than the\nresults of state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.07938",
          "publishedOn": "2021-08-19T01:35:02.276Z",
          "wordCount": 655,
          "title": "FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning. (arXiv:2108.07938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isaac_Medina_B/0/1/0/all/0/1\">Brian K. S. Isaac-Medina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poyser_M/0/1/0/all/0/1\">Matt Poyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Organisciak_D/0/1/0/all/0/1\">Daniel Organisciak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby P. Breckon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>",
          "description": "Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due\nto both negligent and malicious use. For this reason, the automated detection\nand tracking of UAV is a fundamental task in aerial security systems. Common\ntechnologies for UAV detection include visible-band and thermal infrared\nimaging, radio frequency and radar. Recent advances in deep neural networks\n(DNNs) for image-based object detection open the possibility to use visual\ninformation for this detection and tracking task. Furthermore, these detection\narchitectures can be implemented as backbones for visual tracking systems,\nthereby enabling persistent tracking of UAV incursions. To date, no\ncomprehensive performance benchmark exists that applies DNNs to visible-band\nimagery for UAV detection and tracking. To this end, three datasets with varied\nenvironmental conditions for UAV detection and tracking, comprising a total of\n241 videos (331,486 images), are assessed using four detection architectures\nand three tracking frameworks. The best performing detector architecture\nobtains an mAP of 98.6% and the best performing tracking framework obtains a\nMOTA of 96.3%. Cross-modality evaluation is carried out between visible and\ninfrared spectrums, achieving a maximal 82.8% mAP on visible images when\ntraining in the infrared modality. These results provide the first public\nmulti-approach benchmark for state-of-the-art deep learning-based methods and\ngive insight into which detection and tracking architectures are effective in\nthe UAV domain.",
          "link": "http://arxiv.org/abs/2103.13933",
          "publishedOn": "2021-08-19T01:35:02.262Z",
          "wordCount": 721,
          "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural Networks: A Performance Benchmark. (arXiv:2103.13933v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.13216",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lambrou_G/0/1/0/all/0/1\">George I Lambrou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>",
          "description": "Deep learning has revolutionized computer vision utilizing the increased\navailability of big data and the power of parallel computational units such as\ngraphical processing units. The vast majority of deep learning research is\nconducted using images as training data, however the biomedical domain is rich\nin physiological signals that are used for diagnosis and prediction problems.\nIt is still an open research question how to best utilize signals to train deep\nneural networks.\n\nIn this paper we define the term Signal2Image (S2Is) as trainable or\nnon-trainable prefix modules that convert signals, such as\nElectroencephalography (EEG), to image-like representations making them\nsuitable for training image-based deep neural networks defined as `base\nmodels'. We compare the accuracy and time performance of four S2Is (`signal as\nimage', spectrogram, one and two layer Convolutional Neural Networks (CNNs))\ncombined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)\nalong with the depth-wise and 1D variations of the latter. We also provide\nempirical evidence that the one layer CNN S2I performs better in eleven out of\nfifteen tested models than non-trainable S2Is for classifying EEG signals and\nwe present visual comparisons of the outputs of the S2Is.",
          "link": "http://arxiv.org/abs/1904.13216",
          "publishedOn": "2021-08-19T01:35:02.254Z",
          "wordCount": 722,
          "title": "Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v5 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gongjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Deep learning-based 3D object detection has achieved unprecedented success\nwith the advent of large-scale autonomous driving datasets. However, drastic\nperformance degradation remains a critical challenge for cross-domain\ndeployment. In addition, existing 3D domain adaptive detection methods often\nassume prior access to the target domain annotations, which is rarely feasible\nin the real world. To address this challenge, we study a more realistic\nsetting, unsupervised 3D domain adaptive detection, which only utilizes source\ndomain annotations. 1) We first comprehensively investigate the major\nunderlying factors of the domain gap in 3D detection. Our key insight is that\ngeometric mismatch is the key factor of domain shift. 2) Then, we propose a\nnovel and unified framework, Multi-Level Consistency Network (MLC-Net), which\nemploys a teacher-student paradigm to generate adaptive and reliable\npseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level\nconsistency to facilitate cross-domain transfer. Extensive experiments\ndemonstrate that MLC-Net outperforms existing state-of-the-art methods\n(including those using additional target domain information) on standard\nbenchmarks. Notably, our approach is detector-agnostic, which achieves\nconsistent gains on both single- and two-stage 3D detectors.",
          "link": "http://arxiv.org/abs/2107.11355",
          "publishedOn": "2021-08-19T01:35:02.167Z",
          "wordCount": 654,
          "title": "Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency. (arXiv:2107.11355v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parisi_L/0/1/0/all/0/1\">Luca Parisi</a>",
          "description": "This study presents the m-arcsinh Kernel ('m-ar-K') Fast Independent\nComponent Analysis ('FastICA') method ('m-ar-K-FastICA') for feature\nextraction. The kernel trick has enabled dimensionality reduction techniques to\ncapture a higher extent of non-linearity in the data; however, reproducible,\nopen-source kernels to aid with feature extraction are still limited and may\nnot be reliable when projecting features from entropic data. The m-ar-K\nfunction, freely available in Python and compatible with its open-source\nlibrary 'scikit-learn', is hereby coupled with FastICA to achieve more reliable\nfeature extraction in presence of a high extent of randomness in the data,\nreducing the need for pre-whitening. Different classification tasks were\nconsidered, as related to five (N = 5) open access datasets of various degrees\nof information entropy, available from scikit-learn and the University\nCalifornia Irvine (UCI) Machine Learning repository. Experimental results\ndemonstrate improvements in the classification performance brought by the\nproposed feature extraction. The novel m-ar-K-FastICA dimensionality reduction\napproach is compared to the 'FastICA' gold standard method, supporting its\nhigher reliability and computational efficiency, regardless of the underlying\nuncertainty in the data.",
          "link": "http://arxiv.org/abs/2108.07908",
          "publishedOn": "2021-08-19T01:35:02.159Z",
          "wordCount": 654,
          "title": "M-ar-K-Fast Independent Component Analysis. (arXiv:2108.07908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.06803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>",
          "description": "Video data is with complex temporal dynamics due to various factors such as\ncamera motion, speed variation, and different activities. To effectively\ncapture this diverse motion pattern, this paper presents a new temporal\nadaptive module ({\\bf TAM}) to generate video-specific temporal kernels based\non its own feature map. TAM proposes a unique two-level adaptive modeling\nscheme by decoupling the dynamic kernel into a location sensitive importance\nmap and a location invariant aggregation weight. The importance map is learned\nin a local temporal window to capture short-term information, while the\naggregation weight is generated from a global view with a focus on long-term\nstructure. TAM is a modular block and could be integrated into 2D CNNs to yield\na powerful video architecture (TANet) with a very small extra computational\ncost. The extensive experiments on Kinetics-400 and Something-Something\ndatasets demonstrate that our TAM outperforms other temporal modeling methods\nconsistently, and achieves the state-of-the-art performance under the similar\ncomplexity. The code is available at \\url{\nhttps://github.com/liu-zhy/temporal-adaptive-module}.",
          "link": "http://arxiv.org/abs/2005.06803",
          "publishedOn": "2021-08-19T01:35:02.140Z",
          "wordCount": 651,
          "title": "TAM: Temporal Adaptive Module for Video Recognition. (arXiv:2005.06803v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>",
          "description": "Simulators can efficiently generate large amounts of labeled synthetic data\nwith perfect supervision for hard-to-label tasks like semantic segmentation.\nHowever, they introduce a domain gap that severely hurts real-world\nperformance. We propose to use self-supervised monocular depth estimation as a\nproxy task to bridge this gap and improve sim-to-real unsupervised domain\nadaptation (UDA). Our Geometric Unsupervised Domain Adaptation method (GUDA)\nlearns a domain-invariant representation via a multi-task objective combining\nsynthetic semantic supervision with real-world geometric constraints on videos.\nGUDA establishes a new state of the art in UDA for semantic segmentation on\nthree benchmarks, outperforming methods that use domain adversarial learning,\nself-training, or other self-supervised proxy tasks. Furthermore, we show that\nour method scales well with the quality and quantity of synthetic data while\nalso improving depth prediction.",
          "link": "http://arxiv.org/abs/2103.16694",
          "publishedOn": "2021-08-19T01:35:02.122Z",
          "wordCount": 596,
          "title": "Geometric Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2103.16694v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Suncheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuzhuo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1\">Mengyuan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>",
          "description": "Employing clustering strategy to assign unlabeled target images with pseudo\nlabels has become a trend for person re-identification (re-ID) algorithms in\ndomain adaptation. A potential limitation of these clustering-based methods is\nthat they always tend to introduce noisy labels, which will undoubtedly hamper\nthe performance of our re-ID system. To handle this limitation, an intuitive\nsolution is to utilize collaborative training to purify the pseudo label\nquality. However, there exists a challenge that the complementarity of two\nnetworks, which inevitably share a high similarity, becomes weakened gradually\nas training process goes on; worse still, these approaches typically ignore to\nconsider the self-discrepancy of intra-class relations. To address this issue,\nin this paper, we propose a multiple co-teaching framework for domain adaptive\nperson re-ID, opening up a promising direction about self-discrepancy problem\nunder unsupervised condition. On top of that, a mean-teaching mechanism is\nleveraged to enlarge the difference and discover more complementary features.\nComprehensive experiments conducted on several large-scale datasets show that\nour method achieves competitive performance compared with the\nstate-of-the-arts.",
          "link": "http://arxiv.org/abs/2104.02265",
          "publishedOn": "2021-08-19T01:35:02.115Z",
          "wordCount": 664,
          "title": "Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haipeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Kunming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>",
          "description": "Existing optical flow methods are erroneous in challenging scenes, such as\nfog, rain, and night because the basic optical flow assumptions such as\nbrightness and gradient constancy are broken. To address this problem, we\npresent an unsupervised learning approach that fuses gyroscope into optical\nflow learning. Specifically, we first convert gyroscope readings into motion\nfields named gyro field. Second, we design a self-guided fusion module to fuse\nthe background motion extracted from the gyro field with the optical flow and\nguide the network to focus on motion details. To the best of our knowledge,\nthis is the first deep learning-based framework that fuses gyroscope data and\nimage content for optical flow learning. To validate our method, we propose a\nnew dataset that covers regular and challenging scenes. Experiments show that\nour method outperforms the state-of-art methods in both regular and challenging\nscenes. Code and dataset are available at\nhttps://github.com/megvii-research/GyroFlow.",
          "link": "http://arxiv.org/abs/2103.13725",
          "publishedOn": "2021-08-19T01:35:02.108Z",
          "wordCount": 607,
          "title": "GyroFlow: Gyroscope-Guided Unsupervised Optical Flow Learning. (arXiv:2103.13725v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yuecong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_A/0/1/0/all/0/1\">Aiming Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_X/0/1/0/all/0/1\">Xiujuan Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize\nunsegmented signs from image streams. Overfitting is one of the most critical\nproblems in CSLR training, and previous works show that the iterative training\nscheme can partially solve this problem while also costing more training time.\nIn this study, we revisit the iterative training scheme in recent CSLR works\nand realize that sufficient training of the feature extractor is critical to\nsolving the overfitting problem. Therefore, we propose a Visual Alignment\nConstraint (VAC) to enhance the feature extractor with alignment supervision.\nSpecifically, the proposed VAC comprises two auxiliary losses: one focuses on\nvisual features only, and the other enforces prediction alignment between the\nfeature extractor and the alignment module. Moreover, we propose two metrics to\nreflect overfitting by measuring the prediction inconsistency between the\nfeature extractor and the alignment module. Experimental results on two\nchallenging CSLR datasets show that the proposed VAC makes CSLR networks\nend-to-end trainable and achieves competitive performance.",
          "link": "http://arxiv.org/abs/2104.02330",
          "publishedOn": "2021-08-19T01:35:02.101Z",
          "wordCount": 638,
          "title": "Visual Alignment Constraint for Continuous Sign Language Recognition. (arXiv:2104.02330v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1812.02134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oramas_J/0/1/0/all/0/1\">Jose Oramas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>",
          "description": "We address the problem of unpaired geometric image-to-image translation.\nRather than transferring the style of an image as a whole, our goal is to\ntranslate the geometry of an object as depicted in different domains while\npreserving its appearance characteristics. Our model is trained in an unpaired\nfashion, i.e. without the need of paired images during training. It performs\nall steps of the shape transfer within a single model and without additional\npost-processing stages. Extensive experiments on the VITON, CMU-Multi-PIE and\nour own FashionStyle datasets show the effectiveness of the method. In\naddition, we show that despite their low-dimensionality, the features learned\nby our model are useful to the item retrieval task.",
          "link": "http://arxiv.org/abs/1812.02134",
          "publishedOn": "2021-08-19T01:35:02.094Z",
          "wordCount": 617,
          "title": "An Unpaired Shape Transforming Method for Image Translation and Cross-Domain Retrieval. (arXiv:1812.02134v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Herman_J/0/1/0/all/0/1\">James Herman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skabelkin_A/0/1/0/all/0/1\">Alexey Skabelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_I/0/1/0/all/0/1\">Ivan Zhukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumskoy_M/0/1/0/all/0/1\">Max Kumskoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>",
          "description": "Existing research on autonomous driving primarily focuses on urban driving,\nwhich is insufficient for characterising the complex driving behaviour\nunderlying high-speed racing. At the same time, existing racing simulation\nframeworks struggle in capturing realism, with respect to visual rendering,\nvehicular dynamics, and task objectives, inhibiting the transfer of learning\nagents to real-world contexts. We introduce a new environment, where agents\nLearn-to-Race (L2R) in simulated competition-style racing, using multimodal\ninformation--from virtual cameras to a comprehensive array of inertial\nmeasurement sensors. Our environment, which includes a simulator and an\ninterfacing training framework, accurately models vehicle dynamics and racing\nconditions. In this paper, we release the Arrival simulator for autonomous\nracing. Next, we propose the L2R task with challenging metrics, inspired by\nlearning-to-drive challenges, Formula-style racing, and multimodal trajectory\nprediction for autonomous driving. Additionally, we provide the L2R framework\nsuite, facilitating simulated racing on high-precision models of real-world\ntracks. Finally, we provide an official L2R task dataset of expert\ndemonstrations, as well as a series of baseline experiments and reference\nimplementations. We make all code available:\nhttps://github.com/learn-to-race/l2r.",
          "link": "http://arxiv.org/abs/2103.11575",
          "publishedOn": "2021-08-19T01:35:02.073Z",
          "wordCount": 670,
          "title": "Learn-to-Race: A Multimodal Control Environment for Autonomous Racing. (arXiv:2103.11575v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_X/0/1/0/all/0/1\">Xiang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>",
          "description": "During the COVID-19 coronavirus epidemic, almost everyone wears a facial\nmask, which poses a huge challenge to deep face recognition. In this workshop,\nwe organize Masked Face Recognition (MFR) challenge and focus on bench-marking\ndeep face recognition methods under the existence of facial masks. In the MFR\nchallenge, there are two main tracks: the InsightFace track and the WebFace260M\ntrack. For the InsightFace track, we manually collect a large-scale masked face\ntest set with 7K identities. In addition, we also collect a children test set\nincluding 14K identities and a multi-racial test set containing 242K\nidentities. By using these three test sets, we build up an online model testing\nsystem, which can give a comprehensive evaluation of face recognition models.\nTo avoid data privacy problems, no test image is released to the public. As the\nchallenge is still under-going, we will keep on updating the top-ranked\nsolutions as well as this report on the arxiv.",
          "link": "http://arxiv.org/abs/2108.08191",
          "publishedOn": "2021-08-19T01:35:02.055Z",
          "wordCount": 652,
          "title": "Masked Face Recognition Challenge: The InsightFace Track Report. (arXiv:2108.08191v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1\">Mohamed Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1\">Ruben Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael Black</a>",
          "description": "A long-standing goal in computer vision is to capture, model, and\nrealistically synthesize human behavior. Specifically, by learning from data,\nour goal is to enable virtual humans to navigate within cluttered indoor scenes\nand naturally interact with objects. Such embodied behavior has applications in\nvirtual reality, computer games, and robotics, while synthesized behavior can\nbe used as a source of training data. This is challenging because real human\nmotion is diverse and adapts to the scene. For example, a person can sit or lie\non a sofa in many places and with varying styles. It is necessary to model this\ndiversity when synthesizing virtual humans that realistically perform\nhuman-scene interactions. We present a novel data-driven, stochastic motion\nsynthesis method that models different styles of performing a given action with\na target object. Our method, called SAMP, for Scene-Aware Motion Prediction,\ngeneralizes to target objects of various geometries while enabling the\ncharacter to navigate in cluttered scenes. To train our method, we collected\nMoCap data covering various sitting, lying down, walking, and running styles.\nWe demonstrate our method on complex indoor scenes and achieve superior\nperformance compared to existing solutions. Our code and data are available for\nresearch at https://samp.is.tue.mpg.de.",
          "link": "http://arxiv.org/abs/2108.08284",
          "publishedOn": "2021-08-19T01:35:02.048Z",
          "wordCount": 637,
          "title": "Stochastic Scene-Aware Motion Prediction. (arXiv:2108.08284v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Nianjin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>",
          "description": "In this paper, we introduce a new framework for unsupervised deep homography\nestimation. Our contributions are 3 folds. First, unlike previous methods that\nregress 4 offsets for a homography, we propose a homography flow\nrepresentation, which can be estimated by a weighted sum of 8 pre-defined\nhomography flow bases. Second, considering a homography contains 8\nDegree-of-Freedoms (DOFs) that is much less than the rank of the network\nfeatures, we propose a Low Rank Representation (LRR) block that reduces the\nfeature rank, so that features corresponding to the dominant motions are\nretained while others are rejected. Last, we propose a Feature Identity Loss\n(FIL) to enforce the learned image feature warp-equivariant, meaning that the\nresult should be identical if the order of warp operation and feature\nextraction is swapped. With this constraint, the unsupervised optimization is\nachieved more effectively and more stable features are learned. Extensive\nexperiments are conducted to demonstrate the effectiveness of all the newly\nproposed components, and results show that our approach outperforms the\nstate-of-the-art on the homography benchmark datasets both qualitatively and\nquantitatively. Code is available at\nhttps://github.com/megvii-research/BasesHomo.",
          "link": "http://arxiv.org/abs/2103.15346",
          "publishedOn": "2021-08-19T01:35:02.011Z",
          "wordCount": 653,
          "title": "Motion Basis Learning for Unsupervised Deep Homography Estimation with Subspace Projection. (arXiv:2103.15346v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ynjiun Paul Wang</a>",
          "description": "Most of stereo vision works are focusing on computing the dense pixel\ndisparity of a given pair of left and right images. A camera pair usually\nrequired lens undistortion and stereo calibration to provide an undistorted\nepipolar line calibrated image pair for accurate dense pixel disparity\ncomputation. Due to noise, object occlusion, repetitive or lack of texture and\nlimitation of matching algorithms, the pixel disparity accuracy usually suffers\nthe most at those object boundary areas. Although statistically the total\nnumber of pixel disparity errors might be low (under 2% according to the Kitti\nVision Benchmark of current top ranking algorithms), the percentage of these\ndisparity errors at object boundaries are very high. This renders the\nsubsequence 3D object distance detection with much lower accuracy than desired.\nThis paper proposed a different approach for solving a 3D object distance\ndetection by detecting object disparity directly without going through a dense\npixel disparity computation. An example squeezenet Object Disparity-SSD\n(OD-SSD) was constructed to demonstrate an efficient object disparity detection\nwith comparable accuracy compared with Kitti dataset pixel disparity ground\ntruth. Further training and testing results with mixed image dataset captured\nby several different stereo systems may suggest that an OD-SSD might be\nagnostic to stereo system parameters such as a baseline, FOV, lens distortion,\neven left/right camera epipolar line misalignment.",
          "link": "http://arxiv.org/abs/2108.07939",
          "publishedOn": "2021-08-19T01:35:02.004Z",
          "wordCount": 649,
          "title": "Object Disparity. (arXiv:2108.07939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1\">Munan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenglang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Domain shift happens in cross-domain scenarios commonly because of the wide\ngaps between different domains: when applying a deep learning model\nwell-trained in one domain to another target domain, the model usually performs\npoorly. To tackle this problem, unsupervised domain adaptation (UDA) techniques\nare proposed to bridge the gap between different domains, for the purpose of\nimproving model performance without annotation in the target domain.\nParticularly, UDA has a great value for multimodal medical image analysis,\nwhere annotation difficulty is a practical concern. However, most existing UDA\nmethods can only achieve satisfactory improvements in one adaptation direction\n(e.g., MRI to CT), but often perform poorly in the other (CT to MRI), limiting\ntheir practical usage. In this paper, we propose a bidirectional UDA (BiUDA)\nframework based on disentangled representation learning for equally competent\ntwo-way UDA performances. This framework employs a unified domain-aware pattern\nencoder which not only can adaptively encode images in different domains\nthrough a domain controller, but also improve model efficiency by eliminating\nredundant parameters. Furthermore, to avoid distortion of contents and patterns\nof input images during the adaptation process, a content-pattern consistency\nloss is introduced. Additionally, for better UDA segmentation performance, a\nlabel consistency strategy is proposed to provide extra supervision by\nrecomposing target-domain-styled images and corresponding source-domain\nannotations. Comparison experiments and ablation studies conducted on two\npublic datasets demonstrate the superiority of our BiUDA framework to current\nstate-of-the-art UDA methods and the effectiveness of its novel designs. By\nsuccessfully addressing two-way adaptations, our BiUDA framework offers a\nflexible solution of UDA techniques to the real-world scenario.",
          "link": "http://arxiv.org/abs/2108.07979",
          "publishedOn": "2021-08-19T01:35:01.997Z",
          "wordCount": 714,
          "title": "A New Bidirectional Unsupervised Domain Adaptation Segmentation Framework. (arXiv:2108.07979v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shenhan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhi Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1\">YiHao Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>",
          "description": "Co-speech gesture generation is to synthesize a gesture sequence that not\nonly looks real but also matches with the input speech audio. Our method\ngenerates the movements of a complete upper body, including arms, hands, and\nthe head. Although recent data-driven methods achieve great success, challenges\nstill exist, such as limited variety, poor fidelity, and lack of objective\nmetrics. Motivated by the fact that the speech cannot fully determine the\ngesture, we design a method that learns a set of gesture template vectors to\nmodel the latent conditions, which relieve the ambiguity. For our method, the\ntemplate vector determines the general appearance of a generated gesture\nsequence, while the speech audio drives subtle movements of the body, both\nindispensable for synthesizing a realistic gesture sequence. Due to the\nintractability of an objective metric for gesture-speech synchronization, we\nadopt the lip-sync error as a proxy metric to tune and evaluate the\nsynchronization ability of our model. Extensive experiments show the\nsuperiority of our method in both objective and subjective evaluations on\nfidelity and synchronization.",
          "link": "http://arxiv.org/abs/2108.08020",
          "publishedOn": "2021-08-19T01:35:01.981Z",
          "wordCount": 617,
          "title": "Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates. (arXiv:2108.08020v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yidan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Mingsheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>",
          "description": "Convolution on 3D point clouds that generalized from 2D grid-like domains is\nwidely researched yet far from perfect. The standard convolution characterises\nfeature correspondences indistinguishably among 3D points, presenting an\nintrinsic limitation of poor distinctive feature learning. In this paper, we\npropose Adaptive Graph Convolution (AdaptConv) which generates adaptive kernels\nfor points according to their dynamically learned features. Compared with using\na fixed/isotropic kernel, AdaptConv improves the flexibility of point cloud\nconvolutions, effectively and precisely capturing the diverse relations between\npoints from different semantic parts. Unlike popular attentional weight\nschemes, the proposed AdaptConv implements the adaptiveness inside the\nconvolution operation instead of simply assigning different weights to the\nneighboring points. Extensive qualitative and quantitative evaluations show\nthat our method outperforms state-of-the-art point cloud classification and\nsegmentation approaches on several benchmark datasets. Our code is available at\nhttps://github.com/hrzhou2/AdaptConv-master.",
          "link": "http://arxiv.org/abs/2108.08035",
          "publishedOn": "2021-08-19T01:35:01.962Z",
          "wordCount": 585,
          "title": "Adaptive Graph Convolution for Point Cloud Analysis. (arXiv:2108.08035v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Amirul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1\">Matthew Kowal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruce_N/0/1/0/all/0/1\">Neil D. B. Bruce</a>",
          "description": "In this paper, we challenge the common assumption that collapsing the spatial\ndimensions of a 3D (spatial-channel) tensor in a convolutional neural network\n(CNN) into a vector via global pooling removes all spatial information.\nSpecifically, we demonstrate that positional information is encoded based on\nthe ordering of the channel dimensions, while semantic information is largely\nnot. Following this demonstration, we show the real world impact of these\nfindings by applying them to two applications. First, we propose a simple yet\neffective data augmentation strategy and loss function which improves the\ntranslation invariance of a CNN's output. Second, we propose a method to\nefficiently determine which channels in the latent representation are\nresponsible for (i) encoding overall position information or (ii)\nregion-specific positions. We first show that semantic segmentation has a\nsignificant reliance on the overall position channels to make predictions. We\nthen show for the first time that it is possible to perform a `region-specific'\nattack, and degrade a network's performance in a particular part of the input.\nWe believe our findings and demonstrated applications will benefit research\nareas concerned with understanding the characteristics of CNNs.",
          "link": "http://arxiv.org/abs/2108.07884",
          "publishedOn": "2021-08-19T01:35:01.953Z",
          "wordCount": 644,
          "title": "Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs. (arXiv:2108.07884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07936",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kawamata_R/0/1/0/all/0/1\">Ryota Kawamata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Betsui_K/0/1/0/all/0/1\">Keiichi Betsui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kazuyoshi Yamazaki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sakakibara_R/0/1/0/all/0/1\">Rei Sakakibara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shimano_T/0/1/0/all/0/1\">Takeshi Shimano</a>",
          "description": "Compact and low-cost devices are needed for autonomous driving to image and\nmeasure distances to objects 360-degree around. We have been developing an\nomnidirectional stereo camera exploiting two hyperbolic mirrors and a single\nset of a lens and sensor, which makes this camera compact and cost efficient.\nWe establish a new calibration method for this camera considering higher-order\nradial distortion, detailed tangential distortion, an image sensor tilt, and a\nlens-mirror offset. Our method reduces the calibration error by 6.0 and 4.3\ntimes for the upper- and lower-view images, respectively. The random error of\nthe distance measurement is 4.9% and the systematic error is 5.7% up to objects\n14 meters apart, which is improved almost nine times compared to the\nconventional method. The remaining distance errors is due to a degraded optical\nresolution of the prototype, which we plan to make further improvements as\nfuture work.",
          "link": "http://arxiv.org/abs/2108.07936",
          "publishedOn": "2021-08-19T01:35:01.893Z",
          "wordCount": 607,
          "title": "Calibration Method of the Monocular Omnidirectional Stereo Camera. (arXiv:2108.07936v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vieira_e_Silva_A/0/1/0/all/0/1\">Andr&#xe9; Luiz Buarque Vieira-e-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felix_H/0/1/0/all/0/1\">Heitor de Castro Felix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaves_T/0/1/0/all/0/1\">Thiago de Menezes Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1\">Francisco Paulo Magalh&#xe3;es Sim&#xf5;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1\">Veronica Teichrieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Michel Mozinho dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santiago_H/0/1/0/all/0/1\">Hemir da Cunha Santiago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sgotti_V/0/1/0/all/0/1\">Virginia Ad&#xe9;lia Cordeiro Sgotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_H/0/1/0/all/0/1\">Henrique Baptista Duffles Teixeira Lott Neto</a>",
          "description": "Many power line companies are using UAVs to perform their inspection\nprocesses instead of putting their workers at risk by making them climb high\nvoltage power line towers, for instance. A crucial task for the inspection is\nto detect and classify assets in the power transmission lines. However, public\ndata related to power line assets are scarce, preventing a faster evolution of\nthis area. This work proposes the Power Line Assets Dataset, containing\nhigh-resolution and real-world images of multiple high-voltage power line\ncomponents. It has 2,409 annotated objects divided into five classes:\ntransmission tower, insulator, spacer, tower plate, and Stockbridge damper,\nwhich vary in size (resolution), orientation, illumination, angulation, and\nbackground. This work also presents an evaluation with popular deep object\ndetection methods, showing considerable room for improvement. The PLAD dataset\nis publicly available at https://github.com/andreluizbvs/PLAD.",
          "link": "http://arxiv.org/abs/2108.07944",
          "publishedOn": "2021-08-19T01:35:01.885Z",
          "wordCount": 621,
          "title": "PLAD: A Dataset for Multi-Size Power Line Assets Detection in High-Resolution UAV Images. (arXiv:2108.07944v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1\">Niamh Belton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welaratne_I/0/1/0/all/0/1\">Ivan Welaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahlan_A/0/1/0/all/0/1\">Adil Dahlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearne_R/0/1/0/all/0/1\">Ronan T Hearne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagos_M/0/1/0/all/0/1\">Misgina Tsighe Hagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1\">Aonghus Lawlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1\">Kathleen M. Curran</a>",
          "description": "This work employs a pre-trained, multi-view Convolutional Neural Network\n(CNN) with a spatial attention block to optimise knee injury detection. An\nopen-source Magnetic Resonance Imaging (MRI) data set with image-level labels\nwas leveraged for this analysis. As MRI data is acquired from three planes, we\ncompare our technique using data from a single-plane and multiple planes\n(multi-plane). For multi-plane, we investigate various methods of fusing the\nplanes in the network. This analysis resulted in the novel 'MPFuseNet' network\nand state-of-the-art Area Under the Curve (AUC) scores for detecting Anterior\nCruciate Ligament (ACL) tears and Abnormal MRIs, achieving AUC scores of 0.977\nand 0.957 respectively. We then developed an objective metric, Penalised\nLocalisation Accuracy (PLA), to validate the model's localisation ability. This\nmetric compares binary masks generated from Grad-Cam output and the\nradiologist's annotations on a sample of MRIs. We also extracted explainability\nfeatures in a model-agnostic approach that were then verified as clinically\nrelevant by the radiologist.",
          "link": "http://arxiv.org/abs/2108.08136",
          "publishedOn": "2021-08-19T01:35:01.861Z",
          "wordCount": 623,
          "title": "Optimising Knee Injury Detection with Spatial Attention and Validating Localisation Ability. (arXiv:2108.08136v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>",
          "description": "We present ARCH++, an image-based method to reconstruct 3D avatars with\narbitrary clothing styles. Our reconstructed avatars are animation-ready and\nhighly realistic, in both the visible regions from input views and the unseen\nregions. While prior work shows great promise of reconstructing animatable\nclothed humans with various topologies, we observe that there exist fundamental\nlimitations resulting in sub-optimal reconstruction quality. In this paper, we\nrevisit the major steps of image-based avatar reconstruction and address the\nlimitations with ARCH++. First, we introduce an end-to-end point based geometry\nencoder to better describe the semantics of the underlying 3D human body, in\nreplacement of previous hand-crafted features. Second, in order to address the\noccupancy ambiguity caused by topological changes of clothed humans in the\ncanonical pose, we propose a co-supervising framework with cross-space\nconsistency to jointly estimate the occupancy in both the posed and canonical\nspaces. Last, we use image-to-image translation networks to further refine\ndetailed geometry and texture on the reconstructed surface, which improves the\nfidelity and consistency across arbitrary viewpoints. In the experiments, we\ndemonstrate improvements over the state of the art on both public benchmarks\nand user studies in reconstruction quality and realism.",
          "link": "http://arxiv.org/abs/2108.07845",
          "publishedOn": "2021-08-19T01:35:01.772Z",
          "wordCount": 635,
          "title": "ARCH++: Animation-Ready Clothed Human Reconstruction Revisited. (arXiv:2108.07845v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiajun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Pei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "The DeepFakes, which are the facial manipulation techniques, is the emerging\nthreat to digital society. Various DeepFake detection methods and datasets are\nproposed for detecting such data, especially for face-swapping. However, recent\nresearches less consider facial animation, which is also important in the\nDeepFake attack side. It tries to animate a face image with actions provided by\na driving video, which also leads to a concern about the security of recent\npayment systems that reply on liveness detection to authenticate real users via\nrecognising a sequence of user facial actions. However, our experiments show\nthat the existed datasets are not sufficient to develop reliable detection\nmethods. While the current liveness detector cannot defend such videos as the\nattack. As a response, we propose a new human face animation dataset, called\nDeepFake MNIST+, generated by a SOTA image animation generator. It includes\n10,000 facial animation videos in ten different actions, which can spoof the\nrecent liveness detectors. A baseline detection method and a comprehensive\nanalysis of the method is also included in this paper. In addition, we analyze\nthe proposed dataset's properties and reveal the difficulty and importance of\ndetecting animation datasets under different types of motion and compression\nquality.",
          "link": "http://arxiv.org/abs/2108.07949",
          "publishedOn": "2021-08-19T01:35:01.734Z",
          "wordCount": 641,
          "title": "DeepFake MNIST+: A DeepFake Facial Animation Dataset. (arXiv:2108.07949v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marques_B/0/1/0/all/0/1\">Bruno Augusto Dorta Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clua_E/0/1/0/all/0/1\">Esteban Walter Gonzalez Clua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montenegro_A/0/1/0/all/0/1\">Anselmo Antunes Montenegro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1\">Cristina Nader Vasconcelos</a>",
          "description": "The representation of consistent mixed reality (XR) environments requires\nadequate real and virtual illumination composition in real-time. Estimating the\nlighting of a real scenario is still a challenge. Due to the ill-posed nature\nof the problem, classical inverse-rendering techniques tackle the problem for\nsimple lighting setups. However, those assumptions do not satisfy the current\nstate-of-art in computer graphics and XR applications. While many recent works\nsolve the problem using machine learning techniques to estimate the environment\nlight and scene's materials, most of them are limited to geometry or previous\nknowledge. This paper presents a CNN-based model to estimate complex lighting\nfor mixed reality environments with no previous information about the scene. We\nmodel the environment illumination using a set of spherical harmonics (SH)\nenvironment lighting, capable of efficiently represent area lighting. We\npropose a new CNN architecture that inputs an RGB image and recognizes, in\nreal-time, the environment lighting. Unlike previous CNN-based lighting\nestimation methods, we propose using a highly optimized deep neural network\narchitecture, with a reduced number of parameters, that can learn high complex\nlighting scenarios from real-world high-dynamic-range (HDR) environment images.\nWe show in the experiments that the CNN architecture can predict the\nenvironment lighting with an average mean squared error (MSE) of \\num{7.85e-04}\nwhen comparing SH lighting coefficients. We validate our model in a variety of\nmixed reality scenarios. Furthermore, we present qualitative results comparing\nrelights of real-world scenes.",
          "link": "http://arxiv.org/abs/2108.07903",
          "publishedOn": "2021-08-19T01:35:01.702Z",
          "wordCount": 689,
          "title": "Spatially and color consistent environment lighting estimation using deep neural networks for mixed reality. (arXiv:2108.07903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07966",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yucheng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hua_Y/0/1/0/all/0/1\">Yi Hua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aswin C. Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>",
          "description": "Lensless cameras provide a framework to build thin imaging systems by\nreplacing the lens in a conventional camera with an amplitude or phase mask\nnear the sensor. Existing methods for lensless imaging can recover the depth\nand intensity of the scene, but they require solving computationally-expensive\ninverse problems. Furthermore, existing methods struggle to recover dense\nscenes with large depth variations. In this paper, we propose a lensless\nimaging system that captures a small number of measurements using different\npatterns on a programmable mask. In this context, we make three contributions.\nFirst, we present a fast recovery algorithm to recover textures on a fixed\nnumber of depth planes in the scene. Second, we consider the mask design\nproblem, for programmable lensless cameras, and provide a design template for\noptimizing the mask patterns with the goal of improving depth estimation.\nThird, we use a refinement network as a post-processing step to identify and\nremove artifacts in the reconstruction. These modifications are evaluated\nextensively with experimental results on a lensless camera prototype to\nshowcase the performance benefits of the optimized masks and recovery\nalgorithms over the state of the art.",
          "link": "http://arxiv.org/abs/2108.07966",
          "publishedOn": "2021-08-19T01:35:01.695Z",
          "wordCount": 656,
          "title": "A Simple Framework for 3D Lensless Imaging with Programmable Masks. (arXiv:2108.07966v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07948",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Heliang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "An important scenario for image quality assessment (IQA) is to evaluate image\nrestoration (IR) algorithms. The state-of-the-art approaches adopt a\nfull-reference paradigm that compares restored images with their corresponding\npristine-quality images. However, pristine-quality images are usually\nunavailable in blind image restoration tasks and real-world scenarios. In this\npaper, we propose a practical solution named degraded-reference IQA (DR-IQA),\nwhich exploits the inputs of IR models, degraded images, as references.\nSpecifically, we extract reference information from degraded images by\ndistilling knowledge from pristine-quality images. The distillation is achieved\nthrough learning a reference space, where various degraded images are\nencouraged to share the same feature statistics with pristine-quality images.\nAnd the reference space is optimized to capture deep image priors that are\nuseful for quality assessment. Note that pristine-quality images are only used\nduring training. Our work provides a powerful and differentiable metric for\nblind IRs, especially for GAN-based methods. Extensive experiments show that\nour results can even be close to the performance of full-reference settings.",
          "link": "http://arxiv.org/abs/2108.07948",
          "publishedOn": "2021-08-19T01:35:01.662Z",
          "wordCount": 611,
          "title": "Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment. (arXiv:2108.07948v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruijun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>",
          "description": "Deep face recognition (FR) has achieved significantly high accuracy on\nseveral challenging datasets and fosters successful real-world applications,\neven showing high robustness to the illumination variation that is usually\nregarded as a main threat to the FR system. However, in the real world,\nillumination variation caused by diverse lighting conditions cannot be fully\ncovered by the limited face dataset. In this paper, we study the threat of\nlighting against FR from a new angle, i.e., adversarial attack, and identify a\nnew task, i.e., adversarial relighting. Given a face image, adversarial\nrelighting aims to produce a naturally relighted counterpart while fooling the\nstate-of-the-art deep FR methods. To this end, we first propose the physical\nmodel-based adversarial relighting attack (ARA) denoted as\nalbedo-quotient-based adversarial relighting attack (AQ-ARA). It generates\nnatural adversarial light under the physical lighting model and guidance of FR\nsystems and synthesizes adversarially relighted face images. Moreover, we\npropose the auto-predictive adversarial relighting attack (AP-ARA) by training\nan adversarial relighting network (ARNet) to automatically predict the\nadversarial light in a one-step manner according to different input faces,\nallowing efficiency-sensitive applications. More importantly, we propose to\ntransfer the above digital attacks to physical ARA (Phy-ARA) through a precise\nrelighting device, making the estimated adversarial lighting condition\nreproducible in the real world. We validate our methods on three\nstate-of-the-art deep FR methods, i.e., FaceNet, ArcFace, and CosFace, on two\npublic datasets. The extensive and insightful results demonstrate our work can\ngenerate realistic adversarial relighted face images fooling FR easily,\nrevealing the threat of specific light directions and strengths.",
          "link": "http://arxiv.org/abs/2108.07920",
          "publishedOn": "2021-08-19T01:35:01.601Z",
          "wordCount": 690,
          "title": "Adversarial Relighting against Face Recognition. (arXiv:2108.07920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1\">Maja J Matari&#x107;</a>",
          "description": "Automated systems that detect the social behavior of deception can enhance\nhuman well-being across medical, social work, and legal domains. Labeled\ndatasets to train supervised deception detection models can rarely be collected\nfor real-world, high-stakes contexts. To address this challenge, we propose the\nfirst unsupervised approach for detecting real-world, high-stakes deception in\nvideos without requiring labels. This paper presents our novel approach for\naffect-aware unsupervised Deep Belief Networks (DBN) to learn discriminative\nrepresentations of deceptive and truthful behavior. Drawing on psychology\ntheories that link affect and deception, we experimented with unimodal and\nmultimodal DBN-based approaches trained on facial valence, facial arousal,\naudio, and visual features. In addition to using facial affect as a feature on\nwhich DBN models are trained, we also introduce a DBN training procedure that\nuses facial affect as an aligner of audio-visual representations. We conducted\nclassification experiments with unsupervised Gaussian Mixture Model clustering\nto evaluate our approaches. Our best unsupervised approach (trained on facial\nvalence and visual features) achieved an AUC of 80%, outperforming human\nability and performing comparably to fully-supervised models. Our results\nmotivate future work on unsupervised, affect-aware computational approaches for\ndetecting deception and other social behaviors in the wild.",
          "link": "http://arxiv.org/abs/2108.07897",
          "publishedOn": "2021-08-19T01:35:01.554Z",
          "wordCount": 637,
          "title": "Affect-Aware Deep Belief Network Representations for Multimodal Unsupervised Deception Detection. (arXiv:2108.07897v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>",
          "description": "Existing salient object detection (SOD) models mainly rely on CNN-based\nU-shaped structures with skip connections to combine the global contexts and\nlocal spatial details that are crucial for locating salient objects and\nrefining object details, respectively. Despite great successes, the ability of\nCNN in learning global contexts is limited. Recently, the vision transformer\nhas achieved revolutionary progress in computer vision owing to its powerful\nmodeling of global dependencies. However, directly applying the transformer to\nSOD is obviously suboptimal because the transformer lacks the ability to learn\nlocal spatial representations. To this end, this paper explores the combination\nof transformer and CNN to learn both global and local representations for SOD.\nWe propose a transformer-based Asymmetric Bilateral U-Net (AbiU-Net). The\nasymmetric bilateral encoder has a transformer path and a lightweight CNN path,\nwhere the two paths communicate at each encoder stage to learn complementary\nglobal contexts and local spatial details, respectively. The asymmetric\nbilateral decoder also consists of two paths to process features from the\ntransformer and CNN encoder paths, with communication at each decoder stage for\ndecoding coarse salient object locations and find-grained object details,\nrespectively. Such communication between the two encoder/decoder paths enables\nAbiU-Net to learn complementary global and local representations, taking\nadvantage of the natural properties of transformer and CNN, respectively.\nHence, ABiU-Net provides a new perspective for transformer-based SOD. Extensive\nexperiments demonstrate that ABiU-Net performs favorably against previous\nstate-of-the-art SOD methods. The code will be released.",
          "link": "http://arxiv.org/abs/2108.07851",
          "publishedOn": "2021-08-19T01:35:01.476Z",
          "wordCount": 676,
          "title": "Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net. (arXiv:2108.07851v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11001",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Goyal_B/0/1/0/all/0/1\">Bhavya Goyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>",
          "description": "Scene understanding under low-light conditions is a challenging problem. This\nis due to the small number of photons captured by the camera and the resulting\nlow signal-to-noise ratio (SNR). Single-photon cameras (SPCs) are an emerging\nsensing modality that are capable of capturing images with high sensitivity.\nDespite having minimal read-noise, images captured by SPCs in photon-starved\nconditions still suffer from strong shot noise, preventing reliable scene\ninference. We propose photon scale-space a collection of high-SNR images\nspanning a wide range of photons-per-pixel (PPP) levels (but same scene\ncontent) as guides to train inference model on low photon flux images. We\ndevelop training techniques that push images with different illumination levels\ncloser to each other in feature representation space. The key idea is that\nhaving a spectrum of different brightness levels during training enables\neffective guidance, and increases robustness to shot noise even in extreme\nnoise cases. Based on the proposed approach, we demonstrate, via simulations\nand real experiments with a SPAD camera, high-performance on various inference\ntasks such as image classification and monocular depth estimation under ultra\nlow-light, down to < 1 PPP.",
          "link": "http://arxiv.org/abs/2107.11001",
          "publishedOn": "2021-08-18T01:55:02.599Z",
          "wordCount": 658,
          "title": "Photon-Starved Scene Inference using Single Photon Cameras. (arXiv:2107.11001v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zejia Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>",
          "description": "Label distributions in real-world are oftentimes long-tailed and imbalanced,\nresulting in biased models towards dominant labels. While long-tailed\nrecognition has been extensively studied for image classification tasks,\nlimited effort has been made for video domain. In this paper, we introduce\nVideoLT, a large-scale long-tailed video recognition dataset, as a step toward\nreal-world video recognition. Our VideoLT contains 256,218 untrimmed videos,\nannotated into 1,004 classes with a long-tailed distribution. Through extensive\nstudies, we demonstrate that state-of-the-art methods used for long-tailed\nimage recognition do not perform well in the video domain due to the additional\ntemporal dimension in video data. This motivates us to propose FrameStack, a\nsimple yet effective method for long-tailed video recognition task. In\nparticular, FrameStack performs sampling at the frame-level in order to balance\nclass distributions, and the sampling ratio is dynamically determined using\nknowledge derived from the network during training. Experimental results\ndemonstrate that FrameStack can improve classification performance without\nsacrificing overall accuracy. Code and dataset are available at:\nhttps://github.com/17Skye17/VideoLT.",
          "link": "http://arxiv.org/abs/2105.02668",
          "publishedOn": "2021-08-18T01:55:02.592Z",
          "wordCount": 638,
          "title": "VideoLT: Large-scale Long-tailed Video Recognition. (arXiv:2105.02668v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zichao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>",
          "description": "Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes are available at\nhttps://github.com/whwu95/DSANet.",
          "link": "http://arxiv.org/abs/2105.12085",
          "publishedOn": "2021-08-18T01:55:02.586Z",
          "wordCount": 684,
          "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoqian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weicheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>",
          "description": "The convolutional neural network (CNN) is vulnerable to degraded images with\neven very small variations (e.g. corrupted and adversarial samples). One of the\npossible reasons is that CNN pays more attention to the most discriminative\nregions, but ignores the auxiliary features when learning, leading to the lack\nof feature diversity for final judgment. In our method, we propose to\ndynamically suppress significant activation values of CNN by group-wise\ninhibition, but not fixedly or randomly handle them when training. The feature\nmaps with different activation distribution are then processed separately to\ntake the feature independence into account. CNN is finally guided to learn\nricher discriminative features hierarchically for robust classification\naccording to the proposed regularization. Our method is comprehensively\nevaluated under multiple settings, including classification against\ncorruptions, adversarial attacks and low data regime. Extensive experimental\nresults show that the proposed method can achieve significant improvements in\nterms of both robustness and generalization performances, when compared with\nthe state-of-the-art methods. Code is available at\nhttps://github.com/LinusWu/TENET_Training.",
          "link": "http://arxiv.org/abs/2103.02152",
          "publishedOn": "2021-08-18T01:55:02.577Z",
          "wordCount": 649,
          "title": "Group-wise Inhibition based Feature Regularization for Robust Classification. (arXiv:2103.02152v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_I/0/1/0/all/0/1\">Ishwarya Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenakshisundaram_N/0/1/0/all/0/1\">Nishaali Meenakshisundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_I/0/1/0/all/0/1\">Ishwarya Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D_S/0/1/0/all/0/1\">Shiloah Elizabeth D</a>, <a href=\"http://arxiv.org/find/cs/1/au:+C_S/0/1/0/all/0/1\">Sunil Retmin Raj C</a>",
          "description": "Vision plays a crucial role to comprehend the world around us as more than\n85% of the external information is obtained through the vision system. It\nlargely influences our mobility, cognition, information access, and interaction\nwith the environment as well as with other people. Blindness prevents a person\nfrom gaining knowledge of the surrounding environment and makes unassisted\nnavigation, object recognition, obstacle avoidance, and reading tasks major\nchallenges. Many existing systems are often limited by cost and complexity. To\nhelp the visually challenged overcome these difficulties faced in everyday\nlife, we propose the idea of VisBuddy, a smart assistant which will help the\nvisually challenged with their day-to-day activities. VisBuddy is a voice-based\nassistant, where the user can give voice commands to perform specific tasks.\nVisBuddy uses the techniques of image captioning for describing the user's\nsurroundings, optical character recognition (OCR) for reading the text in the\nuser's view, object detection to search and find the objects in a room and web\nscraping to give the user the latest news. VisBuddy has been built by combining\nthe concepts from Deep Learning and the Internet of Things. Thus, VisBuddy\nserves as a cost-efficient, powerful and all-in-one assistant for the visually\nchallenged by helping them with their day-to-day activities.",
          "link": "http://arxiv.org/abs/2108.07761",
          "publishedOn": "2021-08-18T01:55:02.555Z",
          "wordCount": 655,
          "title": "VisBuddy -- A Smart Wearable Assistant for the Visually Challenged. (arXiv:2108.07761v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frederic Z. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>",
          "description": "We address the problem of detecting human-object interactions in images using\ngraphical neural networks. Unlike conventional methods, where nodes send scaled\nbut otherwise identical messages to each of their neighbours, we propose to\ncondition messages between pairs of nodes on their spatial relationships,\nresulting in different messages going to neighbours of the same node. To this\nend, we explore various ways of applying spatial conditioning under a\nmulti-branch structure. Through extensive experimentation we demonstrate the\nadvantages of spatial conditioning for the computation of the adjacency\nstructure, messages and the refined graph features. In particular, we\nempirically show that as the quality of the bounding boxes increases, their\ncoarse appearance features contribute relatively less to the disambiguation of\ninteractions compared to the spatial information. Our method achieves an mAP of\n31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming\nstate-of-the-art on fine-tuned detections.",
          "link": "http://arxiv.org/abs/2012.06060",
          "publishedOn": "2021-08-18T01:55:02.548Z",
          "wordCount": 624,
          "title": "Spatially Conditioned Graphs for Detecting Human-Object Interactions. (arXiv:2012.06060v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Liangbin Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>",
          "description": "Though many attempts have been made in blind super-resolution to restore\nlow-resolution images with unknown and complex degradations, they are still far\nfrom addressing general real-world degraded images. In this work, we extend the\npowerful ESRGAN to a practical restoration application (namely, Real-ESRGAN),\nwhich is trained with pure synthetic data. Specifically, a high-order\ndegradation modeling process is introduced to better simulate complex\nreal-world degradations. We also consider the common ringing and overshoot\nartifacts in the synthesis process. In addition, we employ a U-Net\ndiscriminator with spectral normalization to increase discriminator capability\nand stabilize the training dynamics. Extensive comparisons have shown its\nsuperior visual performance than prior works on various real datasets. We also\nprovide efficient implementations to synthesize training pairs on the fly.",
          "link": "http://arxiv.org/abs/2107.10833",
          "publishedOn": "2021-08-18T01:55:02.542Z",
          "wordCount": 600,
          "title": "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data. (arXiv:2107.10833v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Conventional face super-resolution methods usually assume testing\nlow-resolution (LR) images lie in the same domain as the training ones. Due to\ndifferent lighting conditions and imaging hardware, domain gaps between\ntraining and testing images inevitably occur in many real-world scenarios.\nNeglecting those domain gaps would lead to inferior face super-resolution (FSR)\nperformance. However, how to transfer a trained FSR model to a target domain\nefficiently and effectively has not been investigated. To tackle this problem,\nwe develop a Domain-Aware Pyramid-based Face Super-Resolution network, named\nDAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces\nfrom a target domain by exploiting only a pair of high-resolution (HR) and LR\nexemplar in the target domain. To be specific, our DAP-FSR firstly employs its\nencoder to extract the multi-scale latent representations of the input LR face.\nConsidering only one target domain example is available, we propose to augment\nthe target domain data by mixing the latent representations of the target\ndomain face and source domain ones, and then feed the mixed representations to\nthe decoder of our DAP-FSR. The decoder will generate new face images\nresembling the target domain image style. The generated HR faces in turn are\nused to optimize our decoder to reduce the domain gap. By iteratively updating\nthe latent representations and our decoder, our DAP-FSR will be adapted to the\ntarget domain, thus achieving authentic and high-quality upsampled HR faces.\nExtensive experiments on three newly constructed benchmarks validate the\neffectiveness and superior performance of our DAP-FSR compared to the\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2103.08863",
          "publishedOn": "2021-08-18T01:55:02.534Z",
          "wordCount": 719,
          "title": "Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar. (arXiv:2103.08863v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saksham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>",
          "description": "Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.",
          "link": "http://arxiv.org/abs/2101.06069",
          "publishedOn": "2021-08-18T01:55:02.516Z",
          "wordCount": 702,
          "title": "Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose a point cloud-applicable explainability approach\nbased on local surrogate model-based method to show which components contribute\nto the classification. Moreover, we propose quantitative fidelity validations\nfor generated explanations that enhance the persuasive power of explainability\nand compare the plausibility of different existing point cloud-applicable\nexplainability methods. Our new explainability approach provides a fairly\naccurate, more semantically coherent and widely applicable explanation for\npoint cloud classification tasks. Our code is available at\nhttps://github.com/Explain3D/LIME-3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-08-18T01:55:02.510Z",
          "wordCount": 597,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pavllo_D/0/1/0/all/0/1\">Dario Pavllo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas Kohler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1\">Aurelien Lucchi</a>",
          "description": "Recent advances in differentiable rendering have sparked an interest in\nlearning generative models of textured 3D meshes from image collections. These\nmodels natively disentangle pose and appearance, enable downstream applications\nin computer graphics, and improve the ability of generative models to\nunderstand the concept of image formation. Although there has been prior work\non learning such models from collections of 2D images, these approaches require\na delicate pose estimation step that exploits annotated keypoints, thereby\nrestricting their applicability to a few specific datasets. In this work, we\npropose a GAN framework for generating textured triangle meshes without relying\non such annotations. We show that the performance of our approach is on par\nwith prior work that relies on ground-truth keypoints, and more importantly, we\ndemonstrate the generality of our method by setting new baselines on a larger\nset of categories from ImageNet - for which keypoints are not available -\nwithout any class-specific hyperparameter tuning. We release our code at\nhttps://github.com/dariopavllo/textured-3d-gan",
          "link": "http://arxiv.org/abs/2103.15627",
          "publishedOn": "2021-08-18T01:55:02.503Z",
          "wordCount": 642,
          "title": "Learning Generative Models of Textured 3D Meshes from Real-World Images. (arXiv:2103.15627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jaehui Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jun-Hyuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun-Ho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>",
          "description": "The video-based action recognition task has been extensively studied in\nrecent years. In this paper, we study the structural vulnerability of deep\nlearning-based action recognition models against the adversarial attack using\nthe one frame attack that adds an inconspicuous perturbation to only a single\nframe of a given video clip. Our analysis shows that the models are highly\nvulnerable against the one frame attack due to their structural properties.\nExperiments demonstrate high fooling rates and inconspicuous characteristics of\nthe attack. Furthermore, we show that strong universal one frame perturbations\ncan be obtained under various scenarios. Our work raises the serious issue of\nadversarial vulnerability of the state-of-the-art action recognition models in\nvarious perspectives.",
          "link": "http://arxiv.org/abs/2011.14585",
          "publishedOn": "2021-08-18T01:55:02.497Z",
          "wordCount": 596,
          "title": "Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack. (arXiv:2011.14585v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhisheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "In this paper, we propose Parametric Contrastive Learning (PaCo) to tackle\nlong-tailed recognition. Based on theoretical analysis, we observe supervised\ncontrastive loss tends to bias on high-frequency classes and thus increases the\ndifficulty of imbalanced learning. We introduce a set of parametric class-wise\nlearnable centers to rebalance from an optimization perspective. Further, we\nanalyze our PaCo loss under a balanced setting. Our analysis demonstrates that\nPaCo can adaptively enhance the intensity of pushing samples of the same class\nclose as more samples are pulled together with their corresponding centers and\nbenefit hard example learning. Experiments on long-tailed CIFAR, ImageNet,\nPlaces, and iNaturalist 2018 manifest the new state-of-the-art for long-tailed\nrecognition. On full ImageNet, models trained with PaCo loss surpass supervised\ncontrastive learning across various ResNet backbones, e.g., our ResNet-200\nachieves 81.8% top-1 accuracy. Our code is available at\nhttps://github.com/dvlab-research/Parametric-Contrastive-Learning.",
          "link": "http://arxiv.org/abs/2107.12028",
          "publishedOn": "2021-08-18T01:55:02.491Z",
          "wordCount": 600,
          "title": "Parametric Contrastive Learning. (arXiv:2107.12028v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the test (validation)\nset of the Aff-Wild2 database, the achieved CCC is 0.463 (0.469) for valence\nand 0.492 (0.649) for arousal, which significantly outperforms the baseline\nmethod with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for\nvalence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-08-18T01:55:02.485Z",
          "wordCount": 678,
          "title": "Continuous Emotion Recognition with Audio-visual Leader-follower Attentive Fusion. (arXiv:2107.01175v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03035",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xinghua Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1\">Gongning Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kuanquan Wang</a>",
          "description": "Coronary artery disease (CAD) has posed a leading threat to the lives of\ncardiovascular disease patients worldwide for a long time. Therefore, automated\ndiagnosis of CAD has indispensable significance in clinical medicine. However,\nthe complexity of coronary artery plaques that cause CAD makes the automatic\ndetection of coronary artery stenosis in Coronary CT angiography (CCTA) a\ndifficult task. In this paper, we propose a Transformer network (TR-Net) for\nthe automatic detection of significant stenosis (i.e. luminal narrowing > 50%)\nwhile practically completing the computer-assisted diagnosis of CAD. The\nproposed TR-Net introduces a novel Transformer, and tightly combines\nconvolutional layers and Transformer encoders, allowing their advantages to be\ndemonstrated in the task. By analyzing semantic information sequences, TR-Net\ncan fully understand the relationship between image information in each\nposition of a multiplanar reformatted (MPR) image, and accurately detect\nsignificant stenosis based on both local and global information. We evaluate\nour TR-Net on a dataset of 76 patients from different patients annotated by\nexperienced radiologists. Experimental results illustrate that our TR-Net has\nachieved better results in ACC (0.92), Spec (0.96), PPV (0.84), F1 (0.79) and\nMCC (0.74) indicators compared with the state-of-the-art methods. The source\ncode is publicly available from the link (https://github.com/XinghuaMa/TR-Net).",
          "link": "http://arxiv.org/abs/2107.03035",
          "publishedOn": "2021-08-18T01:55:02.478Z",
          "wordCount": 676,
          "title": "Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries. (arXiv:2107.03035v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1\">Suvrit Sra</a>",
          "description": "The label shift problem refers to the supervised learning setting where the\ntrain and test label distributions do not match. Existing work addressing label\nshift usually assumes access to an \\emph{unlabelled} test sample. This sample\nmay be used to estimate the test label distribution, and to then train a\nsuitably re-weighted classifier. While approaches using this idea have proven\neffective, their scope is limited as it is not always feasible to access the\ntarget domain; further, they require repeated retraining if the model is to be\ndeployed in \\emph{multiple} test environments. Can one instead learn a\n\\emph{single} classifier that is robust to arbitrary label shifts from a broad\nfamily? In this paper, we answer this question by proposing a model that\nminimises an objective based on distributionally robust optimisation (DRO). We\nthen design and analyse a gradient descent-proximal mirror ascent algorithm\ntailored for large-scale problems to optimise the proposed objective. %, and\nestablish its convergence. Finally, through experiments on CIFAR-100 and\nImageNet, we show that our technique can significantly improve performance over\na number of baselines in settings where label shift is present.",
          "link": "http://arxiv.org/abs/2010.12230",
          "publishedOn": "2021-08-18T01:55:02.472Z",
          "wordCount": 671,
          "title": "Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouazizi_A/0/1/0/all/0/1\">Arij Bouazizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiederer_J/0/1/0/all/0/1\">Julian Wiederer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kressel_U/0/1/0/all/0/1\">Ulrich Kressel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>",
          "description": "We present a self-supervised learning algorithm for 3D human pose estimation\nof a single person based on a multiple-view camera system and 2D body pose\nestimates for each view. To train our model, represented by a deep neural\nnetwork, we propose a four-loss function learning algorithm, which does not\nrequire any 2D or 3D body pose ground-truth. The proposed loss functions make\nuse of the multiple-view geometry to reconstruct 3D body pose estimates and\nimpose body pose constraints across the camera views. Our approach utilizes all\navailable camera views during training, while the inference is single-view. In\nour evaluations, we show promising performance on Human3.6M and HumanEva\nbenchmarks, while we also present a generalization study on MPI-INF-3DHP\ndataset, as well as several ablation results. Overall, we outperform all\nself-supervised learning methods and reach comparable results to supervised and\nweakly-supervised learning approaches. Our code and models are publicly\navailable",
          "link": "http://arxiv.org/abs/2108.07777",
          "publishedOn": "2021-08-18T01:55:02.452Z",
          "wordCount": 591,
          "title": "Self-Supervised 3D Human Pose Estimation with Multiple-View Geometry. (arXiv:2108.07777v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenzhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Current state-of-the-art image captioning models adopt autoregressive\ndecoders, \\ie they generate each word by conditioning on previously generated\nwords, which leads to heavy latency during inference. To tackle this issue,\nnon-autoregressive image captioning models have recently been proposed to\nsignificantly accelerate the speed of inference by generating all words in\nparallel. However, these non-autoregressive models inevitably suffer from large\ngeneration quality degradation since they remove words dependence excessively.\nTo make a better trade-off between speed and quality, we introduce a\nsemi-autoregressive model for image captioning~(dubbed as SATIC), which keeps\nthe autoregressive property in global but generates words parallelly in local .\nBased on Transformer, there are only a few modifications needed to implement\nSATIC. Experimental results on the MSCOCO image captioning benchmark show that\nSATIC can achieve a good trade-off without bells and whistles. Code is\navailable at {\\color{magenta}\\url{https://github.com/YuanEZhou/satic}}.",
          "link": "http://arxiv.org/abs/2106.09436",
          "publishedOn": "2021-08-18T01:55:02.427Z",
          "wordCount": 601,
          "title": "Semi-Autoregressive Transformer for Image Captioning. (arXiv:2106.09436v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yan_J/0/1/0/all/0/1\">Junjie yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "We introduce the first Neural Architecture Search (NAS) method to find a\nbetter transformer architecture for image recognition. Recently, transformers\nwithout CNN-based backbones are found to achieve impressive performance for\nimage recognition. However, the transformer is designed for NLP tasks and thus\ncould be sub-optimal when directly used for image recognition. In order to\nimprove the visual representation ability for transformers, we propose a new\nsearch space and searching algorithm. Specifically, we introduce a locality\nmodule that models the local correlations in images explicitly with fewer\ncomputational cost. With the locality module, our search space is defined to\nlet the search algorithm freely trade off between global and local information\nas well as optimizing the low-level design choice in each module. To tackle the\nproblem caused by huge search space, a hierarchical neural architecture search\nmethod is proposed to search the optimal vision transformer from two levels\nseparately with the evolutionary algorithm. Extensive experiments on the\nImageNet dataset demonstrate that our method can find more discriminative and\nefficient transformer variants than the ResNet family (e.g., ResNet101) and the\nbaseline ViT for image classification.",
          "link": "http://arxiv.org/abs/2107.02960",
          "publishedOn": "2021-08-18T01:55:02.407Z",
          "wordCount": 677,
          "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer. (arXiv:2107.02960v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kibeom Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Seogkyu Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>",
          "description": "Style transfer aims to reproduce content images with the styles from\nreference images. Existing universal style transfer methods successfully\ndeliver arbitrary styles to original images either in an artistic or a\nphoto-realistic way. However, the range of 'arbitrary style' defined by\nexisting works is bounded in the particular domain due to their structural\nlimitation. Specifically, the degrees of content preservation and stylization\nare established according to a predefined target domain. As a result, both\nphoto-realistic and artistic models have difficulty in performing the desired\nstyle transfer for the other domain. To overcome this limitation, we propose a\nunified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer\nnot only the style but also the property of domain (i.e., domainness) from a\ngiven reference image. To this end, we design a novel domainness indicator that\ncaptures the domainness value from the texture and structural features of\nreference images. Moreover, we introduce a unified framework with domain-aware\nskip connection to adaptively transfer the stroke and palette to the input\ncontents guided by the domainness indicator. Our extensive experiments validate\nthat our model produces better qualitative results and outperforms previous\nmethods in terms of proxy metrics on both artistic and photo-realistic\nstylizations.",
          "link": "http://arxiv.org/abs/2108.04441",
          "publishedOn": "2021-08-18T01:55:02.390Z",
          "wordCount": 666,
          "title": "Domain-Aware Universal Style Transfer. (arXiv:2108.04441v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alwassel_H/0/1/0/all/0/1\">Humam Alwassel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Due to the large memory footprint of untrimmed videos, current\nstate-of-the-art video localization methods operate atop precomputed video clip\nfeatures. These features are extracted from video encoders typically trained\nfor trimmed action classification tasks, making such features not necessarily\nsuitable for temporal localization. In this work, we propose a novel supervised\npretraining paradigm for clip features that not only trains to classify\nactivities but also considers background clips and global video information to\nimprove temporal sensitivity. Extensive experiments show that using features\ntrained with our novel pretraining strategy significantly improves the\nperformance of recent state-of-the-art methods on three tasks: Temporal Action\nLocalization, Action Proposal Generation, and Dense Video Captioning. We also\nshow that our pretraining approach is effective across three encoder\narchitectures and two pretraining datasets. We believe video feature encoding\nis an important building block for localization algorithms, and extracting\ntemporally-sensitive features should be of paramount importance in building\nmore accurate models. The code and pretrained models are available on our\nproject website.",
          "link": "http://arxiv.org/abs/2011.11479",
          "publishedOn": "2021-08-18T01:55:02.384Z",
          "wordCount": 644,
          "title": "TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks. (arXiv:2011.11479v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Lingwei Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yongwei Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guiqing Li</a>",
          "description": "Human motion prediction is a challenging task due to the stochasticity and\naperiodicity of future poses. Recently, graph convolutional network has been\nproven to be very effective to learn dynamic relations among pose joints, which\nis helpful for pose prediction. On the other hand, one can abstract a human\npose recursively to obtain a set of poses at multiple scales. With the increase\nof the abstraction level, the motion of the pose becomes more stable, which\nbenefits pose prediction too. In this paper, we propose a novel Multi-Scale\nResidual Graph Convolution Network (MSR-GCN) for human pose prediction task in\nthe manner of end-to-end. The GCNs are used to extract features from fine to\ncoarse scale and then from coarse to fine scale. The extracted features at each\nscale are then combined and decoded to obtain the residuals between the input\nand target poses. Intermediate supervisions are imposed on all the predicted\nposes, which enforces the network to learn more representative features. Our\nproposed approach is evaluated on two standard benchmark datasets, i.e., the\nHuman3.6M dataset and the CMU Mocap dataset. Experimental results demonstrate\nthat our method outperforms the state-of-the-art approaches. Code and\npre-trained models are available at https://github.com/Droliven/MSRGCN.",
          "link": "http://arxiv.org/abs/2108.07152",
          "publishedOn": "2021-08-18T01:55:02.377Z",
          "wordCount": 672,
          "title": "MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction. (arXiv:2108.07152v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Yuqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>",
          "description": "We present a task and benchmark dataset for person-centric visual grounding,\nthe problem of linking between people named in a caption and people pictured in\nan image. In contrast to prior work in visual grounding, which is predominantly\nobject-based, our new task masks out the names of people in captions in order\nto encourage methods trained on such image-caption pairs to focus on contextual\ncues (such as rich interactions between multiple people), rather than learning\nassociations between names and appearances. To facilitate this task, we\nintroduce a new dataset, Who's Waldo, mined automatically from image-caption\ndata on Wikimedia Commons. We propose a Transformer-based method that\noutperforms several strong baselines on this task, and are releasing our data\nto the research community to spur work on contextual models that consider both\nvision and language.",
          "link": "http://arxiv.org/abs/2108.07253",
          "publishedOn": "2021-08-18T01:55:02.356Z",
          "wordCount": 609,
          "title": "Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "This paper presents a new vision Transformer, called Swin Transformer, that\ncapably serves as a general-purpose backbone for computer vision. Challenges in\nadapting Transformer from language to vision arise from differences between the\ntwo domains, such as large variations in the scale of visual entities and the\nhigh resolution of pixels in images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose representation is\ncomputed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme\nbrings greater efficiency by limiting self-attention computation to\nnon-overlapping local windows while also allowing for cross-window connection.\nThis hierarchical architecture has the flexibility to model at various scales\nand has linear computational complexity with respect to image size. These\nqualities of Swin Transformer make it compatible with a broad range of vision\ntasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and\ndense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP\non COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its\nperformance surpasses the previous state-of-the-art by a large margin of +2.7\nbox AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the\npotential of Transformer-based models as vision backbones. The hierarchical\ndesign and the shifted window approach also prove beneficial for all-MLP\narchitectures. The code and models are publicly available\nat~\\url{https://github.com/microsoft/Swin-Transformer}.",
          "link": "http://arxiv.org/abs/2103.14030",
          "publishedOn": "2021-08-18T01:55:02.345Z",
          "wordCount": 701,
          "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (arXiv:2103.14030v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chun-Han Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "Federated learning methods enable us to train machine learning models on\ndistributed user data while preserving its privacy. However, it is not always\nfeasible to obtain high-quality supervisory signals from users, especially for\nvision tasks. Unlike typical federated settings with labeled client data, we\nconsider a more practical scenario where the distributed client data is\nunlabeled, and a centralized labeled dataset is available on the server. We\nfurther take the server-client and inter-client domain shifts into account and\npose a domain adaptation problem with one source (centralized server data) and\nmultiple targets (distributed client data). Within this new Federated\nMulti-Target Domain Adaptation (FMTDA) task, we analyze the model performance\nof exiting domain adaptation methods and propose an effective DualAdapt method\nto address the new challenges. Extensive experimental results on image\nclassification and semantic segmentation tasks demonstrate that our method\nachieves high accuracy, incurs minimal communication cost, and requires low\ncomputational resources on client devices.",
          "link": "http://arxiv.org/abs/2108.07792",
          "publishedOn": "2021-08-18T01:55:02.330Z",
          "wordCount": 585,
          "title": "Federated Multi-Target Domain Adaptation. (arXiv:2108.07792v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ping_G/0/1/0/all/0/1\">Guiju Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esfahani_M/0/1/0/all/0/1\">Mahdi Abolfazli Esfahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>",
          "description": "Solving the challenging problem of 3D object reconstruction from a single\nimage appropriately gives existing technologies the ability to perform with a\nsingle monocular camera rather than requiring depth sensors. In recent years,\nthanks to the development of deep learning, 3D reconstruction of a single image\nhas demonstrated impressive progress. Existing researches use Chamfer distance\nas a loss function to guide the training of the neural network. However, the\nChamfer loss will give equal weights to all points inside the 3D point clouds.\nIt tends to sacrifice fine-grained and thin structures to avoid incurring a\nhigh loss, which will lead to visually unsatisfactory results. This paper\nproposes a framework that can recover a detailed three-dimensional point cloud\nfrom a single image by focusing more on boundaries (edge and corner points).\nExperimental results demonstrate that the proposed method outperforms existing\ntechniques significantly, both qualitatively and quantitatively, and has fewer\ntraining parameters.",
          "link": "http://arxiv.org/abs/2108.07685",
          "publishedOn": "2021-08-18T01:55:02.322Z",
          "wordCount": 596,
          "title": "Visual Enhanced 3D Point Cloud Reconstruction from A Single Image. (arXiv:2108.07685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afrasiyabi_A/0/1/0/all/0/1\">Arman Afrasiyabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Lalonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Christian Gagn&#xe9;</a>",
          "description": "We introduce Mixture-based Feature Space Learning (MixtFSL) for obtaining a\nrich and robust feature representation in the context of few-shot image\nclassification. Previous works have proposed to model each base class either\nwith a single point or with a mixture model by relying on offline clustering\nalgorithms. In contrast, we propose to model base classes with mixture models\nby simultaneously training the feature extractor and learning the mixture model\nparameters in an online manner. This results in a richer and more\ndiscriminative feature space which can be employed to classify novel examples\nfrom very few samples. Two main stages are proposed to train the MixtFSL model.\nFirst, the multimodal mixtures for each base class and the feature extractor\nparameters are learned using a combination of two loss functions. Second, the\nresulting network and mixture models are progressively refined through a\nleader-follower learning procedure, which uses the current estimate as a\n\"target\" network. This target network is used to make a consistent assignment\nof instances to mixture components, which increases performance and stabilizes\ntraining. The effectiveness of our end-to-end feature space learning approach\nis demonstrated with extensive experiments on four standard datasets and four\nbackbones. Notably, we demonstrate that when we combine our robust\nrepresentation with recent alignment-based approaches, we achieve new\nstate-of-the-art results in the inductive setting, with an absolute accuracy\nfor 5-shot classification of 82.45 on miniImageNet, 88.20 with tieredImageNet,\nand 60.70 in FC100 using the ResNet-12 backbone.",
          "link": "http://arxiv.org/abs/2011.11872",
          "publishedOn": "2021-08-18T01:55:02.315Z",
          "wordCount": 701,
          "title": "Mixture-based Feature Space Learning for Few-shot Image Classification. (arXiv:2011.11872v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "Dense video captioning aims to generate multiple associated captions with\ntheir temporal locations from the video. Previous methods follow a\nsophisticated \"localize-then-describe\" scheme, which heavily relies on numerous\nhand-crafted components. In this paper, we proposed a simple yet effective\nframework for end-to-end dense video captioning with parallel decoding (PDVC),\nby formulating the dense caption generation as a set prediction task. In\npractice, through stacking a newly proposed event counter on the top of a\ntransformer decoder, the PDVC precisely segments the video into a number of\nevent pieces under the holistic understanding of the video content, which\neffectively increases the coherence and readability of predicted captions.\nCompared with prior arts, the PDVC has several appealing advantages: (1)\nWithout relying on heuristic non-maximum suppression or a recurrent event\nsequence selection network to remove redundancy, PDVC directly produces an\nevent set with an appropriate size; (2) In contrast to adopting the two-stage\nscheme, we feed the enhanced representations of event queries into the\nlocalization head and caption head in parallel, making these two sub-tasks\ndeeply interrelated and mutually promoted through the optimization; (3) Without\nbells and whistles, extensive experiments on ActivityNet Captions and YouCook2\nshow that PDVC is capable of producing high-quality captioning results,\nsurpassing the state-of-the-art two-stage methods when its localization\naccuracy is on par with them. Code is available at\nhttps://github.com/ttengwang/PDVC.",
          "link": "http://arxiv.org/abs/2108.07781",
          "publishedOn": "2021-08-18T01:55:02.308Z",
          "wordCount": 673,
          "title": "End-to-End Dense Video Captioning with Parallel Decoding. (arXiv:2108.07781v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03255",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delbracio_M/0/1/0/all/0/1\">Mauricio Delbracio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelly_D/0/1/0/all/0/1\">Damien Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>",
          "description": "Recent work has shown impressive results on data-driven defocus deblurring\nusing the two-image views available on modern dual-pixel (DP) sensors. One\nsignificant challenge in this line of research is access to DP data. Despite\nmany cameras having DP sensors, only a limited number provide access to the\nlow-level DP sensor images. In addition, capturing training data for defocus\ndeblurring involves a time-consuming and tedious setup requiring the camera's\naperture to be adjusted. Some cameras with DP sensors (e.g., smartphones) do\nnot have adjustable apertures, further limiting the ability to produce the\nnecessary training data. We address the data capture bottleneck by proposing a\nprocedure to generate realistic DP data synthetically. Our synthesis approach\nmimics the optical image formation found on DP sensors and can be applied to\nvirtual scenes rendered with standard computer software. Leveraging these\nrealistic synthetic DP images, we introduce a recurrent convolutional network\n(RCN) architecture that improves deblurring results and is suitable for use\nwith single-frame and multi-frame data (e.g., video) captured by DP sensors.\nFinally, we show that our synthetic DP data is useful for training DNN models\ntargeting video deblurring applications where access to DP data remains\nchallenging.",
          "link": "http://arxiv.org/abs/2012.03255",
          "publishedOn": "2021-08-18T01:55:02.289Z",
          "wordCount": 662,
          "title": "Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data. (arXiv:2012.03255v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1\">Hao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We propose a novel framework for video inpainting by adopting an internal\nlearning strategy. Unlike previous methods that use optical flow for\ncross-frame context propagation to inpaint unknown regions, we show that this\ncan be achieved implicitly by fitting a convolutional neural network to known\nregions. Moreover, to handle challenging sequences with ambiguous backgrounds\nor long-term occlusion, we design two regularization terms to preserve\nhigh-frequency details and long-term temporal consistency. Extensive\nexperiments on the DAVIS dataset demonstrate that the proposed method achieves\nstate-of-the-art inpainting quality quantitatively and qualitatively. We\nfurther extend the proposed method to another challenging task: learning to\nremove an object from a video giving a single object mask in only one frame in\na 4K video.",
          "link": "http://arxiv.org/abs/2108.01912",
          "publishedOn": "2021-08-18T01:55:02.283Z",
          "wordCount": 580,
          "title": "Internal Video Inpainting by Implicit Long-range Propagation. (arXiv:2108.01912v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wittich_D/0/1/0/all/0/1\">Dennis Wittich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottensteiner_F/0/1/0/all/0/1\">Franz Rottensteiner</a>",
          "description": "This paper addresses domain adaptation for the pixel-wise classification of\nremotely sensed data using deep neural networks (DNN) as a strategy to reduce\nthe requirements of DNN with respect to the availability of training data. We\nfocus on the setting in which labelled data are only available in a source\ndomain DS, but not in a target domain DT. Our method is based on adversarial\ntraining of an appearance adaptation network (AAN) that transforms images from\nDS such that they look like images from DT. Together with the original label\nmaps from DS, the transformed images are used to adapt a DNN to DT. We propose\na joint training strategy of the AAN and the classifier, which constrains the\nAAN to transform the images such that they are correctly classified. In this\nway, objects of a certain class are changed such that they resemble objects of\nthe same class in DT. To further improve the adaptation performance, we propose\na new regularization loss for the discriminator network used in domain\nadversarial training. We also address the problem of finding the optimal values\nof the trained network parameters, proposing an unsupervised entropy based\nparameter selection criterion which compensates for the fact that there is no\nvalidation set in DT that could be monitored. As a minor contribution, we\npresent a new weighting strategy for the cross-entropy loss, addressing the\nproblem of imbalanced class distributions. Our method is evaluated in 42\nadaptation scenarios using datasets from 7 cities, all consisting of\nhigh-resolution digital orthophotos and height data. It achieves a positive\ntransfer in all cases, and on average it improves the performance in the target\ndomain by 4.3% in overall accuracy. In adaptation scenarios between datasets\nfrom the ISPRS semantic labelling benchmark our method outperforms those from\nrecent publications by 10-20% with respect to the mean intersection over union.",
          "link": "http://arxiv.org/abs/2108.07779",
          "publishedOn": "2021-08-18T01:55:02.276Z",
          "wordCount": 744,
          "title": "Appearance Based Deep Domain Adaptation for the Classification of Aerial Images. (arXiv:2108.07779v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lina Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xibin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xiaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1\">Junwei Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "Depth completion aims to recover a dense depth map from a sparse depth map\nwith the corresponding color image as input. Recent approaches mainly formulate\ndepth completion as a one-stage end-to-end learning task, which outputs dense\ndepth maps directly. However, the feature extraction and supervision in\none-stage frameworks are insufficient, limiting the performance of these\napproaches. To address this problem, we propose a novel end-to-end residual\nlearning framework, which formulates the depth completion as a two-stage\nlearning task, i.e., a sparse-to-coarse stage and a coarse-to-fine stage.\nFirst, a coarse dense depth map is obtained by a simple CNN framework. Then, a\nrefined depth map is further obtained using a residual learning strategy in the\ncoarse-to-fine stage with a coarse depth map and color image as input.\nSpecially, in the coarse-to-fine stage, a channel shuffle extraction operation\nis utilized to extract more representative features from the color image and\ncoarse depth map, and an energy based fusion operation is exploited to\neffectively fuse these features obtained by channel shuffle operation, thus\nleading to more accurate and refined depth maps. We achieve SoTA performance in\nRMSE on KITTI benchmark. Extensive experiments on other datasets future\ndemonstrate the superiority of our approach over current state-of-the-art depth\ncompletion approaches.",
          "link": "http://arxiv.org/abs/2012.08270",
          "publishedOn": "2021-08-18T01:55:02.269Z",
          "wordCount": 694,
          "title": "FCFR-Net: Feature Fusion based Coarse-to-Fine Residual Learning for Depth Completion. (arXiv:2012.08270v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagelskjaer_F/0/1/0/all/0/1\">Frederik Hagelskjaer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_A/0/1/0/all/0/1\">Anders Glent Buch</a>",
          "description": "Since the introduction of modern deep learning methods for object pose\nestimation, test accuracy and efficiency has increased significantly. For\ntraining, however, large amounts of annotated training data are required for\ngood performance. While the use of synthetic training data prevents the need\nfor manual annotation, there is currently a large performance gap between\nmethods trained on real and synthetic data. This paper introduces a new method,\nwhich bridges this gap.\n\nMost methods trained on synthetic data use 2D images, as domain randomization\nin 2D is more developed. To obtain precise poses, many of these methods perform\na final refinement using 3D data. Our method integrates the 3D data into the\nnetwork to increase the accuracy of the pose estimation. To allow for domain\nrandomization in 3D, a sensor-based data augmentation has been developed.\nAdditionally, we introduce the SparseEdge feature, which uses a wider search\nspace during point cloud propagation to avoid relying on specific features\nwithout increasing run-time.\n\nExperiments on three large pose estimation benchmarks show that the presented\nmethod outperforms previous methods trained on synthetic data and achieves\ncomparable results to existing methods trained on real data.",
          "link": "http://arxiv.org/abs/2011.08517",
          "publishedOn": "2021-08-18T01:55:02.262Z",
          "wordCount": 673,
          "title": "Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization. (arXiv:2011.08517v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1\">Timothee Masquelier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have attracted enormous research interest due\nto temporal information processing capability, low power consumption, and high\nbiological plausibility. However, the formulation of efficient and\nhigh-performance learning algorithms for SNNs is still challenging. Most\nexisting learning methods learn weights only, and require manual tuning of the\nmembrane-related parameters that determine the dynamics of a single spiking\nneuron. These parameters are typically chosen to be the same for all neurons,\nwhich limits the diversity of neurons and thus the expressiveness of the\nresulting SNNs. In this paper, we take inspiration from the observation that\nmembrane-related parameters are different across brain regions, and propose a\ntraining algorithm that is capable of learning not only the synaptic weights\nbut also the membrane time constants of SNNs. We show that incorporating\nlearnable membrane time constants can make the network less sensitive to\ninitial values and can speed up learning. In addition, we reevaluate the\npooling methods in SNNs and find that max-pooling will not lead to significant\ninformation loss and have the advantage of low computation cost and binary\ncompatibility. We evaluate the proposed method for image classification tasks\non both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and\nneuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment\nresults show that the proposed method outperforms the state-of-the-art accuracy\non nearly all datasets, using fewer time-steps. Our codes are available at\nhttps://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.",
          "link": "http://arxiv.org/abs/2007.05785",
          "publishedOn": "2021-08-18T01:55:02.243Z",
          "wordCount": 750,
          "title": "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks. (arXiv:2007.05785v5 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "In this paper, we present a conceptually simple, strong, and efficient\nframework for fully- and weakly-supervised panoptic segmentation, called\nPanoptic FCN. Our approach aims to represent and predict foreground things and\nbackground stuff in a unified fully convolutional pipeline, which can be\noptimized with point-based fully or weak supervision. In particular, Panoptic\nFCN encodes each object instance or stuff category with the proposed kernel\ngenerator and produces the prediction by convolving the high-resolution feature\ndirectly. With this approach, instance-aware and semantically consistent\nproperties for things and stuff can be respectively satisfied in a simple\ngenerate-kernel-then-segment workflow. Without extra boxes for localization or\ninstance separation, the proposed approach outperforms the previous box-based\nand -free models with high efficiency. Furthermore, we propose a new form of\npoint-based annotation for weakly-supervised panoptic segmentation. It only\nneeds several random points for both things and stuff, which dramatically\nreduces the annotation cost of human. The proposed Panoptic FCN is also proved\nto have much superior performance in this weakly-supervised setting, which\nachieves 82% of the fully-supervised performance with only 20 randomly\nannotated points per instance. Extensive experiments demonstrate the\neffectiveness and efficiency of Panoptic FCN on COCO, VOC 2012, Cityscapes, and\nMapillary Vistas datasets. And it sets up a new leading benchmark for both\nfully- and weakly-supervised panoptic segmentation. Our code and models are\nmade publicly available at https://github.com/dvlab-research/PanopticFCN",
          "link": "http://arxiv.org/abs/2108.07682",
          "publishedOn": "2021-08-18T01:55:02.236Z",
          "wordCount": 683,
          "title": "Fully Convolutional Networks for Panoptic Segmentation with Point-based Supervision. (arXiv:2108.07682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.15528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chunlong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianqi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yufan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fuhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1\">Jie Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>",
          "description": "Chromosome classification is an important but difficult and tedious task in\nkaryotyping. Previous methods only classify manually segmented single\nchromosome, which is far from clinical practice. In this work, we propose a\ndetection based method, DeepACC, to locate and fine classify chromosomes\nsimultaneously based on the whole metaphase image. We firstly introduce the\nAdditive Angular Margin Loss to enhance the discriminative power of model. To\nalleviate batch effects, we transform decision boundary of each class\ncase-by-case through a siamese network which make full use of prior knowledges\nthat chromosomes usually appear in pairs. Furthermore, we take the clinically\nseven group criterion as a prior knowledge and design an additional Group\nInner-Adjacency Loss to further reduce inter-class similarities. 3390 metaphase\nimages from clinical laboratory are collected and labelled to evaluate the\nperformance. Results show that the new design brings encouraging performance\ngains comparing to the state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2006.15528",
          "publishedOn": "2021-08-18T01:55:02.227Z",
          "wordCount": 692,
          "title": "DeepACC:Automate Chromosome Classification based on Metaphase Images using Deep Learning Framework Fused with Prior Knowledge. (arXiv:2006.15528v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jiwan Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>",
          "description": "The natural association between visual observations and their corresponding\nsound provides powerful self-supervisory signals for learning video\nrepresentations, which makes the ever-growing amount of online videos an\nattractive source of training data. However, large portions of online videos\ncontain irrelevant audio-visual signals because of edited/overdubbed audio, and\nmodels trained on such uncurated videos have shown to learn suboptimal\nrepresentations. Therefore, existing approaches rely almost exclusively on\ndatasets with predetermined taxonomies of semantic concepts, where there is a\nhigh chance of audio-visual correspondence. Unfortunately, constructing such\ndatasets require labor intensive manual annotation and/or verification, which\nseverely limits the utility of online videos for large-scale learning. In this\nwork, we present an automatic dataset curation approach based on subset\noptimization where the objective is to maximize the mutual information between\naudio and visual channels in videos. We demonstrate that our approach finds\nvideos with high audio-visual correspondence and show that self-supervised\nmodels trained on our data achieve competitive performances compared to models\ntrained on existing manually curated datasets. The most significant benefit of\nour approach is scalability: We release ACAV100M that contains 100 million\nvideos with high audio-visual correspondence, ideal for self-supervised video\nrepresentation learning.",
          "link": "http://arxiv.org/abs/2101.10803",
          "publishedOn": "2021-08-18T01:55:02.220Z",
          "wordCount": 685,
          "title": "ACAV100M: Automatic Curation of Large-Scale Datasets for Audio-Visual Video Representation Learning. (arXiv:2101.10803v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chengjian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_M/0/1/0/all/0/1\">Matthew R. Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>",
          "description": "One-stage object detection is commonly implemented by optimizing two\nsub-tasks: object classification and localization, using heads with two\nparallel branches, which might lead to a certain level of spatial misalignment\nin predictions between the two tasks. In this work, we propose a Task-aligned\nOne-stage Object Detection (TOOD) that explicitly aligns the two tasks in a\nlearning-based manner. First, we design a novel Task-aligned Head (T-Head)\nwhich offers a better balance between learning task-interactive and\ntask-specific features, as well as a greater flexibility to learn the alignment\nvia a task-aligned predictor. Second, we propose Task Alignment Learning (TAL)\nto explicitly pull closer (or even unify) the optimal anchors for the two tasks\nduring training via a designed sample assignment scheme and a task-aligned\nloss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a\n51.1 AP at single-model single-scale testing. This surpasses the recent\none-stage detectors by a large margin, such as ATSS (47.7 AP), GFL (48.2 AP),\nand PAA (49.0 AP), with fewer parameters and FLOPs. Qualitative results also\ndemonstrate the effectiveness of TOOD for better aligning the tasks of object\nclassification and localization. Code is available at\nhttps://github.com/fcjian/TOOD.",
          "link": "http://arxiv.org/abs/2108.07755",
          "publishedOn": "2021-08-18T01:55:02.212Z",
          "wordCount": 624,
          "title": "TOOD: Task-aligned One-stage Object Detection. (arXiv:2108.07755v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Video-based person re-identification (re-ID) aims at matching the same person\nacross video clips. Efficiently exploiting multi-scale fine-grained features\nwhile building the structural interaction among them is pivotal for its\nsuccess. In this paper, we propose a hybrid framework, Dense Interaction\nLearning (DenseIL), that takes the principal advantages of both CNN-based and\nAttention-based architectures to tackle video-based person re-ID difficulties.\nDenseIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN\nencoder is responsible for efficiently extracting discriminative spatial\nfeatures while the DI decoder is designed to densely model spatial-temporal\ninherent interaction across frames. Different from previous works, we\nadditionally let the DI decoder densely attends to intermediate fine-grained\nCNN features and that naturally yields multi-grained spatial-temporal\nrepresentation for each video clip. Moreover, we introduce Spatio-TEmporal\nPositional Embedding (STEP-Emb) into the DI decoder to investigate the\npositional relation among the spatial-temporal inputs. Our experiments\nconsistently and significantly outperform all the state-of-the-art methods on\nmultiple standard video-based person re-ID datasets.",
          "link": "http://arxiv.org/abs/2103.09013",
          "publishedOn": "2021-08-18T01:55:02.193Z",
          "wordCount": 640,
          "title": "Dense Interaction Learning for Video-based Person Re-identification. (arXiv:2103.09013v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Abdullah Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Multi-view projection methods have demonstrated their ability to reach\nstate-of-the-art performance on 3D shape recognition. Those methods learn\ndifferent ways to aggregate information from multiple views. However, the\ncamera view-points for those views tend to be heuristically set and fixed for\nall shapes. To circumvent the lack of dynamism of current multi-view methods,\nwe propose to learn those view-points. In particular, we introduce the\nMulti-View Transformation Network (MVTN) that regresses optimal view-points for\n3D shape recognition, building upon advances in differentiable rendering. As a\nresult, MVTN can be trained end-to-end along with any multi-view network for 3D\nshape classification. We integrate MVTN in a novel adaptive multi-view pipeline\nthat can render either 3D meshes or point clouds. MVTN exhibits clear\nperformance gains in the tasks of 3D shape classification and 3D shape\nretrieval without the need for extra training supervision. In these tasks, MVTN\nachieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the\nmost recent and realistic ScanObjectNN dataset (up to 6% improvement).\nInterestingly, we also show that MVTN can provide network robustness against\nrotation and occlusion in the 3D domain. The code is available at\nhttps://github.com/ajhamdi/MVTN .",
          "link": "http://arxiv.org/abs/2011.13244",
          "publishedOn": "2021-08-18T01:55:02.186Z",
          "wordCount": 675,
          "title": "MVTN: Multi-View Transformation Network for 3D Shape Recognition. (arXiv:2011.13244v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Assessing action quality is challenging due to the subtle differences between\nvideos and large variations in scores. Most existing approaches tackle this\nproblem by regressing a quality score from a single video, suffering a lot from\nthe large inter-video score variations. In this paper, we show that the\nrelations among videos can provide important clues for more accurate action\nquality assessment during both training and inference. Specifically, we\nreformulate the problem of action quality assessment as regressing the relative\nscores with reference to another video that has shared attributes (e.g.,\ncategory and difficulty), instead of learning unreferenced scores. Following\nthis formulation, we propose a new Contrastive Regression (CoRe) framework to\nlearn the relative scores by pair-wise comparison, which highlights the\ndifferences between videos and guides the models to learn the key hints for\nassessment. In order to further exploit the relative information between two\nvideos, we devise a group-aware regression tree to convert the conventional\nscore regression into two easier sub-problems: coarse-to-fine classification\nand regression in small intervals. To demonstrate the effectiveness of CoRe, we\nconduct extensive experiments on three mainstream AQA datasets including AQA-7,\nMTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large\nmargin and establishes new state-of-the-art on all three benchmarks.",
          "link": "http://arxiv.org/abs/2108.07797",
          "publishedOn": "2021-08-18T01:55:02.178Z",
          "wordCount": 654,
          "title": "Group-aware Contrastive Regression for Action Quality Assessment. (arXiv:2108.07797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schutte_A/0/1/0/all/0/1\">August DuMont Sch&#xfc;tte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hetzel_J/0/1/0/all/0/1\">J&#xfc;rgen Hetzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hepp_T/0/1/0/all/0/1\">Tobias Hepp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dietz_B/0/1/0/all/0/1\">Benedikt Dietz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwab_P/0/1/0/all/0/1\">Patrick Schwab</a>",
          "description": "Privacy concerns around sharing personally identifiable information are a\nmajor practical barrier to data sharing in medical research. However, in many\ncases, researchers have no interest in a particular individual's information\nbut rather aim to derive insights at the level of cohorts. Here, we utilize\nGenerative Adversarial Networks (GANs) to create derived medical imaging\ndatasets consisting entirely of synthetic patient data. The synthetic images\nideally have, in aggregate, similar statistical properties to those of a source\ndataset but do not contain sensitive personal information. We assess the\nquality of synthetic data generated by two GAN models for chest radiographs\nwith 14 different radiology findings and brain computed tomography (CT) scans\nwith six types of intracranial hemorrhages. We measure the synthetic image\nquality by the performance difference of predictive models trained on either\nthe synthetic or the real dataset. We find that synthetic data performance\ndisproportionately benefits from a reduced number of unique label combinations.\nOur open-source benchmark also indicates that at low number of samples per\nclass, label overfitting effects start to dominate GAN training. We\nadditionally conducted a reader study in which trained radiologists do not\nperform better than random on discriminating between synthetic and real medical\nimages for intermediate levels of resolutions. In accordance with our benchmark\nresults, the classification accuracy of radiologists increases at higher\nspatial resolution levels. Our study offers valuable guidelines and outlines\npractical conditions under which insights derived from synthetic medical images\nare similar to those that would have been derived from real imaging data. Our\nresults indicate that synthetic data sharing may be an attractive and\nprivacy-preserving alternative to sharing real patient-level data in the right\nsettings.",
          "link": "http://arxiv.org/abs/2012.03769",
          "publishedOn": "2021-08-18T01:55:02.171Z",
          "wordCount": 761,
          "title": "Overcoming Barriers to Data Sharing with Medical Image Generation: A Comprehensive Evaluation. (arXiv:2012.03769v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Benlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "3D point cloud understanding has made great progress in recent years.\nHowever, one major bottleneck is the scarcity of annotated real datasets,\nespecially compared to 2D object detection tasks, since a large amount of labor\nis involved in annotating the real scans of a scene. A promising solution to\nthis problem is to make better use of the synthetic dataset, which consists of\nCAD object models, to boost the learning on real datasets. This can be achieved\nby the pre-training and fine-tuning procedure. However, recent work on 3D\npre-training exhibits failure when transfer features learned on synthetic\nobjects to other real-world applications. In this work, we put forward a new\nmethod called RandomRooms to accomplish this objective. In particular, we\npropose to generate random layouts of a scene by making use of the objects in\nthe synthetic CAD dataset and learn the 3D scene representation by applying\nobject-level contrastive learning on two random scenes generated from the same\nset of synthetic objects. The model pre-trained in this way can serve as a\nbetter initialization when later fine-tuning on the 3D object detection task.\nEmpirically, we show consistent improvement in downstream 3D detection tasks on\nseveral base models, especially when less training data are used, which\nstrongly demonstrates the effectiveness and generalization of our method.\nBenefiting from the rich semantic knowledge and diverse objects from synthetic\ndata, our method establishes the new state-of-the-art on widely-used 3D\ndetection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide\na new perspective for bridging object and scene-level 3D understanding.",
          "link": "http://arxiv.org/abs/2108.07794",
          "publishedOn": "2021-08-18T01:55:02.163Z",
          "wordCount": 720,
          "title": "RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection. (arXiv:2108.07794v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_L/0/1/0/all/0/1\">Louis Gallagher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John B. McDonald</a>",
          "description": "In this paper, we present a system for incrementally reconstructing a dense\n3D model of the geometry of an outdoor environment using a single monocular\ncamera attached to a moving vehicle. Dense models provide a rich representation\nof the environment facilitating higher-level scene understanding, perception,\nand planning. Our system employs dense depth prediction with a hybrid mapping\narchitecture combining state-of-the-art sparse features and dense fusion-based\nvisual SLAM algorithms within an integrated framework. Our novel contributions\ninclude design of hybrid sparse-dense camera tracking and loop closure, and\nscale estimation improvements in dense depth prediction. We use the motion\nestimates from the sparse method to overcome the large and variable inter-frame\ndisplacement typical of outdoor vehicle scenarios. Our system then registers\nthe live image with the dense model using whole-image alignment. This enables\nthe fusion of the live frame and dense depth prediction into the model. Global\nconsistency and alignment between the sparse and dense models are achieved by\napplying pose constraints from the sparse method directly within the\ndeformation of the dense model. We provide qualitative and quantitative results\nfor both trajectory estimation and surface reconstruction accuracy,\ndemonstrating competitive performance on the KITTI dataset. Qualitative results\nof the proposed approach are illustrated in https://youtu.be/Pn2uaVqjskY.\nSource code for the project is publicly available at the following repository\nhttps://github.com/robotvisionmu/DenseMonoSLAM.",
          "link": "http://arxiv.org/abs/2108.07736",
          "publishedOn": "2021-08-18T01:55:02.142Z",
          "wordCount": 683,
          "title": "A Hybrid Sparse-Dense Monocular SLAM System for Autonomous Driving. (arXiv:2108.07736v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lina Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xibin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "Remarkable results have been achieved by DCNN based self-supervised depth\nestimation approaches. However, most of these approaches can only handle either\nday-time or night-time images, while their performance degrades for all-day\nimages due to large domain shift and the variation of illumination between day\nand night images. To relieve these limitations, we propose a domain-separated\nnetwork for self-supervised depth estimation of all-day images. Specifically,\nto relieve the negative influence of disturbing terms (illumination, etc.), we\npartition the information of day and night image pairs into two complementary\nsub-spaces: private and invariant domains, where the former contains the unique\ninformation (illumination, etc.) of day and night images and the latter\ncontains essential shared information (texture, etc.). Meanwhile, to guarantee\nthat the day and night images contain the same information, the\ndomain-separated network takes the day-time images and corresponding night-time\nimages (generated by GAN) as input, and the private and invariant feature\nextractors are learned by orthogonality and similarity loss, where the domain\ngap can be alleviated, thus better depth maps can be expected. Meanwhile, the\nreconstruction and photometric losses are utilized to estimate complementary\ninformation and depth maps effectively. Experimental results demonstrate that\nour approach achieves state-of-the-art depth estimation results for all-day\nimages on the challenging Oxford RobotCar dataset, proving the superiority of\nour proposed approach.",
          "link": "http://arxiv.org/abs/2108.07628",
          "publishedOn": "2021-08-18T01:55:02.021Z",
          "wordCount": 664,
          "title": "Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation. (arXiv:2108.07628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuxiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yupeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>",
          "description": "Unsupervised disentanglement learning is a crucial issue for understanding\nand exploiting deep generative models. Recently, SeFa tries to find latent\ndisentangled directions by performing SVD on the first projection of a\npre-trained GAN. However, it is only applied to the first layer and works in a\npost-processing way. Hessian Penalty minimizes the off-diagonal entries of the\noutput's Hessian matrix to facilitate disentanglement, and can be applied to\nmulti-layers.However, it constrains each entry of output independently, making\nit not sufficient in disentangling the latent directions (e.g., shape, size,\nrotation, etc.) of spatially correlated variations. In this paper, we propose a\nsimple Orthogonal Jacobian Regularization (OroJaR) to encourage deep generative\nmodel to learn disentangled representations. It simply encourages the variation\nof output caused by perturbations on different latent dimensions to be\northogonal, and the Jacobian with respect to the input is calculated to\nrepresent this variation. We show that our OroJaR also encourages the output's\nHessian matrix to be diagonal in an indirect manner. In contrast to the Hessian\nPenalty, our OroJaR constrains the output in a holistic way, making it very\neffective in disentangling latent dimensions corresponding to spatially\ncorrelated variations. Quantitative and qualitative experimental results show\nthat our method is effective in disentangled and controllable image generation,\nand performs favorably against the state-of-the-art methods. Our code is\navailable at https://github.com/csyxwei/OroJaR",
          "link": "http://arxiv.org/abs/2108.07668",
          "publishedOn": "2021-08-18T01:55:02.014Z",
          "wordCount": 676,
          "title": "Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation. (arXiv:2108.07668v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07739",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>",
          "description": "The study of 3D hyperspectral image (HSI) reconstruction refers to the\ninverse process of snapshot compressive imaging, during which the optical\nsystem, e.g., the coded aperture snapshot spectral imaging (CASSI) system,\ncaptures the 3D spatial-spectral signal and encodes it to a 2D measurement.\nWhile numerous sophisticated neural networks have been elaborated for\nend-to-end reconstruction, trade-offs still need to be made among performance,\nefficiency (training and inference time), and feasibility (the ability of\nrestoring high resolution HSI on limited GPU memory). This raises a challenge\nto design a new baseline to conjointly meet the above requirements. In this\npaper, we fill in this blank by proposing a Spatial/Spectral Invariant Residual\nU-Net, namely SSI-ResU-Net. It differentiates with U-Net in three folds--1)\nscale/spectral-invariant learning, 2) nested residual learning, and 3)\ncomputational efficiency. Benefiting from these three modules, the proposed\nSSI-ResU-Net outperforms the current state-of-the-art method TSA-Net by over 3\ndB in PSNR and 0.036 in SSIM while only using 2.82% trainable parameters. To\nthe greatest extent, SSI-ResU-Net achieves competing performance with over\n77.3% reduction in terms of floating-point operations (FLOPs), which for the\nfirst time, makes high-resolution HSI reconstruction feasible under practical\napplication scenarios. Code and pre-trained models are made available at\nhttps://github.com/Jiamian-Wang/HSI_baseline.",
          "link": "http://arxiv.org/abs/2108.07739",
          "publishedOn": "2021-08-18T01:55:01.920Z",
          "wordCount": 647,
          "title": "A New Backbone for Hyperspectral Image Reconstruction. (arXiv:2108.07739v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">You Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_H/0/1/0/all/0/1\">Hee-Soo Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_S/0/1/0/all/0/1\">Soyeon Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Soo-Whan Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yoohwan Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bong-Jin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Youngki Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>",
          "description": "In this work, we present a novel audio-visual dataset for active speaker\ndetection in the wild. A speaker is considered active when his or her face is\nvisible and the voice is audible simultaneously. Although active speaker\ndetection is a crucial pre-processing step for many audio-visual tasks, there\nis no existing dataset of natural human speech to evaluate the performance of\nactive speaker detection. We therefore curate the Active Speakers in the Wild\n(ASW) dataset which contains videos and co-occurring speech segments with dense\nspeech activity labels. Videos and timestamps of audible segments are parsed\nand adopted from VoxConverse, an existing speaker diarisation dataset that\nconsists of videos in the wild. Face tracks are extracted from the videos and\nactive segments are annotated based on the timestamps of VoxConverse in a\nsemi-automatic way. Two reference systems, a self-supervised system and a fully\nsupervised one, are evaluated on the dataset to provide the baseline\nperformances of ASW. Cross-domain evaluation is conducted in order to show the\nnegative effect of dubbed videos in the training data.",
          "link": "http://arxiv.org/abs/2108.07640",
          "publishedOn": "2021-08-18T01:55:01.829Z",
          "wordCount": 653,
          "title": "Look Who's Talking: Active Speaker Detection in the Wild. (arXiv:2108.07640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Deng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhihua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangmiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "We study self-supervised video representation learning, which is a\nchallenging task due to 1) lack of labels for explicit supervision; 2)\nunstructured and noisy visual information. Existing methods mainly use\ncontrastive loss with video clips as the instances and learn visual\nrepresentation by discriminating instances from each other, but they need a\ncareful treatment of negative pairs by either relying on large batch sizes,\nmemory banks, extra modalities or customized mining strategies, which\ninevitably includes noisy data. In this paper, we observe that the consistency\nbetween positive samples is the key to learn robust video representation.\nSpecifically, we propose two tasks to learn the appearance and speed\nconsistency, respectively. The appearance consistency task aims to maximize the\nsimilarity between two clips of the same video with different playback speeds.\nThe speed consistency task aims to maximize the similarity between two clips\nwith the same playback speed but different appearance information. We show that\noptimizing the two tasks jointly consistently improves the performance on\ndownstream tasks, e.g., action recognition and video retrieval. Remarkably, for\naction recognition on the UCF-101 dataset, we achieve 90.8\\% accuracy without\nusing any extra modalities or negative pairs for unsupervised pretraining,\nwhich outperforms the ImageNet supervised pretrained model. Codes and models\nwill be available.",
          "link": "http://arxiv.org/abs/2106.02342",
          "publishedOn": "2021-08-18T01:55:01.729Z",
          "wordCount": 686,
          "title": "ASCNet: Self-supervised Video Representation Learning with Appearance-Speed Consistency. (arXiv:2106.02342v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatzianastasis_M/0/1/0/all/0/1\">Michail Chatzianastasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasoulas_G/0/1/0/all/0/1\">George Dasoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siolas_G/0/1/0/all/0/1\">Georgios Siolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>",
          "description": "Neural Architecture Search (NAS) has recently gained increased attention, as\na class of approaches that automatically searches in an input space of network\narchitectures. A crucial part of the NAS pipeline is the encoding of the\narchitecture that consists of the applied computational blocks, namely the\noperations and the links between them. Most of the existing approaches either\nfail to capture the structural properties of the architectures or use\nhand-engineered vector to encode the operator information. In this paper, we\npropose the replacement of fixed operator encoding with learnable\nrepresentations in the optimization process. This approach, which effectively\ncaptures the relations of different operations, leads to smoother and more\naccurate representations of the architectures and consequently to improved\nperformance of the end task. Our extensive evaluation in ENAS benchmark\ndemonstrates the effectiveness of the proposed operation embeddings to the\ngeneration of highly accurate models, achieving state-of-the-art performance.\nFinally, our method produces top-performing architectures that share similar\noperation and graph patterns, highlighting a strong correlation between the\nstructural properties of the architecture and its performance.",
          "link": "http://arxiv.org/abs/2105.04885",
          "publishedOn": "2021-08-18T01:55:01.722Z",
          "wordCount": 645,
          "title": "Graph-based Neural Architecture Search with Operation Embeddings. (arXiv:2105.04885v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr S. Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelkader_A/0/1/0/all/0/1\">Ali Abdelkader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anany_M/0/1/0/all/0/1\">Mohamed Anany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Behady_O/0/1/0/all/0/1\">Omar El-Behady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisal_M/0/1/0/all/0/1\">Muhammad Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hangal_A/0/1/0/all/0/1\">Asser Hangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1\">Hesham M. Eraqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustafa_M/0/1/0/all/0/1\">Mohamed N. Moustafa</a>",
          "description": "LiDARs and cameras are the two main sensors that are planned to be included\nin many announced autonomous vehicles prototypes. Each of the two provides a\nunique form of data from a different perspective to the surrounding\nenvironment. In this paper, we explore and attempt to answer the question: is\nthere an added benefit by fusing those two forms of data for the purpose of\nsemantic segmentation within the context of autonomous driving? We also attempt\nto show at which level does said fusion prove to be the most useful. We\nevaluated our algorithms on the publicly available SemanticKITTI dataset. All\nfusion models show improvements over the base model, with the mid-level fusion\nshowing the highest improvement of 2.7% in terms of mean Intersection over\nUnion (mIoU) metric.",
          "link": "http://arxiv.org/abs/2108.07661",
          "publishedOn": "2021-08-18T01:55:01.716Z",
          "wordCount": 579,
          "title": "An Evaluation of RGB and LiDAR Fusion for Semantic Segmentation. (arXiv:2108.07661v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07595",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Horgan_C/0/1/0/all/0/1\">Conor C. Horgan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bergholt_M/0/1/0/all/0/1\">Mads S. Bergholt</a>",
          "description": "Deep learning computer vision techniques have achieved many successes in\nrecent years across numerous imaging domains. However, the application of deep\nlearning to spectral data remains a complex task due to the need for\naugmentation routines, specific architectures for spectral data, and\nsignificant memory requirements. Here we present spectrai, an open-source deep\nlearning framework designed to facilitate the training of neural networks on\nspectral data and enable comparison between different methods. Spectrai\nprovides numerous built-in spectral data pre-processing and augmentation\nmethods, neural networks for spectral data including spectral (image)\ndenoising, spectral (image) classification, spectral image segmentation, and\nspectral image super-resolution. Spectrai includes both command line and\ngraphical user interfaces (GUI) designed to guide users through model and\nhyperparameter decisions for a wide range of applications.",
          "link": "http://arxiv.org/abs/2108.07595",
          "publishedOn": "2021-08-18T01:55:01.710Z",
          "wordCount": 566,
          "title": "spectrai: A deep learning framework for spectral data. (arXiv:2108.07595v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zejia Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>",
          "description": "Vision Transformer (ViT) attains state-of-the-art performance in visual\nrecognition, and the variant, Local Vision Transformer, makes further\nimprovements. The major component in Local Vision Transformer, local attention,\nperforms the attention separately over small local windows. We rephrase local\nattention as a channel-wise locally-connected layer and analyze it from two\nnetwork regularization manners, sparse connectivity and weight sharing, as well\nas weight computation. Sparse connectivity: there is no connection across\nchannels, and each position is connected to the positions within a small local\nwindow. Weight sharing: the connection weights for one position are shared\nacross channels or within each group of channels. Dynamic weight: the\nconnection weights are dynamically predicted according to each image instance.\nWe point out that local attention resembles depth-wise convolution and its\ndynamic version in sparse connectivity. The main difference lies in weight\nsharing - depth-wise convolution shares connection weights (kernel weights)\nacross spatial positions. We empirically observe that the models based on\ndepth-wise convolution and the dynamic variant with lower computation\ncomplexity perform on-par with or sometimes slightly better than Swin\nTransformer, an instance of Local Vision Transformer, for ImageNet\nclassification, COCO object detection and ADE semantic segmentation. These\nobservations suggest that Local Vision Transformer takes advantage of two\nregularization forms and dynamic weight to increase the network capacity.",
          "link": "http://arxiv.org/abs/2106.04263",
          "publishedOn": "2021-08-18T01:55:01.704Z",
          "wordCount": 691,
          "title": "Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight. (arXiv:2106.04263v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chengjian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>",
          "description": "The conventional detectors tend to make imbalanced classification and suffer\nperformance drop, when the distribution of the training data is severely\nskewed. In this paper, we propose to use the mean classification score to\nindicate the classification accuracy for each category during training. Based\non this indicator, we balance the classification via an Equilibrium Loss (EBL)\nand a Memory-augmented Feature Sampling (MFS) method. Specifically, EBL\nincreases the intensity of the adjustment of the decision boundary for the weak\nclasses by a designed score-guided loss margin between any two classes. On the\nother hand, MFS improves the frequency and accuracy of the adjustment of the\ndecision boundary for the weak classes through over-sampling the instance\nfeatures of those classes. Therefore, EBL and MFS work collaboratively for\nfinding the classification equilibrium in long-tailed detection, and\ndramatically improve the performance of tail classes while maintaining or even\nimproving the performance of head classes. We conduct experiments on LVIS using\nMask R-CNN with various backbones including ResNet-50-FPN and ResNet-101-FPN to\nshow the superiority of the proposed method. It improves the detection\nperformance of tail classes by 15.6 AP, and outperforms the most recent\nlong-tailed object detectors by more than 1 AP. Code is available at\nhttps://github.com/fcjian/LOCE.",
          "link": "http://arxiv.org/abs/2108.07507",
          "publishedOn": "2021-08-18T01:55:01.685Z",
          "wordCount": 636,
          "title": "Exploring Classification Equilibrium in Long-Tailed Object Detection. (arXiv:2108.07507v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1\">Alasdair Newson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1\">Yann Gousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>",
          "description": "High quality facial image editing is a challenging problem in the movie\npost-production industry, requiring a high degree of control and identity\npreservation. Previous works that attempt to tackle this problem may suffer\nfrom the entanglement of facial attributes and the loss of the person's\nidentity. Furthermore, many algorithms are limited to a certain task. To tackle\nthese limitations, we propose to edit facial attributes via the latent space of\na StyleGAN generator, by training a dedicated latent transformation network and\nincorporating explicit disentanglement and identity preservation terms in the\nloss function. We further introduce a pipeline to generalize our face editing\nto videos. Our model achieves a disentangled, controllable, and\nidentity-preserving facial attribute editing, even in the challenging case of\nreal (i.e., non-synthetic) images and videos. We conduct extensive experiments\non image and video datasets and show that our model outperforms other\nstate-of-the-art methods in visual quality and quantitative evaluation. Source\ncodes are available at https://github.com/InterDigitalInc/latent-transformer.",
          "link": "http://arxiv.org/abs/2106.11895",
          "publishedOn": "2021-08-18T01:55:01.678Z",
          "wordCount": 641,
          "title": "A Latent Transformer for Disentangled Face Editing in Images and Videos. (arXiv:2106.11895v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Nanxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>",
          "description": "Various deep learning techniques have been proposed to solve the single-view\n2D-to-3D pose estimation problem. While the average prediction accuracy has\nbeen improved significantly over the years, the performance on hard poses with\ndepth ambiguity, self-occlusion, and complex or rare poses is still far from\nsatisfactory. In this work, we target these hard poses and present a novel\nskeletal GNN learning solution. To be specific, we propose a hop-aware\nhierarchical channel-squeezing fusion layer to effectively extract relevant\ninformation from neighboring nodes while suppressing undesired noises in GNN\nlearning. In addition, we propose a temporal-aware dynamic graph construction\nprocedure that is robust and effective for 3D pose estimation. Experimental\nresults on the Human3.6M dataset show that our solution achieves 10.3\\% average\nprediction accuracy improvement and greatly improves on hard poses over\nstate-of-the-art techniques. We further apply the proposed technique on the\nskeleton-based action recognition task and also achieve state-of-the-art\nperformance. Our code is available at\nhttps://github.com/ailingzengzzz/Skeletal-GNN.",
          "link": "http://arxiv.org/abs/2108.07181",
          "publishedOn": "2021-08-18T01:55:01.672Z",
          "wordCount": 621,
          "title": "Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation. (arXiv:2108.07181v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Alex Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruilong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tancik_M/0/1/0/all/0/1\">Matthew Tancik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_R/0/1/0/all/0/1\">Ren Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>",
          "description": "We introduce a method to render Neural Radiance Fields (NeRFs) in real time\nusing PlenOctrees, an octree-based 3D representation which supports\nview-dependent effects. Our method can render 800x800 images at more than 150\nFPS, which is over 3000 times faster than conventional NeRFs. We do so without\nsacrificing quality while preserving the ability of NeRFs to perform\nfree-viewpoint rendering of scenes with arbitrary geometry and view-dependent\neffects. Real-time performance is achieved by pre-tabulating the NeRF into a\nPlenOctree. In order to preserve view-dependent effects such as specularities,\nwe factorize the appearance via closed-form spherical basis functions.\nSpecifically, we show that it is possible to train NeRFs to predict a spherical\nharmonic representation of radiance, removing the viewing direction as an input\nto the neural network. Furthermore, we show that PlenOctrees can be directly\noptimized to further minimize the reconstruction loss, which leads to equal or\nbetter quality compared to competing methods. Moreover, this octree\noptimization step can be used to reduce the training time, as we no longer need\nto wait for the NeRF training to converge fully. Our real-time neural rendering\napproach may potentially enable new applications such as 6-DOF industrial and\nproduct visualizations, as well as next generation AR/VR systems. PlenOctrees\nare amenable to in-browser rendering as well; please visit the project page for\nthe interactive online demo, as well as video and code:\nhttps://alexyu.net/plenoctrees",
          "link": "http://arxiv.org/abs/2103.14024",
          "publishedOn": "2021-08-18T01:55:01.666Z",
          "wordCount": 705,
          "title": "PlenOctrees for Real-time Rendering of Neural Radiance Fields. (arXiv:2103.14024v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neimark_D/0/1/0/all/0/1\">Daniel Neimark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_O/0/1/0/all/0/1\">Omri Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zohar_M/0/1/0/all/0/1\">Maya Zohar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asselmann_D/0/1/0/all/0/1\">Dotan Asselmann</a>",
          "description": "This paper presents VTN, a transformer-based framework for video recognition.\nInspired by recent developments in vision transformers, we ditch the standard\napproach in video action recognition that relies on 3D ConvNets and introduce a\nmethod that classifies actions by attending to the entire video sequence\ninformation. Our approach is generic and builds on top of any given 2D spatial\nnetwork. In terms of wall runtime, it trains $16.1\\times$ faster and runs\n$5.1\\times$ faster during inference while maintaining competitive accuracy\ncompared to other state-of-the-art methods. It enables whole video analysis,\nvia a single end-to-end pass, while requiring $1.5\\times$ fewer GFLOPs. We\nreport competitive results on Kinetics-400 and present an ablation study of VTN\nproperties and the trade-off between accuracy and inference speed. We hope our\napproach will serve as a new baseline and start a fresh line of research in the\nvideo recognition domain. Code and models are available at:\nhttps://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md",
          "link": "http://arxiv.org/abs/2102.00719",
          "publishedOn": "2021-08-18T01:55:01.658Z",
          "wordCount": 613,
          "title": "Video Transformer Network. (arXiv:2102.00719v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jinyung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>",
          "description": "Fruit flies are established model systems for studying olfactory learning as\nthey will readily learn to associate odors with both electric shock or sugar\nrewards. The mechanisms of the insect brain apparently responsible for odor\nlearning form a relatively shallow neuronal architecture. Olfactory inputs are\nreceived by the antennal lobe (AL) of the brain, which produces an encoding of\neach odor mixture across ~50 sub-units known as glomeruli. Each of these\nglomeruli then project its component of this feature vector to several of ~2000\nso-called Kenyon Cells (KCs) in a region of the brain known as the mushroom\nbody (MB). Fly responses to odors are generated by small downstream neuropils\nthat decode the higher-order representation from the MB. Research has shown\nthat there is no recognizable pattern in the glomeruli--KC connections (and\nthus the particular higher-order representations); they are akin to\nfingerprints~-- even isogenic flies have different projections. Leveraging\ninsights from this architecture, we propose KCNet, a single-hidden-layer neural\nnetwork that contains sparse, randomized, binary weights between the input\nlayer and the hidden layer and analytically learned weights between the hidden\nlayer and the output layer. Furthermore, we also propose a dynamic optimization\nalgorithm that enables the KCNet to increase performance beyond its structural\nlimits by searching a more efficient set of inputs. For odorant-perception\ntasks that predict perceptual properties of an odorant, we show that KCNet\noutperforms existing data-driven approaches, such as XGBoost. For\nimage-classification tasks, KCNet achieves reasonable performance on benchmark\ndatasets (MNIST, Fashion-MNIST, and EMNIST) without any data-augmentation\nmethods or convolutional layers and shows particularly fast running time. Thus,\nneural networks inspired by the insect brain can be both economical and perform\nwell.",
          "link": "http://arxiv.org/abs/2108.07554",
          "publishedOn": "2021-08-18T01:55:01.632Z",
          "wordCount": 730,
          "title": "KCNet: An Insect-Inspired Single-Hidden-Layer Neural Network with Randomized Binary Weights for Prediction and Classification Tasks. (arXiv:2108.07554v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Meng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Recently, image captioning has aroused great interest in both academic and\nindustrial worlds. Most existing systems are built upon large-scale datasets\nconsisting of image-sentence pairs, which, however, are time-consuming to\nconstruct. In addition, even for the most advanced image captioning systems, it\nis still difficult to realize deep image understanding. In this work, we\nachieve unpaired image captioning by bridging the vision and the language\ndomains with high-level semantic information. The motivation stems from the\nfact that the semantic concepts with the same modality can be extracted from\nboth images and descriptions. To further improve the quality of captions\ngenerated by the model, we propose the Semantic Relationship Explorer, which\nexplores the relationships between semantic concepts for better understanding\nof the image. Extensive experiments on MSCOCO dataset show that we can generate\ndesirable captions without paired datasets. Furthermore, the proposed approach\nboosts five strong baselines under the paired setting, where the most\nsignificant improvement in CIDEr score reaches 8%, demonstrating that it is\neffective and generalizes well to a wide range of models.",
          "link": "http://arxiv.org/abs/2106.10658",
          "publishedOn": "2021-08-18T01:55:01.625Z",
          "wordCount": 634,
          "title": "Exploring Semantic Relationships for Unpaired Image Captioning. (arXiv:2106.10658v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunrui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camporese_G/0/1/0/all/0/1\">Guglielmo Camporese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenjing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1\">Alessandro Sperduti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>",
          "description": "In open set recognition, a classifier has to detect unknown classes that are\nnot known at training time. In order to recognize new categories, the\nclassifier has to project the input samples of known classes in very compact\nand separated regions of the features space for discriminating samples of\nunknown classes. Recently proposed Capsule Networks have shown to outperform\nalternatives in many fields, particularly in image recognition, however they\nhave not been fully applied yet to open-set recognition. In capsule networks,\nscalar neurons are replaced by capsule vectors or matrices, whose entries\nrepresent different properties of objects. In our proposal, during training,\ncapsules features of the same known class are encouraged to match a pre-defined\ngaussian, one for each class. To this end, we use the variational autoencoder\nframework, with a set of gaussian priors as the approximation for the posterior\ndistribution. In this way, we are able to control the compactness of the\nfeatures of the same class around the center of the gaussians, thus controlling\nthe ability of the classifier in detecting samples from unknown classes. We\nconducted several experiments and ablation of our model, obtaining state of the\nart results on different datasets in the open set recognition and unknown\ndetection tasks.",
          "link": "http://arxiv.org/abs/2104.09159",
          "publishedOn": "2021-08-18T01:55:01.619Z",
          "wordCount": 675,
          "title": "Conditional Variational Capsule Network for Open Set Recognition. (arXiv:2104.09159v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>",
          "description": "People navigating in unfamiliar buildings take advantage of myriad visual,\nspatial and semantic cues to efficiently achieve their navigation goals.\nTowards equipping computational agents with similar capabilities, we introduce\nPathdreamer, a visual world model for agents navigating in novel indoor\nenvironments. Given one or more previous visual observations, Pathdreamer\ngenerates plausible high-resolution 360 visual observations (RGB, semantic\nsegmentation and depth) for viewpoints that have not been visited, in buildings\nnot seen during training. In regions of high uncertainty (e.g. predicting\naround corners, imagining the contents of an unseen room), Pathdreamer can\npredict diverse scenes, allowing an agent to sample multiple realistic outcomes\nfor a given trajectory. We demonstrate that Pathdreamer encodes useful and\naccessible visual, spatial and semantic knowledge about human environments by\nusing it in the downstream task of Vision-and-Language Navigation (VLN).\nSpecifically, we show that planning ahead with Pathdreamer brings about half\nthe benefit of looking ahead at actual observations from unobserved parts of\nthe environment. We hope that Pathdreamer will help unlock model-based\napproaches to challenging embodied navigation tasks such as navigating to\nspecified objects and VLN.",
          "link": "http://arxiv.org/abs/2105.08756",
          "publishedOn": "2021-08-18T01:55:01.612Z",
          "wordCount": 657,
          "title": "Pathdreamer: A World Model for Indoor Navigation. (arXiv:2105.08756v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1\">Penghua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_H/0/1/0/all/0/1\">Huaiwei Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>",
          "description": "With the renaissance of deep learning, automatic diagnostic systems for\ncomputed tomography (CT) have achieved many successful applications. However,\nthey are mostly attributed to careful expert annotations, which are often\nscarce in practice. This drives our interest to the unsupervised representation\nlearning. Recent studies have shown that self-supervised learning is an\neffective approach for learning representations, but most of them rely on the\nempirical design of transformations and pretext tasks. To avoid the\nsubjectivity associated with these methods, we propose the MVCNet, a novel\nunsupervised three dimensional (3D) representation learning method working in a\ntransformation-free manner. We view each 3D lesion from different orientations\nto collect multiple two dimensional (2D) views. Then, an embedding function is\nlearned by minimizing a contrastive loss so that the 2D views of the same 3D\nlesion are aggregated, and the 2D views of different lesions are separated. We\nevaluate the representations by training a simple classification head upon the\nembedding layer. Experimental results show that MVCNet achieves\nstate-of-the-art accuracies on the LIDC-IDRI (89.55%), LNDb (77.69%) and\nTianChi (79.96%) datasets for unsupervised representation learning. When\nfine-tuned on 10% of the labeled data, the accuracies are comparable to the\nsupervised learning model (89.46% vs. 85.03%, 73.85% vs. 73.44%, 83.56% vs.\n83.34% on the three datasets, respectively), indicating the superiority of\nMVCNet in learning representations with limited annotations. Code is released\nat: https://github.com/penghuazhai/MVCNet.",
          "link": "http://arxiv.org/abs/2108.07662",
          "publishedOn": "2021-08-18T01:55:01.606Z",
          "wordCount": 696,
          "title": "MVCNet: Multiview Contrastive Network for Unsupervised Representation Learning for 3D CT Lesions. (arXiv:2108.07662v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaodong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Despite the great progress of person re-identification (ReID) with the\nadoption of Convolutional Neural Networks, current ReID models are opaque and\nonly outputs a scalar distance between two persons. There are few methods\nproviding users semantically understandable explanations for why two persons\nare the same one or not. In this paper, we propose a post-hoc method, named\nAttribute-guided Metric Distillation (AMD), to explain existing ReID models.\nThis is the first method to explore attributes to answer: 1) what and where the\nattributes make two persons different, and 2) how much each attribute\ncontributes to the difference. In AMD, we design a pluggable interpreter\nnetwork for target models to generate quantitative contributions of attributes\nand visualize accurate attention maps of the most discriminative attributes. To\nachieve this goal, we propose a metric distillation loss by which the\ninterpreter learns to decompose the distance of two persons into components of\nattributes with knowledge distilled from the target model. Moreover, we propose\nan attribute prior loss to make the interpreter generate attribute-guided\nattention maps and to eliminate biases caused by the imbalanced distribution of\nattributes. This loss can guide the interpreter to focus on the exclusive and\ndiscriminative attributes rather than the large-area but common attributes of\ntwo persons. Comprehensive experiments show that the interpreter can generate\neffective and intuitive explanations for varied models and generalize well\nunder cross-domain settings. As a by-product, the accuracy of target models can\nbe further improved with our interpreter.",
          "link": "http://arxiv.org/abs/2103.01451",
          "publishedOn": "2021-08-18T01:55:01.588Z",
          "wordCount": 722,
          "title": "Explainable Person Re-Identification with Attribute-guided Metric Distillation. (arXiv:2103.01451v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1\">Renjie Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>",
          "description": "In this paper, we investigate the knowledge distillation (KD) strategy for\nobject detection and propose an effective framework applicable to both\nhomogeneous and heterogeneous student-teacher pairs. The conventional feature\nimitation paradigm introduces imitation masks to focus on informative\nforeground areas while excluding the background noises. However, we find that\nthose methods fail to fully utilize the semantic information in all feature\npyramid levels, which leads to inefficiency for knowledge distillation between\nFPN-based detectors. To this end, we propose a novel semantic-guided feature\nimitation technique, which automatically performs soft matching between feature\npairs across all pyramid levels to provide the optimal guidance to the student.\nTo push the envelop even further, we introduce contrastive distillation to\neffectively capture the information encoded in the relationship between\ndifferent feature regions. Finally, we propose a generalized detection KD\npipeline, which is capable of distilling both homogeneous and heterogeneous\ndetector pairs. Our method consistently outperforms the existing detection KD\ntechniques, and works when (1) components in the framework are used separately\nand in conjunction; (2) for both homogeneous and heterogenous student-teacher\npairs and (3) on multiple detection benchmarks. With a powerful\nX101-FasterRCNN-Instaboost detector as the teacher, R50-FasterRCNN reaches\n44.0% AP, R50-RetinaNet reaches 43.3% AP and R50-FCOS reaches 43.1% AP on COCO\ndataset.",
          "link": "http://arxiv.org/abs/2108.07482",
          "publishedOn": "2021-08-18T01:55:01.581Z",
          "wordCount": 663,
          "title": "G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation. (arXiv:2108.07482v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wenyuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyong Li</a>",
          "description": "A table arranging data in rows and columns is a very effective data\nstructure, which has been widely used in business and scientific research.\nConsidering large-scale tabular data in online and offline documents, automatic\ntable recognition has attracted increasing attention from the document analysis\ncommunity. Though human can easily understand the structure of tables, it\nremains a challenge for machines to understand that, especially due to a\nvariety of different table layouts and styles. Existing methods usually model a\ntable as either the markup sequence or the adjacency matrix between different\ntable cells, failing to address the importance of the logical location of table\ncells, e.g., a cell is located in the first row and the second column of the\ntable. In this paper, we reformulate the problem of table structure recognition\nas the table graph reconstruction, and propose an end-to-end trainable table\ngraph reconstruction network (TGRNet) for table structure recognition.\nSpecifically, the proposed method has two main branches, a cell detection\nbranch and a cell logical location branch, to jointly predict the spatial\nlocation and the logical location of different cells. Experimental results on\nthree popular table recognition datasets and a new dataset with table graph\nannotations (TableGraph-350K) demonstrate the effectiveness of the proposed\nTGRNet for table structure recognition. Code and annotations will be made\npublicly available.",
          "link": "http://arxiv.org/abs/2106.10598",
          "publishedOn": "2021-08-18T01:55:01.575Z",
          "wordCount": 707,
          "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition. (arXiv:2106.10598v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiabi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xiaopeng Gong</a>",
          "description": "Image co-segmentation has attracted a lot of attentions in computer vision\ncommunity. In this paper, we propose a new approach to image co-segmentation\nthrough introducing the dense connections into the decoder path of Siamese\nU-net and presenting a new edge enhanced 3D IOU loss measured over distance\nmaps. Considering the rigorous mapping between the signed normalized distance\nmap (SNDM) and the binary segmentation mask, we estimate the SNDMs directly\nfrom original images and use them to determine the segmentation results. We\napply the Siamese U-net for solving this problem and improve its effectiveness\nby densely connecting each layer with subsequent layers in the decoder path.\nFurthermore, a new learning loss is designed to measure the 3D intersection\nover union (IOU) between the generated SNDMs and the labeled SNDMs. The\nexperimental results on commonly used datasets for image co-segmentation\ndemonstrate the effectiveness of our presented dense structure and edge\nenhanced 3D IOU loss of SNDM. To our best knowledge, they lead to the\nstate-of-the-art performance on the Internet and iCoseg datasets.",
          "link": "http://arxiv.org/abs/2108.07491",
          "publishedOn": "2021-08-18T01:55:01.567Z",
          "wordCount": 618,
          "title": "A Dense Siamese U-Net trained with Edge Enhanced 3D IOU Loss for Image Co-segmentation. (arXiv:2108.07491v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zavrtanik_V/0/1/0/all/0/1\">Vitjan Zavrtanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1\">Matej Kristan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skocaj_D/0/1/0/all/0/1\">Danijel Sko&#x10d;aj</a>",
          "description": "Visual surface anomaly detection aims to detect local image regions that\nsignificantly deviate from normal appearance. Recent surface anomaly detection\nmethods rely on generative models to accurately reconstruct the normal areas\nand to fail on anomalies. These methods are trained only on anomaly-free\nimages, and often require hand-crafted post-processing steps to localize the\nanomalies, which prohibits optimizing the feature extraction for maximal\ndetection capability. In addition to reconstructive approach, we cast surface\nanomaly detection primarily as a discriminative problem and propose a\ndiscriminatively trained reconstruction anomaly embedding model (DRAEM). The\nproposed method learns a joint representation of an anomalous image and its\nanomaly-free reconstruction, while simultaneously learning a decision boundary\nbetween normal and anomalous examples. The method enables direct anomaly\nlocalization without the need for additional complicated post-processing of the\nnetwork output and can be trained using simple and general anomaly simulations.\nOn the challenging MVTec anomaly detection dataset, DRAEM outperforms the\ncurrent state-of-the-art unsupervised methods by a large margin and even\ndelivers detection performance close to the fully-supervised methods on the\nwidely used DAGM surface-defect detection dataset, while substantially\noutperforming them in localization accuracy.",
          "link": "http://arxiv.org/abs/2108.07610",
          "publishedOn": "2021-08-18T01:55:01.561Z",
          "wordCount": 628,
          "title": "DR{\\AE}M -- A discriminatively trained reconstruction embedding for surface anomaly detection. (arXiv:2108.07610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>",
          "description": "Light field (LF) image super-resolution (SR) aims at reconstructing\nhigh-resolution LF images from their low-resolution counterparts. Although\nCNN-based methods have achieved remarkable performance in LF image SR, these\nmethods cannot fully model the non-local properties of the 4D LF data. In this\npaper, we propose a simple but effective Transformer-based method for LF image\nSR. In our method, an angular Transformer is designed to incorporate\ncomplementary information among different views, and a spatial Transformer is\ndeveloped to capture both local and long-range dependencies within each\nsub-aperture image. With the proposed angular and spatial Transformers, the\nbeneficial information in an LF can be fully exploited and the SR performance\nis boosted. We validate the effectiveness of our angular and spatial\nTransformers through extensive ablation studies, and compare our method to\nrecent state-of-the-art methods on five public LF datasets. Our method achieves\nsuperior SR performance with a small model size and low computational cost.",
          "link": "http://arxiv.org/abs/2108.07597",
          "publishedOn": "2021-08-18T01:55:01.554Z",
          "wordCount": 602,
          "title": "Light Field Image Super-Resolution with Transformers. (arXiv:2108.07597v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingqiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Adversarial robustness has attracted extensive studies recently by revealing\nthe vulnerability and intrinsic characteristics of deep networks. However,\nexisting works on adversarial robustness mainly focus on balanced datasets,\nwhile real-world data usually exhibits a long-tailed distribution. To push\nadversarial robustness towards more realistic scenarios, in this work we\ninvestigate the adversarial vulnerability as well as defense under long-tailed\ndistributions. In particular, we first reveal the negative impacts induced by\nimbalanced data on both recognition performance and adversarial robustness,\nuncovering the intrinsic challenges of this problem. We then perform a\nsystematic study on existing long-tailed recognition methods in conjunction\nwith the adversarial training framework. Several valuable observations are\nobtained: 1) natural accuracy is relatively easy to improve, 2) fake gain of\nrobust accuracy exists under unreliable evaluation, and 3) boundary error\nlimits the promotion of robustness. Inspired by these observations, we propose\na clean yet effective framework, RoBal, which consists of two dedicated\nmodules, a scale-invariant classifier and data re-balancing via both margin\nengineering at training stage and boundary adjustment during inference.\nExtensive experiments demonstrate the superiority of our approach over other\nstate-of-the-art defense methods. To our best knowledge, we are the first to\ntackle adversarial robustness under long-tailed distributions, which we believe\nwould be a significant step towards real-world robustness. Our code is\navailable at: https://github.com/wutong16/Adversarial_Long-Tail .",
          "link": "http://arxiv.org/abs/2104.02703",
          "publishedOn": "2021-08-18T01:55:01.536Z",
          "wordCount": 696,
          "title": "Adversarial Robustness under Long-Tailed Distribution. (arXiv:2104.02703v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tilmon_B/0/1/0/all/0/1\">Brevin Tilmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppal_S/0/1/0/all/0/1\">Sanjeev J. Koppal</a>",
          "description": "Most monocular depth sensing methods use conventionally captured images that\nare created without considering scene content. In contrast, animal eyes have\nfast mechanical motions, called saccades, that control how the scene is imaged\nby the fovea, where resolution is highest. In this paper, we present the\nSaccadeCam framework for adaptively distributing resolution onto regions of\ninterest in the scene. Our algorithm for adaptive resolution is a\nself-supervised network and we demonstrate results for end-to-end learning for\nmonocular depth estimation. We also show preliminary results with a real\nSaccadeCam hardware prototype.",
          "link": "http://arxiv.org/abs/2103.12981",
          "publishedOn": "2021-08-18T01:55:01.529Z",
          "wordCount": 568,
          "title": "SaccadeCam: Adaptive Visual Attention for Monocular Depth Sensing. (arXiv:2103.12981v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Genser_A/0/1/0/all/0/1\">Alexander Genser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautle_N/0/1/0/all/0/1\">Noel Hautle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makridis_M/0/1/0/all/0/1\">Michail Makridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouvelas_A/0/1/0/all/0/1\">Anastasios Kouvelas</a>",
          "description": "Accurate estimation of the traffic state over a network is essential since it\nis the starting point for designing and implementing any traffic management\nstrategy. Hence, traffic operators and users of a transportation network can\nmake reliable decisions such as influence/change route or mode choice. However,\nthe problem of traffic state estimation from various sensors within an urban\nenvironment is very complex for several different reasons, such as availability\nof sensors, different noise levels, different output quantities, sensor\naccuracy, heterogeneous data fusion, and many more. To provide a better\nunderstanding of this problem, we organized an experimental campaign with video\nmeasurement in an area within the urban network of Zurich, Switzerland. We\nfocus on capturing the traffic state in terms of traffic flow and travel times\nby ensuring measurements from established thermal cameras by the city's\nauthorities, processed video data, and the Google Distance Matrix. We assess\nthe different data sources, and we propose a simple yet efficient Multiple\nLinear Regression (MLR) model to estimate travel times with fusion of various\ndata sources. Comparative results with ground-truth data (derived from video\nmeasurements) show the efficiency and robustness of the proposed methodology.",
          "link": "http://arxiv.org/abs/2108.07698",
          "publishedOn": "2021-08-18T01:55:01.523Z",
          "wordCount": 644,
          "title": "An Experimental Urban Case Study with Various Data Sources and a Model for Traffic Estimation. (arXiv:2108.07698v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trulls_E/0/1/0/all/0/1\">Eduard Trulls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosang_J/0/1/0/all/0/1\">Jan Hosang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>",
          "description": "We propose a novel framework for finding correspondences in images based on a\ndeep neural network that, given two images and a query point in one of them,\nfinds its correspondence in the other. By doing so, one has the option to query\nonly the points of interest and retrieve sparse correspondences, or to query\nall points in an image and obtain dense mappings. Importantly, in order to\ncapture both local and global priors, and to let our model relate between image\nregions using the most relevant among said priors, we realize our network using\na transformer. At inference time, we apply our correspondence network by\nrecursively zooming in around the estimates, yielding a multiscale pipeline\nable to provide highly-accurate correspondences. Our method significantly\noutperforms the state of the art on both sparse and dense correspondence\nproblems on multiple datasets and tasks, ranging from wide-baseline stereo to\noptical flow, without any retraining for a specific dataset. We commit to\nreleasing data, code, and all the tools necessary to train from scratch and\nensure reproducibility.",
          "link": "http://arxiv.org/abs/2103.14167",
          "publishedOn": "2021-08-18T01:55:01.516Z",
          "wordCount": 642,
          "title": "COTR: Correspondence Transformer for Matching Across Images. (arXiv:2103.14167v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopinathan_M/0/1/0/all/0/1\">Muraleekrishna Gopinathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_G/0/1/0/all/0/1\">Giang Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Khalaf_J/0/1/0/all/0/1\">Jumana Abu-Khalaf</a>",
          "description": "Seamless Human-Robot Interaction is the ultimate goal of developing service\nrobotic systems. For this, the robotic agents have to understand their\nsurroundings to better complete a given task. Semantic scene understanding\nallows a robotic agent to extract semantic knowledge about the objects in the\nenvironment. In this work, we present a semantic scene understanding pipeline\nthat fuses 2D and 3D detection branches to generate a semantic map of the\nenvironment. The 2D mask proposals from state-of-the-art 2D detectors are\ninverse-projected to the 3D space and combined with 3D detections from point\nsegmentation networks. Unlike previous works that were evaluated on collected\ndatasets, we test our pipeline on an active photo-realistic robotic environment\n- BenchBot. Our novelty includes rectification of 3D proposals using projected\n2D detections and modality fusion based on object size. This work is done as\npart of the Robotic Vision Scene Understanding Challenge (RVSU). The\nperformance evaluation demonstrates that our pipeline has improved on baseline\nmethods without significant computational bottleneck.",
          "link": "http://arxiv.org/abs/2108.07616",
          "publishedOn": "2021-08-18T01:55:01.493Z",
          "wordCount": 617,
          "title": "Indoor Semantic Scene Understanding using Multi-modality Fusion. (arXiv:2108.07616v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifah_T/0/1/0/all/0/1\">Tariq Alkhalifah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovcharenko_O/0/1/0/all/0/1\">Oleg Ovcharenko</a>",
          "description": "We propose a direct domain adaptation (DDA) approach to enrich the training\nof supervised neural networks on synthetic data by features from real-world\ndata. The process involves a series of linear operations on the input features\nto the NN model, whether they are from the source or target domains, as\nfollows: 1) A cross-correlation of the input data (i.e. images) with a randomly\npicked sample pixel (or pixels) of all images from that domain or the mean of\nall randomly picked sample pixel (or pixels) of all images. 2) The convolution\nof the resulting data with the mean of the autocorrelated input images from the\nother domain. In the training stage, as expected, the input images are from the\nsource domain, and the mean of auto-correlated images are evaluated from the\ntarget domain. In the inference/application stage, the input images are from\nthe target domain, and the mean of auto-correlated images are evaluated from\nthe source domain. The proposed method only manipulates the data from the\nsource and target domains and does not explicitly interfere with the training\nworkflow and network architecture. An application that includes training a\nconvolutional neural network on the MNIST dataset and testing the network on\nthe MNIST-M dataset achieves a 70% accuracy on the test data. A principal\ncomponent analysis (PCA), as well as t-SNE, show that the input features from\nthe source and target domains, after the proposed direct transformations, share\nsimilar properties along with the principal components as compared to the\noriginal MNIST and MNIST-M input features.",
          "link": "http://arxiv.org/abs/2108.07600",
          "publishedOn": "2021-08-18T01:55:01.485Z",
          "wordCount": 691,
          "title": "Direct domain adaptation through reciprocal linear transformations. (arXiv:2108.07600v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lianyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Feng Wang</a>",
          "description": "A large gap exists between fully-supervised object detection and\nweakly-supervised object detection. To narrow this gap, some methods consider\nknowledge transfer from additional fully-supervised dataset. But these methods\ndo not fully exploit discriminative category information in the\nfully-supervised dataset, thus causing low mAP. To solve this issue, we propose\na novel category transfer framework for weakly supervised object detection. The\nintuition is to fully leverage both visually-discriminative and\nsemantically-correlated category information in the fully-supervised dataset to\nenhance the object-classification ability of a weakly-supervised detector. To\nhandle overlapping category transfer, we propose a double-supervision mean\nteacher to gather common category information and bridge the domain gap between\ntwo datasets. To handle non-overlapping category transfer, we propose a\nsemantic graph convolutional network to promote the aggregation of semantic\nfeatures between correlated categories. Experiments are conducted with Pascal\nVOC 2007 as the target weakly-supervised dataset and COCO as the source\nfully-supervised dataset. Our category transfer framework achieves 63.5% mAP\nand 80.3% CorLoc with 5 overlapping categories between two datasets, which\noutperforms the state-of-the-art methods. Codes are avaliable at\nhttps://github.com/MediaBrain-SJTU/CaT.",
          "link": "http://arxiv.org/abs/2108.07487",
          "publishedOn": "2021-08-18T01:55:01.479Z",
          "wordCount": 617,
          "title": "CaT: Weakly Supervised Object Detection with Category Transfer. (arXiv:2108.07487v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfieri_A/0/1/0/all/0/1\">Andrea Alfieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yancong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>",
          "description": "Transformers can generate predictions in two approaches: 1. auto-regressively\nby conditioning each sequence element on the previous ones, or 2. directly\nproduce an output sequences in parallel. While research has mostly explored\nupon this difference on sequential tasks in NLP, we study the difference\nbetween auto-regressive and parallel prediction on visual set prediction tasks,\nand in particular on polygonal shapes in images because polygons are\nrepresentative of numerous types of objects, such as buildings or obstacles for\naerial vehicles. This is challenging for deep learning architectures as a\npolygon can consist of a varying carnality of points. We provide evidence on\nthe importance of natural orders for Transformers, and show the benefit of\ndecomposing complex polygons into collections of points in an auto-regressive\nmanner.",
          "link": "http://arxiv.org/abs/2108.07533",
          "publishedOn": "2021-08-18T01:55:01.472Z",
          "wordCount": 568,
          "title": "Investigating transformers in the decomposition of polygonal shapes as point collections. (arXiv:2108.07533v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07619",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yiasemis_G/0/1/0/all/0/1\">George Yiasemis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanchez_C/0/1/0/all/0/1\">Clara I. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sonke_J/0/1/0/all/0/1\">Jan-Jakob Sonke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>",
          "description": "In spite of its extensive adaptation in almost every medical diagnostic and\nexaminatorial application, Magnetic Resonance Imaging (MRI) is still a slow\nimaging modality which limits its use for dynamic imaging. In recent years,\nParallel Imaging (PI) and Compressed Sensing (CS) have been utilised to\naccelerate the MRI acquisition. In clinical settings, subsampling the k-space\nmeasurements during scanning time using Cartesian trajectories, such as\nrectilinear sampling, is currently the most conventional CS approach applied\nwhich, however, is prone to producing aliased reconstructions. With the advent\nof the involvement of Deep Learning (DL) in accelerating the MRI,\nreconstructing faithful images from subsampled data became increasingly\npromising. Retrospectively applying a subsampling mask onto the k-space data is\na way of simulating the accelerated acquisition of k-space data in real\nclinical setting. In this paper we compare and provide a review for the effect\nof applying either rectilinear or radial retrospective subsampling on the\nquality of the reconstructions outputted by trained deep neural networks. With\nthe same choice of hyper-parameters, we train and evaluate two distinct\nRecurrent Inference Machines (RIMs), one for each type of subsampling. The\nqualitative and quantitative results of our experiments indicate that the model\ntrained on data with radial subsampling attains higher performance and learns\nto estimate reconstructions with higher fidelity paving the way for other DL\napproaches to involve radial subsampling.",
          "link": "http://arxiv.org/abs/2108.07619",
          "publishedOn": "2021-08-18T01:55:01.465Z",
          "wordCount": 679,
          "title": "Deep MRI Reconstruction with Radial Subsampling. (arXiv:2108.07619v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chi-Tung Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jinzheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_W/0/1/0/all/0/1\">Wei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youjing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">YuTing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chien-Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Youbao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wei-Chen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_T/0/1/0/all/0/1\">Ta-Sen Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chien-Hung Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_A/0/1/0/all/0/1\">Adam P. Harrison</a>",
          "description": "Hepatocellular carcinoma (HCC) can be potentially discovered from abdominal\ncomputed tomography (CT) studies under varied clinical scenarios, e.g., fully\ndynamic contrast enhanced (DCE) studies, non-contrast (NC) plus venous phase\n(VP) abdominal studies, or NC-only studies. We develop a flexible\nthree-dimensional deep algorithm, called hetero-phase volumetric detection\n(HPVD), that can accept any combination of contrast-phase inputs and with\nadjustable sensitivity depending on the clinical purpose. We trained HPVD on\n771 DCE CT scans to detect HCCs and tested on external 164 positives and 206\ncontrols, respectively. We compare performance against six clinical readers,\nincluding two radiologists, two hepato-pancreatico-biliary (HPB) surgeons, and\ntwo hepatologists. The area under curve (AUC) of the localization receiver\noperating characteristic (LROC) for NC-only, NC plus VP, and full DCE CT\nyielded 0.71, 0.81, 0.89 respectively. At a high sensitivity operating point of\n80% on DCE CT, HPVD achieved 97% specificity, which is comparable to measured\nphysician performance. We also demonstrate performance improvements over more\ntypical and less flexible non hetero-phase detectors. Thus, we demonstrate that\na single deep learning algorithm can be effectively applied to diverse HCC\ndetection clinical scenarios.",
          "link": "http://arxiv.org/abs/2108.07492",
          "publishedOn": "2021-08-18T01:55:01.459Z",
          "wordCount": 657,
          "title": "A Flexible Three-Dimensional Hetero-phase Computed Tomography Hepatocellular Carcinoma (HCC) Detection Algorithm for Generalizable and Practical HCC Screening. (arXiv:2108.07492v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Ze-Hua Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hui-Liang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bo-Wen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huaqi Zhang</a>",
          "description": "Compared to color images captured by conventional RGB cameras, monochrome\nimages usually have better signal-to-noise ratio (SNR) and richer textures due\nto its higher quantum efficiency. It is thus natural to apply a mono-color\ndual-camera system to restore color images with higher visual quality. In this\npaper, we propose a mono-color image enhancement algorithm that colorizes the\nmonochrome image with the color one. Based on the assumption that adjacent\nstructures with similar luminance values are likely to have similar colors, we\nfirst perform dense scribbling to assign colors to the monochrome pixels\nthrough block matching. Two types of outliers, including occlusion and color\nambiguity, are detected and removed from the initial scribbles. We also\nintroduce a sampling strategy to accelerate the scribbling process. Then, the\ndense scribbles are propagated to the entire image. To alleviate incorrect\ncolor propagation in the regions that have no color hints at all, we generate\nextra color seeds based on the existed scribbles to guide the propagation\nprocess. Experimental results show that, our algorithm can efficiently restore\ncolor images with higher SNR and richer details from the mono-color image\npairs, and achieves good performance in solving the color bleeding problem.",
          "link": "http://arxiv.org/abs/2108.07471",
          "publishedOn": "2021-08-18T01:55:01.452Z",
          "wordCount": 627,
          "title": "Guided Colorization Using Mono-Color Image Pairs. (arXiv:2108.07471v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhihao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songcen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>",
          "description": "Instance segmentation in 3D scenes is fundamental in many applications of\nscene understanding. It is yet challenging due to the compound factors of data\nirregularity and uncertainty in the numbers of instances. State-of-the-art\nmethods largely rely on a general pipeline that first learns point-wise\nfeatures discriminative at semantic and instance levels, followed by a separate\nstep of point grouping for proposing object instances. While promising, they\nhave the shortcomings that (1) the second step is not supervised by the main\nobjective of instance segmentation, and (2) their point-wise feature learning\nand grouping are less effective to deal with data irregularities, possibly\nresulting in fragmented segmentations. To address these issues, we propose in\nthis work an end-to-end solution of Semantic Superpoint Tree Network (SSTNet)\nfor proposing object instances from scene points. Key in SSTNet is an\nintermediate, semantic superpoint tree (SST), which is constructed based on the\nlearned semantic features of superpoints, and which will be traversed and split\nat intermediate tree nodes for proposals of object instances. We also design in\nSSTNet a refinement module, termed CliqueNet, to prune superpoints that may be\nwrongly grouped into instance proposals. Experiments on the benchmarks of\nScanNet and S3DIS show the efficacy of our proposed method. At the time of\nsubmission, SSTNet ranks top on the ScanNet (V2) leaderboard, with 2% higher of\nmAP than the second best method. The source code in PyTorch is available at\nhttps://github.com/Gorilla-Lab-SCUT/SSTNet.",
          "link": "http://arxiv.org/abs/2108.07478",
          "publishedOn": "2021-08-18T01:55:01.446Z",
          "wordCount": 683,
          "title": "Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks. (arXiv:2108.07478v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xiaojing Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Taizhe Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyao Wu</a>",
          "description": "With the development of Generative Adversarial Network, image-based virtual\ntry-on methods have made great progress. However, limited work has explored the\ntask of video-based virtual try-on while it is important in real-world\napplications. Most existing video-based virtual try-on methods usually require\nclothing templates and they can only generate blurred and low-resolution\nresults. To address these challenges, we propose a Memory-based Video virtual\nTry-On Network (MV-TON), which seamlessly transfers desired clothes to a target\nperson without using any clothing templates and generates high-resolution\nrealistic videos. Specifically, MV-TON consists of two modules: 1) a try-on\nmodule that transfers the desired clothes from model images to frame images by\npose alignment and region-wise replacing of pixels; 2) a memory refinement\nmodule that learns to embed the existing generated frames into the latent space\nas external memory for the following frame generation. Experimental results\nshow the effectiveness of our method in the video virtual try-on task and its\nsuperiority over other existing methods.",
          "link": "http://arxiv.org/abs/2108.07502",
          "publishedOn": "2021-08-18T01:55:01.431Z",
          "wordCount": 595,
          "title": "MV-TON: Memory-based Video Virtual Try-on network. (arXiv:2108.07502v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaochen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1\">Benjamin Kellenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rui Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajnsek_I/0/1/0/all/0/1\">Irena Hajnsek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>",
          "description": "Automated animal censuses with aerial imagery are a vital ingredient towards\nwildlife conservation. Recent models are generally based on deep learning and\nthus require vast amounts of training data. Due to their scarcity and minuscule\nsize, annotating animals in aerial imagery is a highly tedious process. In this\nproject, we present a methodology to reduce the amount of required training\ndata by resorting to self-supervised pretraining. In detail, we examine a\ncombination of recent contrastive learning methodologies like Momentum Contrast\n(MoCo) and Cross-Level Instance-Group Discrimination (CLD) to condition our\nmodel on the aerial images without the requirement for labels. We show that a\ncombination of MoCo, CLD, and geometric augmentations outperforms conventional\nmodels pre-trained on ImageNet by a large margin. Crucially, our method still\nyields favorable results even if we reduce the number of training animals to\njust 10%, at which point our best model scores double the recall of the\nbaseline at similar precision. This effectively allows reducing the number of\nrequired annotations to a fraction while still being able to train\nhigh-accuracy models in such highly challenging settings.",
          "link": "http://arxiv.org/abs/2108.07582",
          "publishedOn": "2021-08-18T01:55:01.418Z",
          "wordCount": 637,
          "title": "Self-Supervised Pretraining and Controlled Augmentation Improve Rare Wildlife Recognition in UAV Images. (arXiv:2108.07582v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haitian Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "We propose PR-RRN, a novel neural-network based method for Non-rigid\nStructure-from-Motion (NRSfM). PR-RRN consists of Residual-Recursive Networks\n(RRN) and two extra regularization losses. RRN is designed to effectively\nrecover 3D shape and camera from 2D keypoints with novel residual-recursive\nstructure. As NRSfM is a highly under-constrained problem, we propose two new\npairwise regularization to further regularize the reconstruction. The\nRigidity-based Pairwise Contrastive Loss regularizes the shape representation\nby encouraging higher similarity between the representations of high-rigidity\npairs of frames than low-rigidity pairs. We propose minimum singular-value\nratio to measure the pairwise rigidity. The Pairwise Consistency Loss enforces\nthe reconstruction to be consistent when the estimated shapes and cameras are\nexchanged between pairs. Our approach achieves state-of-the-art performance on\nCMU MOCAP and PASCAL3D+ dataset.",
          "link": "http://arxiv.org/abs/2108.07506",
          "publishedOn": "2021-08-18T01:55:01.399Z",
          "wordCount": 563,
          "title": "PR-RRN: Pairwise-Regularized Residual-Recursive Networks for Non-rigid Structure-from-Motion. (arXiv:2108.07506v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Henglin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>",
          "description": "With the strength of deep generative models, 3D pose transfer regains\nintensive research interests in recent years. Existing methods mainly rely on a\nvariety of constraints to achieve the pose transfer over 3D meshes, e.g., the\nneed for the manually encoding for shape and pose disentanglement. In this\npaper, we present an unsupervised approach to conduct the pose transfer between\nany arbitrate given 3D meshes. Specifically, a novel Intrinsic-Extrinsic\nPreserved Generative Adversarial Network (IEP-GAN) is presented for both\nintrinsic (i.e., shape) and extrinsic (i.e., pose) information preservation.\nExtrinsically, we propose a co-occurrence discriminator to capture the\nstructural/pose invariance from distinct Laplacians of the mesh. Meanwhile,\nintrinsically, a local intrinsic-preserved loss is introduced to preserve the\ngeodesic priors while avoiding the heavy computations. At last, we show the\npossibility of using IEP-GAN to manipulate 3D human meshes in various ways,\nincluding pose transfer, identity swapping and pose interpolation with latent\ncode vector arithmetic. The extensive experiments on various 3D datasets of\nhumans, animals and hands qualitatively and quantitatively demonstrate the\ngenerality of our approach. Our proposed model produces better results and is\nsubstantially more efficient compared to recent state-of-the-art methods. Code\nis available: https://github.com/mikecheninoulu/Unsupervised_IEPGAN.",
          "link": "http://arxiv.org/abs/2108.07520",
          "publishedOn": "2021-08-18T01:55:01.384Z",
          "wordCount": 642,
          "title": "Unsupervised Geodesic-preserved Generative Adversarial Networks for Unconstrained 3D Pose Transfer. (arXiv:2108.07520v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strohm_F/0/1/0/all/0/1\">Florian Strohm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_E/0/1/0/all/0/1\">Ekta Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_S/0/1/0/all/0/1\">Sven Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1\">Philipp M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bace_M/0/1/0/all/0/1\">Mihai B&#xe2;ce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulling_A/0/1/0/all/0/1\">Andreas Bulling</a>",
          "description": "We propose a novel method that leverages human fixations to visually decode\nthe image a person has in mind into a photofit (facial composite). Our method\ncombines three neural networks: An encoder, a scoring network, and a decoder.\nThe encoder extracts image features and predicts a neural activation map for\neach face looked at by a human observer. A neural scoring network compares the\nhuman and neural attention and predicts a relevance score for each extracted\nimage feature. Finally, image features are aggregated into a single feature\nvector as a linear combination of all features weighted by relevance which a\ndecoder decodes into the final photofit. We train the neural scoring network on\na novel dataset containing gaze data of 19 participants looking at collages of\nsynthetic faces. We show that our method significantly outperforms a mean\nbaseline predictor and report on a human study that shows that we can decode\nphotofits that are visually plausible and close to the observer's mental image.",
          "link": "http://arxiv.org/abs/2108.07524",
          "publishedOn": "2021-08-18T01:55:01.365Z",
          "wordCount": 606,
          "title": "Neural Photofit: Gaze-based Mental Image Reconstruction. (arXiv:2108.07524v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1\">Wenbing Tao</a>",
          "description": "Camera and 3D LiDAR sensors have become indispensable devices in modern\nautonomous driving vehicles, where the camera provides the fine-grained\ntexture, color information in 2D space and LiDAR captures more precise and\nfarther-away distance measurements of the surrounding environments. The\ncomplementary information from these two sensors makes the two-modality fusion\nbe a desired option. However, two major issues of the fusion between camera and\nLiDAR hinder its performance, \\ie, how to effectively fuse these two modalities\nand how to precisely align them (suffering from the weak spatiotemporal\nsynchronization problem). In this paper, we propose a coarse-to-fine LiDAR and\ncamera fusion-based network (termed as LIF-Seg) for LiDAR segmentation. For the\nfirst issue, unlike these previous works fusing the point cloud and image\ninformation in a one-to-one manner, the proposed method fully utilizes the\ncontextual information of images and introduces a simple but effective\nearly-fusion strategy. Second, due to the weak spatiotemporal synchronization\nproblem, an offset rectification approach is designed to align these\ntwo-modality features. The cooperation of these two components leads to the\nsuccess of the effective camera-LiDAR fusion. Experimental results on the\nnuScenes dataset show the superiority of the proposed LIF-Seg over existing\nmethods with a large margin. Ablation studies and analyses demonstrate that our\nproposed LIF-Seg can effectively tackle the weak spatiotemporal synchronization\nproblem.",
          "link": "http://arxiv.org/abs/2108.07511",
          "publishedOn": "2021-08-18T01:55:01.328Z",
          "wordCount": 661,
          "title": "LIF-Seg: LiDAR and Camera Image Fusion for 3D LiDAR Semantic Segmentation. (arXiv:2108.07511v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontanini_T/0/1/0/all/0/1\">Tomaso Fontanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donati_L/0/1/0/all/0/1\">Luca Donati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prati_A/0/1/0/all/0/1\">Andrea Prati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>",
          "description": "Gradient-based attention modeling has been used widely as a way to visualize\nand understand convolutional neural networks. However, exploiting these visual\nexplanations during the training of generative adversarial networks (GANs) is\nan unexplored area in computer vision research. Indeed, we argue that this kind\nof information can be used to influence GANs training in a positive way. For\nthis reason, in this paper, it is shown how gradient based attentions can be\nused as knowledge to be conveyed in a teacher-student paradigm for multi-domain\nimage-to-image translation tasks in order to improve the results of the student\narchitecture. Further, it is demonstrated how \"pseudo\"-attentions can also be\nemployed during training when teacher and student networks are trained on\ndifferent domains which share some similarities. The approach is validated on\nmulti-domain facial attributes transfer and human expression synthesis showing\nboth qualitative and quantitative results.",
          "link": "http://arxiv.org/abs/2108.07466",
          "publishedOn": "2021-08-18T01:55:01.272Z",
          "wordCount": 590,
          "title": "Transferring Knowledge with Attention Distillation for Multi-Domain Image-to-Image Translation. (arXiv:2108.07466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangfei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1\">Tzu-Yi Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>",
          "description": "Weakly supervised image segmentation trained with image-level labels usually\nsuffers from inaccurate coverage of object areas during the generation of the\npseudo groundtruth. This is because the object activation maps are trained with\nthe classification objective and lack the ability to generalize. To improve the\ngenerality of the objective activation maps, we propose a region prototypical\nnetwork RPNet to explore the cross-image object diversity of the training set.\nSimilar object parts across images are identified via region feature\ncomparison. Object confidence is propagated between regions to discover new\nobject areas while background regions are suppressed. Experiments show that the\nproposed method generates more complete and accurate pseudo object masks, while\nachieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. In\naddition, we investigate the robustness of the proposed method on reduced\ntraining sets.",
          "link": "http://arxiv.org/abs/2108.07413",
          "publishedOn": "2021-08-18T01:55:01.259Z",
          "wordCount": 576,
          "title": "Cross-Image Region Mining with Region Prototypical Network for Weakly Supervised Segmentation. (arXiv:2108.07413v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Shuang Mei</a>",
          "description": "The existing particle image velocimetry (PIV) do not consider the curvature\neffect of the non-straight particle trajectory, because it seems to be\nimpossible to obtain the curvature information from a pair of particle images.\nAs a result, the computed vector underestimates the real velocity due to the\nstraight-line approximation, that further causes a systematic error for the PIV\ninstrument. In this work, the particle curved trajectory between two recordings\nis firstly explained with the streamline segment of a steady flow\n(diffeomorphic transformation) instead of a single vector, and this idea is\ntermed as diffeomorphic PIV. Specifically, a deformation field is introduced to\ndescribe the particle displacement, i.e., we try to find the optimal velocity\nfield, of which the corresponding deformation vector field agrees with the\nparticle displacement. Because the variation of the deformation function can be\napproximated with the variation of the velocity function, the diffeomorphic PIV\ncan be implemented as iterative PIV. That says, the diffeomorphic PIV warps the\nimages with deformation vector field instead of the velocity, and keeps the\nrest as same as iterative PIVs. Two diffeomorphic deformation schemes --\nforward diffeomorphic deformation interrogation (FDDI) and central\ndiffeomorphic deformation interrogation (CDDI) -- are proposed. Tested on\nsynthetic images, the FDDI achieves significant accuracy improvement across\ndifferent one-pass displacement estimators (cross-correlation, optical flow,\ndeep learning flow). Besides, the results on three real PIV image pairs\ndemonstrate the non-negligible curvature effect for CDI-based PIV, and our FDDI\nprovides larger velocity estimation (more accurate) in the fast curvy\nstreamline areas. The accuracy improvement of the combination of FDDI and\naccurate dense estimator means that our diffeomorphic PIV paves a new way for\ncomplex flow measurement.",
          "link": "http://arxiv.org/abs/2108.07438",
          "publishedOn": "2021-08-18T01:55:01.241Z",
          "wordCount": 710,
          "title": "Diffeomorphic Particle Image Velocimetry. (arXiv:2108.07438v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyunjong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1\">Bumsub Ham</a>",
          "description": "We address the problem of visible-infrared person re-identification\n(VI-reID), that is, retrieving a set of person images, captured by visible or\ninfrared cameras, in a cross-modal setting. Two main challenges in VI-reID are\nintra-class variations across person images, and cross-modal discrepancies\nbetween visible and infrared images. Assuming that the person images are\nroughly aligned, previous approaches attempt to learn coarse image- or rigid\npart-level person representations that are discriminative and generalizable\nacross different modalities. However, the person images, typically cropped by\noff-the-shelf object detectors, are not necessarily well-aligned, which\ndistract discriminative person representation learning. In this paper, we\nintroduce a novel feature learning framework that addresses these problems in a\nunified way. To this end, we propose to exploit dense correspondences between\ncross-modal person images. This allows to address the cross-modal discrepancies\nin a pixel-level, suppressing modality-related features from person\nrepresentations more effectively. This also encourages pixel-wise associations\nbetween cross-modal local features, further facilitating discriminative feature\nlearning for VI-reID. Extensive experiments and analyses on standard VI-reID\nbenchmarks demonstrate the effectiveness of our approach, which significantly\noutperforms the state of the art.",
          "link": "http://arxiv.org/abs/2108.07422",
          "publishedOn": "2021-08-18T01:55:01.234Z",
          "wordCount": 622,
          "title": "Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences. (arXiv:2108.07422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+AlQuabeh_H/0/1/0/all/0/1\">Hilal AlQuabeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawazeer_A/0/1/0/all/0/1\">Ameera Bawazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhashmi_A/0/1/0/all/0/1\">Abdulateef Alhashmi</a>",
          "description": "Data labeling in supervised learning is considered an expensive and\ninfeasible tool in some conditions. The self-supervised learning method is\nproposed to tackle the learning effectiveness with fewer labeled data, however,\nthere is a lack of confidence in the size of labeled data needed to achieve\nadequate results. This study aims to draw a baseline on the proportion of the\nlabeled data that models can appreciate to yield competent accuracy when\ncompared to training with additional labels. The study implements the\nkaggle.com' cats-vs-dogs dataset, Mnist and Fashion-Mnist to investigate the\nself-supervised learning task by implementing random rotations augmentation on\nthe original datasets. To reveal the true effectiveness of the pretext process\nin self-supervised learning, the original dataset is divided into smaller\nbatches, and learning is repeated on each batch with and without the pretext\npre-training. Results show that the pretext process in the self-supervised\nlearning improves the accuracy around 15% in the downstream classification task\nwhen compared to the plain supervised learning.",
          "link": "http://arxiv.org/abs/2108.07464",
          "publishedOn": "2021-08-18T01:55:01.207Z",
          "wordCount": 613,
          "title": "Investigating a Baseline Of Self Supervised Learning Towards Reducing Labeling Costs For Image Classification. (arXiv:2108.07464v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huabin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuangrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>",
          "description": "The crux of self-supervised video representation learning is to build general\nfeatures from unlabeled videos. However, most recent works have mainly focused\non high-level semantics and neglected lower-level representations and their\ntemporal relationship which are crucial for general video understanding. To\naddress these challenges, this paper proposes a multi-level feature\noptimization framework to improve the generalization and temporal modeling\nability of learned video representations. Concretely, high-level features\nobtained from naive and prototypical contrastive learning are utilized to build\ndistribution graphs, guiding the process of low-level and mid-level feature\nlearning. We also devise a simple temporal modeling module from multi-level\nfeatures to enhance motion pattern learning. Experiments demonstrate that\nmulti-level feature optimization with the graph constraint and temporal\nmodeling can greatly improve the representation ability in video understanding.\nCode is available at\nhttps://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization.",
          "link": "http://arxiv.org/abs/2108.02183",
          "publishedOn": "2021-08-18T01:55:01.057Z",
          "wordCount": 600,
          "title": "Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization. (arXiv:2108.02183v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1\">Samuel G. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "Automatic augmentation methods have recently become a crucial pillar for\nstrong model performance in vision tasks. While existing automatic augmentation\nmethods need to trade off simplicity, cost and performance, we present a most\nsimple baseline, TrivialAugment, that outperforms previous methods for almost\nfree. TrivialAugment is parameter-free and only applies a single augmentation\nto each image. Thus, TrivialAugment's effectiveness is very unexpected to us\nand we performed very thorough experiments to study its performance. First, we\ncompare TrivialAugment to previous state-of-the-art methods in a variety of\nimage classification scenarios. Then, we perform multiple ablation studies with\ndifferent augmentation spaces, augmentation methods and setups to understand\nthe crucial requirements for its performance. Additionally, we provide a simple\ninterface to facilitate the widespread adoption of automatic augmentation\nmethods, as well as our full code base for reproducibility. Since our work\nreveals a stagnation in many parts of automatic augmentation research, we end\nwith a short proposal of best practices for sustained future progress in\nautomatic augmentation methods.",
          "link": "http://arxiv.org/abs/2103.10158",
          "publishedOn": "2021-08-18T01:55:01.026Z",
          "wordCount": 630,
          "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation. (arXiv:2103.10158v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11834",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>",
          "description": "Time-lapse fluorescence microscopy (TLFM) is an important and powerful tool\nin synthetic biological research. Modeling TLFM experiments based on real data\nmay enable researchers to repeat certain experiments with minor effort. This\nthesis is a study towards deep learning-based modeling of TLFM experiments on\nthe image level. The modeling of TLFM experiments, by way of the example of\ntrapped yeast cells, is split into two tasks. The first task is to generate\nsynthetic image data based on real image data. To approach this problem, a\nnovel generative adversarial network, for conditionalized and unconditionalized\nimage generation, is proposed. The second task is the simulation of brightfield\nmicroscopy images over multiple discrete time-steps. To tackle this simulation\ntask an advanced future frame prediction model is introduced. The proposed\nmodels are trained and tested on a novel dataset that is presented in this\nthesis. The obtained results showed that the modeling of TLFM experiments, with\ndeep learning, is a proper approach, but requires future research to\neffectively model real-world experiments.",
          "link": "http://arxiv.org/abs/2103.11834",
          "publishedOn": "2021-08-18T01:55:00.994Z",
          "wordCount": 643,
          "title": "Generation and Simulation of Yeast Microscopy Imagery with Deep Learning. (arXiv:2103.11834v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1\">Denys Rozumnyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sroubek_F/0/1/0/all/0/1\">Filip Sroubek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>",
          "description": "We propose the first learning-based approach for fast moving objects\ndetection. Such objects are highly blurred and move over large distances within\none video frame. Fast moving objects are associated with a deblurring and\nmatting problem, also called deblatting. We show that the separation of\ndeblatting into consecutive matting and deblurring allows achieving real-time\nperformance, i.e. an order of magnitude speed-up, and thus enabling new classes\nof application. The proposed method detects fast moving objects as a truncated\ndistance function to the trajectory by learning from synthetic data. For the\nsharp appearance estimation and accurate trajectory estimation, we propose a\nmatting and fitting network that estimates the blurred appearance without\nbackground, followed by an energy minimization based deblurring. The\nstate-of-the-art methods are outperformed in terms of recall, precision,\ntrajectory estimation, and sharp appearance reconstruction. Compared to other\nmethods, such as deblatting, the inference is of several orders of magnitude\nfaster and allows applications such as real-time fast moving object detection\nand retrieval in large video collections.",
          "link": "http://arxiv.org/abs/2012.08216",
          "publishedOn": "2021-08-18T01:55:00.962Z",
          "wordCount": 643,
          "title": "FMODetect: Robust Detection of Fast Moving Objects. (arXiv:2012.08216v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pinyan Lu</a>",
          "description": "Graph matching (GM) has been a building block in many areas including\ncomputer vision and pattern recognition. Despite the recent impressive\nprogress, existing deep GM methods often have difficulty in handling outliers\nin both graphs, which are ubiquitous in practice. We propose a deep\nreinforcement learning (RL) based approach RGM for weighted graph matching,\nwhose sequential node matching scheme naturally fits with the strategy for\nselective inlier matching against outliers, and supports seed graph matching. A\nrevocable action scheme is devised to improve the agent's flexibility against\nthe complex constrained matching task. Moreover, we propose a quadratic\napproximation technique to regularize the affinity matrix, in the presence of\noutliers. As such, the RL agent can finish inlier matching timely when the\nobjective score stop growing, for which otherwise an additional hyperparameter\ni.e. the number of common inliers is needed to avoid matching outliers. In this\npaper, we are focused on learning the back-end solver for the most general form\nof GM: the Lawler's QAP, whose input is the affinity matrix. Our approach can\nalso boost other solvers using the affinity input. Experimental results on both\nsynthetic and real-world datasets showcase its superior performance regarding\nboth matching accuracy and robustness.",
          "link": "http://arxiv.org/abs/2012.08950",
          "publishedOn": "2021-08-18T01:55:00.956Z",
          "wordCount": 686,
          "title": "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching. (arXiv:2012.08950v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klenk_S/0/1/0/all/0/1\">Simon Klenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chui_J/0/1/0/all/0/1\">Jason Chui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>",
          "description": "Event cameras are bio-inspired vision sensors which measure per pixel\nbrightness changes. They offer numerous benefits over traditional, frame-based\ncameras, including low latency, high dynamic range, high temporal resolution\nand low power consumption. Thus, these sensors are suited for robotics and\nvirtual reality applications. To foster the development of 3D perception and\nnavigation algorithms with event cameras, we present the TUM-VIE dataset. It\nconsists of a large variety of handheld and head-mounted sequences in indoor\nand outdoor environments, including rapid motion during sports and high dynamic\nrange scenarios. The dataset contains stereo event data, stereo grayscale\nframes at 20Hz as well as IMU data at 200Hz. Timestamps between all sensors are\nsynchronized in hardware. The event cameras contain a large sensor of 1280x720\npixels, which is significantly larger than the sensors used in existing stereo\nevent datasets (at least by a factor of ten). We provide ground truth poses\nfrom a motion capture system at 120Hz during the beginning and end of each\nsequence, which can be used for trajectory evaluation. TUM-VIE includes\nchallenging sequences where state-of-the art visual SLAM algorithms either fail\nor result in large drift. Hence, our dataset can help to push the boundary of\nfuture research on event-based visual-inertial perception algorithms.",
          "link": "http://arxiv.org/abs/2108.07329",
          "publishedOn": "2021-08-18T01:55:00.917Z",
          "wordCount": 648,
          "title": "TUM-VIE: The TUM Stereo Visual-Inertial Event Dataset. (arXiv:2108.07329v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1\">Ange Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>",
          "description": "Segmenting medical images accurately and reliably is important for disease\ndiagnosis and treatment. It is a challenging task because of the wide variety\nof objects' sizes, shapes, and scanning modalities. Recently, many\nconvolutional neural networks (CNN) have been designed for segmentation tasks\nand achieved great success. Few studies, however, have fully considered the\nsizes of objects and thus most demonstrate poor performance on segmentation of\nsmall objects segmentation. This can have significant impact on early detection\nof disease. This paper proposes a Context Axial Reserve Attention Network\n(CaraNet) to improve the segmentation performance on small objects compared\nwith recent state-of-the-art models. We test our CaraNet on brain tumor (BraTS\n2018) and polyp (Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300 and\nETIS-LaribPolypDB) segmentation. Our CaraNet not only achieves the top-rank\nmean Dice segmentation accuracy, but also shows a distinct advantage in\nsegmentation of small medical objects.",
          "link": "http://arxiv.org/abs/2108.07368",
          "publishedOn": "2021-08-18T01:55:00.909Z",
          "wordCount": 598,
          "title": "CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects. (arXiv:2108.07368v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "We present BN-NAS, neural architecture search with Batch Normalization\n(BN-NAS), to accelerate neural architecture search (NAS). BN-NAS can\nsignificantly reduce the time required by model training and evaluation in NAS.\nSpecifically, for fast evaluation, we propose a BN-based indicator for\npredicting subnet performance at a very early training stage. The BN-based\nindicator further facilitates us to improve the training efficiency by only\ntraining the BN parameters during the supernet training. This is based on our\nobservation that training the whole supernet is not necessary while training\nonly BN parameters accelerates network convergence for network architecture\nsearch. Extensive experiments show that our method can significantly shorten\nthe time of training supernet by more than 10 times and shorten the time of\nevaluating subnets by more than 600,000 times without losing accuracy.",
          "link": "http://arxiv.org/abs/2108.07375",
          "publishedOn": "2021-08-18T01:55:00.886Z",
          "wordCount": 574,
          "title": "BN-NAS: Neural Architecture Search with Batch Normalization. (arXiv:2108.07375v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenju Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruisheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "The paper proposes a Dynamic ResBlock Generative Adversarial Network\n(DRB-GAN) for artistic style transfer. The style code is modeled as the shared\nparameters for Dynamic ResBlocks connecting both the style encoding network and\nthe style transfer network. In the style encoding network, a style class-aware\nattention mechanism is used to attend the style feature representation for\ngenerating the style codes. In the style transfer network, multiple Dynamic\nResBlocks are designed to integrate the style code and the extracted CNN\nsemantic feature and then feed into the spatial window Layer-Instance\nNormalization (SW-LIN) decoder, which enables high-quality synthetic images\nwith artistic style transfer. Moreover, the style collection conditional\ndiscriminator is designed to equip our DRB-GAN model with abilities for both\narbitrary style transfer and collection style transfer during the training\nstage. No matter for arbitrary style transfer or collection style transfer,\nextensive experiments strongly demonstrate that our proposed DRB-GAN\noutperforms state-of-the-art methods and exhibits its superior performance in\nterms of visual quality and efficiency. Our source code is available at\n\\color{magenta}{\\url{https://github.com/xuwenju123/DRB-GAN}}.",
          "link": "http://arxiv.org/abs/2108.07379",
          "publishedOn": "2021-08-18T01:55:00.880Z",
          "wordCount": 619,
          "title": "DRB-GAN: A Dynamic ResBlock Generative Adversarial Network for Artistic Style Transfer. (arXiv:2108.07379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duta_I/0/1/0/all/0/1\">Ionut Cosmin Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "We propose contextual convolution (CoConv) for visual recognition. CoConv is\na direct replacement of the standard convolution, which is the core component\nof convolutional neural networks. CoConv is implicitly equipped with the\ncapability of incorporating contextual information while maintaining a similar\nnumber of parameters and computational cost compared to the standard\nconvolution. CoConv is inspired by neuroscience studies indicating that (i)\nneurons, even from the primary visual cortex (V1 area), are involved in\ndetection of contextual cues and that (ii) the activity of a visual neuron can\nbe influenced by the stimuli placed entirely outside of its theoretical\nreceptive field. On the one hand, we integrate CoConv in the widely-used\nresidual networks and show improved recognition performance over baselines on\nthe core tasks and benchmarks for visual recognition, namely image\nclassification on the ImageNet data set and object detection on the MS COCO\ndata set. On the other hand, we introduce CoConv in the generator of a\nstate-of-the-art Generative Adversarial Network, showing improved generative\nresults on CIFAR-10 and CelebA. Our code is available at\nhttps://github.com/iduta/coconv.",
          "link": "http://arxiv.org/abs/2108.07387",
          "publishedOn": "2021-08-18T01:55:00.846Z",
          "wordCount": 621,
          "title": "Contextual Convolutional Neural Networks. (arXiv:2108.07387v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "Collecting large annotated datasets in Remote Sensing is often expensive and\nthus can become a major obstacle for training advanced machine learning models.\nCommon techniques of addressing this issue, based on the underlying idea of\npre-training the Deep Neural Networks (DNN) on freely available large datasets,\ncannot be used for Remote Sensing due to the unavailability of such large-scale\nlabeled datasets and the heterogeneity of data sources caused by the varying\nspatial and spectral resolution of different sensors. Self-supervised learning\nis an alternative approach that learns feature representation from unlabeled\nimages without using any human annotations. In this paper, we introduce a new\nmethod for land cover mapping by using a clustering based pretext task for\nself-supervised learning. We demonstrate the effectiveness of the method on two\nsocietally relevant applications from the aspect of segmentation performance,\ndiscriminative feature representation learning and the underlying cluster\nstructure. We also show the effectiveness of the active sampling using the\nclusters obtained from our method in improving the mapping accuracy given a\nlimited budget of annotating.",
          "link": "http://arxiv.org/abs/2108.07323",
          "publishedOn": "2021-08-18T01:55:00.840Z",
          "wordCount": 623,
          "title": "Clustering augmented Self-Supervised Learning: Anapplication to Land Cover Mapping. (arXiv:2108.07323v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leo Sampaio Ferraz Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Ponti</a>",
          "description": "Scene Designer is a novel method for searching and generating images using\nfree-hand sketches of scene compositions; i.e. drawings that describe both the\nappearance and relative positions of objects. Our core contribution is a single\nunified model to learn both a cross-modal search embedding for matching\nsketched compositions to images, and an object embedding for layout synthesis.\nWe show that a graph neural network (GNN) followed by Transformer under our\nnovel contrastive learning setting is required to allow learning correlations\nbetween object type, appearance and arrangement, driving a mask generation\nmodule that synthesises coherent scene layouts, whilst also delivering state of\nthe art sketch based visual search of scenes.",
          "link": "http://arxiv.org/abs/2108.07353",
          "publishedOn": "2021-08-18T01:55:00.832Z",
          "wordCount": 571,
          "title": "Scene Designer: a Unified Model for Scene Search and Synthesis from Sketch. (arXiv:2108.07353v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>",
          "description": "With the help of the deep learning paradigm, many point cloud networks have\nbeen invented for visual analysis. However, there is great potential for\ndevelopment of these networks since the given information of point cloud data\nhas not been fully exploited. To improve the effectiveness of existing networks\nin analyzing point cloud data, we propose a plug-and-play module, PnP-3D,\naiming to refine the fundamental point cloud feature representations by\ninvolving more local context and global bilinear response from explicit 3D\nspace and implicit feature space. To thoroughly evaluate our approach, we\nconduct experiments on three standard point cloud analysis tasks, including\nclassification, semantic segmentation, and object detection, where we select\nthree state-of-the-art networks from each task for evaluation. Serving as a\nplug-and-play module, PnP-3D can significantly boost the performances of\nestablished networks. In addition to achieving state-of-the-art results on four\nwidely used point cloud benchmarks, we present comprehensive ablation studies\nand visualizations to demonstrate our approach's advantages. The code will be\navailable at https://github.com/ShiQiu0419/pnp-3d.",
          "link": "http://arxiv.org/abs/2108.07378",
          "publishedOn": "2021-08-18T01:55:00.812Z",
          "wordCount": 598,
          "title": "PnP-3D: A Plug-and-Play for 3D Point Clouds. (arXiv:2108.07378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+OBrien_M/0/1/0/all/0/1\">Molly O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medoff_M/0/1/0/all/0/1\">Mike Medoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukowski_J/0/1/0/all/0/1\">Julia Bukowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Greg Hager</a>",
          "description": "It is well known that Neural Network (network) performance often degrades\nwhen a network is used in novel operating domains that differ from its training\nand testing domains. This is a major limitation, as networks are being\nintegrated into safety critical, cyber-physical systems that must work in\nunconstrained environments, e.g., perception for autonomous vehicles. Training\nnetworks that generalize to novel operating domains and that extract robust\nfeatures is an active area of research, but previous work fails to predict what\nthe network performance will be in novel operating domains. We propose the task\nNetwork Generalization Prediction: predicting the expected network performance\nin novel operating domains. We describe the network performance in terms of an\ninterpretable Context Subspace, and we propose a methodology for selecting the\nfeatures of the Context Subspace that provide the most information about the\nnetwork performance. We identify the Context Subspace for a pretrained Faster\nRCNN network performing pedestrian detection on the Berkeley Deep Drive (BDD)\nDataset, and demonstrate Network Generalization Prediction accuracy within 5%\nor less of observed performance. We also demonstrate that the Context Subspace\nfrom the BDD Dataset is informative for completely unseen datasets, JAAD and\nCityscapes, where predictions have a bias of 10% or less.",
          "link": "http://arxiv.org/abs/2108.07399",
          "publishedOn": "2021-08-18T01:55:00.806Z",
          "wordCount": 643,
          "title": "Network Generalization Prediction for Safety Critical Tasks in Novel Operating Domains. (arXiv:2108.07399v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mantang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>",
          "description": "In this paper, we tackle the problem of dense light field (LF) reconstruction\nfrom sparsely-sampled ones with wide baselines and propose a learnable model,\nnamely dynamic interpolation, to replace the commonly-used geometry warping\noperation. Specifically, with the estimated geometric relation between input\nviews, we first construct a lightweight neural network to dynamically learn\nweights for interpolating neighbouring pixels from input views to synthesize\neach pixel of novel views independently. In contrast to the fixed and\ncontent-independent weights employed in the geometry warping operation, the\nlearned interpolation weights implicitly incorporate the correspondences\nbetween the source and novel views and adapt to different image content\ninformation. Then, we recover the spatial correlation between the independently\nsynthesized pixels of each novel view by referring to that of input views using\na geometry-based spatial refinement module. We also constrain the angular\ncorrelation between the novel views through a disparity-oriented LF structure\nloss. Experimental results on LF datasets with wide baselines show that the\nreconstructed LFs achieve much higher PSNR/SSIM and preserve the LF parallax\nstructure better than state-of-the-art methods. The source code is publicly\navailable at https://github.com/MantangGuo/DI4SLF.",
          "link": "http://arxiv.org/abs/2108.07408",
          "publishedOn": "2021-08-18T01:55:00.740Z",
          "wordCount": 626,
          "title": "Learning Dynamic Interpolation for Extremely Sparse Light Fields with Wide Baselines. (arXiv:2108.07408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poliarnyi_N/0/1/0/all/0/1\">Nikolai Poliarnyi</a>",
          "description": "We present an out-of-core variational approach for surface reconstruction\nfrom a set of aligned depth maps. Input depth maps are supposed to be\nreconstructed from regular photos or/and can be a representation of terrestrial\nLIDAR point clouds. Our approach is based on surface reconstruction via total\ngeneralized variation minimization ($TGV$) because of its strong\nvisibility-based noise-filtering properties and GPU-friendliness. Our main\ncontribution is an out-of-core OpenCL-accelerated adaptation of this numerical\nalgorithm which can handle arbitrarily large real-world scenes with scale\ndiversity.",
          "link": "http://arxiv.org/abs/2107.14790",
          "publishedOn": "2021-08-17T01:54:53.920Z",
          "wordCount": 560,
          "title": "Out-of-Core Surface Reconstruction via Global $TGV$ Minimization. (arXiv:2107.14790v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03002",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>",
          "description": "More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.",
          "link": "http://arxiv.org/abs/2108.03002",
          "publishedOn": "2021-08-17T01:54:53.631Z",
          "wordCount": 666,
          "title": "Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>",
          "description": "Understanding complex social interactions among agents is a key challenge for\ntrajectory prediction. Most existing methods consider the interactions between\npairwise traffic agents or in a local area, while the nature of interactions is\nunlimited, involving an uncertain number of agents and non-local areas\nsimultaneously. Besides, they treat heterogeneous traffic agents the same,\nnamely those among agents of different categories, while neglecting people's\ndiverse reaction patterns toward traffic agents in ifferent categories. To\naddress these problems, we propose a simple yet effective Unlimited\nNeighborhood Interaction Network (UNIN), which predicts trajectories of\nheterogeneous agents in multiple categories. Specifically, the proposed\nunlimited neighborhood interaction module generates the fused-features of all\nagents involved in an interaction simultaneously, which is adaptive to any\nnumber of agents and any range of interaction area. Meanwhile, a hierarchical\ngraph attention module is proposed to obtain category-to-category interaction\nand agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model\nare estimated for generating the future trajectories. Extensive experimental\nresults on benchmark datasets demonstrate a significant performance improvement\nof our method over the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00238",
          "publishedOn": "2021-08-17T01:54:53.614Z",
          "wordCount": 634,
          "title": "Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction. (arXiv:2108.00238v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_A/0/1/0/all/0/1\">Austin Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>",
          "description": "Visual engagement in social media platforms comprises interactions with photo\nposts including comments, shares, and likes. In this paper, we leverage such\nvisual engagement clues as supervisory signals for representation learning.\nHowever, learning from engagement signals is non-trivial as it is not clear how\nto bridge the gap between low-level visual information and high-level social\ninteractions. We present VisE, a weakly supervised learning approach, which\nmaps social images to pseudo labels derived by clustered engagement signals. We\nthen study how models trained in this way benefit subjective downstream\ncomputer vision tasks such as emotion recognition or political bias detection.\nThrough extensive studies, we empirically demonstrate the effectiveness of VisE\nacross a diverse set of classification tasks beyond the scope of conventional\nrecognition.",
          "link": "http://arxiv.org/abs/2104.07767",
          "publishedOn": "2021-08-17T01:54:53.597Z",
          "wordCount": 600,
          "title": "Exploring Visual Engagement Signals for Representation Learning. (arXiv:2104.07767v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nesti_F/0/1/0/all/0/1\">Federico Nesti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_A/0/1/0/all/0/1\">Alessandro Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttazzo_G/0/1/0/all/0/1\">Giorgio Buttazzo</a>",
          "description": "Over the last few years, convolutional neural networks (CNNs) have proved to\nreach super-human performance in visual recognition tasks. However, CNNs can\neasily be fooled by adversarial examples, i.e., maliciously-crafted images that\nforce the networks to predict an incorrect output while being extremely similar\nto those for which a correct output is predicted. Regular adversarial examples\nare not robust to input image transformations, which can then be used to detect\nwhether an adversarial example is presented to the network. Nevertheless, it is\nstill possible to generate adversarial examples that are robust to such\ntransformations.\n\nThis paper extensively explores the detection of adversarial examples via\nimage transformations and proposes a novel methodology, called \\textit{defense\nperturbation}, to detect robust adversarial examples with the same input\ntransformations the adversarial examples are robust to. Such a \\textit{defense\nperturbation} is shown to be an effective counter-measure to robust adversarial\nexamples.\n\nFurthermore, multi-network adversarial examples are introduced. This kind of\nadversarial examples can be used to simultaneously fool multiple networks,\nwhich is critical in systems that use network redundancy, such as those based\non architectures with majority voting over multiple CNNs. An extensive set of\nexperiments based on state-of-the-art CNNs trained on the Imagenet dataset is\nfinally reported.",
          "link": "http://arxiv.org/abs/2101.11466",
          "publishedOn": "2021-08-17T01:54:53.571Z",
          "wordCount": 665,
          "title": "Detecting Adversarial Examples by Input Transformations, Defense Perturbations, and Voting. (arXiv:2101.11466v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Aming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Few-shot object detection (FSOD) aims to strengthen the performance of novel\nobject detection with few labeled samples. To alleviate the constraint of few\nsamples, enhancing the generalization ability of learned features for novel\nobjects plays a key role. Thus, the feature learning process of FSOD should\nfocus more on intrinsical object characteristics, which are invariant under\ndifferent visual changes and therefore are helpful for feature generalization.\nUnlike previous attempts of the meta-learning paradigm, in this paper, we\nexplore how to enhance object features with intrinsical characteristics that\nare universal across different object categories. We propose a new prototype,\nnamely universal prototype, that is learned from all object categories. Besides\nthe advantage of characterizing invariant characteristics, the universal\nprototypes alleviate the impact of unbalanced object categories. After\nenhancing object features with the universal prototypes, we impose a\nconsistency loss to maximize the agreement between the enhanced features and\nthe original ones, which is beneficial for learning invariant object\ncharacteristics. Thus, we develop a new framework of few-shot object detection\nwith universal prototypes ({FSOD}^{up}) that owns the merit of feature\ngeneralization towards novel objects. Experimental results on PASCAL VOC and MS\nCOCO show the effectiveness of {FSOD}^{up}. Particularly, for the 1-shot case\nof VOC Split2, {FSOD}^{up} outperforms the baseline by 6.8% in terms of mAP.",
          "link": "http://arxiv.org/abs/2103.01077",
          "publishedOn": "2021-08-17T01:54:53.564Z",
          "wordCount": 679,
          "title": "Universal-Prototype Enhancing for Few-Shot Object Detection. (arXiv:2103.01077v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiahong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Miao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "With the advance of deep learning technology, automatic video generation from\naudio or text has become an emerging and promising research topic. In this\npaper, we present a novel approach to synthesize video from the text. The\nmethod builds a phoneme-pose dictionary and trains a generative adversarial\nnetwork (GAN) to generate video from interpolated phoneme poses. Compared to\naudio-driven video generation algorithms, our approach has a number of\nadvantages: 1) It only needs a fraction of the training data used by an\naudio-driven approach; 2) It is more flexible and not subject to vulnerability\ndue to speaker variation; 3) It significantly reduces the preprocessing,\ntraining and inference time. We perform extensive experiments to compare the\nproposed method with state-of-the-art talking face generation methods on a\nbenchmark dataset and datasets of our own. The results demonstrate the\neffectiveness and superiority of our approach.",
          "link": "http://arxiv.org/abs/2104.14631",
          "publishedOn": "2021-08-17T01:54:53.547Z",
          "wordCount": 617,
          "title": "Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme-Pose Dictionary. (arXiv:2104.14631v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1\">Kea Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1\">Tyler Derr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "While image understanding on recognition-level has achieved remarkable\nadvancements, reliable visual scene understanding requires comprehensive image\nunderstanding on recognition-level but also cognition-level, which calls for\nexploiting the multi-source information as well as learning different levels of\nunderstanding and extensive commonsense knowledge. In this paper, we propose a\nnovel Cognitive Attention Network (CAN) for visual commonsense reasoning to\nachieve interpretable visual understanding. Specifically, we first introduce an\nimage-text fusion module to fuse information from images and text collectively.\nSecond, a novel inference module is designed to encode commonsense among image,\nquery and response. Extensive experiments on large-scale Visual Commonsense\nReasoning (VCR) benchmark dataset demonstrate the effectiveness of our\napproach. The implementation is publicly available at\nhttps://github.com/tanjatang/CAN",
          "link": "http://arxiv.org/abs/2108.02924",
          "publishedOn": "2021-08-17T01:54:53.528Z",
          "wordCount": 579,
          "title": "Interpretable Visual Understanding with Cognitive Attention Network. (arXiv:2108.02924v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Harim Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Myeong-Seok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "The 3D Morphable Model (3DMM), which is a Principal Component Analysis (PCA)\nbased statistical model that represents a 3D face using linear basis functions,\nhas shown promising results for reconstructing 3D faces from single-view\nin-the-wild images. However, 3DMM has restricted representation power due to\nthe limited number of 3D scans and the global linear basis. To address the\nlimitations of 3DMM, we propose a straightforward learning-based method that\nreconstructs a 3D face mesh through Free-Form Deformation (FFD) for the first\ntime. FFD is a geometric modeling method that embeds a reference mesh within a\nparallelepiped grid and deforms the mesh by moving the sparse control points of\nthe grid. As FFD is based on mathematically defined basis functions, it has no\nlimitation in representation power. Thus, we can recover accurate 3D face\nmeshes by estimating appropriate deviation of control points as deformation\nparameters. Although both 3DMM and FFD are parametric models, it is difficult\nto predict the effect of the 3DMM parameters on the face shape, while the\ndeformation parameters of FFD are interpretable in terms of their effect on the\nfinal shape of the mesh. This practical advantage of FFD allows the resulting\nmesh and control points to serve as a good starting point for 3D face modeling,\nin that ordinary users can fine-tune the mesh by using widely available 3D\nsoftware tools. Experiments on multiple datasets demonstrate how our method\nsuccessfully estimates the 3D face geometry and facial expressions from 2D face\nimages, achieving comparable performance to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.14857",
          "publishedOn": "2021-08-17T01:54:53.522Z",
          "wordCount": 720,
          "title": "Learning Free-Form Deformation for 3D Face Reconstruction from In-The-Wild Images. (arXiv:2105.14857v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Massiceti_D/0/1/0/all/0/1\">Daniela Massiceti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zintgraf_L/0/1/0/all/0/1\">Luisa Zintgraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronskill_J/0/1/0/all/0/1\">John Bronskill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodorou_L/0/1/0/all/0/1\">Lida Theodorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_M/0/1/0/all/0/1\">Matthew Tobias Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutrell_E/0/1/0/all/0/1\">Edward Cutrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_C/0/1/0/all/0/1\">Cecily Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpf_S/0/1/0/all/0/1\">Simone Stumpf</a>",
          "description": "Object recognition has made great advances in the last decade, but\npredominately still relies on many high-quality training examples per object\ncategory. In contrast, learning new objects from only a few examples could\nenable many impactful applications from robotics to user personalization. Most\nfew-shot learning research, however, has been driven by benchmark datasets that\nlack the high variation that these applications will face when deployed in the\nreal-world. To close this gap, we present the ORBIT dataset and benchmark,\ngrounded in the real-world application of teachable object recognizers for\npeople who are blind/low-vision. The dataset contains 3,822 videos of 486\nobjects recorded by people who are blind/low-vision on their mobile phones. The\nbenchmark reflects a realistic, highly challenging recognition problem,\nproviding a rich playground to drive research in robustness to few-shot,\nhigh-variation conditions. We set the benchmark's first state-of-the-art and\nshow there is massive scope for further innovation, holding the potential to\nimpact a broad range of real-world vision applications including tools for the\nblind/low-vision community. We release the dataset at\nhttps://doi.org/10.25383/city.14294597 and benchmark code at\nhttps://github.com/microsoft/ORBIT-Dataset.",
          "link": "http://arxiv.org/abs/2104.03841",
          "publishedOn": "2021-08-17T01:54:53.516Z",
          "wordCount": 689,
          "title": "ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition. (arXiv:2104.03841v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shitong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>",
          "description": "Point clouds acquired from scanning devices are often perturbed by noise,\nwhich affects downstream tasks such as surface reconstruction and analysis. The\ndistribution of a noisy point cloud can be viewed as the distribution of a set\nof noise-free samples $p(x)$ convolved with some noise model $n$, leading to\n$(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy\npoint cloud, we propose to increase the log-likelihood of each point from $p *\nn$ via gradient ascent -- iteratively updating each point's position. Since $p\n* n$ is unknown at test-time, and we only need the score (i.e., the gradient of\nthe log-probability function) to perform gradient ascent, we propose a neural\nnetwork architecture to estimate the score of $p * n$ given only noisy point\nclouds as input. We derive objective functions for training the network and\ndevelop a denoising algorithm leveraging on the estimated scores. Experiments\ndemonstrate that the proposed model outperforms state-of-the-art methods under\na variety of noise models, and shows the potential to be applied in other tasks\nsuch as point cloud upsampling. The code is available at\n\\url{https://github.com/luost26/score-denoise}.",
          "link": "http://arxiv.org/abs/2107.10981",
          "publishedOn": "2021-08-17T01:54:53.509Z",
          "wordCount": 644,
          "title": "Score-Based Point Cloud Denoising. (arXiv:2107.10981v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>",
          "description": "In this work, we address the problem of unsupervised domain adaptation for\nperson re-ID where annotations are available for the source domain but not for\ntarget. Previous methods typically follow a two-stage optimization pipeline,\nwhere the network is first pre-trained on source and then fine-tuned on target\nwith pseudo labels created by feature clustering. Such methods sustain two main\nlimitations. (1) The label noise may hinder the learning of discriminative\nfeatures for recognizing target classes. (2) The domain gap may hinder\nknowledge transferring from source to target. We propose three types of\ntechnical schemes to alleviate these issues. First, we propose a cluster-wise\ncontrastive learning algorithm (CCL) by iterative optimization of feature\nlearning and cluster refinery to learn noise-tolerant representations in the\nunsupervised manner. Second, we adopt a progressive domain adaptation (PDA)\nstrategy to gradually mitigate the domain gap between source and target data.\nThird, we propose Fourier augmentation (FA) for further maximizing the class\nseparability of re-ID models by imposing extra constraints in the Fourier\nspace. We observe that these proposed schemes are capable of facilitating the\nlearning of discriminative feature representations. Experiments demonstrate\nthat our method consistently achieves notable improvements over the\nstate-of-the-art unsupervised re-ID methods on multiple benchmarks, e.g.,\nsurpassing MMT largely by 8.1\\%, 9.9\\%, 11.4\\% and 11.1\\% mAP on the\nMarket-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks,\nrespectively.",
          "link": "http://arxiv.org/abs/2108.03439",
          "publishedOn": "2021-08-17T01:54:53.503Z",
          "wordCount": 684,
          "title": "Towards Discriminative Representation Learning for Unsupervised Person Re-identification. (arXiv:2108.03439v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>",
          "description": "While the untargeted black-box transferability of adversarial perturbations\nhas been extensively studied before, changing an unseen model's decisions to a\nspecific `targeted' class remains a challenging feat. In this paper, we propose\na new generative approach for highly transferable targeted perturbations\n(\\ours). We note that the existing methods are less suitable for this task due\nto their reliance on class-boundary information that changes from one model to\nanother, thus reducing transferability. In contrast, our approach matches the\nperturbed image `distribution' with that of the target class, leading to high\ntargeted transferability rates. To this end, we propose a new objective\nfunction that not only aligns the global distributions of source and target\nimages, but also matches the local neighbourhood structure between the two\ndomains. Based on the proposed objective, we train a generator function that\ncan adaptively synthesize perturbations specific to a given input. Our\ngenerative approach is independent of the source or target domain labels, while\nconsistently performs well against state-of-the-art methods on a wide range of\nattack settings. As an example, we achieve $32.63\\%$ target transferability\nfrom (an adversarially weak) VGG19$_{BN}$ to (a strong) WideResNet on ImageNet\nval. set, which is 4$\\times$ higher than the previous best generative attack\nand 16$\\times$ better than instance-specific iterative attack. Code is\navailable at: {\\small\\url{https://github.com/Muzammal-Naseer/TTP}}.",
          "link": "http://arxiv.org/abs/2103.14641",
          "publishedOn": "2021-08-17T01:54:53.495Z",
          "wordCount": 688,
          "title": "On Generating Transferable Targeted Perturbations. (arXiv:2103.14641v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>",
          "description": "Human behavior understanding with unmanned aerial vehicles (UAVs) is of great\nsignificance for a wide range of applications, which simultaneously brings an\nurgent demand of large, challenging, and comprehensive benchmarks for the\ndevelopment and evaluation of UAV-based models. However, existing benchmarks\nhave limitations in terms of the amount of captured data, types of data\nmodalities, categories of provided tasks, and diversities of subjects and\nenvironments. Here we propose a new benchmark - UAVHuman - for human behavior\nunderstanding with UAVs, which contains 67,428 multi-modal video sequences and\n119 subjects for action recognition, 22,476 frames for pose estimation, 41,290\nframes and 1,144 identities for person re-identification, and 22,263 frames for\nattribute recognition. Our dataset was collected by a flying UAV in multiple\nurban and rural districts in both daytime and nighttime over three months,\nhence covering extensive diversities w.r.t subjects, backgrounds,\nilluminations, weathers, occlusions, camera motions, and UAV flying attitudes.\nSuch a comprehensive and challenging benchmark shall be able to promote the\nresearch of UAV-based human behavior understanding, including action\nrecognition, pose estimation, re-identification, and attribute recognition.\nFurthermore, we propose a fisheye-based action recognition method that\nmitigates the distortions in fisheye videos via learning unbounded\ntransformations guided by flat RGB videos. Experiments show the efficacy of our\nmethod on the UAV-Human dataset. The project page:\nhttps://github.com/SUTDCV/UAV-Human",
          "link": "http://arxiv.org/abs/2104.00946",
          "publishedOn": "2021-08-17T01:54:53.477Z",
          "wordCount": 716,
          "title": "UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles. (arXiv:2104.00946v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiehong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zewei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songcen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqing Li</a>",
          "description": "Category-level 6D object pose and size estimation is to predict full pose\nconfigurations of rotation, translation, and size for object instances observed\nin single, arbitrary views of cluttered scenes. In this paper, we propose a new\nmethod of Dual Pose Network with refined learning of pose consistency for this\ntask, shortened as DualPoseNet. DualPoseNet stacks two parallel pose decoders\non top of a shared pose encoder, where the implicit decoder predicts object\nposes with a working mechanism different from that of the explicit one; they\nthus impose complementary supervision on the training of pose encoder. We\nconstruct the encoder based on spherical convolutions, and design a module of\nSpherical Fusion wherein for a better embedding of pose-sensitive features from\nthe appearance and shape observations. Given no testing CAD models, it is the\nnovel introduction of the implicit decoder that enables the refined pose\nprediction during testing, by enforcing the predicted pose consistency between\nthe two decoders using a self-adaptive loss term. Thorough experiments on\nbenchmarks of both category- and instance-level object pose datasets confirm\nefficacy of our designs. DualPoseNet outperforms existing methods with a large\nmargin in the regime of high precision. Our code is released publicly at\nhttps://github.com/Gorilla-Lab-SCUT/DualPoseNet.",
          "link": "http://arxiv.org/abs/2103.06526",
          "publishedOn": "2021-08-17T01:54:53.470Z",
          "wordCount": 711,
          "title": "DualPoseNet: Category-level 6D Object Pose and Size Estimation Using Dual Pose Network with Refined Learning of Pose Consistency. (arXiv:2103.06526v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasson_Y/0/1/0/all/0/1\">Yana Hasson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Our work aims to obtain 3D reconstruction of hands and manipulated objects\nfrom monocular videos. Reconstructing hand-object manipulations holds a great\npotential for robotics and learning from human demonstrations. The supervised\nlearning approach to this problem, however, requires 3D supervision and remains\nlimited to constrained laboratory settings and simulators for which 3D ground\ntruth is available. In this paper we first propose a learning-free fitting\napproach for hand-object reconstruction which can seamlessly handle two-hand\nobject interactions. Our method relies on cues obtained with common methods for\nobject detection, hand pose estimation and instance segmentation. We\nquantitatively evaluate our approach and show that it can be applied to\ndatasets with varying levels of difficulty for which training data is\nunavailable.",
          "link": "http://arxiv.org/abs/2108.07044",
          "publishedOn": "2021-08-17T01:54:53.450Z",
          "wordCount": 560,
          "title": "Towards unconstrained joint hand-object reconstruction from RGB videos. (arXiv:2108.07044v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjisoa_M/0/1/0/all/0/1\">Micha&#xeb;l Ramamonjisoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firman_M/0/1/0/all/0/1\">Michael Firman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1\">Jamie Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turmukhambetov_D/0/1/0/all/0/1\">Daniyar Turmukhambetov</a>",
          "description": "We present a novel method for predicting accurate depths from monocular\nimages with high efficiency. This optimal efficiency is achieved by exploiting\nwavelet decomposition, which is integrated in a fully differentiable\nencoder-decoder architecture. We demonstrate that we can reconstruct\nhigh-fidelity depth maps by predicting sparse wavelet coefficients. In contrast\nwith previous works, we show that wavelet coefficients can be learned without\ndirect supervision on coefficients. Instead we supervise only the final depth\nimage that is reconstructed through the inverse wavelet transform. We\nadditionally show that wavelet coefficients can be learned in fully\nself-supervised scenarios, without access to ground-truth depth. Finally, we\napply our method to different state-of-the-art monocular depth estimation\nmodels, in each case giving similar or better results compared to the original\nmodel, while requiring less than half the multiply-adds in the decoder network.\nCode at https://github.com/nianticlabs/wavelet-monodepth",
          "link": "http://arxiv.org/abs/2106.02022",
          "publishedOn": "2021-08-17T01:54:53.431Z",
          "wordCount": 608,
          "title": "Single Image Depth Prediction with Wavelet Decomposition. (arXiv:2106.02022v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08357",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Wenbin Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_D/0/1/0/all/0/1\">Dehua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Deep learning based methods, especially convolutional neural networks (CNNs)\nhave been successfully applied in the field of single image super-resolution\n(SISR). To obtain better fidelity and visual quality, most of existing networks\nare of heavy design with massive computation. However, the computation\nresources of modern mobile devices are limited, which cannot easily support the\nexpensive cost. To this end, this paper explores a novel frequency-aware\ndynamic network for dividing the input into multiple parts according to its\ncoefficients in the discrete cosine transform (DCT) domain. In practice, the\nhigh-frequency part will be processed using expensive operations and the\nlower-frequency part is assigned with cheap operations to relieve the\ncomputation burden. Since pixels or image patches belong to low-frequency areas\ncontain relatively few textural details, this dynamic network will not affect\nthe quality of resulting super-resolution images. In addition, we embed\npredictors into the proposed dynamic network to end-to-end fine-tune the\nhandcrafted frequency-aware masks. Extensive experiments conducted on benchmark\nSISR models and datasets show that the frequency-aware dynamic network can be\nemployed for various SISR neural architectures to obtain the better tradeoff\nbetween visual quality and computational complexity. For instance, we can\nreduce the FLOPs of SR models by approximate 50% while preserving\nstate-of-the-art SISR performance.",
          "link": "http://arxiv.org/abs/2103.08357",
          "publishedOn": "2021-08-17T01:54:53.425Z",
          "wordCount": 669,
          "title": "Learning Frequency-aware Dynamic Network for Efficient Super-Resolution. (arXiv:2103.08357v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kenny Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pogue_A/0/1/0/all/0/1\">Alexandra Pogue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_B/0/1/0/all/0/1\">Brett T. Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agha_mohammadi_A/0/1/0/all/0/1\">Ali-akbar Agha-mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1\">Ankur Mehta</a>",
          "description": "Monocular depth inference has gained tremendous attention from researchers in\nrecent years and remains as a promising replacement for expensive\ntime-of-flight sensors, but issues with scale acquisition and implementation\noverhead still plague these systems. To this end, this work presents an\nunsupervised learning framework that is able to predict at-scale depth maps and\negomotion, in addition to camera intrinsics, from a sequence of monocular\nimages via a single network. Our method incorporates both spatial and temporal\ngeometric constraints to resolve depth and pose scale factors, which are\nenforced within the supervisory reconstruction loss functions at training time.\nOnly unlabeled stereo sequences are required for training the weights of our\nsingle-network architecture, which reduces overall implementation overhead as\ncompared to previous methods. Our results demonstrate strong performance when\ncompared to the current state-of-the-art on multiple sequences of the KITTI\ndriving dataset and can provide faster training times with its reduced network\ncomplexity.",
          "link": "http://arxiv.org/abs/2011.01354",
          "publishedOn": "2021-08-17T01:54:53.419Z",
          "wordCount": 656,
          "title": "Unsupervised Monocular Depth Learning with Integrated Intrinsics and Spatio-Temporal Constraints. (arXiv:2011.01354v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turkoglu_M/0/1/0/all/0/1\">Mehmet Ozgur Turkoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perich_G/0/1/0/all/0/1\">Gregor Perich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebisch_F/0/1/0/all/0/1\">Frank Liebisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Streit_C/0/1/0/all/0/1\">Constantin Streit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>",
          "description": "The aim of this paper is to map agricultural crops by classifying satellite\nimage time series. Domain experts in agriculture work with crop type labels\nthat are organised in a hierarchical tree structure, where coarse classes (like\norchards) are subdivided into finer ones (like apples, pears, vines, etc.). We\ndevelop a crop classification method that exploits this expert knowledge and\nsignificantly improves the mapping of rare crop types. The three-level label\nhierarchy is encoded in a convolutional, recurrent neural network (convRNN),\nsuch that for each pixel the model predicts three labels at different level of\ngranularity. This end-to-end trainable, hierarchical network architecture\nallows the model to learn joint feature representations of rare classes (e.g.,\napples, pears) at a coarser level (e.g., orchard), thereby boosting\nclassification performance at the fine-grained level. Additionally, labelling\nat different granularity also makes it possible to adjust the output according\nto the classification scores; as coarser labels with high confidence are\nsometimes more useful for agricultural practice than fine-grained but very\nuncertain labels. We validate the proposed method on a new, large dataset that\nwe make public. ZueriCrop covers an area of 50 km x 48 km in the Swiss cantons\nof Zurich and Thurgau with a total of 116'000 individual fields spanning 48\ncrop classes, and 28,000 (multi-temporal) image patches from Sentinel-2. We\ncompare our proposed hierarchical convRNN model with several baselines,\nincluding methods designed for imbalanced class distributions. The hierarchical\napproach performs superior by at least 9.9 percentage points in F1-score.",
          "link": "http://arxiv.org/abs/2102.08820",
          "publishedOn": "2021-08-17T01:54:53.411Z",
          "wordCount": 731,
          "title": "Crop mapping from image time series: deep learning with multi-scale label hierarchies. (arXiv:2102.08820v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1\">Ben Mildenhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tancik_M/0/1/0/all/0/1\">Matthew Tancik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Peter Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>",
          "description": "The rendering procedure used by neural radiance fields (NeRF) samples a scene\nwith a single ray per pixel and may therefore produce renderings that are\nexcessively blurred or aliased when training or testing images observe scene\ncontent at different resolutions. The straightforward solution of supersampling\nby rendering with multiple rays per pixel is impractical for NeRF, because\nrendering each ray requires querying a multilayer perceptron hundreds of times.\nOur solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to\nrepresent the scene at a continuously-valued scale. By efficiently rendering\nanti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable\naliasing artifacts and significantly improves NeRF's ability to represent fine\ndetails, while also being 7% faster than NeRF and half the size. Compared to\nNeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with\nNeRF and by 60% on a challenging multiscale variant of that dataset that we\npresent. Mip-NeRF is also able to match the accuracy of a brute-force\nsupersampled NeRF on our multiscale dataset while being 22x faster.",
          "link": "http://arxiv.org/abs/2103.13415",
          "publishedOn": "2021-08-17T01:54:53.404Z",
          "wordCount": 660,
          "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. (arXiv:2103.13415v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shi-Wei Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jyh-Horng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jo-Yu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1\">Chien-Hao Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Meng-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fang-Pang Lin</a>",
          "description": "In the monsoon season, sudden flood events occur frequently in urban areas,\nwhich hamper the social and economic activities and may threaten the\ninfrastructure and lives. The use of an efficient large-scale waterlogging\nsensing and information system can provide valuable real-time disaster\ninformation to facilitate disaster management and enhance awareness of the\ngeneral public to alleviate losses during and after flood disasters. Therefore,\nin this study, a visual sensing approach driven by deep neural networks and\ninformation and communication technology was developed to provide an end-to-end\nmechanism to realize waterlogging sensing and event-location mapping. The use\nof a deep sensing system in the monsoon season in Taiwan was demonstrated, and\nwaterlogging events were predicted on the island-wide scale. The system could\nsense approximately 2379 vision sources through an internet of video things\nframework and transmit the event-location information in 5 min. The proposed\napproach can sense waterlogging events at a national scale and provide an\nefficient and highly scalable alternative to conventional waterlogging sensing\nmethods.",
          "link": "http://arxiv.org/abs/2103.05927",
          "publishedOn": "2021-08-17T01:54:53.383Z",
          "wordCount": 651,
          "title": "Deep Sensing of Urban Waterlogging. (arXiv:2103.05927v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08501",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Patel_S/0/1/0/all/0/1\">Shaswat Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lohakare_M/0/1/0/all/0/1\">Maithili Lohakare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prajapati_S/0/1/0/all/0/1\">Samyak Prajapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1\">Shaanya Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_N/0/1/0/all/0/1\">Nancy Patel</a>",
          "description": "Patients with long-standing diabetes often fall prey to Diabetic Retinopathy\n(DR) resulting in changes in the retina of the human eye, which may lead to\nloss of vision in extreme cases. The aim of this study is two-fold: (a) create\ndeep learning models that were trained to grade degraded retinal fundus images\nand (b) to create a browser-based application that will aid in diagnostic\nprocedures by highlighting the key features of the fundus image. In this\nresearch work, we have emulated the images plagued by distortions by degrading\nthe images based on multiple different combinations of Light Transmission\nDisturbance, Image Blurring and insertion of Retinal Artifacts. InceptionV3,\nResNet-50 and InceptionResNetV2 were trained and used to classify retinal\nfundus images based on their severity level and then further used in the\ncreation of a browser-based application, which implements the Integration\nGradient (IG) Attribution Mask on the input image and demonstrates the\npredictions made by the model and the probability associated with each class.",
          "link": "http://arxiv.org/abs/2103.08501",
          "publishedOn": "2021-08-17T01:54:53.347Z",
          "wordCount": 657,
          "title": "DiaRet: A browser-based application for the grading of Diabetic Retinopathy with Integrated Gradients. (arXiv:2103.08501v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hezhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Junfu Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Sign language recognition (SLR) is a challenging problem, involving complex\nmanual features, i.e., hand gestures, and fine-grained non-manual features\n(NMFs), i.e., facial expression, mouth shapes, etc. Although manual features\nare dominant, non-manual features also play an important role in the expression\nof a sign word. Specifically, many sign words convey different meanings due to\nnon-manual features, even though they share the same hand gestures. This\nambiguity introduces great challenges in the recognition of sign words. To\ntackle the above issue, we propose a simple yet effective architecture called\nGlobal-local Enhancement Network (GLE-Net), including two mutually promoted\nstreams towards different crucial aspects of SLR. Of the two streams, one\ncaptures the global contextual relationship, while the other stream captures\nthe discriminative fine-grained cues. Moreover, due to the lack of datasets\nexplicitly focusing on this kind of features, we introduce the first\nnon-manual-features-aware isolated Chinese sign language dataset~(NMFs-CSL)\nwith a total vocabulary size of 1,067 sign words in daily life. Extensive\nexperiments on NMFs-CSL and SLR500 datasets demonstrate the effectiveness of\nour method.",
          "link": "http://arxiv.org/abs/2008.10428",
          "publishedOn": "2021-08-17T01:54:53.340Z",
          "wordCount": 645,
          "title": "Global-local Enhancement Network for NMFs-aware Sign Language Recognition. (arXiv:2008.10428v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "Previous adversarial training raises model robustness under the compromise of\naccuracy on natural data. In this paper, we reduce natural accuracy\ndegradation. We use the model logits from one clean model to guide learning of\nanother one robust model, taking into consideration that logits from the well\ntrained clean model embed the most discriminative features of natural data,\n{\\it e.g.}, generalizable classifier boundary. Our solution is to constrain\nlogits from the robust model that takes adversarial examples as input and makes\nit similar to those from the clean model fed with corresponding natural data.\nIt lets the robust model inherit the classifier boundary of the clean model.\nMoreover, we observe such boundary guidance can not only preserve high natural\naccuracy but also benefit model robustness, which gives new insights and\nfacilitates progress for the adversarial community. Finally, extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny ImageNet testify to the\neffectiveness of our method. We achieve new state-of-the-art robustness on\nCIFAR-100 without additional real or synthetic data with auto-attack benchmark\n\\footnote{\\url{https://github.com/fra31/auto-attack}}. Our code is available at\n\\url{https://github.com/dvlab-research/LBGAT}.",
          "link": "http://arxiv.org/abs/2011.11164",
          "publishedOn": "2021-08-17T01:54:53.331Z",
          "wordCount": 638,
          "title": "Learnable Boundary Guided Adversarial Training. (arXiv:2011.11164v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiqiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>",
          "description": "Traditional normalization techniques (e.g., Batch Normalization and Instance\nNormalization) generally and simplistically assume that training and test data\nfollow the same distribution. As distribution shifts are inevitable in\nreal-world applications, well-trained models with previous normalization\nmethods can perform badly in new environments. Can we develop new normalization\nmethods to improve generalization robustness under distribution shifts? In this\npaper, we answer the question by proposing CrossNorm and SelfNorm. CrossNorm\nexchanges channel-wise mean and variance between feature maps to enlarge\ntraining distribution, while SelfNorm uses attention to recalibrate the\nstatistics to bridge gaps between training and test distributions. CrossNorm\nand SelfNorm can complement each other, though exploring different directions\nin statistics usage. Extensive experiments on different fields (vision and\nlanguage), tasks (classification and segmentation), settings (supervised and\nsemi-supervised), and distribution shift types (synthetic and natural) show the\neffectiveness. Code is available at\nhttps://github.com/amazon-research/crossnorm-selfnorm",
          "link": "http://arxiv.org/abs/2102.02811",
          "publishedOn": "2021-08-17T01:54:53.314Z",
          "wordCount": 621,
          "title": "CrossNorm and SelfNorm for Generalization under Distribution Shifts. (arXiv:2102.02811v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Haoxi Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>",
          "description": "Human imitation has become topical recently, driven by GAN's ability to\ndisentangle human pose and body content. However, the latest methods hardly\nfocus on 3D information, and to avoid self-occlusion, a massive amount of input\nimages are needed. In this paper, we propose RIN, a novel volume-based\nframework for reconstructing a textured 3D model from a single picture and\nimitating a subject with the generated model. Specifically, to estimate most of\nthe human texture, we propose a U-Net-like front-to-back translation network.\nWith both front and back images input, the textured volume recovery module\nallows us to color a volumetric human. A sequence of 3D poses then guides the\ncolored volume via Flowable Disentangle Networks as a volume-to-volume\ntranslation task. To project volumes to a 2D plane during training, we design a\ndifferentiable depth-aware renderer. Our experiments demonstrate that our\nvolume-based model is adequate for human imitation, and the back view can be\nestimated reliably using our network. While prior works based on either 2D pose\nor semantic map often fail for the unstable appearance of a human, our\nframework can still produce concrete results, which are competitive to those\nimagined from multi-view input.",
          "link": "http://arxiv.org/abs/2011.12024",
          "publishedOn": "2021-08-17T01:54:53.307Z",
          "wordCount": 688,
          "title": "RIN: Textured Human Model Recovery and Imitation with a Single Image. (arXiv:2011.12024v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzger_N/0/1/0/all/0/1\">Nando Metzger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turkoglu_M/0/1/0/all/0/1\">Mehmet Ozgur Turkoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "Optical satellite sensors cannot see the Earth's surface through clouds.\nDespite the periodic revisit cycle, image sequences acquired by Earth\nobservation satellites are therefore irregularly sampled in time.\nState-of-the-art methods for crop classification (and other time series\nanalysis tasks) rely on techniques that implicitly assume regular temporal\nspacing between observations, such as recurrent neural networks (RNNs). We\npropose to use neural ordinary differential equations (NODEs) in combination\nwith RNNs to classify crop types in irregularly spaced image sequences. The\nresulting ODE-RNN models consist of two steps: an update step, where a\nrecurrent unit assimilates new input data into the model's hidden state; and a\nprediction step, in which NODE propagates the hidden state until the next\nobservation arrives. The prediction step is based on a continuous\nrepresentation of the latent dynamics, which has several advantages. At the\nconceptual level, it is a more natural way to describe the mechanisms that\ngovern the phenological cycle. From a practical point of view, it makes it\npossible to sample the system state at arbitrary points in time, such that one\ncan integrate observations whenever they are available, and extrapolate beyond\nthe last observation. Our experiments show that ODE-RNN indeed improves\nclassification accuracy over common baselines such as LSTM, GRU, and temporal\nconvolution. The gains are most prominent in the challenging scenario where\nonly few observations are available (i.e., frequent cloud cover). Moreover, we\nshow that the ability to extrapolate translates to better classification\nperformance early in the season, which is important for forecasting.",
          "link": "http://arxiv.org/abs/2012.02542",
          "publishedOn": "2021-08-17T01:54:53.301Z",
          "wordCount": 727,
          "title": "Crop Classification under Varying Cloud Cover with Neural Ordinary Differential Equations. (arXiv:2012.02542v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zia Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salekin_A/0/1/0/all/0/1\">Asif Salekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1\">Tauhidur Rahman</a>",
          "description": "Hyperspectral image (HSI) with narrow spectral bands can capture rich\nspectral information, but it sacrifices its spatial resolution in the process.\nMany machine-learning-based HSI super-resolution (SR) algorithms have been\nproposed recently. However, one of the fundamental limitations of these\napproaches is that they are highly dependent on image and camera settings and\ncan only learn to map an input HSI with one specific setting to an output HSI\nwith another. However, different cameras capture images with different spectral\nresponse functions and bands numbers due to the diversity of HSI cameras.\nConsequently, the existing machine-learning-based approaches fail to learn to\nsuper-resolve HSIs for a wide variety of input-output band settings. We propose\na single Meta-Learning-Based Super-Resolution (MLSR) model, which can take in\nHSI images at an arbitrary number of input bands' peak wavelengths and generate\nSR HSIs with an arbitrary number of output bands' peak wavelengths. We leverage\nNTIRE2020 and ICVL datasets to train and validate the performance of the MLSR\nmodel. The results show that the single proposed model can successfully\ngenerate super-resolved HSI bands at arbitrary input-output band settings. The\nresults are better or at least comparable to baselines that are separately\ntrained on a specific input-output band setting.",
          "link": "http://arxiv.org/abs/2103.10614",
          "publishedOn": "2021-08-17T01:54:53.290Z",
          "wordCount": 672,
          "title": "Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings. (arXiv:2103.10614v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haliassos_A/0/1/0/all/0/1\">Alexandros Haliassos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vougioukas_K/0/1/0/all/0/1\">Konstantinos Vougioukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>",
          "description": "Although current deep learning-based face forgery detectors achieve\nimpressive performance in constrained scenarios, they are vulnerable to samples\ncreated by unseen manipulation methods. Some recent works show improvements in\ngeneralisation but rely on cues that are easily corrupted by common\npost-processing operations such as compression. In this paper, we propose\nLipForensics, a detection approach capable of both generalising to novel\nmanipulations and withstanding various distortions. LipForensics targets\nhigh-level semantic irregularities in mouth movements, which are common in many\ngenerated videos. It consists in first pretraining a spatio-temporal network to\nperform visual speech recognition (lipreading), thus learning rich internal\nrepresentations related to natural mouth motion. A temporal network is\nsubsequently finetuned on fixed mouth embeddings of real and forged data in\norder to detect fake videos based on mouth movements without overfitting to\nlow-level, manipulation-specific artefacts. Extensive experiments show that\nthis simple approach significantly surpasses the state-of-the-art in terms of\ngeneralisation to unseen manipulations and robustness to perturbations, as well\nas shed light on the factors responsible for its performance. Code is available\non GitHub.",
          "link": "http://arxiv.org/abs/2012.07657",
          "publishedOn": "2021-08-17T01:54:53.283Z",
          "wordCount": 663,
          "title": "Lips Don't Lie: A Generalisable and Robust Approach to Face Forgery Detection. (arXiv:2012.07657v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soldan_M/0/1/0/all/0/1\">Mattia Soldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Sisi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1\">Jesper Tegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Grounding language queries in videos aims at identifying the time interval\n(or moment) semantically relevant to a language query. The solution to this\nchallenging task demands understanding videos' and queries' semantic content\nand the fine-grained reasoning about their multi-modal interactions. Our key\nidea is to recast this challenge into an algorithmic graph matching problem.\nFueled by recent advances in Graph Neural Networks, we propose to leverage\nGraph Convolutional Networks to model video and textual information as well as\ntheir semantic alignment. To enable the mutual exchange of information across\nthe modalities, we design a novel Video-Language Graph Matching Network\n(VLG-Net) to match video and query graphs. Core ingredients include\nrepresentation graphs built atop video snippets and query tokens separately and\nused to model intra-modality relationships. A Graph Matching layer is adopted\nfor cross-modal context modeling and multi-modal fusion. Finally, moment\ncandidates are created using masked moment attention pooling by fusing the\nmoment's enriched snippet features. We demonstrate superior performance over\nstate-of-the-art grounding methods on three widely used datasets for temporal\nlocalization of moments in videos with language queries: ActivityNet-Captions,\nTACoS, and DiDeMo.",
          "link": "http://arxiv.org/abs/2011.10132",
          "publishedOn": "2021-08-17T01:54:53.264Z",
          "wordCount": 672,
          "title": "VLG-Net: Video-Language Graph Matching Network for Video Grounding. (arXiv:2011.10132v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lianbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoli Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Zero-Shot Learning (ZSL) targets at recognizing unseen categories by\nleveraging auxiliary information, such as attribute embedding. Despite the\nencouraging results achieved, prior ZSL approaches focus on improving the\ndiscriminant power of seen-class features, yet have largely overlooked the\ngeometric structure of the samples and the prototypes. The subsequent\nattribute-based generative adversarial network (GAN), as a result, also\nneglects the topological information in sample generation and further yields\ninferior performances in classifying the visual features of unseen classes. In\nthis paper, we introduce a novel structure-aware feature generation scheme,\ntermed as SA-GAN, to explicitly account for the topological structure in\nlearning both the latent space and the generative networks. Specifically, we\nintroduce a constraint loss to preserve the initial geometric structure when\nlearning a discriminative latent space, and carry out our GAN training with\nadditional supervising signals from a structure-aware discriminator and a\nreconstruction module. The former supervision distinguishes fake and real\nsamples based on their affinity to class prototypes, while the latter aims to\nreconstruct the original feature space from the generated latent space. This\ntopology-preserving mechanism enables our method to significantly enhance the\ngeneralization capability on unseen-classes and consequently improve the\nclassification performance. Experiments on four benchmarks demonstrate that the\nproposed approach consistently outperforms the state of the art. Our code can\nbe found in the supplementary material and will also be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2108.07032",
          "publishedOn": "2021-08-17T01:54:53.258Z",
          "wordCount": 660,
          "title": "Structure-Aware Feature Generation for Zero-Shot Learning. (arXiv:2108.07032v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_L/0/1/0/all/0/1\">Lingjuan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yunpeng Dong</a>",
          "description": "Object detection in optical remote sensing images is an important and\nchallenging task. In recent years, the methods based on convolutional neural\nnetworks have made good progress. However, due to the large variation in object\nscale, aspect ratio, and arbitrary orientation, the detection performance is\ndifficult to be further improved. In this paper, we discuss the role of\ndiscriminative features in object detection, and then propose a Critical\nFeature Capturing Network (CFC-Net) to improve detection accuracy from three\naspects: building powerful feature representation, refining preset anchors, and\noptimizing label assignment. Specifically, we first decouple the classification\nand regression features, and then construct robust critical features adapted to\nthe respective tasks through the Polarization Attention Module (PAM). With the\nextracted discriminative regression features, the Rotation Anchor Refinement\nModule (R-ARM) performs localization refinement on preset horizontal anchors to\nobtain superior rotation anchors. Next, the Dynamic Anchor Learning (DAL)\nstrategy is given to adaptively select high-quality anchors based on their\nability to capture critical features. The proposed framework creates more\npowerful semantic representations for objects in remote sensing images and\nachieves high-performance real-time object detection. Experimental results on\nthree remote sensing datasets including HRSC2016, DOTA, and UCAS-AOD show that\nour method achieves superior detection performance compared with many\nstate-of-the-art approaches. Code and models are available at\nhttps://github.com/ming71/CFC-Net.",
          "link": "http://arxiv.org/abs/2101.06849",
          "publishedOn": "2021-08-17T01:54:53.253Z",
          "wordCount": 702,
          "title": "CFC-Net: A Critical Feature Capturing Network for Arbitrary-Oriented Object Detection in Remote Sensing Images. (arXiv:2101.06849v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anindya Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anirban Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Deep neural networks are the default choice of learning models for computer\nvision tasks. Extensive work has been carried out in recent years on explaining\ndeep models for vision tasks such as classification. However, recent work has\nshown that it is possible for these models to produce substantially different\nattribution maps even when two very similar images are given to the network,\nraising serious questions about trustworthiness. To address this issue, we\npropose a robust attribution training strategy to improve attributional\nrobustness of deep neural networks. Our method carefully analyzes the\nrequirements for attributional robustness and introduces two new regularizers\nthat preserve a model's attribution map during attacks. Our method surpasses\nstate-of-the-art attributional robustness methods by a margin of approximately\n3% to 9% in terms of attribution robustness measures on several datasets\nincluding MNIST, FMNIST, Flower and GTSRB.",
          "link": "http://arxiv.org/abs/2012.14395",
          "publishedOn": "2021-08-17T01:54:53.247Z",
          "wordCount": 613,
          "title": "Enhanced Regularizers for Attributional Robustness. (arXiv:2012.14395v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "With the tremendous advances in the architecture and scale of convolutional\nneural networks (CNNs) over the past few decades, they can easily reach or even\nexceed the performance of humans in certain tasks. However, a recently\ndiscovered shortcoming of CNNs is that they are vulnerable to adversarial\nattacks. Although the adversarial robustness of CNNs can be improved by\nadversarial training, there is a trade-off between standard accuracy and\nadversarial robustness. From the neural architecture perspective, this paper\naims to improve the adversarial robustness of the backbone CNNs that have a\nsatisfactory accuracy. Under a minimal computational overhead, the introduction\nof a dilation architecture is expected to be friendly with the standard\nperformance of the backbone CNN while pursuing adversarial robustness.\nTheoretical analyses on the standard and adversarial error bounds naturally\nmotivate the proposed neural architecture dilation algorithm. Experimental\nresults on real-world datasets and benchmark neural networks demonstrate the\neffectiveness of the proposed algorithm to balance the accuracy and adversarial\nrobustness.",
          "link": "http://arxiv.org/abs/2108.06885",
          "publishedOn": "2021-08-17T01:54:53.240Z",
          "wordCount": 614,
          "title": "Neural Architecture Dilation for Adversarial Robustness. (arXiv:2108.06885v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.01216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huh_J/0/1/0/all/0/1\">Jaesung Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1\">Triantafyllos Afouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "The goal of this paper is speaker diarisation of videos collected 'in the\nwild'. We make three key contributions. First, we propose an automatic\naudio-visual diarisation method for YouTube videos. Our method consists of\nactive speaker detection using audio-visual methods and speaker verification\nusing self-enrolled speaker models. Second, we integrate our method into a\nsemi-automatic dataset creation pipeline which significantly reduces the number\nof hours required to annotate videos with diarisation labels. Finally, we use\nthis pipeline to create a large-scale diarisation dataset called VoxConverse,\ncollected from 'in the wild' videos, which we will release publicly to the\nresearch community. Our dataset consists of overlapping speech, a large and\ndiverse speaker pool, and challenging background conditions.",
          "link": "http://arxiv.org/abs/2007.01216",
          "publishedOn": "2021-08-17T01:54:53.221Z",
          "wordCount": 642,
          "title": "Spot the conversation: speaker diarisation in the wild. (arXiv:2007.01216v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03932",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ayatollahi_F/0/1/0/all/0/1\">Fazael Ayatollahi</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Shokouhi_S/0/1/0/all/0/1\">Shahriar B. Shokouhi</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Mann_R/0/1/0/all/0/1\">Ritse M. Mann</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a> (2 and 3) ((1) Electrical Engineering Department, Iran University of Science and Technology (IUST), Tehran, Iran, (2) Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, the Netherlands, (3) Department of Radiation Oncology, Netherlands Cancer Institute, Amsterdam, the Netherlands)",
          "description": "Purpose: We propose a deep learning-based computer-aided detection (CADe)\nmethod to detect breast lesions in ultrafast DCE-MRI sequences. This method\nuses both the three-dimensional spatial information and temporal information\nobtained from the early-phase of the dynamic acquisition. Methods: The proposed\nCADe method, based on a modified 3D RetinaNet model, operates on ultrafast T1\nweighted sequences, which are preprocessed for motion compensation, temporal\nnormalization, and are cropped before passing into the model. The model is\noptimized to enable the detection of relatively small breast lesions in a\nscreening setting, focusing on detection of lesions that are harder to\ndifferentiate from confounding structures inside the breast. Results: The\nmethod was developed based on a dataset consisting of 489 ultrafast MRI studies\nobtained from 462 patients containing a total of 572 lesions (365 malignant,\n207 benign) and achieved a detection rate, sensitivity, and detection rate of\nbenign lesions of 0.90 (0.876-0.934), 0.95 (0.934-0.980), and 0.81\n(0.751-0.871) at 4 false positives per normal breast with 10-fold\ncross-testing, respectively. Conclusions: The deep learning architecture used\nfor the proposed CADe application can efficiently detect benign and malignant\nlesions on ultrafast DCE-MRI. Furthermore, utilizing the less visible hard-to\ndetect-lesions in training improves the learning process and, subsequently,\ndetection of malignant breast lesions.",
          "link": "http://arxiv.org/abs/2102.03932",
          "publishedOn": "2021-08-17T01:54:53.206Z",
          "wordCount": 723,
          "title": "Automatic Breast Lesion Detection in Ultrafast DCE-MRI Using Deep Learning. (arXiv:2102.03932v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shihao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "This paper focuses on a new problem of estimating human pose and shape from\nsingle polarization images. Polarization camera is known to be able to capture\nthe polarization of reflected lights that preserves rich geometric cues of an\nobject surface. Inspired by the recent applications in surface normal\nreconstruction from polarization images, in this paper, we attempt to estimate\nhuman pose and shape from single polarization images by leveraging the\npolarization-induced geometric cues. A dedicated two-stage pipeline is\nproposed: given a single polarization image, stage one (Polar2Normal) focuses\non the fine detailed human body surface normal estimation; stage two\n(Polar2Shape) then reconstructs clothed human shape from the polarization image\nand the estimated surface normal. To empirically validate our approach, a\ndedicated dataset (PHSPD) is constructed, consisting of over 500K frames with\naccurate pose and shape annotations. Empirical evaluations on this real-world\ndataset as well as a synthetic dataset, SURREAL, demonstrate the effectiveness\nof our approach. It suggests polarization camera as a promising alternative to\nthe more conventional RGB camera for human pose and shape estimation.",
          "link": "http://arxiv.org/abs/2108.06834",
          "publishedOn": "2021-08-17T01:54:50.538Z",
          "wordCount": 628,
          "title": "Human Pose and Shape Estimation from Single Polarization Images. (arXiv:2108.06834v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.00463",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ikoma_H/0/1/0/all/0/1\">Hayato Ikoma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jeon_D/0/1/0/all/0/1\">Daniel S. Jeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_M/0/1/0/all/0/1\">Min H. Kim</a>",
          "description": "Imaging depth and spectrum have been extensively studied in isolation from\neach other for decades. Recently, hyperspectral-depth (HS-D) imaging emerges to\ncapture both information simultaneously by combining two different imaging\nsystems; one for depth, the other for spectrum. While being accurate, this\ncombinational approach induces increased form factor, cost, capture time, and\nalignment/registration problems. In this work, departing from the combinational\nprinciple, we propose a compact single-shot monocular HS-D imaging method. Our\nmethod uses a diffractive optical element (DOE), the point spread function of\nwhich changes with respect to both depth and spectrum. This enables us to\nreconstruct spectrum and depth from a single captured image. To this end, we\ndevelop a differentiable simulator and a neural-network-based reconstruction\nthat are jointly optimized via automatic differentiation. To facilitate\nlearning the DOE, we present a first HS-D dataset by building a benchtop HS-D\nimager that acquires high-quality ground truth. We evaluate our method with\nsynthetic and real experiments by building an experimental prototype and\nachieve state-of-the-art HS-D imaging results.",
          "link": "http://arxiv.org/abs/2009.00463",
          "publishedOn": "2021-08-17T01:54:50.529Z",
          "wordCount": 659,
          "title": "Single-shot Hyperspectral-Depth Imaging with Learned Diffractive Optics. (arXiv:2009.00463v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.11897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>",
          "description": "State-of-the-art methods in image-to-image translation are capable of\nlearning a mapping from a source domain to a target domain with unpaired image\ndata. Though the existing methods have achieved promising results, they still\nproduce visual artifacts, being able to translate low-level information but not\nhigh-level semantics of input images. One possible reason is that generators do\nnot have the ability to perceive the most discriminative parts between the\nsource and target domains, thus making the generated images low quality. In\nthis paper, we propose a new Attention-Guided Generative Adversarial Networks\n(AttentionGAN) for the unpaired image-to-image translation task. AttentionGAN\ncan identify the most discriminative foreground objects and minimize the change\nof the background. The attention-guided generators in AttentionGAN are able to\nproduce attention masks, and then fuse the generation output with the attention\nmasks to obtain high-quality target images. Accordingly, we also design a novel\nattention-guided discriminator which only considers attended regions. Extensive\nexperiments are conducted on several generative tasks with eight public\ndatasets, demonstrating that the proposed method is effective to generate\nsharper and more realistic images compared with existing competitive models.\nThe code is available at https://github.com/Ha0Tang/AttentionGAN.",
          "link": "http://arxiv.org/abs/1911.11897",
          "publishedOn": "2021-08-17T01:54:50.521Z",
          "wordCount": 722,
          "title": "AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks. (arXiv:1911.11897v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>",
          "description": "We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter\npruning), which slims down a CNN by reducing the width (number of output\nchannels) of convolutional layers. Inspired by the neurobiology research about\nthe independence of remembering and forgetting, we propose to re-parameterize a\nCNN into the remembering parts and forgetting parts, where the former learn to\nmaintain the performance and the latter learn to prune. Via training with\nregular SGD on the former but a novel update rule with penalty gradients on the\nlatter, we realize structured sparsity. Then we equivalently merge the\nremembering and forgetting parts into the original architecture with narrower\nlayers. In this sense, ResRep can be viewed as a successful application of\nStructural Re-parameterization. Such a methodology distinguishes ResRep from\nthe traditional learning-based pruning paradigm that applies a penalty on\nparameters to produce sparsity, which may suppress the parameters essential for\nthe remembering. ResRep slims down a standard ResNet-50 with 76.15% accuracy on\nImageNet to a narrower one with only 45% FLOPs and no accuracy drop, which is\nthe first to achieve lossless pruning with such a high compression ratio. The\ncode and models are at https://github.com/DingXiaoH/ResRep.",
          "link": "http://arxiv.org/abs/2007.03260",
          "publishedOn": "2021-08-17T01:54:50.506Z",
          "wordCount": 704,
          "title": "ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting. (arXiv:2007.03260v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12066",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yanming Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chunyan Wang</a>",
          "description": "The work presented in this paper is to propose a reliable high-quality system\nof Convolutional Neural Network (CNN) for brain tumor segmentation with a low\ncomputation requirement. The system consists of a CNN for the main processing\nfor the segmentation, a pre-CNN block for data reduction and post-CNN\nrefinement block. The unique CNN consists of 7 convolution layers involving\nonly 108 kernels and 20308 trainable parameters. It is custom-designed,\nfollowing the proposed paradigm of ASCNN (application specific CNN), to perform\nmono-modality and cross-modality feature extraction, tumor localization and\npixel classification. Each layer fits the task assigned to it, by means of (i)\nappropriate normalization applied to its input data, (ii) correct convolution\nmodes for the assigned task, and (iii) suitable nonlinear transformation to\noptimize the convolution results. In this specific design context, the number\nof kernels in each of the 7 layers is made to be just-sufficient for its task,\ninstead of exponentially growing over the layers, to increase information\ndensity and to reduce randomness in the processing. The proposed activation\nfunction Full-ReLU helps to halve the number of kernels in convolution layers\nof high-pass filtering without degrading processing quality. A large number of\nexperiments with BRATS2018 dataset have been conducted to measure the\nprocessing quality and reproducibility of the proposed system. The results\ndemonstrate that the system reproduces reliably almost the same output to the\nsame input after retraining. The mean dice scores for enhancing tumor, whole\ntumor and tumor core are 77.2%, 89.2% and 76.3%, respectively. The simple\nstructure and reliable high processing quality of the proposed system will\nfacilitate its implementation and medical applications.",
          "link": "http://arxiv.org/abs/2007.12066",
          "publishedOn": "2021-08-17T01:54:50.466Z",
          "wordCount": 739,
          "title": "A Computation-Efficient CNN System for High-Quality Brain Tumor Segmentation. (arXiv:2007.12066v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prokofiev_K/0/1/0/all/0/1\">Kirill Prokofiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sovrasov_V/0/1/0/all/0/1\">Vladislav Sovrasov</a>",
          "description": "Nowadays deep learning-based methods have achieved a remarkable progress at\nthe image classification task among a wide range of commonly used datasets\n(ImageNet, CIFAR, SVHN, Caltech 101, SUN397, etc.). SOTA performance on each of\nthe mentioned datasets is obtained by careful tuning of the model architecture\nand training tricks according to the properties of the target data. Although\nthis approach allows setting academic records, it is unrealistic that an\naverage data scientist would have enough resources to build a sophisticated\ntraining pipeline for every image classification task he meets in practice.\nThis work is focusing on reviewing the latest augmentation and regularization\nmethods for the image classification and exploring ways to automatically choose\nsome of the most important hyperparameters: total number of epochs, initial\nlearning rate value and it's schedule. Having a training procedure equipped\nwith a lightweight modern CNN architecture (like bileNetV3 or EfficientNet),\nsufficient level of regularization and adaptive to data learning rate schedule,\nwe can achieve a reasonable performance on a variety of downstream image\nclassification tasks without manual tuning of parameters to each particular\ntask. Resulting models are computationally efficient and can be deployed to CPU\nusing the OpenVINO toolkit. Source code is available as a part of the OpenVINO\nTraining Extensions (https://github.com/openvinotoolkit/training_extensions).",
          "link": "http://arxiv.org/abs/2108.07049",
          "publishedOn": "2021-08-17T01:54:50.455Z",
          "wordCount": 654,
          "title": "Towards Efficient and Data Agnostic Image Classification Training Pipeline for Embedded Systems. (arXiv:2108.07049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifei Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>",
          "description": "In this paper, we focus on recognizing 3D shapes from arbitrary views, i.e.,\narbitrary numbers and positions of viewpoints. It is a challenging and\nrealistic setting for view-based 3D shape recognition. We propose a canonical\nview representation to tackle this challenge. We first transform the original\nfeatures of arbitrary views to a fixed number of view features, dubbed\ncanonical view representation, by aligning the arbitrary view features to a set\nof learnable reference view features using optimal transport. In this way, each\n3D shape with arbitrary views is represented by a fixed number of canonical\nview features, which are further aggregated to generate a rich and robust 3D\nshape representation for shape recognition. We also propose a canonical view\nfeature separation constraint to enforce that the view features in canonical\nview representation can be embedded into scattered points in a Euclidean space.\nExperiments on the ModelNet40, ScanObjectNN, and RGBD datasets show that our\nmethod achieves competitive results under the fixed viewpoint settings, and\nsignificantly outperforms the applicable methods under the arbitrary view\nsetting.",
          "link": "http://arxiv.org/abs/2108.07084",
          "publishedOn": "2021-08-17T01:54:50.448Z",
          "wordCount": 618,
          "title": "Learning Canonical View Representation for 3D Shape Recognition with Arbitrary Views. (arXiv:2108.07084v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1905.10748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengchu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhumade_H/0/1/0/all/0/1\">Hesham Alhumade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Die Hu</a>",
          "description": "Typical adversarial-training-based unsupervised domain adaptation methods are\nvulnerable when the source and target datasets are highly-complex or exhibit a\nlarge discrepancy between their data distributions. Recently, several\nLipschitz-constraint-based methods have been explored. The satisfaction of\nLipschitz continuity guarantees a remarkable performance on a target domain.\nHowever, they lack a mathematical analysis of why a Lipschitz constraint is\nbeneficial to unsupervised domain adaptation and usually perform poorly on\nlarge-scale datasets. In this paper, we take the principle of utilizing a\nLipschitz constraint further by discussing how it affects the error bound of\nunsupervised domain adaptation. A connection between them is built and an\nillustration of how Lipschitzness reduces the error bound is presented. A\n\\textbf{local smooth discrepancy} is defined to measure Lipschitzness of a\ntarget distribution in a pointwise way. When constructing a deep end-to-end\nmodel, to ensure the effectiveness and stability of unsupervised domain\nadaptation, three critical factors are considered in our proposed optimization\nstrategy, i.e., the sample amount of a target domain, dimension and batchsize\nof samples. Experimental results demonstrate that our model performs well on\nseveral standard benchmarks. Our ablation study shows that the sample amount of\na target domain, the dimension and batchsize of samples indeed greatly impact\nLipschitz-constraint-based methods' ability to handle large-scale datasets.\nCode is available at https://github.com/CuthbertCai/SRDA.",
          "link": "http://arxiv.org/abs/1905.10748",
          "publishedOn": "2021-08-17T01:54:50.442Z",
          "wordCount": 715,
          "title": "Learning Smooth Representation for Unsupervised Domain Adaptation. (arXiv:1905.10748v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "We introduce a novel dataset for architectural style classification,\nconsisting of 9,485 images of church buildings. Both images and style labels\nwere sourced from Wikipedia. The dataset can serve as a benchmark for various\nresearch fields, as it combines numerous real-world challenges: fine-grained\ndistinctions between classes based on subtle visual features, a comparatively\nsmall sample size, a highly imbalanced class distribution, a high variance of\nviewpoints, and a hierarchical organization of labels, where only some images\nare labeled at the most precise level. In addition, we provide 631 bounding box\nannotations of characteristic visual features for 139 churches from four major\ncategories. These annotations can, for example, be useful for research on\nfine-grained classification, where additional expert knowledge about\ndistinctive object parts is often available. Images and annotations are\navailable at: https://doi.org/10.5281/zenodo.5166987",
          "link": "http://arxiv.org/abs/2108.06959",
          "publishedOn": "2021-08-17T01:54:50.427Z",
          "wordCount": 580,
          "title": "WikiChurches: A Fine-Grained Dataset of Architectural Styles with Real-World Challenges. (arXiv:2108.06959v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "Since the Lipschitz properties of convolutional neural networks (CNNs) are\nwidely considered to be related to adversarial robustness, we theoretically\ncharacterize the $\\ell_1$ norm and $\\ell_\\infty$ norm of 2D multi-channel\nconvolutional layers and provide efficient methods to compute the exact\n$\\ell_1$ norm and $\\ell_\\infty$ norm. Based on our theorem, we propose a novel\nregularization method termed norm decay, which can effectively reduce the norms\nof convolutional layers and fully-connected layers. Experiments show that\nnorm-regularization methods, including norm decay, weight decay, and singular\nvalue clipping, can improve generalization of CNNs. However, they can slightly\nhurt adversarial robustness. Observing this unexpected phenomenon, we compute\nthe norms of layers in the CNNs trained with three different adversarial\ntraining frameworks and surprisingly find that adversarially robust CNNs have\ncomparable or even larger layer norms than their non-adversarially robust\ncounterparts. Furthermore, we prove that under a mild assumption, adversarially\nrobust classifiers can be achieved using neural networks, and an adversarially\nrobust neural network can have an arbitrarily large Lipschitz constant. For\nthis reason, enforcing small norms on CNN layers may be neither necessary nor\neffective in achieving adversarial robustness. The code is available at\nhttps://github.com/youweiliang/norm_robustness.",
          "link": "http://arxiv.org/abs/2009.08435",
          "publishedOn": "2021-08-17T01:54:50.409Z",
          "wordCount": 710,
          "title": "Large Norms of CNN Layers Do Not Hurt Adversarial Robustness. (arXiv:2009.08435v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1\">Man M. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raake_A/0/1/0/all/0/1\">Alexander Raake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjia Zhou</a>",
          "description": "Recent colorization works implicitly predict the semantic information while\nlearning to colorize black-and-white images. Consequently, the generated color\nis easier to be overflowed, and the semantic faults are invisible. As a human\nexperience in colorization, our brains first detect and recognize the objects\nin the photo, then imagine their plausible colors based on many similar objects\nwe have seen in real life, and finally colorize them, as described in the\nteaser. In this study, we simulate that human-like action to let our network\nfirst learn to understand the photo, then colorize it. Thus, our work can\nprovide plausible colors at a semantic level. Plus, the semantic information of\nthe learned model becomes understandable and able to interact. Additionally, we\nalso prove that Instance Normalization is also a missing ingredient for\ncolorization, then re-design the inference flow of U-Net to have two streams of\ndata, providing an appropriate way of normalizing the feature maps from the\nblack-and-white image and its semantic map. As a result, our network can\nprovide plausible colors competitive to the typical colorization works for\nspecific objects.",
          "link": "http://arxiv.org/abs/2006.07587",
          "publishedOn": "2021-08-17T01:54:50.397Z",
          "wordCount": 649,
          "title": "Semantic-driven Colorization. (arXiv:2006.07587v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Groisser_B/0/1/0/all/0/1\">Benjamin Groisser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_A/0/1/0/all/0/1\">Alon Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1\">Ron Kimmel</a>",
          "description": "The proliferation of 3D scanning technology has driven a need for methods to\ninterpret geometric data, particularly for human subjects. In this paper we\npropose an elegant fusion of regression (bottom-up) and generative (top-down)\nmethods to fit a parametric template model to raw scan meshes.\n\nOur first major contribution is an intrinsic convolutional mesh U-net\narchitecture that predicts pointwise correspondence to a template surface.\nSoft-correspondence is formulated as coordinates in a newly-constructed\nCartesian space. Modeling correspondence as Euclidean proximity enables\nefficient optimization, both for network training and for the next step of the\nalgorithm.\n\nOur second contribution is a generative optimization algorithm that uses the\nU-net correspondence predictions to guide a parametric Iterative Closest Point\nregistration. By employing pre-trained human surface parametric models we\nmaximally leverage domain-specific prior knowledge.\n\nThe pairing of a mesh-convolutional network with generative model fitting\nenables us to predict correspondence for real human surface scans including\nocclusions, partialities, and varying genus (e.g. from self-contact). We\nevaluate the proposed method on the FAUST correspondence challenge where we\nachieve 20% (33%) improvement over state of the art methods for inter- (intra-)\nsubject correspondence.",
          "link": "http://arxiv.org/abs/2108.06695",
          "publishedOn": "2021-08-17T01:54:50.373Z",
          "wordCount": 618,
          "title": "U-mesh: Human Correspondence Matching with Mesh Convolutional Networks. (arXiv:2108.06695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bowren_J/0/1/0/all/0/1\">Joshua Bowren</a>",
          "description": "Neural networks, specifically deep convolutional neural networks, have\nachieved unprecedented performance in various computer vision tasks, but the\nrationale for the computations and structures of successful neural networks is\nnot fully understood. Theories abound for the aptitude of convolutional neural\nnetworks for image classification, but less is understood about why such models\nwould be capable of complex visual tasks such as inference and anomaly\nidentification. Here, we propose a sparse coding interpretation of neural\nnetworks that have ReLU activation and of convolutional neural networks in\nparticular. In sparse coding, when the model's basis functions are assumed to\nbe orthogonal, the optimal coefficients are given by the soft-threshold\nfunction of the basis functions projected onto the input image. In a\nnon-negative variant of sparse coding, the soft-threshold function becomes a\nReLU. Here, we derive these solutions via sparse coding with orthogonal-assumed\nbasis functions, then we derive the convolutional neural network forward\ntransformation from a modified non-negative orthogonal sparse coding model with\nan exponential prior parameter for each sparse coding coefficient. Next, we\nderive a complete convolutional neural network without normalization and\npooling by adding logistic regression to a hierarchical sparse coding model.\nFinally we motivate potentially more robust forward transformations by\nmaintaining sparse priors in convolutional neural networks as well performing a\nstronger nonlinear transformation.",
          "link": "http://arxiv.org/abs/2108.06622",
          "publishedOn": "2021-08-17T01:54:50.368Z",
          "wordCount": 647,
          "title": "A Sparse Coding Interpretation of Neural Networks and Theoretical Implications. (arXiv:2108.06622v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinglin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>",
          "description": "Although current face manipulation techniques achieve impressive performance\nregarding quality and controllability, they are struggling to generate temporal\ncoherent face videos. In this work, we explore to take full advantage of the\ntemporal coherence for video face forgery detection. To achieve this, we\npropose a novel end-to-end framework, which consists of two major stages. The\nfirst stage is a fully temporal convolution network (FTCN). The key insight of\nFTCN is to reduce the spatial convolution kernel size to 1, while maintaining\nthe temporal convolution kernel size unchanged. We surprisingly find this\nspecial design can benefit the model for extracting the temporal features as\nwell as improve the generalization capability. The second stage is a Temporal\nTransformer network, which aims to explore the long-term temporal coherence.\nThe proposed framework is general and flexible, which can be directly trained\nfrom scratch without any pre-training models or external datasets. Extensive\nexperiments show that our framework outperforms existing methods and remains\neffective when applied to detect new sorts of face forgery videos.",
          "link": "http://arxiv.org/abs/2108.06693",
          "publishedOn": "2021-08-17T01:54:50.363Z",
          "wordCount": 615,
          "title": "Exploring Temporal Coherence for More General Video Face Forgery Detection. (arXiv:2108.06693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>",
          "description": "Object proposals have become an integral preprocessing steps of many vision\npipelines including object detection, weakly supervised detection, object\ndiscovery, tracking, etc. Compared to the learning-free methods, learning-based\nproposals have become popular recently due to the growing interest in object\ndetection. The common paradigm is to learn object proposals from data labeled\nwith a set of object regions and their corresponding categories. However, this\napproach often struggles with novel objects in the open world that are absent\nin the training set. In this paper, we identify that the problem is that the\nbinary classifiers in existing proposal methods tend to overfit to the training\ncategories. Therefore, we propose a classification-free Object Localization\nNetwork (OLN) which estimates the objectness of each region purely by how well\nthe location and shape of a region overlap with any ground-truth object (e.g.,\ncenterness and IoU). This simple strategy learns generalizable objectness and\noutperforms existing proposals on cross-category generalization on COCO, as\nwell as cross-dataset evaluation on RoboNet, Object365, and EpicKitchens.\nFinally, we demonstrate the merit of OLN for long-tail object detection on\nlarge vocabulary dataset, LVIS, where we notice clear improvement in rare and\ncommon categories.",
          "link": "http://arxiv.org/abs/2108.06753",
          "publishedOn": "2021-08-17T01:54:50.357Z",
          "wordCount": 630,
          "title": "Learning Open-World Object Proposals without Learning to Classify. (arXiv:2108.06753v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linjiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "As a challenging task of high-level video understanding, weakly supervised\ntemporal action localization has been attracting increasing attention. With\nonly video annotations, most existing methods seek to handle this task with a\nlocalization-by-classification framework, which generally adopts a selector to\nselect snippets of high probabilities of actions or namely the foreground.\nNevertheless, the existing foreground selection strategies have a major\nlimitation of only considering the unilateral relation from foreground to\nactions, which cannot guarantee the foreground-action consistency. In this\npaper, we present a framework named FAC-Net based on the I3D backbone, on which\nthree branches are appended, named class-wise foreground classification branch,\nclass-agnostic attention branch and multiple instance learning branch. First,\nour class-wise foreground classification branch regularizes the relation\nbetween actions and foreground to maximize the foreground-background\nseparation. Besides, the class-agnostic attention branch and multiple instance\nlearning branch are adopted to regularize the foreground-action consistency and\nhelp to learn a meaningful foreground classifier. Within each branch, we\nintroduce a hybrid attention mechanism, which calculates multiple attention\nscores for each snippet, to focus on both discriminative and\nless-discriminative snippets to capture the full action boundaries.\nExperimental results on THUMOS14 and ActivityNet1.3 demonstrate the\nstate-of-the-art performance of our method. Our code is available at\nhttps://github.com/LeonHLJ/FAC-Net.",
          "link": "http://arxiv.org/abs/2108.06524",
          "publishedOn": "2021-08-17T01:54:50.351Z",
          "wordCount": 653,
          "title": "Foreground-Action Consistency Network for Weakly Supervised Temporal Action Localization. (arXiv:2108.06524v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1\">Taeyeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyenson_B/0/1/0/all/0/1\">Benjamin Pyenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebig_J/0/1/0/all/0/1\">Juergen Liebig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>",
          "description": "Biology is both an important application area and a source of motivation for\ndevelopment of advanced machine learning techniques. Although much attention\nhas been paid to large and complex data sets resulting from high-throughput\nsequencing, advances in high-quality video recording technology have begun to\ngenerate similarly rich data sets requiring sophisticated techniques from both\ncomputer vision and time-series analysis. Moreover, just as studying gene\nexpression patterns in one organism can reveal general principles that apply to\nother organisms, the study of complex social interactions in an experimentally\ntractable model system, such as a laboratory ant colony, can provide general\nprinciples about the dynamics of other social groups. Here, we focus on one\nsuch example from the study of reproductive regulation in small laboratory\ncolonies of more than 50 Harpegnathos ants. These ants can be artificially\ninduced to begin a ~20 day process of hierarchy reformation. Although the\nconclusion of this process is conspicuous to a human observer, it remains\nunclear which behaviors during the transient period are contributing to the\nprocess. To address this issue, we explore the potential application of\nOne-class Classification (OC) to the detection of abnormal states in ant\ncolonies for which behavioral data is only available for the normal societal\nconditions during training. Specifically, we build upon the Deep Support Vector\nData Description (DSVDD) and introduce the Inner-Outlier Generator (IO-GEN)\nthat synthesizes fake \"inner outlier\" observations during training that are\nnear the center of the DSVDD data description. We show that IO-GEN increases\nthe reliability of the final OC classifier relative to other DSVDD baselines.\nThis method can be used to screen video frames for which additional human\nobservation is needed.",
          "link": "http://arxiv.org/abs/2009.08626",
          "publishedOn": "2021-08-17T01:54:50.324Z",
          "wordCount": 801,
          "title": "Identification of Abnormal States in Videos of Ants Undergoing Social Phase Change. (arXiv:2009.08626v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.08396",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lablanche_S/0/1/0/all/0/1\">Sebastien Lablanche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lablanche_G/0/1/0/all/0/1\">Gerard Lablanche</a>",
          "description": "In this paper we explain a process of super-resolution reconstruction\nallowing to increase the resolution of an image.The need for high-resolution\ndigital images exists in diverse domains, for example the medical and spatial\ndomains. The obtaining of high-resolution digital images can be made at the\ntime of the shooting, but it is often synonymic of important costs because of\nthe necessary material to avoid such costs, it is known how to use methods of\nsuper-resolution reconstruction, consisting from one or several low resolution\nimages to obtain a high-resolution image. The american patent US 9208537\ndescribes such an algorithm. A zone of one low-resolution image is isolated and\ncategorized according to the information contained in pixels forming the\nborders of the zone. The category of it zone determines the type of\ninterpolation used to add pixels in aforementioned zone, to increase the\nneatness of the images. It is also known how to reconstruct a low-resolution\nimage there high-resolution image by using a model of super-resolution\nreconstruction whose learning is based on networks of neurons and on image or a\npicture library. The demand of chinese patent CN 107563965 and the scientist\npublication \"Pixel Recursive Super Resolution\", R. Dahl, M. Norouzi, J. Shlens\npropose such methods. The aim of this paper is to demonstrate that it is\npossible to reconstruct coherent human faces from very degraded pixelated\nimages with a very fast algorithm, more faster than compressed sensing (CS),\neasier to compute and without deep learning, so without important technology\nresources, i.e. a large database of thousands training images (see\narXiv:2003.13063).\n\nThis technological breakthrough has been patented in 2018 with the demand of\nFrench patent FR 1855485 (https://patents.google.com/patent/FR3082980A1, see\nthe HAL reference https://hal.archives-ouvertes.fr/hal-01875898v1).",
          "link": "http://arxiv.org/abs/1904.08396",
          "publishedOn": "2021-08-17T01:54:50.319Z",
          "wordCount": 794,
          "title": "Process of image super-resolution. (arXiv:1904.08396v8 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.06022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xudong Li</a>",
          "description": "To discover powerful yet compact models is an important goal of neural\narchitecture search. Previous two-stage one-shot approaches are limited by\nsearch space with a fixed depth. It seems handy to include an additional skip\nconnection in the search space to make depths variable. However, it creates a\nlarge range of perturbation during supernet training and it has difficulty\ngiving a confident ranking for subnetworks. In this paper, we discover that\nskip connections bring about significant feature inconsistency compared with\nother operations, which potentially degrades the supernet performance. Based on\nthis observation, we tackle the problem by imposing an equivariant learnable\nstabilizer to homogenize such disparities. Experiments show that our proposed\nstabilizer helps to improve the supernet's convergence as well as ranking\nperformance. With an evolutionary search backend that incorporates the\nstabilized supernet as an evaluator, we derive a family of state-of-the-art\narchitectures, the SCARLET series of several depths, especially SCARLET-A\nobtains 76.9% top-1 accuracy on ImageNet. Code is available at\nhttps://github.com/xiaomi-automl/ScarletNAS.",
          "link": "http://arxiv.org/abs/1908.06022",
          "publishedOn": "2021-08-17T01:54:50.313Z",
          "wordCount": 688,
          "title": "SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search. (arXiv:1908.06022v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasu_M/0/1/0/all/0/1\">Malolan Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abreu_N/0/1/0/all/0/1\">Nelson Abreu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasquez_R/0/1/0/all/0/1\">Raysa V&#xe1;squez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">Christian L&#xf3;pez</a>",
          "description": "Vehicle counting systems can help with vehicle analysis and traffic incident\ndetection. Unfortunately, most existing methods require some level of human\ninput to identify the Region of interest (ROI), movements of interest, or to\nestablish a reference point or line to count vehicles from traffic cameras.\nThis work introduces a method to count vehicles from traffic videos that\nautomatically identifies the ROI for the camera, as well as the driving\ntrajectories of the vehicles. This makes the method feasible to use with\nPan-Tilt-Zoom cameras, which are frequently used in developing countries.\nPreliminary results indicate that the proposed method achieves an average\nintersection over the union of 57.05% for the ROI and a mean absolute error of\njust 17.44% at counting vehicles of the traffic video cameras tested.",
          "link": "http://arxiv.org/abs/2108.07135",
          "publishedOn": "2021-08-17T01:54:50.301Z",
          "wordCount": 578,
          "title": "Vehicle-counting with Automatic Region-of-Interest and Driving-Trajectory detection. (arXiv:2108.07135v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinghong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shangxuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yudong Chen</a>",
          "description": "Hashing technology has been widely used in image retrieval due to its\ncomputational and storage efficiency. Recently, deep unsupervised hashing\nmethods have attracted increasing attention due to the high cost of human\nannotations in the real world and the superiority of deep learning technology.\nHowever, most deep unsupervised hashing methods usually pre-compute a\nsimilarity matrix to model the pairwise relationship in the pre-trained feature\nspace. Then this similarity matrix would be used to guide hash learning, in\nwhich most of the data pairs are treated equivalently. The above process is\nconfronted with the following defects: 1) The pre-computed similarity matrix is\ninalterable and disconnected from the hash learning process, which cannot\nexplore the underlying semantic information. 2) The informative data pairs may\nbe buried by the large number of less-informative data pairs. To solve the\naforementioned problems, we propose a \\textbf{Deep Self-Adaptive\nHashing~(DSAH)} model to adaptively capture the semantic information with two\nspecial designs: \\textbf{Adaptive Neighbor Discovery~(AND)} and\n\\textbf{Pairwise Information Content~(PIC)}. Firstly, we adopt the AND to\ninitially construct a neighborhood-based similarity matrix, and then refine\nthis initial similarity matrix with a novel update strategy to further\ninvestigate the semantic structure behind the learned representation. Secondly,\nwe measure the priorities of data pairs with PIC and assign adaptive weights to\nthem, which is relies on the assumption that more dissimilar data pairs contain\nmore discriminative information for hash learning. Extensive experiments on\nseveral benchmark datasets demonstrate that the above two technologies\nfacilitate the deep hashing model to achieve superior performance in a\nself-adaptive manner.",
          "link": "http://arxiv.org/abs/2108.07094",
          "publishedOn": "2021-08-17T01:54:50.295Z",
          "wordCount": 698,
          "title": "Deep Self-Adaptive Hashing for Image Retrieval. (arXiv:2108.07094v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.09843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Weijun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaohu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Rongxing Lu</a>",
          "description": "Although federated learning improves privacy of training data by exchanging\nlocal gradients or parameters rather than raw data, the adversary still can\nleverage local gradients and parameters to obtain local training data by\nlaunching reconstruction and membership inference attacks. To defend such\nprivacy attacks, many noises perturbation methods (like differential privacy or\nCountSketch matrix) have been widely designed. However, the strong defence\nability and high learning accuracy of these schemes cannot be ensured at the\nsame time, which will impede the wide application of FL in practice (especially\nfor medical or financial institutions that require both high accuracy and\nstrong privacy guarantee). To overcome this issue, in this paper, we propose\n\\emph{an efficient model perturbation method for federated learning} to defend\nreconstruction and membership inference attacks launched by curious clients. On\nthe one hand, similar to the differential privacy, our method also selects\nrandom numbers as perturbed noises added to the global model parameters, and\nthus it is very efficient and easy to be integrated in practice. Meanwhile, the\nrandom selected noises are positive real numbers and the corresponding value\ncan be arbitrarily large, and thus the strong defence ability can be ensured.\nOn the other hand, unlike differential privacy or other perturbation methods\nthat cannot eliminate the added noises, our method allows the server to recover\nthe true gradients by eliminating the added noises. Therefore, our method does\nnot hinder learning accuracy at all.",
          "link": "http://arxiv.org/abs/2002.09843",
          "publishedOn": "2021-08-17T01:54:50.278Z",
          "wordCount": 750,
          "title": "An Accuracy-Lossless Perturbation Method for Defending Privacy Attacks in Federated Learning. (arXiv:2002.09843v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrzanowski_W/0/1/0/all/0/1\">Wojciech Chrzanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>",
          "description": "Automatic 3D neuron reconstruction is critical for analysing the morphology\nand functionality of neurons in brain circuit activities. However, the\nperformance of existing tracing algorithms is hinged by the low image quality.\nRecently, a series of deep learning based segmentation methods have been\nproposed to improve the quality of raw 3D optical image stacks by removing\nnoises and restoring neuronal structures from low-contrast background. Due to\nthe variety of neuron morphology and the lack of large neuron datasets, most of\ncurrent neuron segmentation models rely on introducing complex and\nspecially-designed submodules to a base architecture with the aim of encoding\nbetter feature representations. Though successful, extra burden would be put on\ncomputation during inference. Therefore, rather than modifying the base\nnetwork, we shift our focus to the dataset itself. The encoder-decoder backbone\nused in most neuron segmentation models attends only intra-volume voxel points\nto learn structural features of neurons but neglect the shared intrinsic\nsemantic features of voxels belonging to the same category among different\nvolumes, which is also important for expressive representation learning. Hence,\nto better utilise the scarce dataset, we propose to explicitly exploit such\nintrinsic features of voxels through a novel voxel-level cross-volume\nrepresentation learning paradigm on the basis of an encoder-decoder\nsegmentation model. Our method introduces no extra cost during inference.\nEvaluated on 42 3D neuron images from BigNeuron project, our proposed method is\ndemonstrated to improve the learning ability of the original segmentation model\nand further enhancing the reconstruction performance.",
          "link": "http://arxiv.org/abs/2108.06522",
          "publishedOn": "2021-08-17T01:54:50.271Z",
          "wordCount": 697,
          "title": "Voxel-wise Cross-Volume Representation Learning for 3D Neuron Reconstruction. (arXiv:2108.06522v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hanbyul Joo</a>",
          "description": "Most existing monocular 3D pose estimation approaches only focus on a single\nbody part, neglecting the fact that the essential nuance of human motion is\nconveyed through a concert of subtle movements of face, hands, and body. In\nthis paper, we present FrankMocap, a fast and accurate whole-body 3D pose\nestimation system that can produce 3D face, hands, and body simultaneously from\nin-the-wild monocular images. The core idea of FrankMocap is its modular\ndesign: We first run 3D pose regression methods for face, hands, and body\nindependently, followed by composing the regression outputs via an integration\nmodule. The separate regression modules allow us to take full advantage of\ntheir state-of-the-art performances without compromising the original accuracy\nand reliability in practice. We develop three different integration modules\nthat trade off between latency and accuracy. All of them are capable of\nproviding simple yet effective solutions to unify the separate outputs into\nseamless whole-body pose estimation results. We quantitatively and\nqualitatively demonstrate that our modularized system outperforms both the\noptimization-based and end-to-end methods of estimating whole-body pose.",
          "link": "http://arxiv.org/abs/2108.06428",
          "publishedOn": "2021-08-17T01:54:50.261Z",
          "wordCount": 643,
          "title": "FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration. (arXiv:2108.06428v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mac_B/0/1/0/all/0/1\">Brandon Mac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moody_A/0/1/0/all/0/1\">Alan R. Moody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_A/0/1/0/all/0/1\">April Khademi</a>",
          "description": "One of the key limitations in machine learning models is poor performance on\ndata that is out of the domain of the training distribution. This is especially\ntrue for image analysis in magnetic resonance (MR) imaging, as variations in\nhardware and software create non-standard intensities, contrasts, and noise\ndistributions across scanners. Recently, image translation models have been\nproposed to augment data across domains to create synthetic data points. In\nthis paper, we investigate the application an unsupervised image translation\nmodel to augment MR images from a source dataset to a target dataset.\nSpecifically, we want to evaluate how well these models can create synthetic\ndata points representative of the target dataset through image translation, and\nto see if a segmentation model trained these synthetic data points would\napproach the performance of a model trained directly on the target dataset. We\nconsider three configurations of augmentation between datasets consisting of\ntranslation between images, between scanner vendors, and from labels to images.\nIt was found that the segmentation models trained on synthetic data from labels\nto images configuration yielded the closest performance to the segmentation\nmodel trained directly on the target dataset. The Dice coeffcient score per\neach target vendor (GE, Siemens, Philips) for training on synthetic data was\n0.63, 0.64, and 0.58, compared to training directly on target dataset was 0.65,\n0.72, and 0.61.",
          "link": "http://arxiv.org/abs/2108.06434",
          "publishedOn": "2021-08-17T01:54:50.237Z",
          "wordCount": 660,
          "title": "Adapting to Unseen Vendor Domains for MRI Lesion Segmentation. (arXiv:2108.06434v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>",
          "description": "Generating conversational gestures from speech audio is challenging due to\nthe inherent one-to-many mapping between audio and body motions. Conventional\nCNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of\nall possible target motions, resulting in plain/boring motions during\ninference. In order to overcome this problem, we propose a novel conditional\nvariational autoencoder (VAE) that explicitly models one-to-many\naudio-to-motion mapping by splitting the cross-modal latent code into shared\ncode and motion-specific code. The shared code mainly models the strong\ncorrelation between audio and motion (such as the synchronized audio and motion\nbeats), while the motion-specific code captures diverse motion information\nindependent of the audio. However, splitting the latent code into two parts\nposes training difficulties for the VAE model. A mapping network facilitating\nrandom sampling along with other techniques including relaxed motion loss,\nbicycle constraint, and diversity loss are designed to better train the VAE.\nExperiments on both 3D and 2D motion datasets verify that our method generates\nmore realistic and diverse motions than state-of-the-art methods,\nquantitatively and qualitatively. Finally, we demonstrate that our method can\nbe readily used to generate motion sequences with user-specified motion clips\non the timeline. Code and more results are at\nhttps://jingli513.github.io/audio2gestures.",
          "link": "http://arxiv.org/abs/2108.06720",
          "publishedOn": "2021-08-17T01:54:50.224Z",
          "wordCount": 648,
          "title": "Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders. (arXiv:2108.06720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zeyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_P/0/1/0/all/0/1\">Pierce KH Chow</a>",
          "description": "Accurate automatic liver and tumor segmentation plays a vital role in\ntreatment planning and disease monitoring. Recently, deep convolutional neural\nnetwork (DCNNs) has obtained tremendous success in 2D and 3D medical image\nsegmentation. However, 2D DCNNs cannot fully leverage the inter-slice\ninformation, while 3D DCNNs are computationally expensive and memory intensive.\nTo address these issues, we first propose a novel dense-sparse training flow\nfrom a data perspective, in which, densely adjacent slices and sparsely\nadjacent slices are extracted as inputs for regularizing DCNNs, thereby\nimproving the model performance. Moreover, we design a 2.5D light-weight\nnnU-Net from a network perspective, in which, depthwise separable convolutions\nare adopted to improve the efficiency. Extensive experiments on the LiTS\ndataset have demonstrated the superiority of the proposed method.",
          "link": "http://arxiv.org/abs/2108.06761",
          "publishedOn": "2021-08-17T01:54:50.216Z",
          "wordCount": 599,
          "title": "Multi-Slice Dense-Sparse Learning for Efficient Liver and Tumor Segmentation. (arXiv:2108.06761v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_B/0/1/0/all/0/1\">Baitan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>",
          "description": "Considering the fact that students have different abilities to understand the\nknowledge imparted by teachers, a multi-granularity distillation mechanism is\nproposed for transferring more understandable knowledge for student networks. A\nmulti-granularity self-analyzing module of the teacher network is designed,\nwhich enables the student network to learn knowledge from different teaching\npatterns. Furthermore, a stable excitation scheme is proposed for robust\nsupervision for the student training. The proposed distillation mechanism can\nbe embedded into different distillation frameworks, which are taken as\nbaselines. Experiments show the mechanism improves the accuracy by 0.58% on\naverage and by 1.08% in the best over the baselines, which makes its\nperformance superior to the state-of-the-arts. It is also exploited that the\nstudent's ability of fine-tuning and robustness to noisy inputs can be improved\nvia the proposed mechanism. The code is available at\nhttps://github.com/shaoeric/multi-granularity-distillation.",
          "link": "http://arxiv.org/abs/2108.06681",
          "publishedOn": "2021-08-17T01:54:50.210Z",
          "wordCount": 568,
          "title": "Multi-granularity for knowledge distillation. (arXiv:2108.06681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06515",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhonglan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Dongqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingfan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qilong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingyu Liu</a>",
          "description": "In industry, there exist plenty of scenarios where old gray photos need to be\nautomatically colored, such as video sites and archives. In this paper, we\npresent the HistoryNet focusing on historical person's diverse high fidelity\nclothing colorization based on fine grained semantic understanding and prior.\nColorization of historical persons is realistic and practical, however,\nexisting methods do not perform well in the regards. In this paper, a\nHistoryNet including three parts, namely, classification, fine grained semantic\nparsing and colorization, is proposed. Classification sub-module supplies\nclassifying of images according to the eras, nationalities and garment types;\nParsing sub-network supplies the semantic for person contours, clothing and\nbackground in the image to achieve more accurate colorization of clothes and\npersons and prevent color overflow. In the training process, we integrate\nclassification and semantic parsing features into the coloring generation\nnetwork to improve colorization. Through the design of classification and\nparsing subnetwork, the accuracy of image colorization can be improved and the\nboundary of each part of image can be more clearly. Moreover, we also propose a\nnovel Modern Historical Movies Dataset (MHMD) containing 1,353,166 images and\n42 labels of eras, nationalities, and garment types for automatic colorization\nfrom 147 historical movies or TV series made in modern time. Various\nquantitative and qualitative comparisons demonstrate that our method\noutperforms the state-of-the-art colorization methods, especially on military\nuniforms, which has correct colors according to the historical literatures.",
          "link": "http://arxiv.org/abs/2108.06515",
          "publishedOn": "2021-08-17T01:54:50.203Z",
          "wordCount": 694,
          "title": "Focusing on Persons: Colorizing Old Images Learning from Modern Historical Movies. (arXiv:2108.06515v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_X/0/1/0/all/0/1\">Xin Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>",
          "description": "Unsupervised representation learning has achieved outstanding performances\nusing centralized data available on the Internet. However, the increasing\nawareness of privacy protection limits sharing of decentralized unlabeled image\ndata that grows explosively in multiple parties (e.g., mobile phones and\ncameras). As such, a natural problem is how to leverage these data to learn\nvisual representations for downstream tasks while preserving data privacy. To\naddress this problem, we propose a novel federated unsupervised learning\nframework, FedU. In this framework, each party trains models from unlabeled\ndata independently using contrastive learning with an online network and a\ntarget network. Then, a central server aggregates trained models and updates\nclients' models with the aggregated model. It preserves data privacy as each\nparty only has access to its raw data. Decentralized data among multiple\nparties are normally non-independent and identically distributed (non-IID),\nleading to performance degradation. To tackle this challenge, we propose two\nsimple but effective methods: 1) We design the communication protocol to upload\nonly the encoders of online networks for server aggregation and update them\nwith the aggregated encoder; 2) We introduce a new module to dynamically decide\nhow to update predictors based on the divergence caused by non-IID. The\npredictor is the other component of the online network. Extensive experiments\nand ablations demonstrate the effectiveness and significance of FedU. It\noutperforms training with only one party by over 5% and other methods by over\n14% in linear and semi-supervised evaluation on non-IID data.",
          "link": "http://arxiv.org/abs/2108.06492",
          "publishedOn": "2021-08-17T01:54:50.196Z",
          "wordCount": 696,
          "title": "Collaborative Unsupervised Visual Representation Learning from Decentralized Data. (arXiv:2108.06492v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Changwoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hojun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>",
          "description": "We present PICCOLO, a simple and efficient algorithm for omnidirectional\nlocalization. Given a colored point cloud and a 360 panorama image of a scene,\nour objective is to recover the camera pose at which the panorama image is\ntaken. Our pipeline works in an off-the-shelf manner with a single image given\nas a query and does not require any training of neural networks or collecting\nground-truth poses of images. Instead, we match each point cloud color to the\nholistic view of the panorama image with gradient-descent optimization to find\nthe camera pose. Our loss function, called sampling loss, is point\ncloud-centric, evaluated at the projected location of every point in the point\ncloud. In contrast, conventional photometric loss is image-centric, comparing\ncolors at each pixel location. With a simple change in the compared entities,\nsampling loss effectively overcomes the severe visual distortion of\nomnidirectional images, and enjoys the global context of the 360 view to handle\nchallenging scenarios for visual localization. PICCOLO outperforms existing\nomnidirectional localization algorithms in both accuracy and stability when\nevaluated in various environments.",
          "link": "http://arxiv.org/abs/2108.06545",
          "publishedOn": "2021-08-17T01:54:50.190Z",
          "wordCount": 612,
          "title": "PICCOLO: Point Cloud-Centric Omnidirectional Localization. (arXiv:2108.06545v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1\">Or Bar-Shira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "In this chapter, we review biomedical applications and breakthroughs via\nleveraging algorithm unrolling, an important technique that bridges between\ntraditional iterative algorithms and modern deep learning techniques. To\nprovide context, we start by tracing the origin of algorithm unrolling and\nproviding a comprehensive tutorial on how to unroll iterative algorithms into\ndeep networks. We then extensively cover algorithm unrolling in a wide variety\nof biomedical imaging modalities and delve into several representative recent\nworks in detail. Indeed, there is a rich history of iterative algorithms for\nbiomedical image synthesis, which makes the field ripe for unrolling\ntechniques. In addition, we put algorithm unrolling into a broad perspective,\nin order to understand why it is particularly effective and discuss recent\ntrends. Finally, we conclude the chapter by discussing open challenges, and\nsuggesting future research directions.",
          "link": "http://arxiv.org/abs/2108.06637",
          "publishedOn": "2021-08-17T01:54:50.169Z",
          "wordCount": 566,
          "title": "Deep Algorithm Unrolling for Biomedical Imaging. (arXiv:2108.06637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Video captioning targets interpreting the complex visual contents as text\ndescriptions, which requires the model to fully understand video scenes\nincluding objects and their interactions. Prevailing methods adopt\noff-the-shelf object detection networks to give object proposals and use the\nattention mechanism to model the relations between objects. They often miss\nsome undefined semantic concepts of the pretrained model and fail to identify\nexact predicate relationships between objects. In this paper, we investigate an\nopen research task of generating text descriptions for the given videos, and\npropose Cross-Modal Graph (CMG) with meta concepts for video captioning.\nSpecifically, to cover the useful semantic concepts in video captions, we\nweakly learn the corresponding visual regions for text descriptions, where the\nassociated visual regions and textual words are named cross-modal meta\nconcepts. We further build meta concept graphs dynamically with the learned\ncross-modal meta concepts. We also construct holistic video-level and local\nframe-level video graphs with the predicted predicates to model video sequence\nstructures. We validate the efficacy of our proposed techniques with extensive\nexperiments and achieve state-of-the-art results on two public datasets.",
          "link": "http://arxiv.org/abs/2108.06458",
          "publishedOn": "2021-08-17T01:54:50.152Z",
          "wordCount": 617,
          "title": "Cross-Modal Graph with Meta Concepts for Video Captioning. (arXiv:2108.06458v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1\">Vinod Kumar Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sukhdeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>",
          "description": "Handwritten character recognition (HCR) is a challenging learning problem in\npattern recognition, mainly due to similarity in structure of characters,\ndifferent handwriting styles, noisy datasets and a large variety of languages\nand scripts. HCR problem is studied extensively for a few decades but there is\nvery limited research on script independent models. This is because of factors,\nlike, diversity of scripts, focus of the most of conventional research efforts\non handcrafted feature extraction techniques which are language/script specific\nand are not always available, and unavailability of public datasets and codes\nto reproduce the results. On the other hand, deep learning has witnessed huge\nsuccess in different areas of pattern recognition, including HCR, and provides\nend-to-end learning, i.e., automated feature extraction and recognition. In\nthis paper, we have proposed a novel deep learning architecture which exploits\ntransfer learning and image-augmentation for end-to-end learning for script\nindependent handwritten character recognition, called HCR-Net. The network is\nbased on a novel transfer learning approach for HCR, where some of lower layers\nof a pre-trained VGG16 network are utilised. Due to transfer learning and\nimage-augmentation, HCR-Net provides faster training, better performance and\nbetter generalisations. The experimental results on publicly available datasets\nof Bangla, Punjabi, Hindi, English, Swedish, Urdu, Farsi, Tibetan, Kannada,\nMalayalam, Telugu, Marathi, Nepali and Arabic languages prove the efficacy of\nHCR-Net and establishes several new benchmarks. For reproducibility of the\nresults and for the advancements of the HCR research, complete code is publicly\nreleased at \\href{https://github.com/jmdvinodjmd/HCR-Net}{GitHub}.",
          "link": "http://arxiv.org/abs/2108.06663",
          "publishedOn": "2021-08-17T01:54:50.123Z",
          "wordCount": 704,
          "title": "HCR-Net: A deep learning based script independent handwritten character recognition network. (arXiv:2108.06663v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruohao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Dantong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenbo Li</a>",
          "description": "Most recent transformer-based models show impressive performance on vision\ntasks, even better than Convolution Neural Networks (CNN). In this work, we\npresent a novel, flexible, and effective transformer-based model for\nhigh-quality instance segmentation. The proposed method, Segmenting Objects\nwith TRansformers (SOTR), simplifies the segmentation pipeline, building on an\nalternative CNN backbone appended with two parallel subtasks: (1) predicting\nper-instance category via transformer and (2) dynamically generating\nsegmentation mask with the multi-level upsampling module. SOTR can effectively\nextract lower-level feature representations and capture long-range context\ndependencies by Feature Pyramid Network (FPN) and twin transformer,\nrespectively. Meanwhile, compared with the original transformer, the proposed\ntwin transformer is time- and resource-efficient since only a row and a column\nattention are involved to encode pixels. Moreover, SOTR is easy to be\nincorporated with various CNN backbones and transformer model variants to make\nconsiderable improvements for the segmentation accuracy and training\nconvergence. Extensive experiments show that our SOTR performs well on the MS\nCOCO dataset and surpasses state-of-the-art instance segmentation approaches.\nWe hope our simple but strong framework could serve as a preferment baseline\nfor instance-level recognition. Our code is available at\nhttps://github.com/easton-cau/SOTR.",
          "link": "http://arxiv.org/abs/2108.06747",
          "publishedOn": "2021-08-17T01:54:50.092Z",
          "wordCount": 622,
          "title": "SOTR: Segmenting Objects with Transformers. (arXiv:2108.06747v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Thanh T. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thang V. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tung T. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>",
          "description": "Chest radiograph (CXR) interpretation in pediatric patients is error-prone\nand requires a high level of understanding of radiologic expertise. Recently,\ndeep convolutional neural networks (D-CNNs) have shown remarkable performance\nin interpreting CXR in adults. However, there is a lack of evidence indicating\nthat D-CNNs can recognize accurately multiple lung pathologies from pediatric\nCXR scans. In particular, the development of diagnostic models for the\ndetection of pediatric chest diseases faces significant challenges such as (i)\nlack of physician-annotated datasets and (ii) class imbalance problems. In this\npaper, we retrospectively collect a large dataset of 5,017 pediatric CXR scans,\nfor which each is manually labeled by an experienced radiologist for the\npresence of 10 common pathologies. A D-CNN model is then trained on 3,550\nannotated scans to classify multiple pediatric lung pathologies automatically.\nTo address the high-class imbalance issue, we propose to modify and apply\n\"Distribution-Balanced loss\" for training D-CNNs which reshapes the standard\nBinary-Cross Entropy loss (BCE) to efficiently learn harder samples by\ndown-weighting the loss assigned to the majority classes. On an independent\ntest set of 777 studies, the proposed approach yields an area under the\nreceiver operating characteristic (AUC) of 0.709 (95% CI, 0.690-0.729). The\nsensitivity, specificity, and F1-score at the cutoff value are 0.722\n(0.694-0.750), 0.579 (0.563-0.595), and 0.389 (0.373-0.405), respectively.\nThese results significantly outperform previous state-of-the-art methods on\nmost of the target diseases. Moreover, our ablation studies validate the\neffectiveness of the proposed loss function compared to other standard losses,\ne.g., BCE and Focal Loss, for this learning task. Overall, we demonstrate the\npotential of D-CNNs in interpreting pediatric CXRs.",
          "link": "http://arxiv.org/abs/2108.06486",
          "publishedOn": "2021-08-17T01:54:50.076Z",
          "wordCount": 740,
          "title": "Learning to Automatically Diagnose Multiple Diseases in Pediatric Chest Radiographs Using Deep Convolutional Neural Networks. (arXiv:2108.06486v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuting He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Youyong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaomei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaobo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dillenseger_J/0/1/0/all/0/1\">Jean-Louis Dillenseger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coatrieux_J/0/1/0/all/0/1\">Jean-Louis Coatrieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanyu Yang</a>",
          "description": "Renal compartment segmentation on CT images targets on extracting the 3D\nstructure of renal compartments from abdominal CTA images and is of great\nsignificance to the diagnosis and treatment for kidney diseases. However, due\nto the unclear compartment boundary, thin compartment structure and large\nanatomy variation of 3D kidney CT images, deep-learning based renal compartment\nsegmentation is a challenging task. We propose a novel weakly supervised\nlearning framework, Cycle Prototype Network, for 3D renal compartment\nsegmentation. It has three innovations: 1) A Cycle Prototype Learning (CPL) is\nproposed to learn consistency for generalization. It learns from pseudo labels\nthrough the forward process and learns consistency regularization through the\nreverse process. The two processes make the model robust to noise and\nlabel-efficient. 2) We propose a Bayes Weakly Supervised Module (BWSM) based on\ncross-period prior knowledge. It learns prior knowledge from cross-period\nunlabeled data and perform error correction automatically, thus generates\naccurate pseudo labels. 3) We present a Fine Decoding Feature Extractor (FDFE)\nfor fine-grained feature extraction. It combines global morphology information\nand local detail information to obtain feature maps with sharp detail, so the\nmodel will achieve fine segmentation on thin structures. Our model achieves\nDice of 79.1% and 78.7% with only four labeled images, achieving a significant\nimprovement by about 20% than typical prototype model PANet.",
          "link": "http://arxiv.org/abs/2108.06669",
          "publishedOn": "2021-08-17T01:54:50.063Z",
          "wordCount": 686,
          "title": "CPNet: Cycle Prototype Network for Weakly-supervised 3D Renal Compartments Segmentation on CT Images. (arXiv:2108.06669v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_D/0/1/0/all/0/1\">Dung V. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>",
          "description": "X-ray imaging in DICOM format is the most commonly used imaging modality in\nclinical practice, resulting in vast, non-normalized databases. This leads to\nan obstacle in deploying AI solutions for analyzing medical images, which often\nrequires identifying the right body part before feeding the image into a\nspecified AI model. This challenge raises the need for an automated and\nefficient approach to classifying body parts from X-ray scans. Unfortunately,\nto the best of our knowledge, there is no open tool or framework for this task\nto date. To fill this lack, we introduce a DICOM Imaging Router that deploys\ndeep CNNs for categorizing unknown DICOM X-ray images into five anatomical\ngroups: abdominal, adult chest, pediatric chest, spine, and others. To this\nend, a large-scale X-ray dataset consisting of 16,093 images has been collected\nand manually classified. We then trained a set of state-of-the-art deep CNNs\nusing a training set of 11,263 images. These networks were then evaluated on an\nindependent test set of 2,419 images and showed superior performance in\nclassifying the body parts. Specifically, our best performing model achieved a\nrecall of 0.982 (95% CI, 0.977-0.988), a precision of 0.985 (95% CI,\n0.975-0.989) and a F1-score of 0.981 (95% CI, 0.976-0.987), whilst requiring\nless computation for inference (0.0295 second per image). Our external validity\non 1,000 X-ray images shows the robustness of the proposed approach across\nhospitals. These remarkable performances indicate that deep CNNs can accurately\nand effectively differentiate human body parts from X-ray scans, thereby\nproviding potential benefits for a wide range of applications in clinical\nsettings. The dataset, codes, and trained deep learning models from this study\nwill be made publicly available on our project website at https://vindr.ai/.",
          "link": "http://arxiv.org/abs/2108.06490",
          "publishedOn": "2021-08-17T01:54:50.030Z",
          "wordCount": 756,
          "title": "DICOM Imaging Router: An Open Deep Learning Framework for Classification of Body Parts from DICOM X-ray Scans. (arXiv:2108.06490v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "Facial expression recognition (FER) has emerged as an important component of\nhuman-computer interaction systems. Despite recent advancements in FER,\nperformance often drops significantly for non-frontal facial images. We propose\nContrastive Learning of Multi-view facial Expressions (CL-MEx) to exploit\nfacial images captured simultaneously from different angles towards FER. CL-MEx\nis a two-step training framework. In the first step, an encoder network is\npre-trained with the proposed self-supervised contrastive loss, where it learns\nto generate view-invariant embeddings for different views of a subject. The\nmodel is then fine-tuned with labeled data in a supervised setting. We\ndemonstrate the performance of the proposed method on two multi-view FER\ndatasets, KDEF and DDCF, where state-of-the-art performances are achieved.\nFurther experiments show the robustness of our method in dealing with\nchallenging angles and reduced amounts of labeled data.",
          "link": "http://arxiv.org/abs/2108.06723",
          "publishedOn": "2021-08-17T01:54:49.999Z",
          "wordCount": 580,
          "title": "Self-supervised Contrastive Learning of Multi-view Facial Expressions. (arXiv:2108.06723v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_T/0/1/0/all/0/1\">Takaki Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prugel_Bennett_A/0/1/0/all/0/1\">Adam Pr&#xfc;gel-Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_S/0/1/0/all/0/1\">Stefan B. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizarro_O/0/1/0/all/0/1\">Oscar Pizarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thornton_B/0/1/0/all/0/1\">Blair Thornton</a>",
          "description": "This paper describes Georeference Contrastive Learning of visual\nRepresentation (GeoCLR) for efficient training of deep-learning Convolutional\nNeural Networks (CNNs). The method leverages georeference information by\ngenerating a similar image pair using images taken of nearby locations, and\ncontrasting these with an image pair that is far apart. The underlying\nassumption is that images gathered within a close distance are more likely to\nhave similar visual appearance, where this can be reasonably satisfied in\nseafloor robotic imaging applications where image footprints are limited to\nedge lengths of a few metres and are taken so that they overlap along a\nvehicle's trajectory, whereas seafloor substrates and habitats have patch sizes\nthat are far larger. A key advantage of this method is that it is\nself-supervised and does not require any human input for CNN training. The\nmethod is computationally efficient, where results can be generated between\ndives during multi-day AUV missions using computational resources that would be\naccessible during most oceanic field trials. We apply GeoCLR to habitat\nclassification on a dataset that consists of ~86k images gathered using an\nAutonomous Underwater Vehicle (AUV). We demonstrate how the latent\nrepresentations generated by GeoCLR can be used to efficiently guide human\nannotation efforts, where the semi-supervised framework improves classification\naccuracy by an average of 11.8 % compared to state-of-the-art transfer learning\nusing the same CNN and equivalent number of human annotations for training.",
          "link": "http://arxiv.org/abs/2108.06421",
          "publishedOn": "2021-08-17T01:54:49.985Z",
          "wordCount": 676,
          "title": "GeoCLR: Georeference Contrastive Learning for Efficient Seafloor Image Interpretation. (arXiv:2108.06421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>",
          "description": "Person re-identification (ReID) aims to re-identify a person from\nnon-overlapping camera views. Since person ReID data contains sensitive\npersonal information, researchers have adopted federated learning, an emerging\ndistributed training method, to mitigate the privacy leakage risks. However,\nexisting studies rely on data labels that are laborious and time-consuming to\nobtain. We present FedUReID, a federated unsupervised person ReID system to\nlearn person ReID models without any labels while preserving privacy. FedUReID\nenables in-situ model training on edges with unlabeled data. A cloud server\naggregates models from edges instead of centralizing raw data to preserve data\nprivacy. Moreover, to tackle the problem that edges vary in data volumes and\ndistributions, we personalize training in edges with joint optimization of\ncloud and edge. Specifically, we propose personalized epoch to reassign\ncomputation throughout training, personalized clustering to iteratively predict\nsuitable labels for unlabeled data, and personalized update to adapt the server\naggregated model to each edge. Extensive experiments on eight person ReID\ndatasets demonstrate that FedUReID not only achieves higher accuracy but also\nreduces computation cost by 29%. Our FedUReID system with the joint\noptimization will shed light on implementing federated learning to more\nmultimedia tasks without data labels.",
          "link": "http://arxiv.org/abs/2108.06493",
          "publishedOn": "2021-08-17T01:54:49.979Z",
          "wordCount": 653,
          "title": "Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised Person Re-identification. (arXiv:2108.06493v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dennis Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>",
          "description": "Recent progress in 3D object detection from single images leverages monocular\ndepth estimation as a way to produce 3D pointclouds, turning cameras into\npseudo-lidar sensors. These two-stage detectors improve with the accuracy of\nthe intermediate depth estimation network, which can itself be improved without\nmanual labels via large-scale self-supervised learning. However, they tend to\nsuffer from overfitting more than end-to-end methods, are more complex, and the\ngap with similar lidar-based detectors remains significant. In this work, we\npropose an end-to-end, single stage, monocular 3D object detector, DD3D, that\ncan benefit from depth pre-training like pseudo-lidar methods, but without\ntheir limitations. Our architecture is designed for effective information\ntransfer between depth estimation and 3D detection, allowing us to scale with\nthe amount of unlabeled pre-training data. Our method achieves state-of-the-art\nresults on two challenging benchmarks, with 16.34% and 9.28% AP for Cars and\nPedestrians (respectively) on the KITTI-3D benchmark, and 41.5% mAP on\nNuScenes.",
          "link": "http://arxiv.org/abs/2108.06417",
          "publishedOn": "2021-08-17T01:54:49.973Z",
          "wordCount": 598,
          "title": "Is Pseudo-Lidar needed for Monocular 3D Object detection?. (arXiv:2108.06417v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongbin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiaoyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsui Hin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Huaqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "We present MMOCR-an open-source toolbox which provides a comprehensive\npipeline for text detection and recognition, as well as their downstream tasks\nsuch as named entity recognition and key information extraction. MMOCR\nimplements 14 state-of-the-art algorithms, which is significantly more than all\nthe existing open-source OCR projects we are aware of to date. To facilitate\nfuture research and industrial applications of text recognition-related\nproblems, we also provide a large number of trained models and detailed\nbenchmarks to give insights into the performance of text detection, recognition\nand understanding. MMOCR is publicly released at\nhttps://github.com/open-mmlab/mmocr.",
          "link": "http://arxiv.org/abs/2108.06543",
          "publishedOn": "2021-08-17T01:54:49.951Z",
          "wordCount": 563,
          "title": "MMOCR: A Comprehensive Toolbox for Text Detection, Recognition and Understanding. (arXiv:2108.06543v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarna_A/0/1/0/all/0/1\">Aaron Sarna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1\">Dilip Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maschinot_A/0/1/0/all/0/1\">Aaron Maschinot</a>",
          "description": "Disentangled visual representations have largely been studied with generative\nmodels such as Variational AutoEncoders (VAEs). While prior work has focused on\ngenerative methods for disentangled representation learning, these approaches\ndo not scale to large datasets due to current limitations of generative models.\nInstead, we explore regularization methods with contrastive learning, which\ncould result in disentangled representations that are powerful enough for large\nscale datasets and downstream applications. However, we find that unsupervised\ndisentanglement is difficult to achieve due to optimization and initialization\nsensitivity, with trade-offs in task performance. We evaluate disentanglement\nwith downstream tasks, analyze the benefits and disadvantages of each\nregularization used, and discuss future directions.",
          "link": "http://arxiv.org/abs/2108.06613",
          "publishedOn": "2021-08-17T01:54:49.945Z",
          "wordCount": 559,
          "title": "Unsupervised Disentanglement without Autoencoding: Pitfalls and Future Directions. (arXiv:2108.06613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1\">Reza Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhier_L/0/1/0/all/0/1\">Lucas Rouhier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>",
          "description": "Labeling vertebral discs from MRI scans is important for the proper diagnosis\nof spinal related diseases, including multiple sclerosis, amyotrophic lateral\nsclerosis, degenerative cervical myelopathy and cancer. Automatic labeling of\nthe vertebral discs in MRI data is a difficult task because of the similarity\nbetween discs and bone area, the variability in the geometry of the spine and\nsurrounding tissues across individuals, and the variability across scans\n(manufacturers, pulse sequence, image contrast, resolution and artefacts). In\nprevious studies, vertebral disc labeling is often done after a disc detection\nstep and mostly fails when the localization algorithm misses discs or has false\npositive detection. In this work, we aim to mitigate this problem by\nreformulating the semantic vertebral disc labeling using the pose estimation\ntechnique. To do so, we propose a stacked hourglass network with multi-level\nattention mechanism to jointly learn intervertebral disc position and their\nskeleton structure. The proposed deep learning model takes into account the\nstrength of semantic segmentation and pose estimation technique to handle the\nmissing area and false positive detection. To further improve the performance\nof the proposed method, we propose a skeleton-based search space to reduce\nfalse positive detection. The proposed method evaluated on spine generic public\nmulti-center dataset and demonstrated better performance comparing to previous\nwork, on both T1w and T2w contrasts. The method is implemented in ivadomed\n(https://ivadomed.org).",
          "link": "http://arxiv.org/abs/2108.06554",
          "publishedOn": "2021-08-17T01:54:49.939Z",
          "wordCount": 685,
          "title": "Stacked Hourglass Network with a Multi-level Attention Mechanism: Where to Look for Intervertebral Disc Labeling. (arXiv:2108.06554v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaoshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>",
          "description": "In this paper, we present a self-training method, named ST3D++, with a\nholistic pseudo label denoising pipeline for unsupervised domain adaptation on\n3D object detection. ST3D++ aims at reducing noise in pseudo label generation\nas well as alleviating the negative impacts of noisy pseudo labels on model\ntraining. First, ST3D++ pre-trains the 3D object detector on the labeled source\ndomain with random object scaling (ROS) which is designed to reduce target\ndomain pseudo label noise arising from object scale bias of the source domain.\nThen, the detector is progressively improved through alternating between\ngenerating pseudo labels and training the object detector with pseudo-labeled\ntarget domain data. Here, we equip the pseudo label generation process with a\nhybrid quality-aware triplet memory to improve the quality and stability of\ngenerated pseudo labels. Meanwhile, in the model training stage, we propose a\nsource data assisted training strategy and a curriculum data augmentation\npolicy to effectively rectify noisy gradient directions and avoid model\nover-fitting to noisy pseudo labeled data. These specific designs enable the\ndetector to be trained on meticulously refined pseudo labeled target data with\ndenoised training signals, and thus effectively facilitate adapting an object\ndetector to a target domain without requiring annotations. Finally, our method\nis assessed on four 3D benchmark datasets (i.e., Waymo, KITTI, Lyft, and\nnuScenes) for three common categories (i.e., car, pedestrian and bicycle).\nST3D++ achieves state-of-the-art performance on all evaluated settings,\noutperforming the corresponding baseline by a large margin (e.g., 9.6% $\\sim$\n38.16% on Waymo $\\rightarrow$ KITTI in terms of AP$_{\\text{3D}}$), and even\nsurpasses the fully supervised oracle results on the KITTI 3D object detection\nbenchmark with target prior. Code will be available.",
          "link": "http://arxiv.org/abs/2108.06682",
          "publishedOn": "2021-08-17T01:54:49.927Z",
          "wordCount": 722,
          "title": "ST3D++: Denoised Self-training for Unsupervised Domain Adaptation on 3D Object Detection. (arXiv:2108.06682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chaoxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "Intelligent vehicles clearly benefit from the expanded Field of View (FoV) of\nthe 360-degree sensors, but the vast majority of available semantic\nsegmentation training images are captured with pinhole cameras. In this work,\nwe look at this problem through the lens of domain adaptation and bring\npanoramic semantic segmentation to a setting, where labelled training data\noriginates from a different distribution of conventional pinhole camera images.\nFirst, we formalize the task of unsupervised domain adaptation for panoramic\nsemantic segmentation, where a network trained on labelled examples from the\nsource domain of pinhole camera data is deployed in a different target domain\nof panoramic images, for which no labels are available. To validate this idea,\nwe collect and publicly release DensePASS - a novel densely annotated dataset\nfor panoramic segmentation under cross-domain conditions, specifically built to\nstudy the Pinhole-to-Panoramic transfer and accompanied with pinhole camera\ntraining examples obtained from Cityscapes. DensePASS covers both, labelled-\nand unlabelled 360-degree images, with the labelled data comprising 19 classes\nwhich explicitly fit the categories available in the source domain (i.e.\npinhole) data. To meet the challenge of domain shift, we leverage the current\nprogress of attention-based mechanisms and build a generic framework for\ncross-domain panoramic semantic segmentation based on different variants of\nattention-augmented domain adaptation modules. Our framework facilitates\ninformation exchange at local- and global levels when learning the domain\ncorrespondences and improves the domain adaptation performance of two standard\nsegmentation networks by 6.05% and 11.26% in Mean IoU.",
          "link": "http://arxiv.org/abs/2108.06383",
          "publishedOn": "2021-08-17T01:54:49.905Z",
          "wordCount": 720,
          "title": "DensePASS: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation with Attention-Augmented Context Exchange. (arXiv:2108.06383v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong Q. Nguyen</a>",
          "description": "The mainstream image captioning models rely on Convolutional Neural Network\n(CNN) image features to generate captions via recurrent models. Recently, image\nscene graphs have been used to augment captioning models so as to leverage\ntheir structural semantics, such as object entities, relationships and\nattributes. Several studies have noted that the naive use of scene graphs from\na black-box scene graph generator harms image captioning performance and that\nscene graph-based captioning models have to incur the overhead of explicit use\nof image features to generate decent captions. Addressing these challenges, we\npropose \\textbf{SG2Caps}, a framework that utilizes only the scene graph labels\nfor competitive image captioning performance. The basic idea is to close the\nsemantic gap between the two scene graphs - one derived from the input image\nand the other from its caption. In order to achieve this, we leverage the\nspatial location of objects and the Human-Object-Interaction (HOI) labels as an\nadditional HOI graph. SG2Caps outperforms existing scene graph-only captioning\nmodels by a large margin, indicating scene graphs as a promising representation\nfor image captioning. Direct utilization of scene graph labels avoids expensive\ngraph convolutions over high-dimensional CNN features resulting in 49% fewer\ntrainable parameters. Our code is available at:\nhttps://github.com/Kien085/SG2Caps",
          "link": "http://arxiv.org/abs/2102.04990",
          "publishedOn": "2021-08-17T01:54:49.382Z",
          "wordCount": 683,
          "title": "In Defense of Scene Graphs for Image Captioning. (arXiv:2102.04990v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1\">Jeff Mentch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_J/0/1/0/all/0/1\">Jerry Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Measuring the acoustic characteristics of a space is often done by capturing\nits impulse response (IR), a representation of how a full-range stimulus sound\nexcites it. This work generates an IR from a single image, which can then be\napplied to other signals using convolution, simulating the reverberant\ncharacteristics of the space shown in the image. Recording these IRs is both\ntime-intensive and expensive, and often infeasible for inaccessible locations.\nWe use an end-to-end neural network architecture to generate plausible audio\nimpulse responses from single images of acoustic environments. We evaluate our\nmethod both by comparisons to ground truth data and by human expert evaluation.\nWe demonstrate our approach by generating plausible impulse responses from\ndiverse settings and formats including well known places, musical halls, rooms\nin paintings, images from animations and computer games, synthetic environments\ngenerated from text, panoramic images, and video conference backgrounds.",
          "link": "http://arxiv.org/abs/2103.14201",
          "publishedOn": "2021-08-17T01:54:49.372Z",
          "wordCount": 623,
          "title": "Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis. (arXiv:2103.14201v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Ashraful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radke_R/0/1/0/all/0/1\">Richard Radke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>",
          "description": "Tremendous progress has been made in visual representation learning, notably\nwith the recent success of self-supervised contrastive learning methods.\nSupervised contrastive learning has also been shown to outperform its\ncross-entropy counterparts by leveraging labels for choosing where to contrast.\nHowever, there has been little work to explore the transfer capability of\ncontrastive learning to a different domain. In this paper, we conduct a\ncomprehensive study on the transferability of learned representations of\ndifferent contrastive approaches for linear evaluation, full-network transfer,\nand few-shot recognition on 12 downstream datasets from different domains, and\nobject detection tasks on MSCOCO and VOC0712. The results show that the\ncontrastive approaches learn representations that are easily transferable to a\ndifferent downstream task. We further observe that the joint objective of\nself-supervised contrastive loss with cross-entropy/supervised-contrastive loss\nleads to better transferability of these models over their supervised\ncounterparts. Our analysis reveals that the representations learned from the\ncontrastive approaches contain more low/mid-level semantics than cross-entropy\nmodels, which enables them to quickly adapt to a new task. Our codes and models\nwill be publicly available to facilitate future research on transferability of\nvisual representations.",
          "link": "http://arxiv.org/abs/2103.13517",
          "publishedOn": "2021-08-17T01:54:49.353Z",
          "wordCount": 677,
          "title": "A Broad Study on the Transferability of Visual Representations with Contrastive Learning. (arXiv:2103.13517v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "Data quantity and quality are crucial factors for data-driven learning\nmethods. In some target problem domains, there are not many data samples\navailable, which could significantly hinder the learning process. While data\nfrom similar domains may be leveraged to help through domain adaptation,\nobtaining high-quality labeled data for those source domains themselves could\nbe difficult or costly. To address such challenges on data insufficiency for\nclassification problem in a target domain, we propose a weak adaptation\nlearning (WAL) approach that leverages unlabeled data from a similar source\ndomain, a low-cost weak annotator that produces labels based on task-specific\nheuristics, labeling rules, or other methods (albeit with inaccuracy), and a\nsmall amount of labeled data in the target domain. Our approach first conducts\na theoretical analysis on the error bound of the trained classifier with\nrespect to the data quantity and the performance of the weak annotator, and\nthen introduces a multi-stage weak adaptation learning method to learn an\naccurate classifier by lowering the error bound. Our experiments demonstrate\nthe effectiveness of our approach in learning an accurate classifier with\nlimited labeled data in the target domain and unlabeled data in the source\ndomain.",
          "link": "http://arxiv.org/abs/2102.07358",
          "publishedOn": "2021-08-17T01:54:49.337Z",
          "wordCount": 668,
          "title": "Weak Adaptation Learning -- Addressing Cross-domain Data Insufficiency with Weak Annotator. (arXiv:2102.07358v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKay_B/0/1/0/all/0/1\">Bob McKay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Skeleton sequences are lightweight and compact, thus are ideal candidates for\naction recognition on edge devices. Recent skeleton-based action recognition\nmethods extract features from 3D joint coordinates as spatial-temporal cues,\nusing these representations in a graph neural network for feature fusion to\nboost recognition performance. The use of first- and second-order features,\n\\ie{} joint and bone representations, has led to high accuracy. Nonetheless,\nmany models are still confused by actions that have similar motion\ntrajectories. To address these issues, we propose fusing third-order features\nin the form of angular encoding into modern architectures to robustly capture\nthe relationships between joints and body parts. This simple fusion with\npopular spatial-temporal graph neural networks achieves new state-of-the-art\naccuracy in two large benchmarks, including NTU60 and NTU120, while employing\nfewer parameters and reduced run time. Our source code is publicly available\nat: https://github.com/ZhenyueQin/Angular-Skeleton-Encoding.",
          "link": "http://arxiv.org/abs/2105.01563",
          "publishedOn": "2021-08-17T01:54:49.319Z",
          "wordCount": 641,
          "title": "Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition. (arXiv:2105.01563v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qidong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Recent research shows deep neural networks are vulnerable to different types\nof attacks, such as adversarial attack, data poisoning attack and backdoor\nattack. Among them, backdoor attack is the most cunning one and can occur in\nalmost every stage of deep learning pipeline. Therefore, backdoor attack has\nattracted lots of interests from both academia and industry. However, most\nexisting backdoor attack methods are either visible or fragile to some\neffortless pre-processing such as common data transformations. To address these\nlimitations, we propose a robust and invisible backdoor attack called \"Poison\nInk\". Concretely, we first leverage the image structures as target poisoning\nareas, and fill them with poison ink (information) to generate the trigger\npattern. As the image structure can keep its semantic meaning during the data\ntransformation, such trigger pattern is inherently robust to data\ntransformations. Then we leverage a deep injection network to embed such\ntrigger pattern into the cover image to achieve stealthiness. Compared to\nexisting popular backdoor attack methods, Poison Ink outperforms both in\nstealthiness and robustness. Through extensive experiments, we demonstrate\nPoison Ink is not only general to different datasets and network architectures,\nbut also flexible for different attack scenarios. Besides, it also has very\nstrong resistance against many state-of-the-art defense techniques.",
          "link": "http://arxiv.org/abs/2108.02488",
          "publishedOn": "2021-08-17T01:54:49.301Z",
          "wordCount": 666,
          "title": "Poison Ink: Robust and Invisible Backdoor Attack. (arXiv:2108.02488v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dae-Hyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dong-Kyun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sung-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Ji-Hoon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Brain-computer interface (BCI) is used for communication between humans and\ndevices by recognizing status and intention of humans. Communication between\nhumans and a drone using electroencephalogram (EEG) signals is one of the most\nchallenging issues in the BCI domain. In particular, the control of drone\nswarms (the direction and formation) has more advantages compared to the\ncontrol of a drone. The visual imagery (VI) paradigm is that subjects visually\nimagine specific objects or scenes. Reduction of the variability among EEG\nsignals of subjects is essential for practical BCI-based systems. In this\nstudy, we proposed the subepoch-wise feature encoder (SEFE) to improve the\nperformances in the subject-independent tasks by using the VI dataset. This\nstudy is the first attempt to demonstrate the possibility of generalization\namong subjects in the VI-based BCI. We used the leave-one-subject-out\ncross-validation for evaluating the performances. We obtained higher\nperformances when including our proposed module than excluding our proposed\nmodule. The DeepConvNet with SEFE showed the highest performance of 0.72 among\nsix different decoding models. Hence, we demonstrated the feasibility of\ndecoding the VI dataset in the subject-independent task with robust\nperformances by using our proposed module.",
          "link": "http://arxiv.org/abs/2106.04026",
          "publishedOn": "2021-08-17T01:54:49.279Z",
          "wordCount": 679,
          "title": "Subject-Independent Brain-Computer Interface for Decoding High-Level Visual Imagery Tasks. (arXiv:2106.04026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "We present a graph-convolution-reinforced transformer, named Mesh Graphormer,\nfor 3D human pose and mesh reconstruction from a single image. Recently both\ntransformers and graph convolutional neural networks (GCNNs) have shown\npromising progress in human mesh reconstruction. Transformer-based approaches\nare effective in modeling non-local interactions among 3D mesh vertices and\nbody joints, whereas GCNNs are good at exploiting neighborhood vertex\ninteractions based on a pre-specified mesh topology. In this paper, we study\nhow to combine graph convolutions and self-attentions in a transformer to model\nboth local and global interactions. Experimental results show that our proposed\nmethod, Mesh Graphormer, significantly outperforms the previous\nstate-of-the-art methods on multiple benchmarks, including Human3.6M, 3DPW, and\nFreiHAND datasets. Code and pre-trained models are available at\nhttps://github.com/microsoft/MeshGraphormer",
          "link": "http://arxiv.org/abs/2104.00272",
          "publishedOn": "2021-08-17T01:54:49.262Z",
          "wordCount": 575,
          "title": "Mesh Graphormer. (arXiv:2104.00272v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14403",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>",
          "description": "We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages. In this framework, the slice images of a CT-scan volume are first\nproprocessed using segmentation techniques to filter out images of closed lung,\nand to remove the useless background. Then a resampling method is used to\nselect one or multiple sets of a fixed number of slice images for training and\nvalidation. A 3D CNN network with BERT is used to classify this set of selected\nslice images. In this network, an embedding feature is also extracted. In cases\nwhere there are more than one set of slice images in a volume, the features of\nall sets are extracted and pooled into a global feature vector for the whole\nCT-scan volume. A simple multiple-layer perceptron (MLP) network is used to\nfurther classify the aggregated feature vector. The models are trained and\nevaluated on the provided training and validation datasets. On the validation\ndataset, the accuracy is 0.9278 and the F1 score is 0.9261.",
          "link": "http://arxiv.org/abs/2106.14403",
          "publishedOn": "2021-08-17T01:54:49.257Z",
          "wordCount": 677,
          "title": "A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images. (arXiv:2106.14403v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "Despite much recent progress in video-based person re-identification (re-ID),\nthe current state-of-the-art still suffers from common real-world challenges\nsuch as appearance similarity among various people, occlusions, and frame\nmisalignment. To alleviate these problems, we propose Spatio-Temporal\nRepresentation Factorization (STRF), a flexible new computational unit that can\nbe used in conjunction with most existing 3D convolutional neural network\narchitectures for re-ID. The key innovations of STRF over prior work include\nexplicit pathways for learning discriminative temporal and spatial features,\nwith each component further factorized to capture complementary person-specific\nappearance and motion information. Specifically, temporal factorization\ncomprises two branches, one each for static features (e.g., the color of\nclothes) that do not change much over time, and dynamic features (e.g., walking\npatterns) that change over time. Further, spatial factorization also comprises\ntwo branches to learn both global (coarse segments) as well as local (finer\nsegments) appearance features, with the local features particularly useful in\ncases of occlusion or spatial misalignment. These two factorization operations\ntaken together result in a modular architecture for our parameter-wise light\nSTRF unit that can be plugged in between any two 3D convolutional layers,\nresulting in an end-to-end learning framework. We empirically show that STRF\nimproves performance of various existing baseline architectures while\ndemonstrating new state-of-the-art results using standard person re-ID\nevaluation protocols on three benchmarks.",
          "link": "http://arxiv.org/abs/2107.11878",
          "publishedOn": "2021-08-17T01:54:49.245Z",
          "wordCount": 697,
          "title": "Spatio-Temporal Representation Factorization for Video-based Person Re-Identification. (arXiv:2107.11878v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Delong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhifu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>",
          "description": "Gun violence is a severe problem in the world, particularly in the United\nStates. Deep learning methods have been studied to detect guns in surveillance\nvideo cameras or smart IP cameras and to send a real-time alert to security\npersonals. One problem for the development of gun detection algorithms is the\nlack of large public datasets. In this work, we first publish a dataset with\n51K annotated gun images for gun detection and other 51K cropped gun chip\nimages for gun classification we collect from a few different sources. To our\nknowledge, this is the largest dataset for the study of gun detection. This\ndataset can be downloaded at www.linksprite.com/gun-detection-datasets. We\npresent a gun detection system using a smart IP camera as an embedded edge\ndevice, and a cloud server as a manager for device, data, alert, and to further\nreduce the false positive rate. We study to find solutions for gun detection in\nan embedded device, and for gun classification on the edge device and the cloud\nserver. This edge/cloud framework makes the deployment of gun detection in the\nreal world possible.",
          "link": "http://arxiv.org/abs/2105.01058",
          "publishedOn": "2021-08-17T01:54:49.216Z",
          "wordCount": 665,
          "title": "A Dataset and System for Real-Time Gun Detection in Surveillance Video Using Deep Learning. (arXiv:2105.01058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-08-17T01:54:49.209Z",
          "wordCount": 653,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Learning from a limited number of samples is challenging since the learned\nmodel can easily become overfitted based on the biased distribution formed by\nonly a few training examples. In this paper, we calibrate the distribution of\nthese few-sample classes by transferring statistics from the classes with\nsufficient examples, then an adequate number of examples can be sampled from\nthe calibrated distribution to expand the inputs to the classifier. We assume\nevery dimension in the feature representation follows a Gaussian distribution\nso that the mean and the variance of the distribution can borrow from that of\nsimilar classes whose statistics are better estimated with an adequate number\nof samples. Our method can be built on top of off-the-shelf pretrained feature\nextractors and classification models without extra parameters. We show that a\nsimple logistic regression classifier trained using the features sampled from\nour calibrated distribution can outperform the state-of-the-art accuracy on two\ndatasets (~5% improvement on miniImageNet compared to the next best). The\nvisualization of these generated features demonstrates that our calibrated\ndistribution is an accurate estimation.",
          "link": "http://arxiv.org/abs/2101.06395",
          "publishedOn": "2021-08-17T01:54:49.201Z",
          "wordCount": 659,
          "title": "Free Lunch for Few-shot Learning: Distribution Calibration. (arXiv:2101.06395v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+lin_G/0/1/0/all/0/1\">Guosheng lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "Currently, the state-of-the-art methods treat few-shot semantic segmentation\ntask as a conditional foreground-background segmentation problem, assuming each\nclass is independent. In this paper, we introduce the concept of meta-class,\nwhich is the meta information (e.g. certain middle-level features) shareable\namong all classes. To explicitly learn meta-class representations in few-shot\nsegmentation task, we propose a novel Meta-class Memory based few-shot\nsegmentation method (MM-Net), where we introduce a set of learnable memory\nembeddings to memorize the meta-class information during the base class\ntraining and transfer to novel classes during the inference stage. Moreover,\nfor the $k$-shot scenario, we propose a novel image quality measurement module\nto select images from the set of support images. A high-quality class prototype\ncould be obtained with the weighted sum of support image features based on the\nquality measure. Experiments on both PASCAL-$5^i$ and COCO dataset shows that\nour proposed method is able to achieve state-of-the-art results in both 1-shot\nand 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\\% mIoU on\nthe COCO dataset in 1-shot setting, which is 5.1\\% higher than the previous\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02958",
          "publishedOn": "2021-08-17T01:54:49.194Z",
          "wordCount": 641,
          "title": "Learning Meta-class Memory for Few-Shot Semantic Segmentation. (arXiv:2108.02958v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Beibei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>",
          "description": "Gait recognition is one of the most important biometric technologies and has\nbeen applied in many fields. Recent gait recognition frameworks represent each\ngait frame by descriptors extracted from either global appearances or local\nregions of humans. However, the representations based on global information\noften neglect the details of the gait frame, while local region based\ndescriptors cannot capture the relations among neighboring regions, thus\nreducing their discriminativeness. In this paper, we propose a novel feature\nextraction and fusion framework to achieve discriminative feature\nrepresentations for gait recognition. Towards this goal, we take advantage of\nboth global visual information and local region details and develop a Global\nand Local Feature Extractor (GLFE). Specifically, our GLFE module is composed\nof our newly designed multiple global and local convolutional layers (GLConv)\nto ensemble global and local features in a principle manner. Furthermore, we\npresent a novel operation, namely Local Temporal Aggregation (LTA), to further\npreserve the spatial information by reducing the temporal resolution to obtain\nhigher spatial resolution. With the help of our GLFE and LTA, our method\nsignificantly improves the discriminativeness of our visual features, thus\nimproving the gait recognition performance. Extensive experiments demonstrate\nthat our proposed method outperforms state-of-the-art gait recognition methods\non two popular datasets.",
          "link": "http://arxiv.org/abs/2011.01461",
          "publishedOn": "2021-08-17T01:54:49.178Z",
          "wordCount": 677,
          "title": "Gait Recognition via Effective Global-Local Feature Representation and Local Temporal Aggregation. (arXiv:2011.01461v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guo-Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yifan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Knowledge distillation (KD) is a popular method to train efficient networks\n(\"student\") with the help of high-capacity networks (\"teacher\"). Traditional\nmethods use the teacher's soft logits as extra supervision to train the student\nnetwork. In this paper, we argue that it is more advantageous to make the\nstudent mimic the teacher's features in the penultimate layer. Not only the\nstudent can directly learn more effective information from the teacher feature,\nfeature mimicking can also be applied for teachers trained without a softmax\nlayer. Experiments show that it can achieve higher accuracy than traditional\nKD. To further facilitate feature mimicking, we decompose a feature vector into\nthe magnitude and the direction. We argue that the teacher should give more\nfreedom to the student feature's magnitude, and let the student pay more\nattention on mimicking the feature direction. To meet this requirement, we\npropose a loss term based on locality-sensitive hashing (LSH). With the help of\nthis new loss, our method indeed mimics feature directions more accurately,\nrelaxes constraints on feature magnitudes, and achieves state-of-the-art\ndistillation accuracy. We provide theoretical analyses of how LSH facilitates\nfeature direction mimicking, and further extend feature mimicking to\nmulti-label recognition and object detection.",
          "link": "http://arxiv.org/abs/2011.01424",
          "publishedOn": "2021-08-17T01:54:49.173Z",
          "wordCount": 664,
          "title": "Distilling Knowledge by Mimicking Features. (arXiv:2011.01424v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1\">Cheng Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>",
          "description": "This paper tackles the task of category-level pose estimation for garments.\nWith a near infinite degree of freedom, a garment's full configuration (i.e.,\nposes) is often described by the per-vertex 3D locations of its entire 3D\nsurface. However, garments are also commonly subject to extreme cases of\nself-occlusion, especially when folded or crumpled, making it challenging to\nperceive their full 3D surface. To address these challenges, we propose\nGarmentNets, where the key idea is to formulate the deformable object pose\nestimation problem as a shape completion task in the canonical space. This\ncanonical space is defined across garments instances within a category,\ntherefore, specifies the shared category-level pose. By mapping the observed\npartial surface to the canonical space and completing it in this space, the\noutput representation describes the garment's full configuration using a\ncomplete 3D mesh with the per-vertex canonical coordinate label. To properly\nhandle the thin 3D structure presented on garments, we proposed a novel 3D\nshape representation using the generalized winding number field. Experiments\ndemonstrate that GarmentNets is able to generalize to unseen garment instances\nand achieve significantly better performance compared to alternative\napproaches.",
          "link": "http://arxiv.org/abs/2104.05177",
          "publishedOn": "2021-08-17T01:54:49.167Z",
          "wordCount": 663,
          "title": "GarmentNets: Category-Level Pose Estimation for Garments via Canonical Space Shape Completion. (arXiv:2104.05177v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1\">Maksim Siniukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1\">Dmitriy Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>",
          "description": "Video-quality measurement plays a critical role in the development of\nvideo-processing applications. In this paper, we show how video preprocessing\ncan artificially increase the popular quality metric VMAF and its\ntuning-resistant version, VMAF NEG. We propose a pipeline that tunes\nprocessing-algorithm parameters to increase VMAF by up to 218.8%. A subjective\ncomparison revealed that for most preprocessing methods, a video's visual\nquality drops or stays unchanged. We also show that some preprocessing methods\ncan increase VMAF NEG scores by up to 23.6%.",
          "link": "http://arxiv.org/abs/2107.04510",
          "publishedOn": "2021-08-17T01:54:49.136Z",
          "wordCount": 550,
          "title": "Hacking VMAF and VMAF NEG: vulnerability to different preprocessing methods. (arXiv:2107.04510v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Prasen Kumar Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bisht_I/0/1/0/all/0/1\">Ira Bisht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>",
          "description": "Background: Underwater images, in general, suffer from low contrast and high\ncolor distortions due to the non-uniform attenuation of the light as it\npropagates through the water. In addition, the degree of attenuation varies\nwith the wavelength resulting in the asymmetric traversing of colors. Despite\nthe prolific works for underwater image restoration (UIR) using deep learning,\nthe above asymmetricity has not been addressed in the respective network\nengineering.\n\nContributions: As the first novelty, this paper shows that attributing the\nright receptive field size (context) based on the traversing range of the color\nchannel may lead to a substantial performance gain for the task of UIR.\nFurther, it is important to suppress the irrelevant multi-contextual features\nand increase the representational power of the model. Therefore, as a second\nnovelty, we have incorporated an attentive skip mechanism to adaptively refine\nthe learned multi-contextual features. The proposed framework, called Deep\nWaveNet, is optimized using the traditional pixel-wise and feature-based cost\nfunctions. An extensive set of experiments have been carried out to show the\nefficacy of the proposed scheme over existing best-published literature on\nbenchmark datasets. More importantly, we have demonstrated a comprehensive\nvalidation of enhanced images across various high-level vision tasks, e.g.,\nunderwater image semantic segmentation, and diver's 2D pose estimation. A\nsample video to exhibit our real-world performance is available at\n\\url{https://tinyurl.com/yzcrup9n}. Also, we have open-sourced our framework at\n\\url{https://github.com/pksvision/Deep-WaveNet-UnderwaterImage-Restoration}.",
          "link": "http://arxiv.org/abs/2106.07910",
          "publishedOn": "2021-08-17T01:54:49.130Z",
          "wordCount": 698,
          "title": "Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration. (arXiv:2106.07910v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shumeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaixin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "Deep learning has achieved promising segmentation performance on 3D left\natrium MR images. However, annotations for segmentation tasks are expensive,\ncostly and difficult to obtain. In this paper, we introduce a novel\nhierarchical consistency regularized mean teacher framework for 3D left atrium\nsegmentation. In each iteration, the student model is optimized by multi-scale\ndeep supervision and hierarchical consistency regularization, concurrently.\nExtensive experiments have shown that our method achieves competitive\nperformance as compared with full annotation, outperforming other\nstate-of-the-art semi-supervised segmentation methods.",
          "link": "http://arxiv.org/abs/2105.10369",
          "publishedOn": "2021-08-17T01:54:49.123Z",
          "wordCount": 583,
          "title": "Hierarchical Consistency Regularized Mean Teacher for Semi-supervised 3D Left Atrium Segmentation. (arXiv:2105.10369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Golestaneh_S/0/1/0/all/0/1\">S. Alireza Golestaneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadsetan_S/0/1/0/all/0/1\">Saba Dadsetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>",
          "description": "The goal of No-Reference Image Quality Assessment (NR-IQA) is to estimate the\nperceptual image quality in accordance with subjective evaluations, it is a\ncomplex and unsolved problem due to the absence of the pristine reference\nimage. In this paper, we propose a novel model to address the NR-IQA task by\nleveraging a hybrid approach that benefits from Convolutional Neural Networks\n(CNNs) and self-attention mechanism in Transformers to extract both local and\nnon-local features from the input image. We capture local structure information\nof the image via CNNs, then to circumvent the locality bias among the extracted\nCNNs features and obtain a non-local representation of the image, we utilize\nTransformers on the extracted features where we model them as a sequential\ninput to the Transformer model. Furthermore, to improve the monotonicity\ncorrelation between the subjective and objective scores, we utilize the\nrelative distance information among the images within each batch and enforce\nthe relative ranking among them. Last but not least, we observe that the\nperformance of NR-IQA models degrades when we apply equivariant transformations\n(e.g. horizontal flipping) to the inputs. Therefore, we propose a method that\nleverages self-consistency as a source of self-supervision to improve the\nrobustness of NRIQA models. Specifically, we enforce self-consistency between\nthe outputs of our quality assessment model for each image and its\ntransformation (horizontally flipped) to utilize the rich self-supervisory\ninformation and reduce the uncertainty of the model. To demonstrate the\neffectiveness of our work, we evaluate it on seven standard IQA datasets (both\nsynthetic and authentic) and show that our model achieves state-of-the-art\nresults on various datasets.",
          "link": "http://arxiv.org/abs/2108.06858",
          "publishedOn": "2021-08-17T01:54:49.117Z",
          "wordCount": 704,
          "title": "No-Reference Image Quality Assessment via Transformers, Relative Ranking, and Self-Consistency. (arXiv:2108.06858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhuo Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenzhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dewen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietikainen_M/0/1/0/all/0/1\">Matti Pietik&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>",
          "description": "Recently, deep Convolutional Neural Networks (CNNs) can achieve human-level\nperformance in edge detection with the rich and abstract edge representation\ncapacities. However, the high performance of CNN based edge detection is\nachieved with a large pretrained CNN backbone, which is memory and energy\nconsuming. In addition, it is surprising that the previous wisdom from the\ntraditional edge detectors, such as Canny, Sobel, and LBP are rarely\ninvestigated in the rapid-developing deep learning era. To address these\nissues, we propose a simple, lightweight yet effective architecture named Pixel\nDifference Network (PiDiNet) for efficient edge detection. Extensive\nexperiments on BSDS500, NYUD, and Multicue are provided to demonstrate its\neffectiveness, and its high training and inference efficiency. Surprisingly,\nwhen training from scratch with only the BSDS500 and VOC datasets, PiDiNet can\nsurpass the recorded result of human perception (0.807 vs. 0.803 in ODS\nF-measure) on the BSDS500 dataset with 100 FPS and less than 1M parameters. A\nfaster version of PiDiNet with less than 0.1M parameters can still achieve\ncomparable performance among state of the arts with 200 FPS. Results on the\nNYUD and Multicue datasets show similar observations. The codes are available\nat https://github.com/zhuoinoulu/pidinet.",
          "link": "http://arxiv.org/abs/2108.07009",
          "publishedOn": "2021-08-17T01:54:49.111Z",
          "wordCount": 639,
          "title": "Pixel Difference Networks for Efficient Edge Detection. (arXiv:2108.07009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_H/0/1/0/all/0/1\">Hemanth Sai Ram Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vani_M/0/1/0/all/0/1\">M Vani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "The capsule network is a distinct and promising segment of the neural network\nfamily that drew attention due to its unique ability to maintain the\nequivariance property by preserving the spatial relationship amongst the\nfeatures. The capsule network has attained unprecedented success over image\nclassification tasks with datasets such as MNIST and affNIST by encoding the\ncharacteristic features into the capsules and building the parse-tree\nstructure. However, on the datasets involving complex foreground and background\nregions such as CIFAR-10, the performance of the capsule network is sub-optimal\ndue to its naive data routing policy and incompetence towards extracting\ncomplex features. This paper proposes a new design strategy for capsule network\narchitecture for efficiently dealing with complex images. The proposed method\nincorporates wide bottleneck residual modules and the Squeeze and Excitation\nattention blocks upheld by the modified FM routing algorithm to address the\ndefined problem. A wide bottleneck residual module facilitates extracting\ncomplex features followed by the squeeze and excitation attention block to\nenable channel-wise attention by suppressing the trivial features. This setup\nallows channel inter-dependencies at almost no computational cost, thereby\nenhancing the representation ability of capsules on complex images. We\nextensively evaluate the performance of the proposed model on three publicly\navailable datasets, namely CIFAR-10, Fashion MNIST, and SVHN, to outperform the\ntop-5 performance on CIFAR-10 and Fashion MNIST with highly competitive\nperformance on the SVHN dataset.",
          "link": "http://arxiv.org/abs/2108.03627",
          "publishedOn": "2021-08-17T01:54:49.093Z",
          "wordCount": 702,
          "title": "WideCaps: A Wide Attention based Capsule Network for Image Classification. (arXiv:2108.03627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yazhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingkun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Self-supervised learning for depth estimation possesses several advantages\nover supervised learning. The benefits of no need for ground-truth depth,\nonline fine-tuning, and better generalization with unlimited data attract\nresearchers to seek self-supervised solutions. In this work, we propose a new\nself-supervised framework for stereo matching utilizing multiple images\ncaptured at aligned camera positions. A cross photometric loss, an\nuncertainty-aware mutual-supervision loss, and a new smoothness loss are\nintroduced to optimize the network in learning disparity maps end-to-end\nwithout ground-truth depth information. To train this framework, we build a new\nmultiscopic dataset consisting of synthetic images rendered by 3D engines and\nreal images captured by real cameras. After being trained with only the\nsynthetic images, our network can perform well in unseen outdoor scenes. Our\nexperiment shows that our model obtains better disparity maps than previous\nunsupervised methods on the KITTI dataset and is comparable to supervised\nmethods when generalized to unseen data. Our source code and dataset are\navailable at https://sites.google.com/view/multiscopic.",
          "link": "http://arxiv.org/abs/2104.04170",
          "publishedOn": "2021-08-17T01:54:49.087Z",
          "wordCount": 646,
          "title": "Stereo Matching by Self-supervision of Multiscopic Vision. (arXiv:2104.04170v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>",
          "description": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.",
          "link": "http://arxiv.org/abs/2108.03857",
          "publishedOn": "2021-08-17T01:54:49.080Z",
          "wordCount": 613,
          "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>",
          "description": "Vision transformer has achieved competitive performance on a variety of\ncomputer vision applications. However, their storage, run-time memory, and\ncomputational demands are hindering the deployment to mobile devices. Here we\npresent a vision transformer pruning approach, which identifies the impacts of\ndimensions in each layer of transformer and then executes pruning accordingly.\nBy encouraging dimension-wise sparsity in the transformer, important dimensions\nautomatically emerge. A great number of dimensions with small importance scores\ncan be discarded to achieve a high pruning ratio without significantly\ncompromising accuracy. The pipeline for vision transformer pruning is as\nfollows: 1) training with sparsity regularization; 2) pruning dimensions of\nlinear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of\nthe proposed algorithm are well evaluated and analyzed on ImageNet dataset to\ndemonstrate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2104.08500",
          "publishedOn": "2021-08-17T01:54:49.074Z",
          "wordCount": 614,
          "title": "Vision Transformer Pruning. (arXiv:2104.08500v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>",
          "description": "Most existing Siamese-based tracking methods execute the classification and\nregression of the target object based on the similarity maps. However, they\neither employ a single map from the last convolutional layer which degrades the\nlocalization accuracy in complex scenarios or separately use multiple maps for\ndecision making, introducing intractable computations for aerial mobile\nplatforms. Thus, in this work, we propose an efficient and effective\nhierarchical feature transformer (HiFT) for aerial tracking. Hierarchical\nsimilarity maps generated by multi-level convolutional layers are fed into the\nfeature transformer to achieve the interactive fusion of spatial (shallow\nlayers) and semantics cues (deep layers). Consequently, not only the global\ncontextual information can be raised, facilitating the target search, but also\nour end-to-end architecture with the transformer can efficiently learn the\ninterdependencies among multi-level features, thereby discovering a\ntracking-tailored feature space with strong discriminability. Comprehensive\nevaluations on four aerial benchmarks have proven the effectiveness of HiFT.\nReal-world tests on the aerial platform have strongly validated its\npracticability with a real-time speed. Our code is available at\nhttps://github.com/vision4robotics/HiFT.",
          "link": "http://arxiv.org/abs/2108.00202",
          "publishedOn": "2021-08-17T01:54:49.067Z",
          "wordCount": 638,
          "title": "HiFT: Hierarchical Feature Transformer for Aerial Tracking. (arXiv:2108.00202v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>",
          "description": "Graph-based multi-view clustering has become an active topic due to the\nefficiency in characterizing both the complex structure and relationship\nbetween multimedia data. However, existing methods have the following\nshortcomings: (1) They are inefficient or even fail for graph learning in large\nscale due to the graph construction and eigen-decomposition. (2) They cannot\nwell exploit both the complementary information and spatial structure embedded\nin graphs of different views. To well exploit complementary information and\ntackle the scalability issue plaguing graph-based multi-view clustering, we\npropose an efficient multiple graph learning model via a small number of anchor\npoints and tensor Schatten p-norm minimization. Specifically, we construct a\nhidden and tractable large graph by anchor graph for each view and well exploit\ncomplementary information embedded in anchor graphs of different views by\ntensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,\nwhich scales linearly with the data size, to solve our proposed model.\nExtensive experimental results on several datasets indicate that our proposed\nmethod outperforms some state-of-the-art multi-view clustering algorithms.",
          "link": "http://arxiv.org/abs/2106.15382",
          "publishedOn": "2021-08-17T01:54:49.049Z",
          "wordCount": 629,
          "title": "Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1\">Victor G. Turrisi da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "This paper presents solo-learn, a library of self-supervised methods for\nvisual representation learning. Implemented in Python, using Pytorch and\nPytorch lightning, the library fits both research and industry needs by\nfeaturing distributed training pipelines with mixed-precision, faster data\nloading via Nvidia DALI, online linear evaluation for better prototyping, and\nmany additional training tricks. Our goal is to provide an easy-to-use library\ncomprising a large amount of Self-supervised Learning (SSL) methods, that can\nbe easily extended and fine-tuned by the community. solo-learn opens up avenues\nfor exploiting large-budget SSL solutions on inexpensive smaller\ninfrastructures and seeks to democratize SSL by making it accessible to all.\nThe source code is available at https://github.com/vturrisi/solo-learn.",
          "link": "http://arxiv.org/abs/2108.01775",
          "publishedOn": "2021-08-17T01:54:49.043Z",
          "wordCount": 583,
          "title": "Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. (arXiv:2108.01775v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khawaled_S/0/1/0/all/0/1\">Samah Khawaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>",
          "description": "Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays an important role in the safe deployment of\nreal-world medical applications and research-oriented processing pipelines, and\nin improving generalization capabilities. Currently available approaches for\nuncertainty estimation, including the variational encoder-decoder architecture\nand the inference-time dropout approach, require specific network architectures\nand assume parametric distribution of the latent space which may result in\nsub-optimal characterization of the posterior distribution for the predicted\ndeformation-fields. We introduce the NPBDREG, a fully non-parametric Bayesian\nframework for unsupervised DNN-based deformable image registration by combining\nan \\texttt{Adam} optimizer with stochastic gradient Langevin dynamics (SGLD) to\ncharacterize the true posterior distribution through posterior sampling. The\nNPBDREG provides a principled non-parametric way to characterize the true\nposterior distribution, thus providing improved uncertainty estimates and\nconfidence measures in a theoretically well-founded and computationally\nefficient way. We demonstrated the added-value of NPBDREG, compared to the\nbaseline probabilistic \\texttt{VoxelMorph} unsupervised model (PrVXM), on brain\nMRI images registration using $390$ image pairs from four publicly available\ndatabases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a slight\nimprovement in the registration accuracy compared to PrVXM (Dice score of\n$0.73$ vs. $0.68$, $p \\ll 0.01$), a better generalization capability for data\ncorrupted by a mixed structure noise (e.g Dice score of $0.729$ vs. $0.686$ for\n$\\alpha=0.2$) and last but foremost, a significantly better correlation of the\npredicted uncertainty with out-of-distribution data ($r>0.95$ vs. $r<0.5$).",
          "link": "http://arxiv.org/abs/2108.06771",
          "publishedOn": "2021-08-17T01:54:49.019Z",
          "wordCount": 676,
          "title": "NPBDREG: A Non-parametric Bayesian Deep-Learning Based Approach for Diffeomorphic Brain MRI Registration. (arXiv:2108.06771v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1\">Rowel Atienza</a>",
          "description": "Scene text recognition (STR) is a challenging task in computer vision due to\nthe large number of possible text appearances in natural scenes. Most STR\nmodels rely on synthetic datasets for training since there are no sufficiently\nbig and publicly available labelled real datasets. Since STR models are\nevaluated using real data, the mismatch between training and testing data\ndistributions results into poor performance of models especially on challenging\ntext that are affected by noise, artifacts, geometry, structure, etc. In this\npaper, we introduce STRAug which is made of 36 image augmentation functions\ndesigned for STR. Each function mimics certain text image properties that can\nbe found in natural scenes, caused by camera sensors, or induced by signal\nprocessing operations but poorly represented in the training dataset. When\napplied to strong baseline models using RandAugment, STRAug significantly\nincreases the overall absolute accuracy of STR models across regular and\nirregular test datasets by as much as 2.10% on Rosetta, 1.48% on R2AM, 1.30% on\nCRNN, 1.35% on RARE, 1.06% on TRBA and 0.89% on GCRNN. The diversity and\nsimplicity of API provided by STRAug functions enable easy replication and\nvalidation of existing data augmentation methods for STR. STRAug is available\nat https://github.com/roatienza/straug.",
          "link": "http://arxiv.org/abs/2108.06949",
          "publishedOn": "2021-08-17T01:54:49.012Z",
          "wordCount": 640,
          "title": "Data Augmentation for Scene Text Recognition. (arXiv:2108.06949v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinyue Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yujin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongliang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>",
          "description": "The classic Monte Carlo path tracing can achieve high quality rendering at\nthe cost of heavy computation. Recent works make use of deep neural networks to\naccelerate this process, by improving either low-resolution or fewer-sample\nrendering with super-resolution or denoising neural networks in\npost-processing. However, denoising and super-resolution have only been\nconsidered separately in previous work. We show in this work that Monte Carlo\npath tracing can be further accelerated by joint super-resolution and denoising\n(SRD) in post-processing. This new type of joint filtering allows only a\nlow-resolution and fewer-sample (thus noisy) image to be rendered by path\ntracing, which is then fed into a deep neural network to produce a\nhigh-resolution and clean image. The main contribution of this work is a new\nend-to-end network architecture, specifically designed for the SRD task. It\ncontains two cascaded stages with shared components. We discover that denoising\nand super-resolution require very different receptive fields, a key insight\nthat leads to the introduction of deformable convolution into the network\ndesign. Extensive experiments show that the proposed method outperforms\nprevious methods and their variants adopted for the SRD task.",
          "link": "http://arxiv.org/abs/2108.06915",
          "publishedOn": "2021-08-17T01:54:48.989Z",
          "wordCount": 626,
          "title": "End-to-End Adaptive Monte Carlo Denoising and Super-Resolution. (arXiv:2108.06915v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>",
          "description": "Nowadays, Deep Convolutional Neural Networks (DCNNs) are widely used in\nfabric defect detection, which come with the cost of expensive training and\ncomplex model parameters. With the observation that most fabrics are defect\nfree in practice, a two-step Cascaded Zoom-In Network (CZI-Net) is proposed for\npatterned fabric defect detection. In the CZI-Net, the Aggregated HOG (A-HOG)\nand SIFT features are used to instead of simple convolution filters for feature\nextraction. Moreover, in order to extract more distinctive features, the\nfeature representation layer and full connection layer are included in the\nCZI-Net. In practice, Most defect-free fabrics only involve in the first step\nof our method and avoid a costive computation in the second step, which makes\nvery fast fabric detection. More importantly, we propose the\nLocality-constrained Reconstruction Error (LCRE) in the first step and\nRestrictive Locality-constrained Coding (RLC), Bag-of-Indexes (BoI) methods in\nthe second step. We also analyse the connections between different coding\nmethods and conclude that the index of visual words plays an essential role in\nthe coding methods. In conclusion, experiments based on real-world datasets are\nimplemented and demonstrate that our proposed method is not only\ncomputationally simple but also with high detection accuracy.",
          "link": "http://arxiv.org/abs/2108.06760",
          "publishedOn": "2021-08-17T01:54:48.982Z",
          "wordCount": 635,
          "title": "A Cascaded Zoom-In Network for Patterned Fabric Defect Detection. (arXiv:2108.06760v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arindam Chaudhuri</a>",
          "description": "BSplines are one of the most promising curves in computer graphics. They are\nblessed with some superior geometric properties which make them an ideal\ncandidate for several applications in computer aided design industry. In this\narticle, some basic properties of B-Spline curves are presented. Two\nsignificant B-Spline properties viz convex hull property and repeated points\neffects are discussed. The BSplines computation in computational devices is\nalso illustrated. An industry application based on image processing where\nB-Spline curve reconstructs the 3D surfaces for CT image datasets of inner\norgans further highlights the strength of these curves",
          "link": "http://arxiv.org/abs/2108.06617",
          "publishedOn": "2021-08-17T01:54:48.927Z",
          "wordCount": 522,
          "title": "B-Splines. (arXiv:2108.06617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_D/0/1/0/all/0/1\">Donghyeon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Youngmin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1\">Bumsub Ham</a>",
          "description": "We address the problem of generalized zero-shot semantic segmentation (GZS3)\npredicting pixel-wise semantic labels for seen and unseen classes. Most GZS3\nmethods adopt a generative approach that synthesizes visual features of unseen\nclasses from corresponding semantic ones (e.g., word2vec) to train novel\nclassifiers for both seen and unseen classes. Although generative methods show\ndecent performance, they have two limitations: (1) the visual features are\nbiased towards seen classes; (2) the classifier should be retrained whenever\nnovel unseen classes appear. We propose a discriminative approach to address\nthese limitations in a unified framework. To this end, we leverage visual and\nsemantic encoders to learn a joint embedding space, where the semantic encoder\ntransforms semantic features to semantic prototypes that act as centers for\nvisual features of corresponding classes. Specifically, we introduce\nboundary-aware regression (BAR) and semantic consistency (SC) losses to learn\ndiscriminative features. Our approach to exploiting the joint embedding space,\ntogether with BAR and SC terms, alleviates the seen bias problem. At test time,\nwe avoid the retraining process by exploiting semantic prototypes as a\nnearest-neighbor (NN) classifier. To further alleviate the bias problem, we\nalso propose an inference technique, dubbed Apollonius calibration (AC), that\nmodulates the decision boundary of the NN classifier to the Apollonius circle\nadaptively. Experimental results demonstrate the effectiveness of our\nframework, achieving a new state of the art on standard benchmarks.",
          "link": "http://arxiv.org/abs/2108.06536",
          "publishedOn": "2021-08-17T01:54:48.914Z",
          "wordCount": 669,
          "title": "Exploiting a Joint Embedding Space for Generalized Zero-Shot Semantic Segmentation. (arXiv:2108.06536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkpen_D/0/1/0/all/0/1\">Diana Inkpen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Roby_A/0/1/0/all/0/1\">Ahmed El-Roby</a>",
          "description": "Adversarial domain adaptation has made impressive advances in transferring\nknowledge from the source domain to the target domain by aligning feature\ndistributions of both domains. These methods focus on minimizing domain\ndivergence and regard the adaptability, which is measured as the expected error\nof the ideal joint hypothesis on these two domains, as a small constant.\nHowever, these approaches still face two issues: (1) Adversarial domain\nalignment distorts the original feature distributions, deteriorating the\nadaptability; (2) Transforming feature representations to be domain-invariant\nneeds to sacrifice domain-specific variations, resulting in weaker\ndiscriminability. In order to alleviate these issues, we propose\ncategory-invariant feature enhancement (CIFE), a general mechanism that\nenhances the adversarial domain adaptation through optimizing the adaptability.\nSpecifically, the CIFE approach introduces category-invariant features to boost\nthe discriminability of domain-invariant features with preserving the\ntransferability. Experiments show that the CIFE could improve upon\nrepresentative adversarial domain adaptation methods to yield state-of-the-art\nresults on five benchmarks.",
          "link": "http://arxiv.org/abs/2108.06583",
          "publishedOn": "2021-08-17T01:54:48.908Z",
          "wordCount": 601,
          "title": "Towards Category and Domain Alignment: Category-Invariant Feature Enhancement for Adversarial Domain Adaptation. (arXiv:2108.06583v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Shiyu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoqun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_W/0/1/0/all/0/1\">Weize Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong-Ming Yan</a>",
          "description": "Removing undesirable specular highlight from a single input image is of\ncrucial importance to many computer vision and graphics tasks. Existing methods\ntypically remove specular highlight for medical images and specific-object\nimages, however, they cannot handle the images with text. In addition, the\nimpact of specular highlight on text recognition is rarely studied by text\ndetection and recognition community. Therefore, in this paper, we first raise\nand study the text-aware single image specular highlight removal problem. The\ncore goal is to improve the accuracy of text detection and recognition by\nremoving the highlight from text images. To tackle this challenging problem, we\nfirst collect three high-quality datasets with fine-grained annotations, which\nwill be appropriately released to facilitate the relevant research. Then, we\ndesign a novel two-stage network, which contains a highlight detection network\nand a highlight removal network. The output of highlight detection network\nprovides additional information about highlight regions to guide the subsequent\nhighlight removal network. Moreover, we suggest a measurement set including the\nend-to-end text detection and recognition evaluation and auxiliary visual\nquality evaluation. Extensive experiments on our collected datasets demonstrate\nthe superior performance of the proposed method.",
          "link": "http://arxiv.org/abs/2108.06881",
          "publishedOn": "2021-08-17T01:54:48.891Z",
          "wordCount": 624,
          "title": "Text-Aware Single Image Specular Highlight Removal. (arXiv:2108.06881v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>",
          "description": "Food image classification is challenging for real-world applications since\nexisting methods require static datasets for training and are not capable of\nlearning from sequentially available new food images. Online continual learning\naims to learn new classes from data stream by using each new data only once\nwithout forgetting the previously learned knowledge. However, none of the\nexisting works target food image analysis, which is more difficult to learn\nincrementally due to its high intra-class variation with the unbalanced and\nunpredictable characteristics of future food class distribution. In this paper,\nwe address these issues by introducing (1) a novel clustering based exemplar\nselection algorithm to store the most representative data belonging to each\nlearned food for knowledge replay, and (2) an effective online learning regime\nusing balanced training batch along with the knowledge distillation on\naugmented exemplars to maintain the model performance on all learned classes.\nOur method is evaluated on a challenging large scale food image database,\nFood-1K, by varying the number of newly added food classes. Our results show\nsignificant improvements compared with existing state-of-the-art online\ncontinual learning methods, showing great potential to achieve lifelong\nlearning for food image classification in real world.",
          "link": "http://arxiv.org/abs/2108.06781",
          "publishedOn": "2021-08-17T01:54:48.885Z",
          "wordCount": 631,
          "title": "Online Continual Learning For Visual Food Classification. (arXiv:2108.06781v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>",
          "description": "Conventional video inpainting is neither object-oriented nor occlusion-aware,\nmaking it liable to obvious artifacts when large occluded object regions are\ninpainted. This paper presents occlusion-aware video object inpainting, which\nrecovers both the complete shape and appearance for occluded objects in videos\ngiven their visible mask segmentation.\n\nTo facilitate this new research, we construct the first large-scale video\nobject inpainting benchmark YouTube-VOI to provide realistic occlusion\nscenarios with both occluded and visible object masks available. Our technical\ncontribution VOIN jointly performs video object shape completion and occluded\ntexture generation. In particular, the shape completion module models\nlong-range object coherence while the flow completion module recovers accurate\nflow with sharp motion boundary, for propagating temporally-consistent texture\nto the same moving object across frames. For more realistic results, VOIN is\noptimized using both T-PatchGAN and a new spatio-temporal attention-based\nmulti-class discriminator.\n\nFinally, we compare VOIN and strong baselines on YouTube-VOI. Experimental\nresults clearly demonstrate the efficacy of our method including inpainting\ncomplex and dynamic objects. VOIN degrades gracefully with inaccurate input\nvisible mask.",
          "link": "http://arxiv.org/abs/2108.06765",
          "publishedOn": "2021-08-17T01:54:48.876Z",
          "wordCount": 598,
          "title": "Occlusion-Aware Video Object Inpainting. (arXiv:2108.06765v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianzhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuansheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>",
          "description": "Most publicly available datasets for image classification are with single\nlabels, while images are inherently multi-labeled in our daily life. Such an\nannotation gap makes many pre-trained single-label classification models fail\nin practical scenarios. This annotation issue is more concerned for aerial\nimages: Aerial data collected from sensors naturally cover a relatively large\nland area with multiple labels, while annotated aerial datasets, which are\npublicly available (e.g., UCM, AID), are single-labeled. As manually annotating\nmulti-label aerial images would be time/labor-consuming, we propose a novel\nself-correction integrated domain adaptation (SCIDA) method for automatic\nmulti-label learning. SCIDA is weakly supervised, i.e., automatically learning\nthe multi-label image classification model from using massive, publicly\navailable single-label images. To achieve this goal, we propose a novel\nLabel-Wise self-Correction (LWC) module to better explore underlying label\ncorrelations. This module also makes the unsupervised domain adaptation (UDA)\nfrom single- to multi-label data possible. For model training, the proposed\nmodel only uses single-label information yet requires no prior knowledge of\nmulti-labeled data; and it predicts labels for multi-label aerial images. In\nour experiments, trained with single-labeled MAI-AID-s and MAI-UCM-s datasets,\nthe proposed model is tested directly on our collected Multi-scene Aerial Image\n(MAI) dataset.",
          "link": "http://arxiv.org/abs/2108.06810",
          "publishedOn": "2021-08-17T01:54:48.868Z",
          "wordCount": 643,
          "title": "SCIDA: Self-Correction Integrated Domain Adaptation from Single- to Multi-label Aerial Images. (arXiv:2108.06810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neseem_M/0/1/0/all/0/1\">Marina Neseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reda_S/0/1/0/all/0/1\">Sherief Reda</a>",
          "description": "Convolutional Neural Networks achieve state-of-the-art accuracy in object\ndetection tasks. However, they have large computational and energy requirements\nthat challenge their deployment on resource-constrained edge devices. Object\ndetection takes an image as an input, and identifies the existing object\nclasses as well as their locations in the image. In this paper, we leverage the\nprior knowledge about the probabilities that different object categories can\noccur jointly to increase the efficiency of object detection models. In\nparticular, our technique clusters the object categories based on their spatial\nco-occurrence probability. We use those clusters to design an adaptive network.\nDuring runtime, a branch controller decides which part(s) of the network to\nexecute based on the spatial context of the input frame. Our experiments using\nCOCO dataset show that our adaptive object detection model achieves up to 45%\nreduction in the energy consumption, and up to 27% reduction in the latency,\nwith a small loss in the average precision (AP) of object detection.",
          "link": "http://arxiv.org/abs/2108.06850",
          "publishedOn": "2021-08-17T01:54:48.863Z",
          "wordCount": 612,
          "title": "AdaCon: Adaptive Context-Aware Object Detection for Resource-Constrained Embedded Devices. (arXiv:2108.06850v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saavedra_Ruiz_M/0/1/0/all/0/1\">Miguel Saavedra-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_Vargas_A/0/1/0/all/0/1\">Ana Mario Pinto-Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Cano_V/0/1/0/all/0/1\">Victor Romero-Cano</a>",
          "description": "Autonomous landing is a capability that is essential to achieve the full\npotential of multi-rotor drones in many social and industrial applications. The\nimplementation and testing of this capability on physical platforms is risky\nand resource-intensive; hence, in order to ensure both a sound design process\nand a safe deployment, simulations are required before implementing a physical\nprototype. This paper presents the development of a monocular visual system,\nusing a software-in-the-loop methodology, that autonomously and efficiently\nlands a quadcopter drone on a predefined landing pad, thus reducing the risks\nof the physical testing stage. In addition to ensuring that the autonomous\nlanding system as a whole fulfils the design requirements using a Gazebo-based\nsimulation, our approach provides a tool for safe parameter tuning and design\ntesting prior to physical implementation. Finally, the proposed monocular\nvision-only approach to landing pad tracking made it possible to effectively\nimplement the system in an F450 quadcopter drone with the standard\ncomputational capabilities of an Odroid XU4 embedded processor.",
          "link": "http://arxiv.org/abs/2108.06616",
          "publishedOn": "2021-08-17T01:54:48.847Z",
          "wordCount": 616,
          "title": "Monocular visual autonomous landing system for quadcopter drones using software in the loop. (arXiv:2108.06616v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+She_M/0/1/0/all/0/1\">Mengkun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakath_D/0/1/0/all/0/1\">David Nakath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koser_K/0/1/0/all/0/1\">Kevin K&#xf6;ser</a>",
          "description": "Underwater cameras are typically placed behind glass windows to protect them\nfrom the water. Spherical glass, a dome port, is well suited for high water\npressures at great depth, allows for a large field of view, and avoids\nrefraction if a pinhole camera is positioned exactly at the sphere's center.\nAdjusting a real lens perfectly to the dome center is a challenging task, both\nin terms of how to actually guide the centering process (e.g. visual servoing)\nand how to measure the alignment quality, but also, how to mechanically perform\nthe alignment. Consequently, such systems are prone to being decentered by some\noffset, leading to challenging refraction patterns at the sphere that\ninvalidate the pinhole camera model. We show that the overall camera system\nbecomes an axial camera, even for thick domes as used for deep sea exploration\nand provide a non-iterative way to compute the center of refraction without\nrequiring knowledge of exact air, glass or water properties. We also analyze\nthe refractive geometry at the sphere, looking at effects such as forward- vs.\nbackward decentering, iso-refraction curves and obtain a 6th-degree polynomial\nequation for forward projection of 3D points in thin domes. We then propose a\npure underwater calibration procedure to estimate the decentering from multiple\nimages. This estimate can either be used during adjustment to guide the\nmechanical position of the lens, or can be considered in photogrammetric\nunderwater applications.",
          "link": "http://arxiv.org/abs/2108.06575",
          "publishedOn": "2021-08-17T01:54:48.842Z",
          "wordCount": 667,
          "title": "Refractive Geometry for Underwater Domes. (arXiv:2108.06575v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cahall_D/0/1/0/all/0/1\">Daniel E. Cahall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal C. Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathallah_Shaykh_H/0/1/0/all/0/1\">Hassan M. Fathallah-Shaykh</a>",
          "description": "Magnetic resonance imaging (MRI) is routinely used for brain tumor diagnosis,\ntreatment planning, and post-treatment surveillance. Recently, various models\nbased on deep neural networks have been proposed for the pixel-level\nsegmentation of tumors in brain MRIs. However, the structural variations,\nspatial dissimilarities, and intensity inhomogeneity in MRIs make segmentation\na challenging task. We propose a new end-to-end brain tumor segmentation\narchitecture based on U-Net that integrates Inception modules and dilated\nconvolutions into its contracting and expanding paths. This allows us to\nextract local structural as well as global contextual information. We performed\nsegmentation of glioma sub-regions, including tumor core, enhancing tumor, and\nwhole tumor using Brain Tumor Segmentation (BraTS) 2018 dataset. Our proposed\nmodel performed significantly better than the state-of-the-art U-Net-based\nmodel ($p<0.05$) for tumor core and whole tumor segmentation.",
          "link": "http://arxiv.org/abs/2108.06772",
          "publishedOn": "2021-08-17T01:54:48.836Z",
          "wordCount": 572,
          "title": "Dilated Inception U-Net (DIU-Net) for Brain Tumor Segmentation. (arXiv:2108.06772v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>",
          "description": "Unsupervised person re-identification (re-ID) has attracted increasing\nresearch interests because of its scalability and possibility for real-world\napplications. State-of-the-art unsupervised re-ID methods usually follow a\nclustering-based strategy, which generates pseudo labels by clustering and\nmaintains a memory to store instance features and represent the centroid of the\nclusters for contrastive learning. This approach suffers two problems. First,\nthe centroid generated by unsupervised learning may not be a perfect prototype.\nForcing images to get closer to the centroid emphasizes the result of\nclustering, which could accumulate clustering errors during iterations. Second,\nprevious methods utilize features obtained at different training iterations to\nrepresent one centroid, which is not consistent with the current training\nsample, since the features are not directly comparable. To this end, we propose\nan unsupervised re-ID approach with a stochastic learning strategy.\nSpecifically, we adopt a stochastic updated memory, where a random instance\nfrom a cluster is used to update the cluster-level memory for contrastive\nlearning. In this way, the relationship between randomly selected pair of\nimages are learned to avoid the training bias caused by unreliable pseudo\nlabels. The stochastic memory is also always up-to-date for classifying to keep\nthe consistency. Besides, to relieve the issue of camera variance, a unified\ndistance matrix is proposed during clustering, where the distance bias from\ndifferent camera domain is reduced and the variances of identities is\nemphasized.",
          "link": "http://arxiv.org/abs/2108.06938",
          "publishedOn": "2021-08-17T01:54:48.830Z",
          "wordCount": 658,
          "title": "Unsupervised Person Re-identification with Stochastic Training Strategy. (arXiv:2108.06938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peisheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>",
          "description": "Diabetic retinopathy (DR) is one of the most common eye conditions among\ndiabetic patients. However, vision loss occurs primarily in the late stages of\nDR, and the symptoms of visual impairment, ranging from mild to severe, can\nvary greatly, adding to the burden of diagnosis and treatment in clinical\npractice. Deep learning methods based on retinal images have achieved\nremarkable success in automatic DR grading, but most of them neglect that the\npresence of diabetes usually affects both eyes, and ophthalmologists usually\ncompare both eyes concurrently for DR diagnosis, leaving correlations between\nleft and right eyes unexploited. In this study, simulating the diagnostic\nprocess, we propose a two-stream binocular network to capture the subtle\ncorrelations between left and right eyes, in which, paired images of eyes are\nfed into two identical subnetworks separately during training. We design a\ncontrastive grading loss to learn binocular correlation for five-class DR\ndetection, which maximizes inter-class dissimilarity while minimizing the\nintra-class difference. Experimental results on the EyePACS dataset show the\nsuperiority of the proposed binocular model, outperforming monocular methods by\na large margin.",
          "link": "http://arxiv.org/abs/2108.06763",
          "publishedOn": "2021-08-17T01:54:48.824Z",
          "wordCount": 659,
          "title": "Two Eyes Are Better Than One: Exploiting Binocular Correlation for Diabetic Retinopathy Severity Grading. (arXiv:2108.06763v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berjon_D/0/1/0/all/0/1\">Daniel Berj&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuevas_C/0/1/0/all/0/1\">Carlos Cuevas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Narciso Garc&#xed;a</a>",
          "description": "Augmented reality applications are beginning to change the way sports are\nbroadcast, providing richer experiences and valuable insights to fans. The\nfirst step of augmented reality systems is camera calibration, possibly based\non detecting the line markings of the field of play. Most existing proposals\nfor line detection rely on edge detection and Hough transform, but optical\ndistortion and extraneous edges cause inaccurate or spurious detections of line\nmarkings. We propose a novel strategy to automatically and accurately segment\nline markings based on a stochastic watershed transform that is robust to\noptical distortions, since it makes no assumptions about line straightness, and\nis unaffected by the presence of players or the ball in the field of play.\nFirstly, the playing field as a whole is segmented completely eliminating the\nstands and perimeter boards. Then the line markings are extracted.\n\nThe strategy has been tested on a new and public database composed by 60\nannotated images from matches in five stadiums. The results obtained have\nproven that the proposed segmentation algorithm allows successful and precise\ndetection of most line mark pixels.",
          "link": "http://arxiv.org/abs/2108.06432",
          "publishedOn": "2021-08-17T01:54:48.808Z",
          "wordCount": 622,
          "title": "Soccer line mark segmentation with stochastic watershed transform. (arXiv:2108.06432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Annan Li</a>",
          "description": "Fine-grained action recognition is attracting increasing attention due to the\nemerging demand of specific action understanding in real-world applications,\nwhereas the data of rare fine-grained categories is very limited. Therefore, we\npropose the few-shot fine-grained action recognition problem, aiming to\nrecognize novel fine-grained actions with only few samples given for each\nclass. Although progress has been made in coarse-grained actions, existing\nfew-shot recognition methods encounter two issues handling fine-grained\nactions: the inability to capture subtle action details and the inadequacy in\nlearning from data with low inter-class variance. To tackle the first issue, a\nhuman vision inspired bidirectional attention module (BAM) is proposed.\nCombining top-down task-driven signals with bottom-up salient stimuli, BAM\ncaptures subtle action details by accurately highlighting informative\nspatio-temporal regions. To address the second issue, we introduce contrastive\nmeta-learning (CML). Compared with the widely adopted ProtoNet-based method,\nCML generates more discriminative video representations for low inter-class\nvariance data, since it makes full use of potential contrastive pairs in each\ntraining episode. Furthermore, to fairly compare different models, we establish\nspecific benchmark protocols on two large-scale fine-grained action recognition\ndatasets. Extensive experiments show that our method consistently achieves\nstate-of-the-art performance across evaluated tasks.",
          "link": "http://arxiv.org/abs/2108.06647",
          "publishedOn": "2021-08-17T01:54:48.802Z",
          "wordCount": 642,
          "title": "Few-Shot Fine-Grained Action Recognition via Bidirectional Attention and Contrastive Meta-Learning. (arXiv:2108.06647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiangeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Charles R. Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>",
          "description": "In autonomous driving, a LiDAR-based object detector should perform reliably\nat different geographic locations and under various weather conditions. While\nrecent 3D detection research focuses on improving performance within a single\ndomain, our study reveals that the performance of modern detectors can drop\ndrastically cross-domain. In this paper, we investigate unsupervised domain\nadaptation (UDA) for LiDAR-based 3D object detection. On the Waymo Domain\nAdaptation dataset, we identify the deteriorating point cloud quality as the\nroot cause of the performance drop. To address this issue, we present Semantic\nPoint Generation (SPG), a general approach to enhance the reliability of LiDAR\ndetectors against domain shifts. Specifically, SPG generates semantic points at\nthe predicted foreground regions and faithfully recovers missing parts of the\nforeground objects, which are caused by phenomena such as occlusions, low\nreflectance or weather interference. By merging the semantic points with the\noriginal points, we obtain an augmented point cloud, which can be directly\nconsumed by modern LiDAR-based detectors. To validate the wide applicability of\nSPG, we experiment with two representative detectors, PointPillars and PV-RCNN.\nOn the UDA task, SPG significantly improves both detectors across all object\ncategories of interest and at all difficulty levels. SPG can also benefit\nobject detection in the original domain. On the Waymo Open Dataset and KITTI,\nSPG improves 3D detection results of these two methods across all categories.\nCombined with PV-RCNN, SPG achieves state-of-the-art 3D detection results on\nKITTI.",
          "link": "http://arxiv.org/abs/2108.06709",
          "publishedOn": "2021-08-17T01:54:48.794Z",
          "wordCount": 682,
          "title": "SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation. (arXiv:2108.06709v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1\">Sara Abdali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilescu_M/0/1/0/all/0/1\">M. Alex O. Vasilescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1\">Evangelos E. Papalexakis</a>",
          "description": "Generative neural network architectures such as GANs, may be used to generate\nsynthetic instances to compensate for the lack of real data. However, they may\nbe employed to create media that may cause social, political or economical\nupheaval. One emerging media is \"Deepfake\".Techniques that can discriminate\nbetween such media is indispensable. In this paper, we propose a modified\nmultilinear (tensor) method, a combination of linear and multilinear\nregressions for representing fake and real data. We test our approach by\nrepresenting Deepfakes with our modified multilinear (tensor) approach and\nperform SVM classification with encouraging results.",
          "link": "http://arxiv.org/abs/2108.06702",
          "publishedOn": "2021-08-17T01:54:48.788Z",
          "wordCount": 532,
          "title": "Deepfake Representation with Multilinear Regression. (arXiv:2108.06702v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1\">Jiayao Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sifan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yubo Cui</a>",
          "description": "3D single object tracking is a key issue for robotics. In this paper, we\npropose a transformer module called Point-Track-Transformer (PTT) for point\ncloud-based 3D single object tracking. PTT module contains three blocks for\nfeature embedding, position encoding, and self-attention feature computation.\nFeature embedding aims to place features closer in the embedding space if they\nhave similar semantic information. Position encoding is used to encode\ncoordinates of point clouds into high dimension distinguishable features.\nSelf-attention generates refined attention features by computing attention\nweights. Besides, we embed the PTT module into the open-source state-of-the-art\nmethod P2B to construct PTT-Net. Experiments on the KITTI dataset reveal that\nour PTT-Net surpasses the state-of-the-art by a noticeable margin (~10\\%).\nAdditionally, PTT-Net could achieve real-time performance (~40FPS) on NVIDIA\n1080Ti GPU. Our code is open-sourced for the robotics community at\nhttps://github.com/shanjiayao/PTT.",
          "link": "http://arxiv.org/abs/2108.06455",
          "publishedOn": "2021-08-17T01:54:48.783Z",
          "wordCount": 584,
          "title": "PTT: Point-Track-Transformer Module for 3D Single Object Tracking in Point Clouds. (arXiv:2108.06455v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1\">Pedro C. Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1\">Jo&#xe3;o Ribeiro Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_M/0/1/0/all/0/1\">Mohsen Saffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sequeira_A/0/1/0/all/0/1\">Ana F. Sequeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">Jaime S. Cardoso</a>",
          "description": "The recent Covid-19 pandemic and the fact that wearing masks in public is now\nmandatory in several countries, created challenges in the use of face\nrecognition systems (FRS). In this work, we address the challenge of masked\nface recognition (MFR) and focus on evaluating the verification performance in\nFRS when verifying masked vs unmasked faces compared to verifying only unmasked\nfaces. We propose a methodology that combines the traditional triplet loss and\nthe mean squared error (MSE) intending to improve the robustness of an MFR\nsystem in the masked-unmasked comparison mode. The results obtained by our\nproposed method show improvements in a detailed step-wise ablation study. The\nconducted study showed significant performance gains induced by our proposed\ntraining paradigm and modified triplet loss on two evaluation databases.",
          "link": "http://arxiv.org/abs/2108.00996",
          "publishedOn": "2021-08-17T01:54:48.777Z",
          "wordCount": 663,
          "title": "My Eyes Are Up Here: Promoting Focus on Uncovered Regions in Masked Face Recognition. (arXiv:2108.00996v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huanqiang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingfu Zhang</a>",
          "description": "This paper investigates the problem of reconstructing hyperspectral (HS)\nimages from single RGB images captured by commercial cameras, \\textbf{without}\nusing paired HS and RGB images during training. To tackle this challenge, we\npropose a new lightweight and end-to-end learning-based framework.\nSpecifically, on the basis of the intrinsic imaging degradation model of RGB\nimages from HS images, we progressively spread the differences between input\nRGB images and re-projected RGB images from recovered HS images via effective\nunsupervised camera spectral response function estimation. To enable the\nlearning without paired ground-truth HS images as supervision, we adopt the\nadversarial learning manner and boost it with a simple yet effective\n$\\mathcal{L}_1$ gradient clipping scheme. Besides, we embed the semantic\ninformation of input RGB images to locally regularize the unsupervised\nlearning, which is expected to promote pixels with identical semantics to have\nconsistent spectral signatures. In addition to conducting quantitative\nexperiments over two widely-used datasets for HS image reconstruction from\nsynthetic RGB images, we also evaluate our method by applying recovered HS\nimages from real RGB images to HS-based visual tracking. Extensive results show\nthat our method significantly outperforms state-of-the-art unsupervised methods\nand even exceeds the latest supervised method under some settings. The source\ncode is public available at\nhttps://github.com/zbzhzhy/Unsupervised-Spectral-Reconstruction.",
          "link": "http://arxiv.org/abs/2108.06659",
          "publishedOn": "2021-08-17T01:54:48.762Z",
          "wordCount": 651,
          "title": "Semantic-embedded Unsupervised Spectral Reconstruction from Single RGB Images in the Wild. (arXiv:2108.06659v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>",
          "description": "Three popular feature descriptors of computer vision such as SIFT, SURF, and\nORB compared and evaluated. The number of correct features extracted and\nmatched for the original hand hygiene pose-Rub hands palm to palm image and\nrotated image. An accuracy score calculated based on the total number of\nmatches and the correct number of matches produced. The experiment demonstrated\nthat ORB algorithm outperforms by giving the high number of correct matches in\nless amount of time. ORB feature detection technique applied over handwashing\nvideo recordings for feature extraction and hand hygiene pose classification as\na future work. OpenCV utilized to apply the algorithms within python scripts.",
          "link": "http://arxiv.org/abs/2108.06537",
          "publishedOn": "2021-08-17T01:54:48.756Z",
          "wordCount": 534,
          "title": "Feature Identification and Matching for Hand Hygiene Pose. (arXiv:2108.06537v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiahui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Bin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yueqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwen Yu</a>",
          "description": "Surface defect detection plays an increasingly important role in\nmanufacturing industry to guarantee the product quality. Many deep learning\nmethods have been widely used in surface defect detection tasks, and have been\nproven to perform well in defects classification and location. However, deep\nlearning-based detection methods often require plenty of data for training,\nwhich fail to apply to the real industrial scenarios since the distribution of\ndefect categories is often imbalanced. In other words, common defect classes\nhave many samples but rare defect classes have extremely few samples, and it is\ndifficult for these methods to well detect rare defect classes. To solve the\nimbalanced distribution problem, in this paper we propose TL-SDD: a novel\nTransfer Learning-based method for Surface Defect Detection. First, we adopt a\ntwo-phase training scheme to transfer the knowledge from common defect classes\nto rare defect classes. Second, we propose a novel Metric-based Surface Defect\nDetection (M-SDD) model. We design three modules for this model: (1) feature\nextraction module: containing feature fusion which combines high-level semantic\ninformation with low-level structural information. (2) feature reweighting\nmodule: transforming examples to a reweighting vector that indicates the\nimportance of features. (3) distance metric module: learning a metric space in\nwhich defects are classified by computing distances to representations of each\ncategory. Finally, we validate the performance of our proposed method on a real\ndataset including surface defects of aluminum profiles. Compared to the\nbaseline methods, the performance of our proposed method has improved by up to\n11.98% for rare defect classes.",
          "link": "http://arxiv.org/abs/2108.06939",
          "publishedOn": "2021-08-17T01:54:48.751Z",
          "wordCount": 704,
          "title": "TL-SDD: A Transfer Learning-Based Method for Surface Defect Detection with Few Samples. (arXiv:2108.06939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sehun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_H/0/1/0/all/0/1\">Hyunjun Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>",
          "description": "Most recent studies on detecting and localizing temporal anomalies have\nmainly employed deep neural networks to learn the normal patterns of temporal\ndata in an unsupervised manner. Unlike them, the goal of our work is to fully\nutilize instance-level (or weak) anomaly labels, which only indicate whether\nany anomalous events occurred or not in each instance of temporal data. In this\npaper, we present WETAS, a novel framework that effectively identifies\nanomalous temporal segments (i.e., consecutive time points) in an input\ninstance. WETAS learns discriminative features from the instance-level labels\nso that it infers the sequential order of normal and anomalous segments within\neach instance, which can be used as a rough segmentation mask. Based on the\ndynamic time warping (DTW) alignment between the input instance and its\nsegmentation mask, WETAS obtains the result of temporal segmentation, and\nsimultaneously, it further enhances itself by using the mask as additional\nsupervision. Our experiments show that WETAS considerably outperforms other\nbaselines in terms of the localization of temporal anomalies, and also it\nprovides more informative results than point-level detection methods.",
          "link": "http://arxiv.org/abs/2108.06816",
          "publishedOn": "2021-08-17T01:54:48.745Z",
          "wordCount": 636,
          "title": "Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping. (arXiv:2108.06816v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanjun Wu</a>",
          "description": "Automatic security inspection using computer vision technology is a\nchallenging task in real-world scenarios due to various factors, including\nintra-class variance, class imbalance, and occlusion. Most of the previous\nmethods rarely solve the cases that the prohibited items are deliberately\nhidden in messy objects due to the lack of large-scale datasets, restricted\ntheir applications in real-world scenarios. Towards real-world prohibited item\ndetection, we collect a large-scale dataset, named as PIDray, which covers\nvarious cases in real-world scenarios for prohibited item detection, especially\nfor deliberately hidden items. With an intensive amount of effort, our dataset\ncontains $12$ categories of prohibited items in $47,677$ X-ray images with\nhigh-quality annotated segmentation masks and bounding boxes. To the best of\nour knowledge, it is the largest prohibited items detection dataset to date.\nMeanwhile, we design the selective dense attention network (SDANet) to\nconstruct a strong baseline, which consists of the dense attention module and\nthe dependency refinement module. The dense attention module formed by the\nspatial and channel-wise dense attentions, is designed to learn the\ndiscriminative features to boost the performance. The dependency refinement\nmodule is used to exploit the dependencies of multi-scale features. Extensive\nexperiments conducted on the collected PIDray dataset demonstrate that the\nproposed method performs favorably against the state-of-the-art methods,\nespecially for detecting the deliberately hidden items.",
          "link": "http://arxiv.org/abs/2108.07020",
          "publishedOn": "2021-08-17T01:54:48.731Z",
          "wordCount": 661,
          "title": "Towards Real-World Prohibited Item Detection: A Large-Scale X-ray Benchmark. (arXiv:2108.07020v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jinkun Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhijie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "3D object detection is an important task in computer vision. Most existing\nmethods require a large number of high-quality 3D annotations, which are\nexpensive to collect. Especially for outdoor scenes, the problem becomes more\nsevere due to the sparseness of the point cloud and the complexity of urban\nscenes. Semi-supervised learning is a promising technique to mitigate the data\nannotation issue. Inspired by this, we propose a novel semi-supervised\nframework based on pseudo-labeling for outdoor 3D object detection tasks. We\ndesign the Adaptive Class Confidence Selection module (ACCS) to generate\nhigh-quality pseudo-labels. Besides, we propose Holistic Point Cloud\nAugmentation (HPCA) for unlabeled data to improve robustness. Experiments on\nthe KITTI benchmark demonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.06649",
          "publishedOn": "2021-08-17T01:54:48.725Z",
          "wordCount": 567,
          "title": "Semi-supervised 3D Object Detection via Adaptive Pseudo-Labeling. (arXiv:2108.06649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tantawy_D/0/1/0/all/0/1\">Dina Tantawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahran_M/0/1/0/all/0/1\">Mohamed Zahran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wassal_A/0/1/0/all/0/1\">Amr Wassal</a>",
          "description": "Since its invention, Generative adversarial networks (GANs) have shown\noutstanding results in many applications. Generative Adversarial Networks are\npowerful yet, resource-hungry deep-learning models. Their main difference from\nordinary deep learning models is the nature of their output. For example, GAN\noutput can be a whole image versus other models detecting objects or\nclassifying images. Thus, the architecture and numeric precision of the network\naffect the quality and speed of the solution. Hence, accelerating GANs is\npivotal. Accelerating GANs can be classified into three main tracks: (1) Memory\ncompression, (2) Computation optimization, and (3) Data-flow optimization.\nBecause data transfer is the main source of energy usage, memory compression\nleads to the most savings. Thus, in this paper, we survey memory compression\ntechniques for CNN-Based GANs. Additionally, the paper summarizes opportunities\nand challenges in GANs acceleration and suggests open research problems to be\nfurther investigated.",
          "link": "http://arxiv.org/abs/2108.06626",
          "publishedOn": "2021-08-17T01:54:48.690Z",
          "wordCount": 590,
          "title": "A Survey on GAN Acceleration Using Memory Compression Technique. (arXiv:2108.06626v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with a limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-08-17T01:54:48.668Z",
          "wordCount": 608,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>",
          "description": "Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled\nsource domain and an unsupervised loss in an unlabeled target domain, which\noften faces more severe overfitting (than classical supervised learning) as the\nsupervised source loss has clear domain gap and the unsupervised target loss is\noften noisy due to the lack of annotations. This paper presents RDA, a robust\ndomain adaptation technique that introduces adversarial attacking to mitigate\noverfitting in UDA. We achieve robust domain adaptation by a novel Fourier\nadversarial attacking (FAA) method that allows large magnitude of perturbation\nnoises but has minimal modification of image semantics, the former is critical\nto the effectiveness of its generated adversarial samples due to the existence\nof 'domain gaps'. Specifically, FAA decomposes images into multiple frequency\ncomponents (FCs) and generates adversarial samples by just perturbating certain\nFCs that capture little semantic information. With FAA-generated samples, the\ntraining can continue the 'random walk' and drift into an area with a flat loss\nlandscape, leading to more robust domain adaptation. Extensive experiments over\nmultiple domain adaptation tasks show that RDA can work with different computer\nvision tasks with superior performance.",
          "link": "http://arxiv.org/abs/2106.02874",
          "publishedOn": "2021-08-17T01:54:48.662Z",
          "wordCount": 670,
          "title": "RDA: Robust Domain Adaptation via Fourier Adversarial Attacking. (arXiv:2106.02874v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pohn_D/0/1/0/all/0/1\">Daniela P&#xf6;hn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillmann_P/0/1/0/all/0/1\">Peter Hillmann</a>",
          "description": "With the pandemic of COVID-19, people around the world increasingly work from\nhome. Each natural person typically has several digital identities with\ndifferent associated information. During the last years, various identity and\naccess management approaches have gained attraction, helping for example to\naccess other organization's services within trust boundaries. The resulting\nheterogeneity creates a high complexity to differentiate between these\napproaches and scenarios as participating entity; combining them is even\nharder. Last but not least, various actors have a different understanding or\nperspective of the terms, like 'service', in this context. Our paper describes\na reference service with standard components in generic federated identity\nmanagement. This is utilized with modern Enterprise Architecture using the\nframework ArchiMate. The proposed universal federated identity management\nservice model (FIMSM) is applied to describe various federated identity\nmanagement scenarios in a generic service-oriented way. The presented reference\ndesign is approved in multiple aspects and is easily applicable in numerous\nscenarios.",
          "link": "http://arxiv.org/abs/2108.06701",
          "publishedOn": "2021-08-17T01:54:48.603Z",
          "wordCount": 649,
          "title": "Reference Service Model for Federated Identity Management. (arXiv:2108.06701v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1\">Niamh Belton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1\">Aonghus Lawlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1\">Kathleen M. Curran</a>",
          "description": "Noisy data present in medical imaging datasets can often aid the development\nof robust models that are equipped to handle real-world data. However, if the\nbad data contains insufficient anatomical information, it can have a severe\nnegative effect on the model's performance. We propose a novel methodology\nusing a semi-supervised Siamese network to identify bad data. This method\nrequires only a small pool of 'reference' medical images to be reviewed by a\nnon-expert human to ensure the major anatomical structures are present in the\nField of View. The model trains on this reference set and identifies bad data\nby using the Siamese network to compute the distance between the reference set\nand all other medical images in the dataset. This methodology achieves an Area\nUnder the Curve (AUC) of 0.989 for identifying bad data. Code will be available\nat https://git.io/JYFuV.",
          "link": "http://arxiv.org/abs/2108.07130",
          "publishedOn": "2021-08-17T01:54:48.582Z",
          "wordCount": 594,
          "title": "Semi-Supervised Siamese Network for Identifying Bad Data in Medical Imaging Datasets. (arXiv:2108.07130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rundi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changxi Zheng</a>",
          "description": "Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation --- describing a shape\nas a sequence of computer-aided design (CAD) operations. Unlike meshes and\npoint clouds, CAD models encode the user creation process of 3D shapes, widely\nused in numerous industrial and engineering design tasks. However, the\nsequential and irregular structure of CAD operations poses significant\nchallenges for existing 3D generative models. Drawing an analogy between CAD\noperations and natural language, we propose a CAD generative network based on\nthe Transformer. We demonstrate the performance of our model for both shape\nautoencoding and random shape generation. To train our network, we create a new\nCAD dataset consisting of 178,238 models and their CAD construction sequences.\nWe have made this dataset publicly available to promote future research on this\ntopic.",
          "link": "http://arxiv.org/abs/2105.09492",
          "publishedOn": "2021-08-17T01:54:48.561Z",
          "wordCount": 649,
          "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arai_H/0/1/0/all/0/1\">Hayato Arai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onga_Y/0/1/0/all/0/1\">Yuto Onga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikuta_K/0/1/0/all/0/1\">Kumpei Ikuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chayama_Y/0/1/0/all/0/1\">Yusuke Chayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oishi_K/0/1/0/all/0/1\">Kenichi Oishi</a>",
          "description": "To build a robust and practical content-based image retrieval (CBIR) system\nthat is applicable to a clinical brain MRI database, we propose a new framework\n-- Disease-oriented image embedding with pseudo-scanner standardization\n(DI-PSS) -- that consists of two core techniques, data harmonization and a\ndimension reduction algorithm. Our DI-PSS uses skull stripping and\nCycleGAN-based image transformations that map to a standard brain followed by\ntransformation into a brain image taken with a given reference scanner. Then,\nour 3D convolutioinal autoencoders (3D-CAE) with deep metric learning acquires\na low-dimensional embedding that better reflects the characteristics of the\ndisease. The effectiveness of our proposed framework was tested on the\nT1-weighted MRIs selected from the Alzheimer's Disease Neuroimaging Initiative\nand the Parkinson's Progression Markers Initiative. We confirmed that our PSS\ngreatly reduced the variability of low-dimensional embeddings caused by\ndifferent scanner and datasets. Compared with the baseline condition, our PSS\nreduced the variability in the distance from Alzheimer's disease (AD) to\nclinically normal (CN) and Parkinson disease (PD) cases by 15.8-22.6% and\n18.0-29.9%, respectively. These properties allow DI-PSS to generate lower\ndimensional representations that are more amenable to disease classification.\nIn AD and CN classification experiments based on spectral clustering, PSS\nimproved the average accuracy and macro-F1 by 6.2% and 10.7%, respectively.\nGiven the potential of the DI-PSS for harmonizing images scanned by MRI\nscanners that were not used to scan the training data, we expect that the\nDI-PSS is suitable for application to a large number of legacy MRIs scanned in\nheterogeneous environments.",
          "link": "http://arxiv.org/abs/2108.06518",
          "publishedOn": "2021-08-17T01:54:48.532Z",
          "wordCount": 707,
          "title": "Disease-oriented image embedding with pseudo-scanner standardization for content-based image retrieval on 3D brain MRI. (arXiv:2108.06518v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kaiming He</a>",
          "description": "This paper does not describe a novel method. Instead, it studies a\nstraightforward, incremental, yet must-know baseline given the recent progress\nin computer vision: self-supervised learning for Vision Transformers (ViT).\nWhile the training recipes for standard convolutional networks have been highly\nmature and robust, the recipes for ViT are yet to be built, especially in the\nself-supervised scenarios where training becomes more challenging. In this\nwork, we go back to basics and investigate the effects of several fundamental\ncomponents for training self-supervised ViT. We observe that instability is a\nmajor issue that degrades accuracy, and it can be hidden by apparently good\nresults. We reveal that these results are indeed partial failure, and they can\nbe improved when training is made more stable. We benchmark ViT results in MoCo\nv3 and several other self-supervised frameworks, with ablations in various\naspects. We discuss the currently positive evidence as well as challenges and\nopen questions. We hope that this work will provide useful data points and\nexperience for future research.",
          "link": "http://arxiv.org/abs/2104.02057",
          "publishedOn": "2021-08-17T01:54:48.526Z",
          "wordCount": 660,
          "title": "An Empirical Study of Training Self-Supervised Vision Transformers. (arXiv:2104.02057v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Ailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yanfei Zhong</a>",
          "description": "For high spatial resolution (HSR) remote sensing images, bitemporal\nsupervised learning always dominates change detection using many pairwise\nlabeled bitemporal images. However, it is very expensive and time-consuming to\npairwise label large-scale bitemporal HSR remote sensing images. In this paper,\nwe propose single-temporal supervised learning (STAR) for change detection from\na new perspective of exploiting object changes in unpaired images as\nsupervisory signals. STAR enables us to train a high-accuracy change detector\nonly using \\textbf{unpaired} labeled images and generalize to real-world\nbitemporal images. To evaluate the effectiveness of STAR, we design a simple\nyet effective change detector called ChangeStar, which can reuse any deep\nsemantic segmentation architecture by the ChangeMixin module. The comprehensive\nexperimental results show that ChangeStar outperforms the baseline with a large\nmargin under single-temporal supervision and achieves superior performance\nunder bitemporal supervision. Code is available at\nhttps://github.com/Z-Zheng/ChangeStar",
          "link": "http://arxiv.org/abs/2108.07002",
          "publishedOn": "2021-08-17T01:54:48.515Z",
          "wordCount": 591,
          "title": "Change is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery. (arXiv:2108.07002v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shihua/0/1/0/all/0/1\">Shihua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1\">Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhichao/0/1/0/all/0/1\">Zhichao</a>, Lu, Ran, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng/0/1/0/all/0/1\">Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng/0/1/0/all/0/1\">Cheng</a>, He",
          "description": "Recent advancements in deep neural networks have made remarkable\nleap-forwards in dense image prediction. However, the issue of feature\nalignment remains as neglected by most existing approaches for simplicity.\nDirect pixel addition between upsampled and local features leads to feature\nmaps with misaligned contexts that, in turn, translate to mis-classifications\nin prediction, especially on object boundaries. In this paper, we propose a\nfeature alignment module that learns transformation offsets of pixels to\ncontextually align upsampled higher-level features; and another feature\nselection module to emphasize the lower-level features with rich spatial\ndetails. We then integrate these two modules in a top-down pyramidal\narchitecture and present the Feature-aligned Pyramid Network (FaPN). Extensive\nexperimental evaluations on four dense prediction tasks and four datasets have\ndemonstrated the efficacy of FaPN, yielding an overall improvement of 1.2 - 2.6\npoints in AP / mIoU over FPN when paired with Faster / Mask R-CNN. In\nparticular, our FaPN achieves the state-of-the-art of 56.7% mIoU on ADE20K when\nintegrated within Mask-Former. The code is available from\nhttps://github.com/EMI-Group/FaPN.",
          "link": "http://arxiv.org/abs/2108.07058",
          "publishedOn": "2021-08-17T01:54:48.487Z",
          "wordCount": 609,
          "title": "FaPN: Feature-aligned Pyramid Network for Dense Image Prediction. (arXiv:2108.07058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jia Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1\">Andre Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jingxuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuli_M/0/1/0/all/0/1\">Mathieu Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "Neural Architecture Search (NAS) has shifted network design from using human\nintuition to leveraging search algorithms guided by evaluation metrics. We\nstudy channel size optimization in convolutional neural networks (CNN) and\nidentify the role it plays in model accuracy and complexity. Current channel\nsize selection methods are generally limited by discrete sample spaces while\nsuffering from manual iteration and simple heuristics. To solve this, we\nintroduce an efficient dynamic scaling algorithm -- CONet -- that automatically\noptimizes channel sizes across network layers for a given CNN. Two metrics --\n``\\textit{Rank}\" and \"\\textit{Rank Average Slope}\" -- are introduced to\nidentify the information accumulated in training. The algorithm dynamically\nscales channel sizes up or down over a fixed searching phase. We conduct\nexperiments on CIFAR10/100 and ImageNet datasets and show that CONet can find\nefficient and accurate architectures searched in ResNet, DARTS, and DARTS+\nspaces that outperform their baseline models.",
          "link": "http://arxiv.org/abs/2108.06822",
          "publishedOn": "2021-08-17T01:54:48.481Z",
          "wordCount": 609,
          "title": "CONet: Channel Optimization for Convolutional Neural Networks. (arXiv:2108.06822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junheum Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chul Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>",
          "description": "We propose a novel video frame interpolation algorithm based on asymmetric\nbilateral motion estimation (ABME), which synthesizes an intermediate frame\nbetween two input frames. First, we predict symmetric bilateral motion fields\nto interpolate an anchor frame. Second, we estimate asymmetric bilateral\nmotions fields from the anchor frame to the input frames. Third, we use the\nasymmetric fields to warp the input frames backward and reconstruct the\nintermediate frame. Last, to refine the intermediate frame, we develop a new\nsynthesis network that generates a set of dynamic filters and a residual frame\nusing local and global information. Experimental results show that the proposed\nalgorithm achieves excellent performance on various datasets. The source codes\nand pretrained models are available at https://github.com/JunHeum/ABME.",
          "link": "http://arxiv.org/abs/2108.06815",
          "publishedOn": "2021-08-17T01:54:48.441Z",
          "wordCount": 559,
          "title": "Asymmetric Bilateral Motion Estimation for Video Frame Interpolation. (arXiv:2108.06815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuyun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yufei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper aims to explain adversarial attacks in terms of how adversarial\nperturbations contribute to the attacking task. We estimate attributions of\ndifferent image regions to the decrease of the attacking cost based on the\nShapley value. We define and quantify interactions among adversarial\nperturbation pixels, and decompose the entire perturbation map into relatively\nindependent perturbation components. The decomposition of the perturbation map\nshows that adversarially-trained DNNs have more perturbation components in the\nforeground than normally-trained DNNs. Moreover, compared to the\nnormally-trained DNN, the adversarially-trained DNN have more components which\nmainly decrease the score of the true category. Above analyses provide new\ninsights into the understanding of adversarial attacks.",
          "link": "http://arxiv.org/abs/2108.06895",
          "publishedOn": "2021-08-17T01:54:48.429Z",
          "wordCount": 555,
          "title": "Interpreting Attributions and Interactions of Adversarial Attacks. (arXiv:2108.06895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "We study how to evaluate the quantitative information content of a region\nwithin an image for a particular label. To this end, we bridge class activation\nmaps with information theory. We develop an informative class activation map\n(infoCAM). Given a classification task, infoCAM depict how to accumulate\ninformation of partial regions to that of the entire image toward a label.\nThus, we can utilise infoCAM to locate the most informative features for a\nlabel. When applied to an image classification task, infoCAM performs better\nthan the traditional classification map in the weakly supervised object\nlocalisation task. We achieve state-of-the-art results on Tiny-ImageNet.",
          "link": "http://arxiv.org/abs/2106.10472",
          "publishedOn": "2021-08-17T01:54:48.257Z",
          "wordCount": 564,
          "title": "Informative Class Activation Maps. (arXiv:2106.10472v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1\">Jeova F. S. Rocha Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felzenszwalb_P/0/1/0/all/0/1\">Pedro Felzenszwalb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_M/0/1/0/all/0/1\">Marilyn Vazquez</a>",
          "description": "Image segmentation algorithms often depend on appearance models that\ncharacterize the distribution of pixel values in different image regions. We\ndescribe a new approach for estimating appearance models directly from an\nimage, without explicit consideration of the pixels that make up each region.\nOur approach is based on novel algebraic expressions that relate local image\nstatistics to the appearance of spatially coherent regions. We describe two\nalgorithms that can use the aforementioned algebraic expressions to estimate\nappearance models directly from an image. The first algorithm solves a system\nof linear and quadratic equations using a least squares formulation. The second\nalgorithm is a spectral method based on an eigenvector computation. We present\nexperimental results that demonstrate the proposed methods work well in\npractice and lead to effective image segmentation algorithms.",
          "link": "http://arxiv.org/abs/2102.11121",
          "publishedOn": "2021-08-17T01:54:48.228Z",
          "wordCount": 597,
          "title": "Direct Estimation of Appearance Models for Segmentation. (arXiv:2102.11121v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jihoon Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuu_C/0/1/0/all/0/1\">Cheng-hsin Wuu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsuan-ru Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>",
          "description": "We contribute HAA500, a manually annotated human-centric atomic action\ndataset for action recognition on 500 classes with over 591K labeled frames. To\nminimize ambiguities in action classification, HAA500 consists of highly\ndiversified classes of fine-grained atomic actions, where only consistent\nactions fall under the same label, e.g., \"Baseball Pitching\" vs \"Free Throw in\nBasketball\". Thus HAA500 is different from existing atomic action datasets,\nwhere coarse-grained atomic actions were labeled with coarse action-verbs such\nas \"Throw\". HAA500 has been carefully curated to capture the precise movement\nof human figures with little class-irrelevant motions or spatio-temporal label\nnoises. The advantages of HAA500 are fourfold: 1) human-centric actions with a\nhigh average of 69.7% detectable joints for the relevant human poses; 2) high\nscalability since adding a new class can be done under 20-60 minutes; 3)\ncurated videos capturing essential elements of an atomic action without\nirrelevant frames; 4) fine-grained atomic action classes. Our extensive\nexperiments including cross-data validation using datasets collected in the\nwild demonstrate the clear benefits of human-centric and atomic characteristics\nof HAA500, which enable training even a baseline deep learning model to improve\nprediction by attending to atomic human poses. We detail the HAA500 dataset\nstatistics and collection methodology and compare quantitatively with existing\naction recognition datasets.",
          "link": "http://arxiv.org/abs/2009.05224",
          "publishedOn": "2021-08-17T01:54:48.208Z",
          "wordCount": 685,
          "title": "HAA500: Human-Centric Atomic Action Dataset with Curated Videos. (arXiv:2009.05224v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>",
          "description": "Generalized zero-shot learning (GZSL) aims to classify samples under the\nassumption that some classes are not observable during training. To bridge the\ngap between the seen and unseen classes, most GZSL methods attempt to associate\nthe visual features of seen classes with attributes or to generate unseen\nsamples directly. Nevertheless, the visual features used in the prior\napproaches do not necessarily encode semantically related information that the\nshared attributes refer to, which degrades the model generalization to unseen\nclasses. To address this issue, in this paper, we propose a novel semantics\ndisentangling framework for the generalized zero-shot learning task (SDGZSL),\nwhere the visual features of unseen classes are firstly estimated by a\nconditional VAE and then factorized into semantic-consistent and\nsemantic-unrelated latent vectors. In particular, a total correlation penalty\nis applied to guarantee the independence between the two factorized\nrepresentations, and the semantic consistency of which is measured by the\nderived relation network. Extensive experiments conducted on four GZSL\nbenchmark datasets have evidenced that the semantic-consistent features\ndisentangled by the proposed SDGZSL are more generalizable in tasks of\ncanonical and generalized zero-shot learning. Our source code is available at\nhttps://github.com/uqzhichen/SDGZSL.",
          "link": "http://arxiv.org/abs/2101.07978",
          "publishedOn": "2021-08-17T01:54:48.190Z",
          "wordCount": 682,
          "title": "Semantics Disentangling for Generalized Zero-Shot Learning. (arXiv:2101.07978v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.06814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>",
          "description": "As billions of personal data being shared through social media and network,\nthe data privacy and security have drawn an increasing attention. Several\nattempts have been made to alleviate the leakage of identity information from\nface photos, with the aid of, e.g., image obfuscation techniques. However, most\nof the present results are either perceptually unsatisfactory or ineffective\nagainst face recognition systems. Our goal in this paper is to develop a\ntechnique that can encrypt the personal photos such that they can protect users\nfrom unauthorized face recognition systems but remain visually identical to the\noriginal version for human beings. To achieve this, we propose a targeted\nidentity-protection iterative method (TIP-IM) to generate adversarial identity\nmasks which can be overlaid on facial images, such that the original identities\ncan be concealed without sacrificing the visual quality. Extensive experiments\ndemonstrate that TIP-IM provides 95\\%+ protection success rate against various\nstate-of-the-art face recognition models under practical test scenarios.\nBesides, we also show the practical and effective applicability of our method\non a commercial API service.",
          "link": "http://arxiv.org/abs/2003.06814",
          "publishedOn": "2021-08-17T01:54:48.183Z",
          "wordCount": 659,
          "title": "Towards Face Encryption by Generating Adversarial Identity Masks. (arXiv:2003.06814v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1\">Kyungseo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Computer-aided diagnosis has recently received attention for its advantage of\nlow cost and time efficiency. Although deep learning played a major role in the\nrecent success of acne detection, there are still several challenges such as\ncolor shift by inconsistent illumination, variation in scales, and high density\ndistribution. To address these problems, we propose an acne detection network\nwhich consists of three components, specifically: Composite Feature Refinement,\nDynamic Context Enhancement, and Mask-Aware Multi-Attention. First, Composite\nFeature Refinement integrates semantic information and fine details to enrich\nfeature representation, which mitigates the adverse impact of imbalanced\nillumination. Then, Dynamic Context Enhancement controls different receptive\nfields of multi-scale features for context enhancement to handle scale\nvariation. Finally, Mask-Aware Multi-Attention detects densely arranged and\nsmall acne by suppressing uninformative regions and highlighting probable acne\nregions. Experiments are performed on acne image dataset ACNE04 and natural\nimage dataset PASCAL VOC 2007. We demonstrate how our method achieves the\nstate-of-the-art result on ACNE04 and competitive performance with previous\nstate-of-the-art methods on the PASCAL VOC 2007.",
          "link": "http://arxiv.org/abs/2105.14891",
          "publishedOn": "2021-08-17T01:54:48.170Z",
          "wordCount": 641,
          "title": "ACNet: Mask-Aware Attention with Dynamic Context Enhancement for Robust Acne Detection. (arXiv:2105.14891v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Existing methods for arbitrary-shaped text detection in natural scenes face\ntwo critical issues, i.e., 1) fracture detections at the gaps in a text\ninstance; and 2) inaccurate detections of arbitrary-shaped text instances with\ndiverse background context. To address these issues, we propose a novel method\nnamed Intra- and Inter-Instance Collaborative Learning (I3CL). Specifically, to\naddress the first issue, we design an effective convolutional module with\nmultiple receptive fields, which is able to collaboratively learn better\ncharacter and gap feature representations at local and long ranges inside a\ntext instance. To address the second issue, we devise an instance-based\ntransformer module to exploit the dependencies between different text instances\nand a global context module to exploit the semantic context from the shared\nbackground, which are able to collaboratively learn more discriminative text\nfeature representation. In this way, I3CL can effectively exploit the intra-\nand inter-instance dependencies together in a unified end-to-end trainable\nframework. Besides, to make full use of the unlabeled data, we design an\neffective semi-supervised learning method to leverage the pseudo labels via an\nensemble strategy. Without bells and whistles, experimental results show that\nthe proposed I3CL sets new state-of-the-art results on three challenging public\nbenchmarks, i.e., an F-measure of 77.5% on ICDAR2019-ArT, 86.9% on Total-Text,\nand 86.4% on CTW-1500. Notably, our I3CL with the ResNeSt-101 backbone ranked\n1st place on the ICDAR2019-ArT leaderboard. The source code will be made\npublicly available.",
          "link": "http://arxiv.org/abs/2108.01343",
          "publishedOn": "2021-08-17T01:54:48.146Z",
          "wordCount": 692,
          "title": "I3CL:Intra- and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection. (arXiv:2108.01343v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+John_T/0/1/0/all/0/1\">Thrupthi Ann John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>",
          "description": "As Deep Neural Network models for face processing tasks approach human-like\nperformance, their deployment in critical applications such as law enforcement\nand access control has seen an upswing, where any failure may have far-reaching\nconsequences. We need methods to build trust in deployed systems by making\ntheir working as transparent as possible. Existing visualization algorithms are\ndesigned for object recognition and do not give insightful results when applied\nto the face domain. In this work, we present 'Canonical Saliency Maps', a new\nmethod that highlights relevant facial areas by projecting saliency maps onto a\ncanonical face model. We present two kinds of Canonical Saliency Maps:\nimage-level maps and model-level maps. Image-level maps highlight facial\nfeatures responsible for the decision made by a deep face model on a given\nimage, thus helping to understand how a DNN made a prediction on the image.\nModel-level maps provide an understanding of what the entire DNN model focuses\non in each task and thus can be used to detect biases in the model. Our\nqualitative and quantitative results show the usefulness of the proposed\ncanonical saliency maps, which can be used on any deep face model regardless of\nthe architecture.",
          "link": "http://arxiv.org/abs/2105.01386",
          "publishedOn": "2021-08-17T01:54:48.121Z",
          "wordCount": 682,
          "title": "Canonical Saliency Maps: Decoding Deep Face Models. (arXiv:2105.01386v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15765",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1\">Jinli Suo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>",
          "description": "High resolution images are widely used in our daily life, whereas high-speed\nvideo capture is challenging due to the low frame rate of cameras working at\nthe high resolution mode. Digging deeper, the main bottleneck lies in the low\nthroughput of existing imaging systems. Towards this end, snapshot compressive\nimaging (SCI) was proposed as a promising solution to improve the throughput of\nimaging systems by compressive sampling and computational reconstruction.\nDuring acquisition, multiple high-speed images are encoded and collapsed to a\nsingle measurement. After this, algorithms are employed to retrieve the video\nframes from the coded snapshot. Recently developed Plug-and-Play (PnP)\nalgorithms make it possible for SCI reconstruction in large-scale problems.\nHowever, the lack of high-resolution encoding systems still precludes SCI's\nwide application. In this paper, we build a novel hybrid coded aperture\nsnapshot compressive imaging (HCA-SCI) system by incorporating a dynamic liquid\ncrystal on silicon and a high-resolution lithography mask. We further implement\na PnP reconstruction algorithm with cascaded denoisers for high quality\nreconstruction. Based on the proposed HCA-SCI system and algorithm, we achieve\na 10-mega pixel SCI system to capture high-speed scenes, leading to a high\nthroughput of 4.6G voxels per second. Both simulation and real data experiments\nverify the feasibility and performance of our proposed HCA-SCI scheme.",
          "link": "http://arxiv.org/abs/2106.15765",
          "publishedOn": "2021-08-17T01:54:48.102Z",
          "wordCount": 689,
          "title": "10-mega pixel snapshot compressive imaging with a hybrid coded aperture. (arXiv:2106.15765v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shermin_T/0/1/0/all/0/1\">Tasfia Shermin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Shyh Wei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohel_F/0/1/0/all/0/1\">Ferdous Sohel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1\">Manzur Murshed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guojun Lu</a>",
          "description": "Embedding learning (EL) and feature synthesizing (FS) are two of the popular\ncategories of fine-grained GZSL methods. EL or FS using global features cannot\ndiscriminate fine details in the absence of local features. On the other hand,\nEL or FS methods exploiting local features either neglect direct attribute\nguidance or global information. Consequently, neither method performs well. In\nthis paper, we propose to explore global and direct attribute-supervised local\nvisual features for both EL and FS categories in an integrated manner for\nfine-grained GZSL. The proposed integrated network has an EL sub-network and a\nFS sub-network. Consequently, the proposed integrated network can be tested in\ntwo ways. We propose a novel two-step dense attention mechanism to discover\nattribute-guided local visual features. We introduce new mutual learning\nbetween the sub-networks to exploit mutually beneficial information for\noptimization. Moreover, we propose to compute source-target class similarity\nbased on mutual information and transfer-learn the target classes to reduce\nbias towards the source domain during testing. We demonstrate that our proposed\nmethod outperforms contemporary methods on benchmark datasets.",
          "link": "http://arxiv.org/abs/2101.02141",
          "publishedOn": "2021-08-17T01:54:48.001Z",
          "wordCount": 661,
          "title": "Integrated Generalized Zero-Shot Learning for Fine-Grained Classification. (arXiv:2101.02141v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Haoxi Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>",
          "description": "Prevalence of deeper networks driven by self-attention is in stark contrast\nto underexplored point-based methods. In this paper, we propose groupwise\nself-attention as the basic block to construct our network: SepNet. Our\nproposed module can effectively capture both local and global dependencies.\nThis module computes the features of a group based on the summation of the\nweighted features of any point within the group. For convenience, we generalize\ngroupwise operations to assemble this module. To further facilitate our\nnetworks, we deepen and widen SepNet on the tasks of segmentation and\nclassification respectively, and verify its practicality. Specifically, SepNet\nachieves state-of-the-art for the tasks of classification and segmentation on\nmost of the datasets. We show empirical evidence that SepNet can obtain extra\naccuracy in classification or segmentation from increased width or depth,\nrespectively.",
          "link": "http://arxiv.org/abs/2011.14285",
          "publishedOn": "2021-08-17T01:54:47.992Z",
          "wordCount": 602,
          "title": "Deeper or Wider Networks of Point Clouds with Self-attention?. (arXiv:2011.14285v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ajian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenxu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1\">Anyang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zijian Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalante_H/0/1/0/all/0/1\">Hugo Jair Escalante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>",
          "description": "The threat of 3D masks to face recognition systems is increasingly serious\nand has been widely concerned by researchers. To facilitate the study of the\nalgorithms, a large-scale High-Fidelity Mask dataset, namely CASIA-SURF\nHiFiMask (briefly HiFiMask) has been collected. Specifically, it consists of a\ntotal amount of 54, 600 videos which are recorded from 75 subjects with 225\nrealistic masks under 7 new kinds of sensors. Based on this dataset and\nProtocol 3 which evaluates both the discrimination and generalization ability\nof the algorithm under the open set scenarios, we organized a 3D High-Fidelity\nMask Face Presentation Attack Detection Challenge to boost the research of 3D\nmask-based attack detection. It attracted 195 teams for the development phase\nwith a total of 18 teams qualifying for the final round. All the results were\nverified and re-run by the organizing team, and the results were used for the\nfinal ranking. This paper presents an overview of the challenge, including the\nintroduction of the dataset used, the definition of the protocol, the\ncalculation of the evaluation criteria, and the summary and publication of the\ncompetition results. Finally, we focus on introducing and analyzing the top\nranking algorithms, the conclusion summary, and the research ideas for mask\nattack detection provided by this competition.",
          "link": "http://arxiv.org/abs/2108.06968",
          "publishedOn": "2021-08-17T01:54:47.969Z",
          "wordCount": 660,
          "title": "3D High-Fidelity Mask Face Presentation Attack Detection Challenge. (arXiv:2108.06968v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu-Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng-Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Sparse voxel-based 3D convolutional neural networks (CNNs) are widely used\nfor various 3D vision tasks. Sparse voxel-based 3D CNNs create sparse non-empty\nvoxels from the 3D input and perform 3D convolution operations on them only. We\npropose a simple yet effective padding scheme --- interpolation-aware padding\nto pad a few empty voxels adjacent to the non-empty voxels and involve them in\nthe 3D CNN computation so that all neighboring voxels exist when computing\npoint-wise features via the trilinear interpolation. For fine-grained 3D vision\ntasks where point-wise features are essential, like semantic segmentation and\n3D detection, our network achieves higher prediction accuracy than the existing\nnetworks using the nearest neighbor interpolation or the normalized trilinear\ninterpolation with the zero-padding or the octree-padding scheme. Through\nextensive comparisons on various 3D segmentation and detection tasks, we\ndemonstrate the superiority of 3D sparse CNNs with our padding scheme in\nconjunction with feature interpolation.",
          "link": "http://arxiv.org/abs/2108.06925",
          "publishedOn": "2021-08-17T01:54:47.963Z",
          "wordCount": 583,
          "title": "Interpolation-Aware Padding for 3D Sparse Convolutional Neural Networks. (arXiv:2108.06925v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.06555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaogang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "Adversarial training is promising for improving robustness of deep neural\nnetworks towards adversarial perturbations, especially on the classification\ntask. The effect of this type of training on semantic segmentation, contrarily,\njust commences. We make the initial attempt to explore the defense strategy on\nsemantic segmentation by formulating a general adversarial training procedure\nthat can perform decently on both adversarial and clean samples. We propose a\ndynamic divide-and-conquer adversarial training (DDC-AT) strategy to enhance\nthe defense effect, by setting additional branches in the target model during\ntraining, and dealing with pixels with diverse properties towards adversarial\nperturbation. Our dynamical division mechanism divides pixels into multiple\nbranches automatically. Note all these additional branches can be abandoned\nduring inference and thus leave no extra parameter and computation cost.\nExtensive experiments with various segmentation models are conducted on PASCAL\nVOC 2012 and Cityscapes datasets, in which DDC-AT yields satisfying performance\nunder both white- and black-box attack.",
          "link": "http://arxiv.org/abs/2003.06555",
          "publishedOn": "2021-08-17T01:54:47.944Z",
          "wordCount": 617,
          "title": "Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation. (arXiv:2003.06555v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sheyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lina Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1\">Sonal Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowsell_C/0/1/0/all/0/1\">Corwyn Rowsell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damaskinos_S/0/1/0/all/0/1\">Savvas Damaskinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhou Wang</a>",
          "description": "AI technology has made remarkable achievements in computational pathology\n(CPath), especially with the help of deep neural networks. However, the network\nperformance is highly related to architecture design, which commonly requires\nhuman experts with domain knowledge. In this paper, we combat this challenge\nwith the recent advance in neural architecture search (NAS) to find an optimal\nnetwork for CPath applications. In particular, we use differentiable\narchitecture search (DARTS) for its efficiency. We first adopt a probing metric\nto show that the original DARTS lacks proper hyperparameter tuning on the CIFAR\ndataset, and how the generalization issue can be addressed using an adaptive\noptimization strategy. We then apply our searching framework on CPath\napplications by searching for the optimum network architecture on a\nhistological tissue type dataset (ADP). Results show that the searched network\noutperforms state-of-the-art networks in terms of prediction accuracy and\ncomputation complexity. We further conduct extensive experiments to demonstrate\nthe transferability of the searched network to new CPath applications, the\nrobustness against downscaled inputs, as well as the reliability of\npredictions.",
          "link": "http://arxiv.org/abs/2108.06859",
          "publishedOn": "2021-08-17T01:54:47.937Z",
          "wordCount": 616,
          "title": "Probeable DARTS with Application to Computational Pathology. (arXiv:2108.06859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1905.01722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>",
          "description": "Video-based person re-identification has drawn massive attention in recent\nyears due to its extensive applications in video surveillance. While deep\nlearning-based methods have led to significant progress, these methods are\nlimited by ineffectively using complementary information, which is blamed on\nnecessary data augmentation in the training process. Data augmentation has been\nwidely used to mitigate the over-fitting trap and improve the ability of\nnetwork representation. However, the previous methods adopt image-based data\naugmentation scheme to individually process the input frames, which corrupts\nthe complementary information between consecutive frames and causes performance\ndegradation. Extensive experiments on three benchmark datasets demonstrate that\nour framework outperforms the most recent state-of-the-art methods. We also\nperform cross-dataset validation to prove the generality of our method.",
          "link": "http://arxiv.org/abs/1905.01722",
          "publishedOn": "2021-08-17T01:54:47.931Z",
          "wordCount": 609,
          "title": "Intra-clip Aggregation for Video Person Re-identification. (arXiv:1905.01722v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Most polyp segmentation methods use CNNs as their backbone, leading to two\nkey issues when exchanging information between the encoder and decoder: 1)\ntaking into account the differences in contribution between different-level\nfeatures; and 2) designing effective mechanism for fusing these features.\nDifferent from existing CNN-based methods, we adopt a transformer encoder,\nwhich learns more powerful and robust representations. In addition, considering\nthe image acquisition influence and elusive properties of polyps, we introduce\nthree novel modules, including a cascaded fusion module (CFM), a camouflage\nidentification module (CIM), a and similarity aggregation module (SAM). Among\nthese, the CFM is used to collect the semantic and location information of\npolyps from high-level features, while the CIM is applied to capture polyp\ninformation disguised in low-level features. With the help of the SAM, we\nextend the pixel features of the polyp area with high-level semantic position\ninformation to the entire polyp area, thereby effectively fusing cross-level\nfeatures. The proposed model, named \\ourmodel, effectively suppresses noises in\nthe features and significantly improves their expressive capabilities.\nExtensive experiments on five widely adopted datasets show that the proposed\nmodel is more robust to various challenging situations (e.g., appearance\nchanges, small objects) than existing methods, and achieves the new\nstate-of-the-art performance. The proposed model is available at\nhttps://github.com/DengPingFan/Polyp-PVT .",
          "link": "http://arxiv.org/abs/2108.06932",
          "publishedOn": "2021-08-17T01:54:47.924Z",
          "wordCount": 654,
          "title": "Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+kim_D/0/1/0/all/0/1\">Dohyung kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1\">Bumsub Ham</a>",
          "description": "We address the problem of network quantization, that is, reducing bit-widths\nof weights and/or activations to lighten network architectures. Quantization\nmethods use a rounding function to map full-precision values to the nearest\nquantized ones, but this operation is not differentiable. There are mainly two\napproaches to training quantized networks with gradient-based optimizers.\nFirst, a straight-through estimator (STE) replaces the zero derivative of the\nrounding with that of an identity function, which causes a gradient mismatch\nproblem. Second, soft quantizers approximate the rounding with continuous\nfunctions at training time, and exploit the rounding for quantization at test\ntime. This alleviates the gradient mismatch, but causes a quantizer gap\nproblem. We alleviate both problems in a unified framework. To this end, we\nintroduce a novel quantizer, dubbed a distance-aware quantizer (DAQ), that\nmainly consists of a distance-aware soft rounding (DASR) and a temperature\ncontroller. To alleviate the gradient mismatch problem, DASR approximates the\ndiscrete rounding with the kernel soft argmax, which is based on our insight\nthat the quantization can be formulated as a distance-based assignment problem\nbetween full-precision values and quantized ones. The controller adjusts the\ntemperature parameter in DASR adaptively according to the input, addressing the\nquantizer gap problem. Experimental results on standard benchmarks show that\nDAQ outperforms the state of the art significantly for various bit-widths\nwithout bells and whistles.",
          "link": "http://arxiv.org/abs/2108.06983",
          "publishedOn": "2021-08-17T01:54:47.918Z",
          "wordCount": 646,
          "title": "Distance-aware Quantization. (arXiv:2108.06983v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_N/0/1/0/all/0/1\">Ni Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangzhu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xiaoliang Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chuyang Ye</a>",
          "description": "Brain lesion segmentation provides a valuable tool for clinical diagnosis,\nand convolutional neural networks (CNNs) have achieved unprecedented success in\nthe task. Data augmentation is a widely used strategy that improves the\ntraining of CNNs, and the design of the augmentation method for brain lesion\nsegmentation is still an open problem. In this work, we propose a simple data\naugmentation approach, dubbed as CarveMix, for CNN-based brain lesion\nsegmentation. Like other \"mix\"-based methods, such as Mixup and CutMix,\nCarveMix stochastically combines two existing labeled images to generate new\nlabeled samples. Yet, unlike these augmentation strategies based on image\ncombination, CarveMix is lesion-aware, where the combination is performed with\nan attention on the lesions and a proper annotation is created for the\ngenerated image. Specifically, from one labeled image we carve a region of\ninterest (ROI) according to the lesion location and geometry, and the size of\nthe ROI is sampled from a probability distribution. The carved ROI then\nreplaces the corresponding voxels in a second labeled image, and the annotation\nof the second image is replaced accordingly as well. In this way, we generate\nnew labeled images for network training and the lesion information is\npreserved. To evaluate the proposed method, experiments were performed on two\nbrain lesion datasets. The results show that our method improves the\nsegmentation accuracy compared with other simple data augmentation approaches.",
          "link": "http://arxiv.org/abs/2108.06883",
          "publishedOn": "2021-08-17T01:54:47.912Z",
          "wordCount": 674,
          "title": "CarveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation. (arXiv:2108.06883v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Aming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "To improve the generalization of detectors, for domain adaptive object\ndetection (DAOD), recent advances mainly explore aligning feature-level\ndistributions between the source and single-target domain, which may neglect\nthe impact of domain-specific information existing in the aligned features.\nTowards DAOD, it is important to extract domain-invariant object\nrepresentations. To this end, in this paper, we try to disentangle\ndomain-invariant representations from domain-specific representations. And we\npropose a novel disentangled method based on vector decomposition. Firstly, an\nextractor is devised to separate domain-invariant representations from the\ninput, which are used for extracting object proposals. Secondly,\ndomain-specific representations are introduced as the differences between the\ninput and domain-invariant representations. Through the difference operation,\nthe gap between the domain-specific and domain-invariant representations is\nenlarged, which promotes domain-invariant representations to contain more\ndomain-irrelevant information. In the experiment, we separately evaluate our\nmethod on the single- and compound-target case. For the single-target case,\nexperimental results of four domain-shift scenes show our method obtains a\nsignificant performance gain over baseline methods. Moreover, for the\ncompound-target case (i.e., the target is a compound of two different domains\nwithout domain labels), our method outperforms baseline methods by around 4%,\nwhich demonstrates the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.06685",
          "publishedOn": "2021-08-17T01:54:47.893Z",
          "wordCount": 637,
          "title": "Vector-Decomposed Disentanglement for Domain-Invariant Object Detection. (arXiv:2108.06685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_P/0/1/0/all/0/1\">Puspita Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Surbhi Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Richa Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatsa_M/0/1/0/all/0/1\">Mayank Vatsa</a>",
          "description": "Identifying and mitigating bias in deep learning algorithms has gained\nsignificant popularity in the past few years due to its impact on the society.\nResearchers argue that models trained on balanced datasets with good\nrepresentation provide equal and unbiased performance across subgroups.\nHowever, \\textit{can seemingly unbiased pre-trained model become biased when\ninput data undergoes certain distortions?} For the first time, we attempt to\nanswer this question in the context of face recognition. We provide a\nsystematic analysis to evaluate the performance of four state-of-the-art deep\nface recognition models in the presence of image distortions across different\n\\textit{gender} and \\textit{race} subgroups. We have observed that image\ndistortions have a relationship with the performance gap of the model across\ndifferent subgroups.",
          "link": "http://arxiv.org/abs/2108.06581",
          "publishedOn": "2021-08-17T01:54:47.888Z",
          "wordCount": 572,
          "title": "Unravelling the Effect of Image Distortions for Biased Prediction of Pre-trained Face Recognition Models. (arXiv:2108.06581v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1\">Peter Cho-Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lingyang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torgonskiy_M/0/1/0/all/0/1\">Maxim Torgonskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>",
          "description": "Interpreting the decision logic behind effective deep convolutional neural\nnetworks (CNN) on images complements the success of deep learning models.\nHowever, the existing methods can only interpret some specific decision logic\non individual or a small number of images. To facilitate human\nunderstandability and generalization ability, it is important to develop\nrepresentative interpretations that interpret common decision logics of a CNN\non a large group of similar images, which reveal the common semantics data\ncontributes to many closely related predictions. In this paper, we develop a\nnovel unsupervised approach to produce a highly representative interpretation\nfor a large number of similar images. We formulate the problem of finding\nrepresentative interpretations as a co-clustering problem, and convert it into\na submodular cost submodular cover problem based on a sample of the linear\ndecision boundaries of a CNN. We also present a visualization and similarity\nranking method. Our extensive experiments demonstrate the excellent performance\nof our method.",
          "link": "http://arxiv.org/abs/2108.06384",
          "publishedOn": "2021-08-17T01:54:47.881Z",
          "wordCount": 601,
          "title": "Finding Representative Interpretations on Convolutional Neural Networks. (arXiv:2108.06384v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Deepak Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>",
          "description": "Accurate detection and segmentation of marine debris is important for keeping\nthe water bodies clean. This paper presents a novel dataset for marine debris\nsegmentation collected using a Forward Looking Sonar (FLS). The dataset\nconsists of 1868 FLS images captured using ARIS Explorer 3000 sensor. The\nobjects used to produce this dataset contain typical house-hold marine debris\nand distractor marine objects (tires, hooks, valves,etc), divided in 11 classes\nplus a background class. Performance of state of the art semantic segmentation\narchitectures with a variety of encoders have been analyzed on this dataset and\npresented as baseline results. Since the images are grayscale, no pretrained\nweights have been used. Comparisons are made using Intersection over Union\n(IoU). The best performing model is Unet with ResNet34 backbone at 0.7481 mIoU.\nThe dataset is available at\nhttps://github.com/mvaldenegro/marine-debris-fls-datasets/",
          "link": "http://arxiv.org/abs/2108.06800",
          "publishedOn": "2021-08-17T01:54:47.875Z",
          "wordCount": 586,
          "title": "The Marine Debris Dataset for Forward-Looking Sonar Semantic Segmentation. (arXiv:2108.06800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shihao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoqin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shoushun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "Event camera is an emerging imaging sensor for capturing dynamics of moving\nobjects as events, which motivates our work in estimating 3D human pose and\nshape from the event signals. Events, on the other hand, have their unique\nchallenges: rather than capturing static body postures, the event signals are\nbest at capturing local motions. This leads us to propose a two-stage deep\nlearning approach, called EventHPE. The first-stage, FlowNet, is trained by\nunsupervised learning to infer optical flow from events. Both events and\noptical flow are closely related to human body dynamics, which are fed as input\nto the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate\nthe discrepancy between image-based flow (optical flow) and shape-based flow\n(vertices movement of human body shape), a novel flow coherence loss is\nintroduced by exploiting the fact that both flows are originated from the\nidentical human motion. An in-house event-based 3D human dataset is curated\nthat comes with 3D pose and shape annotations, which is by far the largest one\nto our knowledge. Empirical evaluations on DHP19 dataset and our in-house\ndataset demonstrate the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2108.06819",
          "publishedOn": "2021-08-17T01:54:47.870Z",
          "wordCount": 638,
          "title": "EventHPE: Event-based 3D Human Pose and Shape Estimation. (arXiv:2108.06819v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pantazis_O/0/1/0/all/0/1\">Omiros Pantazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brostow_G/0/1/0/all/0/1\">Gabriel Brostow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_K/0/1/0/all/0/1\">Kate Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>",
          "description": "We address the problem of learning self-supervised representations from\nunlabeled image collections. Unlike existing approaches that attempt to learn\nuseful features by maximizing similarity between augmented versions of each\ninput image or by speculatively picking negative samples, we instead also make\nuse of the natural variation that occurs in image collections that are captured\nusing static monitoring cameras. To achieve this, we exploit readily available\ncontext data that encodes information such as the spatial and temporal\nrelationships between the input images. We are able to learn representations\nthat are surprisingly effective for downstream supervised classification, by\nfirst identifying high probability positive pairs at training time, i.e. those\nimages that are likely to depict the same visual concept. For the critical task\nof global biodiversity monitoring, this results in image features that can be\nadapted to challenging visual species classification tasks with limited human\nsupervision. We present results on four different camera trap image\ncollections, across three different families of self-supervised learning\nmethods, and show that careful image selection at training time results in\nsuperior performance compared to existing baselines such as conventional\nself-supervised training and transfer learning.",
          "link": "http://arxiv.org/abs/2108.06435",
          "publishedOn": "2021-08-17T01:54:47.853Z",
          "wordCount": 633,
          "title": "Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring. (arXiv:2108.06435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Antoine Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tuan-Hung Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>",
          "description": "In this work, we address the task of unsupervised domain adaptation (UDA) for\nsemantic segmentation in presence of multiple target domains: The objective is\nto train a single model that can handle all these domains at test time. Such a\nmulti-target adaptation is crucial for a variety of scenarios that real-world\nautonomous systems must handle. It is a challenging setup since one faces not\nonly the domain gap between the labeled source set and the unlabeled target\nset, but also the distribution shifts existing within the latter among the\ndifferent target domains. To this end, we introduce two adversarial frameworks:\n(i) multi-discriminator, which explicitly aligns each target domain to its\ncounterparts, and (ii) multi-target knowledge transfer, which learns a\ntarget-agnostic model thanks to a multi-teacher/single-student distillation\nmechanism.The evaluation is done on four newly-proposed multi-target benchmarks\nfor UDA in semantic segmentation. In all tested scenarios, our approaches\nconsistently outperform baselines, setting competitive standards for the novel\ntask.",
          "link": "http://arxiv.org/abs/2108.06962",
          "publishedOn": "2021-08-17T01:54:47.848Z",
          "wordCount": 605,
          "title": "Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation. (arXiv:2108.06962v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Simon Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirghodsi_S/0/1/0/all/0/1\">Sohrab Amirghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Sarah Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Image harmonization aims to improve the quality of image compositing by\nmatching the \"appearance\" (\\eg, color tone, brightness and contrast) between\nforeground and background images. However, collecting large-scale annotated\ndatasets for this task requires complex professional retouching. Instead, we\npropose a novel Self-Supervised Harmonization framework (SSH) that can be\ntrained using just \"free\" natural images without being edited. We reformulate\nthe image harmonization problem from a representation fusion perspective, which\nseparately processes the foreground and background examples, to address the\nbackground occlusion issue. This framework design allows for a dual data\naugmentation method, where diverse [foreground, background, pseudo GT] triplets\ncan be generated by cropping an image with perturbations using 3D color lookup\ntables (LUTs). In addition, we build a real-world harmonization dataset as\ncarefully created by expert users, for evaluation and benchmarking purposes.\nOur results show that the proposed self-supervised method outperforms previous\nstate-of-the-art methods in terms of reference metrics, visual quality, and\nsubject user study. Code and dataset are available at\n\\url{https://github.com/VITA-Group/SSHarmonization}.",
          "link": "http://arxiv.org/abs/2108.06805",
          "publishedOn": "2021-08-17T01:54:47.842Z",
          "wordCount": 616,
          "title": "SSH: A Self-Supervised Framework for Image Harmonization. (arXiv:2108.06805v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>",
          "description": "Vision-and-language pretraining (VLP) aims to learn generic multimodal\nrepresentations from massive image-text pairs. While various successful\nattempts have been proposed, learning fine-grained semantic alignments between\nimage-text pairs plays a key role in their approaches. Nevertheless, most\nexisting VLP approaches have not fully utilized the intrinsic knowledge within\nthe image-text pairs, which limits the effectiveness of the learned alignments\nand further restricts the performance of their models. To this end, we\nintroduce a new VLP method called ROSITA, which integrates the cross- and\nintra-modal knowledge in a unified scene graph to enhance the semantic\nalignments. Specifically, we introduce a novel structural knowledge masking\n(SKM) strategy to use the scene graph structure as a priori to perform masked\nlanguage (region) modeling, which enhances the semantic alignments by\neliminating the interference information within and across modalities.\nExtensive ablation studies and comprehensive analysis verifies the\neffectiveness of ROSITA in semantic alignments. Pretrained with both in-domain\nand out-of-domain datasets, ROSITA significantly outperforms existing\nstate-of-the-art VLP methods on three typical vision-and-language tasks over\nsix benchmark datasets.",
          "link": "http://arxiv.org/abs/2108.07073",
          "publishedOn": "2021-08-17T01:54:47.836Z",
          "wordCount": 634,
          "title": "ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration. (arXiv:2108.07073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yongwei Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guiqing Li</a>",
          "description": "In this paper, we propose $\\text{HF}^2$-VAD, a Hybrid framework that\nintegrates Flow reconstruction and Frame prediction seamlessly to handle Video\nAnomaly Detection. Firstly, we design the network of ML-MemAE-SC (Multi-Level\nMemory modules in an Autoencoder with Skip Connections) to memorize normal\npatterns for optical flow reconstruction so that abnormal events can be\nsensitively identified with larger flow reconstruction errors. More\nimportantly, conditioned on the reconstructed flows, we then employ a\nConditional Variational Autoencoder (CVAE), which captures the high correlation\nbetween video frame and optical flow, to predict the next frame given several\nprevious frames. By CVAE, the quality of flow reconstruction essentially\ninfluences that of frame prediction. Therefore, poorly reconstructed optical\nflows of abnormal events further deteriorate the quality of the final predicted\nfuture frame, making the anomalies more detectable. Experimental results\ndemonstrate the effectiveness of the proposed method. Code is available at\n\\href{https://github.com/LiUzHiAn/hf2vad}{https://github.com/LiUzHiAn/hf2vad}.",
          "link": "http://arxiv.org/abs/2108.06852",
          "publishedOn": "2021-08-17T01:54:47.830Z",
          "wordCount": 608,
          "title": "A Hybrid Video Anomaly Detection Framework via Memory-Augmented Flow Reconstruction and Flow-Guided Frame Prediction. (arXiv:2108.06852v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06460",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hong_K/0/1/0/all/0/1\">Kai Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Cailian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yancheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>",
          "description": "This work presents an unsupervised deep learning scheme that exploiting\nhigh-dimensional assisted score-based generative model for color image\nrestoration tasks. Considering that the sample number and internal dimension in\nscore-based generative model have key influence on estimating the gradients of\ndata distribution, two different high-dimensional ways are proposed: The\nchannel-copy transformation increases the sample number and the pixel-scale\ntransformation decreases feasible space dimension. Subsequently, a set of\nhigh-dimensional tensors represented by these transformations are used to train\nthe network through denoising score matching. Then, sampling is performed by\nannealing Langevin dynamics and alternative data-consistency update.\nFurthermore, to alleviate the difficulty of learning high-dimensional\nrepresentation, a progressive strategy is proposed to leverage the performance.\nThe proposed unsupervised learning and iterative restoration algo-rithm, which\ninvolves a pre-trained generative network to obtain prior, has transparent and\nclear interpretation compared to other data-driven approaches. Experimental\nresults on demosaicking and inpainting conveyed the remarkable performance and\ndiversity of our proposed method.",
          "link": "http://arxiv.org/abs/2108.06460",
          "publishedOn": "2021-08-17T01:54:47.804Z",
          "wordCount": 614,
          "title": "High-dimensional Assisted Generative Model for Color Image Restoration. (arXiv:2108.06460v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_M/0/1/0/all/0/1\">Mohammad Reza Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarei_A/0/1/0/all/0/1\">Ariyan Zarei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Hoshin V. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnard_K/0/1/0/all/0/1\">Kobus Barnard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrangi_A/0/1/0/all/0/1\">Ali Behrangi</a>",
          "description": "Accurate and timely estimation of precipitation is critical for issuing\nhazard warnings (e.g., for flash floods or landslides). Current remotely sensed\nprecipitation products have a few hours of latency, associated with the\nacquisition and processing of satellite data. By applying a robust nowcasting\nsystem to these products, it is (in principle) possible to reduce this latency\nand improve their applicability, value, and impact. However, the development of\nsuch a system is complicated by the chaotic nature of the atmosphere, and the\nconsequent rapid changes that can occur in the structures of precipitation\nsystems In this work, we develop two approaches (hereafter referred to as\nNowcasting-Nets) that use Recurrent and Convolutional deep neural network\nstructures to address the challenge of precipitation nowcasting. A total of\nfive models are trained using Global Precipitation Measurement (GPM) Integrated\nMulti-satellitE Retrievals for GPM (IMERG) precipitation data over the Eastern\nContiguous United States (CONUS) and then tested against independent data for\nthe Eastern and Western CONUS. The models were designed to provide forecasts\nwith a lead time of up to 1.5 hours and, by using a feedback loop approach, the\nability of the models to extend the forecast time to 4.5 hours was also\ninvestigated. Model performance was compared against the Random Forest (RF) and\nLinear Regression (LR) machine learning methods, and also against a persistence\nbenchmark (BM) that used the most recent observation as the forecast.\nIndependent IMERG observations were used as a reference, and experiments were\nconducted to examine both overall statistics and case studies involving\nspecific precipitation events. Overall, the forecasts provided by the\nNowcasting-Net models are superior, with the Convolutional Nowcasting Network\nwith Residual Head (CNC-R) achieving 25%, 28%, and 46% improvement in the test\n...",
          "link": "http://arxiv.org/abs/2108.06868",
          "publishedOn": "2021-08-17T01:54:47.798Z",
          "wordCount": 746,
          "title": "Nowcasting-Nets: Deep Neural Network Structures for Precipitation Nowcasting Using IMERG. (arXiv:2108.06868v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guodong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>",
          "description": "Over the past few years, the success in action recognition on short trimmed\nvideos has led more investigations towards the temporal segmentation of actions\nin untrimmed long videos. Recently, supervised approaches have achieved\nexcellent performance in segmenting complex human actions in untrimmed videos.\nHowever, besides action labels, such approaches also require the start and end\npoints of each action, which is expensive and tedious to collect.\n\nIn this paper, we aim to learn the action segments taking only the high-level\nactivity labels as input. Under the setting where no action-level supervision\nis provided, Hungarian matching is often used to find the mapping between\nsegments and ground truth actions to evaluate the model and report the\nperformance. On the one hand, we show that with the high-level supervision, we\nare able to generalize the Hungarian matching settings from the current video\nand activity level to the global level. The extended global-level matching\nallows for the shared actions across activities. On the other hand, we propose\na novel action discovery framework that automatically discovers constituent\nactions in videos with the activity classification task. Specifically, we\ndefine a finite number of prototypes to form a dual representation of a video\nsequence. These collectively learned prototypes are considered discovered\nactions. This classification setting endows our approach the capability of\ndiscovering potentially shared actions across multiple complex activities.\nExtensive experiments demonstrate that the discovered actions are helpful in\nperforming temporal action segmentation and activity recognition.",
          "link": "http://arxiv.org/abs/2108.06706",
          "publishedOn": "2021-08-17T01:54:47.791Z",
          "wordCount": 677,
          "title": "Temporal Action Segmentation with High-level Complex Activity Labels. (arXiv:2108.06706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianchao Yang</a>",
          "description": "Generative Adversarial Networks (GANs) have witnessed prevailing success in\nyielding outstanding images, however, they are burdensome to deploy on\nresource-constrained devices due to ponderous computational costs and hulking\nmemory usage. Although recent efforts on compressing GANs have acquired\nremarkable results, they still exist potential model redundancies and can be\nfurther compressed. To solve this issue, we propose a novel online\nmulti-granularity distillation (OMGD) scheme to obtain lightweight GANs, which\ncontributes to generating high-fidelity images with low computational demands.\nWe offer the first attempt to popularize single-stage online distillation for\nGAN-oriented compression, where the progressively promoted teacher generator\nhelps to refine the discriminator-free based student generator. Complementary\nteacher generators and network layers provide comprehensive and\nmulti-granularity concepts to enhance visual fidelity from diverse dimensions.\nExperimental results on four benchmark datasets demonstrate that OMGD successes\nto compress 40x MACs and 82.5X parameters on Pix2Pix and CycleGAN, without loss\nof image quality. It reveals that OMGD provides a feasible solution for the\ndeployment of real-time image translation on resource-constrained devices. Our\ncode and models are made public at: https://github.com/bytedance/OMGD.",
          "link": "http://arxiv.org/abs/2108.06908",
          "publishedOn": "2021-08-17T01:54:47.786Z",
          "wordCount": 614,
          "title": "Online Multi-Granularity Distillation for GAN Compression. (arXiv:2108.06908v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruikui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruijie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>",
          "description": "The transferability and robustness of adversarial examples are two practical\nyet important properties for black-box adversarial attacks. In this paper, we\nexplore effective mechanisms to boost both of them from the perspective of\nnetwork hierarchy, where a typical network can be hierarchically divided into\noutput stage, intermediate stage and input stage. Since over-specialization of\nsource model, we can hardly improve the transferability and robustness of the\nadversarial perturbations in the output stage. Therefore, we focus on the\nintermediate and input stages in this paper and propose a transferable and\nrobust adversarial perturbation generation (TRAP) method. Specifically, we\npropose the dynamically guided mechanism to continuously calculate accurate\ndirectional guidances for perturbation generation in the intermediate stage. In\nthe input stage, instead of the single-form transformation augmentations\nadopted in the existing methods, we leverage multiform affine transformation\naugmentations to further enrich the input diversity and boost the robustness\nand transferability of the adversarial perturbations. Extensive experiments\ndemonstrate that our TRAP achieves impressive transferability and high\nrobustness against certain interferences.",
          "link": "http://arxiv.org/abs/2108.07033",
          "publishedOn": "2021-08-17T01:54:47.768Z",
          "wordCount": 613,
          "title": "Exploring Transferable and Robust Adversarial Perturbation Generation from the Perspective of Network Hierarchy. (arXiv:2108.07033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>",
          "description": "Recent works have theoretically and empirically shown that deep neural\nnetworks (DNNs) have an inherent vulnerability to small perturbations. Applying\nthe Deep k-Nearest Neighbors (DkNN) classifier, we observe a dramatically\nincreasing robustness-accuracy trade-off as the layer goes deeper. In this\nwork, we propose a Deep Adversarially-Enhanced k-Nearest Neighbors (DAEkNN)\nmethod which achieves higher robustness than DkNN and mitigates the\nrobustness-accuracy trade-off in deep layers through two key elements. First,\nDAEkNN is based on an adversarially trained model. Second, DAEkNN makes\npredictions by leveraging a weighted combination of benign and adversarial\ntraining data. Empirically, we find that DAEkNN improves both the robustness\nand the robustness-accuracy trade-off on MNIST and CIFAR-10 datasets.",
          "link": "http://arxiv.org/abs/2108.06797",
          "publishedOn": "2021-08-17T01:54:47.762Z",
          "wordCount": 539,
          "title": "Deep Adversarially-Enhanced k-Nearest Neighbors. (arXiv:2108.06797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_T/0/1/0/all/0/1\">Tianrui Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Annan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_X/0/1/0/all/0/1\">Xinyu Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>",
          "description": "Video-based person re-identification (Re-ID) which aims to associate people\nacross non-overlapping cameras using surveillance video is a challenging task.\nPedestrian attribute, such as gender, age and clothing characteristics contains\nrich and supplementary information but is less explored in video person Re-ID.\nIn this work, we propose a novel network architecture named Attribute Salience\nAssisted Network (ASA-Net) for attribute-assisted video person Re-ID, which\nachieved considerable improvement to existing works by two methods.First, to\nlearn a better separation of the target from background, we propose to learn\nthe visual attention from middle-level attribute instead of high-level\nidentities. The proposed Attribute Salient Region Enhance (ASRE) module can\nattend more accurately on the body of pedestrian. Second, we found that many\nidentity-irrelevant but object or subject-relevant factors like the view angle\nand movement of the target pedestrian can greatly influence the two dimensional\nappearance of a pedestrian. This problem can be mitigated by investigating both\nidentity-relevant and identity-irrelevant attributes via a novel triplet loss\nwhich is referred as the Pose~\\&~Motion-Invariant (PMI) triplet loss.",
          "link": "http://arxiv.org/abs/2108.06946",
          "publishedOn": "2021-08-17T01:54:47.756Z",
          "wordCount": 604,
          "title": "Video Person Re-identification using Attribute-enhanced Features. (arXiv:2108.06946v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haobin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xinyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "Lacking the ability to sense ambient environments effectively, blind and\nvisually impaired people (BVIP) face difficulty in walking outdoors, especially\nin urban areas. Therefore, tools for assisting BVIP are of great importance. In\nthis paper, we propose a novel \"flying guide dog\" prototype for BVIP assistance\nusing drone and street view semantic segmentation. Based on the walkable areas\nextracted from the segmentation prediction, the drone can adjust its movement\nautomatically and thus lead the user to walk along the walkable path. By\nrecognizing the color of pedestrian traffic lights, our prototype can help the\nuser to cross a street safely. Furthermore, we introduce a new dataset named\nPedestrian and Vehicle Traffic Lights (PVTL), which is dedicated to traffic\nlight recognition. The result of our user study in real-world scenarios shows\nthat our prototype is effective and easy to use, providing new insight into\nBVIP assistance.",
          "link": "http://arxiv.org/abs/2108.07007",
          "publishedOn": "2021-08-17T01:54:47.665Z",
          "wordCount": 632,
          "title": "Flying Guide Dog: Walkable Path Discovery for the Visually Impaired Utilizing Drones and Transformer-based Semantic Segmentation. (arXiv:2108.07007v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chaochao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_T/0/1/0/all/0/1\">Thomas Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoan Li</a>",
          "description": "Registration of 3D anatomic structures to their 2D dual fluoroscopic X-ray\nimages is a widely used motion tracking technique. However, deep learning\nimplementation is often impeded by a paucity of medical images and ground\ntruths. In this study, we proposed a transfer learning strategy for 3D-to-2D\nregistration using deep neural networks trained from an artificial dataset.\nDigitally reconstructed radiographs (DRRs) and radiographic skull landmarks\nwere automatically created from craniocervical CT data of a female subject.\nThey were used to train a residual network (ResNet) for landmark detection and\na cycle generative adversarial network (GAN) to eliminate the style difference\nbetween DRRs and actual X-rays. Landmarks on the X-rays experiencing GAN style\ntranslation were detected by the ResNet, and were used in triangulation\noptimization for 3D-to-2D registration of the skull in actual dual-fluoroscope\nimages (with a non-orthogonal setup, point X-ray sources, image distortions,\nand partially captured skull regions). The registration accuracy was evaluated\nin multiple scenarios of craniocervical motions. In walking, learning-based\nregistration for the skull had angular/position errors of 3.9 +- 2.1 deg / 4.6\n+- 2.2 mm. However, the accuracy was lower during functional neck activity, due\nto overly small skull regions imaged on the dual fluoroscopic images at\nend-range positions. The methodology to strategically augment artificial\ntraining data can tackle the complicated skull registration scenario, and has\npotentials to extend to widespread registration scenarios.",
          "link": "http://arxiv.org/abs/2108.06466",
          "publishedOn": "2021-08-17T01:54:47.644Z",
          "wordCount": 683,
          "title": "Transfer Learning from an Artificial Radiograph-landmark Dataset for Registration of the Anatomic Skull Model to Dual Fluoroscopic X-ray Images. (arXiv:2108.06466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bultmann_S/0/1/0/all/0/1\">Simon Bultmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quenzel_J/0/1/0/all/0/1\">Jan Quenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "Unmanned aerial vehicles (UAVs) equipped with multiple complementary sensors\nhave tremendous potential for fast autonomous or remote-controlled semantic\nscene analysis, e.g., for disaster examination. In this work, we propose a UAV\nsystem for real-time semantic inference and fusion of multiple sensor\nmodalities. Semantic segmentation of LiDAR scans and RGB images, as well as\nobject detection on RGB and thermal images, run online onboard the UAV computer\nusing lightweight CNN architectures and embedded inference accelerators. We\nfollow a late fusion approach where semantic information from multiple\nmodalities augments 3D point clouds and image segmentation masks while also\ngenerating an allocentric semantic map. Our system provides augmented semantic\nimages and point clouds with $\\approx\\,$9$\\,$Hz. We evaluate the integrated\nsystem in real-world experiments in an urban environment.",
          "link": "http://arxiv.org/abs/2108.06608",
          "publishedOn": "2021-08-17T01:54:47.587Z",
          "wordCount": 575,
          "title": "Real-Time Multi-Modal Semantic Fusion on Unmanned Aerial Vehicles. (arXiv:2108.06608v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Binghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>",
          "description": "Few-shot semantic segmentation is a challenging task of predicting object\ncategories in pixel-wise with only few annotated samples. However, existing\napproaches still face two main challenges. First, huge feature distinction\nbetween support and query images causes knowledge transferring barrier, which\nharms the segmentation performance. Second, few support samples cause\nunrepresentative of support features, hardly to guide high-quality query\nsegmentation. To deal with the above two issues, we propose self-distillation\nembedded supervised affinity attention model (SD-AANet) to improve the\nperformance of few-shot segmentation task. Specifically, the self-distillation\nguided prototype module (SDPM) extracts intrinsic prototype by\nself-distillation between support and query to capture representative features.\nThe supervised affinity attention module (SAAM) adopts support ground truth to\nguide the production of high quality query attention map, which can learn\naffinity information to focus on whole area of query target. Extensive\nexperiments prove that our SD-AANet significantly improves the performance\ncomparing with existing methods. Comprehensive ablation experiments and\nvisualization studies also show the significant effect of SDPM and SAAM for\nfew-shot segmentation task. On benchmark datasets, PASCAL-5i and COCO-20i, our\nproposed SD-AANet both achieve state-of-the-art results. Our code will be\npublicly available soon.",
          "link": "http://arxiv.org/abs/2108.06600",
          "publishedOn": "2021-08-17T01:54:47.575Z",
          "wordCount": 636,
          "title": "A Self-Distillation Embedded Supervised Affinity Attention Model for Few-Shot Segmentation. (arXiv:2108.06600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "A lifespan face synthesis (LFS) model aims to generate a set of\nphoto-realistic face images of a person's whole life, given only one snapshot\nas reference. The generated face image given a target age code is expected to\nbe age-sensitive reflected by bio-plausible transformations of shape and\ntexture, while being identity preserving. This is extremely challenging because\nthe shape and texture characteristics of a face undergo separate and highly\nnonlinear transformations w.r.t. age. Most recent LFS models are based on\ngenerative adversarial networks (GANs) whereby age code conditional\ntransformations are applied to a latent face representation. They benefit\ngreatly from the recent advancements of GANs. However, without explicitly\ndisentangling their latent representations into the texture, shape and identity\nfactors, they are fundamentally limited in modeling the nonlinear age-related\ntransformation on texture and shape whilst preserving identity. In this work, a\nnovel LFS model is proposed to disentangle the key face characteristics\nincluding shape, texture and identity so that the unique shape and texture age\ntransformations can be modeled effectively. This is achieved by extracting\nshape, texture and identity features separately from an encoder. Critically,\ntwo transformation modules, one conditional convolution based and the other\nchannel attention based, are designed for modeling the nonlinear shape and\ntexture feature transformations respectively. This is to accommodate their\nrather distinct aging processes and ensure that our synthesized images are both\nage-sensitive and identity preserving. Extensive experiments show that our LFS\nmodel is clearly superior to the state-of-the-art alternatives. Codes and demo\nare available on our project website:\n\\url{https://senhe.github.io/projects/iccv_2021_lifespan_face}.",
          "link": "http://arxiv.org/abs/2108.02874",
          "publishedOn": "2021-08-16T01:57:42.822Z",
          "wordCount": 711,
          "title": "Disentangled Lifespan Face Synthesis. (arXiv:2108.02874v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xuejing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>",
          "description": "The nonlocal-based blocks are designed for capturing long-range\nspatial-temporal dependencies in computer vision tasks. Although having shown\nexcellent performance, they still lack the mechanism to encode the rich,\nstructured information among elements in an image or video. In this paper, to\ntheoretically analyze the property of these nonlocal-based blocks, we provide a\nnew perspective to interpret them, where we view them as a set of graph filters\ngenerated on a fully-connected graph. Specifically, when choosing the Chebyshev\ngraph filter, a unified formulation can be derived for explaining and analyzing\nthe existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage,\ndouble attention block). Furthermore, by concerning the property of spectral,\nwe propose an efficient and robust spectral nonlocal block, which can be more\nrobust and flexible to catch long-range dependencies when inserted into deep\nneural networks than the existing nonlocal blocks. Experimental results\ndemonstrate the clear-cut improvements and practical applicabilities of our\nmethod on image classification, action recognition, semantic segmentation, and\nperson re-identification tasks.",
          "link": "http://arxiv.org/abs/2108.02451",
          "publishedOn": "2021-08-16T00:47:33.983Z",
          "wordCount": 627,
          "title": "Unifying Nonlocal Blocks for Neural Networks. (arXiv:2108.02451v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiafei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheston Tan</a>",
          "description": "Recent advancements in deep learning, computer vision, and embodied AI have\ngiven rise to synthetic causal reasoning video datasets. These datasets\nfacilitate the development of AI algorithms that can reason about physical\ninteractions between objects. However, datasets thus far have primarily focused\non elementary physical events such as rolling or falling. There is currently a\nscarcity of datasets that focus on the physical interactions that humans\nperform daily with objects in the real world. To address this scarcity, we\nintroduce SPACE: A Simulator for Physical Interactions and Causal Learning in\n3D Environments. The SPACE simulator allows us to generate the SPACE dataset, a\nsynthetic video dataset in a 3D environment, to systematically evaluate\nphysics-based models on a range of physical causal reasoning tasks. Inspired by\ndaily object interactions, the SPACE dataset comprises videos depicting three\ntypes of physical events: containment, stability and contact. These events make\nup the vast majority of the basic physical interactions between objects. We\nthen further evaluate it with a state-of-the-art physics-based deep model and\nshow that the SPACE dataset improves the learning of intuitive physics with an\napproach inspired by curriculum learning. Repository:\nhttps://github.com/jiafei1224/SPACE",
          "link": "http://arxiv.org/abs/2108.06180",
          "publishedOn": "2021-08-16T00:47:33.910Z",
          "wordCount": 648,
          "title": "SPACE: A Simulator for Physical Interactions and Causal Learning in 3D Environments. (arXiv:2108.06180v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berghoff_C/0/1/0/all/0/1\">Christian Berghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielik_P/0/1/0/all/0/1\">Pavol Bielik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neu_M/0/1/0/all/0/1\">Matthias Neu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsankov_P/0/1/0/all/0/1\">Petar Tsankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twickel_A/0/1/0/all/0/1\">Arndt von Twickel</a>",
          "description": "In the last years, AI systems, in particular neural networks, have seen a\ntremendous increase in performance, and they are now used in a broad range of\napplications. Unlike classical symbolic AI systems, neural networks are trained\nusing large data sets and their inner structure containing possibly billions of\nparameters does not lend itself to human interpretation. As a consequence, it\nis so far not feasible to provide broad guarantees for the correct behaviour of\nneural networks during operation if they process input data that significantly\ndiffer from those seen during training. However, many applications of AI\nsystems are security- or safety-critical, and hence require obtaining\nstatements on the robustness of the systems when facing unexpected events,\nwhether they occur naturally or are induced by an attacker in a targeted way.\nAs a step towards developing robust AI systems for such applications, this\npaper presents how the robustness of AI systems can be practically examined and\nwhich methods and metrics can be used to do so. The robustness testing\nmethodology is described and analysed for the example use case of traffic sign\nrecognition in autonomous driving.",
          "link": "http://arxiv.org/abs/2108.06159",
          "publishedOn": "2021-08-16T00:47:33.891Z",
          "wordCount": 686,
          "title": "Robustness testing of AI systems: A case study for traffic sign recognition. (arXiv:2108.06159v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shoukui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Erjin Zhou</a>",
          "description": "Human pose estimation deeply relies on visual clues and anatomical\nconstraints between parts to locate keypoints. Most existing CNN-based methods\ndo well in visual representation, however, lacking in the ability to explicitly\nlearn the constraint relationships between keypoints. In this paper, we propose\na novel approach based on Token representation for human Pose\nestimation~(TokenPose). In detail, each keypoint is explicitly embedded as a\ntoken to simultaneously learn constraint relationships and appearance cues from\nimages. Extensive experiments show that the small and large TokenPose models\nare on par with state-of-the-art CNN-based counterparts while being more\nlightweight. Specifically, our TokenPose-S and TokenPose-L achieve $72.5$ AP\nand $75.8$ AP on COCO validation dataset respectively, with significant\nreduction in parameters ($\\downarrow80.6\\%$; $\\downarrow$ $56.8\\%$) and GFLOPs\n($\\downarrow$ $75.3\\%$; $\\downarrow$ $24.7\\%$). Code is publicly available.",
          "link": "http://arxiv.org/abs/2104.03516",
          "publishedOn": "2021-08-16T00:47:33.874Z",
          "wordCount": 623,
          "title": "TokenPose: Learning Keypoint Tokens for Human Pose Estimation. (arXiv:2104.03516v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>",
          "description": "Weakly-supervised learning (WSL) has recently triggered substantial interest\nas it mitigates the lack of pixel-wise annotations.\n\nGiven global image labels, WSL methods yield pixel-level predictions\n(segmentations), which enable to interpret class predictions. Despite their\nrecent success, mostly with natural images, such methods can face important\nchallenges when the foreground and background regions have similar visual cues,\nyielding high false-positive rates in segmentations, as is the case in\nchallenging histology images. WSL training is commonly driven by standard\nclassification losses, which implicitly maximize model confidence, and locate\nthe discriminative regions linked to classification decisions. Therefore, they\nlack mechanisms for modeling explicitly non-discriminative regions and reducing\nfalse-positive rates. We propose novel regularization terms, which enable the\nmodel to seek both non-discriminative and discriminative regions, while\ndiscouraging unbalanced segmentations. We introduce high uncertainty as a\ncriterion to localize non-discriminative regions that do not affect classifier\ndecision, and describe it with original Kullback-Leibler (KL) divergence losses\nevaluating the deviation of posterior predictions from the uniform\ndistribution. Our KL terms encourage high uncertainty of the model when the\nlatter inputs the latent non-discriminative regions. Our loss integrates: (i) a\ncross-entropy seeking a foreground, where model confidence about class\nprediction is high; (ii) a KL regularizer seeking a background, where model\nuncertainty is high; and (iii) log-barrier terms discouraging unbalanced\nsegmentations. Comprehensive experiments and ablation studies over the public\nGlaS colon cancer data and a Camelyon16 patch-based benchmark for breast cancer\nshow substantial improvements over state-of-the-art WSL methods, and confirm\nthe effect of our new regularizers.",
          "link": "http://arxiv.org/abs/2011.07221",
          "publishedOn": "2021-08-16T00:47:33.869Z",
          "wordCount": 741,
          "title": "Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty. (arXiv:2011.07221v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06757",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kamran_S/0/1/0/all/0/1\">Sharif Amit Kamran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_K/0/1/0/all/0/1\">Khondker Fariha Hossain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tavakkoli_A/0/1/0/all/0/1\">Alireza Tavakkoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuckerbrod_S/0/1/0/all/0/1\">Stewart Lee Zuckerbrod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baker_S/0/1/0/all/0/1\">Salah A. Baker</a>",
          "description": "In Fluorescein Angiography (FA), an exogenous dye is injected in the\nbloodstream to image the vascular structure of the retina. The injected dye can\ncause adverse reactions such as nausea, vomiting, anaphylactic shock, and even\ndeath. In contrast, color fundus imaging is a non-invasive technique used for\nphotographing the retina but does not have sufficient fidelity for capturing\nits vascular structure. The only non-invasive method for capturing retinal\nvasculature is optical coherence tomography-angiography (OCTA). However, OCTA\nequipment is quite expensive, and stable imaging is limited to small areas on\nthe retina. In this paper, we propose a novel conditional generative\nadversarial network (GAN) capable of simultaneously synthesizing FA images from\nfundus photographs while predicting retinal degeneration. The proposed system\nhas the benefit of addressing the problem of imaging retinal vasculature in a\nnon-invasive manner as well as predicting the existence of retinal\nabnormalities. We use a semi-supervised approach to train our GAN using\nmultiple weighted losses on different modalities of data. Our experiments\nvalidate that the proposed architecture exceeds recent state-of-the-art\ngenerative networks for fundus-to-angiography synthesis. Moreover, our vision\ntransformer-based discriminators generalize quite well on out-of-distribution\ndata sets for retinal disease prediction.",
          "link": "http://arxiv.org/abs/2104.06757",
          "publishedOn": "2021-08-16T00:47:33.863Z",
          "wordCount": 688,
          "title": "VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers. (arXiv:2104.06757v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yiting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>",
          "description": "Domain adaptation for semantic segmentation enables to alleviate the need for\nlarge-scale pixel-wise annotations. Recently, self-supervised learning (SSL)\nwith a combination of image-to-image translation shows great effectiveness in\nadaptive segmentation. The most common practice is to perform SSL along with\nimage translation to well align a single domain (the source or target).\nHowever, in this single-domain paradigm, unavoidable visual inconsistency\nraised by image translation may affect subsequent learning. In this paper,\nbased on the observation that domain adaptation frameworks performed in the\nsource and target domain are almost complementary in terms of image translation\nand SSL, we propose a novel dual path learning (DPL) framework to alleviate\nvisual inconsistency. Concretely, DPL contains two complementary and\ninteractive single-domain adaptation pipelines aligned in source and target\ndomain respectively. The inference of DPL is extremely simple, only one\nsegmentation model in the target domain is employed. Novel technologies such as\ndual path image translation and dual path adaptive segmentation are proposed to\nmake two paths promote each other in an interactive manner. Experiments on\nGTA5$\\rightarrow$Cityscapes and SYNTHIA$\\rightarrow$Cityscapes scenarios\ndemonstrate the superiority of our DPL model over the state-of-the-art methods.\nThe code and models are available at: \\url{https://github.com/royee182/DPL}",
          "link": "http://arxiv.org/abs/2108.06337",
          "publishedOn": "2021-08-16T00:47:33.858Z",
          "wordCount": 643,
          "title": "Dual Path Learning for Domain Adaptation of Semantic Segmentation. (arXiv:2108.06337v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kucherenko_T/0/1/0/all/0/1\">Taras Kucherenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_R/0/1/0/all/0/1\">Rajmund Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonell_P/0/1/0/all/0/1\">Patrik Jonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_M/0/1/0/all/0/1\">Michael Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>",
          "description": "We propose a new framework for gesture generation, aiming to allow\ndata-driven approaches to produce more semantically rich gestures. Our approach\nfirst predicts whether to gesture, followed by a prediction of the gesture\nproperties. Those properties are then used as conditioning for a modern\nprobabilistic gesture-generation model capable of high-quality output. This\nempowers the approach to generate gestures that are both diverse and\nrepresentational. Follow-ups and more information can be found on the project\npage: https://svito-zar.github.io/speech2properties2gestures/ .",
          "link": "http://arxiv.org/abs/2106.14736",
          "publishedOn": "2021-08-16T00:47:33.852Z",
          "wordCount": 587,
          "title": "Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for Generating Representational Gestures from Speech. (arXiv:2106.14736v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Benefiting from large-scale pre-training, we have witnessed significant\nperformance boost on the popular Visual Question Answering (VQA) task. Despite\nrapid progress, it remains unclear whether these state-of-the-art (SOTA) models\nare robust when encountering examples in the wild. To study this, we introduce\nAdversarial VQA, a new large-scale VQA benchmark, collected iteratively via an\nadversarial human-and-model-in-the-loop procedure. Through this new benchmark,\nwe discover several interesting findings. (i) Surprisingly, we find that during\ndataset collection, non-expert annotators can easily attack SOTA VQA models\nsuccessfully. (ii) Both large-scale pre-trained models and adversarial training\nmethods achieve far worse performance on the new benchmark than over standard\nVQA v2 dataset, revealing the fragility of these models while demonstrating the\neffectiveness of our adversarial dataset. (iii) When used for data\naugmentation, our dataset can effectively boost model performance on other\nrobust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light\non robustness study in the community and serve as a valuable benchmark for\nfuture work.",
          "link": "http://arxiv.org/abs/2106.00245",
          "publishedOn": "2021-08-16T00:47:33.836Z",
          "wordCount": 649,
          "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models. (arXiv:2106.00245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Celong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>",
          "description": "In this paper, we present an efficient and robust deep learning solution for\nnovel view synthesis of complex scenes. In our approach, a 3D scene is\nrepresented as a light field, i.e., a set of rays, each of which has a\ncorresponding color when reaching the image plane. For efficient novel view\nrendering, we adopt a 4D parameterization of the light field, where each ray is\ncharacterized by a 4D parameter. We then formulate the light field as a 4D\nfunction that maps 4D coordinates to corresponding color values. We train a\ndeep fully connected network to optimize this implicit function and memorize\nthe 3D scene. Then, the scene-specific model is used to synthesize novel views.\nDifferent from previous light field approaches which require dense view\nsampling to reliably render novel views, our method can render novel views by\nsampling rays and querying the color for each ray from the network directly,\nthus enabling high-quality light field rendering with a sparser set of training\nimages. Our method achieves state-of-the-art novel view synthesis results while\nmaintaining an interactive frame rate.",
          "link": "http://arxiv.org/abs/2105.07112",
          "publishedOn": "2021-08-16T00:47:33.830Z",
          "wordCount": 722,
          "title": "NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. (arXiv:2105.07112v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antoniadis_P/0/1/0/all/0/1\">Panagiotis Antoniadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_I/0/1/0/all/0/1\">Ioannis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filntisis_P/0/1/0/all/0/1\">Panagiotis P. Filntisis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>",
          "description": "In this work we tackle the task of video-based audio-visual emotion\nrecognition, within the premises of the 2nd Workshop and Competition on\nAffective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions,\nhead/body orientation and low image resolution constitute factors that can\npotentially hinder performance in case of methodologies that solely rely on the\nextraction and analysis of facial features. In order to alleviate this problem,\nwe leverage both bodily and contextual features, as part of a broader emotion\nrecognition framework. We choose to use a standard CNN-RNN cascade as the\nbackbone of our proposed model for sequence-to-sequence (seq2seq) learning.\nApart from learning through the RGB input modality, we construct an aural\nstream which operates on sequences of extracted mel-spectrograms. Our extensive\nexperiments on the challenging and newly assembled Aff-Wild2 dataset verify the\nvalidity of our intuitive multi-stream and multi-modal approach towards emotion\nrecognition in-the-wild. Emphasis is being laid on the the beneficial influence\nof the human body and scene context, as aspects of the emotion recognition\nprocess that have been left relatively unexplored up to this point. All the\ncode was implemented using PyTorch and is publicly available.",
          "link": "http://arxiv.org/abs/2107.03465",
          "publishedOn": "2021-08-16T00:47:33.823Z",
          "wordCount": 688,
          "title": "An audiovisual and contextual approach for categorical and continuous emotion recognition in-the-wild. (arXiv:2107.03465v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Monocular 3D object detection is an important task for autonomous driving\nconsidering its advantage of low cost. It is much more challenging than\nconventional 2D cases due to its inherent ill-posed property, which is mainly\nreflected in the lack of depth information. Recent progress on 2D detection\noffers opportunities to better solving this problem. However, it is non-trivial\nto make a general adapted 2D detector work in this 3D task. In this paper, we\nstudy this problem with a practice built on a fully convolutional single-stage\ndetector and propose a general framework FCOS3D. Specifically, we first\ntransform the commonly defined 7-DoF 3D targets to the image domain and\ndecouple them as 2D and 3D attributes. Then the objects are distributed to\ndifferent feature levels with consideration of their 2D scales and assigned\nonly according to the projected 3D-center for the training procedure.\nFurthermore, the center-ness is redefined with a 2D Gaussian distribution based\non the 3D-center to fit the 3D target formulation. All of these make this\nframework simple yet effective, getting rid of any 2D detection or 2D-3D\ncorrespondence priors. Our solution achieves 1st place out of all the\nvision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020.\nCode and models are released at https://github.com/open-mmlab/mmdetection3d.",
          "link": "http://arxiv.org/abs/2104.10956",
          "publishedOn": "2021-08-16T00:47:33.818Z",
          "wordCount": 712,
          "title": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection. (arXiv:2104.10956v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.06519",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1\">Ganna Platonova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1\">Dalibor Stys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1\">Pavel Soucek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1\">Kirill Lonhus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1\">Jan Valenta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1\">Renata Rychtarikova</a>",
          "description": "The most realistic information about the transparent sample such as a live\ncell can be obtained only using bright-field light microscopy. At\nhigh-intensity pulsing LED illumination, we captured a primary\n12-bit-per-channel (bpc) response from an observed sample using a bright-field\nmicroscope equipped with a high-resolution (4872x3248) image sensor. In order\nto suppress data distortions originating from the light interactions with\nelements in the optical path, poor sensor reproduction (geometrical defects of\nthe camera sensor and some peculiarities of sensor sensitivity), we propose a\nspectroscopic approach for the correction of this uncompressed 12-bpc data by\nsimultaneous calibration of all parts of the experimental arrangement.\nMoreover, the final intensities of the corrected images are proportional to the\nphoton fluxes detected by a camera sensor. It can be visualized in 8-bpc\nintensity depth after the Least Information Loss compression.",
          "link": "http://arxiv.org/abs/1903.06519",
          "publishedOn": "2021-08-16T00:47:33.811Z",
          "wordCount": 652,
          "title": "Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao-Jung Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krylov_V/0/1/0/all/0/1\">Vladimir Krylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahyot_R/0/1/0/all/0/1\">Rozenn Dahyot</a>",
          "description": "In this paper we propose an approach to perform semantic segmentation of 3D\npoint cloud data by importing the geographic information from a 2D GIS layer\n(OpenStreetMap). The proposed automatic procedure identifies meaningful units\nsuch as buildings and adjusts their locations to achieve best fit between the\nGIS polygonal perimeters and the point cloud. Our processing pipeline is\npresented and illustrated by segmenting point cloud data of Trinity College\nDublin (Ireland) campus constructed from optical imagery collected by a drone.",
          "link": "http://arxiv.org/abs/2108.06306",
          "publishedOn": "2021-08-16T00:47:33.787Z",
          "wordCount": 516,
          "title": "3D point cloud segmentation using GIS. (arXiv:2108.06306v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Termritthikun_C/0/1/0/all/0/1\">Chakkrit Termritthikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamtsho_Y/0/1/0/all/0/1\">Yeshi Jamtsho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ieamsaard_J/0/1/0/all/0/1\">Jirarat Ieamsaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muneesawang_P/0/1/0/all/0/1\">Paisarn Muneesawang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Ivan Lee</a>",
          "description": "The goals of this research were to search for Convolutional Neural Network\n(CNN) architectures, suitable for an on-device processor with limited computing\nresources, performing at substantially lower Network Architecture Search (NAS)\ncosts. A new algorithm entitled an Early Exit Population Initialisation (EE-PI)\nfor Evolutionary Algorithm (EA) was developed to achieve both goals. The EE-PI\nreduces the total number of parameters in the search process by filtering the\nmodels with fewer parameters than the maximum threshold. It will look for a new\nmodel to replace those models with parameters more than the threshold. Thereby,\nreducing the number of parameters, memory usage for model storage and\nprocessing time while maintaining the same performance or accuracy. The search\ntime was reduced to 0.52 GPU day. This is a huge and significant achievement\ncompared to the NAS of 4 GPU days achieved using NSGA-Net, 3,150 GPU days by\nthe AmoebaNet model, and the 2,000 GPU days by the NASNet model. As well, Early\nExit Evolutionary Algorithm networks (EEEA-Nets) yield network architectures\nwith minimal error and computational cost suitable for a given dataset as a\nclass of network algorithms. Using EEEA-Net on CIFAR-10, CIFAR-100, and\nImageNet datasets, our experiments showed that EEEA-Net achieved the lowest\nerror rate among state-of-the-art NAS models, with 2.46% for CIFAR-10, 15.02%\nfor CIFAR-100, and 23.8% for ImageNet dataset. Further, we implemented this\nimage recognition architecture for other tasks, such as object detection,\nsemantic segmentation, and keypoint detection tasks, and, in our experiments,\nEEEA-Net-C2 outperformed MobileNet-V3 on all of these various tasks. (The\nalgorithm code is available at https://github.com/chakkritte/EEEA-Net).",
          "link": "http://arxiv.org/abs/2108.06156",
          "publishedOn": "2021-08-16T00:47:33.781Z",
          "wordCount": 754,
          "title": "EEEA-Net: An Early Exit Evolutionary Neural Architecture Search. (arXiv:2108.06156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets, often with better accuracy and fewer\nparameters. Our model eliminates the requirement for class token and positional\nembeddings through a novel sequence pooling strategy and the use of\nconvolution/s. It is flexible in terms of model size, and can have as little as\n0.28M parameters while achieving good results. Our model can reach 98.00%\naccuracy when training from scratch on CIFAR-10, which is a significant\nimprovement over previous Transformer based models. It also outperforms many\nmodern CNN based approaches, such as ResNet, and even some recent NAS-based\napproaches, such as Proxyless-NAS. Our simple and compact design democratizes\ntransformers by making them accessible to those with limited computing\nresources and/or dealing with small datasets. Our method also works on larger\ndatasets, such as ImageNet (82.71% accuracy with 29% parameters of ViT), and\nNLP tasks as well. Our code and pre-trained models are publicly available at\nhttps://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-08-16T00:47:33.774Z",
          "wordCount": 760,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Kenji Suzuki</a>",
          "description": "Affective Behavior Analysis is an important part in human-computer\ninteraction. Existing multi-task affective behavior recognition methods suffer\nfrom the problem of incomplete labeled datasets. To tackle this problem, this\npaper presents a semi-supervised model with a mean teacher framework to\nleverage additional unlabeled data. To be specific, a multi-task model is\nproposed to learn three different kinds of facial affective representations\nsimultaneously. After that, the proposed model is assigned to be student and\nteacher networks. When training with unlabeled data, the teacher network is\nemployed to predict pseudo labels for student network training, which allows it\nto learn from unlabeled data. Experimental results showed that our proposed\nmethod achieved much better performance than baseline model and ranked 4th in\nboth competition track 1 and track 2, and 6th in track 3, which verifies that\nthe proposed network can effectively learn from incomplete datasets.",
          "link": "http://arxiv.org/abs/2107.04225",
          "publishedOn": "2021-08-16T00:47:33.767Z",
          "wordCount": 622,
          "title": "A Multi-task Mean Teacher for Semi-supervised Facial Affective Behavior Analysis. (arXiv:2107.04225v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1\">Hassan Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_W/0/1/0/all/0/1\">Wei-Hong Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>",
          "description": "We present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Specifically, our\nVideo-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts\nmultimodal representations that are rich enough to benefit a variety of\ndownstream tasks. We train VATT end-to-end from scratch using multimodal\ncontrastive losses and evaluate its performance by the downstream tasks of\nvideo action recognition, audio event classification, image classification, and\ntext-to-video retrieval. Furthermore, we study a modality-agnostic\nsingle-backbone Transformer by sharing weights among the three modalities. We\nshow that the convolution-free VATT outperforms state-of-the-art ConvNet-based\narchitectures in the downstream tasks. Especially, VATT's vision Transformer\nachieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and\n41.1% on Moments in Time, new records while avoiding supervised pre-training.\nTransferring to image classification leads to 78.7% top-1 accuracy on ImageNet\ncompared to 64.7% by training the same Transformer from scratch, showing the\ngeneralizability of our model despite the domain gap between videos and images.\nVATT's audio Transformer also sets a new record on waveform-based audio event\nrecognition by achieving the mAP of 39.4% on AudioSet without any supervised\npre-training. VATT's source code is publicly available.",
          "link": "http://arxiv.org/abs/2104.11178",
          "publishedOn": "2021-08-16T00:47:33.756Z",
          "wordCount": 689,
          "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text. (arXiv:2104.11178v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michele_B/0/1/0/all/0/1\">Bj&#xf6;rn Michele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1\">Gilles Puy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>",
          "description": "While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D\nimages, its application to 3D data is still recent and scarce, with just a few\nmethods limited to classification. We present the first generative approach for\nboth ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both\nclassification and, for the first time, semantic segmentation. We show that it\nreaches or outperforms the state of the art on ModelNet40 classification for\nboth inductive ZSL and inductive GZSL. For semantic segmentation, we created\nthree benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and\nSemanticKITTI. Our experiments show that our method outperforms strong\nbaselines, which we additionally propose for this task.",
          "link": "http://arxiv.org/abs/2108.06230",
          "publishedOn": "2021-08-16T00:47:33.739Z",
          "wordCount": 557,
          "title": "Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Cloud. (arXiv:2108.06230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weixiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongtao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirikjian_G/0/1/0/all/0/1\">Gregory Chirikjian</a>",
          "description": "Probabilistic point cloud registration methods are becoming more popular\nbecause of their robustness. However, unlike point-to-plane variants of\niterative closest point (ICP) which incorporate local surface geometric\ninformation such as surface normals, most probabilistic methods (e.g., coherent\npoint drift (CPD)) ignore such information and build Gaussian mixture models\n(GMMs) with isotropic Gaussian covariances. This results in sphere-like GMM\ncomponents which only penalize the point-to-point distance between the two\npoint clouds. In this paper, we propose a novel method called CPD with Local\nSurface Geometry (LSG-CPD) for rigid point cloud registration. Our method\nadaptively adds different levels of point-to-plane penalization on top of the\npoint-to-point penalization based on the flatness of the local surface. This\nresults in GMM components with anisotropic covariances. We formulate point\ncloud registration as a maximum likelihood estimation (MLE) problem and solve\nit with the Expectation-Maximization (EM) algorithm. In the E step, we\ndemonstrate that the computation can be recast into simple matrix manipulations\nand efficiently computed on a GPU. In the M step, we perform an unconstrained\noptimization on a matrix Lie group to efficiently update the rigid\ntransformation of the registration. The proposed method outperforms\nstate-of-the-art algorithms in terms of accuracy and robustness on various\ndatasets captured with range scanners, RGBD cameras, and LiDARs. Also, it is\nsignificantly faster than modern implementations of CPD. The source code is\navailable at https://github.com/ChirikjianLab/LSG-CPD.git.",
          "link": "http://arxiv.org/abs/2103.15039",
          "publishedOn": "2021-08-16T00:47:33.733Z",
          "wordCount": 702,
          "title": "LSG-CPD: Coherent Point Drift with Local Surface Geometry for Point Cloud Registration. (arXiv:2103.15039v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhongyi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haoliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>",
          "description": "Unsupervised domain adaptation (UDA) enables a learning machine to adapt from\na labeled source domain to an unlabeled domain under the distribution shift.\nThanks to the strong representation ability of deep neural networks, recent\nremarkable achievements in UDA resort to learning domain-invariant features.\nIntuitively, the hope is that a good feature representation, together with the\nhypothesis learned from the source domain, can generalize well to the target\ndomain. However, the learning processes of domain-invariant features and source\nhypothesis inevitably involve domain-specific information that would degrade\nthe generalizability of UDA models on the target domain. In this paper,\nmotivated by the lottery ticket hypothesis that only partial parameters are\nessential for generalization, we find that only partial parameters are\nessential for learning domain-invariant information and generalizing well in\nUDA. Such parameters are termed transferable parameters. In contrast, the other\nparameters tend to fit domain-specific details and often fail to generalize,\nwhich we term as untransferable parameters. Driven by this insight, we propose\nTransferable Parameter Learning (TransPar) to reduce the side effect brought by\ndomain-specific information in the learning process and thus enhance the\nmemorization of domain-invariant information. Specifically, according to the\ndistribution discrepancy degree, we divide all parameters into transferable and\nuntransferable ones in each training iteration. We then perform separate\nupdates rules for the two types of parameters. Extensive experiments on image\nclassification and regression tasks (keypoint detection) show that TransPar\noutperforms prior arts by non-trivial margins. Moreover, experiments\ndemonstrate that TransPar can be integrated into the most popular deep UDA\nnetworks and be easily extended to handle any data distribution shift\nscenarios.",
          "link": "http://arxiv.org/abs/2108.06129",
          "publishedOn": "2021-08-16T00:47:33.726Z",
          "wordCount": 703,
          "title": "Learning Transferable Parameters for Unsupervised Domain Adaptation. (arXiv:2108.06129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burov_A/0/1/0/all/0/1\">Andrei Burov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>",
          "description": "We present a novel method for temporal coherent reconstruction and tracking\nof clothed humans. Given a monocular RGB-D sequence, we learn a person-specific\nbody model which is based on a dynamic surface function network. To this end,\nwe explicitly model the surface of the person using a multi-layer perceptron\n(MLP) which is embedded into the canonical space of the SMPL body model. With\nclassical forward rendering, the represented surface can be rasterized using\nthe topology of a template mesh. For each surface point of the template mesh,\nthe MLP is evaluated to predict the actual surface location. To handle\npose-dependent deformations, the MLP is conditioned on the SMPL pose\nparameters. We show that this surface representation as well as the pose\nparameters can be learned in a self-supervised fashion using the principle of\nanalysis-by-synthesis and differentiable rasterization. As a result, we are\nable to reconstruct a temporally coherent mesh sequence from the input data.\nThe underlying surface representation can be used to synthesize new animations\nof the reconstructed person including pose-dependent deformations.",
          "link": "http://arxiv.org/abs/2104.03978",
          "publishedOn": "2021-08-16T00:47:33.719Z",
          "wordCount": 648,
          "title": "Dynamic Surface Function Networks for Clothed Human Bodies. (arXiv:2104.03978v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youpeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grammenos_R/0/1/0/all/0/1\">Ryan Grammenos</a>",
          "description": "The ever-increasing amount of global refuse is overwhelming the waste and\nrecycling management industries. The need for smart systems for environmental\nmonitoring and the enhancement of recycling processes is thus greater than\never. Amongst these efforts lies IBM's Wastenet project which aims to improve\nrecycling by using artificial intelligence for waste classification. The work\nreported in this paper builds on this project through the use of transfer\nlearning and data augmentation techniques to ameliorate classification\naccuracy. Starting with a convolutional neural network (CNN), a systematic\napproach is followed for selecting appropriate splitting ratios and for tuning\nmultiple training parameters including learning rate schedulers, layers\nfreezing, batch sizes and loss functions, in the context of the given scenario\nwhich requires classification of waste into different recycling types. Results\nare compared and contrasted using 10-fold cross validation and demonstrate that\nthe model developed achieves a 91.21% test accuracy. Subsequently, a range of\ndata augmentation techniques are then incorporated into this work including\nflipping, rotation, shearing, zooming, and brightness control. Results show\nthat these augmentation techniques further improve the test accuracy of the\nfinal model to 95.40%. Unlike other work reported in the field, this paper\nprovides full details regarding the training of the model. Furthermore, the\ncode for this work has been made open-source and we have demonstrated that the\nmodel can perform successful real-time classification of recycling waste items\nusing a standard computer webcam.",
          "link": "http://arxiv.org/abs/2108.06274",
          "publishedOn": "2021-08-16T00:47:33.712Z",
          "wordCount": 676,
          "title": "Towards artificially intelligent recycling Improving image processing for waste classification. (arXiv:2108.06274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1909.11855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>",
          "description": "The transformer self-attention network has been extensively used in research\ndomains such as computer vision, image processing, and natural language\nprocessing. The transformer, however, has not been actively used in graph\nneural networks, where constructing an advanced aggregation function is\nessential. To this end, we present an effective model, named UGformer, which --\nby leveraging a transformer self-attention mechanism followed by a recurrent\ntransition -- induces an advanced aggregation function to learn graph\nrepresentations. Experimental results show that UGformer achieves\nstate-of-the-art accuracies on well-known benchmark datasets for graph\nclassification.",
          "link": "http://arxiv.org/abs/1909.11855",
          "publishedOn": "2021-08-16T00:47:33.693Z",
          "wordCount": 643,
          "title": "Universal Graph Transformer Self-Attention Networks. (arXiv:1909.11855v10 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1\">Giuseppe Scarpa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciotola_M/0/1/0/all/0/1\">Matteo Ciotola</a>",
          "description": "A reliable quality assessment procedure for pansharpening methods is of\ncritical importance for the development of the related solutions.\nUnfortunately, the lack of ground-truths to be used as guidance for an\nobjective evaluation has pushed the community to resort to either\nreference-based reduced-resolution indexes or to no-reference subjective\nquality indexes that can be applied on full-resolution datasets. In particular,\nthe reference-based approach leverages on Wald's protocol, a resolution\ndegradation process that allows one to synthesize data with related ground\ntruth. Both solutions, however, present critical shortcomings that we aim to\nmitigate in this work by means of an alternative no-reference full-resolution\nframework. On one side we introduce a protocol, namely the reprojection\nprotocol, which allows to handle the spectral fidelity problem. On the other\nside, a new index of the spatial consistency between the pansharpened image and\nthe panchromatic band at full resolution is proposed. The experimental results\nshow the effectiveness of the proposed approach which is confirmed also by\nvisual inspection.",
          "link": "http://arxiv.org/abs/2108.06144",
          "publishedOn": "2021-08-16T00:47:33.686Z",
          "wordCount": 587,
          "title": "Full-resolution quality assessment for pansharpening. (arXiv:2108.06144v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianzhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yating Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "Geometry Projection is a powerful depth estimation method in monocular 3D\nobject detection. It estimates depth dependent on heights, which introduces\nmathematical priors into the deep model. But projection process also introduces\nthe error amplification problem, in which the error of the estimated height\nwill be amplified and reflected greatly at the output depth. This property\nleads to uncontrollable depth inferences and also damages the training\nefficiency. In this paper, we propose a Geometry Uncertainty Projection Network\n(GUP Net) to tackle the error amplification problem at both inference and\ntraining stages. Specifically, a GUP module is proposed to obtains the\ngeometry-guided uncertainty of the inferred depth, which not only provides high\nreliable confidence for each depth but also benefits depth learning.\nFurthermore, at the training stage, we propose a Hierarchical Task Learning\nstrategy to reduce the instability caused by error amplification. This learning\nalgorithm monitors the learning situation of each task by a proposed indicator\nand adaptively assigns the proper loss weights for different tasks according to\ntheir pre-tasks situation. Based on that, each task starts learning only when\nits pre-tasks are learned well, which can significantly improve the stability\nand efficiency of the training process. Extensive experiments demonstrate the\neffectiveness of the proposed method. The overall model can infer more reliable\nobject depth than existing methods and outperforms the state-of-the-art\nimage-based monocular 3D detectors by 3.74% and 4.7% AP40 of the car and\npedestrian categories on the KITTI benchmark.",
          "link": "http://arxiv.org/abs/2107.13774",
          "publishedOn": "2021-08-16T00:47:33.679Z",
          "wordCount": 708,
          "title": "Geometry Uncertainty Projection Network for Monocular 3D Object Detection. (arXiv:2107.13774v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "GAN inversion aims to invert a given image back into the latent space of a\npretrained GAN model, for the image to be faithfully reconstructed from the\ninverted code by the generator. As an emerging technique to bridge the real and\nfake image domains, GAN inversion plays an essential role in enabling the\npretrained GAN models such as StyleGAN and BigGAN to be used for real image\nediting applications. Meanwhile, GAN inversion also provides insights on the\ninterpretation of GAN's latent space and how the realistic images can be\ngenerated. In this paper, we provide an overview of GAN inversion with a focus\non its recent algorithms and applications. We cover important techniques of GAN\ninversion and their applications to image restoration and image manipulation.\nWe further elaborate on some trends and challenges for future directions.",
          "link": "http://arxiv.org/abs/2101.05278",
          "publishedOn": "2021-08-16T00:47:33.659Z",
          "wordCount": 630,
          "title": "GAN Inversion: A Survey. (arXiv:2101.05278v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shugong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_S/0/1/0/all/0/1\">Shiyi Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yue Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyong Chen</a>",
          "description": "Although recent works based on deep learning have made progress in improving\nrecognition accuracy on scene text recognition, how to handle low-quality text\nimages in end-to-end deep networks remains a research challenge. In this paper,\nwe propose an Iterative Fusion based Recognizer (IFR) for low quality scene\ntext recognition, taking advantage of refined text images input and robust\nfeature representation. IFR contains two branches which focus on scene text\nrecognition and low quality scene text image recovery respectively. We utilize\nan iterative collaboration between two branches, which can effectively\nalleviate the impact of low quality input. A feature fusion module is proposed\nto strengthen the feature representation of the two branches, where the\nfeatures from the Recognizer are Fused with image Restoration branch, referred\nto as RRF. Without changing the recognition network structure, extensive\nquantitative and qualitative experimental results show that the proposed method\nsignificantly outperforms the baseline methods in boosting the recognition\naccuracy of benchmark datasets and low resolution images in TextZoom dataset.",
          "link": "http://arxiv.org/abs/2108.06166",
          "publishedOn": "2021-08-16T00:47:33.653Z",
          "wordCount": 610,
          "title": "IFR: Iterative Fusion Based Recognizer For Low Quality Scene Text Recognition. (arXiv:2108.06166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haomin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">T.Y. Alvin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_C/0/1/0/all/0/1\">Catalina Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_Z/0/1/0/all/0/1\">Zelia Correa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>",
          "description": "Algorithmic decision support is rapidly becoming a staple of personalized\nmedicine, especially for high-stakes recommendations in which access to certain\ninformation can drastically alter the course of treatment, and thus, patient\noutcome; a prominent example is radiomics for cancer subtyping. Because in\nthese scenarios the stakes are high, it is desirable for decision systems to\nnot only provide recommendations but supply transparent reasoning in support\nthereof. For learning-based systems, this can be achieved through an\ninterpretable design of the inference pipeline. Herein we describe an automated\nyet interpretable system for uveal melanoma subtyping with digital cytology\nimages from fine needle aspiration biopsies. Our method embeds every\nautomatically segmented cell of a candidate cytology image as a point in a 2D\nmanifold defined by many representative slides, which enables reasoning about\nthe cell-level composition of the tissue sample, paving the way for\ninterpretable subtyping of the biopsy. Finally, a rule-based slide-level\nclassification algorithm is trained on the partitions of the circularly\ndistorted 2D manifold. This process results in a simple rule set that is\nevaluated automatically but highly transparent for human verification. On our\nin house cytology dataset of 88 uveal melanoma patients, the proposed method\nachieves an accuracy of 87.5% that compares favorably to all competing\napproaches, including deep \"black box\" models. The method comes with a user\ninterface to facilitate interaction with cell-level content, which may offer\nadditional insights for pathological assessment.",
          "link": "http://arxiv.org/abs/2108.06246",
          "publishedOn": "2021-08-16T00:47:33.637Z",
          "wordCount": 691,
          "title": "An Interpretable Algorithm for Uveal Melanoma Subtyping from Whole Slide Cytology Images. (arXiv:2108.06246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Depu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zejia Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>",
          "description": "The recently-developed DETR approach applies the transformer encoder and\ndecoder architecture to object detection and achieves promising performance. In\nthis paper, we handle the critical issue, slow training convergence, and\npresent a conditional cross-attention mechanism for fast DETR training. Our\napproach is motivated by that the cross-attention in DETR relies highly on the\ncontent embeddings for localizing the four extremities and predicting the box,\nwhich increases the need for high-quality content embeddings and thus the\ntraining difficulty. Our approach, named conditional DETR, learns a conditional\nspatial query from the decoder embedding for decoder multi-head\ncross-attention. The benefit is that through the conditional spatial query,\neach cross-attention head is able to attend to a band containing a distinct\nregion, e.g., one object extremity or a region inside the object box. This\nnarrows down the spatial range for localizing the distinct regions for object\nclassification and box regression, thus relaxing the dependence on the content\nembeddings and easing the training. Empirical results show that conditional\nDETR converges 6.7x faster for the backbones R50 and R101 and 10x faster for\nstronger backbones DC5-R50 and DC5-R101. Code is available at\nhttps://git.io/ConditionalDETR.",
          "link": "http://arxiv.org/abs/2108.06152",
          "publishedOn": "2021-08-16T00:47:33.631Z",
          "wordCount": 633,
          "title": "Conditional DETR for Fast Training Convergence. (arXiv:2108.06152v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>",
          "description": "Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, most existing\nlearning-based approaches usually suffer from limited manually annotated\nmedical data, which poses a major practical problem for accurate and robust\nmedical image segmentation. In addition, most existing semi-supervised\napproaches are usually not robust compared with the supervised counterparts,\nand also lack explicit modeling of geometric structure and semantic\ninformation, both of which limit the segmentation accuracy. In this work, we\npresent SimCVD, a simple contrastive distillation framework that significantly\nadvances state-of-the-art voxel-wise representation learning. We first describe\nan unsupervised training strategy, which takes two views of an input volume and\npredicts their signed distance maps of object boundaries in a contrastive\nobjective, with only two independent dropout as mask. This simple approach\nworks surprisingly well, performing on the same level as previous fully\nsupervised methods with much less labeled data. We hypothesize that dropout can\nbe viewed as a minimal form of data augmentation and makes the network robust\nto representation collapse. Then, we propose to perform structural distillation\nby distilling pair-wise similarities. We evaluate SimCVD on two popular\ndatasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\ndataset. The results on the LA dataset demonstrate that, in two types of\nlabeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\nprevious best results. Our method can be trained in an end-to-end fashion,\nshowing the promise of utilizing SimCVD as a general framework for downstream\ntasks, such as medical image synthesis and registration.",
          "link": "http://arxiv.org/abs/2108.06227",
          "publishedOn": "2021-08-16T00:47:33.612Z",
          "wordCount": 724,
          "title": "SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tailor_S/0/1/0/all/0/1\">Shyam A. Tailor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_R/0/1/0/all/0/1\">Ren&#xe9; de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azevedo_T/0/1/0/all/0/1\">Tiago Azevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1\">Matthew Mattina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_P/0/1/0/all/0/1\">Partha Maji</a>",
          "description": "In recent years graph neural network (GNN)-based approaches have become a\npopular strategy for processing point cloud data, regularly achieving\nstate-of-the-art performance on a variety of tasks. To date, the research\ncommunity has primarily focused on improving model expressiveness, with\nsecondary thought given to how to design models that can run efficiently on\nresource constrained mobile devices including smartphones or mixed reality\nheadsets. In this work we make a step towards improving the efficiency of these\nmodels by making the observation that these GNN models are heavily limited by\nthe representational power of their first, feature extracting, layer. We find\nthat it is possible to radically simplify these models so long as the feature\nextraction layer is retained with minimal degradation to model performance;\nfurther, we discover that it is possible to improve performance overall on\nModelNet40 and S3DIS by improving the design of the feature extractor. Our\napproach reduces memory consumption by 20$\\times$ and latency by up to\n9.9$\\times$ for graph layers in models such as DGCNN; overall, we achieve\nspeed-ups of up to 4.5$\\times$ and peak memory reductions of 72.5%.",
          "link": "http://arxiv.org/abs/2108.06317",
          "publishedOn": "2021-08-16T00:47:33.570Z",
          "wordCount": 645,
          "title": "Towards Efficient Point Cloud Graph Neural Networks Through Architectural Simplification. (arXiv:2108.06317v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08871",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Haan_K/0/1/0/all/0/1\">Kevin de Haan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yijie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuckerman_J/0/1/0/all/0/1\">Jonathan E. Zuckerman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisk_A/0/1/0/all/0/1\">Anthony E. Sisk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_M/0/1/0/all/0/1\">Miguel F. P. Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jen_K/0/1/0/all/0/1\">Kuang-Yu Jen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nobori_A/0/1/0/all/0/1\">Alexander Nobori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liou_S/0/1/0/all/0/1\">Sofia Liou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Sarah Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riahi_R/0/1/0/all/0/1\">Rana Riahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivenson_Y/0/1/0/all/0/1\">Yair Rivenson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wallace_W/0/1/0/all/0/1\">W. Dean Wallace</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>",
          "description": "Pathology is practiced by visual inspection of histochemically stained\nslides. Most commonly, the hematoxylin and eosin (H&E) stain is used in the\ndiagnostic workflow and it is the gold standard for cancer diagnosis. However,\nin many cases, especially for non-neoplastic diseases, additional \"special\nstains\" are used to provide different levels of contrast and color to tissue\ncomponents and allow pathologists to get a clearer diagnostic picture. In this\nstudy, we demonstrate the utility of supervised learning-based computational\nstain transformation from H&E to different special stains (Masson's Trichrome,\nperiodic acid-Schiff and Jones silver stain) using tissue sections from kidney\nneedle core biopsies. Based on evaluation by three renal pathologists, followed\nby adjudication by a fourth renal pathologist, we show that the generation of\nvirtual special stains from existing H&E images improves the diagnosis in\nseveral non-neoplastic kidney diseases sampled from 58 unique subjects. A\nsecond study performed by three pathologists found that the quality of the\nspecial stains generated by the stain transformation network was statistically\nequivalent to those generated through standard histochemical staining. As the\ntransformation of H&E images into special stains can be achieved within 1 min\nor less per patient core specimen slide, this stain-to-stain transformation\nframework can improve the quality of the preliminary diagnosis when additional\nspecial stains are needed, along with significant savings in time and cost,\nreducing the burden on healthcare system and patients.",
          "link": "http://arxiv.org/abs/2008.08871",
          "publishedOn": "2021-08-16T00:47:33.564Z",
          "wordCount": 763,
          "title": "Deep learning-based transformation of the H&E stain into special stains. (arXiv:2008.08871v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nesti_F/0/1/0/all/0/1\">Federico Nesti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossolini_G/0/1/0/all/0/1\">Giulio Rossolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Saasha Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_A/0/1/0/all/0/1\">Alessandro Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttazzo_G/0/1/0/all/0/1\">Giorgio Buttazzo</a>",
          "description": "Deep learning and convolutional neural networks allow achieving impressive\nperformance in computer vision tasks, such as object detection and semantic\nsegmentation (SS). However, recent studies have shown evident weaknesses of\nsuch models against adversarial perturbations. In a real-world scenario\ninstead, like autonomous driving, more attention should be devoted to\nreal-world adversarial examples (RWAEs), which are physical objects (e.g.,\nbillboards and printable patches) optimized to be adversarial to the entire\nperception pipeline. This paper presents an in-depth evaluation of the\nrobustness of popular SS models by testing the effects of both digital and\nreal-world adversarial patches. These patches are crafted with powerful attacks\nenriched with a novel loss function. Firstly, an investigation on the\nCityscapes dataset is conducted by extending the Expectation Over\nTransformation (EOT) paradigm to cope with SS. Then, a novel attack\noptimization, called scene-specific attack, is proposed. Such an attack\nleverages the CARLA driving simulator to improve the transferability of the\nproposed EOT-based attack to a real 3D environment. Finally, a printed physical\nbillboard containing an adversarial patch was tested in an outdoor driving\nscenario to assess the feasibility of the studied attacks in the real world.\nExhaustive experiments revealed that the proposed attack formulations\noutperform previous work to craft both digital and real-world adversarial\npatches for SS. At the same time, the experimental results showed how these\nattacks are notably less effective in the real world, hence questioning the\npractical relevance of adversarial attacks to SS models for autonomous/assisted\ndriving.",
          "link": "http://arxiv.org/abs/2108.06179",
          "publishedOn": "2021-08-16T00:47:33.549Z",
          "wordCount": 692,
          "title": "Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks. (arXiv:2108.06179v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.11582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_P/0/1/0/all/0/1\">Panhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xuejing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Lizhu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_A/0/1/0/all/0/1\">Anlong Ming</a>",
          "description": "Occlusion relationship reasoning based on convolution neural networks\nconsists of two subtasks: occlusion boundary extraction and occlusion\norientation inference. Due to the essential differences between the two\nsubtasks in the feature expression at the higher and lower stages, it is\nchallenging to carry on them simultaneously in one network. To address this\nissue, we propose a novel Dual-path Decoder Network, which uniformly extracts\nocclusion information at higher stages and separates into two paths to recover\nboundary and occlusion orientation respectively in lower stages. Besides,\nconsidering the restriction of occlusion orientation presentation to occlusion\norientation learning, we design a new orthogonal representation for occlusion\norientation and proposed the Orthogonal Orientation Regression loss which can\nget rid of the unfitness between occlusion representation and learning and\nfurther prompt the occlusion orientation learning. Finally, we apply a\nmulti-scale loss together with our proposed orientation regression loss to\nguide the boundary and orientation path learning respectively. Experiments\ndemonstrate that our proposed method achieves state-of-the-art results on PIOD\nand BSDS ownership datasets.",
          "link": "http://arxiv.org/abs/1911.11582",
          "publishedOn": "2021-08-16T00:47:33.527Z",
          "wordCount": 655,
          "title": "DDNet: Dual-path Decoder Network for Occlusion Relationship Reasoning. (arXiv:1911.11582v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>",
          "description": "Vision-language Navigation (VLN) tasks require an agent to navigate\nstep-by-step while perceiving the visual observations and comprehending a\nnatural language instruction. Large data bias, which is caused by the disparity\nratio between the small data scale and large navigation space, makes the VLN\ntask challenging. Previous works have proposed various data augmentation\nmethods to reduce data bias. However, these works do not explicitly reduce the\ndata bias across different house scenes. Therefore, the agent would overfit to\nthe seen scenes and achieve poor navigation performance in the unseen scenes.\nTo tackle this problem, we propose the Random Environmental Mixup (REM) method,\nwhich generates cross-connected house scenes as augmented data via mixuping\nenvironment. Specifically, we first select key viewpoints according to the room\nconnection graph for each scene. Then, we cross-connect the key views of\ndifferent scenes to construct augmented scenes. Finally, we generate augmented\ninstruction-path pairs in the cross-connected scenes. The experimental results\non benchmark datasets demonstrate that our augmentation data via REM help the\nagent reduce its performance gap between the seen and unseen environment and\nimprove the overall performance, making our model the best existing approach on\nthe standard VLN benchmark.",
          "link": "http://arxiv.org/abs/2106.07876",
          "publishedOn": "2021-08-16T00:47:33.521Z",
          "wordCount": 661,
          "title": "Vision-Language Navigation with Random Environmental Mixup. (arXiv:2106.07876v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anthony Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murez_Z/0/1/0/all/0/1\">Zak Murez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_N/0/1/0/all/0/1\">Nikhil Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudas_S/0/1/0/all/0/1\">Sof&#xed;a Dudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawke_J/0/1/0/all/0/1\">Jeffrey Hawke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badrinarayanan_V/0/1/0/all/0/1\">Vijay Badrinarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kendall_A/0/1/0/all/0/1\">Alex Kendall</a>",
          "description": "Driving requires interacting with road agents and predicting their future\nbehaviour in order to navigate safely. We present FIERY: a probabilistic future\nprediction model in bird's-eye view from monocular cameras. Our model predicts\nfuture instance segmentation and motion of dynamic agents that can be\ntransformed into non-parametric future trajectories. Our approach combines the\nperception, sensor fusion and prediction components of a traditional autonomous\ndriving stack by estimating bird's-eye-view prediction directly from surround\nRGB monocular camera inputs. FIERY learns to model the inherent stochastic\nnature of the future solely from camera driving data in an end-to-end manner,\nwithout relying on HD maps, and predicts multimodal future trajectories. We\nshow that our model outperforms previous prediction baselines on the NuScenes\nand Lyft datasets. The code and trained models are available at\nhttps://github.com/wayveai/fiery.",
          "link": "http://arxiv.org/abs/2104.10490",
          "publishedOn": "2021-08-16T00:47:33.516Z",
          "wordCount": 618,
          "title": "FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras. (arXiv:2104.10490v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drees_D/0/1/0/all/0/1\">Dominik Drees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eilers_F/0/1/0/all/0/1\">Florian Eilers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyi Jiang</a>",
          "description": "The random walker method for image segmentation is a popular tool for\nsemi-automatic image segmentation, especially in the biomedical field. However,\nits linear asymptotic run time and memory requirements make application to 3D\ndatasets of increasing sizes impractical. We propose a hierarchical framework\nthat, to the best of our knowledge, is the first attempt to overcome these\nrestrictions for the random walker algorithm and achieves sublinear run time\nand constant memory complexity. The goal of this framework is -- rather than\nimproving the segmentation quality compared to the baseline method -- to make\ninteractive segmentation on out-of-core datasets possible. The method is\nevaluated quantitavely on synthetic data and the CT-ORG dataset where the\nexpected improvements in algorithm run time while maintaining high segmentation\nquality are confirmed. The incremental (i.e., interaction update) run time is\ndemonstrated to be in seconds on a standard PC even for volumes of hundreds of\nGigabytes in size. In a small case study the applicability to large real world\nfrom current biomedical research is demonstrated. An implementation of the\npresented method is publicly available in version 5.2 of the widely used volume\nrendering and processing software Voreen (https://www.uni-muenster.de/Voreen/).",
          "link": "http://arxiv.org/abs/2103.09564",
          "publishedOn": "2021-08-16T00:47:33.510Z",
          "wordCount": 657,
          "title": "Hierarchical Random Walker Segmentation for Large Volumetric Biomedical Images. (arXiv:2103.09564v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenpeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuemiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "Existing GAN inversion methods are stuck in a paradox that the inverted codes\ncan either achieve high-fidelity reconstruction, or retain the editing\ncapability. Having only one of them clearly cannot realize real image editing.\nIn this paper, we resolve this paradox by introducing consecutive images (\\eg,\nvideo frames or the same person with different poses) into the inversion\nprocess. The rationale behind our solution is that the continuity of\nconsecutive images leads to inherent editable directions. This inborn property\nis used for two unique purposes: 1) regularizing the joint inversion process,\nsuch that each of the inverted code is semantically accessible from one of the\nother and fastened in a editable domain; 2) enforcing inter-image coherence,\nsuch that the fidelity of each inverted code can be maximized with the\ncomplement of other images. Extensive experiments demonstrate that our\nalternative significantly outperforms state-of-the-art methods in terms of\nreconstruction fidelity and editability on both the real image dataset and\nsynthesis dataset. Furthermore, our method provides the first support of\nvideo-based GAN inversion, and an interesting application of unsupervised\nsemantic transfer from consecutive images. Source code can be found at:\n\\url{https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs}.",
          "link": "http://arxiv.org/abs/2107.13812",
          "publishedOn": "2021-08-16T00:47:33.505Z",
          "wordCount": 662,
          "title": "From Continuity to Editability: Inverting GANs with Consecutive Images. (arXiv:2107.13812v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quang Huy Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_J/0/1/0/all/0/1\">Jae Kyu Suhr</a>",
          "description": "Autonomous parking systems start with the detection of available parking\nslots. Parking slot detection performance has been dramatically improved by\ndeep learning techniques. Deep learning-based object detection methods can be\ncategorized into one-stage and two-stage approaches. Although it is well-known\nthat the two-stage approach outperforms the one-stage approach in general\nobject detection, they have performed similarly in parking slot detection so\nfar. We consider this is because the two-stage approach has not yet been\nadequately specialized for parking slot detection. Thus, this paper proposes a\nhighly specialized two-stage parking slot detector that uses region-specific\nmulti-scale feature extraction. In the first stage, the proposed method finds\nthe entrance of the parking slot as a region proposal by estimating its center,\nlength, and orientation. The second stage of this method designates specific\nregions that most contain the desired information and extracts features from\nthem. That is, features for the location and orientation are separately\nextracted from only the specific regions that most contain the locational and\norientational information. In addition, multi-resolution feature maps are\nutilized to increase both positioning and classification accuracies. A\nhigh-resolution feature map is used to extract detailed information (location\nand orientation), while another low-resolution feature map is used to extract\nsemantic information (type and occupancy). In experiments, the proposed method\nwas quantitatively evaluated with two large-scale public parking slot detection\ndatasets and outperformed previous methods, including both one-stage and\ntwo-stage approaches.",
          "link": "http://arxiv.org/abs/2108.06185",
          "publishedOn": "2021-08-16T00:47:33.481Z",
          "wordCount": 671,
          "title": "CNN-based Two-Stage Parking Slot Detection Using Region-Specific Multi-Scale Feature Extraction. (arXiv:2108.06185v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1\">Junjie Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>",
          "description": "Image quality assessment (IQA) is an important research topic for\nunderstanding and improving visual experience. The current state-of-the-art IQA\nmethods are based on convolutional neural networks (CNNs). The performance of\nCNN-based models is often compromised by the fixed shape constraint in batch\ntraining. To accommodate this, the input images are usually resized and cropped\nto a fixed shape, causing image quality degradation. To address this, we design\na multi-scale image quality Transformer (MUSIQ) to process native resolution\nimages with varying sizes and aspect ratios. With a multi-scale image\nrepresentation, our proposed method can capture image quality at different\ngranularities. Furthermore, a novel hash-based 2D spatial embedding and a scale\nembedding is proposed to support the positional embedding in the multi-scale\nrepresentation. Experimental results verify that our method can achieve\nstate-of-the-art performance on multiple large scale IQA datasets such as\nPaQ-2-PiQ, SPAQ and KonIQ-10k.",
          "link": "http://arxiv.org/abs/2108.05997",
          "publishedOn": "2021-08-16T00:47:33.453Z",
          "wordCount": 578,
          "title": "MUSIQ: Multi-scale Image Quality Transformer. (arXiv:2108.05997v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1\">Tianrui Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1\">Divya Kothandaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyamoorthy_A/0/1/0/all/0/1\">Adarsh Jagan Sathyamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a new learning-based method for identifying safe and navigable\nregions in off-road terrains and unstructured environments from RGB images. Our\napproach consists of classifying groups of terrains based on their navigability\nlevels using coarse-grained semantic segmentation. We propose a bottleneck\ntransformer-based deep neural network architecture that uses a novel group-wise\nattention mechanism to distinguish between navigability levels of different\nterrains. Our group-wise attention heads enable the network to explicitly focus\non the different groups and improve the accuracy. We show through extensive\nevaluations on the RUGD and RELLIS-3D datasets that our learning algorithm\nimproves visual perception accuracy in off-road terrains for navigation. We\ncompare our approach with prior work on these datasets and achieve an\nimprovement over the state-of-the-art mIoU by 6.74-39.1% on RUGD and\n3.82-10.64% on RELLIS-3D. In addition, we deploy our method on a Clearpath\nJackal robot. Our approach improves the performance of the navigation algorithm\nin terms of average progress towards the goal by 54.73% and the false positives\nin terms of forbidden region by 29.96%.",
          "link": "http://arxiv.org/abs/2103.04233",
          "publishedOn": "2021-08-16T00:47:33.447Z",
          "wordCount": 644,
          "title": "GANav: Group-wise Attention Network for Classifying Navigable Regions in Unstructured Outdoor Environments. (arXiv:2103.04233v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1\">Hrishav Bakul Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mg_T/0/1/0/all/0/1\">Theint Haythi Mg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_P/0/1/0/all/0/1\">Pradip Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_C/0/1/0/all/0/1\">Chayan Sarkar</a>",
          "description": "Robots in our daily surroundings are increasing day by day. Their usability\nand acceptability largely depend on their explicit and implicit interaction\ncapability with fellow human beings. As a result, social behavior is one of the\nmost sought-after qualities that a robot can possess. However, there is no\nspecific aspect and/or feature that defines socially acceptable behavior and it\nlargely depends on the situation, application, and society. In this article, we\ninvestigate one such social behavior for collocated robots. Imagine a group of\npeople is interacting with each other and we want to join the group. We as\nhuman beings do it in a socially acceptable manner, i.e., within the group, we\ndo position ourselves in such a way that we can participate in the group\nactivity without disturbing/obstructing anybody. To possess such a quality,\nfirst, a robot needs to determine the formation of the group and then determine\na position for itself, which we humans do implicitly. The theory of f-formation\ncan be utilized for this purpose. As the types of formations can be very\ndiverse, detecting the social groups is not a trivial task. In this article, we\nprovide a comprehensive survey of the existing work on social interaction and\ngroup detection using f-formation for robotics and other applications. We also\nput forward a novel holistic survey framework combining all the possible\nconcerns and modules relevant to this problem. We define taxonomies based on\nmethods, camera views, datasets, detection capabilities and scale, evaluation\napproaches, and application areas. We discuss certain open challenges and\nlimitations in current literature along with possible future research\ndirections based on this framework. In particular, we discuss the existing\nmethods/techniques and their relative merits and demerits, applications, and\nprovide a set of unsolved but relevant problems in this domain.",
          "link": "http://arxiv.org/abs/2108.06181",
          "publishedOn": "2021-08-16T00:47:33.429Z",
          "wordCount": 790,
          "title": "Detecting socially interacting groups using f-formation: A survey of taxonomy, methods, datasets, applications, challenges, and future research directions. (arXiv:2108.06181v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohaimenuzzaman_M/0/1/0/all/0/1\">Md Mohaimenuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1\">Christoph Bergmeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_B/0/1/0/all/0/1\">Bernd Meyer</a>",
          "description": "Deep Learning has celebrated resounding successes in many application areas\nof relevance to the Internet-of-Things, for example, computer vision and\nmachine listening. To fully harness the power of deep leaning for the IoT,\nthese technologies must ultimately be brought directly to the edge. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization and the recent advancement of XNOR-Net. This paper examines the\nsuitability of these techniques for audio classification in microcontrollers.\nWe present an XNOR-Net for end-to-end raw audio classification and a\ncomprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring about 8x more computation. We\nshow that these insights are consistent between raw audio classification and\nimage classification using standard benchmark sets.To the best of our\nknowledge, this is the first study applying XNOR to end-to-end audio\nclassification and evaluating it in the context of alternative techniques. All\ncode is publicly available on GitHub.",
          "link": "http://arxiv.org/abs/2108.06128",
          "publishedOn": "2021-08-16T00:47:33.404Z",
          "wordCount": 682,
          "title": "Pruning vs XNOR-Net: A Comprehensive Study on Deep Learning for Audio Classification in Microcontrollers. (arXiv:2108.06128v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taher_M/0/1/0/all/0/1\">Mohammad Reza Hosseinzadeh Taher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghighi_F/0/1/0/all/0/1\">Fatemeh Haghighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruibin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotway_M/0/1/0/all/0/1\">Michael B. Gotway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jianming Liang</a>",
          "description": "Transfer learning from supervised ImageNet models has been frequently used in\nmedical image analysis. Yet, no large-scale evaluation has been conducted to\nbenchmark the efficacy of newly-developed pre-training techniques for medical\nimage analysis, leaving several important questions unanswered. As the first\nstep in this direction, we conduct a systematic study on the transferability of\nmodels pre-trained on iNat2021, the most recent large-scale fine-grained\ndataset, and 14 top self-supervised ImageNet models on 7 diverse medical tasks\nin comparison with the supervised ImageNet model. Furthermore, we present a\npractical approach to bridge the domain gap between natural and medical images\nby continually (pre-)training supervised ImageNet models on medical images. Our\ncomprehensive evaluation yields new insights: (1) pre-trained models on\nfine-grained data yield distinctive local representations that are more\nsuitable for medical segmentation tasks, (2) self-supervised ImageNet models\nlearn holistic features more effectively than supervised ImageNet models, and\n(3) continual pre-training can bridge the domain gap between natural and\nmedical images. We hope that this large-scale open evaluation of transfer\nlearning can direct the future research of deep learning for medical imaging.\nAs open science, all codes and pre-trained models are available on our GitHub\npage https://github.com/JLiangLab/BenchmarkTransferLearning.",
          "link": "http://arxiv.org/abs/2108.05930",
          "publishedOn": "2021-08-16T00:47:33.395Z",
          "wordCount": 671,
          "title": "A Systematic Benchmarking Analysis of Transfer Learning for Medical Image Analysis. (arXiv:2108.05930v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huanfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Menghui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenxia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1\">Qiangqiang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In the fields of image restoration and image fusion, model-driven methods and\ndata-driven methods are the two representative frameworks. However, both\napproaches have their respective advantages and disadvantages. The model-driven\nmethods consider the imaging mechanism, which is deterministic and\ntheoretically reasonable; however, they cannot easily model complicated\nnonlinear problems. The data-driven methods have a stronger prior knowledge\nlearning capability for huge data, especially for nonlinear statistical\nfeatures; however, the interpretability of the networks is poor, and they are\nover-dependent on training data. In this paper, we systematically investigate\nthe coupling of model-driven and data-driven methods, which has rarely been\nconsidered in the remote sensing image restoration and fusion communities. We\nare the first to summarize the coupling approaches into the following three\ncategories: 1) data-driven and model-driven cascading methods; 2) variational\nmodels with embedded learning; and 3) model-constrained network learning\nmethods. The typical existing and potential coupling methods for remote sensing\nimage restoration and fusion are introduced with application examples. This\npaper also gives some new insights into the potential future directions, in\nterms of both methods and applications.",
          "link": "http://arxiv.org/abs/2108.06073",
          "publishedOn": "2021-08-16T00:47:33.390Z",
          "wordCount": 626,
          "title": "Coupling Model-Driven and Data-Driven Methods for Remote Sensing Image Restoration and Fusion. (arXiv:2108.06073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hyeon_Woo_N/0/1/0/all/0/1\">Nam Hyeon-Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Bin_M/0/1/0/all/0/1\">Moon Ye-Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>",
          "description": "To overcome the burdens on frequent model uploads and downloads during\nfederated learning (FL), we propose a communication-efficient\nre-parameterization, FedPara. Our method re-parameterizes the model's layers\nusing low-rank matrices or tensors followed by the Hadamard product. Different\nfrom the conventional low-rank parameterization, our method is not limited to\nlow-rank constraints. Thereby, our FedPara has a larger capacity than the\nlow-rank one, even with the same number of parameters. It can achieve\ncomparable performance to the original models while requiring 2.8 to 10.1 times\nlower communication costs than the original models, which is not achievable by\nthe traditional low-rank parameterization. Moreover, the efficiency can be\nfurther improved by combining our method and other efficient FL techniques\nbecause our method is compatible with others. We also extend our method to a\npersonalized FL application, pFedPara, which separates parameters into global\nand local ones. We show that pFedPara outperforms competing personalized FL\nmethods with more than three times fewer parameters.",
          "link": "http://arxiv.org/abs/2108.06098",
          "publishedOn": "2021-08-16T00:47:33.256Z",
          "wordCount": 594,
          "title": "FedPara: Low-rank Hadamard Product Parameterization for Efficient Federated Learning. (arXiv:2108.06098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fengming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xuelei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianhang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiguang Liu</a>",
          "description": "Real-time detection and tracking of fast-moving objects have achieved great\nsuccess in various fields. However, many existing methods, especially low-cost\nones, are difficult to achieve real-time and long-term object detection and\ntracking. Here, a non-imaging strategy is proposed, including two stages, to\nrealize fast-moving object detection and tracking in real-time and for the long\nterm: 1) a contour-moments-based method is proposed to optimize the Hadamard\npattern sequence. And then reconstructing projection curves of the object based\non single-pixel imaging technology. The projection curve, which including the\nobject location information, is reconstructed directly with the measurements\ncollected by a single-pixel detector; 2) The fastest changing position in the\nprojection curve can be obtained by solving first-order gradients. A gradient\ndifferential is used in two first-order gradients to calculate a differential\ncurve with the sudden change positions. Finally, we can obtain the boundary\ninformation of the fast-moving object. We experimentally demonstrate that our\napproach can achieve a temporal resolution of 105 frames per second at a 1.28%\nsampling rate by using a 22,000 Hz digital micro-mirror device. The detection\nand tracking algorithm of the proposed strategy is computationally efficient.\nCompared with the state-of-the-art methods, our approach can make the sampling\nrate lower. Additionally, the strategy acquires not more than 1MB of data for\neach frame, which is capable of fast-moving object real-time and long-term\ndetection and tracking.",
          "link": "http://arxiv.org/abs/2108.06009",
          "publishedOn": "2021-08-16T00:47:33.232Z",
          "wordCount": 667,
          "title": "Non-imaging real-time detection and tracking of fast-moving objects. (arXiv:2108.06009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Haocheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xinyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhao Wu</a>",
          "description": "Due to the sparsity and irregularity of the 3D data, approaches that directly\nprocess points have become popular. Among all point-based models,\nTransformer-based models have achieved state-of-the-art performance by fully\npreserving point interrelation. However, most of them spend high percentage of\ntotal time on sparse data accessing (e.g., Farthest Point Sampling (FPS) and\nneighbor points query), which becomes the computation burden. Therefore, we\npresent a novel 3D Transformer, called Point-Voxel Transformer (PVT) that\nleverages self-attention computation in points to gather global context\nfeatures, while performing multi-head self-attention (MSA) computation in\nvoxels to capture local information and reduce the irregular data access.\nAdditionally, to further reduce the cost of MSA computation, we design a cyclic\nshifted boxing scheme which brings greater efficiency by limiting the MSA\ncomputation to non-overlapping local boxes while also preserving cross-box\nconnection. Our method fully exploits the potentials of Transformer\narchitecture, paving the road to efficient and accurate recognition results.\nEvaluated on classification and segmentation benchmarks, our PVT not only\nachieves strong accuracy but outperforms previous state-of-the-art\nTransformer-based models with 9x measured speedup on average. For 3D object\ndetection task, we replace the primitives in Frustrum PointNet with PVT layer\nand achieve the improvement of 8.6%.",
          "link": "http://arxiv.org/abs/2108.06076",
          "publishedOn": "2021-08-16T00:47:33.225Z",
          "wordCount": 645,
          "title": "Point-Voxel Transformer: An Efficient Approach To 3D Deep Learning. (arXiv:2108.06076v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiaopeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Riquan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Litong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huabin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>",
          "description": "Deep semi-supervised learning (SSL) has experienced significant attention in\nrecent years, to leverage a huge amount of unlabeled data to improve the\nperformance of deep learning with limited labeled data. Pseudo-labeling is a\npopular approach to expand the labeled dataset. However, whether there is a\nmore effective way of labeling remains an open problem. In this paper, we\npropose to label only the most representative samples to expand the labeled\nset. Representative samples, selected by indegree of corresponding nodes on a\ndirected k-nearest neighbor (kNN) graph, lie in the k-nearest neighborhood of\nmany other samples. We design a graph neural network (GNN) labeler to label\nthem in a progressive learning manner. Aided by the progressive GNN labeler,\nour deep SSL approach outperforms state-of-the-art methods on several popular\nSSL benchmarks including CIFAR-10, SVHN, and ILSVRC-2012. Notably, we achieve\n72.1% top-1 accuracy, surpassing the previous best result by 3.3%, on the\nchallenging ImageNet benchmark with only $10\\%$ labeled data.",
          "link": "http://arxiv.org/abs/2108.06070",
          "publishedOn": "2021-08-16T00:47:33.219Z",
          "wordCount": 594,
          "title": "Progressive Representative Labeling for Deep Semi-Supervised Learning. (arXiv:2108.06070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuefan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Shinjae Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuewei Lin</a>",
          "description": "While deep neural networks have shown impressive performance in many tasks,\nthey are fragile to carefully designed adversarial attacks. We propose a novel\nadversarial training-based model by Attention Guided Knowledge Distillation and\nBi-directional Metric Learning (AGKD-BML). The attention knowledge is obtained\nfrom a weight-fixed model trained on a clean dataset, referred to as a teacher\nmodel, and transferred to a model that is under training on adversarial\nexamples (AEs), referred to as a student model. In this way, the student model\nis able to focus on the correct region, as well as correcting the intermediate\nfeatures corrupted by AEs to eventually improve the model accuracy. Moreover,\nto efficiently regularize the representation in feature space, we propose a\nbidirectional metric learning. Specifically, given a clean image, it is first\nattacked to its most confusing class to get the forward AE. A clean image in\nthe most confusing class is then randomly picked and attacked back to the\noriginal class to get the backward AE. A triplet loss is then used to shorten\nthe representation distance between original image and its AE, while enlarge\nthat between the forward and backward AEs. We conduct extensive adversarial\nrobustness experiments on two widely used datasets with different attacks. Our\nproposed AGKD-BML model consistently outperforms the state-of-the-art\napproaches. The code of AGKD-BML will be available at:\nhttps://github.com/hongw579/AGKD-BML.",
          "link": "http://arxiv.org/abs/2108.06017",
          "publishedOn": "2021-08-16T00:47:33.206Z",
          "wordCount": 676,
          "title": "AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning. (arXiv:2108.06017v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haitao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>",
          "description": "Semantic change detection (SCD) extends the change detection (CD) task to\nprovide not only the change locations but also the detailed semantic categories\n(before and after the observation intervals). This fine-grained change\ninformation is more useful in land-cover/land-use (LC/LU) applications. Recent\nstudies indicate that the SCD can be modeled through a triple-branch\nConvolutional Neural Network (CNN), which contains two temporal branches and a\nchange branch. However, in this architecture, the connections between the\ntemporal branches and the change branch are weak. To overcome these\nlimitations, we propose a novel CNN architecture for the SCD, where the\ntemporal features are re-used and are deeply merged in the temporal branch.\nFurthermore, we elaborate on this architecture to model the bi-temporal\nsemantic correlations. The resulting Bi-temporal Semantic Reasoning Network\n(Bi-SRNet) contains two types of semantic reasoning blocks to reason both\nsingle-temporal and cross-temporal semantic correlations, as well as a novel\nloss function to improve the semantic consistency of change detection results.\nExperimental results on a benchmark dataset show that the proposed architecture\nobtains significant accuracy improvements over the existing approaches, while\nthe added designs in the Bi-SRNet further improves the segmentation of both\nsemantic categories and the changed areas. The codes in this paper are\naccessible at: https://github.com/ggsDing/Bi-SRNet",
          "link": "http://arxiv.org/abs/2108.06103",
          "publishedOn": "2021-08-16T00:47:33.200Z",
          "wordCount": 671,
          "title": "Bi-Temporal Semantic Reasoning for the Semantic Change Detection of HR Remote Sensing Images. (arXiv:2108.06103v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Real world traffic sign recognition is an important step towards building\nautonomous vehicles, most of which highly dependent on Deep Neural Networks\n(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to\nadversarial examples. Many attack methods have been proposed to understand and\ngenerate adversarial examples, such as gradient based attack, score based\nattack, decision based attack, and transfer based attacks. However, most of\nthese algorithms are ineffective in real-world road sign attack, because (1)\niteratively learning perturbations for each frame is not realistic for a fast\nmoving car and (2) most optimization algorithms traverse all pixels equally\nwithout considering their diverse contribution. To alleviate these problems,\nthis paper proposes the targeted attention attack (TAA) method for real world\nroad sign attack. Specifically, we have made the following contributions: (1)\nwe leverage the soft attention map to highlight those important pixels and skip\nthose zero-contributed areas - this also helps to generate natural\nperturbations, (2) we design an efficient universal attack that optimizes a\nsingle perturbation/noise based on a set of training images under the guidance\nof the pre-trained attention map, (3) we design a simple objective function\nthat can be easily optimized, (4) we evaluate the effectiveness of TAA on real\nworld data sets. Experimental results validate that the TAA method improves the\nattack successful rate (nearly 10%) and reduces the perturbation loss (about a\nquarter) compared with the popular RP2 method. Additionally, our TAA also\nprovides good properties, e.g., transferability and generalization capability.\nWe provide code and data to ensure the reproducibility:\nhttps://github.com/AdvAttack/RoadSignAttack.",
          "link": "http://arxiv.org/abs/2010.04331",
          "publishedOn": "2021-08-16T00:47:33.193Z",
          "wordCount": 756,
          "title": "Targeted Physical-World Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_F/0/1/0/all/0/1\">Feng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiu Yu</a>",
          "description": "The multi-modal salient object detection model based on RGB-D information has\nbetter robustness in the real world. However, it remains nontrivial to better\nadaptively balance effective multi-modal information in the feature fusion\nphase. In this letter, we propose a novel gated recoding network (GRNet) to\nevaluate the information validity of the two modes, and balance their\ninfluence. Our framework is divided into three phases: perception phase,\nrecoding mixing phase and feature integration phase. First, A perception\nencoder is adopted to extract multi-level single-modal features, which lays the\nfoundation for multi-modal semantic comparative analysis. Then, a\nmodal-adaptive gate unit (MGU) is proposed to suppress the invalid information\nand transfer the effective modal features to the recoding mixer and the hybrid\nbranch decoder. The recoding mixer is responsible for recoding and mixing the\nbalanced multi-modal information. Finally, the hybrid branch decoder completes\nthe multi-level feature integration under the guidance of an optional edge\nguidance stream (OEGS). Experiments and analysis on eight popular benchmarks\nverify that our framework performs favorably against 9 state-of-art methods.",
          "link": "http://arxiv.org/abs/2108.06281",
          "publishedOn": "2021-08-16T00:47:33.174Z",
          "wordCount": 609,
          "title": "Modal-Adaptive Gated Recoding Network for RGB-D Salient Object Detection. (arXiv:2108.06281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pissas_T/0/1/0/all/0/1\">Theodoros Pissas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravasio_C/0/1/0/all/0/1\">Claudio Ravasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1\">Lyndon Da Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeles_C/0/1/0/all/0/1\">Christos Bergeles</a>",
          "description": "Our work proposes neural network design choices that set the state-of-the-art\non a challenging public benchmark on cataract surgery, CaDIS. Our methodology\nachieves strong performance across three semantic segmentation tasks with\nincreasingly granular surgical tool class sets by effectively handling class\nimbalance, an inherent challenge in any surgical video. We consider and\nevaluate two conceptually simple data oversampling methods as well as different\nloss functions. We show significant performance gains across network\narchitectures and tasks especially on the rarest tool classes, thereby\npresenting an approach for achieving high performance when imbalanced granular\ndatasets are considered. Our code and trained models are available at\nhttps://github.com/RViMLab/MICCAI2021_Cataract_semantic_segmentation and\nqualitative results on unseen surgical video can be found at\nhttps://youtu.be/twVIPUj1WZM.",
          "link": "http://arxiv.org/abs/2108.06119",
          "publishedOn": "2021-08-16T00:47:33.168Z",
          "wordCount": 561,
          "title": "Effective semantic segmentation in Cataract Surgery: What matters most?. (arXiv:2108.06119v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1\">Berkan Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>",
          "description": "Image caption generation is one of the most challenging problems at the\nintersection of visual recognition and natural language modeling domains. In\nthis work, we propose and study a practically important variant of this problem\nwhere test images may contain visual objects with no corresponding visual or\ntextual training examples. For this problem, we propose a detection-driven\napproach based on a generalized zero-shot detection model and a template-based\nsentence generation model. In order to improve the detection component, we\njointly define a class-to-class similarity based class representation and a\npractical score calibration mechanism. We also propose a novel evaluation\nmetric that provides complimentary insights to the captioning outputs, by\nseparately handling the visual and non-visual components of the captions. Our\nexperiments show that the proposed zero-shot detection model obtains\nstate-of-the-art performance on the MS-COCO dataset and the zero-shot\ncaptioning approach yields promising results.",
          "link": "http://arxiv.org/abs/2108.06165",
          "publishedOn": "2021-08-16T00:47:33.161Z",
          "wordCount": 574,
          "title": "Detection and Captioning with Unseen Object Classes. (arXiv:2108.06165v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Renshu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuozhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>",
          "description": "Vehicle tracking is an essential task in the multi-object tracking (MOT)\nfield. A distinct characteristic in vehicle tracking is that the trajectories\nof vehicles are fairly smooth in both the world coordinate and the image\ncoordinate. Hence, models that capture motion consistencies are of high\nnecessity. However, tracking with the standalone motion-based trackers is quite\nchallenging because targets could get lost easily due to limited information,\ndetection error and occlusion. Leveraging appearance information to assist\nobject re-identification could resolve this challenge to some extent. However,\ndoing so requires extra computation while appearance information is sensitive\nto occlusion as well. In this paper, we try to explore the significance of\nmotion patterns for vehicle tracking without appearance information. We propose\na novel approach that tackles the association issue for long-term tracking with\nthe exclusive fully-exploited motion information. We address the tracklet\nembedding issue with the proposed reconstruct-to-embed strategy based on deep\ngraph convolutional neural networks (GCN). Comprehensive experiments on the\nKITTI-car tracking dataset and UA-Detrac dataset show that the proposed method,\nthough without appearance information, could achieve competitive performance\nwith the state-of-the-art (SOTA) trackers. The source code will be available at\nhttps://github.com/GaoangW/LGMTracker.",
          "link": "http://arxiv.org/abs/2108.06029",
          "publishedOn": "2021-08-16T00:47:33.124Z",
          "wordCount": 650,
          "title": "Track without Appearance: Learn Box and Tracklet Embedding with Local and Global Motion Patterns for Vehicle Tracking. (arXiv:2108.06029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Keke Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1\">Dingruibo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Weilong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianpeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yawen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhaoquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhihong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "Overconfident predictions on out-of-distribution (OOD) samples is a thorny\nissue for deep neural networks. The key to resolve the OOD overconfidence issue\ninherently is to build a subset of OOD samples and then suppress predictions on\nthem. This paper proposes the Chamfer OOD examples (CODEs), whose distribution\nis close to that of in-distribution samples, and thus could be utilized to\nalleviate the OOD overconfidence issue effectively by suppressing predictions\non them. To obtain CODEs, we first generate seed OOD examples via\nslicing&splicing operations on in-distribution samples from different\ncategories, and then feed them to the Chamfer generative adversarial network\nfor distribution transformation, without accessing to any extra data. Training\nwith suppressing predictions on CODEs is validated to alleviate the OOD\noverconfidence issue largely without hurting classification accuracy, and\noutperform the state-of-the-art methods. Besides, we demonstrate CODEs are\nuseful for improving OOD detection and classification.",
          "link": "http://arxiv.org/abs/2108.06024",
          "publishedOn": "2021-08-16T00:47:33.101Z",
          "wordCount": 588,
          "title": "CODEs: Chamfer Out-of-Distribution Examples against Overconfidence Issue. (arXiv:2108.06024v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt\nfrom a labeled source domain to an unlabeled target domain. Previous work is\nmainly built upon convolutional neural networks (CNNs) to learn\ndomain-invariant representations. With the recent exponential increase in\napplying Vision Transformer (ViT) to vision tasks, the capability of ViT in\nadapting cross-domain knowledge, however, remains unexplored in the literature.\nTo fill this gap, this paper first comprehensively investigates the\ntransferability of ViT on a variety of domain adaptation tasks. Surprisingly,\nViT demonstrates superior transferability over its CNNs-based counterparts with\na large margin, while the performance can be further improved by incorporating\nadversarial adaptation. Notwithstanding, directly using CNNs-based adaptation\nstrategies fails to take the advantage of ViT's intrinsic merits (e.g.,\nattention mechanism and sequential image representation) which play an\nimportant role in knowledge transfer. To remedy this, we propose an unified\nframework, namely Transferable Vision Transformer (TVT), to fully exploit the\ntransferability of ViT for domain adaptation. Specifically, we delicately\ndevise a novel and effective unit, which we term Transferability Adaption\nModule (TAM). By injecting learned transferabilities into attention blocks, TAM\ncompels ViT focus on both transferable and discriminative features. Besides, we\nleverage discriminative clustering to enhance feature diversity and separation\nwhich are undermined during adversarial domain alignment. To verify its\nversatility, we perform extensive studies of TVT on four benchmarks and the\nexperimental results demonstrate that TVT attains significant improvements\ncompared to existing state-of-the-art UDA methods.",
          "link": "http://arxiv.org/abs/2108.05988",
          "publishedOn": "2021-08-16T00:47:33.091Z",
          "wordCount": 677,
          "title": "TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation. (arXiv:2108.05988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuqian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chunlei Song</a>",
          "description": "Image matting and image harmonization are two important tasks in image\ncomposition. Image matting, aiming to achieve foreground boundary details, and\nimage harmonization, aiming to make the background compatible with the\nforeground, are both promising yet challenging tasks. Previous works consider\noptimizing these two tasks separately, which may lead to a sub-optimal\nsolution. We propose to optimize matting and harmonization simultaneously to\nget better performance on both the two tasks and achieve more natural results.\nWe propose a new Generative Adversarial (GAN) framework which optimizing the\nmatting network and the harmonization network based on a self-attention\ndiscriminator. The discriminator is required to distinguish the natural images\nfrom different types of fake synthesis images. Extensive experiments on our\nconstructed dataset demonstrate the effectiveness of our proposed method. Our\ndataset and dataset generating pipeline can be found in\n\\url{https://git.io/HaMaGAN}",
          "link": "http://arxiv.org/abs/2108.06087",
          "publishedOn": "2021-08-16T00:47:33.081Z",
          "wordCount": 588,
          "title": "A Generative Adversarial Framework for Optimizing Image Matting and Harmonization Simultaneously. (arXiv:2108.06087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Garcia_C/0/1/0/all/0/1\">Carlos Gonzalez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_J/0/1/0/all/0/1\">Juan Carlos Ruiz-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Tapiador_S/0/1/0/all/0/1\">Sergio Romero-Tapiador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rengifo_S/0/1/0/all/0/1\">Santiago Rengifo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caruana_M/0/1/0/all/0/1\">Miguel Caruana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiajia Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yecheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1\">Javier Galbally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Moises Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_M/0/1/0/all/0/1\">Miguel Angel Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Barrero_M/0/1/0/all/0/1\">Marta Gomez-Barrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodashinsky_I/0/1/0/all/0/1\">Ilya Hodashinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarin_K/0/1/0/all/0/1\">Konstantin Sarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slezkin_A/0/1/0/all/0/1\">Artem Slezkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardamova_M/0/1/0/all/0/1\">Marina Bardamova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svetlakov_M/0/1/0/all/0/1\">Mikhail Svetlakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_M/0/1/0/all/0/1\">Mohammad Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szucs_C/0/1/0/all/0/1\">Cintia Lia Szucs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovari_B/0/1/0/all/0/1\">Bence Kovari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulsmeyer_F/0/1/0/all/0/1\">Falk Pulsmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1\">Mohamad Wehbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1\">Dario Zanca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1\">Sumaiya Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sarthak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabin_S/0/1/0/all/0/1\">Suraiya Jabin</a>",
          "description": "This article presents SVC-onGoing, an on-going competition for on-line\nsignature verification where researchers can easily benchmark their systems\nagainst the state of the art in an open common platform using large-scale\npublic databases, such as DeepSignDB and SVC2021_EvalDB, and standard\nexperimental protocols. SVC-onGoing is based on the ICDAR 2021 Competition on\nOn-Line Signature Verification (SVC 2021), which has been extended to allow\nparticipants anytime. The goal of SVC-onGoing is to evaluate the limits of\non-line signature verification systems on popular scenarios (office/mobile) and\nwriting inputs (stylus/finger) through large-scale public databases. Three\ndifferent tasks are considered in the competition, simulating realistic\nscenarios as both random and skilled forgeries are simultaneously considered on\neach task. The results obtained in SVC-onGoing prove the high potential of deep\nlearning methods in comparison with traditional methods. In particular, the\nbest signature verification system has obtained Equal Error Rate (EER) values\nof 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3). Future studies in the\nfield should be oriented to improve the performance of signature verification\nsystems on the challenging mobile scenarios of SVC-onGoing in which several\nmobile devices and the finger are used during the signature acquisition.",
          "link": "http://arxiv.org/abs/2108.06090",
          "publishedOn": "2021-08-16T00:47:33.068Z",
          "wordCount": 683,
          "title": "SVC-onGoing: Signature Verification Competition. (arXiv:2108.06090v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ronge_R/0/1/0/all/0/1\">Raphael Ronge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nho_K/0/1/0/all/0/1\">Kwangsik Nho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>",
          "description": "The current state-of-the-art deep neural networks (DNNs) for Alzheimer's\nDisease diagnosis use different biomarker combinations to classify patients,\nbut do not allow extracting knowledge about the interactions of biomarkers.\nHowever, to improve our understanding of the disease, it is paramount to\nextract such knowledge from the learned model. In this paper, we propose a Deep\nFactorization Machine model that combines the ability of DNNs to learn complex\nrelationships and the ease of interpretability of a linear model. The proposed\nmodel has three parts: (i) an embedding layer to deal with sparse categorical\ndata, (ii) a Factorization Machine to efficiently learn pairwise interactions,\nand (iii) a DNN to implicitly model higher order interactions. In our\nexperiments on data from the Alzheimer's Disease Neuroimaging Initiative, we\ndemonstrate that our proposed model classifies cognitive normal, mild cognitive\nimpaired, and demented patients more accurately than competing models. In\naddition, we show that valuable knowledge about the interactions among\nbiomarkers can be obtained.",
          "link": "http://arxiv.org/abs/2108.05916",
          "publishedOn": "2021-08-16T00:47:33.031Z",
          "wordCount": 608,
          "title": "Alzheimer's Disease Diagnosis via Deep Factorization Machine Models. (arXiv:2108.05916v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">D.Y. Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">X.J. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">H. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">J. Kittler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">T.Y. Xu</a>",
          "description": "In this paper, we propose a photorealistic style transfer network to\nemphasize the natural effect of photorealistic image stylization. In general,\ndistortion of the image content and lacking of details are two typical issues\nin the style transfer field. To this end, we design a novel framework employing\nthe U-Net structure to maintain the rich spatial clues, with a multi-layer\nfeature aggregation (MFA) method to simultaneously provide the details obtained\nby the shallow layers in the stylization processing. In particular, an encoder\nbased on the dense block and a decoder form a symmetrical structure of U-Net\nare jointly staked to realize an effective feature extraction and image\nreconstruction. Besides, a transfer module based on MFA and \"adaptive instance\nnormalization\" (AdaIN) is inserted in the skip connection positions to achieve\nthe stylization. Accordingly, the stylized image possesses the texture of a\nreal photo and preserves rich content details without introducing any mask or\npost-processing steps. The experimental results on public datasets demonstrate\nthat our method achieves a more faithful structural similarity with a lower\nstyle loss, reflecting the effectiveness and merit of our approach.",
          "link": "http://arxiv.org/abs/2108.06113",
          "publishedOn": "2021-08-16T00:47:33.014Z",
          "wordCount": 630,
          "title": "UMFA: A photorealistic style transfer method based on U-Net and multi-layer feature aggregation. (arXiv:2108.06113v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.00761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_E/0/1/0/all/0/1\">Elia Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Francesco Silvestri</a>",
          "description": "A free-floating bike-sharing system (FFBSS) is a dockless rental system where\nan individual can borrow a bike and returns it anywhere, within the service\narea. To improve the rental service, available bikes should be distributed over\nthe entire service area: a customer leaving from any position is then more\nlikely to find a near bike and then to use the service. Moreover, spreading\nbikes among the entire service area increases urban spatial equity since the\nbenefits of FFBSS are not a prerogative of just a few zones. For guaranteeing\nsuch distribution, the FFBSS operator can use vans to manually relocate bikes,\nbut it incurs high economic and environmental costs. We propose a novel\napproach that exploits the existing bike flows generated by customers to\ndistribute bikes. More specifically, by envisioning the problem as an Influence\nMaximization problem, we show that it is possible to position batches of bikes\non a small number of zones, and then the daily use of FFBSS will efficiently\nspread these bikes on a large area. We show that detecting these zones is\nNP-complete, but there exists a simple and efficient $1-1/e$ approximation\nalgorithm; our approach is then evaluated on a dataset of rides from the\nfree-floating bike-sharing system of the city of Padova.",
          "link": "http://arxiv.org/abs/2107.00761",
          "publishedOn": "2021-08-20T01:53:53.880Z",
          "wordCount": 695,
          "title": "On the Bike Spreading Problem. (arXiv:2107.00761v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuben Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haipeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiafa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Song Guo</a>",
          "description": "Unmanned aerial vehicles (UAVs), or say drones, are envisioned to support\nextensive applications in next-generation wireless networks in both civil and\nmilitary fields. Empowering UAVs networks intelligence by artificial\nintelligence (AI) especially machine learning (ML) techniques is inevitable and\nappealing to enable the aforementioned applications. To solve the problems of\ntraditional cloud-centric ML for UAV networks such as privacy concern,\nunacceptable latency, and resource burden, a distributed ML technique,\n\\textit(i.e.), federated learning (FL), has been recently proposed to enable\nmultiple UAVs to collaboratively train ML model without letting out raw data.\nHowever, almost all existing FL paradigms are still centralized, \\textit{i.e.},\na central entity is in charge of ML model aggregation and fusion over the whole\nnetwork, which could result in the issue of a single point of failure and are\ninappropriate to UAV networks with both unreliable nodes and links. Thus\nmotivated, in this article, we propose a novel architecture called DFL-UN\n(\\underline{D}ecentralized \\underline{F}ederated \\underline{L}earning for\n\\underline{U}AV \\underline{N}etworks), which enables FL within UAV networks\nwithout a central entity. We also conduct a preliminary simulation study to\nvalidate the feasibility and effectiveness of the DFL-UN architecture. Finally,\nwe discuss the main challenges and potential research directions in the DFL-UN.",
          "link": "http://arxiv.org/abs/2104.07557",
          "publishedOn": "2021-08-20T01:53:53.870Z",
          "wordCount": 690,
          "title": "Decentralized Federated Learning for UAV Networks: Architecture, Challenges, and Opportunities. (arXiv:2104.07557v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1\">Jonathan Balloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "Modern computer vision applications suffer from catastrophic forgetting when\nincrementally learning new concepts over time. The most successful approaches\nto alleviate this forgetting require extensive replay of previously seen data,\nwhich is problematic when memory constraints or data legality concerns exist.\nIn this work, we consider the high-impact problem of Data-Free\nClass-Incremental Learning (DFCIL), where an incremental learning agent must\nlearn new concepts over time without storing generators or training data from\npast tasks. One approach for DFCIL is to replay synthetic images produced by\ninverting a frozen copy of the learner's classification model, but we show this\napproach fails for common class-incremental benchmarks when using standard\ndistillation strategies. We diagnose the cause of this failure and propose a\nnovel incremental distillation strategy for DFCIL, contributing a modified\ncross-entropy training and importance-weighted feature distillation, and show\nthat our method results in up to a 25.1% increase in final task accuracy\n(absolute difference) compared to SOTA DFCIL methods for common\nclass-incremental benchmarks. Our method even outperforms several standard\nreplay based methods which store a coreset of images.",
          "link": "http://arxiv.org/abs/2106.09701",
          "publishedOn": "2021-08-20T01:53:53.864Z",
          "wordCount": 666,
          "title": "Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>",
          "description": "A myriad of recent breakthroughs in hand-crafted neural architectures for\nvisual recognition have highlighted the urgent need to explore hybrid\narchitectures consisting of diversified building blocks. Meanwhile, neural\narchitecture search methods are surging with an expectation to reduce human\nefforts. However, whether NAS methods can efficiently and effectively handle\ndiversified search spaces with disparate candidates (e.g. CNNs and\ntransformers) is still an open question. In this work, we present Block-wisely\nSelf-supervised Neural Architecture Search (BossNAS), an unsupervised NAS\nmethod that addresses the problem of inaccurate architecture rating caused by\nlarge weight-sharing space and biased supervision in previous methods. More\nspecifically, we factorize the search space into blocks and utilize a novel\nself-supervised training scheme, named ensemble bootstrapping, to train each\nblock separately before searching them as a whole towards the population\ncenter. Additionally, we present HyTra search space, a fabric-like hybrid\nCNN-transformer search space with searchable down-sampling positions. On this\nchallenging search space, our searched model, BossNet-T, achieves up to 82.5%\naccuracy on ImageNet, surpassing EfficientNet by 2.4% with comparable compute\ntime. Moreover, our method achieves superior architecture rating accuracy with\n0.78 and 0.76 Spearman correlation on the canonical MBConv search space with\nImageNet and on NATS-Bench size search space with CIFAR-100, respectively,\nsurpassing state-of-the-art NAS methods. Code:\nhttps://github.com/changlin31/BossNAS",
          "link": "http://arxiv.org/abs/2103.12424",
          "publishedOn": "2021-08-20T01:53:53.857Z",
          "wordCount": 705,
          "title": "BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search. (arXiv:2103.12424v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1\">Raviteja Vemulapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1\">Philip Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Bradley Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_L/0/1/0/all/0/1\">Lior Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Wu</a>",
          "description": "Collecting labeled data for the task of semantic segmentation is expensive\nand time-consuming, as it requires dense pixel-level annotations. While recent\nConvolutional Neural Network (CNN) based semantic segmentation approaches have\nachieved impressive results by using large amounts of labeled training data,\ntheir performance drops significantly as the amount of labeled data decreases.\nThis happens because deep CNNs trained with the de facto cross-entropy loss can\neasily overfit to small amounts of labeled data. To address this issue, we\npropose a simple and effective contrastive learning-based training strategy in\nwhich we first pretrain the network using a pixel-wise, label-based contrastive\nloss, and then fine-tune it using the cross-entropy loss. This approach\nincreases intra-class compactness and inter-class separability, thereby\nresulting in a better pixel classifier. We demonstrate the effectiveness of the\nproposed training strategy using the Cityscapes and PASCAL VOC 2012\nsegmentation datasets. Our results show that pretraining with the proposed\ncontrastive loss results in large performance gains (more than 20% absolute\nimprovement in some settings) when the amount of labeled data is limited. In\nmany settings, the proposed contrastive pretraining strategy, which does not\nuse any additional data, is able to match or outperform the widely-used\nImageNet pretraining strategy that uses more than a million additional labeled\nimages.",
          "link": "http://arxiv.org/abs/2012.06985",
          "publishedOn": "2021-08-20T01:53:53.839Z",
          "wordCount": 710,
          "title": "Contrastive Learning for Label-Efficient Semantic Segmentation. (arXiv:2012.06985v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soorki_M/0/1/0/all/0/1\">Mehdi Naderi Soorki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Choong Seon Hong</a>",
          "description": "In this paper, a novel framework for guaranteeing ultra-reliable millimeter\nwave (mmW) communications using multiple artificial intelligence (AI)-enabled\nreconfigurable intelligent surfaces (RISs) is proposed. The use of multiple\nAI-powered RISs allows changing the propagation direction of the signals\ntransmitted from a mmW access point (AP) thereby improving coverage\nparticularly for non-line-of-sight (NLoS) areas. However, due to the\npossibility of highly stochastic blockage over mmW links, designing an\nintelligent controller to jointly optimize the mmW AP beam and RIS phase shifts\nis a daunting task. In this regard, first, a parametric risk-sensitive episodic\nreturn is proposed to maximize the expected bit rate and mitigate the risk of\nmmW link blockage. Then, a closed-form approximation of the policy gradient of\nthe risk-sensitive episodic return is analytically derived. Next, the problem\nof joint beamforming for mmW AP and phase shift control for mmW RISs is modeled\nas an identical payoff stochastic game within a cooperative multi-agent\nenvironment, in which the agents are the mmW AP and the RISs. Two centralized\nand distributed controllers are proposed to control the policies of the mmW AP\nand RISs. To directly find an optimal solution, the parametric functional-form\npolicies for these controllers are modeled using deep recurrent neural networks\n(RNNs). Simulation results show that the error between policies of the optimal\nand the RNN-based controllers is less than 1.5%. Moreover, the variance of the\nachievable rates resulting from the deep RNN-based controllers is 60% less than\nthe variance of the risk-averse baseline.",
          "link": "http://arxiv.org/abs/2104.00075",
          "publishedOn": "2021-08-20T01:53:53.831Z",
          "wordCount": 722,
          "title": "Ultra-Reliable Indoor Millimeter Wave Communications using Multiple Artificial Intelligence-Powered Intelligent Surfaces. (arXiv:2104.00075v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaman_A/0/1/0/all/0/1\">Anadi Chaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belius_D/0/1/0/all/0/1\">David Belius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>",
          "description": "Convolutional neural networks (CNNs) have been tremendously successful in\nsolving imaging inverse problems. To understand their success, an effective\nstrategy is to construct simpler and mathematically more tractable\nconvolutional sparse coding (CSC) models that share essential ingredients with\nCNNs. Existing CSC methods, however, underperform leading CNNs in challenging\ninverse problems. We hypothesize that the performance gap may be attributed in\npart to how they process images at different spatial scales: While many CNNs\nuse multiscale feature representations, existing CSC models mostly rely on\nsingle-scale dictionaries. To close the performance gap, we thus propose a\nmultiscale convolutional dictionary structure. The proposed dictionary\nstructure is derived from the U-Net, arguably the most versatile and widely\nused CNN for image-to-image learning problems. We show that incorporating the\nproposed multiscale dictionary in an otherwise standard CSC framework yields\nperformance competitive with state-of-the-art CNNs across a range of\nchallenging inverse problems including CT and MRI reconstruction. Our work thus\ndemonstrates the effectiveness and scalability of the multiscale CSC approach\nin solving challenging inverse problems.",
          "link": "http://arxiv.org/abs/2011.12815",
          "publishedOn": "2021-08-20T01:53:53.824Z",
          "wordCount": 641,
          "title": "Learning Multiscale Convolutional Dictionaries for Image Reconstruction. (arXiv:2011.12815v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Talpur_A/0/1/0/all/0/1\">Anum Talpur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurusamy_M/0/1/0/all/0/1\">Mohan Gurusamy</a>",
          "description": "Machine Learning (ML) has emerged as an attractive and viable technique to\nprovide effective solutions for a wide range of application domains. An\nimportant application domain is vehicular networks wherein ML-based approaches\nare found to be very useful to address various problems. The use of wireless\ncommunication between vehicular nodes and/or infrastructure makes it vulnerable\nto different types of attacks. In this regard, ML and its variants are gaining\npopularity to detect attacks and deal with different kinds of security issues\nin vehicular communication. In this paper, we present a comprehensive survey of\nML-based techniques for different security issues in vehicular networks. We\nfirst briefly introduce the basics of vehicular networks and different types of\ncommunications. Apart from the traditional vehicular networks, we also consider\nmodern vehicular network architectures. We propose a taxonomy of security\nattacks in vehicular networks and discuss various security challenges and\nrequirements. We classify the ML techniques developed in the literature\naccording to their use in vehicular network applications. We explain the\nsolution approaches and working principles of these ML techniques in addressing\nvarious security challenges and provide insightful discussion. The limitations\nand challenges in using ML-based methods in vehicular networks are discussed.\nFinally, we present observations and lessons learned before we conclude our\nwork.",
          "link": "http://arxiv.org/abs/2105.15035",
          "publishedOn": "2021-08-20T01:53:53.815Z",
          "wordCount": 684,
          "title": "Machine Learning for Security in Vehicular Networks: A Comprehensive Survey. (arXiv:2105.15035v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1\">Gyri Reiersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1\">David Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Forest carbon offsets are increasingly popular and can play a significant\nrole in financing climate mitigation, forest conservation, and reforestation.\nMeasuring how much carbon is stored in forests is, however, still largely done\nvia expensive, time-consuming, and sometimes unaccountable field measurements.\nTo overcome these limitations, many verification bodies are leveraging machine\nlearning (ML) algorithms to estimate forest carbon from satellite or aerial\nimagery. Aerial imagery allows for tree species or family classification, which\nimproves the satellite imagery-based forest type classification. However,\naerial imagery is significantly more expensive to collect and it is unclear by\nhow much the higher resolution improves the forest carbon estimation. This\nproposal paper describes the first systematic comparison of forest carbon\nestimation from aerial imagery, satellite imagery, and ground-truth field\nmeasurements via deep learning-based algorithms for a tropical reforestation\nproject. Our initial results show that forest carbon estimates from satellite\nimagery can overestimate above-ground biomass by up to 10-times for tropical\nreforestation projects. The significant difference between aerial and\nsatellite-derived forest carbon measurements shows the potential for aerial\nimagery-based ML algorithms and raises the importance to extend this study to a\nglobal benchmark between options for carbon measurements.",
          "link": "http://arxiv.org/abs/2107.11320",
          "publishedOn": "2021-08-20T01:53:53.809Z",
          "wordCount": 693,
          "title": "Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arkabandhu Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mingchao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Chris Jermaine</a>",
          "description": "Recent papers have suggested that transfer learning can outperform\nsophisticated meta-learning methods for few-shot image classification. We take\nthis hypothesis to its logical conclusion, and suggest the use of an ensemble\nof high-quality, pre-trained feature extractors for few-shot image\nclassification. We show experimentally that a library of pre-trained feature\nextractors combined with a simple feed-forward network learned with an\nL2-regularizer can be an excellent option for solving cross-domain few-shot\nimage classification. Our experimental results suggest that this simpler\nsample-efficient approach far outperforms several well-established\nmeta-learning algorithms on a variety of few-shot tasks.",
          "link": "http://arxiv.org/abs/2101.00562",
          "publishedOn": "2021-08-20T01:53:53.788Z",
          "wordCount": 596,
          "title": "Few-shot Image Classification: Just Use a Library of Pre-trained Feature Extractors and a Simple Classifier. (arXiv:2101.00562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.00618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie P. Kaelbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Alberto Rodriguez</a>",
          "description": "Pushing is a fundamental robotic skill. Existing work has shown how to\nexploit models of pushing to achieve a variety of tasks, including grasping\nunder uncertainty, in-hand manipulation and clearing clutter. Such models,\nhowever, are approximate, which limits their applicability. Learning-based\nmethods can reason directly from raw sensory data with accuracy, and have the\npotential to generalize to a wider diversity of scenarios. However, developing\nand testing such methods requires rich-enough datasets. In this paper we\nintroduce Omnipush, a dataset with high variety of planar pushing behavior. In\nparticular, we provide 250 pushes for each of 250 objects, all recorded with\nRGB-D and a high precision tracking system. The objects are constructed so as\nto systematically explore key factors that affect pushing -- the shape of the\nobject and its mass distribution -- which have not been broadly explored in\nprevious datasets, and allow to study generalization in model learning.\nOmnipush includes a benchmark for meta-learning dynamic models, which requires\nalgorithms that make good predictions and estimate their own uncertainty. We\nalso provide an RGB video prediction benchmark and propose other relevant tasks\nthat can be suited with this dataset.\n\nData and code are available at\n\\url{https://web.mit.edu/mcube/omnipush-dataset/}.",
          "link": "http://arxiv.org/abs/1910.00618",
          "publishedOn": "2021-08-20T01:53:53.782Z",
          "wordCount": 697,
          "title": "Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video. (arXiv:1910.00618v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_V/0/1/0/all/0/1\">Vasu Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "Adversarial training is one of the most effective defenses against\nadversarial attacks. Previous works suggest that overfitting is a dominant\nphenomenon in adversarial training leading to a large generalization gap\nbetween test and train accuracy in neural networks. In this work, we show that\nthe observed generalization gap is closely related to the choice of the\nactivation function. In particular, we show that using activation functions\nwith low (exact or approximate) curvature values has a regularization effect\nthat significantly reduces both the standard and robust generalization gaps in\nadversarial training. We observe this effect for both differentiable/smooth\nactivations such as SiLU as well as non-differentiable/non-smooth activations\nsuch as LeakyReLU. In the latter case, the \"approximate\" curvature of the\nactivation is low. Finally, we show that for activation functions with low\ncurvature, the double descent phenomenon for adversarially trained models does\nnot occur.",
          "link": "http://arxiv.org/abs/2102.07861",
          "publishedOn": "2021-08-20T01:53:53.775Z",
          "wordCount": 605,
          "title": "Low Curvature Activations Reduce Overfitting in Adversarial Training. (arXiv:2102.07861v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soremekun_E/0/1/0/all/0/1\">Ezekiel Soremekun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1\">Sakshi Udeshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Sudipta Chattopadhyay</a>",
          "description": "Software often produces biased outputs. In particular, machine learning (ML)\nbased software are known to produce erroneous predictions when processing\ndiscriminatory inputs. Such unfair program behavior can be caused by societal\nbias. In the last few years, Amazon, Microsoft and Google have provided\nsoftware services that produce unfair outputs, mostly due to societal bias\n(e.g. gender or race). In such events, developers are saddled with the task of\nconducting fairness testing. Fairness testing is challenging; developers are\ntasked with generating discriminatory inputs that reveal and explain biases.\n\nWe propose a grammar-based fairness testing approach (called ASTRAEA) which\nleverages context-free grammars to generate discriminatory inputs that reveal\nfairness violations in software systems. Using probabilistic grammars, ASTRAEA\nalso provides fault diagnosis by isolating the cause of observed software bias.\nASTRAEA's diagnoses facilitate the improvement of ML fairness.\n\nASTRAEA was evaluated on 18 software systems that provide three major natural\nlanguage processing (NLP) services. In our evaluation, ASTRAEA generated\nfairness violations with a rate of ~18%. ASTRAEA generated over 573K\ndiscriminatory test cases and found over 102K fairness violations. Furthermore,\nASTRAEA improves software fairness by ~76%, via model-retraining.",
          "link": "http://arxiv.org/abs/2010.02542",
          "publishedOn": "2021-08-20T01:53:53.768Z",
          "wordCount": 650,
          "title": "Astraea: Grammar-based Fairness Testing. (arXiv:2010.02542v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+OLuing_M/0/1/0/all/0/1\">Mervyn O&#x27;Luing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prestwich_S/0/1/0/all/0/1\">Steven Prestwich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarim_S/0/1/0/all/0/1\">S. Armagan Tarim</a>",
          "description": "This study combines simulated annealing with delta evaluation to solve the\njoint stratification and sample allocation problem. In this problem, atomic\nstrata are partitioned into mutually exclusive and collectively exhaustive\nstrata. Each stratification is a solution, the quality of which is measured by\nits cost. The Bell number of possible solutions is enormous for even a moderate\nnumber of atomic strata and an additional layer of complexity is added with the\nevaluation time of each solution. Many larger scale combinatorial optimisation\nproblems cannot be solved to optimality because the search for an optimum\nsolution requires a prohibitive amount of computation time; a number of local\nsearch heuristic algorithms have been designed for this problem but these can\nbecome trapped in local minima preventing any further improvements. We add to\nthe existing suite of local search algorithms a simulated annealing algorithm\nthat allows for an escape from local minima and uses delta evaluation to\nexploit the similarity between consecutive solutions and thereby reduce the\nevaluation time. We compare the simulated annealing algorithm with two recent\nalgorithms. In both cases the SAA attains a solution of comparable quality in\nconsiderably less computation time.",
          "link": "http://arxiv.org/abs/2011.13006",
          "publishedOn": "2021-08-20T01:53:53.762Z",
          "wordCount": 669,
          "title": "A Simulated Annealing Algorithm for Joint Stratification and Sample Allocation Designs. (arXiv:2011.13006v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14410",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>",
          "description": "Modern evolvements of the technologies have been leading to a profound\ninfluence on the financial market. The introduction of constituents like\nExchange-Traded Funds, and the wide-use of advanced technologies such as\nalgorithmic trading, results in a boom of the data which provides more\nopportunities to reveal deeper insights. However, traditional statistical\nmethods always suffer from the high-dimensional, high-correlation, and\ntime-varying instinct of the financial data. In this dissertation, we focus on\ndeveloping techniques to stress these difficulties. With the proposed\nmethodologies, we can have more interpretable models, clearer explanations, and\nbetter predictions.",
          "link": "http://arxiv.org/abs/2107.14410",
          "publishedOn": "2021-08-20T01:53:53.744Z",
          "wordCount": 550,
          "title": "The Adaptive Multi-Factor Model and the Financial Market. (arXiv:2107.14410v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09235",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_G/0/1/0/all/0/1\">Guanyang Wang</a>",
          "description": "The exchange algorithm is one of the most popular extensions of the\nMetropolis--Hastings algorithm to sample from doubly-intractable distributions.\nHowever, the theoretical exploration of the exchange algorithm is very limited.\nFor example, natural questions like `Does exchange algorithm converge at a\ngeometric rate?' or `Does the exchange algorithm admit a Central Limit\nTheorem?' have not been answered yet. In this paper, we study the theoretical\nproperties of the exchange algorithm, in terms of asymptotic variance and\nconvergence speed. We compare the exchange algorithm with the original\nMetropolis--Hastings algorithm and provide both necessary and sufficient\nconditions for the geometric ergodicity of the exchange algorithm. Moreover, we\nprove that our results can be applied to various practical applications such as\nlocation models, Gaussian models, Poisson models, and a large class of\nexponential families, which includes most of the practical applications of the\nexchange algorithm. A central limit theorem for the exchange algorithm is also\nestablished. Our results justify the theoretical usefulness of the exchange\nalgorithm.",
          "link": "http://arxiv.org/abs/2005.09235",
          "publishedOn": "2021-08-20T01:53:53.737Z",
          "wordCount": 651,
          "title": "On the Theoretical Properties of the Exchange Algorithm. (arXiv:2005.09235v4 [stat.CO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen-Hsuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei-Chiu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "Neural Radiance Fields (NeRF) have recently gained a surge of interest within\nthe computer vision community for its power to synthesize photorealistic novel\nviews of real-world scenes. One limitation of NeRF, however, is its requirement\nof accurate camera poses to learn the scene representations. In this paper, we\npropose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from\nimperfect (or even unknown) camera poses -- the joint problem of learning\nneural 3D representations and registering camera frames. We establish a\ntheoretical connection to classical image alignment and show that\ncoarse-to-fine registration is also applicable to NeRF. Furthermore, we show\nthat na\\\"ively applying positional encoding in NeRF has a negative impact on\nregistration with a synthesis-based objective. Experiments on synthetic and\nreal-world data show that BARF can effectively optimize the neural scene\nrepresentations and resolve large camera pose misalignment at the same time.\nThis enables view synthesis and localization of video sequences from unknown\ncamera poses, opening up new avenues for visual localization systems (e.g.\nSLAM) and potential applications for dense 3D mapping and reconstruction.",
          "link": "http://arxiv.org/abs/2104.06405",
          "publishedOn": "2021-08-20T01:53:53.731Z",
          "wordCount": 657,
          "title": "BARF: Bundle-Adjusting Neural Radiance Fields. (arXiv:2104.06405v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuru_N/0/1/0/all/0/1\">Nurullah Giray Kuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>",
          "description": "From CNNs to attention mechanisms, encoding inductive biases into neural\nnetworks has been a fruitful source of improvement in machine learning. Adding\nauxiliary losses to the main objective function is a general way of encoding\nbiases that can help networks learn better representations. However, since\nauxiliary losses are minimized only on training data, they suffer from the same\ngeneralization gap as regular task losses. Moreover, by adding a term to the\nloss function, the model optimizes a different objective than the one we care\nabout. In this work we address both problems: first, we take inspiration from\n\\textit{transductive learning} and note that after receiving an input but\nbefore making a prediction, we can fine-tune our networks on any unsupervised\nloss. We call this process {\\em tailoring}, because we customize the model to\neach input to ensure our prediction satisfies the inductive bias. Second, we\nformulate {\\em meta-tailoring}, a nested optimization similar to that in\nmeta-learning, and train our models to perform well on the task objective after\nadapting them using an unsupervised loss. The advantages of tailoring and\nmeta-tailoring are discussed theoretically and demonstrated empirically on a\ndiverse set of examples.",
          "link": "http://arxiv.org/abs/2009.10623",
          "publishedOn": "2021-08-20T01:53:53.721Z",
          "wordCount": 703,
          "title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.",
          "link": "http://arxiv.org/abs/2108.04351",
          "publishedOn": "2021-08-20T01:53:53.681Z",
          "wordCount": 664,
          "title": "Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_C/0/1/0/all/0/1\">Chaitanya K. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappart_Q/0/1/0/all/0/1\">Quentin Cappart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousseau_L/0/1/0/all/0/1\">Louis-Martin Rousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_T/0/1/0/all/0/1\">Thomas Laurent</a>",
          "description": "End-to-end training of neural network solvers for combinatorial optimization\nproblems such as the Travelling Salesman Problem is intractable and inefficient\nbeyond a few hundreds of nodes. While state-of-the-art Machine Learning\napproaches perform closely to classical solvers when trained on trivially small\nsizes, they are unable to generalize the learnt policy to larger instances of\npractical scales. Towards leveraging transfer learning to solve large-scale\nTSPs, this paper identifies inductive biases, model architectures and learning\nalgorithms that promote generalization to instances larger than those seen in\ntraining. Our controlled experiments provide the first principled investigation\ninto such zero-shot generalization, revealing that extrapolating beyond\ntraining data requires rethinking the neural combinatorial optimization\npipeline, from network layers and learning paradigms to evaluation protocols.",
          "link": "http://arxiv.org/abs/2006.07054",
          "publishedOn": "2021-08-20T01:53:53.633Z",
          "wordCount": 604,
          "title": "Learning TSP Requires Rethinking Generalization. (arXiv:2006.07054v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiaheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "We show when maximizing a properly defined $f$-divergence measure with\nrespect to a classifier's predictions and the supervised labels is robust with\nlabel noise. Leveraging its variational form, we derive a nice decoupling\nproperty for a family of $f$-divergence measures when label noise presents,\nwhere the divergence is shown to be a linear combination of the variational\ndifference defined on the clean distribution and a bias term introduced due to\nthe noise. The above derivation helps us analyze the robustness of different\n$f$-divergence functions. With established robustness, this family of\n$f$-divergence functions arises as useful metrics for the problem of learning\nwith noisy labels, which do not require the specification of the labels' noise\nrate. When they are possibly not robust, we propose fixes to make them so. In\naddition to the analytical results, we present thorough experimental evidence.\nOur code is available at\nhttps://github.com/UCSC-REAL/Robust-f-divergence-measures.",
          "link": "http://arxiv.org/abs/2011.03687",
          "publishedOn": "2021-08-20T01:53:53.627Z",
          "wordCount": 622,
          "title": "When Optimizing $f$-divergence is Robust with Label Noise. (arXiv:2011.03687v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01174",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Choi_Y/0/1/0/all/0/1\">Yeunju Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_Y/0/1/0/all/0/1\">Youngmoon Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suh_Y/0/1/0/all/0/1\">Youngjoo Suh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hoirin Kim</a>",
          "description": "Although recent end-to-end text-to-speech (TTS) systems have achieved\nhigh-quality speech synthesis, there are still several factors that degrade the\nquality of synthesized speech, including lack of training data or information\nloss during knowledge distillation. To address the problem, we propose a novel\nway to train a TTS model under the supervision of perceptual loss, which\nmeasures the distance between the maximum speech quality score and the\npredicted one. We first pre-train a mean opinion score (MOS) prediction model\nand then train a TTS model in the direction of maximizing the MOS of\nsynthesized speech predicted by the pre-trained MOS prediction model. Through\nthis method, we can improve the quality of synthesized speech universally\n(i.e., regardless of the network architecture or the cause of the speech\nquality degradation) and efficiently (i.e., without increasing the inference\ntime or the model complexity). The evaluation results for MOS and phone error\nrate demonstrate that our proposed approach improves previous models in terms\nof both naturalness and intelligibility.",
          "link": "http://arxiv.org/abs/2011.01174",
          "publishedOn": "2021-08-20T01:53:53.619Z",
          "wordCount": 639,
          "title": "Perceptually Guided End-to-End Text-to-Speech With MOS Prediction. (arXiv:2011.01174v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03465",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Sarwan Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sahoo_B/0/1/0/all/0/1\">Bikram Sahoo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ullah_N/0/1/0/all/0/1\">Naimat Ullah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zelikovskiy_A/0/1/0/all/0/1\">Alexander Zelikovskiy</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1\">Murray Patterson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1\">Imdadullah Khan</a>",
          "description": "With the rapid spread of the novel coronavirus (COVID-19) across the globe\nand its continuous mutation, it is of pivotal importance to design a system to\nidentify different known (and unknown) variants of SARS-CoV-2. Identifying\nparticular variants helps to understand and model their spread patterns, design\neffective mitigation strategies, and prevent future outbreaks. It also plays a\ncrucial role in studying the efficacy of known vaccines against each variant\nand modeling the likelihood of breakthrough infections. It is well known that\nthe spike protein contains most of the information/variation pertaining to\ncoronavirus variants.\n\nIn this paper, we use spike sequences to classify different variants of the\ncoronavirus in humans. We show that preserving the order of the amino acids\nhelps the underlying classifiers to achieve better performance. We also show\nthat we can train our model to outperform the baseline algorithms using only a\nsmall number of training samples ($1\\%$ of the data). Finally, we show the\nimportance of the different amino acids which play a key role in identifying\nvariants and how they coincide with those reported by the USA's Centers for\nDisease Control and Prevention (CDC).",
          "link": "http://arxiv.org/abs/2108.03465",
          "publishedOn": "2021-08-20T01:53:53.613Z",
          "wordCount": 688,
          "title": "A k-mer Based Approach for SARS-CoV-2 Variant Identification. (arXiv:2108.03465v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1\">Maithra Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1\">Thomas Unterthiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1\">Alexey Dosovitskiy</a>",
          "description": "Convolutional neural networks (CNNs) have so far been the de-facto model for\nvisual data. Recent work has shown that (Vision) Transformer models (ViT) can\nachieve comparable or even superior performance on image classification tasks.\nThis raises a central question: how are Vision Transformers solving these\ntasks? Are they acting like convolutional networks, or learning entirely\ndifferent visual representations? Analyzing the internal representation\nstructure of ViTs and CNNs on image classification benchmarks, we find striking\ndifferences between the two architectures, such as ViT having more uniform\nrepresentations across all layers. We explore how these differences arise,\nfinding crucial roles played by self-attention, which enables early aggregation\nof global information, and ViT residual connections, which strongly propagate\nfeatures from lower to higher layers. We study the ramifications for spatial\nlocalization, demonstrating ViTs successfully preserve input spatial\ninformation, with noticeable effects from different classification methods.\nFinally, we study the effect of (pretraining) dataset scale on intermediate\nfeatures and transfer learning, and conclude with a discussion on connections\nto new architectures such as the MLP-Mixer.",
          "link": "http://arxiv.org/abs/2108.08810",
          "publishedOn": "2021-08-20T01:53:53.605Z",
          "wordCount": 622,
          "title": "Do Vision Transformers See Like Convolutional Neural Networks?. (arXiv:2108.08810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weyns_D/0/1/0/all/0/1\">Danny Weyns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1\">Thomas B&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Ren&#xe8; Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belbachir_A/0/1/0/all/0/1\">Ahmed Nabil Belbachir</a>",
          "description": "Computing systems form the backbone of many aspects of our life, hence they\nare becoming as vital as water, electricity, and road infrastructures for our\nsociety. Yet, engineering long running computing systems that achieve their\ngoals in ever-changing environments pose significant challenges. Currently, we\ncan build computing systems that adjust or learn over time to match changes\nthat were anticipated. However, dealing with unanticipated changes, such as\nanomalies, novelties, new goals or constraints, requires system evolution,\nwhich remains in essence a human-driven activity. Given the growing complexity\nof computing systems and the vast amount of highly complex data to process,\nthis approach will eventually become unmanageable. To break through the status\nquo, we put forward a new paradigm for the design and operation of computing\nsystems that we coin \"lifelong computing.\" The paradigm starts from\ncomputing-learning systems that integrate computing/service modules and\nlearning modules. Computing warehouses offer such computing elements together\nwith data sheets and usage guides. When detecting anomalies, novelties, new\ngoals or constraints, a lifelong computing system activates an evolutionary\nself-learning engine that runs online experiments to determine how the\ncomputing-learning system needs to evolve to deal with the changes, thereby\nchanging its architecture and integrating new computing elements from computing\nwarehouses as needed. Depending on the domain at hand, some activities of\nlifelong computing systems can be supported by humans. We motivate the need for\nlifelong computing with a future fish farming scenario, outline a blueprint\narchitecture for lifelong computing systems, and highlight key research\nchallenges to realise the vision of lifelong computing.",
          "link": "http://arxiv.org/abs/2108.08802",
          "publishedOn": "2021-08-20T01:53:53.590Z",
          "wordCount": 683,
          "title": "Lifelong Computing. (arXiv:2108.08802v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2006.12245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bateni_P/0/1/0/all/0/1\">Peyman Bateni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_J/0/1/0/all/0/1\">Jarred Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "We develop a transductive meta-learning method that uses unlabelled instances\nto improve few-shot image classification performance. Our approach combines a\nregularized Mahalanobis-distance-based soft k-means clustering procedure with a\nmodified state of the art neural adaptive feature extractor to achieve improved\ntest-time classification accuracy using unlabelled data. We evaluate our method\non transductive few-shot learning tasks, in which the goal is to jointly\npredict labels for query (test) examples given a set of support (training)\nexamples. We achieve state-of-the-art performance on the Meta-Dataset,\nmini-ImageNet and tiered-ImageNet benchmarks.",
          "link": "http://arxiv.org/abs/2006.12245",
          "publishedOn": "2021-08-20T01:53:53.584Z",
          "wordCount": 587,
          "title": "Enhancing Few-Shot Image Classification with Unlabelled Examples. (arXiv:2006.12245v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04921",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wellawatte_G/0/1/0/all/0/1\">Geemi P. Wellawatte</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chakraborty_M/0/1/0/all/0/1\">Maghesree Chakraborty</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gandhi_H/0/1/0/all/0/1\">Heta A. Gandhi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+White_A/0/1/0/all/0/1\">Andrew D. White</a>",
          "description": "The selection of coarse-grained (CG) mapping operators is a critical step for\nCG molecular dynamics (MD) simulation. It is still an open question about what\nis optimal for this choice and there is a need for theory. The current\nstate-of-the art method is mapping operators manually selected by experts. In\nthis work, we demonstrate an automated approach by viewing this problem as\nsupervised learning where we seek to reproduce the mapping operators produced\nby experts. We present a graph neural network based CG mapping predictor called\nDEEP SUPERVISED GRAPH PARTITIONING MODEL(DSGPM) that treats mapping operators\nas a graph segmentation problem. DSGPM is trained on a novel dataset,\nHuman-annotated Mappings (HAM), consisting of 1,206 molecules with expert\nannotated mapping operators. HAM can be used to facilitate further research in\nthis area. Our model uses a novel metric learning objective to produce\nhigh-quality atomic features that are used in spectral clustering. The results\nshow that the DSGPM outperforms state-of-the-art methods in the field of graph\nsegmentation. Finally, we find that predicted CG mapping operators indeed\nresult in good CG MD models when used in simulation.",
          "link": "http://arxiv.org/abs/2007.04921",
          "publishedOn": "2021-08-20T01:53:53.577Z",
          "wordCount": 656,
          "title": "Graph Neural Network Based Coarse-Grained Mapping Prediction. (arXiv:2007.04921v3 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10253",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_C/0/1/0/all/0/1\">Chenyi Zhang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Leng_J/0/1/0/all/0/1\">Jiaqi Leng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_T/0/1/0/all/0/1\">Tongyang Li</a>",
          "description": "We initiate the study of quantum algorithms for escaping from saddle points\nwith provable guarantee. Given a function $f\\colon\\mathbb{R}^{n}\\to\\mathbb{R}$,\nour quantum algorithm outputs an $\\epsilon$-approximate second-order stationary\npoint using $\\tilde{O}(\\log^{2} (n)/\\epsilon^{1.75})$ queries to the quantum\nevaluation oracle (i.e., the zeroth-order oracle). Compared to the classical\nstate-of-the-art algorithm by Jin et al. with $\\tilde{O}(\\log^{6}\n(n)/\\epsilon^{1.75})$ queries to the gradient oracle (i.e., the first-order\noracle), our quantum algorithm is polynomially better in terms of $\\log n$ and\nmatches its complexity in terms of $1/\\epsilon$. Technically, our main\ncontribution is the idea of replacing the classical perturbations in gradient\ndescent methods by simulating quantum wave equations, which constitutes the\nimprovement in the quantum query complexity with $\\log n$ factors for escaping\nfrom saddle points. We also show how to use a quantum gradient computation\nalgorithm due to Jordan to replace the classical gradient queries by quantum\nevaluation queries with the same complexity. Finally, we also perform numerical\nexperiments that support our theoretical findings.",
          "link": "http://arxiv.org/abs/2007.10253",
          "publishedOn": "2021-08-20T01:53:53.569Z",
          "wordCount": 634,
          "title": "Quantum algorithms for escaping from saddle points. (arXiv:2007.10253v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08752",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Feng_D/0/1/0/all/0/1\">Dai Feng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Baumgartner_R/0/1/0/all/0/1\">Richard Baumgartner</a>",
          "description": "Kernels ensuing from tree ensembles such as random forest (RF) or gradient\nboosted trees (GBT), when used for kernel learning, have been shown to be\ncompetitive to their respective tree ensembles (particularly in higher\ndimensional scenarios). On the other hand, it has been also shown that\nperformance of the kernel algorithms depends on the degree of the kernel-target\nalignment. However, the kernel-target alignment for kernel learning based on\nthe tree ensembles has not been investigated and filling this gap is the main\ngoal of our work.\n\nUsing the eigenanalysis of the kernel matrix, we demonstrate that for\ncontinuous targets good performance of the tree-based kernel learning is\nassociated with strong kernel-target alignment. Moreover, we show that well\nperforming tree ensemble based kernels are characterized by strong target\naligned components that are expressed through scalar products between the\neigenvectors of the kernel matrix and the target. This suggests that when tree\nensemble based kernel learning is successful, relevant information for the\nsupervised problem is concentrated near lower dimensional manifold spanned by\nthe target aligned components. Persistence of the strong target aligned\ncomponents in tree ensemble based kernels is further supported by sensitivity\nanalysis via landmark learning. In addition to a comprehensive simulation\nstudy, we also provide experimental results from several real life data sets\nthat are in line with the simulations.",
          "link": "http://arxiv.org/abs/2108.08752",
          "publishedOn": "2021-08-20T01:53:53.562Z",
          "wordCount": 661,
          "title": "A Framework for an Assessment of the Kernel-target Alignment in Tree Ensemble Kernel Learning. (arXiv:2108.08752v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00570",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Peng_T/0/1/0/all/0/1\">Tommy Peng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Malik_A/0/1/0/all/0/1\">Avinash Malik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bear_L/0/1/0/all/0/1\">Laura R. Bear</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Trew_M/0/1/0/all/0/1\">Mark L. Trew</a>",
          "description": "The proposed method re-frames traditional inverse problems of\nelectrocardiography into regression problems, constraining the solution space\nby decomposing signals with multidimensional Gaussian impulse basis functions.\nImpulse HSPs were generated with single Gaussian basis functions at discrete\nheart surface locations and projected to corresponding BSPs using a volume\nconductor torso model. Both BSP (inputs) and HSP (outputs) were mapped to\nregular 2D surface meshes and used to train a neural network. Predictive\ncapabilities of the network were tested with unseen synthetic and experimental\ndata. A dense full connected single hidden layer neural network was trained to\nmap body surface impulses to heart surface Gaussian basis functions for\nreconstructing HSP. Synthetic pulses moving across the heart surface were\npredicted from the neural network with root mean squared error of $9.1\\pm1.4$%.\nPredicted signals were robust to noise up to 20 dB and errors due to\ndisplacement and rotation of the heart within the torso were bounded and\npredictable. A shift of the heart 40 mm toward the spine resulted in a 4\\%\nincrease in signal feature localization error. The set of training impulse\nfunction data could be reduced and prediction error remained bounded. Recorded\nHSPs from in-vitro pig hearts were reliably decomposed using space-time\nGaussian basis functions. Predicted HSPs for left-ventricular pacing had a mean\nabsolute error of $10.4\\pm11.4$ ms. Other pacing scenarios were analyzed with\nsimilar success. Conclusion: Impulses from Gaussian basis functions are\npotentially an effective and robust way to train simple neural network data\nmodels for reconstructing HSPs from decomposed BSPs. The HSPs predicted by the\nneural network can be used to generate activation maps that non-invasively\nidentify features of cardiac electrical dysfunction and can guide subsequent\ntreatment options.",
          "link": "http://arxiv.org/abs/2102.00570",
          "publishedOn": "2021-08-20T01:53:53.545Z",
          "wordCount": 755,
          "title": "Impulse data models for the inverse problem of electrocardiography. (arXiv:2102.00570v3 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1\">Klemen Kotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>",
          "description": "In the past few years, we have witnessed remarkable breakthroughs in\nself-supervised representation learning. Despite the success and adoption of\nrepresentations learned through this paradigm, much is yet to be understood\nabout how different training methods and datasets influence performance on\ndownstream tasks. In this paper, we analyze contrastive approaches as one of\nthe most successful and popular variants of self-supervised representation\nlearning. We perform this analysis from the perspective of the training\nalgorithms, pre-training datasets and end tasks. We examine over 700 training\nexperiments including 30 encoders, 4 pre-training datasets and 20 diverse\ndownstream tasks. Our experiments address various questions regarding the\nperformance of self-supervised models compared to their supervised\ncounterparts, current benchmarks used for evaluation, and the effect of the\npre-training data on end task performance. Our Visual Representation Benchmark\n(ViRB) is available at: https://github.com/allenai/virb.",
          "link": "http://arxiv.org/abs/2103.14005",
          "publishedOn": "2021-08-20T01:53:53.538Z",
          "wordCount": 608,
          "title": "Contrasting Contrastive Self-Supervised Representation Learning Pipelines. (arXiv:2103.14005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1\">Megh Shukla</a>",
          "description": "Active learning algorithms select a subset of data for annotation to maximize\nthe model performance on a budget. One such algorithm is Expected Gradient\nLength, which as the name suggests uses the approximate gradient induced per\nexample in the sampling process. While Expected Gradient Length has been\nsuccessfully used for classification and regression, the formulation for\nregression remains intuitively driven. Hence, our theoretical contribution\ninvolves deriving this formulation, thereby supporting the experimental\nevidence. Subsequently, we show that expected gradient length in regression is\nequivalent to Bayesian uncertainty. If certain assumptions are infeasible, our\nalgorithmic contribution (EGL++) approximates the effect of ensembles with a\nsingle deterministic network. Instead of computing multiple possible inferences\nper input, we leverage previously annotated samples to quantify the probability\nof previous labels being the true label. Such an approach allows us to extend\nexpected gradient length to a new task: human pose estimation. We perform\nexperimental validation on two human pose datasets (MPII and LSP/LSPET),\nhighlighting the interpretability and competitiveness of EGL++ with different\nactive learning algorithms for human pose estimation.",
          "link": "http://arxiv.org/abs/2104.09493",
          "publishedOn": "2021-08-20T01:53:53.531Z",
          "wordCount": 649,
          "title": "Bayesian Uncertainty and Expected Gradient Length - Regression: Two Sides Of The Same Coin?. (arXiv:2104.09493v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1\">Shay Vargaftik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basat_R/0/1/0/all/0/1\">Ran Ben Basat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portnoy_A/0/1/0/all/0/1\">Amit Portnoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendelson_G/0/1/0/all/0/1\">Gal Mendelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Itzhak_Y/0/1/0/all/0/1\">Yaniv Ben-Itzhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitzenmacher_M/0/1/0/all/0/1\">Michael Mitzenmacher</a>",
          "description": "Federated learning commonly relies on algorithms such as distributed\n(mini-batch) SGD, where multiple clients compute their gradients and send them\nto a central coordinator for averaging and updating the model. To optimize the\ntransmission time and the scalability of the training process, clients often\nuse lossy compression to reduce the message sizes. DRIVE is a recent state of\nthe art algorithm that compresses gradients using one bit per coordinate (with\nsome lower-order overhead). In this technical report, we generalize DRIVE to\nsupport any bandwidth constraint as well as extend it to support heterogeneous\nclient resources and make it robust to packet loss.",
          "link": "http://arxiv.org/abs/2108.08842",
          "publishedOn": "2021-08-20T01:53:53.524Z",
          "wordCount": 561,
          "title": "Communication-Efficient Federated Learning via Robust Distributed Mean Estimation. (arXiv:2108.08842v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08775",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sankar_R/0/1/0/all/0/1\">Rahul Sankar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhudev_A/0/1/0/all/0/1\">Amithash M Prabhudev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahesh_P/0/1/0/all/0/1\">P A Mahesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prakashini_K/0/1/0/all/0/1\">K Prakashini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sudha Kiran Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "The world is going through a challenging phase due to the disastrous effect\ncaused by the COVID-19 pandemic on the healthcare system and the economy. The\nrate of spreading, post-COVID-19 symptoms, and the occurrence of new strands of\nCOVID-19 have put the healthcare systems in disruption across the globe. Due to\nthis, the task of accurately screening COVID-19 cases has become of utmost\npriority. Since the virus infects the respiratory system, Chest X-Ray is an\nimaging modality that is adopted extensively for the initial screening. We have\nperformed a comprehensive study that uses CXR images to identify COVID-19 cases\nand realized the necessity of having a more generalizable model. We utilize\nMobileNetV2 architecture as the feature extractor and integrate it into Capsule\nNetworks to construct a fully automated and lightweight model termed as\nMobileCaps. MobileCaps is trained and evaluated on the publicly available\ndataset with the model ensembling and Bayesian optimization strategies to\nefficiently classify CXR images of patients with COVID-19 from non-COVID-19\npneumonia and healthy cases. The proposed model is further evaluated on two\nadditional RT-PCR confirmed datasets to demonstrate the generalizability. We\nalso introduce MobileCaps-S and leverage it for performing severity assessment\nof CXR images of COVID-19 based on the Radiographic Assessment of Lung Edema\n(RALE) scoring technique. Our classification model achieved an overall recall\nof 91.60, 94.60, 92.20, and a precision of 98.50, 88.21, 92.62 for COVID-19,\nnon-COVID-19 pneumonia, and healthy cases, respectively. Further, the severity\nassessment model attained an R$^2$ coefficient of 70.51. Owing to the fact that\nthe proposed models have fewer trainable parameters than the state-of-the-art\nmodels reported in the literature, we believe our models will go a long way in\naiding healthcare systems in the battle against the pandemic.",
          "link": "http://arxiv.org/abs/2108.08775",
          "publishedOn": "2021-08-20T01:53:53.518Z",
          "wordCount": 814,
          "title": "MobileCaps: A Lightweight Model for Screening and Severity Analysis of COVID-19 Chest X-Ray Images. (arXiv:2108.08775v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okerinde_A/0/1/0/all/0/1\">Ademola Okerinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">William Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theis_T/0/1/0/all/0/1\">Tom Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nafi_N/0/1/0/all/0/1\">Nasik Nafi</a>",
          "description": "Class imbalance is an inherent problem in many machine learning\nclassification tasks. This often leads to trained models that are unusable for\nany practical purpose. In this study we explore an unsupervised approach to\naddress these imbalances by leveraging transfer learning from pre-trained image\nclassification models to encoder-based Generative Adversarial Network (eGAN).\nTo the best of our knowledge, this is the first work to tackle this problem\nusing GAN without needing to augment with synthesized fake images.\n\nIn the proposed approach we use the discriminator network to output a\nnegative or positive score. We classify as minority, test samples with negative\nscores and as majority those with positive scores. Our approach eliminates\nepistemic uncertainty in model predictions, as the P(minority) + P(majority)\nneed not sum up to 1. The impact of transfer learning and combinations of\ndifferent pre-trained image classification models at the generator and\ndiscriminator is also explored. Best result of 0.69 F1-score was obtained on\nCIFAR-10 classification task with imbalance ratio of 1:2500.\n\nOur approach also provides a mechanism of thresholding the specificity or\nsensitivity of our machine learning system. Keywords: Class imbalance, Transfer\nLearning, GAN, nash equilibrium",
          "link": "http://arxiv.org/abs/2104.04162",
          "publishedOn": "2021-08-20T01:53:53.501Z",
          "wordCount": 675,
          "title": "eGAN: Unsupervised approach to class imbalance using transfer learning. (arXiv:2104.04162v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cawood_P/0/1/0/all/0/1\">Pieter Cawood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1\">Terence L. van Zyl</a>",
          "description": "We investigate ensembling techniques in forecasting and examine their\npotential for use in nonseasonal time-series similar to those in the early days\nof the COVID-19 pandemic. Developing improved forecast methods is essential as\nthey provide data-driven decisions to organisations and decision-makers during\ncritical phases. We propose using late data fusion, using a stacked ensemble of\ntwo forecasting models and two meta-features that prove their predictive power\nduring a preliminary forecasting stage. The final ensembles include a Prophet\nand long short term memory (LSTM) neural network as base models. The base\nmodels are combined by a multilayer perceptron (MLP), taking into account\nmeta-features that indicate the highest correlation with each base model's\nforecast accuracy. We further show that the inclusion of meta-features\ngenerally improves the ensemble's forecast accuracy across two forecast\nhorizons of seven and fourteen days. This research reinforces previous work and\ndemonstrates the value of combining traditional statistical models with deep\nlearning models to produce more accurate forecast models for time-series across\ndomains.",
          "link": "http://arxiv.org/abs/2108.08723",
          "publishedOn": "2021-08-20T01:53:53.495Z",
          "wordCount": 654,
          "title": "Feature-weighted Stacking for Nonseasonal Time Series Forecasts: A Case Study of the COVID-19 Epidemic Curves. (arXiv:2108.08723v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1\">Meena Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_A/0/1/0/all/0/1\">Alexander Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "Large-scale, two-sided matching platforms must find market outcomes that\nalign with user preferences while simultaneously learning these preferences\nfrom data. However, since preferences are inherently uncertain during learning,\nthe classical notion of stability (Gale and Shapley, 1962; Shapley and Shubik,\n1971) is unattainable in these settings. To bridge this gap, we develop a\nframework and algorithms for learning stable market outcomes under uncertainty.\nOur primary setting is matching with transferable utilities, where the platform\nboth matches agents and sets monetary transfers between them. We design an\nincentive-aware learning objective that captures the distance of a market\noutcome from equilibrium. Using this objective, we analyze the complexity of\nlearning as a function of preference structure, casting learning as a\nstochastic multi-armed bandit problem. Algorithmically, we show that \"optimism\nin the face of uncertainty,\" the principle underlying many bandit algorithms,\napplies to a primal-dual formulation of matching with transfers and leads to\nnear-optimal regret bounds. Our work takes a first step toward elucidating when\nand how stable matchings arise in large, data-driven marketplaces.",
          "link": "http://arxiv.org/abs/2108.08843",
          "publishedOn": "2021-08-20T01:53:53.488Z",
          "wordCount": 616,
          "title": "Learning Equilibria in Matching Markets from Bandit Feedback. (arXiv:2108.08843v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1805.05052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1\">Alexander Jung</a>",
          "description": "Machine learning (ML) has become a commodity in our every-day lives. We\nroutinely ask ML empowered smartphones to suggest lovely food places or to\nguide us through a strange place. ML methods have also become standard tools in\nmany fields of science and engineering. A plethora of ML applications transform\nhuman lives at unprecedented pace and scale. This book portrays ML as the\ncombination of three basic components: data, model and loss. ML methods combine\nthese three components within computationally efficient implementations of the\nbasic scientific principle \"trial and error\". This principle consists of the\ncontinuous adaptation of a hypothesis about a phenomenon that generates data.\nML methods use a hypothesis to compute predictions for future events. We\nbelieve that thinking about ML as combinations of three components given by\ndata, model, and loss helps to navigate the steadily growing offer for\nready-to-use ML methods. Our three-component picture of ML allows a unified\ntreatment of a wide range of concepts and techniques which seem quite unrelated\nat first sight. The regularization effect of early stopping in iterative\nmethods is due to the shrinking of the effective hypothesis space.\nPrivacy-preserving ML is obtained by particular choices for the features of\ndata points. Explainable ML methods are characterized by particular choices for\nthe hypothesis space. To make good use of ML tools it is instrumental to\nunderstand its underlying principles at different levels of detail. On a lower\nlevel, this tutorial helps ML engineers to choose suitable methods for the\napplication at hand. The book also offers a higher-level view on the\nimplementation of ML methods which is typically required to manage a team of ML\nengineers and data scientists.",
          "link": "http://arxiv.org/abs/1805.05052",
          "publishedOn": "2021-08-20T01:53:53.480Z",
          "wordCount": 842,
          "title": "Machine Learning: The Basics. (arXiv:1805.05052v14 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_O/0/1/0/all/0/1\">Ozioma Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McSharry_P/0/1/0/all/0/1\">Patrick McSharry</a>",
          "description": "Modelling, simulation, and forecasting offer a means of facilitating better\nplanning and decision-making. These quantitative approaches can add value\nbeyond traditional methods that do not rely on data and are particularly\nrelevant for public transportation. Lagos is experiencing rapid urbanization\nand currently has a population of just under 15 million. Both long waiting\ntimes and uncertain travel times has driven many people to acquire their own\nvehicle or use alternative modes of transport. This has significantly increased\nthe number of vehicles on the roads leading to even more traffic and greater\ntraffic congestion. This paper investigates urban travel demand in Lagos and\nexplores passenger dynamics in time and space. Using individual commuter trip\ndata from tickets purchased from the Lagos State Bus Rapid Transit (BRT), the\ndemand patterns through the hours of the day, days of the week and bus stations\nare analysed. This study aims to quantify demand from actual passenger trips\nand estimate the impact that dynamic scheduling could have on passenger waiting\ntimes. Station segmentation is provided to cluster stations by their demand\ncharacteristics in order to tailor specific bus schedules. Intra-day public\ntransportation demand in Lagos BRT is analysed and predictions are compared.\nSimulations using fixed and dynamic bus scheduling demonstrate that the average\nwaiting time could be reduced by as much as 80%. The load curves, insights and\nthe approach developed will be useful for informing policymaking in Lagos and\nsimilar African cities facing the challenges of rapid urbanization.",
          "link": "http://arxiv.org/abs/2105.11816",
          "publishedOn": "2021-08-20T01:53:53.472Z",
          "wordCount": 697,
          "title": "Public Transportation Demand Analysis: A Case Study of Metropolitan Lagos. (arXiv:2105.11816v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15245",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Dai_W/0/1/0/all/0/1\">Wenhan Dai</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhi Zeng</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Dou_D/0/1/0/all/0/1\">Daowei Dou</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Chen_J/0/1/0/all/0/1\">Jianping Chen</a> (1 and 2), <a href=\"http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1\">Junli Li</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a> (1) ((1) Department of Engineering Physics, Tsinghua University, Beijing, China, (2) College of Nuclear Science and Technology, Beijing Normal University, Beijing, China)",
          "description": "The monitoring of Cs-137 in seawater using scintillation detector relies on\nthe spectrum analysis method to extract the Cs-137 concentration. And when in\npoor statistic situation, the calculation result of the traditional net peak\narea (NPA) method has a large uncertainty. We present a machine learning based\nmethod to better analyze the gamma-ray spectrum with low Cs-137 concentration.\nWe apply multilayer perceptron (MLP) to analyze the 662 keV full energy peak of\nCs-137 in the seawater spectrum. And the MLP can be trained with a few measured\nbackground spectrums by combining the simulated Cs-137 signal with measured\nbackground spectrums. Thus, it can save the time of preparing and measuring the\nstandard samples for generating the training dataset. To validate the MLP-based\nmethod, we use Geant4 and background gamma-ray spectrums measured by a seaborne\nmonitoring device to generate an independent test dataset to test the result by\nour method and the traditional NPA method. We find that the MLP-based method\nachieves a root mean squared error of 0.159, 2.3 times lower than that of the\ntraditional net peak area method, indicating the MLP-based method improves the\nprecision of Cs-137 concentration calculation",
          "link": "http://arxiv.org/abs/2010.15245",
          "publishedOn": "2021-08-20T01:53:53.466Z",
          "wordCount": 719,
          "title": "A marine radioisotope gamma-ray spectrum analysis method based on Monte Carlo simulation and MLP neural network. (arXiv:2010.15245v2 [physics.ins-det] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00568",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Clavijo_J/0/1/0/all/0/1\">Jose M. Clavijo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Glaysher_P/0/1/0/all/0/1\">Paul Glaysher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katzy_J/0/1/0/all/0/1\">Judith M. Katzy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jitsev_J/0/1/0/all/0/1\">Jenia Jitsev</a>",
          "description": "We apply adversarial domain adaptation in unsupervised setting to reduce\nsample bias in a supervised high energy physics events classifier training. We\nmake use of a neural network containing event and domain classifier with a\ngradient reversal layer to simultaneously enable signal versus background\nevents classification on the one hand, while on the other hand minimising the\ndifference in response of the network to background samples originating from\ndifferent MC models via adversarial domain classification loss. We show the\nsuccessful bias removal on the example of simulated events at the LHC with\n$t\\bar{t}H$ signal versus $t\\bar{t}b\\bar{b}$ background classification and\ndiscuss implications and limitations of the method",
          "link": "http://arxiv.org/abs/2005.00568",
          "publishedOn": "2021-08-20T01:53:53.449Z",
          "wordCount": 600,
          "title": "Adversarial domain adaptation to reduce sample bias of a high energy physics classifier. (arXiv:2005.00568v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02081",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1\">Pierre Thodoroff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1\">Neil D. Lawrence</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1\">Austen Lamacraft</a>",
          "description": "The Schr\\\"odinger bridge problem (SBP) finds the most likely stochastic\nevolution between two probability distributions given a prior stochastic\nevolution. As well as applications in the natural sciences, problems of this\nkind have important applications in machine learning such as dataset alignment\nand hypothesis testing. Whilst the theory behind this problem is relatively\nmature, scalable numerical recipes to estimate the Schr\\\"odinger bridge remain\nan active area of research. We prove an equivalence between the SBP and maximum\nlikelihood estimation enabling direct application of successful machine\nlearning techniques. We propose a numerical procedure to estimate SBPs using\nGaussian process and demonstrate the practical usage of our approach in\nnumerical simulations and experiments.",
          "link": "http://arxiv.org/abs/2106.02081",
          "publishedOn": "2021-08-20T01:53:53.442Z",
          "wordCount": 594,
          "title": "Solving Schr\\\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1\">David LeBauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1\">Max Burnette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1\">Noah Fahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1\">Rob Kooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1\">Kenton McHenry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "A core objective of the TERRA-REF project was to generate an open-access\nreference dataset for the evaluation of sensing technologies to study plants\nunder field conditions. The TERRA-REF program deployed a suite of\nhigh-resolution, cutting edge technology sensors on a gantry system with the\naim of scanning 1 hectare (10$^4$) at around 1 mm$^2$ spatial resolution\nmultiple times per week. The system contains co-located sensors including a\nstereo-pair RGB camera, a thermal imager, a laser scanner to capture 3D\nstructure, and two hyperspectral cameras covering wavelengths of 300-2500nm.\nThis sensor data is provided alongside over sixty types of traditional plant\nphenotype measurements that can be used to train new machine learning models.\nAssociated weather and environmental measurements, information about agronomic\nmanagement and experimental design, and the genomic sequences of hundreds of\nplant varieties have been collected and are available alongside the sensor and\nplant phenotype data.\n\nOver the course of four years and ten growing seasons, the TERRA-REF system\ngenerated over 1 PB of sensor data and almost 45 million files. The subset that\nhas been released to the public domain accounts for two seasons and about half\nof the total data volume. This provides an unprecedented opportunity for\ninvestigations far beyond the core biological scope of the project.\n\nThe focus of this paper is to provide the Computer Vision and Machine\nLearning communities an overview of the available data and some potential\napplications of this one of a kind data.",
          "link": "http://arxiv.org/abs/2107.14072",
          "publishedOn": "2021-08-20T01:53:52.906Z",
          "wordCount": 737,
          "title": "What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1\">Andrea Zanette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wainwright_M/0/1/0/all/0/1\">Martin J. Wainwright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>",
          "description": "Actor-critic methods are widely used in offline reinforcement learning\npractice, but are not so well-understood theoretically. We propose a new\noffline actor-critic algorithm that naturally incorporates the pessimism\nprinciple, leading to several key advantages compared to the state of the art.\nThe algorithm can operate when the Bellman evaluation operator is closed with\nrespect to the action value function of the actor's policies; this is a more\ngeneral setting than the low-rank MDP model. Despite the added generality, the\nprocedure is computationally tractable as it involves the solution of a\nsequence of second-order programs. We prove an upper bound on the suboptimality\ngap of the policy returned by the procedure that depends on the data coverage\nof any arbitrary, possibly data dependent comparator policy. The achievable\nguarantee is complemented with a minimax lower bound that is matching up to\nlogarithmic factors.",
          "link": "http://arxiv.org/abs/2108.08812",
          "publishedOn": "2021-08-20T01:53:52.892Z",
          "wordCount": 588,
          "title": "Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning. (arXiv:2108.08812v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>",
          "description": "Adversarial attack algorithms are dominated by penalty methods, which are\nslow in practice, or more efficient distance-customized methods, which are\nheavily tailored to the properties of the distance considered. We propose a\nwhite-box attack algorithm to generate minimally perturbed adversarial examples\nbased on Augmented Lagrangian principles. We bring several algorithmic\nmodifications, which have a crucial effect on performance. Our attack enjoys\nthe generality of penalty methods and the computational efficiency of\ndistance-customized algorithms, and can be readily used for a wide set of\ndistances. We compare our attack to state-of-the-art methods on three datasets\nand several models, and consistently obtain competitive performances with\nsimilar or lower computational complexity.",
          "link": "http://arxiv.org/abs/2011.11857",
          "publishedOn": "2021-08-20T01:53:52.886Z",
          "wordCount": 578,
          "title": "Augmented Lagrangian Adversarial Attacks. (arXiv:2011.11857v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02283",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Sun_N/0/1/0/all/0/1\">Ningning Sun</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wells_M/0/1/0/all/0/1\">Martin T. Wells</a>",
          "description": "This paper builds the clustering model of measures of market microstructure\nfeatures which are popular in predicting the stock returns. In a 10-second time\nfrequency, we study the clustering structure of different measures to find out\nthe best ones for predicting. In this way, we can predict more accurately with\na limited number of predictors, which removes the noise and makes the model\nmore interpretable.",
          "link": "http://arxiv.org/abs/2107.02283",
          "publishedOn": "2021-08-20T01:53:52.880Z",
          "wordCount": 514,
          "title": "Clustering Structure of Microstructure Measures. (arXiv:2107.02283v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_K/0/1/0/all/0/1\">Kushal Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1\">Pradeep Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_D/0/1/0/all/0/1\">Devarajan Sridharan</a>",
          "description": "Deep networks often make confident, yet incorrect, predictions when tested\nwith outlier data that is far removed from their training distributions.\nLikelihoods computed by deep generative models are a candidate metric for\noutlier detection with unlabeled data. Yet, previous studies have shown that\nsuch likelihoods are unreliable and can be easily biased by simple\ntransformations to input data. Here, we examine outlier detection with\nvariational autoencoders (VAEs), among the simplest class of deep generative\nmodels. First, we show that a theoretically-grounded correction readily\nameliorates a key bias with VAE likelihood estimates. The bias correction is\nmodel-free, sample-specific, and accurately computed with the Bernoulli and\ncontinuous Bernoulli visible distributions. Second, we show that a well-known\npreprocessing technique, contrast normalization, extends the effectiveness of\nbias correction to natural image datasets. Third, we show that the variance of\nthe likelihoods computed over an ensemble of VAEs also enables robust outlier\ndetection. We perform a comprehensive evaluation of our remedies with nine\n(grayscale and natural) image datasets, and demonstrate significant advantages,\nin terms of both speed and accuracy, over four other state-of-the-art methods.\nOur lightweight remedies are biologically inspired and may serve to achieve\nefficient outlier detection with many types of deep generative models.",
          "link": "http://arxiv.org/abs/2108.08760",
          "publishedOn": "2021-08-20T01:53:52.869Z",
          "wordCount": 645,
          "title": "Efficient remedies for outlier detection with variational autoencoders. (arXiv:2108.08760v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perumal_R/0/1/0/all/0/1\">Rylan Perumal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1\">Terence L van Zyl</a>",
          "description": "Parameter calibration is a significant challenge in agent-based modelling and\nsimulation (ABMS). An agent-based model's (ABM) complexity grows as the number\nof parameters required to be calibrated increases. This parameter expansion\nleads to the ABMS equivalent of the \\say{curse of dimensionality}. In\nparticular, infeasible computational requirements searching an infinite\nparameter space. We propose a more comprehensive and adaptive ABMS Framework\nthat can effectively swap out parameterisation strategies and surrogate models\nto parameterise an infectious disease ABM. This framework allows us to evaluate\ndifferent strategy-surrogate combinations' performance in accuracy and\nefficiency (speedup). We show that we achieve better than parity in accuracy\nacross the surrogate assisted sampling strategies and the baselines. Also, we\nidentify that the Metric Stochastic Response Surface strategy combined with the\nSupport Vector Machine surrogate is the best overall in getting closest to the\ntrue synthetic parameters. Also, we show that DYnamic COOrdindate Search Using\nResponse Surface Models with XGBoost as a surrogate attains in combination the\nhighest probability of approximating a cumulative synthetic daily infection\ndata distribution and achieves the most significant speedup with regards to our\nanalysis. Lastly, we show in a real-world setting that DYCORS XGBoost and MSRS\nSVM can approximate the real world cumulative daily infection distribution with\n$97.12$\\% and $96.75$\\% similarity respectively.",
          "link": "http://arxiv.org/abs/2108.08809",
          "publishedOn": "2021-08-20T01:53:52.863Z",
          "wordCount": 651,
          "title": "Surrogate Assisted Strategies (The Parameterisation of an Infectious Disease Agent-Based Model). (arXiv:2108.08809v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neill_D/0/1/0/all/0/1\">Daniel B. Neill</a>",
          "description": "Machine learning is gaining popularity in a broad range of areas working with\ngeographic data, such as ecology or atmospheric sciences. Here, data often\nexhibit spatial effects, which can be difficult to learn for neural networks.\nIn this study, we propose SXL, a method for embedding information on the\nautoregressive nature of spatial data directly into the learning process using\nauxiliary tasks. We utilize the local Moran's I, a popular measure of local\nspatial autocorrelation, to \"nudge\" the model to learn the direction and\nmagnitude of local spatial effects, complementing the learning of the primary\ntask. We further introduce a novel expansion of Moran's I to multiple\nresolutions, thus capturing spatial interactions over longer and shorter\ndistances simultaneously. The novel multi-resolution Moran's I can be\nconstructed easily and as a multi-dimensional tensor offers seamless\nintegration into existing machine learning frameworks. Throughout a range of\nexperiments using real-world data, we highlight how our method consistently\nimproves the training of neural networks in unsupervised and supervised\nlearning tasks. In generative spatial modeling experiments, we propose a novel\nloss for auxiliary task GANs utilizing task uncertainty weights. Our proposed\nmethod outperforms domain-specific spatial interpolation benchmarks,\nhighlighting its potential for downstream applications. This study bridges\nexpertise from geographic information science and machine learning, showing how\nthis integration of disciplines can help to address domain-specific challenges.\nThe code for our experiments is available on Github:\nhttps://github.com/konstantinklemmer/sxl.",
          "link": "http://arxiv.org/abs/2006.10461",
          "publishedOn": "2021-08-20T01:53:52.856Z",
          "wordCount": 712,
          "title": "Auxiliary-task learning for geographic data with autoregressive embeddings. (arXiv:2006.10461v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Attention mechanism has demonstrated great potential in fine-grained visual\nrecognition tasks. In this paper, we present a counterfactual attention\nlearning method to learn more effective attention based on causal inference.\nUnlike most existing methods that learn visual attention based on conventional\nlikelihood, we propose to learn the attention with counterfactual causality,\nwhich provides a tool to measure the attention quality and a powerful\nsupervisory signal to guide the learning process. Specifically, we analyze the\neffect of the learned visual attention on network prediction through\ncounterfactual intervention and maximize the effect to encourage the network to\nlearn more useful attention for fine-grained image recognition. Empirically, we\nevaluate our method on a wide range of fine-grained recognition tasks where\nattention plays a crucial role, including fine-grained image categorization,\nperson re-identification, and vehicle re-identification. The consistent\nimprovement on all benchmarks demonstrates the effectiveness of our method.\nCode is available at https://github.com/raoyongming/CAL",
          "link": "http://arxiv.org/abs/2108.08728",
          "publishedOn": "2021-08-20T01:53:52.849Z",
          "wordCount": 601,
          "title": "Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification. (arXiv:2108.08728v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.12571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xinjian Luo</a>",
          "description": "As a decentralized model training method, federated learning is designed to\nintegrate the isolated data islands and protect data privacy. Recent studies,\nhowever, have demonstrated that the Generative Adversarial Network (GAN) based\nattacks can be used in federated learning to learn the distribution of the\nvictim's private dataset and accordingly reconstruct human-distinguishable\nimages. In this paper, we exploit defenses against GAN-based attacks in\nfederated learning, and propose a framework, Anti-GAN, to prevent attackers\nfrom learning the real distribution of the victim's data. The core idea of\nAnti-GAN is to corrupt the visual features of the victim's private training\nimages, such that the images restored by the attacker are indistinguishable to\nhuman eyes. Specifically, in Anti-GAN, the victim first projects the personal\ndataset onto a GAN's generator, then mixes the fake images generated by the\ngenerator with the real images to obtain the training dataset, which will be\nfed into the federated model for training. We redesign the structure of the\nvictim's GAN to encourage it to learn the classification features (instead of\nthe visual features) of the real images. We further introduce an unsupervised\ntask to the GAN model for obfuscating the visual features of the generated\nimages. The experiments demonstrate that Anti-GAN can effectively prevent the\nattacker from learning the distribution of the private images, meanwhile\ncausing little harm to the accuracy of the federated model.",
          "link": "http://arxiv.org/abs/2004.12571",
          "publishedOn": "2021-08-20T01:53:52.842Z",
          "wordCount": 687,
          "title": "Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning. (arXiv:2004.12571v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1\">Ilya Makarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1\">Andrey Savchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korovko_A/0/1/0/all/0/1\">Arseny Korovko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherstyuk_L/0/1/0/all/0/1\">Leonid Sherstyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severin_N/0/1/0/all/0/1\">Nikita Severin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikheev_A/0/1/0/all/0/1\">Aleksandr Mikheev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babaev_D/0/1/0/all/0/1\">Dmitrii Babaev</a>",
          "description": "Many tasks in graph machine learning, such as link prediction and node\nclassification, are typically solved by using representation learning, in which\neach node or edge in the network is encoded via an embedding. Though there\nexists a lot of network embeddings for static graphs, the task becomes much\nmore complicated when the dynamic (i.e. temporal) network is analyzed. In this\npaper, we propose a novel approach for dynamic network representation learning\nbased on Temporal Graph Network by using a highly custom message generating\nfunction by extracting Causal Anonymous Walks. For evaluation, we provide a\nbenchmark pipeline for the evaluation of temporal network embeddings. This work\nprovides the first comprehensive comparison framework for temporal network\nrepresentation learning in every available setting for graph machine learning\nproblems involving node classification and link prediction. The proposed model\noutperforms state-of-the-art baseline models. The work also justifies the\ndifference between them based on evaluation in various transductive/inductive\nedge/node classification tasks. In addition, we show the applicability and\nsuperior performance of our model in the real-world downstream graph machine\nlearning task provided by one of the top European banks, involving credit\nscoring based on transaction data.",
          "link": "http://arxiv.org/abs/2108.08754",
          "publishedOn": "2021-08-20T01:53:52.797Z",
          "wordCount": 634,
          "title": "Temporal Graph Network Embedding with Causal Anonymous Walks Representations. (arXiv:2108.08754v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_K/0/1/0/all/0/1\">Kaan Gokcesu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_H/0/1/0/all/0/1\">Hakan Gokcesu</a>",
          "description": "In this work, we aim to calibrate the score outputs of an estimator for the\nbinary classification problem by finding an 'optimal' mapping to class\nprobabilities, where the 'optimal' mapping is in the sense that minimizes the\nclassification error (or equivalently, maximizes the accuracy). We show that\nfor the given target variables and the score outputs of an estimator, an\n'optimal' soft mapping, which monotonically maps the score values to\nprobabilities, is a hard mapping that maps the score values to $0$ and $1$. We\nshow that for class weighted (where the accuracy for one class is more\nimportant) and sample weighted (where the samples' accurate classifications are\nnot equally important) errors, or even general linear losses; this hard mapping\ncharacteristic is preserved. We propose a sequential recursive merger approach,\nwhich produces an 'optimal' hard mapping (for the observed samples so far)\nsequentially with each incoming new sample. Our approach has a logarithmic in\nsample size time complexity, which is optimally efficient.",
          "link": "http://arxiv.org/abs/2108.08780",
          "publishedOn": "2021-08-20T01:53:52.787Z",
          "wordCount": 599,
          "title": "Optimally Efficient Sequential Calibration of Binary Classifiers to Minimize Classification Error. (arXiv:2108.08780v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08687",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Craig_K/0/1/0/all/0/1\">Katy Craig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Trillos_N/0/1/0/all/0/1\">Nicol&#xe1;s Garc&#xed;a Trillos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Slepcev_D/0/1/0/all/0/1\">Dejan Slep&#x10d;ev</a>",
          "description": "In this work we build a unifying framework to interpolate between\ndensity-driven and geometry-based algorithms for data clustering, and\nspecifically, to connect the mean shift algorithm with spectral clustering at\ndiscrete and continuum levels. We seek this connection through the introduction\nof Fokker-Planck equations on data graphs. Besides introducing new forms of\nmean shift algorithms on graphs, we provide new theoretical insights on the\nbehavior of the family of diffusion maps in the large sample limit as well as\nprovide new connections between diffusion maps and mean shift dynamics on a\nfixed graph. Several numerical examples illustrate our theoretical findings and\nhighlight the benefits of interpolating density-driven and geometry-based\nclustering algorithms.",
          "link": "http://arxiv.org/abs/2108.08687",
          "publishedOn": "2021-08-20T01:53:52.779Z",
          "wordCount": 571,
          "title": "Clustering dynamics on graphs: from spectral clustering to mean shift through Fokker-Planck interpolation. (arXiv:2108.08687v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuyue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>",
          "description": "In generative adversarial imitation learning (GAIL), the agent aims to learn\na policy from an expert demonstration so that its performance cannot be\ndiscriminated from the expert policy on a certain predefined reward set. In\nthis paper, we study GAIL in both online and offline settings with linear\nfunction approximation, where both the transition and reward function are\nlinear in the feature maps. Besides the expert demonstration, in the online\nsetting the agent can interact with the environment, while in the offline\nsetting the agent only accesses an additional dataset collected by a prior. For\nonline GAIL, we propose an optimistic generative adversarial policy\noptimization algorithm (OGAP) and prove that OGAP achieves\n$\\widetilde{\\mathcal{O}}(H^2 d^{3/2}K^{1/2}+KH^{3/2}dN_1^{-1/2})$ regret. Here\n$N_1$ represents the number of trajectories of the expert demonstration, $d$ is\nthe feature dimension, and $K$ is the number of episodes.\n\nFor offline GAIL, we propose a pessimistic generative adversarial policy\noptimization algorithm (PGAP). For an arbitrary additional dataset, we obtain\nthe optimality gap of PGAP, achieving the minimax lower bound in the\nutilization of the additional dataset. Assuming sufficient coverage on the\nadditional dataset, we show that PGAP achieves\n$\\widetilde{\\mathcal{O}}(H^{2}dK^{-1/2}\n+H^2d^{3/2}N_2^{-1/2}+H^{3/2}dN_1^{-1/2} \\ )$ optimality gap. Here $N_2$\nrepresents the number of trajectories of the additional dataset with sufficient\ncoverage.",
          "link": "http://arxiv.org/abs/2108.08765",
          "publishedOn": "2021-08-20T01:53:52.766Z",
          "wordCount": 674,
          "title": "Provably Efficient Generative Adversarial Imitation Learning for Online and Offline Setting with Linear Function Approximation. (arXiv:2108.08765v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08818",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>",
          "description": "We introduce the notion of Point in Time Economic Scenario Generation (PiT\nESG) with a clear mathematical problem formulation to unify and compare\neconomic scenario generation approaches conditional on forward looking market\ndata. Such PiT ESGs should provide quicker and more flexible reactions to\nsudden economic changes than traditional ESGs calibrated solely to long periods\nof historical data. We specifically take as economic variable the S&P500 Index\nwith the VIX Index as forward looking market data to compare the nonparametric\nfiltered historical simulation, GARCH model with joint likelihood estimation\n(parametric), Restricted Boltzmann Machine and the conditional Variational\nAutoencoder (Generative Networks) for their suitability as PiT ESG. Our\nevaluation consists of statistical tests for model fit and benchmarking the out\nof sample forecasting quality with a strategy backtest using model output as\nstop loss criterion. We find that both Generative Networks outperform the\nnonparametric and classic parametric model in our tests, but that the CVAE\nseems to be particularly well suited for our purposes: yielding more robust\nperformance and being computationally lighter.",
          "link": "http://arxiv.org/abs/2108.08818",
          "publishedOn": "2021-08-20T01:53:52.748Z",
          "wordCount": 617,
          "title": "Discriminating modelling approaches for Point in Time Economic Scenario Generation. (arXiv:2108.08818v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1\">Uriel Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>",
          "description": "Graph neural networks (GNNs), has been widely used for supervised learning\ntasks in graphs reaching state-of-the-art results. However, little work was\ndedicated to creating unbiased GNNs, i.e., where the classification is\nuncorrelated with sensitive attributes, such as race or gender. Some ignore the\nsensitive attributes or optimize for the criteria of statistical parity for\nfairness. However, it has been shown that neither approaches ensure fairness,\nbut rather cripple the utility of the prediction task. In this work, we present\na GNN framework that allows optimizing representations for the notion of\nEqualized Odds fairness criteria. The architecture is composed of three\ncomponents: (1) a GNN classifier predicting the utility class, (2) a sampler\nlearning the distribution of the sensitive attributes of the nodes given their\nlabels. It generates samples fed into a (3) discriminator that discriminates\nbetween true and sampled sensitive attributes using a novel \"permutation loss\"\nfunction. Using these components, we train a model to neglect information\nregarding the sensitive attribute only with respect to its label. To the best\nof our knowledge, we are the first to optimize GNNs for the equalized odds\ncriteria. We evaluate our classifier over several graph datasets and sensitive\nattributes and show our algorithm reaches state-of-the-art results.",
          "link": "http://arxiv.org/abs/2108.08800",
          "publishedOn": "2021-08-20T01:53:52.734Z",
          "wordCount": 637,
          "title": "EqGNN: Equalized Node Opportunity in Graphs. (arXiv:2108.08800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1\">Tobias Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1\">Silvan Mertes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangelova_S/0/1/0/all/0/1\">Stanislava Rangelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flutura_S/0/1/0/all/0/1\">Simon Flutura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>",
          "description": "Virtual Reality (VR) games that feature physical activities have been shown\nto increase players' motivation to do physical exercise. However, for such\nexercises to have a positive healthcare effect, they have to be repeated\nseveral times a week. To maintain player motivation over longer periods of\ntime, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the\ngame's challenge according to the player's capabilities. For exercise games,\nthis is mostly done by tuning specific in-game parameters like the speed of\nobjects. In this work, we propose to use experience-driven Procedural Content\nGeneration for DDA in VR exercise games by procedurally generating levels that\nmatch the player's current capabilities. Not only finetuning specific\nparameters but creating completely new levels has the potential to decrease\nrepetition over longer time periods and allows for the simultaneous adaptation\nof the cognitive and physical challenge of the exergame. As a proof-of-concept,\nwe implement an initial prototype in which the player must traverse a maze that\nincludes several exercise rooms, whereby the generation of the maze is realized\nby a neural network. Passing those exercise rooms requires the player to\nperform physical activities. To match the player's capabilities, we use Deep\nReinforcement Learning to adjust the structure of the maze and to decide which\nexercise rooms to include in the maze. We evaluate our prototype in an\nexploratory user study utilizing both biodata and subjective questionnaires.",
          "link": "http://arxiv.org/abs/2108.08762",
          "publishedOn": "2021-08-20T01:53:52.715Z",
          "wordCount": 681,
          "title": "Dynamic Difficulty Adjustment in Virtual Reality Exergames through Experience-driven Procedural Content Generation. (arXiv:2108.08762v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims\nat inferring novel object categories in an unlabeled set by leveraging from\nprior knowledge of a labeled set containing different, but related classes.\nExisting approaches tackle this problem by considering multiple objective\nfunctions, usually involving specialized loss terms for the labeled and the\nunlabeled samples respectively, and often requiring auxiliary regularization\nterms. In this paper, we depart from this traditional scheme and introduce a\nUNified Objective function (UNO) for discovering novel classes, with the\nexplicit purpose of favoring synergy between supervised and unsupervised\nlearning. Using a multi-view self-labeling strategy, we generate pseudo-labels\nthat can be treated homogeneously with ground truth labels. This leads to a\nsingle classification objective operating on both known and unknown classes.\nDespite its simplicity, UNO outperforms the state of the art by a significant\nmargin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The\nproject page is available at: \\url{https://ncd-uno.github.io}.",
          "link": "http://arxiv.org/abs/2108.08536",
          "publishedOn": "2021-08-20T01:53:52.709Z",
          "wordCount": 608,
          "title": "A Unified Objective for Novel Class Discovery. (arXiv:2108.08536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vignesh Nanda Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edakunni_N/0/1/0/all/0/1\">Narayanan U Edakunni</a>",
          "description": "In recent years, gradient boosted decision trees have become popular in\nbuilding robust machine learning models on big data. The primary technique that\nhas enabled these algorithms success has been distributing the computation\nwhile building the decision trees. A distributed decision tree building, in\nturn, has been enabled by building quantiles of the big datasets and choosing\nthe candidate split points from these quantile sets. In XGBoost, for instance,\na sophisticated quantile building algorithm is employed to identify the\ncandidate split points for the decision trees. This method is often projected\nto yield better results when the computation is distributed. In this paper, we\ndispel the notion that these methods provide more accurate and scalable methods\nfor building decision trees in a distributed manner. In a significant\ncontribution, we show theoretically and empirically that choosing the split\npoints uniformly at random provides the same or even better performance in\nterms of accuracy and computational efficiency. Hence, a simple random\nselection of points suffices for decision tree building compared to more\nsophisticated methods.",
          "link": "http://arxiv.org/abs/2108.08790",
          "publishedOn": "2021-08-20T01:53:52.702Z",
          "wordCount": 605,
          "title": "Simple is better: Making Decision Trees faster using random sampling. (arXiv:2108.08790v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1\">Changwon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_K/0/1/0/all/0/1\">Kyeong-Joong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sungsu Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Won-Yong Shin</a>",
          "description": "In recent years, many recommender systems using network embedding (NE) such\nas graph neural networks (GNNs) have been extensively studied in the sense of\nimproving recommendation accuracy. However, such attempts have focused mostly\non utilizing only the information of positive user-item interactions with high\nratings. Thus, there is a challenge on how to make use of low rating scores for\nrepresenting users' preferences since low ratings can be still informative in\ndesigning NE-based recommender systems. In this study, we present SiReN, a new\nsign-aware recommender system based on GNN models. Specifically, SiReN has\nthree key components: 1) constructing a signed bipartite graph for more\nprecisely representing users' preferences, which is split into two\nedge-disjoint graphs with positive and negative edges each, 2) generating two\nembeddings for the partitioned graphs with positive and negative edges via a\nGNN model and a multi-layer perceptron (MLP), respectively, and then using an\nattention model to obtain the final embeddings, and 3) establishing a\nsign-aware Bayesian personalized ranking (BPR) loss function in the process of\noptimization. Through comprehensive experiments, we empirically demonstrate\nthat SiReN consistently outperforms state-of-the-art NE-aided recommendation\nmethods.",
          "link": "http://arxiv.org/abs/2108.08735",
          "publishedOn": "2021-08-20T01:53:52.696Z",
          "wordCount": 640,
          "title": "SiReN: Sign-Aware Recommendation Using Graph Neural Networks. (arXiv:2108.08735v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salimi_Badr_A/0/1/0/all/0/1\">Armin Salimi-Badr</a>",
          "description": "In this paper, a new interval type-2 fuzzy neural network able to construct\nnon-separable fuzzy rules with adaptive shapes is introduced. To reflect the\nuncertainty, the shape of fuzzy sets considered to be uncertain. Therefore, a\nnew form of interval type-2 fuzzy sets based on a general Gaussian model able\nto construct different shapes (including triangular, bell-shaped, trapezoidal)\nis proposed. To consider the interactions among input variables, input vectors\nare transformed to new feature spaces with uncorrelated variables proper for\ndefining each fuzzy rule. Next, the new features are fed to a fuzzification\nlayer using proposed interval type-2 fuzzy sets with adaptive shape.\nConsequently, interval type-2 non-separable fuzzy rules with proper shapes,\nconsidering the local interactions of variables and the uncertainty are formed.\nFor type reduction the contribution of the upper and lower firing strengths of\neach fuzzy rule are adaptively selected separately. To train different\nparameters of the network, the Levenberg-Marquadt optimization method is\nutilized. The performance of the proposed method is investigated on clean and\nnoisy datasets to show the ability to consider the uncertainty. Moreover, the\nproposed paradigm, is successfully applied to real-world time-series\npredictions, regression problems, and nonlinear system identification.\nAccording to the experimental results, the performance of our proposed model\noutperforms other methods with a more parsimonious structure.",
          "link": "http://arxiv.org/abs/2108.08704",
          "publishedOn": "2021-08-20T01:53:52.689Z",
          "wordCount": 668,
          "title": "IT2CFNN: An Interval Type-2 Correlation-Aware Fuzzy Neural Network to Construct Non-Separable Fuzzy Rules with Uncertain and Adaptive Shapes for Nonlinear Function Approximation. (arXiv:2108.08704v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08635",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sagar Dasgupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1\">Mhafuzul Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mashrur Chowdhury</a>",
          "description": "This paper presents a sensor fusion based Global Navigation Satellite System\n(GNSS) spoofing attack detection framework for autonomous vehicles (AV) that\nconsists of two concurrent strategies: (i) detection of vehicle state using\npredicted location shift -- i.e., distance traveled between two consecutive\ntimestamps -- and monitoring of vehicle motion state -- i.e., standstill/ in\nmotion; and (ii) detection and classification of turns (i.e., left or right).\nData from multiple low-cost in-vehicle sensors (i.e., accelerometer, steering\nangle sensor, speed sensor, and GNSS) are fused and fed into a recurrent neural\nnetwork model, which is a long short-term memory (LSTM) network for predicting\nthe location shift, i.e., the distance that an AV travels between two\nconsecutive timestamps. This location shift is then compared with the\nGNSS-based location shift to detect an attack. We have then combined k-Nearest\nNeighbors (k-NN) and Dynamic Time Warping (DTW) algorithms to detect and\nclassify left and right turns using data from the steering angle sensor. To\nprove the efficacy of the sensor fusion-based attack detection framework,\nattack datasets are created for four unique and sophisticated spoofing\nattacks-turn-by-turn, overshoot, wrong turn, and stop, using the publicly\navailable real-world Honda Research Institute Driving Dataset (HDD). Our\nanalysis reveals that the sensor fusion-based detection framework successfully\ndetects all four types of spoofing attacks within the required computational\nlatency threshold.",
          "link": "http://arxiv.org/abs/2108.08635",
          "publishedOn": "2021-08-20T01:53:52.671Z",
          "wordCount": 675,
          "title": "A Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles. (arXiv:2108.08635v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1\">Abdullatif Albaseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1\">Mohamed Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>",
          "description": "Clustered Federated Multitask Learning (CFL) was introduced as an efficient\nscheme to obtain reliable specialized models when data is imbalanced and\ndistributed in a non-i.i.d. (non-independent and identically distributed)\nfashion amongst clients. While a similarity measure metric, like the cosine\nsimilarity, can be used to endow groups of the client with a specialized model,\nthis process can be arduous as the server should involve all clients in each of\nthe federated learning rounds. Therefore, it is imperative that a subset of\nclients is selected periodically due to the limited bandwidth and latency\nconstraints at the network edge. To this end, this paper proposes a new client\nselection algorithm that aims to accelerate the convergence rate for obtaining\nspecialized machine learning models that achieve high test accuracies for all\nclient groups. Specifically, we introduce a client selection approach that\nleverages the devices' heterogeneity to schedule the clients based on their\nround latency and exploits the bandwidth reuse for clients that consume more\ntime to update the model. Then, the server performs model averaging and\nclusters the clients based on predefined thresholds. When a specific cluster\nreaches a stationary point, the proposed algorithm uses a greedy scheduling\nalgorithm for that group by selecting the clients with less latency to update\nthe model. Extensive experiments show that the proposed approach lowers the\ntraining time and accelerates the convergence rate by up to 50% while imbuing\neach client with a specialized model that is fit for its local data\ndistribution.",
          "link": "http://arxiv.org/abs/2108.08768",
          "publishedOn": "2021-08-20T01:53:52.665Z",
          "wordCount": 701,
          "title": "Client Selection Approach in Support of Clustered Federated Learning over Wireless Edge Networks. (arXiv:2108.08768v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1\">Maria-Florina Balcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1\">Mikhail Khodak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dravyansh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>",
          "description": "We analyze the meta-learning of the initialization and step-size of learning\nalgorithms for piecewise-Lipschitz functions, a non-convex setting with\napplications to both machine learning and algorithms. Starting from recent\nregret bounds for the exponential forecaster on losses with dispersed\ndiscontinuities, we generalize them to be initialization-dependent and then use\nthis result to propose a practical meta-learning procedure that learns both the\ninitialization and the step-size of the algorithm from multiple online learning\ntasks. Asymptotically, we guarantee that the average regret across tasks scales\nwith a natural notion of task-similarity that measures the amount of overlap\nbetween near-optimal regions of different tasks. Finally, we instantiate the\nmethod and its guarantee in two important settings: robust meta-learning and\nmulti-task data-driven algorithm design.",
          "link": "http://arxiv.org/abs/2108.08770",
          "publishedOn": "2021-08-20T01:53:52.658Z",
          "wordCount": 541,
          "title": "Learning-to-learn non-convex piecewise-Lipschitz functions. (arXiv:2108.08770v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Daoyi Dong</a>",
          "description": "Tensor Train (TT) approach has been successfully applied in the modelling of\nthe multilinear interaction of features. Nevertheless, the existing models lack\nflexibility and generalizability, as they only model a single type of\nhigh-order correlation. In practice, multiple multilinear correlations may\nexist within the features. In this paper, we present a novel Residual Tensor\nTrain (ResTT) which integrates the merits of TT and residual structure to\ncapture the multilinear feature correlations, from low to higher orders, within\nthe same model. In particular, we prove that the fully-connected layer in\nneural networks and the Volterra series can be taken as special cases of ResTT.\nFurthermore, we derive the rule for weight initialization that stabilizes the\ntraining of ResTT based on a mean-field analysis. We prove that such a rule is\nmuch more relaxed than that of TT, which means ResTT can easily address the\nvanishing and exploding gradient problem that exists in the current TT models.\nNumerical experiments demonstrate that ResTT outperforms the state-of-the-art\ntensor network approaches, and is competitive with the benchmark deep learning\nmodels on MNIST and Fashion-MNIST datasets.",
          "link": "http://arxiv.org/abs/2108.08659",
          "publishedOn": "2021-08-20T01:53:52.651Z",
          "wordCount": 622,
          "title": "Residual Tensor Train: a Flexible and Efficient Approach for Learning Multiple Multilinear Correlations. (arXiv:2108.08659v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Point clouds captured in real-world applications are often incomplete due to\nthe limited sensor resolution, single viewpoint, and occlusion. Therefore,\nrecovering the complete point clouds from partial ones becomes an indispensable\ntask in many practical applications. In this paper, we present a new method\nthat reformulates point cloud completion as a set-to-set translation problem\nand design a new model, called PoinTr that adopts a transformer encoder-decoder\narchitecture for point cloud completion. By representing the point cloud as a\nset of unordered groups of points with position embeddings, we convert the\npoint cloud to a sequence of point proxies and employ the transformers for\npoint cloud generation. To facilitate transformers to better leverage the\ninductive bias about 3D geometric structures of point clouds, we further devise\na geometry-aware block that models the local geometric relationships\nexplicitly. The migration of transformers enables our model to better learn\nstructural knowledge and preserve detailed information for point cloud\ncompletion. Furthermore, we propose two more challenging benchmarks with more\ndiverse incomplete point clouds that can better reflect the real-world\nscenarios to promote future research. Experimental results show that our method\noutperforms state-of-the-art methods by a large margin on both the new\nbenchmarks and the existing ones. Code is available at\nhttps://github.com/yuxumin/PoinTr",
          "link": "http://arxiv.org/abs/2108.08839",
          "publishedOn": "2021-08-20T01:53:52.644Z",
          "wordCount": 663,
          "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers. (arXiv:2108.08839v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lennon_K/0/1/0/all/0/1\">Kyle Lennon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fransen_K/0/1/0/all/0/1\">Katharina Fransen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_A/0/1/0/all/0/1\">Alexander O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yumeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefeen_Y/0/1/0/all/0/1\">Yamin Arefeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Although LEGO sets have entertained generations of children and adults, the\nchallenge of designing customized builds matching the complexity of real-world\nor imagined scenes remains too great for the average enthusiast. In order to\nmake this feat possible, we implement a system that generates a LEGO brick\nmodel from 2D images. We design a novel solution to this problem that uses an\noctree-structured autoencoder trained on 3D voxelized models to obtain a\nfeasible latent representation for model reconstruction, and a separate network\ntrained to predict this latent representation from 2D images. LEGO models are\nobtained by algorithmic conversion of the 3D voxelized model to bricks. We\ndemonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An\noctree architecture enables the flexibility to produce multiple resolutions to\nbest fit a user's creative vision or design needs. In order to demonstrate the\nbroad applicability of our system, we generate step-by-step building\ninstructions and animations for LEGO models of objects and human faces.\nFinally, we test these automatically generated LEGO sets by constructing\nphysical builds using real LEGO bricks.",
          "link": "http://arxiv.org/abs/2108.08477",
          "publishedOn": "2021-08-20T01:53:52.636Z",
          "wordCount": 627,
          "title": "Image2Lego: Customized LEGO Set Generation from Images. (arXiv:2108.08477v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Welle_M/0/1/0/all/0/1\">Michael C. Welle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poklukar_P/0/1/0/all/0/1\">Petra Poklukar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1\">Danica Kragic</a>",
          "description": "The state-of-the-art unsupervised contrastive visual representation learning\nmethods that have emerged recently (SimCLR, MoCo, SwAV) all make use of data\naugmentations in order to construct a pretext task of instant discrimination\nconsisting of similar and dissimilar pairs of images. Similar pairs are\nconstructed by randomly extracting patches from the same image and applying\nseveral other transformations such as color jittering or blurring, while\ntransformed patches from different image instances in a given batch are\nregarded as dissimilar pairs. We argue that this approach can result similar\npairs that are \\textit{semantically} dissimilar. In this work, we address this\nproblem by introducing a \\textit{batch curation} scheme that selects batches\nduring the training process that are more inline with the underlying\ncontrastive objective. We provide insights into what constitutes beneficial\nsimilar and dissimilar pairs as well as validate \\textit{batch curation} on\nCIFAR10 by integrating it in the SimCLR model.",
          "link": "http://arxiv.org/abs/2108.08643",
          "publishedOn": "2021-08-20T01:53:52.618Z",
          "wordCount": 573,
          "title": "Batch Curation for Unsupervised Contrastive Representation Learning. (arXiv:2108.08643v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.04390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huijuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "Meta-learning has been the most common framework for few-shot learning in\nrecent years. It learns the model from collections of few-shot classification\ntasks, which is believed to have a key advantage of making the training\nobjective consistent with the testing objective. However, some recent works\nreport that by training for whole-classification, i.e. classification on the\nwhole label-set, it can get comparable or even better embedding than many\nmeta-learning algorithms. The edge between these two lines of works has yet\nbeen underexplored, and the effectiveness of meta-learning in few-shot learning\nremains unclear. In this paper, we explore a simple process: meta-learning over\na whole-classification pre-trained model on its evaluation metric. We observe\nthis simple method achieves competitive performance to state-of-the-art methods\non standard benchmarks. Our further analysis shed some light on understanding\nthe trade-offs between the meta-learning objective and the whole-classification\nobjective in few-shot learning.",
          "link": "http://arxiv.org/abs/2003.04390",
          "publishedOn": "2021-08-20T01:53:52.611Z",
          "wordCount": 639,
          "title": "Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning. (arXiv:2003.04390v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kontolati_K/0/1/0/all/0/1\">Katiana Kontolati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_N/0/1/0/all/0/1\">Natalie Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_N/0/1/0/all/0/1\">Nishant Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1\">Diane Oyen</a>",
          "description": "Constructing probability densities for inference in high-dimensional spectral\ndata is often intractable. In this work, we use normalizing flows on structured\nspectral latent spaces to estimate such densities, enabling downstream\ninference tasks. In addition, we evaluate a method for uncertainty\nquantification when predicting unobserved state vectors associated with each\nspectrum. We demonstrate the capability of this approach on laser-induced\nbreakdown spectroscopy data collected by the ChemCam instrument on the Mars\nrover Curiosity. Using our approach, we are able to generate realistic spectral\nsamples and to accurately predict state vectors with associated well-calibrated\nuncertainties. We anticipate that this methodology will enable efficient\nprobabilistic modeling of spectral data, leading to potential advances in\nseveral areas, including out-of-distribution detection and sensitivity\nanalysis.",
          "link": "http://arxiv.org/abs/2108.08709",
          "publishedOn": "2021-08-20T01:53:52.604Z",
          "wordCount": 572,
          "title": "Neural density estimation and uncertainty quantification for laser induced breakdown spectroscopy spectra. (arXiv:2108.08709v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08631",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Inui_K/0/1/0/all/0/1\">Koji Inui</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kato_Y/0/1/0/all/0/1\">Yasuyuki Kato</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Motome_Y/0/1/0/all/0/1\">Yukitoshi Motome</a>",
          "description": "We propose a general framework for finding the ground state of many-body\nfermionic systems by using feed-forward neural networks. The anticommutation\nrelation for fermions is usually implemented to a variational wave function by\nthe Slater determinant (or Pfaffian), which is a computational bottleneck\nbecause of the numerical cost of $O(N^3)$ for $N$ particles. We bypass this\nbottleneck by explicitly calculating the sign changes associated with particle\nexchanges in real space and using fully connected neural networks for\noptimizing the rest parts of the wave function. This reduces the computational\ncost to $O(N^2)$ or less. We show that the accuracy of the approximation can be\nimproved by optimizing the \"variance\" of the energy simultaneously with the\nenergy itself. We also find that a reweighting method in Monte Carlo sampling\ncan stabilize the calculation. These improvements can be applied to other\napproaches based on variational Monte Carlo methods. Moreover, we show that the\naccuracy can be further improved by using the symmetry of the system, the\nrepresentative states, and an additional neural network implementing a\ngeneralized Gutzwiller-Jastrow factor. We demonstrate the efficiency of the\nmethod by applying it to a two-dimensional Hubbard model.",
          "link": "http://arxiv.org/abs/2108.08631",
          "publishedOn": "2021-08-20T01:53:52.597Z",
          "wordCount": 643,
          "title": "Determinant-free fermionic wave function using feed-forward neural network. (arXiv:2108.08631v1 [cond-mat.str-el])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08473",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jonathan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_B/0/1/0/all/0/1\">Bowen Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ram_R/0/1/0/all/0/1\">Rahul Ram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">David Liang</a>",
          "description": "In this work, deep learning algorithms are used to classify fundus images in\nterms of diabetic retinopathy severity. Six different combinations of two model\narchitectures, the Dense Convolutional Network-121 and the Residual Neural\nNetwork-50 and three image types, RGB, Green, and High Contrast, were tested to\nfind the highest performing combination. We achieved an average validation loss\nof 0.17 and a max validation accuracy of 85 percent. By testing out multiple\ncombinations, certain combinations of parameters performed better than others,\nthough minimal variance was found overall. Green filtration was shown to\nperform the poorest, while amplified contrast appeared to have a negligible\neffect in comparison to RGB analysis. ResNet50 proved to be less of a robust\nmodel as opposed to DenseNet121.",
          "link": "http://arxiv.org/abs/2108.08473",
          "publishedOn": "2021-08-20T01:53:52.591Z",
          "wordCount": 603,
          "title": "Classification of Diabetic Retinopathy Severity in Fundus Images with DenseNet121 and ResNet50. (arXiv:2108.08473v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">H. Eric Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Baljeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filev_D/0/1/0/all/0/1\">Dimitar Filev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huei Peng</a>",
          "description": "The field of Meta Reinforcement Learning (Meta-RL) has seen substantial\nadvancements recently. In particular, off-policy methods were developed to\nimprove the data efficiency of Meta-RL techniques. \\textit{Probabilistic\nembeddings for actor-critic RL} (PEARL) is currently one of the leading\napproaches for multi-MDP adaptation problems. A major drawback of many existing\nMeta-RL methods, including PEARL, is that they do not explicitly consider the\nsafety of the prior policy when it is exposed to a new task for the very first\ntime. This is very important for some real-world applications, including field\nrobots and Autonomous Vehicles (AVs). In this paper, we develop the PEARL PLUS\n(PEARL$^+$) algorithm, which optimizes the policy for both prior safety and\nposterior adaptation. Building on top of PEARL, our proposed PEARL$^+$\nalgorithm introduces a prior regularization term in the reward function and a\nnew Q-network for recovering the state-action value with prior context\nassumption, to improve the robustness and safety of the trained network\nexposing to a new task for the first time. The performance of the PEARL$^+$\nmethod is demonstrated by solving three safety-critical decision-making\nproblems related to robots and AVs, including two MuJoCo benchmark problems.\nFrom the simulation experiments, we show that the safety of the prior policy is\nsignificantly improved compared to that of the original PEARL method.",
          "link": "http://arxiv.org/abs/2108.08448",
          "publishedOn": "2021-08-20T01:53:52.571Z",
          "wordCount": 670,
          "title": "Prior Is All You Need to Improve the Robustness and Safety for the First Time Deployment of Meta RL. (arXiv:2108.08448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastani_H/0/1/0/all/0/1\">Hamsa Bastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinchaisri_W/0/1/0/all/0/1\">Wichinpong Park Sinchaisri</a>",
          "description": "A key aspect of human intelligence is their ability to convey their knowledge\nto others in succinct forms. However, despite their predictive power, current\nmachine learning models are largely blackboxes, making it difficult for humans\nto extract useful insights. Focusing on sequential decision-making, we design a\nnovel machine learning algorithm that conveys its insights to humans in the\nform of interpretable \"tips\". Our algorithm selects the tip that best bridges\nthe gap in performance between human users and the optimal policy. We evaluate\nour approach through a series of randomized controlled user studies where\nparticipants manage a virtual kitchen. Our experiments show that the tips\ngenerated by our algorithm can significantly improve human performance relative\nto intuitive baselines. In addition, we discuss a number of empirical insights\nthat can help inform the design of algorithms intended for human-AI\ncollaboration. For instance, we find evidence that participants do not simply\nblindly follow our tips; instead, they combine them with their own experience\nto discover additional strategies for improving performance.",
          "link": "http://arxiv.org/abs/2108.08454",
          "publishedOn": "2021-08-20T01:53:52.565Z",
          "wordCount": 598,
          "title": "Improving Human Decision-Making with Machine Learning. (arXiv:2108.08454v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.11722",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zenati_H/0/1/0/all/0/1\">Houssam Zenati</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1\">Alberto Bietti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Martin_M/0/1/0/all/0/1\">Matthieu Martin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Diemert_E/0/1/0/all/0/1\">Eustache Diemert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mairal_J/0/1/0/all/0/1\">Julien Mairal</a>",
          "description": "Counterfactual reasoning from logged data has become increasingly important\nfor many applications such as web advertising or healthcare. In this paper, we\naddress the problem of learning stochastic policies with continuous actions\nfrom the viewpoint of counterfactual risk minimization (CRM). While the CRM\nframework is appealing and well studied for discrete actions, the continuous\naction case raises new challenges about modelization, optimization, and~offline\nmodel selection with real data which turns out to be particularly challenging.\nOur paper contributes to these three aspects of the CRM estimation pipeline.\nFirst, we introduce a modelling strategy based on a joint kernel embedding of\ncontexts and actions, which overcomes the shortcomings of previous\ndiscretization approaches. Second, we empirically show that the optimization\naspect of counterfactual learning is important, and we demonstrate the benefits\nof proximal point algorithms and differentiable estimators. Finally, we propose\nan evaluation protocol for offline policies in real-world logged systems, which\nis challenging since policies cannot be replayed on test data, and we release a\nnew large-scale dataset along with multiple synthetic, yet realistic,\nevaluation setups.",
          "link": "http://arxiv.org/abs/2004.11722",
          "publishedOn": "2021-08-20T01:53:52.559Z",
          "wordCount": 654,
          "title": "Counterfactual Learning of Stochastic Policies withContinuous Actions: from Models to Offline Evaluation. (arXiv:2004.11722v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sohns_J/0/1/0/all/0/1\">Jan-Tobias Sohns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Michaela Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jirasek_F/0/1/0/all/0/1\">Fabian Jirasek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasse_H/0/1/0/all/0/1\">Hans Hasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leitte_H/0/1/0/all/0/1\">Heike Leitte</a>",
          "description": "Embeddings of high-dimensional data are widely used to explore data, to\nverify analysis results, and to communicate information. Their explanation, in\nparticular with respect to the input attributes, is often difficult. With\nlinear projects like PCA the axes can still be annotated meaningfully. With\nnon-linear projections this is no longer possible and alternative strategies\nsuch as attribute-based color coding are required. In this paper, we review\nexisting augmentation techniques and discuss their limitations. We present the\nNon-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation\nstrategy for projected data (rangesets) with interactive analysis in a small\nmultiples setting. Rangesets use a set-based visualization approach for binned\nattribute values that enable the user to quickly observe structure and detect\noutliers. We detail the link between algebraic topology and rangesets and\ndemonstrate the utility of NoLiES in case studies with various challenges\n(complex attribute value distribution, many attributes, many data points) and a\nreal-world application to understand latent features of matrix completion in\nthermodynamics.",
          "link": "http://arxiv.org/abs/2108.08706",
          "publishedOn": "2021-08-20T01:53:52.551Z",
          "wordCount": 602,
          "title": "Attribute-based Explanations of Non-Linear Embeddings of High-Dimensional Data. (arXiv:2108.08706v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sagar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollis_C/0/1/0/all/0/1\">Courtland Hollis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkison_T/0/1/0/all/0/1\">Travis Atkison</a>",
          "description": "An adaptive traffic signal controller (ATSC) combined with a connected\nvehicle (CV) concept uses real-time vehicle trajectory data to regulate green\ntime and has the ability to reduce intersection waiting time significantly and\nthereby improve travel time in a signalized corridor. However, the CV-based\nATSC increases the size of the surface vulnerable to potential cyber-attack,\nallowing an attacker to generate disastrous traffic congestion in a roadway\nnetwork. An attacker can congest a route by generating fake vehicles by\nmaintaining traffic and car-following rules at a slow rate so that the signal\ntiming and phase change without having any abrupt changes in number of\nvehicles. Because of the adaptive nature of ATSC, it is a challenge to model\nthis kind of attack and also to develop a strategy for detection. This paper\nintroduces an innovative \"slow poisoning\" cyberattack for a waiting time based\nATSC algorithm and a corresponding detection strategy. Thus, the objectives of\nthis paper are to: (i) develop a \"slow poisoning\" attack generation strategy\nfor an ATSC, and (ii) develop a prediction-based \"slow poisoning\" attack\ndetection strategy using a recurrent neural network -- i.e., long short-term\nmemory model. We have generated a \"slow poisoning\" attack modeling strategy\nusing a microscopic traffic simulator -- Simulation of Urban Mobility (SUMO) --\nand used generated data from the simulation to develop both the attack model\nand detection model. Our analyses revealed that the attack strategy is\neffective in creating a congestion in an approach and detection strategy is\nable to flag the attack.",
          "link": "http://arxiv.org/abs/2108.08627",
          "publishedOn": "2021-08-20T01:53:52.545Z",
          "wordCount": 695,
          "title": "An Innovative Attack Modelling and Attack Detection Approach for a Waiting Time-based Adaptive Traffic Signal Controller. (arXiv:2108.08627v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehdy_A/0/1/0/all/0/1\">A K M Nuhil Mehdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrpouyan_H/0/1/0/all/0/1\">Hoda Mehrpouyan</a>",
          "description": "The concern regarding users' data privacy has risen to its highest level due\nto the massive increase in communication platforms, social networking sites,\nand greater users' participation in online public discourse. An increasing\nnumber of people exchange private information via emails, text messages, and\nsocial media without being aware of the risks and implications. Researchers in\nthe field of Natural Language Processing (NLP) have concentrated on creating\ntools and strategies to identify, categorize, and sanitize private information\nin text data since a substantial amount of data is exchanged in textual form.\nHowever, most of the detection methods solely rely on the existence of\npre-identified keywords in the text and disregard the inference of the\nunderlying meaning of the utterance in a specific context. Hence, in some\nsituations, these tools and algorithms fail to detect disclosure, or the\nproduced results are miss-classified. In this paper, we propose a multi-input,\nmulti-output hybrid neural network which utilizes transfer-learning,\nlinguistics, and metadata to learn the hidden patterns. Our goal is to better\nclassify disclosure/non-disclosure content in terms of the context of\nsituation. We trained and evaluated our model on a human-annotated ground truth\ndataset, containing a total of 5,400 tweets. The results show that the proposed\nmodel was able to identify privacy disclosure through tweets with an accuracy\nof 77.4% while classifying the information type of those tweets with an\nimpressive accuracy of 99%, by jointly learning for two separate tasks.",
          "link": "http://arxiv.org/abs/2108.08483",
          "publishedOn": "2021-08-20T01:53:52.539Z",
          "wordCount": 699,
          "title": "A Multi-input Multi-output Transformer-based Hybrid Neural Network for Multi-class Privacy Disclosure Detection. (arXiv:2108.08483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03193",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Khatri_S/0/1/0/all/0/1\">Sumeet Khatri</a>",
          "description": "Distributing entanglement over long distances is one of the central tasks in\nquantum networks. An important problem, especially for near-term quantum\nnetworks, is to develop optimal entanglement distribution protocols that take\ninto account the limitations of current and near-term hardware, such as quantum\nmemories with limited coherence time. We address this problem by initiating the\nstudy of quantum network protocols for entanglement distribution using the\ntheory of decision processes, such that optimal protocols (referred to as\npolicies in the context of decision processes) can be found using dynamic\nprogramming or reinforcement learning algorithms. As a first step, in this work\nwe focus exclusively on the elementary link level. We start by defining a\nquantum decision process for elementary links, along with figures of merit for\nevaluating policies. We then provide two algorithms for determining policies,\none of which we prove to be optimal (with respect to fidelity and success\nprobability) among all policies. Then we show that the previously-studied\nmemory-cutoff protocol can be phrased as a policy within our decision process\nframework, allowing us to obtain several new fundamental results about it. The\nconceptual developments and results of this work pave the way for the\nsystematic study of the fundamental limitations of near-term quantum networks,\nand the requirements for physically realizing them.",
          "link": "http://arxiv.org/abs/2007.03193",
          "publishedOn": "2021-08-20T01:53:52.519Z",
          "wordCount": 684,
          "title": "Policies for elementary links in a quantum network. (arXiv:2007.03193v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08628",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sagar Dasgupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_T/0/1/0/all/0/1\">Tonmoy Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>",
          "description": "A resilient and robust positioning, navigation, and timing (PNT) system is a\nnecessity for the navigation of autonomous vehicles (AVs). Global Navigation\nSatelite System (GNSS) provides satellite-based PNT services. However, a\nspoofer can temper an authentic GNSS signal and could transmit wrong position\ninformation to an AV. Therefore, a GNSS must have the capability of real-time\ndetection and feedback-correction of spoofing attacks related to PNT receivers,\nwhereby it will help the end-user (autonomous vehicle in this case) to navigate\nsafely if it falls into any compromises. This paper aims to develop a deep\nreinforcement learning (RL)-based turn-by-turn spoofing attack detection using\nlow-cost in-vehicle sensor data. We have utilized Honda Driving Dataset to\ncreate attack and non-attack datasets, develop a deep RL model, and evaluate\nthe performance of the RL-based attack detection model. We find that the\naccuracy of the RL model ranges from 99.99% to 100%, and the recall value is\n100%. However, the precision ranges from 93.44% to 100%, and the f1 score\nranges from 96.61% to 100%. Overall, the analyses reveal that the RL model is\neffective in turn-by-turn spoofing attack detection.",
          "link": "http://arxiv.org/abs/2108.08628",
          "publishedOn": "2021-08-20T01:53:52.513Z",
          "wordCount": 626,
          "title": "A Reinforcement Learning Approach for GNSS Spoofing Attack Detection of Autonomous Vehicles. (arXiv:2108.08628v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merkle_F/0/1/0/all/0/1\">Florian Merkle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samsinger_M/0/1/0/all/0/1\">Maximilian Samsinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schottle_P/0/1/0/all/0/1\">Pascal Sch&#xf6;ttle</a>",
          "description": "The vulnerability of deep neural networks against adversarial examples -\ninputs with small imperceptible perturbations - has gained a lot of attention\nin the research community recently. Simultaneously, the number of parameters of\nstate-of-the-art deep learning models has been growing massively, with\nimplications on the memory and computational resources required to train and\ndeploy such models. One approach to control the size of neural networks is\nretrospectively reducing the number of parameters, so-called neural network\npruning. Available research on the impact of neural network pruning on the\nadversarial robustness is fragmentary and often does not adhere to established\nprinciples of robustness evaluation. We close this gap by evaluating the\nrobustness of pruned models against L-0, L-2 and L-infinity attacks for a wide\nrange of attack strengths, several architectures, data sets, pruning methods,\nand compression rates. Our results confirm that neural network pruning and\nadversarial robustness are not mutually exclusive. Instead, sweet spots can be\nfound that are favorable in terms of model size and adversarial robustness.\nFurthermore, we extend our analysis to situations that incorporate additional\nassumptions on the adversarial scenario and show that depending on the\nsituation, different strategies are optimal.",
          "link": "http://arxiv.org/abs/2108.08560",
          "publishedOn": "2021-08-20T01:53:52.507Z",
          "wordCount": 621,
          "title": "Pruning in the Face of Adversaries. (arXiv:2108.08560v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuba_J/0/1/0/all/0/1\">Jakub Grudzien Kuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1\">Muning Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Linghui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shangding Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mguni_D/0/1/0/all/0/1\">David Henry Mguni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "Policy gradient (PG) methods are popular reinforcement learning (RL) methods\nwhere a baseline is often applied to reduce the variance of gradient estimates.\nIn multi-agent RL (MARL), although the PG theorem can be naturally extended,\nthe effectiveness of multi-agent PG (MAPG) methods degrades as the variance of\ngradient estimates increases rapidly with the number of agents. In this paper,\nwe offer a rigorous analysis of MAPG methods by, firstly, quantifying the\ncontributions of the number of agents and agents' explorations to the variance\nof MAPG estimators. Based on this analysis, we derive the optimal baseline (OB)\nthat achieves the minimal variance. In comparison to the OB, we measure the\nexcess variance of existing MARL algorithms such as vanilla MAPG and COMA.\nConsidering using deep neural networks, we also propose a surrogate version of\nOB, which can be seamlessly plugged into any existing PG methods in MARL. On\nbenchmarks of Multi-Agent MuJoCo and StarCraft challenges, our OB technique\neffectively stabilises training and improves the performance of multi-agent PPO\nand COMA algorithms by a significant margin.",
          "link": "http://arxiv.org/abs/2108.08612",
          "publishedOn": "2021-08-20T01:53:52.500Z",
          "wordCount": 621,
          "title": "Settling the Variance of Multi-Agent Policy Gradients. (arXiv:2108.08612v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>",
          "description": "Existing self-supervised learning methods learn representation by means of\npretext tasks which are either (1) discriminating that explicitly specify which\nfeatures should be separated or (2) aligning that precisely indicate which\nfeatures should be closed together, but ignore the fact how to jointly and\nprincipally define which features to be repelled and which ones to be\nattracted. In this work, we combine the positive aspects of the discriminating\nand aligning methods, and design a hybrid method that addresses the above\nissue. Our method explicitly specifies the repulsion and attraction mechanism\nrespectively by discriminative predictive task and concurrently maximizing\nmutual information between paired views sharing redundant information. We\nqualitatively and quantitatively show that our proposed model learns better\nfeatures that are more effective for the diverse downstream tasks ranging from\nclassification to semantic segmentation. Our experiments on nine established\nbenchmarks show that the proposed model consistently outperforms the existing\nstate-of-the-art results of self-supervised and transfer learning protocol.",
          "link": "http://arxiv.org/abs/2108.08562",
          "publishedOn": "2021-08-20T01:53:52.493Z",
          "wordCount": 602,
          "title": "Concurrent Discrimination and Alignment for Self-Supervised Feature Learning. (arXiv:2108.08562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schnorr_D/0/1/0/all/0/1\">David Schn&#xf6;rr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnorr_C/0/1/0/all/0/1\">Christoph Schn&#xf6;rr</a>",
          "description": "The Turing mechanism describes the emergence of spatial patterns due to\nspontaneous symmetry breaking in reaction-diffusion processes and underlies\nmany developmental processes. Identifying Turing mechanisms in biological\nsystems defines a challenging problem. This paper introduces an approach to the\nprediction of Turing parameter values from observed Turing patterns. The\nparameter values correspond to a parametrized system of reaction-diffusion\nequations that generate Turing patterns as steady state. The Gierer-Meinhardt\nmodel with four parameters is chosen as a case study. A novel invariant pattern\nrepresentation based on resistance distance histograms is employed, along with\nWasserstein kernels, in order to cope with the highly variable arrangement of\nlocal pattern structure that depends on the initial conditions which are\nassumed to be unknown. This enables to compute physically plausible distances\nbetween patterns, to compute clusters of patterns and, above all, model\nparameter prediction: for small training sets, classical state-of-the-art\nmethods including operator-valued kernels outperform neural networks that are\napplied to raw pattern data, whereas for large training sets the latter are\nmore accurate. Excellent predictions are obtained for single parameter values\nand reasonably accurate results for jointly predicting all parameter values.",
          "link": "http://arxiv.org/abs/2108.08542",
          "publishedOn": "2021-08-20T01:53:52.468Z",
          "wordCount": 618,
          "title": "Learning System Parameters from Turing Patterns. (arXiv:2108.08542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suin_M/0/1/0/all/0/1\">Maitreya Suin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>",
          "description": "We present a general learning-based solution for restoring images suffering\nfrom spatially-varying degradations. Prior approaches are typically\ndegradation-specific and employ the same processing across different images and\ndifferent pixels within. However, we hypothesize that such spatially rigid\nprocessing is suboptimal for simultaneously restoring the degraded pixels as\nwell as reconstructing the clean regions of the image. To overcome this\nlimitation, we propose SPAIR, a network design that harnesses\ndistortion-localization information and dynamically adjusts computation to\ndifficult regions in the image. SPAIR comprises of two components, (1) a\nlocalization network that identifies degraded pixels, and (2) a restoration\nnetwork that exploits knowledge from the localization network in filter and\nfeature domain to selectively and adaptively restore degraded pixels. Our key\nidea is to exploit the non-uniformity of heavy degradations in spatial-domain\nand suitably embed this knowledge within distortion-guided modules performing\nsparse normalization, feature extraction and attention. Our architecture is\nagnostic to physical formation model and generalizes across several types of\nspatially-varying degradations. We demonstrate the efficacy of SPAIR\nindividually on four restoration tasks-removal of rain-streaks, raindrops,\nshadows and motion blur. Extensive qualitative and quantitative comparisons\nwith prior art on 11 benchmark datasets demonstrate that our\ndegradation-agnostic network design offers significant performance gains over\nstate-of-the-art degradation-specific architectures. Code available at\nhttps://github.com/human-analysis/spatially-adaptive-image-restoration.",
          "link": "http://arxiv.org/abs/2108.08617",
          "publishedOn": "2021-08-20T01:53:52.461Z",
          "wordCount": 653,
          "title": "Spatially-Adaptive Image Restoration using Distortion-Guided Networks. (arXiv:2108.08617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krokotsch_T/0/1/0/all/0/1\">Tilman Krokotsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knaak_M/0/1/0/all/0/1\">Mirko Knaak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guhmann_C/0/1/0/all/0/1\">Clemens G&#xfc;hmann</a>",
          "description": "RUL estimation suffers from a server data imbalance where data from machines\nnear their end of life is rare. Additionally, the data produced by a machine\ncan only be labeled after the machine failed. Semi-Supervised Learning (SSL)\ncan incorporate the unlabeled data produced by machines that did not yet fail.\nPrevious work on SSL evaluated their approaches under unrealistic conditions\nwhere the data near failure was still available. Even so, only moderate\nimprovements were made. This paper proposes a novel SSL approach based on\nself-supervised pre-training. The method can outperform two competing\napproaches from the literature and a supervised baseline under realistic\nconditions on the NASA C-MAPSS dataset. Nevertheless, we observe degraded\nperformance in some circumstances and discuss possible causes.",
          "link": "http://arxiv.org/abs/2108.08721",
          "publishedOn": "2021-08-20T01:53:52.454Z",
          "wordCount": 558,
          "title": "Improving Semi-Supervised Learning for Remaining Useful Lifetime Estimation Through Self-Supervision. (arXiv:2108.08721v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Mingjun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zikui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1\">Srikanth V. Krishnamurthy</a>",
          "description": "Vision systems that deploy Deep Neural Networks (DNNs) are known to be\nvulnerable to adversarial examples. Recent research has shown that checking the\nintrinsic consistencies in the input data is a promising way to detect\nadversarial attacks (e.g., by checking the object co-occurrence relationships\nin complex scenes). However, existing approaches are tied to specific models\nand do not offer generalizability. Motivated by the observation that language\ndescriptions of natural scene images have already captured the object\nco-occurrence relationships that can be learned by a language model, we develop\na novel approach to perform context consistency checks using such language\nmodels. The distinguishing aspect of our approach is that it is independent of\nthe deployed object detector and yet offers very high accuracy in terms of\ndetecting adversarial examples in practical scenes with multiple objects.",
          "link": "http://arxiv.org/abs/2108.08421",
          "publishedOn": "2021-08-20T01:53:52.435Z",
          "wordCount": 590,
          "title": "Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes. (arXiv:2108.08421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "The performance of a computer vision model depends on the size and quality of\nits training data. Recent studies have unveiled previously-unknown composition\nbiases in common image datasets which then lead to skewed model outputs, and\nhave proposed methods to mitigate these biases. However, most existing works\nassume that human-generated annotations can be considered gold-standard and\nunbiased. In this paper, we reveal that this assumption can be problematic, and\nthat special care should be taken to prevent models from learning such\nannotation biases. We focus on facial expression recognition and compare the\nlabel biases between lab-controlled and in-the-wild datasets. We demonstrate\nthat many expression datasets contain significant annotation biases between\ngenders, especially when it comes to the happy and angry expressions, and that\ntraditional methods cannot fully mitigate such biases in trained models. To\nremove expression annotation bias, we propose an AU-Calibrated Facial\nExpression Recognition (AUC-FER) framework that utilizes facial action units\n(AUs) and incorporates the triplet loss into the objective function.\nExperimental results suggest that the proposed method is more effective in\nremoving expression annotation bias than existing techniques.",
          "link": "http://arxiv.org/abs/2108.08504",
          "publishedOn": "2021-08-20T01:53:52.429Z",
          "wordCount": 631,
          "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition. (arXiv:2108.08504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Heat management plays an important role in engineering. Temperature field\nreconstruction of heat source systems (TFR-HSS) with limited monitoring\ntensors, performs an essential role in heat management. However, prior methods\nwith common interpolations usually cannot provide accurate reconstruction. In\naddition, there exists no public dataset for widely research of reconstruction\nmethods to further boost the field reconstruction in engineering. To overcome\nthis problem, this work construct a specific dataset, namely TFRD, for TFR-HSS\ntask with commonly used methods, including the interpolation methods and the\nsurrogate model based methods, as baselines to advance the research over\ntemperature field reconstruction. First, the TFR-HSS task is mathematically\nmodelled from real-world engineering problem and three types of numerically\nmodellings have been constructed to transform the problem into discrete mapping\nforms. Besides, this work selects four typical reconstruction problem with\ndifferent heat source information and boundary conditions and generate the\nstandard samples as training and testing samples for further research. Finally,\na comprehensive review of the prior methods for TFR-HSS task as well as recent\nwidely used deep learning methods is given and we provide a performance\nanalysis of typical methods on TFRD, which can be served as the baseline\nresults on this benchmark.",
          "link": "http://arxiv.org/abs/2108.08298",
          "publishedOn": "2021-08-20T01:53:52.409Z",
          "wordCount": 649,
          "title": "TFRD: A Benchmark Dataset for Research on Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2108.08298v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizi_F/0/1/0/all/0/1\">Faezeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematollahi_M/0/1/0/all/0/1\">Mohammad Ali Nematollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassannataj_E/0/1/0/all/0/1\">Edris Hassannataj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>",
          "description": "Coronary heart disease (CAD) is one of the crucial reasons for cardiovascular\nmortality in middle-aged people worldwide. The most typical tool is angiography\nfor diagnosing CAD. The challenges of CAD diagnosis using angiography are\ncostly and have side effects. One of the alternative solutions is the use of\nmachine learning-based patterns for CAD diagnosis. Hence, this paper provides a\nnew hybrid machine learning model called Genetic Support Vector Machine and\nAnalysis of Variance (GSVMA). The ANOVA is known as the kernel function for\nSVM. The proposed model is performed based on the Z-Alizadeh Sani dataset. A\ngenetic optimization algorithm is used to select crucial features. In addition,\nSVM with Anova, Linear SVM, and LibSVM with radial basis function methods were\napplied to classify the dataset. As a result, the GSVMA hybrid method performs\nbetter than other methods. This proposed method has the highest accuracy of\n89.45% through a 10-fold cross-validation technique with 35 selected features\non the Z-Alizadeh Sani dataset. Therefore, the genetic optimization algorithm\nis very effective for improving accuracy. The computer-aided GSVMA method can\nbe helped clinicians with CAD diagnosis.",
          "link": "http://arxiv.org/abs/2108.08292",
          "publishedOn": "2021-08-20T01:53:52.311Z",
          "wordCount": 634,
          "title": "GSVMA: A Genetic-Support Vector Machine-Anova method for CAD diagnosis based on Z-Alizadeh Sani dataset. (arXiv:2108.08292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>",
          "description": "Uncertainty in machine learning is not generally taught as general knowledge\nin Machine Learning course curricula. In this paper we propose a short\ncurriculum for a course about uncertainty in machine learning, and complement\nthe course with a selection of use cases, aimed to trigger discussion and let\nstudents play with the concepts of uncertainty in a programming setting. Our\nuse cases cover the concept of output uncertainty, Bayesian neural networks and\nweight distributions, sources of uncertainty, and out of distribution\ndetection. We expect that this curriculum and set of use cases motivates the\ncommunity to adopt these important concepts into courses for safety in AI.",
          "link": "http://arxiv.org/abs/2108.08712",
          "publishedOn": "2021-08-20T01:53:52.302Z",
          "wordCount": 550,
          "title": "Teaching Uncertainty Quantification in Machine Learning through Use Cases. (arXiv:2108.08712v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garau_N/0/1/0/all/0/1\">Nicola Garau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisagno_N/0/1/0/all/0/1\">Niccol&#xf2; Bisagno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brodka_P/0/1/0/all/0/1\">Piotr Br&#xf3;dka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conci_N/0/1/0/all/0/1\">Nicola Conci</a>",
          "description": "Human Pose Estimation (HPE) aims at retrieving the 3D position of human\njoints from images or videos. We show that current 3D HPE methods suffer a lack\nof viewpoint equivariance, namely they tend to fail or perform poorly when\ndealing with viewpoints unseen at training time. Deep learning methods often\nrely on either scale-invariant, translation-invariant, or rotation-invariant\noperations, such as max-pooling. However, the adoption of such procedures does\nnot necessarily improve viewpoint generalization, rather leading to more\ndata-dependent methods. To tackle this issue, we propose a novel capsule\nautoencoder network with fast Variational Bayes capsule routing, named DECA. By\nmodeling each joint as a capsule entity, combined with the routing algorithm,\nour approach can preserve the joints' hierarchical and geometrical structure in\nthe feature space, independently from the viewpoint. By achieving viewpoint\nequivariance, we drastically reduce the network data dependency at training\ntime, resulting in an improved ability to generalize for unseen viewpoints. In\nthe experimental validation, we outperform other methods on depth images from\nboth seen and unseen viewpoints, both top-view, and front-view. In the RGB\ndomain, the same network gives state-of-the-art results on the challenging\nviewpoint transfer task, also establishing a new framework for top-view HPE.\nThe code can be found at https://github.com/mmlab-cv/DECA.",
          "link": "http://arxiv.org/abs/2108.08557",
          "publishedOn": "2021-08-20T01:53:52.296Z",
          "wordCount": 668,
          "title": "DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders. (arXiv:2108.08557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yilin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haozhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinggao Liu</a>",
          "description": "The effectiveness of shortcut/skip-connection has been widely verified, which\ninspires massive explorations on neural architecture design. This work attempts\nto find an effective way to design new network architectures. It is discovered\nthat the main difference between network architectures can be reflected in\ntheir recursion formulas. Based on this, a methodology is proposed to design\nnovel network architectures from the perspective of mathematical formulas.\nAfterwards, a case study is provided to generate an improved architecture based\non ResNet. Furthermore, the new architecture is compared with ResNet and then\ntested on ResNet-based networks. Massive experiments are conducted on CIFAR and\nImageNet, which witnesses the significant performance improvements provided by\nthe architecture.",
          "link": "http://arxiv.org/abs/2108.08689",
          "publishedOn": "2021-08-20T01:53:52.269Z",
          "wordCount": 562,
          "title": "Analyze and Design Network Architectures by Recursion Formulas. (arXiv:2108.08689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08670",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chakrabarti_K/0/1/0/all/0/1\">Kushal Chakrabarti</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gupta_N/0/1/0/all/0/1\">Nirupam Gupta</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chopra_N/0/1/0/all/0/1\">Nikhil Chopra</a>",
          "description": "This paper studies a distributed multi-agent convex optimization problem. The\nsystem comprises multiple agents in this problem, each with a set of local data\npoints and an associated local cost function. The agents are connected to a\nserver, and there is no inter-agent communication. The agents' goal is to learn\na parameter vector that optimizes the aggregate of their local costs without\nrevealing their local data points. In principle, the agents can solve this\nproblem by collaborating with the server using the traditional distributed\ngradient-descent method. However, when the aggregate cost is ill-conditioned,\nthe gradient-descent method (i) requires a large number of iterations to\nconverge, and (ii) is highly unstable against process noise. We propose an\niterative pre-conditioning technique to mitigate the deleterious effects of the\ncost function's conditioning on the convergence rate of distributed\ngradient-descent. Unlike the conventional pre-conditioning techniques, the\npre-conditioner matrix in our proposed technique updates iteratively to\nfacilitate implementation on the distributed network. In the distributed\nsetting, we provably show that the proposed algorithm converges linearly with\nan improved rate of convergence than the traditional and adaptive\ngradient-descent methods. Additionally, for the special case when the minimizer\nof the aggregate cost is unique, our algorithm converges superlinearly. We\ndemonstrate our algorithm's superior performance compared to prominent\ndistributed algorithms for solving real logistic regression problems and\nemulating neural network training via a noisy quadratic model, thereby\nsignifying the proposed algorithm's efficiency for distributively solving\nnon-convex optimization. Moreover, we empirically show that the proposed\nalgorithm results in faster training without compromising the generalization\nperformance.",
          "link": "http://arxiv.org/abs/2108.08670",
          "publishedOn": "2021-08-20T01:53:52.263Z",
          "wordCount": 696,
          "title": "On Accelerating Distributed Convex Optimizations. (arXiv:2108.08670v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_S/0/1/0/all/0/1\">Shen-Lung Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston Hsu</a>",
          "description": "Spatial-temporal prediction is a critical problem for intelligent\ntransportation, which is helpful for tasks such as traffic control and accident\nprevention. Previous studies rely on large-scale traffic data collected from\nsensors. However, it is unlikely to deploy sensors in all regions due to the\ndevice and maintenance costs. This paper addresses the problem via outdoor\ncellular traffic distilled from over two billion records per day in a telecom\ncompany, because outdoor cellular traffic induced by user mobility is highly\nrelated to transportation traffic. We study road intersections in urban and aim\nto predict future outdoor cellular traffic of all intersections given historic\noutdoor cellular traffic. Furthermore, We propose a new model for multivariate\nspatial-temporal prediction, mainly consisting of two extending graph attention\nnetworks (GAT). First GAT is used to explore correlations among multivariate\ncellular traffic. Another GAT leverages the attention mechanism into graph\npropagation to increase the efficiency of capturing spatial dependency.\nExperiments show that the proposed model significantly outperforms the\nstate-of-the-art methods on our dataset.",
          "link": "http://arxiv.org/abs/2108.08307",
          "publishedOn": "2021-08-20T01:53:52.250Z",
          "wordCount": 612,
          "title": "Multivariate and Propagation Graph Attention Network for Spatial-Temporal Prediction with Outdoor Cellular Traffic. (arXiv:2108.08307v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Multi-view network embedding aims at projecting nodes in the network to\nlow-dimensional vectors, while preserving their multiple relations and\nattribute information. Contrastive learning-based methods have preliminarily\nshown promising performance in this task. However, most contrastive\nlearning-based methods mostly rely on high-quality graph embedding and explore\nless on the relationships between different graph views. To deal with these\ndeficiencies, we design a novel node-to-node Contrastive learning framework for\nMulti-view network Embedding (CREME), which mainly contains two contrastive\nobjectives: Multi-view fusion InfoMax and Inter-view InfoMin. The former\nobjective distills information from embeddings generated from different graph\nviews, while the latter distinguishes different graph views better to capture\nthe complementary information between them. Specifically, we first apply a view\nencoder to generate each graph view representation and utilize a multi-view\naggregator to fuse these representations. Then, we unify the two contrastive\nobjectives into one learning objective for training. Extensive experiments on\nthree real-world datasets show that CREME outperforms existing methods\nconsistently.",
          "link": "http://arxiv.org/abs/2108.08296",
          "publishedOn": "2021-08-20T01:53:52.244Z",
          "wordCount": 594,
          "title": "Deep Contrastive Learning for Multi-View Network Embedding. (arXiv:2108.08296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zirui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Ziyi Ye</a>",
          "description": "Federated Learning (FL) is a privacy-protected machine learning paradigm that\nallows model to be trained directly at the edge without uploading data. One of\nthe biggest challenges faced by FL in practical applications is the\nheterogeneity of edge node data, which will slow down the convergence speed and\ndegrade the performance of the model. For the above problems, a representative\nsolution is to add additional constraints in the local training, such as\nFedProx, FedCurv and FedCL. However, the above algorithms still have room for\nimprovement. We propose to use the aggregation of all models obtained in the\npast as new constraint target to further improve the performance of such\nalgorithms. Experiments in various settings demonstrate that our method\nsignificantly improves the convergence speed and performance of the model.",
          "link": "http://arxiv.org/abs/2108.08577",
          "publishedOn": "2021-08-20T01:53:52.237Z",
          "wordCount": 563,
          "title": "Towards More Efficient Federated Learning with Better Optimization Objects. (arXiv:2108.08577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1\">Michal Sej&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1\">V&#xe1;clav Moravec</a>",
          "description": "This paper describes a novel dataset consisting of sentences with semantic\nsimilarity annotations. The data originate from the journalistic domain in the\nCzech language. We describe the process of collecting and annotating the data\nin detail. The dataset contains 138,556 human annotations divided into train\nand test sets. In total, 485 journalism students participated in the creation\nprocess. To increase the reliability of the test set, we compute the annotation\nas an average of 9 individual annotations. We evaluate the quality of the\ndataset by measuring inter and intra annotation annotators' agreements. Beside\nagreement numbers, we provide detailed statistics of the collected dataset. We\nconclude our paper with a baseline experiment of building a system for\npredicting the semantic similarity of sentences. Due to the massive number of\ntraining annotations (116 956), the model can perform significantly better than\nan average annotator (0,92 versus 0,86 of Person's correlation coefficients).",
          "link": "http://arxiv.org/abs/2108.08708",
          "publishedOn": "2021-08-20T01:53:52.222Z",
          "wordCount": 594,
          "title": "Czech News Dataset for Semanic Textual Similarity. (arXiv:2108.08708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sunwoong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanga Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1\">Kwanjung Yee</a>",
          "description": "Though inverse approach is computationally efficient in aerodynamic design as\nthe desired target performance distribution is specified, it has some\nsignificant limitations that prevent full efficiency from being achieved.\nFirst, the iterative procedure should be repeated whenever the specified target\ndistribution changes. Target distribution optimization can be performed to\nclarify the ambiguity in specifying this distribution, but several additional\nproblems arise in this process such as loss of the representation capacity due\nto parameterization of the distribution, excessive constraints for a realistic\ndistribution, inaccuracy of quantities of interest due to theoretical/empirical\npredictions, and the impossibility of explicitly imposing geometric\nconstraints. To deal with these issues, a novel inverse design optimization\nframework with a two-step deep learning approach is proposed. A variational\nautoencoder and multi-layer perceptron are used to generate a realistic target\ndistribution and predict the quantities of interest and shape parameters from\nthe generated distribution, respectively. Then, target distribution\noptimization is performed as the inverse design optimization. The proposed\nframework applies active learning and transfer learning techniques to improve\naccuracy and efficiency. Finally, the framework is validated through\naerodynamic shape optimizations of the airfoil of a wind turbine blade, where\ninverse design is actively being applied. The results of the optimizations show\nthat this framework is sufficiently accurate, efficient, and flexible to be\napplied to other inverse design engineering applications.",
          "link": "http://arxiv.org/abs/2108.08500",
          "publishedOn": "2021-08-20T01:53:52.215Z",
          "wordCount": 680,
          "title": "Inverse design optimization framework via a two-step deep learning approach: application to a wind turbine airfoil. (arXiv:2108.08500v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Minglei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yanfang Ye</a>",
          "description": "Illicit drug trafficking via social media sites such as Instagram has become\na severe problem, thus drawing a great deal of attention from law enforcement\nand public health agencies. How to identify illicit drug dealers from social\nmedia data has remained a technical challenge due to the following reasons. On\nthe one hand, the available data are limited because of privacy concerns with\ncrawling social media sites; on the other hand, the diversity of drug dealing\npatterns makes it difficult to reliably distinguish drug dealers from common\ndrug users. Unlike existing methods that focus on posting-based detection, we\npropose to tackle the problem of illicit drug dealer identification by\nconstructing a large-scale multimodal dataset named Identifying Drug Dealers on\nInstagram (IDDIG). Totally nearly 4,000 user accounts, of which over 1,400 are\ndrug dealers, have been collected from Instagram with multiple data sources\nincluding post comments, post images, homepage bio, and homepage images. We\nthen design a quadruple-based multimodal fusion method to combine the multiple\ndata sources associated with each user account for drug dealer identification.\nExperimental results on the constructed IDDIG dataset demonstrate the\neffectiveness of the proposed method in identifying drug dealers (almost 95%\naccuracy). Moreover, we have developed a hashtag-based community detection\ntechnique for discovering evolving patterns, especially those related to\ngeography and drug types.",
          "link": "http://arxiv.org/abs/2108.08301",
          "publishedOn": "2021-08-20T01:53:52.200Z",
          "wordCount": 662,
          "title": "Identifying Illicit Drug Dealers on Instagram with Large-scale Multimodal Data Fusion. (arXiv:2108.08301v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dolin_P/0/1/0/all/0/1\">Pavel Dolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dHauthuille_L/0/1/0/all/0/1\">Luc d&#x27;Hauthuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vattani_A/0/1/0/all/0/1\">Andrea Vattani</a>",
          "description": "Twitch chats pose a unique problem in natural language understanding due to a\nlarge presence of neologisms, specifically emotes. There are a total of 8.06\nmillion emotes, over 400k of which were used in the week studied. There is\nvirtually no information on the meaning or sentiment of emotes, and with a\nconstant influx of new emotes and drift in their frequencies, it becomes\nimpossible to maintain an updated manually-labeled dataset. Our paper makes a\ntwo fold contribution. First we establish a new baseline for sentiment analysis\non Twitch data, outperforming the previous supervised benchmark by 7.9% points.\nSecondly, we introduce a simple but powerful unsupervised framework based on\nword embeddings and k-NN to enrich existing models with out-of-vocabulary\nknowledge. This framework allows us to auto-generate a pseudo-dictionary of\nemotes and we show that we can nearly match the supervised benchmark above even\nwhen injecting such emote knowledge into sentiment classifiers trained on\nextraneous datasets such as movie reviews or Twitter.",
          "link": "http://arxiv.org/abs/2108.08411",
          "publishedOn": "2021-08-20T01:53:52.168Z",
          "wordCount": 591,
          "title": "FeelsGoodMan: Inferring Semantics of Twitch Neologisms. (arXiv:2108.08411v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_R/0/1/0/all/0/1\">Reyan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turja_M/0/1/0/all/0/1\">Md Asadullah Turja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahneh_F/0/1/0/all/0/1\">Faryad Darabi Sahneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_M/0/1/0/all/0/1\">Mithun Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobourov_S/0/1/0/all/0/1\">Stephen Kobourov</a>",
          "description": "Graph neural networks have been successful in many learning problems and\nreal-world applications. A recent line of research explores the power of graph\nneural networks to solve combinatorial and graph algorithmic problems such as\nsubgraph isomorphism, detecting cliques, and the traveling salesman problem.\nHowever, many NP-complete problems are as of yet unexplored using this method.\nIn this paper, we tackle the Steiner Tree Problem. We employ four learning\nframeworks to compute low cost Steiner trees: feed-forward neural networks,\ngraph neural networks, graph convolutional networks, and a graph attention\nmodel. We use these frameworks in two fundamentally different ways: 1) to train\nthe models to learn the actual Steiner tree nodes, 2) to train the model to\nlearn good Steiner point candidates to be connected to the constructed tree\nusing a shortest path in a greedy fashion. We illustrate the robustness of our\nheuristics on several random graph generation models as well as the SteinLib\ndata library. Our finding suggests that the out-of-the-box application of GNN\nmethods does worse than the classic 2-approximation method. However, when\ncombined with a greedy shortest path construction, it even does slightly better\nthan the 2-approximation algorithm. This result sheds light on the fundamental\ncapabilities and limitations of graph learning techniques on classical\nNP-complete problems.",
          "link": "http://arxiv.org/abs/2108.08368",
          "publishedOn": "2021-08-20T01:53:52.160Z",
          "wordCount": 643,
          "title": "Computing Steiner Trees using Graph Neural Networks. (arXiv:2108.08368v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yushan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seversky_L/0/1/0/all/0/1\">Lee Seversky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengtao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dahai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Houbing Song</a>",
          "description": "Anomaly detection has been a challenging task given high-dimensional\nmultivariate time series data generated by networked sensors and actuators in\nCyber-Physical Systems (CPS). Besides the highly nonlinear, complex, and\ndynamic natures of such time series, the lack of labeled data impedes data\nexploitation in a supervised manner and thus prevents an accurate detection of\nabnormal phenomenons. On the other hand, the collected data at the edge of the\nnetwork is often privacy sensitive and large in quantity, which may hinder the\ncentralized training at the main server. To tackle these issues, we propose an\nunsupervised time series anomaly detection framework in a federated fashion to\ncontinuously monitor the behaviors of interconnected devices within a network\nand alerts for abnormal incidents so that countermeasures can be taken before\nundesired consequences occur. To be specific, we leave the training data\ndistributed at the edge to learn a shared Variational Autoencoder (VAE) based\non Convolutional Gated Recurrent Unit (ConvGRU) model, which jointly captures\nfeature and temporal dependencies in the multivariate time series data for\nrepresentation learning and downstream anomaly detection tasks. Experiments on\nthree real-world networked sensor datasets illustrate the advantage of our\napproach over other state-of-the-art models. We also conduct extensive\nexperiments to demonstrate the effectiveness of our detection framework under\nnon-federated and federated settings in terms of overall performance and\ndetection latency.",
          "link": "http://arxiv.org/abs/2108.08404",
          "publishedOn": "2021-08-20T01:53:52.146Z",
          "wordCount": 674,
          "title": "Federated Variational Learning for Anomaly Detection in Multivariate Time Series. (arXiv:2108.08404v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08305",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1\">Lichuan Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1\">Royson Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mohamed S. Abdelfattah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_H/0/1/0/all/0/1\">Hongkai Wen</a>",
          "description": "Deep learning-based blind super-resolution (SR) methods have recently\nachieved unprecedented performance in upscaling frames with unknown\ndegradation. These models are able to accurately estimate the unknown\ndownscaling kernel from a given low-resolution (LR) image in order to leverage\nthe kernel during restoration. Although these approaches have largely been\nsuccessful, they are predominantly image-based and therefore do not exploit the\ntemporal properties of the kernels across multiple video frames. In this paper,\nwe investigated the temporal properties of the kernels and highlighted its\nimportance in the task of blind video super-resolution. Specifically, we\nmeasured the kernel temporal consistency of real-world videos and illustrated\nhow the estimated kernels might change per frame in videos of varying\ndynamicity of the scene and its objects. With this new insight, we revisited\nprevious popular video SR approaches, and showed that previous assumptions of\nusing a fixed kernel throughout the restoration process can lead to visual\nartifacts when upscaling real-world videos. In order to counteract this, we\ntailored existing single-image and video SR techniques to leverage kernel\nconsistency during both kernel estimation and video upscaling processes.\nExtensive experiments on synthetic and real-world videos show substantial\nrestoration gains quantitatively and qualitatively, achieving the new\nstate-of-the-art in blind video SR and underlining the potential of exploiting\nkernel temporal consistency.",
          "link": "http://arxiv.org/abs/2108.08305",
          "publishedOn": "2021-08-20T01:53:52.138Z",
          "wordCount": 665,
          "title": "Temporal Kernel Consistency for Blind Video Super-Resolution. (arXiv:2108.08305v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08467",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Niyas_S/0/1/0/all/0/1\">S. Niyas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1\">M Anand Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "Computer-aided medical image analysis plays a significant role in assisting\nmedical practitioners for expert clinical diagnosis and deciding the optimal\ntreatment plan. At present, convolutional neural networks (CNN) are the\npreferred choice for medical image analysis. In addition, with the rapid\nadvancements in three-dimensional (3D) imaging systems and the availability of\nexcellent hardware and software support to process large volumes of data, 3D\ndeep learning methods are gaining popularity in medical image analysis. Here,\nwe present an extensive review of the recently evolved 3D deep learning methods\nin medical image segmentation. Furthermore, the research gaps and future\ndirections in 3D medical image segmentation are discussed.",
          "link": "http://arxiv.org/abs/2108.08467",
          "publishedOn": "2021-08-20T01:53:52.112Z",
          "wordCount": 574,
          "title": "Medical Image Segmentation using 3D Convolutional Neural Networks: A Review. (arXiv:2108.08467v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Ming Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xianzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>",
          "description": "Federated learning (FL) can protect data privacy in distributed learning\nsince it merely collects local gradients from users without access to their\ndata. However, FL is fragile in the presence of heterogeneity that is commonly\nencountered in practical settings, e.g., non-IID data over different users.\nExisting FL approaches usually update a single global model to capture the\nshared knowledge of all users by aggregating their gradients, regardless of the\ndiscrepancy between their data distributions. By comparison, a mixture of\nmultiple global models could capture the heterogeneity across various users if\nassigning the users to different global models (i.e., centers) in FL. To this\nend, we propose a novel multi-center aggregation mechanism . It learns multiple\nglobal models from data, and simultaneously derives the optimal matching\nbetween users and centers. We then formulate it as a bi-level optimization\nproblem that can be efficiently solved by a stochastic expectation maximization\n(EM) algorithm. Experiments on multiple benchmark datasets of FL show that our\nmethod outperforms several popular FL competitors. The source code are open\nsource on Github.",
          "link": "http://arxiv.org/abs/2108.08647",
          "publishedOn": "2021-08-20T01:53:52.105Z",
          "wordCount": 610,
          "title": "Multi-Center Federated Learning. (arXiv:2108.08647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharifnassab_A/0/1/0/all/0/1\">Arsalan Sharifnassab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehkaleybar_S/0/1/0/all/0/1\">Saber Salehkaleybar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golestani_S/0/1/0/all/0/1\">S. Jamaloddin Golestani</a>",
          "description": "We consider the problem of federated learning in a one-shot setting in which\nthere are $m$ machines, each observing $n$ samples function from an unknown\ndistribution on non-convex loss functions. Let $F:[-1,1]^d\\to\\mathbb{R}$ be the\nexpected loss function with respect to this unknown distribution. The goal is\nto find an estimate of the minimizer of $F$. Based on its observations, each\nmachine generates a signal of bounded length $B$ and sends it to a server. The\nsever collects signals of all machines and outputs an estimate of the minimizer\nof $F$. We propose a distributed learning algorithm, called Multi-Resolution\nEstimator for Non-Convex loss function (MRE-NC), whose expected error is\nbounded by $\\max\\big(1/\\sqrt{n}(mB)^{1/d}, 1/\\sqrt{mn}\\big)$, up to\npolylogarithmic factors. We also provide a matching lower bound on the\nperformance of any algorithm, showing that MRE-NC is order optimal in terms of\n$n$ and $m$. Experiments on synthetic and real data show the effectiveness of\nMRE-NC in distributed learning of model's parameters for non-convex loss\nfunctions.",
          "link": "http://arxiv.org/abs/2108.08677",
          "publishedOn": "2021-08-20T01:53:52.092Z",
          "wordCount": 592,
          "title": "Order Optimal One-Shot Federated Learning for non-Convex Loss Functions. (arXiv:2108.08677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junna~Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuisheng~Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_%7E/0/1/0/all/0/1\">~Cui~Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuan Zhang</a>",
          "description": "Kernel logistic regression (KLR) is a classical nonlinear classifier in\nstatistical machine learning. Newton method with quadratic convergence rate can\nsolve KLR problem more effectively than the gradient method. However, an\nobvious limitation of Newton method for training large-scale problems is the\n$O(n^{3})$ time complexity and $O(n^{2})$ space complexity, where $n$ is the\nnumber of training instances. In this paper, we employ the multilevel circulant\nmatrix (MCM) approximate kernel matrix to save in storage space and accelerate\nthe solution of the KLR. Combined with the characteristics of MCM and our\ningenious design, we propose an MCM approximate Newton iterative method. We\nfirst simplify the Newton direction according to the semi-positivity of the\nkernel matrix and then perform a two-step approximation of the Newton direction\nby using MCM. Our method reduces the time complexity of each iteration to $O(n\n\\log n)$ by using the multidimensional fast Fourier transform (mFFT). In\naddition, the space complexity can be reduced to $O(n)$ due to the built-in\nperiodicity of MCM. Experimental results on some large-scale binary and\nmulti-classification problems show that our method makes KLR scalable for\nlarge-scale problems, with less memory consumption, and converges to test\naccuracy without sacrifice in a shorter time.",
          "link": "http://arxiv.org/abs/2108.08605",
          "publishedOn": "2021-08-20T01:53:52.085Z",
          "wordCount": 631,
          "title": "Using Multilevel Circulant Matrix Approximate to Speed Up Kernel Logistic Regression. (arXiv:2108.08605v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weicheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "This paper studies the relative importance of attention heads in\nTransformer-based models to aid their interpretability in cross-lingual and\nmulti-lingual tasks. Prior research has found that only a few attention heads\nare important in each mono-lingual Natural Language Processing (NLP) task and\npruning the remaining heads leads to comparable or improved performance of the\nmodel. However, the impact of pruning attention heads is not yet clear in\ncross-lingual and multi-lingual tasks. Through extensive experiments, we show\nthat (1) pruning a number of attention heads in a multi-lingual\nTransformer-based model has, in general, positive effects on its performance in\ncross-lingual and multi-lingual tasks and (2) the attention heads to be pruned\ncan be ranked using gradients and identified with a few trial experiments. Our\nexperiments focus on sequence labeling tasks, with potential applicability on\nother cross-lingual and multi-lingual tasks. For comprehensiveness, we examine\ntwo pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and\nXLM-R, on three tasks across 9 languages each. We also discuss the validity of\nour findings and their extensibility to truly resource-scarce languages and\nother task settings.",
          "link": "http://arxiv.org/abs/2108.08375",
          "publishedOn": "2021-08-20T01:53:52.068Z",
          "wordCount": 626,
          "title": "Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks. (arXiv:2108.08375v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samajdar_A/0/1/0/all/0/1\">Ananda Samajdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1\">Jan Moritz Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denton_M/0/1/0/all/0/1\">Matthew Denton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1\">Tushar Krishna</a>",
          "description": "Design space exploration is an important but costly step involved in the\ndesign/deployment of custom architectures to squeeze out maximum possible\nperformance and energy efficiency. Conventionally, optimizations require\niterative sampling of the design space using simulation or heuristic tools. In\nthis paper we investigate the possibility of learning the optimization task\nusing machine learning and hence using the learnt model to predict optimal\nparameters for the design and mapping space of custom architectures, bypassing\nany exploration step. We use three case studies involving the optimal array\ndesign, SRAM buffer sizing, mapping, and schedule determination for\nsystolic-array-based custom architecture design and mapping space. Within the\npurview of these case studies, we show that it is possible to capture the\ndesign space and train a model to \"generalize\" prediction the optimal design\nand mapping parameters when queried with workload and design constraints. We\nperform systematic design-aware and statistical analysis of the optimization\nspace for our case studies and highlight the patterns in the design space. We\nformulate the architecture design and mapping as a machine learning problem\nthat allows us to leverage existing ML models for training and inference. We\ndesign and train a custom network architecture called AIRCHITECT, which is\ncapable of learning the architecture design space with as high as 94.3% test\naccuracy and predicting optimal configurations which achieve on average\n(GeoMean) of 99.9% the best possible performance on a test dataset with $10^5$\nGEMM workloads.",
          "link": "http://arxiv.org/abs/2108.08295",
          "publishedOn": "2021-08-20T01:53:52.060Z",
          "wordCount": 674,
          "title": "AIRCHITECT: Learning Custom Architecture Design and Mapping Space. (arXiv:2108.08295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.",
          "link": "http://arxiv.org/abs/2108.08468",
          "publishedOn": "2021-08-20T01:53:52.022Z",
          "wordCount": 701,
          "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirignano_J/0/1/0/all/0/1\">Justin Sirignano</a>",
          "description": "Actor-critic algorithms are widely used in reinforcement learning, but are\nchallenging to mathematically analyze due to the online arrival of non-i.i.d.\ndata samples. The distribution of the data samples dynamically changes as the\nmodel is updated, introducing a complex feedback loop between the data\ndistribution and the reinforcement learning algorithm. We prove that, under a\ntime rescaling, the online actor-critic algorithm with tabular parametrization\nconverges to an ordinary differential equations (ODEs) as the number of updates\nbecomes large. The proof first establishes the geometric ergodicity of the data\nsamples under a fixed actor policy. Then, using a Poisson equation, we prove\nthat the fluctuations of the data samples around a dynamic probability measure,\nwhich is a function of the evolving actor model, vanish as the number of\nupdates become large. Once the ODE limit has been derived, we study its\nconvergence properties using a two time-scale analysis which asymptotically\nde-couples the critic ODE from the actor ODE. The convergence of the critic to\nthe solution of the Bellman equation and the actor to the optimal policy are\nproven. In addition, a convergence rate to this global minimum is also\nestablished. Our convergence analysis holds under specific choices for the\nlearning rates and exploration rates in the actor-critic algorithm, which could\nprovide guidance for the implementation of actor-critic algorithms in practice.",
          "link": "http://arxiv.org/abs/2108.08655",
          "publishedOn": "2021-08-20T01:53:52.000Z",
          "wordCount": 668,
          "title": "Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. (arXiv:2108.08655v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Peixi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Li Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Recently, the generalization behavior of Convolutional Neural Networks (CNN)\nis gradually transparent through explanation techniques with the frequency\ncomponents decomposition. However, the importance of the phase spectrum of the\nimage for a robust vision system is still ignored. In this paper, we notice\nthat the CNN tends to converge at the local optimum which is closely related to\nthe high-frequency components of the training images, while the amplitude\nspectrum is easily disturbed such as noises or common corruptions. In contrast,\nmore empirical studies found that humans rely on more phase components to\nachieve robust recognition. This observation leads to more explanations of the\nCNN's generalization behaviors in both robustness to common perturbations and\nout-of-distribution detection, and motivates a new perspective on data\naugmentation designed by re-combing the phase spectrum of the current image and\nthe amplitude spectrum of the distracter image. That is, the generated samples\nforce the CNN to pay more attention to the structured information from phase\ncomponents and keep robust to the variation of the amplitude. Experiments on\nseveral image datasets indicate that the proposed method achieves\nstate-of-the-art performances on multiple generalizations and calibration\ntasks, including adaptability for common corruptions and surface variations,\nout-of-distribution detection, and adversarial attack.",
          "link": "http://arxiv.org/abs/2108.08487",
          "publishedOn": "2021-08-20T01:53:51.969Z",
          "wordCount": 654,
          "title": "Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain. (arXiv:2108.08487v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08350",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1\">Shanny Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>",
          "description": "Accurately modeling power distribution grids is crucial for designing\neffective monitoring and decision making algorithms. This paper addresses the\npartial observability issue of data-driven distribution modeling in order to\nimprove the accuracy of line parameter estimation. Inspired by the sparse\nchanges in residential loads, we advocate to regularize the group sparsity of\nthe unobservable injections in a bi-linear estimation problem. The alternating\nminimization scheme of guaranteed convergence is proposed to take advantage of\nconvex subproblems with efficient solutions. Numerical results using real-world\nload data on the single-phase equivalent of the IEEE 123-bus test case have\ndemonstrated the accuracy improvements of the proposed solution over existing\nwork for both parameter estimation and voltage modeling.",
          "link": "http://arxiv.org/abs/2108.08350",
          "publishedOn": "2021-08-20T01:53:51.955Z",
          "wordCount": 547,
          "title": "Data-driven Modeling for Distribution Grids Under Partial Observability. (arXiv:2108.08350v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tauscher_Z/0/1/0/all/0/1\">Zachary Tauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yushan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Houbing Song</a>",
          "description": "With massive data being generated daily and the ever-increasing\ninterconnectivity of the world's Internet infrastructures, a machine learning\nbased intrusion detection system (IDS) has become a vital component to protect\nour economic and national security. In this paper, we perform a comprehensive\nstudy on NSL-KDD, a network traffic dataset, by visualizing patterns and\nemploying different learning-based models to detect cyber attacks. Unlike\nprevious shallow learning and deep learning models that use the single learning\nmodel approach for intrusion detection, we adopt a hierarchy strategy, in which\nthe intrusion and normal behavior are classified firstly, and then the specific\ntypes of attacks are classified. We demonstrate the advantage of the\nunsupervised representation learning model in binary intrusion detection tasks.\nBesides, we alleviate the data imbalance problem with SVM-SMOTE oversampling\ntechnique in 4-class classification and further demonstrate the effectiveness\nand the drawback of the oversampling mechanism with a deep neural network as a\nbase model.",
          "link": "http://arxiv.org/abs/2108.08394",
          "publishedOn": "2021-08-20T01:53:51.948Z",
          "wordCount": 610,
          "title": "Learning to Detect: A Data-driven Approach for Network Intrusion Detection. (arXiv:2108.08394v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-08-20T01:53:51.828Z",
          "wordCount": 613,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haghtalab_N/0/1/0/all/0/1\">Nika Haghtalab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roughgarden_T/0/1/0/all/0/1\">Tim Roughgarden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1\">Abhishek Shetty</a>",
          "description": "We prove novel algorithmic guarantees for several online problems in the\nsmoothed analysis model. In this model, at each time an adversary chooses an\ninput distribution with density function bounded above by $\\tfrac{1}{\\sigma}$\ntimes that of the uniform distribution; nature then samples an input from this\ndistribution. Crucially, our results hold for {\\em adaptive} adversaries that\ncan choose an input distribution based on the decisions of the algorithm and\nthe realizations of the inputs in the previous time steps.\n\nThis paper presents a general technique for proving smoothed algorithmic\nguarantees against adaptive adversaries, in effect reducing the setting of\nadaptive adversaries to the simpler case of oblivious adversaries. We apply\nthis technique to prove strong smoothed guarantees for three problems:\n\n-Online learning: We consider the online prediction problem, where instances\nare generated from an adaptive sequence of $\\sigma$-smooth distributions and\nthe hypothesis class has VC dimension $d$. We bound the regret by\n$\\tilde{O}\\big(\\sqrt{T d\\ln(1/\\sigma)} + d\\sqrt{\\ln(T/\\sigma)}\\big)$. This\nanswers open questions of [RST11,Hag18].\n\n-Online discrepancy minimization: We consider the online Koml\\'os problem,\nwhere the input is generated from an adaptive sequence of $\\sigma$-smooth and\nisotropic distributions on the $\\ell_2$ unit ball. We bound the $\\ell_\\infty$\nnorm of the discrepancy vector by $\\tilde{O}\\big(\\ln^2\\!\\big(\n\\frac{nT}{\\sigma}\\big) \\big)$.\n\n-Dispersion in online optimization: We consider online optimization of\npiecewise Lipschitz functions where functions with $\\ell$ discontinuities are\nchosen by a smoothed adaptive adversary and show that the resulting sequence is\n$\\big( {\\sigma}/{\\sqrt{T\\ell}}, \\tilde O\\big(\\sqrt{T\\ell}\n\\big)\\big)$-dispersed. This matches the parameters of [BDV18] for oblivious\nadversaries, up to log factors.",
          "link": "http://arxiv.org/abs/2102.08446",
          "publishedOn": "2021-08-20T01:53:51.809Z",
          "wordCount": 719,
          "title": "Smoothed Analysis with Adaptive Adversaries. (arXiv:2102.08446v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1\">Ilias Diakonikolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1\">Daniel M. Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontonis_V/0/1/0/all/0/1\">Vasilis Kontonis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1\">Christos Tzamos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarifis_N/0/1/0/all/0/1\">Nikos Zarifis</a>",
          "description": "We study the problem of PAC learning halfspaces on $\\mathbb{R}^d$ with\nMassart noise under Gaussian marginals. In the Massart noise model, an\nadversary is allowed to flip the label of each point $\\mathbf{x}$ with\nprobability $\\eta(\\mathbf{x}) \\leq \\eta$, for some parameter $\\eta \\in\n[0,1/2]$.\n\nThe goal of the learner is to output a hypothesis with missclassification\nerror $\\mathrm{opt} + \\epsilon$, where $\\mathrm{opt}$ is the error of the\ntarget halfspace. Prior work studied this problem assuming that the target\nhalfspace is homogeneous and that the parameter $\\eta$ is strictly smaller than\n$1/2$. We explore how the complexity of the problem changes when either of\nthese assumptions is removed, establishing the following threshold phenomena:\n\nFor $\\eta = 1/2$, we prove a lower bound of $d^{\\Omega (\\log(1/\\epsilon))}$\non the complexity of any Statistical Query (SQ) algorithm for the problem,\nwhich holds even for homogeneous halfspaces. On the positive side, we give a\nnew learning algorithm for arbitrary halfspaces in this regime with sample\ncomplexity and running time $O_\\epsilon(1) \\, d^{O(\\log(1/\\epsilon))}$.\n\nFor $\\eta <1/2$, we establish a lower bound of $d^{\\Omega(\\log(1/\\gamma))}$\non the SQ complexity of the problem, where $\\gamma = \\max\\{\\epsilon,\n\\min\\{\\mathbf{Pr}[f(\\mathbf{x}) = 1], \\mathbf{Pr}[f(\\mathbf{x}) = -1]\\} \\}$ and\n$f$ is the target halfspace. In particular, this implies an SQ lower bound of\n$d^{\\Omega (\\log(1/\\epsilon) )}$ for learning arbitrary Massart halfspaces\n(even for small constant $\\eta$). We complement this lower bound with a new\nlearning algorithm for this regime with sample complexity and runtime\n$d^{O_{\\eta}(\\log(1/\\gamma))} \\mathrm{poly}(1/\\epsilon)$.\n\nTaken together, our results qualitatively characterize the complexity of\nlearning halfspaces in the Massart model.",
          "link": "http://arxiv.org/abs/2108.08767",
          "publishedOn": "2021-08-20T01:53:51.753Z",
          "wordCount": 709,
          "title": "Threshold Phenomena in Learning Halfspaces with Massart Noise. (arXiv:2108.08767v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngkee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1\">Won Joon Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youn Kyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>",
          "description": "In modern deep learning research, finding optimal (or near optimal) neural\nnetwork models is one of major research directions and it is widely studied in\nmany applications. In this paper, the main research trends of neural\narchitecture search (NAS) are classified as neuro-evolutionary algorithms,\nreinforcement learning based algorithms, and one-shot architecture search\napproaches. Furthermore, each research trend is introduced and finally all the\nmajor three trends are compared. Lastly, the future research directions of NAS\nresearch trends are discussed.",
          "link": "http://arxiv.org/abs/2108.08474",
          "publishedOn": "2021-08-20T01:53:51.742Z",
          "wordCount": 534,
          "title": "Trends in Neural Architecture Search: Towards the Acceleration of Search. (arXiv:2108.08474v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovachki_N/0/1/0/all/0/1\">Nikola Kovachki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Burigede Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1\">Kamyar Azizzadenesheli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kaushik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuart_A/0/1/0/all/0/1\">Andrew Stuart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>",
          "description": "The classical development of neural networks has primarily focused on\nlearning mappings between finite dimensional Euclidean spaces or finite sets.\nWe propose a generalization of neural networks tailored to learn operators\nmapping between infinite dimensional function spaces. We formulate the\napproximation of operators by composition of a class of linear integral\noperators and nonlinear activation functions, so that the composed operator can\napproximate complex nonlinear operators. Furthermore, we introduce four classes\nof operator parameterizations: graph-based operators, low-rank operators,\nmultipole graph-based operators, and Fourier operators and describe efficient\nalgorithms for computing with each one. The proposed neural operators are\nresolution-invariant: they share the same network parameters between different\ndiscretizations of the underlying function spaces and can be used for zero-shot\nsuper-resolutions. Numerically, the proposed models show superior performance\ncompared to existing machine learning based methodologies on Burgers' equation,\nDarcy flow, and the Navier-Stokes equation, while being several order of\nmagnitude faster compared to conventional PDE solvers.",
          "link": "http://arxiv.org/abs/2108.08481",
          "publishedOn": "2021-08-20T01:53:51.735Z",
          "wordCount": 595,
          "title": "Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "Although automated metrics are commonly used to evaluate NLG systems, they\noften correlate poorly with human judgements. Newer metrics such as BERTScore\nhave addressed many weaknesses in prior metrics such as BLEU and ROUGE, which\nrely on n-gram matching. These newer methods, however, are still limited in\nthat they do not consider the generation context, so they cannot properly\nreward generated text that is correct but deviates from the given reference.\n\nIn this paper, we propose Language Model Augmented Relevance Score (MARS), a\nnew context-aware metric for NLG evaluation. MARS leverages off-the-shelf\nlanguage models, guided by reinforcement learning, to create augmented\nreferences that consider both the generation context and available human\nreferences, which are then used as additional references to score generated\ntext. Compared with seven existing metrics in three common NLG tasks, MARS not\nonly achieves higher correlation with human reference judgements, but also\ndifferentiates well-formed candidates from adversarial samples to a larger\ndegree.",
          "link": "http://arxiv.org/abs/2108.08485",
          "publishedOn": "2021-08-20T01:53:51.717Z",
          "wordCount": 589,
          "title": "Language Model Augmented Relevance Score. (arXiv:2108.08485v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Sen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weishen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>",
          "description": "Federated learning (FL) has gain growing interests for its capability of\nlearning from distributed data sources collectively without the need of\naccessing the raw data samples across different sources. So far FL research has\nmostly focused on improving the performance, how the algorithmic disparity will\nbe impacted for the model learned from FL and the impact of algorithmic\ndisparity on the utility inconsistency are largely unexplored. In this paper,\nwe propose an FL framework to jointly consider performance consistency and\nalgorithmic fairness across different local clients (data sources). We derive\nour framework from a constrained multi-objective optimization perspective, in\nwhich we learn a model satisfying fairness constraints on all clients with\nconsistent performance. Specifically, we treat the algorithm prediction loss at\neach local client as an objective and maximize the worst-performing client with\nfairness constraints through optimizing a surrogate maximum function with all\nobjectives involved. A gradient-based procedure is employed to achieve the\nPareto optimality of this optimization problem. Theoretical analysis is\nprovided to prove that our method can converge to a Pareto solution that\nachieves the min-max performance with fairness constraints on all clients.\nComprehensive experiments on synthetic and real-world datasets demonstrate the\nsuperiority that our approach over baselines and its effectiveness in achieving\nboth fairness and consistency across all local clients.",
          "link": "http://arxiv.org/abs/2108.08435",
          "publishedOn": "2021-08-20T01:53:51.710Z",
          "wordCount": 639,
          "title": "Fair and Consistent Federated Learning. (arXiv:2108.08435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>",
          "description": "Self-supervised learning has been successfully applied to pre-train video\nrepresentations, which aims at efficient adaptation from pre-training domain to\ndownstream tasks. Existing approaches merely leverage contrastive loss to learn\ninstance-level discrimination. However, lack of category information will lead\nto hard-positive problem that constrains the generalization ability of this\nkind of methods. We find that the multi-task process of meta learning can\nprovide a solution to this problem. In this paper, we propose a\nMeta-Contrastive Network (MCN), which combines the contrastive learning and\nmeta learning, to enhance the learning ability of existing self-supervised\napproaches. Our method contains two training stages based on model-agnostic\nmeta learning (MAML), each of which consists of a contrastive branch and a meta\nbranch. Extensive evaluations demonstrate the effectiveness of our method. For\ntwo downstream tasks, i.e., video action recognition and video retrieval, MCN\noutperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. To be\nmore specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8%\nand 54.5% for video action recognition, as well as 52.5% and 23.7% for video\nretrieval.",
          "link": "http://arxiv.org/abs/2108.08426",
          "publishedOn": "2021-08-20T01:53:51.703Z",
          "wordCount": 618,
          "title": "Self-Supervised Video Representation Learning with Meta-Contrastive Network. (arXiv:2108.08426v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dunjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>",
          "description": "With the popularity of blockchain technology, the financial security issues\nof blockchain transaction networks have become increasingly serious. Phishing\nscam detection methods will protect possible victims and build a healthier\nblockchain ecosystem. Usually, the existing works define phishing scam\ndetection as a node classification task by learning the potential features of\nusers through graph embedding methods such as random walk or graph neural\nnetwork (GNN). However, these detection methods are suffered from high\ncomplexity due to the large scale of the blockchain transaction network,\nignoring temporal information of the transaction. Addressing this problem, we\ndefined the transaction pattern graphs for users and transformed the phishing\nscam detection into a graph classification task. To extract richer information\nfrom the input graph, we proposed a multi-channel graph classification model\n(MCGC) with multiple feature extraction channels for GNN. The transaction\npattern graphs and MCGC are more able to detect potential phishing scammers by\nextracting the transaction pattern features of the target users. Extensive\nexperiments on seven benchmark and Ethereum datasets demonstrate that the\nproposed MCGC can not only achieve state-of-the-art performance in the graph\nclassification task but also achieve effective phishing scam detection based on\nthe target users' transaction pattern graphs.",
          "link": "http://arxiv.org/abs/2108.08456",
          "publishedOn": "2021-08-20T01:53:51.697Z",
          "wordCount": 624,
          "title": "Blockchain Phishing Scam Detection via Multi-channel Graph Classification. (arXiv:2108.08456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "The unsupervised domain adaptation (UDA) has been widely adopted to alleviate\nthe data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is adapted for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vector that violates the poset constraints by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-08-19T01:35:03.309Z",
          "wordCount": 651,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07958",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yuksel_O/0/1/0/all/0/1\">Oguz Kaan Yuksel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chavdarova_T/0/1/0/all/0/1\">Tatjana Chavdarova</a>",
          "description": "Data augmentation is a widely adopted technique for avoiding overfitting when\ntraining deep neural networks. However, this approach requires domain-specific\nknowledge and is often limited to a fixed set of hard-coded transformations.\nRecently, several works proposed to use generative models for generating\nsemantically meaningful perturbations to train a classifier. However, because\naccurate encoding and decoding are critical, these methods, which use\narchitectures that approximate the latent-variable inference, remained limited\nto pilot studies on small datasets.\n\nExploiting the exactly reversible encoder-decoder structure of normalizing\nflows, we perform on-manifold perturbations in the latent space to define fully\nunsupervised data augmentations. We demonstrate that such perturbations match\nthe performance of advanced data augmentation techniques -- reaching 96.6% test\naccuracy for CIFAR-10 using ResNet-18 and outperform existing methods,\nparticularly in low data regimes -- yielding 10--25% relative improvement of\ntest accuracy from classical training. We find that our latent adversarial\nperturbations adaptive to the classifier throughout its training are most\neffective, yielding the first test accuracy improvement results on real-world\ndatasets -- CIFAR-10/100 -- via latent-space perturbations.",
          "link": "http://arxiv.org/abs/2108.07958",
          "publishedOn": "2021-08-19T01:35:03.169Z",
          "wordCount": 620,
          "title": "Semantic Perturbations with Normalizing Flows for Improved Generalization. (arXiv:2108.07958v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yizeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>",
          "description": "In this paper, we explore the spatial redundancy in video recognition with\nthe aim to improve the computational efficiency. It is observed that the most\ninformative region in each frame of a video is usually a small image patch,\nwhich shifts smoothly across frames. Therefore, we model the patch localization\nproblem as a sequential decision task, and propose a reinforcement learning\nbased approach for efficient spatially adaptive video recognition (AdaFocus).\nIn specific, a light-weighted ConvNet is first adopted to quickly process the\nfull video sequence, whose features are used by a recurrent policy network to\nlocalize the most task-relevant regions. Then the selected patches are inferred\nby a high-capacity network for the final prediction. During offline inference,\nonce the informative patch sequence has been generated, the bulk of computation\ncan be done in parallel, and is efficient on modern GPU devices. In addition,\nwe demonstrate that the proposed method can be easily extended by further\nconsidering the temporal redundancy, e.g., dynamically skipping less valuable\nframes. Extensive experiments on five benchmark datasets, i.e., ActivityNet,\nFCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is\nsignificantly more efficient than the competitive baselines. Code is available\nat https://github.com/blackfeather-wang/AdaFocus.",
          "link": "http://arxiv.org/abs/2105.03245",
          "publishedOn": "2021-08-19T01:35:03.162Z",
          "wordCount": 675,
          "title": "Adaptive Focus for Efficient Video Recognition. (arXiv:2105.03245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarpour_S/0/1/0/all/0/1\">Saber Jafarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davydov_A/0/1/0/all/0/1\">Alexander Davydov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proskurnikov_A/0/1/0/all/0/1\">Anton V. Proskurnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullo_F/0/1/0/all/0/1\">Francesco Bullo</a>",
          "description": "Implicit neural networks, a.k.a., deep equilibrium networks, are a class of\nimplicit-depth learning models where function evaluation is performed by\nsolving a fixed point equation. They generalize classic feedforward models and\nare equivalent to infinite-depth weight-tied feedforward networks. While\nimplicit models show improved accuracy and significant reduction in memory\nconsumption, they can suffer from ill-posedness and convergence instability.\n\nThis paper provides a new framework to design well-posed and robust implicit\nneural networks based upon contraction theory for the non-Euclidean norm\n$\\ell_\\infty$. Our framework includes (i) a novel condition for well-posedness\nbased on one-sided Lipschitz constants, (ii) an average iteration for computing\nfixed-points, and (iii) explicit estimates on input-output Lipschitz constants.\nAdditionally, we design a training problem with the well-posedness condition\nand the average iteration as constraints and, to achieve robust models, with\nthe input-output Lipschitz constant as a regularizer. Our $\\ell_\\infty$\nwell-posedness condition leads to a larger polytopic training search space than\nexisting conditions and our average iteration enjoys accelerated convergence.\nFinally, we perform several numerical experiments for function estimation and\ndigit classification through the MNIST data set. Our numerical results\ndemonstrate improved accuracy and robustness of the implicit models with\nsmaller input-output Lipschitz bounds.",
          "link": "http://arxiv.org/abs/2106.03194",
          "publishedOn": "2021-08-19T01:35:03.115Z",
          "wordCount": 678,
          "title": "Robust Implicit Networks via Non-Euclidean Contractions. (arXiv:2106.03194v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaodian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wanhang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuihai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>",
          "description": "In recent years, federated learning (FL) has been widely applied for\nsupporting decentralized collaborative learning scenarios. Among existing FL\nmodels, federated logistic regression (FLR) is a widely used statistic model\nand has been used in various industries. To ensure data security and user\nprivacy, FLR leverages homomorphic encryption (HE) to protect the exchanged\ndata among different collaborative parties. However, HE introduces significant\ncomputational overhead (i.e., the cost of data encryption/decryption and\ncalculation over encrypted data), which eventually becomes the performance\nbottleneck of the whole system. In this paper, we propose HAFLO, a GPU-based\nsolution to improve the performance of FLR. The core idea of HAFLO is to\nsummarize a set of performance-critical homomorphic operators (HO) used by FLR\nand accelerate the execution of these operators through a joint optimization of\nstorage, IO, and computation. The preliminary results show that our\nacceleration on FATE, a popular FL framework, achieves a 49.9$\\times$ speedup\nfor heterogeneous LR and 88.4$\\times$ for homogeneous LR.",
          "link": "http://arxiv.org/abs/2107.13797",
          "publishedOn": "2021-08-19T01:35:02.928Z",
          "wordCount": 606,
          "title": "HAFLO: GPU-Based Acceleration for Federated Logistic Regression. (arXiv:2107.13797v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunrui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camporese_G/0/1/0/all/0/1\">Guglielmo Camporese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenjing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1\">Alessandro Sperduti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>",
          "description": "In open set recognition, a classifier has to detect unknown classes that are\nnot known at training time. In order to recognize new categories, the\nclassifier has to project the input samples of known classes in very compact\nand separated regions of the features space for discriminating samples of\nunknown classes. Recently proposed Capsule Networks have shown to outperform\nalternatives in many fields, particularly in image recognition, however they\nhave not been fully applied yet to open-set recognition. In capsule networks,\nscalar neurons are replaced by capsule vectors or matrices, whose entries\nrepresent different properties of objects. In our proposal, during training,\ncapsules features of the same known class are encouraged to match a pre-defined\ngaussian, one for each class. To this end, we use the variational autoencoder\nframework, with a set of gaussian priors as the approximation for the posterior\ndistribution. In this way, we are able to control the compactness of the\nfeatures of the same class around the center of the gaussians, thus controlling\nthe ability of the classifier in detecting samples from unknown classes. We\nconducted several experiments and ablation of our model, obtaining state of the\nart results on different datasets in the open set recognition and unknown\ndetection tasks.",
          "link": "http://arxiv.org/abs/2104.09159",
          "publishedOn": "2021-08-19T01:35:02.807Z",
          "wordCount": 683,
          "title": "Conditional Variational Capsule Network for Open Set Recognition. (arXiv:2104.09159v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shota Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Learning from positive and unlabeled (PU) data is an important problem in\nvarious applications. Most of the recent approaches for PU classification\nassume that the class-prior (the ratio of positive samples) in the training\nunlabeled dataset is identical to that of the test data, which does not hold in\nmany practical cases. In addition, we usually do not know the class-priors of\nthe training and test data, thus we have no clue on how to train a classifier\nwithout them. To address these problems, we propose a novel PU classification\nmethod based on density ratio estimation. A notable advantage of our proposed\nmethod is that it does not require the class-priors in the training phase;\nclass-prior shift is incorporated only in the test phase. We theoretically\njustify our proposed method and experimentally demonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2107.05045",
          "publishedOn": "2021-08-19T01:35:02.609Z",
          "wordCount": 593,
          "title": "Positive-Unlabeled Classification under Class-Prior Shift: A Prior-invariant Approach Based on Density Ratio Estimation. (arXiv:2107.05045v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyun Zhang</a>",
          "description": "Sequence-to-sequence models provide a viable new approach to generative\nsummarization, allowing models that are no longer limited to simply selecting\nand recombining sentences from the original text. However, these models have\nthree drawbacks: their grasp of the details of the original text is often\ninaccurate, and the text generated by such models often has repetitions, while\nit is difficult to handle words that are beyond the word list. In this paper,\nwe propose a new architecture that combines reinforcement learning and\nadversarial generative networks to enhance the sequence-to-sequence attention\nmodel. First, we use a hybrid pointer-generator network that copies words\ndirectly from the source text, contributing to accurate reproduction of\ninformation without sacrificing the ability of generators to generate new\nwords. Second, we use both intra-temporal and intra-decoder attention to\npenalize summarized content and thus discourage repetition. We apply our model\nto our own proposed COVID-19 paper title summarization task and achieve close\napproximations to the current model on ROUEG, while bringing better\nreadability.",
          "link": "http://arxiv.org/abs/2105.15176",
          "publishedOn": "2021-08-19T01:35:02.518Z",
          "wordCount": 668,
          "title": "Reinforced Generative Adversarial Network for Abstractive Text Summarization. (arXiv:2105.15176v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (1) ((1) ETH Zurich, (2) Google)",
          "description": "Deep generative models can synthesize photorealistic images of human faces\nwith novel identities. However, a key challenge to the wide applicability of\nsuch techniques is to provide independent control over semantically meaningful\nparameters: appearance, head pose, face shape, and facial expressions. In this\npaper, we propose VariTex - to the best of our knowledge the first method that\nlearns a variational latent feature space of neural face textures, which allows\nsampling of novel identities. We combine this generative model with a\nparametric face model and gain explicit control over head pose and facial\nexpressions. To generate complete images of human heads, we propose an additive\ndecoder that adds plausible details such as hair. A novel training scheme\nenforces a pose-independent latent space and in consequence, allows learning a\none-to-many mapping between latent codes and pose-conditioned exterior regions.\nThe resulting method can generate geometrically consistent images of novel\nidentities under fine-grained control over head pose, face shape, and facial\nexpressions. This facilitates a broad range of downstream tasks, like sampling\nnovel identities, changing the head pose, expression transfer, and more. Code\nand models are available for research on https://mcbuehler.github.io/VariTex.",
          "link": "http://arxiv.org/abs/2104.05988",
          "publishedOn": "2021-08-19T01:35:02.499Z",
          "wordCount": 694,
          "title": "VariTex: Variational Neural Face Textures. (arXiv:2104.05988v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13721",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saparbayeva_B/0/1/0/all/0/1\">Bayan Saparbayeva</a>",
          "description": "Manifold-valued functional data analysis (FDA) recently becomes an active\narea of research motivated by the raising availability of trajectories or\nlongitudinal data observed on non-linear manifolds. The challenges of analyzing\nsuch data come from many aspects, including infinite dimensionality and\nnonlinearity, as well as time-domain or phase variability. In this paper, we\nstudy the amplitude part of manifold-valued functions on $\\mathbb{S}^2$, which\nis invariant to random time warping or re-parameterization. Utilizing the nice\ngeometry of $\\mathbb{S}^2$, we develop a set of efficient and accurate tools\nfor temporal alignment of functions, geodesic computing, and sample mean\ncalculation. At the heart of these tools, they rely on gradient descent\nalgorithms with carefully derived gradients. We show the advantages of these\nnewly developed tools over its competitors with extensive simulations and real\ndata and demonstrate the importance of considering the amplitude part of\nfunctions instead of mixing it with phase variability in manifold-valued FDA.",
          "link": "http://arxiv.org/abs/2107.13721",
          "publishedOn": "2021-08-19T01:35:02.460Z",
          "wordCount": 613,
          "title": "Amplitude Mean of Functional Data on $\\mathbb{S}^2$. (arXiv:2107.13721v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adikari_T/0/1/0/all/0/1\">Tharindu B. Adikari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Draper_S/0/1/0/all/0/1\">Stark C. Draper</a>",
          "description": "An increasing bottleneck in decentralized optimization is communication.\nBigger models and growing datasets mean that decentralization of computation is\nimportant and that the amount of information exchanged is quickly growing.\nWhile compression techniques have been introduced to cope with the latter, none\nhas considered leveraging the temporal correlations that exist in consecutive\nvector updates. An important example is distributed momentum-SGD where temporal\ncorrelation is enhanced by the low-pass-filtering effect of applying momentum.\nIn this paper we design and analyze compression methods that exploit temporal\ncorrelation in systems both with and without error-feedback. Experiments with\nthe ImageNet dataset demonstrate that our proposed methods offer significant\nreduction in the rate of communication at only a negligible increase in\ncomputation complexity. We further analyze the convergence of SGD when\ncompression is applied with error-feedback. In the literature, convergence\nguarantees are developed only for compressors that provide error-bounds\npoint-wise, i.e., for each input to the compressor. In contrast, many important\ncodes (e.g. rate-distortion codes) provide error-bounds only in expectation and\nthus provide a more general guarantee. In this paper we prove the convergence\nof SGD under an expected error assumption by establishing a bound for the\nminimum gradient norm.",
          "link": "http://arxiv.org/abs/2108.07827",
          "publishedOn": "2021-08-19T01:35:02.434Z",
          "wordCount": 700,
          "title": "Compressing gradients by exploiting temporal correlation in momentum-SGD. (arXiv:2108.07827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04379",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Liu_Z/0/1/0/all/0/1\">Ziming Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Qian_S/0/1/0/all/0/1\">Sitian Qian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yan_Y/0/1/0/all/0/1\">Yuxuan Yan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_T/0/1/0/all/0/1\">Tianyi Yang</a>",
          "description": "Principal component analysis (PCA) has achieved great success in unsupervised\nlearning by identifying covariance correlations among features. If the data\ncollection fails to capture the covariance information, PCA will not be able to\ndiscover meaningful modes. In particular, PCA will fail the spatial Gaussian\nProcess (GP) model in the undersampling regime, i.e. the averaged distance of\nneighboring anchor points (spatial features) is greater than the correlation\nlength of GP. Counterintuitively, by drawing the connection between PCA and\nSchr\\\"odinger equation, we can not only attack the undersampling challenge but\nalso compute in an efficient and decoupled way with the proposed algorithm\ncalled Schr\\\"odinger PCA. Our algorithm only requires variances of features and\nestimated correlation length as input, constructs the corresponding\nSchr\\\"odinger equation, and solves it to obtain the energy eigenstates, which\ncoincide with principal components. We will also establish the connection of\nour algorithm to the model reduction techniques in the partial differential\nequation (PDE) community, where the steady-state Schr\\\"odinger operator is\nidentified as a second-order approximation to the covariance function.\nNumerical experiments are implemented to testify the validity and efficiency of\nthe proposed algorithm, showing its potential for unsupervised learning tasks\non general graphs and manifolds.",
          "link": "http://arxiv.org/abs/2006.04379",
          "publishedOn": "2021-08-19T01:35:02.420Z",
          "wordCount": 672,
          "title": "Schr\\\"{o}dinger PCA: On the Duality between Principal Component Analysis and Schr\\\"{o}dinger Equation. (arXiv:2006.04379v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1\">Shin&#x27;ya Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1\">Sekitoshi Kanai</a>",
          "description": "Generative adversarial networks built from deep convolutional neural networks\n(GANs) lack the ability to exactly replicate the high-frequency components of\nnatural images. To alleviate this issue, we introduce two novel training\ntechniques called frequency dropping (F-Drop) and frequency matching (F-Match).\nThe key idea of F-Drop is to filter out unnecessary high-frequency components\nfrom the input images of the discriminators. This simple modification prevents\nthe discriminators from being confused by perturbations of the high-frequency\ncomponents. In addition, F-Drop makes the GANs focus on fitting in the\nlow-frequency domain, in which there are the dominant components of natural\nimages. F-Match minimizes the difference between real and fake images in the\nfrequency domain for generating more realistic images. F-Match is implemented\nas a regularization term in the objective functions of the generators; it\npenalizes the batch mean error in the frequency domain. F-Match helps the\ngenerators to fit in the high-frequency domain filtered out by F-Drop to the\nreal image. We experimentally demonstrate that the combination of F-Drop and\nF-Match improves the generative performance of GANs in both the frequency and\nspatial domain on multiple image benchmarks.",
          "link": "http://arxiv.org/abs/2106.02343",
          "publishedOn": "2021-08-19T01:35:02.331Z",
          "wordCount": 666,
          "title": "F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08143",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Sarwan Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tamkanat-E-Ali/0/1/0/all/0/1\">Tamkanat-E-Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Asad Khan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1\">Imdadullah Khan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1\">Murray Patterson</a>",
          "description": "SARS-CoV-2, like any other virus, continues to mutate as it spreads,\naccording to an evolutionary process. Unlike any other virus, the number of\ncurrently available sequences of SARS-CoV-2 in public databases such as GISAID\nis already several million. This amount of data has the potential to uncover\nthe evolutionary dynamics of a virus like never before. However, a million is\nalready several orders of magnitude beyond what can be processed by the\ntraditional methods designed to reconstruct a virus's evolutionary history,\nsuch as those that build a phylogenetic tree. Hence, new and scalable methods\nwill need to be devised in order to make use of the ever increasing number of\nviral sequences being collected.\n\nSince identifying variants is an important part of understanding the\nevolution of a virus, in this paper, we propose an approach based on clustering\nsequences to identify the current major SARS-CoV-2 variants. Using a $k$-mer\nbased feature vector generation and efficient feature selection methods, our\napproach is effective in identifying variants, as well as being efficient and\nscalable to millions of sequences. Such a clustering method allows us to show\nthe relative proportion of each variant over time, giving the rate of spread of\neach variant in different locations -- something which is important for vaccine\ndevelopment and distribution. We also compute the importance of each amino acid\nposition of the spike protein in identifying a given variant in terms of\ninformation gain. Positions of high variant-specific importance tend to agree\nwith those reported by the USA's Centers for Disease Control and Prevention\n(CDC), further demonstrating our approach.",
          "link": "http://arxiv.org/abs/2108.08143",
          "publishedOn": "2021-08-19T01:35:02.269Z",
          "wordCount": 743,
          "title": "Effective and scalable clustering of SARS-CoV-2 sequences. (arXiv:2108.08143v1 [q-bio.PE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fitzpatrick_B/0/1/0/all/0/1\">Benjamin Fitzpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinyu &quot;Sherwin&quot; Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straub_J/0/1/0/all/0/1\">Jeremy Straub</a>",
          "description": "Expert systems have been used to enable computers to make recommendations and\ndecisions. This paper presents the use of a machine learning trained expert\nsystem (MLES) for phishing site detection and fake news detection. Both topics\nshare a similar goal: to design a rule-fact network that allows a computer to\nmake explainable decisions like domain experts in each respective area. The\nphishing website detection study uses a MLES to detect potential phishing\nwebsites by analyzing site properties (like URL length and expiration time).\nThe fake news detection study uses a MLES rule-fact network to gauge news story\ntruthfulness based on factors such as emotion, the speaker's political\naffiliation status, and job. The two studies use different MLES network\nimplementations, which are presented and compared herein. The fake news study\nutilized a more linear design while the phishing project utilized a more\ncomplex connection structure. Both networks' inputs are based on commonly\navailable data sets.",
          "link": "http://arxiv.org/abs/2108.08264",
          "publishedOn": "2021-08-19T01:35:02.246Z",
          "wordCount": 604,
          "title": "Fake News and Phishing Detection Using a Machine Learning Trained Expert System. (arXiv:2108.08264v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.15343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hindy_H/0/1/0/all/0/1\">Hanan Hindy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tachtatzis_C/0/1/0/all/0/1\">Christos Tachtatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkinson_R/0/1/0/all/0/1\">Robert Atkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brosset_D/0/1/0/all/0/1\">David Brosset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bures_M/0/1/0/all/0/1\">Miroslav Bures</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonovic_I/0/1/0/all/0/1\">Ivan Andonovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michie_C/0/1/0/all/0/1\">Craig Michie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellekens_X/0/1/0/all/0/1\">Xavier Bellekens</a>",
          "description": "The use of supervised Machine Learning (ML) to enhance Intrusion Detection\nSystems has been the subject of significant research. Supervised ML is based\nupon learning by example, demanding significant volumes of representative\ninstances for effective training and the need to re-train the model for every\nunseen cyber-attack class. However, retraining the models in-situ renders the\nnetwork susceptible to attacks owing to the time-window required to acquire a\nsufficient volume of data. Although anomaly detection systems provide a\ncoarse-grained defence against unseen attacks, these approaches are\nsignificantly less accurate and suffer from high false-positive rates. Here, a\ncomplementary approach referred to as 'One-Shot Learning', whereby a limited\nnumber of examples of a new attack-class is used to identify a new attack-class\n(out of many) is detailed. The model grants a new cyber-attack classification\nwithout retraining. A Siamese Network is trained to differentiate between\nclasses based on pairs similarities, rather than features, allowing to identify\nnew and previously unseen attacks. The performance of a pre-trained model to\nclassify attack-classes based only on one example is evaluated using three\ndatasets. Results confirm the adaptability of the model in classifying unseen\nattacks and the trade-off between performance and the need for distinctive\nclass representation.",
          "link": "http://arxiv.org/abs/2006.15343",
          "publishedOn": "2021-08-19T01:35:02.152Z",
          "wordCount": 681,
          "title": "Leveraging Siamese Networks for One-Shot Intrusion Detection Model. (arXiv:2006.15343v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>",
          "description": "A multilayer perceptron (MLP) is typically made of multiple fully connected\nlayers with nonlinear activation functions. There have been several approaches\nto make them better (e.g. faster convergence, better convergence limit, etc.).\nBut the researches lack in more structured ways to test them. We test different\nMLP architectures by carrying out the experiments on the age and gender\ndatasets. We empirically show that by whitening inputs before every linear\nlayer and adding skip connections, our proposed MLP architecture can result in\nbetter performance. Since the whitening process includes dropouts, it can also\nbe used to approximate Bayesian inference. We have open sourced our code\nreleased models and docker images at https://github.com/tae898/age-gender/.",
          "link": "http://arxiv.org/abs/2108.08186",
          "publishedOn": "2021-08-19T01:35:02.040Z",
          "wordCount": 544,
          "title": "Generalizing MLPs With Dropouts, Batch Normalization, and Skip Connections. (arXiv:2108.08186v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bousquet_O/0/1/0/all/0/1\">Olivier Bousquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1\">Mark Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efremenko_K/0/1/0/all/0/1\">Klim Efremenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kol_G/0/1/0/all/0/1\">Gillat Kol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Shay Moran</a>",
          "description": "Hypothesis Selection is a fundamental distribution learning problem where\ngiven a comparator-class $Q=\\{q_1,\\ldots, q_n\\}$ of distributions, and a\nsampling access to an unknown target distribution $p$, the goal is to output a\ndistribution $q$ such that $\\mathsf{TV}(p,q)$ is close to $opt$, where $opt =\n\\min_i\\{\\mathsf{TV}(p,q_i)\\}$ and $\\mathsf{TV}(\\cdot, \\cdot)$ denotes the\ntotal-variation distance. Despite the fact that this problem has been studied\nsince the 19th century, its complexity in terms of basic resources, such as\nnumber of samples and approximation guarantees, remains unsettled (this is\ndiscussed, e.g., in the charming book by Devroye and Lugosi `00). This is in\nstark contrast with other (younger) learning settings, such as PAC learning,\nfor which these complexities are well understood.\n\nWe derive an optimal $2$-approximation learning strategy for the Hypothesis\nSelection problem, outputting $q$ such that $\\mathsf{TV}(p,q) \\leq2 \\cdot opt +\n\\eps$, with a (nearly) optimal sample complexity of~$\\tilde O(\\log\nn/\\epsilon^2)$. This is the first algorithm that simultaneously achieves the\nbest approximation factor and sample complexity: previously, Bousquet, Kane,\nand Moran (COLT `19) gave a learner achieving the optimal $2$-approximation,\nbut with an exponentially worse sample complexity of $\\tilde\nO(\\sqrt{n}/\\epsilon^{2.5})$, and Yatracos~(Annals of Statistics `85) gave a\nlearner with optimal sample complexity of $O(\\log n /\\epsilon^2)$ but with a\nsub-optimal approximation factor of $3$.",
          "link": "http://arxiv.org/abs/2108.07880",
          "publishedOn": "2021-08-19T01:35:02.033Z",
          "wordCount": 657,
          "title": "Statistically Near-Optimal Hypothesis Selection. (arXiv:2108.07880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10437",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sieun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eunho Lee</a>",
          "description": "Recently, there has been discussions on the ill-posed nature of\nsuper-resolution that multiple possible reconstructions exist for a given\nlow-resolution image. Using normalizing flows, SRflow[23] achieves\nstate-of-the-art perceptual quality by learning the distribution of the output\ninstead of a deterministic output to one estimate. In this paper, we adapt the\nconcepts of SRFlow to improve GAN-based super-resolution by properly\nimplementing the one-to-many property. We modify the generator to estimate a\ndistribution as a mapping from random noise. We improve the content loss that\nhampers the perceptual training objectives. We also propose additional training\ntechniques to further enhance the perceptual quality of generated images. Using\nour proposed methods, we were able to improve the performance of ESRGAN[1] in\nx4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual\nextreme SR by applying our methods to RFB-ESRGAN[21].",
          "link": "http://arxiv.org/abs/2106.10437",
          "publishedOn": "2021-08-19T01:35:01.930Z",
          "wordCount": 601,
          "title": "One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rongmei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "Due to the over-parameterization nature, neural networks are a powerful tool\nfor nonlinear function approximation. In order to achieve good generalization\non unseen data, a suitable inductive bias is of great importance for neural\nnetworks. One of the most straightforward ways is to regularize the neural\nnetwork with some additional objectives. L2 regularization serves as a standard\nregularization for neural networks. Despite its popularity, it essentially\nregularizes one dimension of the individual neuron, which is not strong enough\nto control the capacity of highly over-parameterized neural networks. Motivated\nby this, hyperspherical uniformity is proposed as a novel family of relational\nregularizations that impact the interaction among neurons. We consider several\ngeometrically distinct ways to achieve hyperspherical uniformity. The\neffectiveness of hyperspherical uniformity is justified by theoretical insights\nand empirical evaluations.",
          "link": "http://arxiv.org/abs/2103.01649",
          "publishedOn": "2021-08-19T01:35:01.871Z",
          "wordCount": 598,
          "title": "Learning with Hyperspherical Uniformity. (arXiv:2103.01649v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altman_R/0/1/0/all/0/1\">Russ Altman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Simran Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arx_S/0/1/0/all/0/1\">Sydney von Arx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1\">Michael S. Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brynjolfsson_E/0/1/0/all/0/1\">Erik Brynjolfsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1\">Rodrigo Castellon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri Chatterji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Annie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creel_K/0/1/0/all/0/1\">Kathleen Creel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jared Quincy Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1\">Dora Demszky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doumbouya_M/0/1/0/all/0/1\">Moussa Doumbouya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etchemendy_J/0/1/0/all/0/1\">John Etchemendy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1\">Trevor Gale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillespie_L/0/1/0/all/0/1\">Lauren Gillespie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1\">Karan Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossman_S/0/1/0/all/0/1\">Shelby Grossman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_N/0/1/0/all/0/1\">Neel Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1\">John Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jenny Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_K/0/1/0/all/0/1\">Kyle Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1\">Thomas Icard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalluri_P/0/1/0/all/0/1\">Pratyusha Kalluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1\">Siddharth Karamcheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keeling_G/0/1/0/all/0/1\">Geoff Keeling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khani_F/0/1/0/all/0/1\">Fereshte Khani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohd_P/0/1/0/all/0/1\">Pang Wei Kohd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krass_M/0/1/0/all/0/1\">Mark Krass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuditipudi_R/0/1/0/all/0/1\">Rohith Kuditipudi</a>, et al. (62 additional authors not shown)",
          "description": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT,\nDALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a\nwide range of downstream tasks. We call these models foundation models to\nunderscore their critically central yet incomplete character. This report\nprovides a thorough account of the opportunities and risks of foundation\nmodels, ranging from their capabilities (e.g., language, vision, robotics,\nreasoning, human interaction) and technical principles(e.g., model\narchitectures, training procedures, data, systems, security, evaluation,\ntheory) to their applications (e.g., law, healthcare, education) and societal\nimpact (e.g., inequity, misuse, economic and environmental impact, legal and\nethical considerations). Though foundation models are based on standard deep\nlearning and transfer learning, their scale results in new emergent\ncapabilities,and their effectiveness across so many tasks incentivizes\nhomogenization. Homogenization provides powerful leverage but demands caution,\nas the defects of the foundation model are inherited by all the adapted models\ndownstream. Despite the impending widespread deployment of foundation models,\nwe currently lack a clear understanding of how they work, when they fail, and\nwhat they are even capable of due to their emergent properties. To tackle these\nquestions, we believe much of the critical research on foundation models will\nrequire deep interdisciplinary collaboration commensurate with their\nfundamentally sociotechnical nature.",
          "link": "http://arxiv.org/abs/2108.07258",
          "publishedOn": "2021-08-19T01:35:01.830Z",
          "wordCount": 937,
          "title": "On the Opportunities and Risks of Foundation Models. (arXiv:2108.07258v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>",
          "description": "Model inversion (MI) attacks are aimed at reconstructing training data from\nmodel parameters. Such attacks have triggered increasing concerns about\nprivacy, especially given a growing number of online model repositories.\nHowever, existing MI attacks against deep neural networks (DNNs) have large\nroom for performance improvement. We present a novel inversion-specific GAN\nthat can better distill knowledge useful for performing attacks on private\nmodels from public data. In particular, we train the discriminator to\ndifferentiate not only the real and fake samples but the soft-labels provided\nby the target model. Moreover, unlike previous work that directly searches for\na single data point to represent a target class, we propose to model a private\ndata distribution for each target class. Our experiments show that the\ncombination of these techniques can significantly boost the success rate of the\nstate-of-the-art MI attacks by 150%, and generalize better to a variety of\ndatasets and models. Our code is available at\nhttps://github.com/SCccc21/Knowledge-Enriched-DMI.",
          "link": "http://arxiv.org/abs/2010.04092",
          "publishedOn": "2021-08-19T01:35:01.813Z",
          "wordCount": 619,
          "title": "Improved Techniques for Model Inversion Attacks. (arXiv:2010.04092v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>",
          "description": "Solving geometric tasks involving point clouds by using machine learning is a\nchallenging problem. Standard feed-forward neural networks combine linear or,\nif the bias parameter is included, affine layers and activation functions.\nTheir geometric modeling is limited, which motivated the prior work introducing\nthe multilayer hypersphere perceptron (MLHP). Its constituent part, i.e., the\nhypersphere neuron, is obtained by applying a conformal embedding of Euclidean\nspace. By virtue of Clifford algebra, it can be implemented as the Cartesian\ndot product of inputs and weights. If the embedding is applied in a manner\nconsistent with the dimensionality of the input space geometry, the decision\nsurfaces of the model units become combinations of hyperspheres and make the\ndecision-making process geometrically interpretable for humans. Our extension\nof the MLHP model, the multilayer geometric perceptron (MLGP), and its\nrespective layer units, i.e., geometric neurons, are consistent with the 3D\ngeometry and provide a geometric handle of the learned coefficients. In\nparticular, the geometric neuron activations are isometric in 3D, which is\nnecessary for rotation and translation equivariance. When classifying the 3D\nTetris shapes, we quantitatively show that our model requires no activation\nfunction in the hidden layers other than the embedding to outperform the\nvanilla multilayer perceptron. In the presence of noise in the data, our model\nis also superior to the MLHP.",
          "link": "http://arxiv.org/abs/2006.06507",
          "publishedOn": "2021-08-19T01:35:01.794Z",
          "wordCount": 690,
          "title": "Embed Me If You Can: A Geometric Perceptron. (arXiv:2006.06507v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Ze Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yimin Wen</a>",
          "description": "In many practical data mining scenarios, such as network intrusion detection,\nTwitter spam detection, and computer-aided diagnosis, a source domain that is\ndifferent from but related to a target domain is very common. In addition, a\nlarge amount of unlabeled data is available in both source and target domains,\nbut labeling each of them is difficult, expensive, time-consuming, and sometime\nunnecessary. Therefore, it is very important and worthwhile to fully explore\nthe labeled and unlabeled data in source and target domains to settle the task\nin target domain. In this paper, a new semi-supervised inductive transfer\nlearning framework, named \\emph{Co-Transfer} is proposed. Co-Transfer first\ngenerates three TrAdaBoost classifiers for transfer learning from the source\ndomain to the target domain, and meanwhile another three TrAdaBoost classifiers\nare generated for transfer learning from the target domain to the source\ndomain, using bootstraped samples from the original labeled data. In each round\nof co-transfer, each group of TrAdaBoost classifiers are refined using the\ncarefully labeled data. Finally, the group of TrAdaBoost classifiers learned to\ntransfer from the source domain to the target domain produce the final\nhypothesis. Experiments results illustrate Co-Transfer can effectively exploit\nand reuse the labeled and unlabeled data in source and target domains.",
          "link": "http://arxiv.org/abs/2108.07930",
          "publishedOn": "2021-08-19T01:35:01.778Z",
          "wordCount": 629,
          "title": "A new semi-supervised inductive transfer learning framework: Co-Transfer. (arXiv:2108.07930v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_B/0/1/0/all/0/1\">Binay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1\">Anirban Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matha_H/0/1/0/all/0/1\">Harika Matha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_K/0/1/0/all/0/1\">Kunal Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsai_L/0/1/0/all/0/1\">Lalitdutt Parsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1\">Vijay Agneeswaran</a>",
          "description": "Reducing the number of failures in a production system is one of the most\nchallenging problems in technology driven industries, such as, the online\nretail industry. To address this challenge, change management has emerged as a\npromising sub-field in operations that manages and reviews the changes to be\ndeployed in production in a systematic manner. However, it is practically\nimpossible to manually review a large number of changes on a daily basis and\nassess the risk associated with them. This warrants the development of an\nautomated system to assess the risk associated with a large number of changes.\nThere are a few commercial solutions available to address this problem but\nthose solutions lack the ability to incorporate domain knowledge and continuous\nfeedback from domain experts into the risk assessment process. As part of this\nwork, we aim to bridge the gap between model-driven risk assessment of change\nrequests and the assessment of domain experts by building a continuous feedback\nloop into the risk assessment process. Here we present our work to build an\nend-to-end machine learning system along with the discussion of some of\npractical challenges we faced related to extreme skewness in class\ndistribution, concept drift, estimation of the uncertainty associated with the\nmodel's prediction and the overall scalability of the system.",
          "link": "http://arxiv.org/abs/2108.07951",
          "publishedOn": "2021-08-19T01:35:01.766Z",
          "wordCount": 661,
          "title": "Look Before You Leap! Designing a Human-Centered AI System for Change Risk Assessment. (arXiv:2108.07951v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yangdi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1\">Yang Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbo He</a>",
          "description": "Recent studies on the memorization effects of deep neural networks on noisy\nlabels show that the networks first fit the correctly-labeled training samples\nbefore memorizing the mislabeled samples. Motivated by this early-learning\nphenomenon, we propose a novel method to prevent memorization of the mislabeled\nsamples. Unlike the existing approaches which use the model output to identify\nor ignore the mislabeled samples, we introduce an indicator branch to the\noriginal model and enable the model to produce a confidence value for each\nsample. The confidence values are incorporated in our loss function which is\nlearned to assign large confidence values to correctly-labeled samples and\nsmall confidence values to mislabeled samples. We also propose an auxiliary\nregularization term to further improve the robustness of the model. To improve\nthe performance, we gradually correct the noisy labels with a well-designed\ntarget estimation strategy. We provide the theoretical analysis and conduct the\nexperiments on synthetic and real-world datasets, demonstrating that our\napproach achieves comparable results to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.08212",
          "publishedOn": "2021-08-19T01:35:01.748Z",
          "wordCount": 602,
          "title": "Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Sen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weishen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>",
          "description": "In this paper, we focus on effective learning over a collaborative research\nnetwork involving multiple clients. Each client has its own sample population\nwhich may not be shared with other clients due to privacy concerns. The goal is\nto learn a model for each client, which behaves better than the one learned\nfrom its own data, through secure collaborations with other clients in the\nnetwork. Due to the discrepancies of the sample distributions across different\nclients, it is not necessarily that collaborating with everyone will lead to\nthe best local models. We propose a learning to collaborate framework, where\neach client can choose to collaborate with certain members in the network to\nachieve a \"collaboration equilibrium\", where smaller collaboration coalitions\nare formed within the network so that each client can obtain the model with the\nbest utility. We propose the concept of benefit graph which describes how each\nclient can benefit from collaborating with other clients and develop a Pareto\noptimization approach to obtain it. Finally the collaboration coalitions can be\nderived from it based on graph operations. Our framework provides a new way of\nsetting up collaborations in a research network. Experiments on both synthetic\nand real world data sets are provided to demonstrate the effectiveness of our\nmethod.",
          "link": "http://arxiv.org/abs/2108.07926",
          "publishedOn": "2021-08-19T01:35:01.742Z",
          "wordCount": 634,
          "title": "Learning to Collaborate. (arXiv:2108.07926v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunlu_O/0/1/0/all/0/1\">Onur G&#xfc;nl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_M/0/1/0/all/0/1\">Matthieu Bloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_R/0/1/0/all/0/1\">Rafael F. Schaefer</a>",
          "description": "We consider a distributed function computation problem in which parties\nobserving noisy versions of a remote source facilitate the computation of a\nfunction of their observations at a fusion center through public communication.\nThe distributed function computation is subject to constraints, including not\nonly reliability and storage but also privacy and secrecy. Specifically, 1) the\nremote source should remain private from an eavesdropper and the fusion center,\nmeasured in terms of the information leaked about the remote source; 2) the\nfunction computed should remain secret from the eavesdropper, measured in terms\nof the information leaked about the arguments of the function, to ensure\nsecrecy regardless of the exact function used. We derive the exact rate regions\nfor lossless and lossy single-function computation and illustrate the lossy\nsingle-function computation rate region for an information bottleneck example,\nin which the optimal auxiliary random variables are characterized for\nbinary-input symmetric-output channels. We extend the approach to lossless and\nlossy asynchronous multiple-function computations with joint secrecy and\nprivacy constraints, in which case inner and outer bounds for the rate regions\ndiffering only in the Markov chain conditions imposed are characterized.",
          "link": "http://arxiv.org/abs/2106.09485",
          "publishedOn": "2021-08-19T01:35:01.728Z",
          "wordCount": 680,
          "title": "Secure Multi-Function Computation with Private Remote Sources. (arXiv:2106.09485v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1\">Anastasiya Dykyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1\">Paulo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1\">Eric Jul</a>",
          "description": "Capillaries are the smallest vessels in the body responsible for the delivery\nof oxygen and nutrients to the surrounding cells. Various diseases have been\nshown to alter the density of nutritive capillaries and the flow velocity of\nerythrocytes. In previous studies, capillary density and flow velocity have\nbeen assessed manually by trained specialists. Manual analysis of a standard\n20-second long microvascular video takes on average 20 minutes and requires\nextensive training. Several studies have reported that manual analysis hinders\nthe application of microvascular microscopy in a clinical setting. In this\npaper, we present a fully automated state-of-the-art system, called\nCapillaryNet, that can quantify skin nutritive capillary density and red blood\ncell velocity from handheld microscopy videos. Moreover, CapillaryNet measures\nseveral novel microvascular parameters that researchers were previously unable\nto quantify, i.e. capillary hematocrit and Intra-capillary flow velocity\nheterogeneity. Our system has been used to analyze skin microcirculation videos\nfrom various patient groups (COVID-19, pancreatitis, and acute heart diseases).\nOur proposed system excels from existing capillary detection systems as it\ncombines the speed of traditional computer vision algorithms and the accuracy\nof convolutional neural networks.",
          "link": "http://arxiv.org/abs/2104.11574",
          "publishedOn": "2021-08-19T01:35:01.721Z",
          "wordCount": 740,
          "title": "CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaochao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Longfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanfeng Liu</a>",
          "description": "Cross-Domain Recommendation (CDR) and Cross-System Recommendation (CSR) have\nbeen proposed to improve the recommendation accuracy in a target dataset\n(domain/system) with the help of a source one with relatively richer\ninformation. However, most existing CDR and CSR approaches are single-target,\nnamely, there is a single target dataset, which can only help the target\ndataset and thus cannot benefit the source dataset. In this paper, we focus on\nthree new scenarios, i.e., Dual-Target CDR (DTCDR), Multi-Target CDR (MTCDR),\nand CDR+CSR, and aim to improve the recommendation accuracy in all datasets\nsimultaneously for all scenarios. To do this, we propose a unified framework,\ncalled GA (based on Graph embedding and Attention techniques), for all three\nscenarios. In GA, we first construct separate heterogeneous graphs to generate\nmore representative user and item embeddings. Then, we propose an element-wise\nattention mechanism to effectively combine the embeddings of common entities\n(users/items) learned from different datasets. Moreover, to avoid negative\ntransfer, we further propose a Personalized training strategy to minimize the\nembedding difference of common entities between a richer dataset and a sparser\ndataset, deriving three new models, i.e., GA-DTCDR-P, GA-MTCDR-P, and\nGA-CDR+CSR-P, for the three scenarios respectively. Extensive experiments\nconducted on four real-world datasets demonstrate that our proposed GA models\nsignificantly outperform the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.07976",
          "publishedOn": "2021-08-19T01:35:01.684Z",
          "wordCount": 676,
          "title": "A Unified Framework for Cross-Domain and Cross-System Recommendations. (arXiv:2108.07976v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jinsung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Soyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_M/0/1/0/all/0/1\">Minju Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seunghyeon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chiyoung Song</a>",
          "description": "Mobile digital billboards are an effective way to augment brand-awareness.\nAmong various such mobile billboards, taxicab rooftop devices are emerging in\nthe market as a brand new media. Motov is a leading company in South Korea in\nthe taxicab rooftop advertising market. In this work, we present a lightweight\nyet accurate deep learning-based method to predict taxicabs' next locations to\nbetter prepare for targeted advertising based on demographic information of\nlocations. Considering the fact that next POI recommendation datasets are\nfrequently sparse, we design our presented model based on neural ordinary\ndifferential equations (NODEs), which are known to be robust to\nsparse/incorrect input, with several enhancements. Our model, which we call\nLightMove, has a larger prediction accuracy, a smaller number of parameters,\nand/or a smaller training/inference time, when evaluating with various\ndatasets, in comparison with state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.04993",
          "publishedOn": "2021-08-19T01:35:01.647Z",
          "wordCount": 605,
          "title": "LightMove: A Lightweight Next-POI Recommendation for Taxicab Rooftop Advertising. (arXiv:2108.04993v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isaac_Medina_B/0/1/0/all/0/1\">Brian K. S. Isaac-Medina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poyser_M/0/1/0/all/0/1\">Matt Poyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Organisciak_D/0/1/0/all/0/1\">Daniel Organisciak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby P. Breckon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>",
          "description": "Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due\nto both negligent and malicious use. For this reason, the automated detection\nand tracking of UAV is a fundamental task in aerial security systems. Common\ntechnologies for UAV detection include visible-band and thermal infrared\nimaging, radio frequency and radar. Recent advances in deep neural networks\n(DNNs) for image-based object detection open the possibility to use visual\ninformation for this detection and tracking task. Furthermore, these detection\narchitectures can be implemented as backbones for visual tracking systems,\nthereby enabling persistent tracking of UAV incursions. To date, no\ncomprehensive performance benchmark exists that applies DNNs to visible-band\nimagery for UAV detection and tracking. To this end, three datasets with varied\nenvironmental conditions for UAV detection and tracking, comprising a total of\n241 videos (331,486 images), are assessed using four detection architectures\nand three tracking frameworks. The best performing detector architecture\nobtains an mAP of 98.6% and the best performing tracking framework obtains a\nMOTA of 96.3%. Cross-modality evaluation is carried out between visible and\ninfrared spectrums, achieving a maximal 82.8% mAP on visible images when\ntraining in the infrared modality. These results provide the first public\nmulti-approach benchmark for state-of-the-art deep learning-based methods and\ngive insight into which detection and tracking architectures are effective in\nthe UAV domain.",
          "link": "http://arxiv.org/abs/2103.13933",
          "publishedOn": "2021-08-19T01:35:01.628Z",
          "wordCount": 709,
          "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural Networks: A Performance Benchmark. (arXiv:2103.13933v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1\">Madhurananda Pahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klopper_M/0/1/0/all/0/1\">Marisa Klopper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warren_R/0/1/0/all/0/1\">Robin Warren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1\">Thomas Niesler</a>",
          "description": "We present an experimental investigation into the effectiveness of transfer\nlearning and bottleneck feature extraction in detecting COVID-19 from audio\nrecordings of cough, breath and speech.\n\nThis type of screening is non-contact, does not require specialist medical\nexpertise or laboratory facilities and can be deployed on inexpensive consumer\nhardware.\n\nWe use datasets that contain recordings of coughing, sneezing, speech and\nother noises, but do not contain COVID-19 labels, to pre-train three deep\nneural networks: a CNN, an LSTM and a Resnet50.\n\nThese pre-trained networks are subsequently either fine-tuned using smaller\ndatasets of coughing with COVID-19 labels in the process of transfer learning,\nor are used as bottleneck feature extractors.\n\nResults show that a Resnet50 classifier trained by this transfer learning\nprocess delivers optimal or near-optimal performance across all datasets\nachieving areas under the receiver operating characteristic (ROC AUC) of 0.98,\n0.94 and 0.92 respectively for all three sound classes (coughs, breaths and\nspeech).\n\nThis indicates that coughs carry the strongest COVID-19 signature, followed\nby breath and speech.\n\nOur results also show that applying transfer learning and extracting\nbottleneck features using the larger datasets without COVID-19 labels led not\nonly to improve performance, but also to minimise the standard deviation of the\nclassifier AUCs among the outer folds of the leave-$p$-out cross-validation,\nindicating better generalisation.\n\nWe conclude that deep transfer learning and bottleneck feature extraction can\nimprove COVID-19 cough, breath and speech audio classification, yielding\nautomatic classifiers with higher accuracy.",
          "link": "http://arxiv.org/abs/2104.02477",
          "publishedOn": "2021-08-19T01:35:01.620Z",
          "wordCount": 778,
          "title": "COVID-19 Detection in Cough, Breath and Speech using Deep Transfer Learning and Bottleneck Features. (arXiv:2104.02477v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1\">Maxime Kayser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1\">Cornelius Emde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Recently, there has been an increasing number of efforts to introduce models\ncapable of generating natural language explanations (NLEs) for their\npredictions on vision-language (VL) tasks. Such models are appealing, because\nthey can provide human-friendly and comprehensive explanations. However, there\nis a lack of comparison between existing methods, which is due to a lack of\nre-usable evaluation frameworks and a scarcity of datasets. In this work, we\nintroduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable\nvision-language tasks that establishes a unified evaluation framework and\nprovides the first comprehensive comparison of existing approaches that\ngenerate NLEs for VL tasks. It spans four models and three datasets and both\nautomatic metrics and human evaluation are used to assess model-generated\nexplanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs\n(over 430k instances). We also propose a new model that combines UNITER, which\nlearns joint embeddings of images and text, and GPT-2, a pre-trained language\nmodel that is well-suited for text generation. It surpasses the previous state\nof the art by a large margin across all datasets. Code and data are available\nhere: https://github.com/maximek3/e-ViL.",
          "link": "http://arxiv.org/abs/2105.03761",
          "publishedOn": "2021-08-19T01:35:01.609Z",
          "wordCount": 681,
          "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhongxi Zheng</a>",
          "description": "Learning from noisy labels is an important concern because of the lack of\naccurate ground-truth labels in plenty of real-world scenarios. In practice,\nvarious approaches for this concern first make some corrections corresponding\nto potentially noisy-labeled instances, and then update predictive model with\ninformation of the made corrections. However, in specific areas, such as\nmedical histopathology whole slide image analysis (MHWSIA), it is often\ndifficult or even impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. In this paper, we\nfocus on alleviating these two problems. For the problem 1), we present\none-step abductive multi-target learning (OSAMTL) that imposes a one-step\nlogical reasoning upon machine learning via a multi-target learning procedure\nto constrain the predictions of the learning model to be subject to our prior\nknowledge about the true target. For the problem 2), we propose a logical\nassessment formula (LAF) that evaluates the logical rationality of the outputs\nof an approach by estimating the consistencies between the predictions of the\nlearning model and the logical facts narrated from the results of the one-step\nlogical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori\n(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable\nthe machine learning model achieving logically more rational predictions, which\nis beyond various state-of-the-art approaches in handling complex noisy labels.",
          "link": "http://arxiv.org/abs/2011.14956",
          "publishedOn": "2021-08-19T01:35:01.593Z",
          "wordCount": 767,
          "title": "Handling Noisy Labels via One-Step Abductive Multi-Target Learning: An Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter J. Scheirer</a>",
          "description": "Machine learning has the potential to fuel further advances in data science,\nbut it is greatly hindered by an ad hoc design process, poor data hygiene, and\na lack of statistical rigor in model evaluation. Recently, these issues have\nbegun to attract more attention as they have caused public and embarrassing\nissues in research and development. Drawing from our experience as machine\nlearning researchers, we follow the machine learning process from algorithm\ndesign to data collection to model evaluation, drawing attention to common\npitfalls and providing practical recommendations for improvements. At each\nstep, case studies are introduced to highlight how these pitfalls occur in\npractice, and where things could be improved.",
          "link": "http://arxiv.org/abs/2011.02832",
          "publishedOn": "2021-08-19T01:35:01.572Z",
          "wordCount": 589,
          "title": "Pitfalls in Machine Learning Research: Reexamining the Development Cycle. (arXiv:2011.02832v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03408",
          "author": "<a href=\"http://arxiv.org/find/hep-ex/1/au:+Hong_T/0/1/0/all/0/1\">Tae Min Hong</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Carlson_B/0/1/0/all/0/1\">Benjamin Carlson</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Eubanks_B/0/1/0/all/0/1\">Brandon Eubanks</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Racz_S/0/1/0/all/0/1\">Stephen Racz</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Roche_S/0/1/0/all/0/1\">Stephen Roche</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Stelzer_J/0/1/0/all/0/1\">Joerg Stelzer</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Stumpp_D/0/1/0/all/0/1\">Daniel Stumpp</a>",
          "description": "We present a novel implementation of classification using the machine\nlearning / artificial intelligence method called boosted decision trees (BDT)\non field programmable gate arrays (FPGA). The firmware implementation of binary\nclassification requiring 100 training trees with a maximum depth of 4 using\nfour input variables gives a latency value of about 10 ns, independent of the\nclock speed from 100 to 320 MHz in our setup. The low timing values are\nachieved by restructuring the BDT layout and reconfiguring its parameters. The\nFPGA resource utilization is also kept low at a range from 0.01% to 0.2% in our\nsetup. A software package called fwXmachina achieves this implementation. Our\nintended user is an expert of custom electronics-based trigger systems in high\nenergy physics experiments or anyone that needs decisions at the lowest latency\nvalues for real-time event classification. Two problems from high energy\nphysics are considered, in the separation of electrons vs. photons and in the\nselection of vector boson fusion-produced Higgs bosons vs. the rejection of the\nmultijet processes.",
          "link": "http://arxiv.org/abs/2104.03408",
          "publishedOn": "2021-08-19T01:35:01.564Z",
          "wordCount": 677,
          "title": "Nanosecond machine learning event classification with boosted decision trees in FPGA for high energy physics. (arXiv:2104.03408v3 [hep-ex] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajihassani_O/0/1/0/all/0/1\">Omid Hajihassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardakanian_O/0/1/0/all/0/1\">Omid Ardakanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khazaei_H/0/1/0/all/0/1\">Hamzeh Khazaei</a>",
          "description": "The abundance of data collected by sensors in Internet of Things (IoT)\ndevices, and the success of deep neural networks in uncovering hidden patterns\nin time series data have led to mounting privacy concerns. This is because\nprivate and sensitive information can be potentially learned from sensor data\nby applications that have access to this data. In this paper, we aim to examine\nthe tradeoff between utility and privacy loss by learning low-dimensional\nrepresentations that are useful for data obfuscation. We propose deterministic\nand probabilistic transformations in the latent space of a variational\nautoencoder to synthesize time series data such that intrusive inferences are\nprevented while desired inferences can still be made with sufficient accuracy.\nIn the deterministic case, we use a linear transformation to move the\nrepresentation of input data in the latent space such that the reconstructed\ndata is likely to have the same public attribute but a different private\nattribute than the original input data. In the probabilistic case, we apply the\nlinear transformation to the latent representation of input data with some\nprobability. We compare our technique with autoencoder-based anonymization\ntechniques and additionally show that it can anonymize data in real time on\nresource-constrained edge devices.",
          "link": "http://arxiv.org/abs/2011.08315",
          "publishedOn": "2021-08-19T01:35:01.539Z",
          "wordCount": 669,
          "title": "Privacy-preserving Data Analysis through Representation Learning and Transformation. (arXiv:2011.08315v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yikun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Manan Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1\">Evangelos Theodorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1\">Naira Hovakimyan</a>",
          "description": "A reinforcement learning (RL) policy trained in a nominal environment could\nfail in a new/perturbed environment due to the existence of dynamic variations.\nExisting robust methods try to obtain a fixed policy for all envisioned dynamic\nvariation scenarios through robust or adversarial training. These methods could\nlead to conservative performance due to emphasis on the worst case, and often\ninvolve tedious modifications to the training environment. We propose an\napproach to robustifying a pre-trained non-robust RL policy with\n$\\mathcal{L}_1$ adaptive control. Leveraging the capability of an\n$\\mathcal{L}_1$ control law in the fast estimation of and active compensation\nfor dynamic variations, our approach can significantly improve the robustness\nof an RL policy trained in a standard (i.e., non-robust) way, either in a\nsimulator or in the real world. Numerical experiments are provided to validate\nthe efficacy of the proposed approach.",
          "link": "http://arxiv.org/abs/2106.02249",
          "publishedOn": "2021-08-19T01:35:01.527Z",
          "wordCount": 630,
          "title": "Robustifying Reinforcement Learning Policies with $\\mathcal{L}_1$ Adaptive Control. (arXiv:2106.02249v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bailey_A/0/1/0/all/0/1\">Andrew Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1\">Mark D. Plumbley</a>",
          "description": "Depression is a large-scale mental health problem and a challenging area for\nmachine learning researchers in detection of depression. Datasets such as\nDistress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) have been created\nto aid research in this area. However, on top of the challenges inherent in\naccurately detecting depression, biases in datasets may result in skewed\nclassification performance. In this paper we examine gender bias in the\nDAIC-WOZ dataset. We show that gender biases in DAIC-WOZ can lead to an\noverreporting of performance. By different concepts from Fair Machine Learning,\nsuch as data re-distribution, and using raw audio features, we can mitigate\nagainst the harmful effects of bias.",
          "link": "http://arxiv.org/abs/2010.15120",
          "publishedOn": "2021-08-19T01:35:01.507Z",
          "wordCount": 597,
          "title": "Gender Bias in Depression Detection Using Audio Features. (arXiv:2010.15120v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1\">Nikhila Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>",
          "description": "We present Worldsheet, a method for novel view synthesis using just a single\nRGB image as input. The main insight is that simply shrink-wrapping a planar\nmesh sheet onto the input image, consistent with the learned intermediate\ndepth, captures underlying geometry sufficient to generate photorealistic\nunseen views with large viewpoint changes. To operationalize this, we propose a\nnovel differentiable texture sampler that allows our wrapped mesh sheet to be\ntextured and rendered differentiably into an image from a target viewpoint. Our\napproach is category-agnostic, end-to-end trainable without using any 3D\nsupervision, and requires a single image at test time. We also explore a simple\nextension by stacking multiple layers of Worldsheets to better handle\nocclusions. Worldsheet consistently outperforms prior state-of-the-art methods\non single-image view synthesis across several datasets. Furthermore, this\nsimple idea captures novel views surprisingly well on a wide range of\nhigh-resolution in-the-wild images, converting them into navigable 3D pop-ups.\nVideo results and code are available at https://worldsheet.github.io.",
          "link": "http://arxiv.org/abs/2012.09854",
          "publishedOn": "2021-08-19T01:35:01.500Z",
          "wordCount": 668,
          "title": "Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image. (arXiv:2012.09854v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Sheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Ju Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jiang Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junshan Zhang</a>",
          "description": "In order to meet the requirements for performance, safety, and latency in\nmany IoT applications, intelligent decisions must be made right here right now\nat the network edge. However, the constrained resources and limited local data\namount pose significant challenges to the development of edge AI. To overcome\nthese challenges, we explore continual edge learning capable of leveraging the\nknowledge transfer from previous tasks. Aiming to achieve fast and continual\nedge learning, we propose a platform-aided federated meta-learning architecture\nwhere edge nodes collaboratively learn a meta-model, aided by the knowledge\ntransfer from prior tasks. The edge learning problem is cast as a regularized\noptimization problem, where the valuable knowledge learned from previous tasks\nis extracted as regularization. Then, we devise an ADMM based federated\nmeta-learning algorithm, namely ADMM-FedMeta, where ADMM offers a natural\nmechanism to decompose the original problem into many subproblems which can be\nsolved in parallel across edge nodes and the platform. Further, a variant of\ninexact-ADMM method is employed where the subproblems are `solved' via linear\napproximation as well as Hessian estimation to reduce the computational cost\nper round to $\\mathcal{O}(n)$. We provide a comprehensive analysis of\nADMM-FedMeta, in terms of the convergence properties, the rapid adaptation\nperformance, and the forgetting effect of prior knowledge transfer, for the\ngeneral non-convex case. Extensive experimental studies demonstrate the\neffectiveness and efficiency of ADMM-FedMeta, and showcase that it\nsubstantially outperforms the existing baselines.",
          "link": "http://arxiv.org/abs/2012.08677",
          "publishedOn": "2021-08-19T01:35:01.493Z",
          "wordCount": 722,
          "title": "Inexact-ADMM Based Federated Meta-Learning for Fast and Continual Edge Learning. (arXiv:2012.08677v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rempe_D/0/1/0/all/0/1\">Davis Rempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1\">Aaron Hertzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>",
          "description": "We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal\npose and shape. Though substantial progress has been made in estimating 3D\nhuman motion and shape from dynamic observations, recovering plausible pose\nsequences in the presence of noise and occlusions remains a challenge. For this\npurpose, we propose an expressive generative model in the form of a conditional\nvariational autoencoder, which learns a distribution of the change in pose at\neach step of a motion sequence. Furthermore, we introduce a flexible\noptimization-based approach that leverages HuMoR as a motion prior to robustly\nestimate plausible pose and shape from ambiguous observations. Through\nextensive evaluations, we demonstrate that our model generalizes to diverse\nmotions and body shapes after training on a large motion capture dataset, and\nenables motion reconstruction from multiple input modalities including 3D\nkeypoints and RGB(-D) videos.",
          "link": "http://arxiv.org/abs/2105.04668",
          "publishedOn": "2021-08-19T01:35:01.464Z",
          "wordCount": 623,
          "title": "HuMoR: 3D Human Motion Model for Robust Pose Estimation. (arXiv:2105.04668v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fiorini_S/0/1/0/all/0/1\">Stefano Fiorini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciavotta_M/0/1/0/all/0/1\">Michele Ciavotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurino_A/0/1/0/all/0/1\">Andrea Maurino</a>",
          "description": "In recent years, studying and predicting alternative mobility (e.g., sharing\nservices) patterns in urban environments has become increasingly important as\naccurate and timely information on current and future vehicle flows can\nsuccessfully increase the quality and availability of transportation services.\nThis need is aggravated during the current pandemic crisis, which pushes\npolicymakers and private citizens to seek social-distancing compliant urban\nmobility services, such as electric bikes and scooter sharing offerings.\nHowever, predicting the number of incoming and outgoing vehicles for different\ncity areas is challenging due to the nonlinear spatial and temporal\ndependencies typical of urban mobility patterns. In this work, we propose\nSTREED-Net, a novel deep learning network with a multi-attention (spatial and\ntemporal) mechanism that effectively captures and exploits complex spatial and\ntemporal patterns in mobility data. The results of a thorough experimental\nanalysis using real-life data are reported, indicating that the proposed model\nimproves the state-of-the-art for this task.",
          "link": "http://arxiv.org/abs/2103.00983",
          "publishedOn": "2021-08-19T01:35:01.444Z",
          "wordCount": 640,
          "title": "Listening to the city, attentively: A Spatio-Temporal Attention Boosted Autoencoder for the Short-Term Flow Prediction Problem. (arXiv:2103.00983v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Prarthana Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1\">Krzysztof Czarnecki</a>",
          "description": "Existing point-cloud based 3D object detectors use convolution-like operators\nto process information in a local neighbourhood with fixed-weight kernels and\naggregate global context hierarchically. However, non-local neural networks and\nself-attention for 2D vision have shown that explicitly modeling long-range\ninteractions can lead to more robust and competitive models. In this paper, we\npropose two variants of self-attention for contextual modeling in 3D object\ndetection by augmenting convolutional features with self-attention features. We\nfirst incorporate the pairwise self-attention mechanism into the current\nstate-of-the-art BEV, voxel and point-based detectors and show consistent\nimprovement over strong baseline models of up to 1.5 3D AP while simultaneously\nreducing their parameter footprint and computational cost by 15-80% and 30-50%,\nrespectively, on the KITTI validation set. We next propose a self-attention\nvariant that samples a subset of the most representative features by learning\ndeformations over randomly sampled locations. This not only allows us to scale\nexplicit global contextual modeling to larger point-clouds, but also leads to\nmore discriminative and informative feature descriptors. Our method can be\nflexibly applied to most state-of-the-art detectors with increased accuracy and\nparameter and compute efficiency. We show our proposed method improves 3D\nobject detection performance on KITTI, nuScenes and Waymo Open datasets. Code\nis available at https://github.com/AutoVision-cloud/SA-Det3D.",
          "link": "http://arxiv.org/abs/2101.02672",
          "publishedOn": "2021-08-19T01:35:01.437Z",
          "wordCount": 694,
          "title": "SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection. (arXiv:2101.02672v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Arghyadip Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "In the regret-based formulation of Multi-armed Bandit (MAB) problems, except\nin rare instances, much of the literature focuses on arms with i.i.d. rewards.\nIn this paper, we consider the problem of obtaining regret guarantees for MAB\nproblems in which the rewards of each arm form a Markov chain which may not\nbelong to a single parameter exponential family. To achieve logarithmic regret\nin such problems is not difficult: a variation of standard Kullback-Leibler\nUpper Confidence Bound (KL-UCB) does the job. However, the constants obtained\nfrom such an analysis are poor for the following reason: i.i.d. rewards are a\nspecial case of Markov rewards and it is difficult to design an algorithm that\nworks well independent of whether the underlying model is truly Markovian or\ni.i.d. To overcome this issue, we introduce a novel algorithm that identifies\nwhether the rewards from each arm are truly Markovian or i.i.d. using a total\nvariation distance-based test. Our algorithm then switches from using a\nstandard KL-UCB to a specialized version of KL-UCB when it determines that the\narm reward is Markovian, thus resulting in low regret for both i.i.d. and\nMarkovian settings.",
          "link": "http://arxiv.org/abs/2009.06606",
          "publishedOn": "2021-08-19T01:35:01.429Z",
          "wordCount": 663,
          "title": "Adaptive KL-UCB based Bandit Algorithms for Markovian and i.i.d. Settings. (arXiv:2009.06606v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Regenwetter_L/0/1/0/all/0/1\">Lyle Regenwetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curry_B/0/1/0/all/0/1\">Brent Curry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faez Ahmed</a>",
          "description": "In this paper, we present \"BIKED,\" a dataset comprised of 4500 individually\ndesigned bicycle models sourced from hundreds of designers. We expect BIKED to\nenable a variety of data-driven design applications for bicycles and support\nthe development of data-driven design methods. The dataset is comprised of a\nvariety of design information including assembly images, component images,\nnumerical design parameters, and class labels. In this paper, we first discuss\nthe processing of the dataset, then highlight some prominent research questions\nthat BIKED can help address. Of these questions, we further explore the\nfollowing in detail: 1) Are there prominent gaps in the current bicycle market\nand design space? We explore the design space using unsupervised dimensionality\nreduction methods. 2) How does one identify the class of a bicycle and what\nfactors play a key role in defining it? We address the bicycle classification\ntask by training a multitude of classifiers using different forms of design\ndata and identifying parameters of particular significance through\npermutation-based interpretability analysis. 3) How does one synthesize new\nbicycles using different representation methods? We consider numerous machine\nlearning methods to generate new bicycle models as well as interpolate between\nand extrapolate from existing models using Variational Autoencoders. The\ndataset and code are available at this http URL",
          "link": "http://arxiv.org/abs/2103.05844",
          "publishedOn": "2021-08-19T01:35:01.422Z",
          "wordCount": 686,
          "title": "BIKED: A Dataset for Computational Bicycle Design with Machine Learning Benchmarks. (arXiv:2103.05844v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shafran_A/0/1/0/all/0/1\">Avital Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peleg_S/0/1/0/all/0/1\">Shmuel Peleg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Membership inference attacks (MIA) try to detect if data samples were used to\ntrain a neural network model, e.g. to detect copyright abuses. We show that\nmodels with higher dimensional input and output are more vulnerable to MIA, and\naddress in more detail models for image translation and semantic segmentation,\nincluding medical image segmentation. We show that reconstruction-errors can\nlead to very effective MIA attacks as they are indicative of memorization.\nUnfortunately, reconstruction error alone is less effective at discriminating\nbetween non-predictable images used in training and easy to predict images that\nwere never seen before. To overcome this, we propose using a novel\npredictability error that can be computed for each sample, and its computation\ndoes not require a training set. Our membership error, obtained by subtracting\nthe predictability error from the reconstruction error, is shown to achieve\nhigh MIA accuracy on an extensive number of benchmarks.",
          "link": "http://arxiv.org/abs/2102.07762",
          "publishedOn": "2021-08-19T01:35:01.397Z",
          "wordCount": 617,
          "title": "Membership Inference Attacks are Easier on Difficult Problems. (arXiv:2102.07762v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Te-Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahjoubfar_A/0/1/0/all/0/1\">Ata Mahjoubfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prusinski_D/0/1/0/all/0/1\">Daniel Prusinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_L/0/1/0/all/0/1\">Luis Stevens</a>",
          "description": "Neuromorphic computing mimics the neural activity of the brain through\nemulating spiking neural networks. In numerous machine learning tasks,\nneuromorphic chips are expected to provide superior solutions in terms of cost\nand power efficiency. Here, we explore the application of Loihi, a neuromorphic\ncomputing chip developed by Intel, for the computer vision task of image\nretrieval. We evaluated the functionalities and the performance metrics that\nare critical in content-based visual search and recommender systems using\ndeep-learning embeddings. Our results show that the neuromorphic solution is\nabout 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and\n12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a\nlightweight convolutional neural network without batching while maintaining the\nsame level of matching accuracy. The study validates the potential of\nneuromorphic computing in low-power image retrieval, as a complementary\nparadigm to the existing von Neumann architectures.",
          "link": "http://arxiv.org/abs/2008.01380",
          "publishedOn": "2021-08-19T01:35:01.375Z",
          "wordCount": 604,
          "title": "Neuromorphic Computing for Content-based Image Retrieval. (arXiv:2008.01380v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelrahman_G/0/1/0/all/0/1\">Ghodai Abdelrahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>",
          "description": "Tracing a student's knowledge is vital for tailoring the learning experience.\nRecent knowledge tracing methods tend to respond to these challenges by\nmodelling knowledge state dynamics across learning concepts. However, they\nstill suffer from several inherent challenges including: modelling forgetting\nbehaviours and identifying relationships among latent concepts. To address\nthese challenges, in this paper, we propose a novel knowledge tracing model,\nnamely \\emph{Deep Graph Memory Network} (DGMN). In this model, we incorporate a\nforget gating mechanism into an attention memory structure in order to capture\nforgetting behaviours dynamically during the knowledge tracing process.\nParticularly, this forget gating mechanism is built upon attention forgetting\nfeatures over latent concepts considering their mutual dependencies. Further,\nthis model has the capability of learning relationships between latent concepts\nfrom a dynamic latent concept graph in light of a student's evolving knowledge\nstates. A comprehensive experimental evaluation has been conducted using four\nwell-established benchmark datasets. The results show that DGMN consistently\noutperforms the state-of-the-art KT models over all the datasets. The\neffectiveness of modelling forgetting behaviours and learning latent concept\ngraphs has also been analyzed in our experiments.",
          "link": "http://arxiv.org/abs/2108.08105",
          "publishedOn": "2021-08-19T01:35:01.365Z",
          "wordCount": 607,
          "title": "Deep Graph Memory Networks for Forgetting-Robust Knowledge Tracing. (arXiv:2108.08105v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09001",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hellkvist_M/0/1/0/all/0/1\">Martin Hellkvist</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ozcelikkale_A/0/1/0/all/0/1\">Ay&#xe7;a &#xd6;z&#xe7;elikkale</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ahlen_A/0/1/0/all/0/1\">Anders Ahl&#xe9;n</a>",
          "description": "Distributed learning provides an attractive framework for scaling the\nlearning task by sharing the computational load over multiple nodes in a\nnetwork. Here, we investigate the performance of distributed learning for\nlarge-scale linear regression where the model parameters, i.e., the unknowns,\nare distributed over the network. We adopt a statistical learning approach. In\ncontrast to works that focus on the performance on the training data, we focus\non the generalization error, i.e., the performance on unseen data. We provide\nhigh-probability bounds on the generalization error for both isotropic and\ncorrelated Gaussian data as well as sub-gaussian data. These results reveal the\ndependence of the generalization performance on the partitioning of the model\nover the network. In particular, our results show that the generalization error\nof the distributed solution can be substantially higher than that of the\ncentralized solution even when the error on the training data is at the same\nlevel for both the centralized and distributed approaches. Our numerical\nresults illustrate the performance with both real-world image data as well as\nsynthetic data.",
          "link": "http://arxiv.org/abs/2101.09001",
          "publishedOn": "2021-08-19T01:35:01.355Z",
          "wordCount": 640,
          "title": "Linear Regression with Distributed Learning: A Generalization Error Perspective. (arXiv:2101.09001v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zilong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1\">Robert Birke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1\">Aditya Kunar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Y. Chen</a>",
          "description": "Generative Adversarial Networks (GANs) are typically trained to synthesize\ndata, from images and more recently tabular data, under the assumption of\ndirectly accessible training data. Recently, federated learning (FL) is an\nemerging paradigm that features decentralized learning on client's local data\nwith a privacy-preserving capability. And, while learning GANs to synthesize\nimages on FL systems has just been demonstrated, it is unknown if GANs for\ntabular data can be learned from decentralized data sources. Moreover, it\nremains unclear which distributed architecture suits them best. Different from\nimage GANs, state-of-the-art tabular GANs require prior knowledge on the data\ndistribution of each (discrete and continuous) column to agree on a common\nencoding -- risking privacy guarantees. In this paper, we propose Fed-TGAN, the\nfirst Federated learning framework for Tabular GANs. To effectively learn a\ncomplex tabular GAN on non-identical participants, Fed-TGAN designs two novel\nfeatures: (i) a privacy-preserving multi-source feature encoding for model\ninitialization; and (ii) table similarity aware weighting strategies to\naggregate local models for countering data skew. We extensively evaluate the\nproposed Fed-TGAN against variants of decentralized learning architectures on\nfour widely used datasets. Results show that Fed-TGAN accelerates training time\nper epoch up to 200% compared to the alternative architectures, for both IID\nand Non-IID data. Overall, Fed-TGAN not only stabilizes the training loss, but\nalso achieves better similarity between generated and original data.",
          "link": "http://arxiv.org/abs/2108.07927",
          "publishedOn": "2021-08-19T01:35:01.345Z",
          "wordCount": 656,
          "title": "Fed-TGAN: Federated Learning Framework for Synthesizing Tabular Data. (arXiv:2108.07927v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkhatib_N/0/1/0/all/0/1\">Natasha Alkhatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghauch_H/0/1/0/all/0/1\">Hadi Ghauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danger_J/0/1/0/all/0/1\">Jean-Luc Danger</a>",
          "description": "Intrusion Detection Systems are widely used to detect cyberattacks,\nespecially on protocols vulnerable to hacking attacks such as SOME/IP. In this\npaper, we present a deep learning-based sequential model for offline intrusion\ndetection on SOME/IP application layer protocol. To assess our intrusion\ndetection system, we have generated and labeled a dataset with several classes\nrepresenting realistic intrusions, and a normal class - a significant\ncontribution due to the absence of such publicly available datasets.\nFurthermore, we also propose a simple recurrent neural network (RNN), as an\ninstance of deep learning-based sequential model, that we apply to our\ngenerated dataset. The numerical results show that RNN excel at predicting\nin-vehicle intrusions, with F1 Scores and AUC values of 0.99 for each type of\nintrusion.",
          "link": "http://arxiv.org/abs/2108.08262",
          "publishedOn": "2021-08-19T01:35:01.291Z",
          "wordCount": 565,
          "title": "SOME/IP Intrusion Detection using Deep Learning-based Sequential Models in Automotive Ethernet Networks. (arXiv:2108.08262v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cong_Z/0/1/0/all/0/1\">Zicun Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_P/0/1/0/all/0/1\">Pei Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feida Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.",
          "link": "http://arxiv.org/abs/2108.07915",
          "publishedOn": "2021-08-19T01:35:01.264Z",
          "wordCount": 624,
          "title": "Data Pricing in Machine Learning Pipelines. (arXiv:2108.07915v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08038",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+OLuing_M/0/1/0/all/0/1\">Mervyn O&#x27;Luing</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Prestwich_S/0/1/0/all/0/1\">Steven Prestwich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tarim_S/0/1/0/all/0/1\">S. Armagan Tarim</a>",
          "description": "In this paper we combine the k-means and/or k-means type algorithms with a\nhill climbing algorithm in stages to solve the joint stratification and sample\nallocation problem. This is a combinatorial optimisation problem in which we\nsearch for the optimal stratification from the set of all possible\nstratifications of basic strata. Each stratification being a solution the\nquality of which is measured by its cost. This problem is intractable for\nlarger sets. Furthermore evaluating the cost of each solution is expensive. A\nnumber of heuristic algorithms have already been developed to solve this\nproblem with the aim of finding acceptable solutions in reasonable computation\ntimes. However, the heuristics for these algorithms need to be trained in order\nto optimise performance in each instance. We compare the above multi-stage\ncombination of algorithms with three recent algorithms and report the solution\ncosts, evaluation times and training times. The multi-stage combinations\ngenerally compare well with the recent algorithms both in the case of atomic\nand continuous strata and provide the survey designer with a greater choice of\nalgorithms to choose from.",
          "link": "http://arxiv.org/abs/2108.08038",
          "publishedOn": "2021-08-19T01:35:01.256Z",
          "wordCount": 630,
          "title": "Combining K-means type algorithms with Hill Climbing for Joint Stratification and Sample Allocation Designs. (arXiv:2108.08038v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjinappa_C/0/1/0/all/0/1\">Chethan K. Anjinappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guvenc_I/0/1/0/all/0/1\">Ismail Guvenc</a>",
          "description": "The utilization of millimeter-wave (mmWave) bands in 5G networks poses new\nchallenges to network planning. Vulnerability to blockages at mmWave bands can\ncause coverage holes (CHs) in the radio environment, leading to radio link\nfailure when a user enters these CHs. Detection of the CHs carries critical\nimportance so that necessary remedies can be introduced to improve coverage. In\nthis letter, we propose a novel approach to identify the CHs in an unsupervised\nfashion using a state-of-the-art manifold learning technique: uniform manifold\napproximation and projection. The key idea is to preserve the\nlocal-connectedness structure inherent in the collected unlabelled channel\nsamples, such that the CHs from the service area are detectable. Our results on\nthe DeepMIMO dataset scenario demonstrate that the proposed method can learn\nthe structure within the data samples and provide visual holes in the\nlow-dimensional embedding while preserving the CH boundaries. Once the CH\nboundary is determined in the low-dimensional embedding, channel-based\nlocalization techniques can be applied to these samples to obtain the\ngeographical boundaries of the CHs.",
          "link": "http://arxiv.org/abs/2108.07854",
          "publishedOn": "2021-08-19T01:35:01.249Z",
          "wordCount": 621,
          "title": "Coverage Hole Detection for mmWave Networks: An Unsupervised Learning Approach. (arXiv:2108.07854v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10643",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1\">Jie Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gallego_B/0/1/0/all/0/1\">Blanca Gallego</a>",
          "description": "Causal inference in longitudinal observational health data often requires the\naccurate estimation of treatment effects on time-to-event outcomes in the\npresence of time-varying covariates. To tackle this sequential treatment effect\nestimation problem, we have developed a causal dynamic survival (CDS) model\nthat uses the potential outcomes framework with the recurrent sub-networks with\nrandom seed ensembles to estimate the difference in survival curves of its\nconfidence interval. Using simulated survival datasets, the CDS model has shown\ngood causal effect estimation performance across scenarios of sample dimension,\nevent rate, confounding and overlapping. However, increasing the sample size is\nnot effective to alleviate the adverse impact from high level of confounding.\nIn two large clinical cohort studies, our model identified the expected\nconditional average treatment effect and detected individual effect\nheterogeneity over time and patient subgroups. CDS provides individualised\nabsolute treatment effect estimations to improve clinical decisions.",
          "link": "http://arxiv.org/abs/2101.10643",
          "publishedOn": "2021-08-19T01:35:01.238Z",
          "wordCount": 661,
          "title": "Casual Inference using Deep Bayesian Dynamic Survival Model (CDS). (arXiv:2101.10643v8 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1\">Yannic Kilcher</a>",
          "description": "Benford's Law (BL) or the Significant Digit Law defines the probability\ndistribution of the first digit of numerical values in a data sample. This Law\nis observed in many naturally occurring datasets. It can be seen as a measure\nof naturalness of a given distribution and finds its application in areas like\nanomaly and fraud detection. In this work, we address the following question:\nIs the distribution of the Neural Network parameters related to the network's\ngeneralization capability? To that end, we first define a metric, MLH (Model\nEnthalpy),that measures the closeness of a set of numbers to Benford's Law and\nwe show empirically that it is a strong predictor of Validation Accuracy.\nSecond, we use MLH as an alternative to Validation Accuracy for Early Stopping,\nremoving the need for a Validation set. We provide experimental evidence that\neven if the optimal size of the validation set is known before-hand, the peak\ntest accuracy attained is lower than not using a validation set at all.\nFinally, we investigate the connection of BL to Free Energy Principle and First\nLaw of Thermodynamics, showing that MLH is a component of the internal energy\nof the learning system and optimization as an analogy to minimizing the total\nenergy to attain equilibrium.",
          "link": "http://arxiv.org/abs/2102.03313",
          "publishedOn": "2021-08-19T01:35:01.228Z",
          "wordCount": 652,
          "title": "Rethinking Neural Networks With Benford's Law. (arXiv:2102.03313v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Triet H. M. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hin_D/0/1/0/all/0/1\">David Hin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croft_R/0/1/0/all/0/1\">Roland Croft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">M. Ali Babar</a>",
          "description": "It is increasingly suggested to identify Software Vulnerabilities (SVs) in\ncode commits to give early warnings about potential security risks. However,\nthere is a lack of effort to assess vulnerability-contributing commits right\nafter they are detected to provide timely information about the exploitability,\nimpact and severity of SVs. Such information is important to plan and\nprioritize the mitigation for the identified SVs. We propose a novel Deep\nmulti-task learning model, DeepCVA, to automate seven Commit-level\nVulnerability Assessment tasks simultaneously based on Common Vulnerability\nScoring System (CVSS) metrics. We conduct large-scale experiments on 1,229\nvulnerability-contributing commits containing 542 different SVs in 246\nreal-world software projects to evaluate the effectiveness and efficiency of\nour model. We show that DeepCVA is the best-performing model with 38% to 59.8%\nhigher Matthews Correlation Coefficient than many supervised and unsupervised\nbaseline models. DeepCVA also requires 6.3 times less training and validation\ntime than seven cumulative assessment models, leading to significantly less\nmodel maintenance cost as well. Overall, DeepCVA presents the first effective\nand efficient solution to automatically assess SVs early in software systems.",
          "link": "http://arxiv.org/abs/2108.08041",
          "publishedOn": "2021-08-19T01:35:01.210Z",
          "wordCount": 638,
          "title": "DeepCVA: Automated Commit-level Vulnerability Assessment with Deep Multi-task Learning. (arXiv:2108.08041v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08180",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jinhua Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingxin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Sheng Chen</a>",
          "description": "In this paper, sparsification techniques aided online prediction algorithms\nin a reproducing kernel Hilbert space are studied for nonstationary time\nseries. The online prediction algorithms as usual consist of the selection of\nkernel structure parameters and the kernel weight vector updating. For\nstructure parameters, the kernel dictionary is selected by some sparsification\ntechniques with online selective modeling criteria, and moreover the kernel\ncovariance matrix is intermittently optimized in the light of the covariance\nmatrix adaptation evolution strategy (CMA-ES). Optimizing the real symmetric\ncovariance matrix can not only improve the kernel structure's flexibility by\nthe cross relatedness of the input variables, but also partly alleviate the\nprediction uncertainty caused by the kernel dictionary selection for\nnonstationary time series. In order to sufficiently capture the underlying\ndynamic characteristics in prediction-error time series, a generalized\noptimization strategy is designed to construct the kernel dictionary\nsequentially in multiple kernel connection modes. The generalized optimization\nstrategy provides a more self-contained way to construct the entire kernel\nconnections, which enhances the ability to adaptively track the changing\ndynamic characteristics. Numerical simulations have demonstrated that the\nproposed approach has superior prediction performance for nonstationary time\nseries.",
          "link": "http://arxiv.org/abs/2108.08180",
          "publishedOn": "2021-08-19T01:35:01.204Z",
          "wordCount": 649,
          "title": "Structure Parameter Optimized Kernel Based Online Prediction with a Generalized Optimization Strategy for Nonstationary Time Series. (arXiv:2108.08180v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08077",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Wadhawan_K/0/1/0/all/0/1\">Kahini Wadhawan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Han_B/0/1/0/all/0/1\">Barbara A. Han</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Fischhoff_I/0/1/0/all/0/1\">Ilya R. Fischhoff</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Castellanos_A/0/1/0/all/0/1\">Adrian C. Castellanos</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Varsani_A/0/1/0/all/0/1\">Arvind Varsani</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Varshney_K/0/1/0/all/0/1\">Kush R. Varshney</a>",
          "description": "Current methods for viral discovery target evolutionarily conserved proteins\nthat accurately identify virus families but remain unable to distinguish the\nzoonotic potential of newly discovered viruses. Here, we apply an\nattention-enhanced long-short-term memory (LSTM) deep neural net classifier to\na highly conserved viral protein target to predict zoonotic potential across\nbetacoronaviruses. The classifier performs with a 94% accuracy. Analysis and\nvisualization of attention at the sequence and structure-level features\nindicate possible association between important protein-protein interactions\ngoverning viral replication in zoonotic betacoronaviruses and zoonotic\ntransmission.",
          "link": "http://arxiv.org/abs/2108.08077",
          "publishedOn": "2021-08-19T01:35:01.193Z",
          "wordCount": 595,
          "title": "Towards Interpreting Zoonotic Potential of Betacoronavirus Sequences With Attention. (arXiv:2108.08077v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/1910.09499",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hu_J/0/1/0/all/0/1\">Jiaxin Hu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_C/0/1/0/all/0/1\">Chanwoo Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1\">Miaoyan Wang</a>",
          "description": "Higher-order tensors have received increased attention across science and\nengineering. While most tensor decomposition methods are developed for a single\ntensor observation, scientific studies often collect side information, in the\nform of node features and interactions thereof, together with the tensor data.\nSuch data problems are common in neuroimaging, network analysis, and\nspatial-temporal modeling. Identifying the relationship between a\nhigh-dimensional tensor and side information is important yet challenging.\nHere, we develop a tensor decomposition method that incorporates multiple\nfeature matrices as side information. Unlike unsupervised tensor decomposition,\nour supervised decomposition captures the effective dimension reduction of the\ndata tensor confined to feature space of interest. An efficient alternating\noptimization algorithm with provable spectral initialization is further\ndeveloped. Our proposal handles a broad range of data types, including\ncontinuous, count, and binary observations. We apply the method to diffusion\ntensor imaging data from human connectome project and multi-relational\npolitical network data. We identify the key global connectivity pattern and\npinpoint the local regions that are associated with available features. Our\nsimulation code, R-package tensorregress, and datasets used in the paper are\navailable at https://CRAN.R-project.org/package=tensorregress.",
          "link": "http://arxiv.org/abs/1910.09499",
          "publishedOn": "2021-08-19T01:35:01.183Z",
          "wordCount": 650,
          "title": "Supervised tensor decomposition with features on multiple modes. (arXiv:1910.09499v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rumberger_J/0/1/0/all/0/1\">Josef Lorenz Rumberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_P/0/1/0/all/0/1\">Peter Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohmen_M/0/1/0/all/0/1\">Melanie Dohmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guarino_V/0/1/0/all/0/1\">Vanessa Emanuela Guarino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokarian_A/0/1/0/all/0/1\">Ashkan Mokarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mais_L/0/1/0/all/0/1\">Lisa Mais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_J/0/1/0/all/0/1\">Jan Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1\">Dagmar Kainmueller</a>",
          "description": "Metric learning has received conflicting assessments concerning its\nsuitability for solving instance segmentation tasks. It has been dismissed as\ntheoretically flawed due to the shift equivariance of the employed CNNs and\ntheir respective inability to distinguish same-looking objects. Yet it has been\nshown to yield state of the art results for a variety of tasks, and practical\nissues have mainly been reported in the context of tile-and-stitch approaches,\nwhere discontinuities at tile boundaries have been observed. To date, neither\nof the reported issues have undergone thorough formal analysis. In our work, we\ncontribute a comprehensive formal analysis of the shift equivariance properties\nof encoder-decoder-style CNNs, which yields a clear picture of what can and\ncannot be achieved with metric learning in the face of same-looking objects. In\nparticular, we prove that a standard encoder-decoder network that takes\n$d$-dimensional images as input, with $l$ pooling layers and pooling factor\n$f$, has the capacity to distinguish at most $f^{dl}$ same-looking objects, and\nwe show that this upper limit can be reached. Furthermore, we show that to\navoid discontinuities in a tile-and-stitch approach, assuming standard batch\nsize 1, it is necessary to employ valid convolutions in combination with a\ntraining output window size strictly greater than $f^l$, while at test-time it\nis necessary to crop tiles to size $n\\cdot f^l$ before stitching, with $n\\geq\n1$. We complement these theoretical findings by discussing a number of\ninsightful special cases for which we show empirical results on synthetic data.",
          "link": "http://arxiv.org/abs/2101.05846",
          "publishedOn": "2021-08-19T01:35:01.167Z",
          "wordCount": 737,
          "title": "How Shift Equivariance Impacts Metric Learning for Instance Segmentation. (arXiv:2101.05846v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Predictor-based algorithms have achieved remarkable performance in the Neural\nArchitecture Search (NAS) tasks. However, these methods suffer from high\ncomputation costs, as training the performance predictor usually requires\ntraining and evaluating hundreds of architectures from scratch. Previous works\nalong this line mainly focus on reducing the number of architectures required\nto fit the predictor. In this work, we tackle this challenge from a different\nperspective - improve search efficiency by cutting down the computation budget\nof architecture training. We propose NOn-uniform Successive Halving (NOSH), a\nhierarchical scheduling algorithm that terminates the training of\nunderperforming architectures early to avoid wasting budget. To effectively\nleverage the non-uniform supervision signals produced by NOSH, we formulate\npredictor-based architecture search as learning to rank with pairwise\ncomparisons. The resulting method - RANK-NOSH, reduces the search budget by ~5x\nwhile achieving competitive or even better performance than previous\nstate-of-the-art predictor-based methods on various spaces and datasets.",
          "link": "http://arxiv.org/abs/2108.08019",
          "publishedOn": "2021-08-19T01:35:01.156Z",
          "wordCount": 609,
          "title": "RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving. (arXiv:2108.08019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07856",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fitzke_M/0/1/0/all/0/1\">Michael Fitzke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitley_D/0/1/0/all/0/1\">Derick Whitley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yau_W/0/1/0/all/0/1\">Wilson Yau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_F/0/1/0/all/0/1\">Fernando Rodrigues Jr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fadeev_V/0/1/0/all/0/1\">Vladimir Fadeev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bacmeister_C/0/1/0/all/0/1\">Cindy Bacmeister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carter_C/0/1/0/all/0/1\">Chris Carter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Edwards_J/0/1/0/all/0/1\">Jeffrey Edwards</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkinson_M/0/1/0/all/0/1\">Mark Parkinson</a>",
          "description": "Background: Histopathology is an important modality for the diagnosis and\nmanagement of many diseases in modern healthcare, and plays a critical role in\ncancer care. Pathology samples can be large and require multi-site sampling,\nleading to upwards of 20 slides for a single tumor, and the human-expert tasks\nof site selection and and quantitative assessment of mitotic figures are time\nconsuming and subjective. Automating these tasks in the setting of a digital\npathology service presents significant opportunities to improve workflow\nefficiency and augment human experts in practice. Approach: Multiple\nstate-of-the-art deep learning techniques for histopathology image\nclassification and mitotic figure detection were used in the development of\nOncoPetNet. Additionally, model-free approaches were used to increase speed and\naccuracy. The robust and scalable inference engine leverages Pytorch's\nperformance optimizations as well as specifically developed speed up techniques\nin inference. Results: The proposed system, demonstrated significantly improved\nmitotic counting performance for 41 cancer cases across 14 cancer types\ncompared to human expert baselines. In 21.9% of cases use of OncoPetNet led to\nchange in tumor grading compared to human expert evaluation. In deployment, an\neffective 0.27 min/slide inference was achieved in a high throughput veterinary\ndiagnostic pathology service across 2 centers processing 3,323 digital whole\nslide images daily. Conclusion: This work represents the first successful\nautomated deployment of deep learning systems for real-time expert-level\nperformance on important histopathology tasks at scale in a high volume\nclinical practice. The resulting impact outlines important considerations for\nmodel development, deployment, clinical decision making, and informs best\npractices for implementation of deep learning systems in digital histopathology\npractices.",
          "link": "http://arxiv.org/abs/2108.07856",
          "publishedOn": "2021-08-19T01:35:01.148Z",
          "wordCount": 758,
          "title": "OncoPetNet: A Deep Learning based AI system for mitotic figure counting on H&E stained whole slide digital images in a large veterinary diagnostic lab setting. (arXiv:2108.07856v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07872",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gupta_P/0/1/0/all/0/1\">Priya Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Han_C/0/1/0/all/0/1\">Cuize Han</a>",
          "description": "E-commerce websites use machine learned ranking models to serve shopping\nresults to customers. Typically, the websites log the customer search events,\nwhich include the query entered and the resulting engagement with the shopping\nresults, such as clicks and purchases. Each customer search event serves as\ninput training data for the models, and the individual customer engagement\nserves as a signal for customer preference. So a purchased shopping result, for\nexample, is perceived to be more important than one that is not. However, new\nor under-impressed products do not have enough customer engagement signals and\nend up at a disadvantage when being ranked alongside popular products. In this\npaper, we propose a novel method for data curation that aggregates all customer\nengagements within a day for the same query to use as input training data. This\naggregated customer engagement gives the models a complete picture of the\nrelative importance of shopping results. Training models on this aggregated\ndata leads to less reliance on behavioral features. This helps mitigate the\ncold start problem and boosted relevant new products to top search results. In\nthis paper, we present the offline and online analysis and results comparing\nthe individual and aggregated customer engagement models trained on e-commerce\ndata.",
          "link": "http://arxiv.org/abs/2108.07872",
          "publishedOn": "2021-08-19T01:35:01.137Z",
          "wordCount": 626,
          "title": "Aggregated Customer Engagement Model. (arXiv:2108.07872v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simic_I/0/1/0/all/0/1\">Ilija &#x160;imi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabol_V/0/1/0/all/0/1\">Vedran Sabol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veas_E/0/1/0/all/0/1\">Eduardo Veas</a>",
          "description": "Deep learning models have recently demonstrated remarkable results in a\nvariety of tasks, which is why they are being increasingly applied in\nhigh-stake domains, such as industry, medicine, and finance. Considering that\nautomatic predictions in these domains might have a substantial impact on the\nwell-being of a person, as well as considerable financial and legal\nconsequences to an individual or a company, all actions and decisions that\nresult from applying these models have to be accountable. Given that a\nsubstantial amount of data that is collected in high-stake domains are in the\nform of time series, in this paper we examine the current state of eXplainable\nAI (XAI) methods with a focus on approaches for opening up deep learning black\nboxes for the task of time series classification. Finally, our contribution\nalso aims at deriving promising directions for future work, to advance XAI for\ndeep learning on time series data.",
          "link": "http://arxiv.org/abs/2108.08009",
          "publishedOn": "2021-08-19T01:35:01.116Z",
          "wordCount": 609,
          "title": "XAI Methods for Neural Time Series Classification: A Brief Review. (arXiv:2108.08009v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parisi_L/0/1/0/all/0/1\">Luca Parisi</a>",
          "description": "This study presents the m-arcsinh Kernel ('m-ar-K') Fast Independent\nComponent Analysis ('FastICA') method ('m-ar-K-FastICA') for feature\nextraction. The kernel trick has enabled dimensionality reduction techniques to\ncapture a higher extent of non-linearity in the data; however, reproducible,\nopen-source kernels to aid with feature extraction are still limited and may\nnot be reliable when projecting features from entropic data. The m-ar-K\nfunction, freely available in Python and compatible with its open-source\nlibrary 'scikit-learn', is hereby coupled with FastICA to achieve more reliable\nfeature extraction in presence of a high extent of randomness in the data,\nreducing the need for pre-whitening. Different classification tasks were\nconsidered, as related to five (N = 5) open access datasets of various degrees\nof information entropy, available from scikit-learn and the University\nCalifornia Irvine (UCI) Machine Learning repository. Experimental results\ndemonstrate improvements in the classification performance brought by the\nproposed feature extraction. The novel m-ar-K-FastICA dimensionality reduction\napproach is compared to the 'FastICA' gold standard method, supporting its\nhigher reliability and computational efficiency, regardless of the underlying\nuncertainty in the data.",
          "link": "http://arxiv.org/abs/2108.07908",
          "publishedOn": "2021-08-19T01:35:01.071Z",
          "wordCount": 654,
          "title": "M-ar-K-Fast Independent Component Analysis. (arXiv:2108.07908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_L/0/1/0/all/0/1\">Lin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1\">Karan Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Ellie X. Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Sushant Prakash</a>",
          "description": "Deep retrieval models are widely used for learning entity representations and\nrecommendations. Federated learning provides a privacy-preserving way to train\nthese models without requiring centralization of user data. However, federated\ndeep retrieval models usually perform much worse than their centralized\ncounterparts due to non-IID (independent and identically distributed) training\ndata on clients, an intrinsic property of federated learning that limits\nnegatives available for training. We demonstrate that this issue is distinct\nfrom the commonly studied client drift problem. This work proposes\nbatch-insensitive losses as a way to alleviate the non-IID negatives issue for\nfederated movie recommendation. We explore a variety of techniques and identify\nthat batch-insensitive losses can effectively improve the performance of\nfederated deep retrieval models, increasing the relative recall of the\nfederated model by up to 93.15% and reducing the relative gap in recall between\nit and a centralized model from 27.22% - 43.14% to 0.53% - 2.42%. We\nopen-source our code framework to accelerate further research and applications\nof federated deep retrieval models.",
          "link": "http://arxiv.org/abs/2108.07931",
          "publishedOn": "2021-08-19T01:35:01.046Z",
          "wordCount": 604,
          "title": "Learning Federated Representations and Recommendations with Limited Negatives. (arXiv:2108.07931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cholakov_R/0/1/0/all/0/1\">Radostin Cholakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolev_T/0/1/0/all/0/1\">Todor Kolev</a>",
          "description": "Recurrent Neural Networks were, until recently, one of the best ways to\ncapture the timely dependencies in sequences. However, with the introduction of\nthe Transformer, it has been proven that an architecture with only\nattention-mechanisms without any RNN can improve on the results in various\nsequence processing tasks (e.g. NLP). Multiple studies since then have shown\nthat similar approaches can be applied for images, point clouds, video, audio\nor time series forecasting. Furthermore, solutions such as the Perceiver or the\nInformer have been introduced to expand on the applicability of the\nTransformer. Our main objective is testing and evaluating the effectiveness of\napplying Transformer-like models on time series data, tackling susceptibility\nto anomalies, context awareness and space complexity by fine-tuning the\nhyperparameters, preprocessing the data, applying dimensionality reduction or\nconvolutional encodings, etc. We are also looking at the problem of next-frame\nprediction and exploring ways to modify existing solutions in order to achieve\nhigher performance and learn generalized knowledge.",
          "link": "http://arxiv.org/abs/2108.08224",
          "publishedOn": "2021-08-19T01:35:01.037Z",
          "wordCount": 606,
          "title": "Transformers predicting the future. Applying attention in next-frame and time series forecasting. (arXiv:2108.08224v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salimibeni_M/0/1/0/all/0/1\">Mohammad Salimibeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiakhondi_Meybodi_Z/0/1/0/all/0/1\">Zohreh Hajiakhondi-Meybodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingxu Wang</a>",
          "description": "Recently, as a consequence of the COVID-19 pandemic, dependence on Contact\nTracing (CT) models has significantly increased to prevent spread of this\nhighly contagious virus and be prepared for the potential future ones. Since\nthe spreading probability of the novel coronavirus in indoor environments is\nmuch higher than that of the outdoors, there is an urgent and unmet quest to\ndevelop/design efficient, autonomous, trustworthy, and secure indoor CT\nsolutions. Despite such an urgency, this field is still in its infancy. The\npaper addresses this gap and proposes the Trustworthy Blockchain-enabled system\nfor Indoor Contact Tracing (TB-ICT) framework. The TB-ICT framework is proposed\nto protect privacy and integrity of the underlying CT data from unauthorized\naccess. More specifically, it is a fully distributed and innovative blockchain\nplatform exploiting the proposed dynamic Proof of Work (dPoW) credit-based\nconsensus algorithm coupled with Randomized Hash Window (W-Hash) and dynamic\nProof of Credit (dPoC) mechanisms to differentiate between honest and dishonest\nnodes. The TB-ICT not only provides a decentralization in data replication but\nalso quantifies the node's behavior based on its underlying credit-based\nmechanism. For achieving high localization performance, we capitalize on\navailability of Internet of Things (IoT) indoor localization infrastructures,\nand develop a data driven localization model based on Bluetooth Low Energy\n(BLE) sensor measurements. The simulation results show that the proposed TB-ICT\nprevents the COVID-19 from spreading by implementation of a highly accurate\ncontact tracing model while improving the users' privacy and security.",
          "link": "http://arxiv.org/abs/2108.08275",
          "publishedOn": "2021-08-19T01:35:01.031Z",
          "wordCount": 729,
          "title": "TB-ICT: A Trustworthy Blockchain-Enabled System for Indoor COVID-19 Contact Tracing. (arXiv:2108.08275v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1\">Adam Lerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">David Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noam Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to\nfind a set of policies that play optimally together. Policies learned through\nself-play may adopt arbitrary conventions and implicitly rely on multi-step\nreasoning based on fragile assumptions about other agents' actions and thus\nfail when paired with humans or independently trained agents at test time. To\naddress this, we present off-belief learning (OBL). At each timestep OBL agents\nfollow a policy $\\pi_1$ that is optimized assuming past actions were taken by a\ngiven, fixed policy ($\\pi_0$), but assuming that future actions will be taken\nby $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy\nthat does not rely on inferences based on other agents' behavior (an optimal\ngrounded policy). OBL can be iterated in a hierarchy, where the optimal policy\nfrom one level becomes the input to the next, thereby introducing multi-level\ncognitive reasoning in a controlled manner. Unlike existing approaches, which\nmay converge to any equilibrium policy, OBL converges to a unique policy,\nmaking it suitable for zero-shot coordination (ZSC). OBL can be scaled to\nhigh-dimensional settings with a fictitious transition mechanism and shows\nstrong performance in both a toy-setting and the benchmark human-AI & ZSC\nproblem Hanabi.",
          "link": "http://arxiv.org/abs/2103.04000",
          "publishedOn": "2021-08-19T01:35:01.014Z",
          "wordCount": 690,
          "title": "Off-Belief Learning. (arXiv:2103.04000v5 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdoli_M/0/1/0/all/0/1\">Mahsan Abdoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahrabi_J/0/1/0/all/0/1\">Jamal Shahrabi</a>",
          "description": "Credit scoring models, which are among the most potent risk management tools\nthat banks and financial institutes rely on, have been a popular subject for\nresearch in the past few decades. Accordingly, many approaches have been\ndeveloped to address the challenges in classifying loan applicants and improve\nand facilitate decision-making. The imbalanced nature of credit scoring\ndatasets, as well as the heterogeneous nature of features in credit scoring\ndatasets, pose difficulties in developing and implementing effective credit\nscoring models, targeting the generalization power of classification models on\nunseen data. In this paper, we propose the Bagging Supervised Autoencoder\nClassifier (BSAC) that mainly leverages the superior performance of the\nSupervised Autoencoder, which learns low-dimensional embeddings of the input\ndata exclusively with regards to the ultimate classification task of credit\nscoring, based on the principles of multi-task learning. BSAC also addresses\nthe data imbalance problem by employing a variant of the Bagging process based\non the undersampling of the majority class. The obtained results from our\nexperiments on the benchmark and real-life credit scoring datasets illustrate\nthe robustness and effectiveness of the Bagging Supervised Autoencoder\nClassifier in the classification of loan applicants that can be regarded as a\npositive development in credit scoring models.",
          "link": "http://arxiv.org/abs/2108.07800",
          "publishedOn": "2021-08-19T01:35:01.007Z",
          "wordCount": 633,
          "title": "Bagging Supervised Autoencoder Classifier for Credit Scoring. (arXiv:2108.07800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.06592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>",
          "description": "Previous literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features. However, this was done\nwithout considering the description length of the learned representations which\nis a direct and unbiased measure of the model complexity. In this paper, first\nwe introduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined $\\varphi$. We lastly present Sparsely Activated Networks\n(SANs) that consist of kernels with shared weights that, during encoding, are\nconvolved with the input and then passed through a sparse activation function.\nDuring decoding, the same weights are convolved with the sparse activation map\nand subsequently the partial reconstructions from each weight are summed to\nreconstruct the input. We compare SANs using the five previously defined\nactivation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,\nFMNIST) and show that models that are selected using $\\varphi$ have small\ndescription representation length and consist of interpretable kernels.",
          "link": "http://arxiv.org/abs/1907.06592",
          "publishedOn": "2021-08-19T01:35:00.997Z",
          "wordCount": 700,
          "title": "Sparsely Activated Networks. (arXiv:1907.06592v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raman_R/0/1/0/all/0/1\">Raja CSP Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_R/0/1/0/all/0/1\">Rohith Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perumal_D/0/1/0/all/0/1\">Divya Perumal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_V/0/1/0/all/0/1\">Vedha Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1\">Talha Abdur Rahman</a>",
          "description": "The Prevalence of Community support and engagement for different domains in\nthe tech industry has changed and evolved throughout the years. In this study,\nwe aim to understand, analyze and predict the trends of technology in a\nscientific manner, having collected data on numerous topics and their growth\nthroughout the years in the past decade. We apply machine learning models on\ncollected data, to understand, analyze and forecast the trends in the\nadvancement of different fields. We show that certain technical concepts such\nas python, machine learning, and Keras have an undisputed uptrend, finally\nconcluding that the Stackindex model forecasts with high accuracy and can be a\nviable tool for forecasting different tech domains.",
          "link": "http://arxiv.org/abs/2108.08120",
          "publishedOn": "2021-08-19T01:35:00.989Z",
          "wordCount": 549,
          "title": "Stack Index Prediction Using Time-Series Analysis. (arXiv:2108.08120v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girase_H/0/1/0/all/0/1\">Harshayu Girase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1\">Haiming Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1\">Srikanth Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanehara_A/0/1/0/all/0/1\">Akira Kanehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>",
          "description": "Recent advances in trajectory prediction have shown that explicit reasoning\nabout agents' intent is important to accurately forecast their motion. However,\nthe current research activities are not directly applicable to intelligent and\nsafety critical systems. This is mainly because very few public datasets are\navailable, and they only consider pedestrian-specific intents for a short\ntemporal horizon from a restricted egocentric view. To this end, we propose\nLOKI (LOng term and Key Intentions), a novel large-scale dataset that is\ndesigned to tackle joint trajectory and intention prediction for heterogeneous\ntraffic agents (pedestrians and vehicles) in an autonomous driving setting. The\nLOKI dataset is created to discover several factors that may affect intention,\nincluding i) agent's own will, ii) social interactions, iii) environmental\nconstraints, and iv) contextual information. We also propose a model that\njointly performs trajectory and intention prediction, showing that recurrently\nreasoning about intention can assist with trajectory prediction. We show our\nmethod outperforms state-of-the-art trajectory prediction methods by upto\n$27\\%$ and also provide a baseline for frame-wise intention estimation.",
          "link": "http://arxiv.org/abs/2108.08236",
          "publishedOn": "2021-08-19T01:35:00.982Z",
          "wordCount": 640,
          "title": "LOKI: Long Term and Key Intentions for Trajectory Prediction. (arXiv:2108.08236v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08214",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Silva_M/0/1/0/all/0/1\">Mariana Da Silva</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sudre_C/0/1/0/all/0/1\">Carole H. Sudre</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Garcia_K/0/1/0/all/0/1\">Kara Garcia</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bass_C/0/1/0/all/0/1\">Cher Bass</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>",
          "description": "Biomechanical modeling of tissue deformation can be used to simulate\ndifferent scenarios of longitudinal brain evolution. In this work,we present a\ndeep learning framework for hyper-elastic strain modelling of brain atrophy,\nduring healthy ageing and in Alzheimer's Disease. The framework directly models\nthe effects of age, disease status, and scan interval to regress regional\npatterns of atrophy, from which a strain-based model estimates deformations.\nThis model is trained and validated using 3D structural magnetic resonance\nimaging data from the ADNI cohort. Results show that the framework can estimate\nrealistic deformations, following the known course of Alzheimer's disease, that\nclearly differentiate between healthy and demented patterns of ageing. This\nsuggests the framework has potential to be incorporated into explainable models\nof disease, for the exploration of interventions and counterfactual examples.",
          "link": "http://arxiv.org/abs/2108.08214",
          "publishedOn": "2021-08-19T01:35:00.974Z",
          "wordCount": 601,
          "title": "Distinguishing Healthy Ageing from Dementia: a Biomechanical Simulation of Brain Atrophy using Deep Networks. (arXiv:2108.08214v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08230",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Nauck_C/0/1/0/all/0/1\">Christian Nauck</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lindner_M/0/1/0/all/0/1\">Michael Lindner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schurholt_K/0/1/0/all/0/1\">Konstantin Sch&#xfc;rholt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1\">Haoming Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schultz_P/0/1/0/all/0/1\">Paul Schultz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kurths_J/0/1/0/all/0/1\">J&#xfc;rgen Kurths</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Isenhardt_I/0/1/0/all/0/1\">Ingrid Isenhardt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hellmann_F/0/1/0/all/0/1\">Frank Hellmann</a>",
          "description": "The prediction of dynamical stability of power grids becomes more important\nand challenging with increasing shares of renewable energy sources due to their\ndecentralized structure, reduced inertia and volatility. We investigate the\nfeasibility of applying graph neural networks (GNN) to predict dynamic\nstability of synchronisation in complex power grids using the single-node basin\nstability (SNBS) as a measure. To do so, we generate two synthetic datasets for\ngrids with 20 and 100 nodes respectively and estimate SNBS using Monte-Carlo\nsampling. Those datasets are used to train and evaluate the performance of\neight different GNN-models. All models use the full graph without\nsimplifications as input and predict SNBS in a nodal-regression-setup. We show\nthat SNBS can be predicted in general and the performance significantly changes\nusing different GNN-models. Furthermore, we observe interesting transfer\ncapabilities of our approach: GNN-models trained on smaller grids can directly\nbe applied on larger grids without the need of retraining.",
          "link": "http://arxiv.org/abs/2108.08230",
          "publishedOn": "2021-08-19T01:35:00.968Z",
          "wordCount": 620,
          "title": "Predicting Dynamic Stability of Power Grids using Graph Neural Networks. (arXiv:2108.08230v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1\">Pengfei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>",
          "description": "Differentiable architecture search (DARTS) marks a milestone in Neural\nArchitecture Search (NAS), boasting simplicity and small search costs. However,\nDARTS still suffers from frequent performance collapse, which happens when some\noperations, such as skip connections, zeroes and poolings, dominate the\narchitecture. In this paper, we are the first to point out that the phenomenon\nis attributed to bi-level optimization. We propose Single-DARTS which merely\nuses single-level optimization, updating network weights and architecture\nparameters simultaneously with the same data batch. Even single-level\noptimization has been previously attempted, no literature provides a systematic\nexplanation on this essential point. Replacing the bi-level optimization,\nSingle-DARTS obviously alleviates performance collapse as well as enhances the\nstability of architecture search. Experiment results show that Single-DARTS\nachieves state-of-the-art performance on mainstream search spaces. For\ninstance, on NAS-Benchmark-201, the searched architectures are nearly optimal\nones. We also validate that the single-level optimization framework is much\nmore stable than the bi-level one. We hope that this simple yet effective\nmethod will give some insights on differential architecture search. The code is\navailable at https://github.com/PencilAndBike/Single-DARTS.git.",
          "link": "http://arxiv.org/abs/2108.08128",
          "publishedOn": "2021-08-19T01:35:00.961Z",
          "wordCount": 616,
          "title": "Single-DARTS: Towards Stable Architecture Search. (arXiv:2108.08128v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattapalli_S/0/1/0/all/0/1\">Sai Mattapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athavale_R/0/1/0/all/0/1\">Rishi Athavale</a>",
          "description": "Due to morphological similarity at the microscopic level, making an accurate\nand time-sensitive distinction between blood cells affected by Acute\nLymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage\nof machine learning architectures. However, three of the most common models,\nVGG, ResNet, and Inception, each come with their own set of flaws with room for\nimprovement which demands the need for a superior model. ALLNet, the proposed\nhybrid convolutional neural network architecture, consists of a combination of\nthe VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019\n(available here) contains 10,691 images of white blood cells which were used to\ntrain and test the models. 7,272 of the images in the dataset are of cells with\nALL and 3,419 of them are of healthy cells. Of the images, 60% were used to\ntrain the model, 20% were used for the cross-validation set, and 20% were used\nfor the test set. ALLNet outperformed the VGG, ResNet, and the Inception models\nacross the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%,\na specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803\nin the cross-validation set. In the test set, ALLNet achieved an accuracy of\n92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of\n0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the\nclinical workspace can better treat the thousands of people suffering from ALL\nacross the world, many of whom are children.",
          "link": "http://arxiv.org/abs/2108.08195",
          "publishedOn": "2021-08-19T01:35:00.911Z",
          "wordCount": 711,
          "title": "ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells. (arXiv:2108.08195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.",
          "link": "http://arxiv.org/abs/2108.08217",
          "publishedOn": "2021-08-19T01:35:00.905Z",
          "wordCount": 678,
          "title": "X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Bin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwen Yu</a>",
          "description": "The prediction of express delivery sequence, i.e., modeling and estimating\nthe volumes of daily incoming and outgoing parcels for delivery, is critical\nfor online business, logistics, and positive customer experience, and\nspecifically for resource allocation optimization and promotional activity\narrangement. A precise estimate of consumer delivery requests has to involve\nsequential factors such as shopping behaviors, weather conditions, events,\nbusiness campaigns, and their couplings. Besides, conventional sequence\nprediction assumes a stable sequence evolution, failing to address complex\nnonlinear sequences and various feature effects in the above multi-source data.\nAlthough deep networks and attention mechanisms demonstrate the potential of\ncomplex sequence modeling, extant networks ignore the heterogeneous and\ncoupling situation between features and sequences, resulting in weak prediction\naccuracy. To address these issues, we propose DeepExpress - a deep-learning\nbased express delivery sequence prediction model, which extends the classic\nseq2seq framework to learning complex coupling between sequence and features.\nDeepExpress leverages an express delivery seq2seq learning, a\ncarefully-designed heterogeneous feature representation, and a novel joint\ntraining attention mechanism to adaptively map heterogeneous data, and capture\nsequence-feature coupling for precise estimation. Experimental results on\nreal-world data demonstrate that the proposed method outperforms both shallow\nand deep baseline models.",
          "link": "http://arxiv.org/abs/2108.08170",
          "publishedOn": "2021-08-19T01:35:00.897Z",
          "wordCount": 638,
          "title": "DeepExpress: Heterogeneous and Coupled Sequence Modeling for Express Delivery Prediction. (arXiv:2108.08170v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1\">Md Monowar Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Noman Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>",
          "description": "In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.",
          "link": "http://arxiv.org/abs/2108.07971",
          "publishedOn": "2021-08-19T01:35:00.890Z",
          "wordCount": 531,
          "title": "De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_P/0/1/0/all/0/1\">Peyman Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_I/0/1/0/all/0/1\">Ingrid Chieh Yu</a>",
          "description": "Counterfactual explanation methods interpret the outputs of a machine\nlearning model in the form of \"what-if scenarios\" without compromising the\nfidelity-interpretability trade-off. They explain how to obtain a desired\nprediction from the model by recommending small changes to the input features,\naka recourse. We believe an actionable recourse should be created based on\nsound counterfactual explanations originating from the distribution of the\nground-truth data and linked to the domain knowledge. Moreover, it needs to\npreserve the coherency between changed/unchanged features while satisfying\nuser/domain-specified constraints. This paper introduces CARE, a modular\nexplanation framework that addresses the model- and user-level desiderata in a\nconsecutive and structured manner. We tackle the existing requirements by\nproposing novel and efficient solutions that are formulated in a\nmulti-objective optimization framework. The designed framework enables\nincluding arbitrary requirements and generating counterfactual explanations and\nactionable recourse by choice. As a model-agnostic approach, CARE generates\nmultiple, diverse explanations for any black-box model in tabular\nclassification and regression settings. Several experiments on standard data\nsets and black-box models demonstrate the effectiveness of our modular\nframework and its superior performance compared to the baselines.",
          "link": "http://arxiv.org/abs/2108.08197",
          "publishedOn": "2021-08-19T01:35:00.883Z",
          "wordCount": 615,
          "title": "CARE: Coherent Actionable Recourse based on Sound Counterfactual Explanations. (arXiv:2108.08197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>",
          "description": "For all the ways convolutional neural nets have revolutionized computer\nvision in recent years, one important aspect has received surprisingly little\nattention: the effect of image size on the accuracy of tasks being trained for.\nTypically, to be efficient, the input images are resized to a relatively small\nspatial resolution (e.g. 224x224), and both training and inference are carried\nout at this resolution. The actual mechanism for this re-scaling has been an\nafterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic\nare commonly used in most machine learning software frameworks. But do these\nresizers limit the on task performance of the trained networks? The answer is\nyes. Indeed, we show that the typical linear resizer can be replaced with\nlearned resizers that can substantially improve performance. Importantly, while\nthe classical resizers typically result in better perceptual quality of the\ndownscaled images, our proposed learned resizers do not necessarily give better\nvisual quality, but instead improve task performance. Our learned image resizer\nis jointly trained with a baseline vision model. This learned CNN-based resizer\ncreates machine friendly visual manipulations that lead to a consistent\nimprovement of the end task metric over the baseline model. Specifically, here\nwe focus on the classification task with the ImageNet dataset, and experiment\nwith four different models to learn resizers adapted to each model. Moreover,\nwe show that the proposed resizer can also be useful for fine-tuning the\nclassification baselines for other vision tasks. To this end, we experiment\nwith three different baselines to develop image quality assessment (IQA) models\non the AVA dataset.",
          "link": "http://arxiv.org/abs/2103.09950",
          "publishedOn": "2021-08-19T01:35:00.863Z",
          "wordCount": 727,
          "title": "Learning to Resize Images for Computer Vision Tasks. (arXiv:2103.09950v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "Generating context-aware language that embodies diverse emotions is an\nimportant step towards building empathetic NLP systems. In this paper, we\npropose a formulation of modulated layer normalization -- a technique inspired\nby computer vision -- that allows us to use large-scale language models for\nemotional response generation. In automatic and human evaluation on the\nMojiTalk dataset, our proposed modulated layer normalization method outperforms\nprior baseline methods while maintaining diversity, fluency, and coherence. Our\nmethod also obtains competitive performance even when using only 10% of the\navailable training data.",
          "link": "http://arxiv.org/abs/2108.07886",
          "publishedOn": "2021-08-19T01:35:00.855Z",
          "wordCount": 525,
          "title": "Modulating Language Models with Emotions. (arXiv:2108.07886v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.00400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>",
          "description": "Recent literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features, but without considering\nthe description length of the representations. In this thesis, first we\nintroduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined metric $\\varphi$. We lastly present Sparsely Activated\nNetworks (SANs) that consist of kernels with shared weights that, during\nencoding, are convolved with the input and then passed through a sparse\nactivation function. During decoding, the same weights are convolved with the\nsparse activation map and subsequently the partial reconstructions from each\nweight are summed to reconstruct the input. We compare SANs using the five\npreviously defined activation functions on a variety of datasets (Physionet,\nUCI-epilepsy, MNIST, FMNIST) and show that models that are selected using\n$\\varphi$ have small description representation length and consist of\ninterpretable kernels.",
          "link": "http://arxiv.org/abs/1911.00400",
          "publishedOn": "2021-08-19T01:35:00.846Z",
          "wordCount": 660,
          "title": "Sparsely Activated Networks: A new method for decomposing and compressing data. (arXiv:1911.00400v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Seong Jin Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myoung Ho Kim</a>",
          "description": "Link prediction is one of the key problems for graph-structured data. With\nthe advancement of graph neural networks, graph autoencoders (GAEs) and\nvariational graph autoencoders (VGAEs) have been proposed to learn graph\nembeddings in an unsupervised way. It has been shown that these methods are\neffective for link prediction tasks. However, they do not work well in link\npredictions when a node whose degree is zero (i.g., isolated node) is involved.\nWe have found that GAEs/VGAEs make embeddings of isolated nodes close to zero\nregardless of their content features. In this paper, we propose a novel\nVariational Graph Normalized AutoEncoder (VGNAE) that utilize\n$L_2$-normalization to derive better embeddings for isolated nodes. We show\nthat our VGNAEs outperform the existing state-of-the-art models for link\nprediction tasks. The code is available at\nhttps://github.com/SeongJinAhn/VGNAE.",
          "link": "http://arxiv.org/abs/2108.08046",
          "publishedOn": "2021-08-19T01:35:00.836Z",
          "wordCount": 557,
          "title": "Variational Graph Normalized Auto-Encoders. (arXiv:2108.08046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08253",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Kavakli_K/0/1/0/all/0/1\">Koray Kavakl&#x131;</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Urey_H/0/1/0/all/0/1\">Hakan Urey</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Aksit_K/0/1/0/all/0/1\">Kaan Ak&#x15f;it</a>",
          "description": "Computer-Generated Holography (CGH) algorithms often fall short in matching\nsimulations with results from a physical holographic display. Our work\naddresses this mismatch by learning the holographic light transport in\nholographic displays. Using a camera and a holographic display, we capture the\nimage reconstructions of optimized holograms that rely on ideal simulations to\ngenerate a dataset. Inspired by the ideal simulations, we learn a\ncomplex-valued convolution kernel that can propagate given holograms to\ncaptured photographs in our dataset. Our method can dramatically improve\nsimulation accuracy and image quality in holographic displays while paving the\nway for physically informed learning approaches.",
          "link": "http://arxiv.org/abs/2108.08253",
          "publishedOn": "2021-08-19T01:35:00.823Z",
          "wordCount": 523,
          "title": "Learned holographic light transport. (arXiv:2108.08253v1 [physics.optics])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghdaei_A/0/1/0/all/0/1\">Ali Aghdaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhuo Feng</a>",
          "description": "Hypergraphs allow modeling problems with multi-way high-order relationships.\nHowever, the computational cost of most existing hypergraph-based algorithms\ncan be heavily dependent upon the input hypergraph sizes. To address the\never-increasing computational challenges, graph coarsening can be potentially\napplied for preprocessing a given hypergraph by aggressively aggregating its\nvertices (nodes). However, state-of-the-art hypergraph partitioning\n(clustering) methods that incorporate heuristic graph coarsening techniques are\nnot optimized for preserving the structural (global) properties of hypergraphs.\nIn this work, we propose an efficient spectral hypergraph coarsening scheme\n(HyperSF) for well preserving the original spectral (structural) properties of\nhypergraphs. Our approach leverages a recent strongly-local max-flow-based\nclustering algorithm for detecting the sets of hypergraph vertices that\nminimize ratio cut. To further improve the algorithm efficiency, we propose a\ndivide-and-conquer scheme by leveraging spectral clustering of the bipartite\ngraphs corresponding to the original hypergraphs. Our experimental results for\na variety of hypergraphs extracted from real-world VLSI design benchmarks show\nthat the proposed hypergraph coarsening algorithm can significantly improve the\nmulti-way conductance of hypergraph clustering as well as runtime efficiency\nwhen compared with existing state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2108.07901",
          "publishedOn": "2021-08-19T01:35:00.797Z",
          "wordCount": 607,
          "title": "HyperSF: Spectral Hypergraph Coarsening via Flow-based Local Clustering. (arXiv:2108.07901v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chia-Hsiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yu-Shin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yuan-Yao Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hung-Yueh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kai-Chiang Wu</a>",
          "description": "Neural architecture search can discover neural networks with good\nperformance, and One-Shot approaches are prevalent. One-Shot approaches\ntypically require a supernet with weight sharing and predictors that predict\nthe performance of architecture. However, the previous methods take much time\nto generate performance predictors thus are inefficient. To this end, we\npropose FOX-NAS that consists of fast and explainable predictors based on\nsimulated annealing and multivariate regression. Our method is\nquantization-friendly and can be efficiently deployed to the edge. The\nexperiments on different hardware show that FOX-NAS models outperform some\nother popular neural network architectures. For example, FOX-NAS matches\nMobileNetV2 and EfficientNet-Lite0 accuracy with 240% and 40% less latency on\nthe edge CPU. FOX-NAS is the 3rd place winner of the 2020 Low-Power Computer\nVision Challenge (LPCVC), DSP classification track. See all evaluation results\nat https://lpcv.ai/competitions/2020. Search code and pre-trained models are\nreleased at https://github.com/great8nctu/FOX-NAS.",
          "link": "http://arxiv.org/abs/2108.08189",
          "publishedOn": "2021-08-19T01:35:00.789Z",
          "wordCount": 598,
          "title": "FOX-NAS: Fast, On-device and Explainable Neural Architecture Search. (arXiv:2108.08189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diers_J/0/1/0/all/0/1\">Jan Diers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pigorsch_C/0/1/0/all/0/1\">Christian Pigorsch</a>",
          "description": "Out-of-distribution detection (OOD) deals with anomalous input to neural\nnetworks. In the past, specialized methods have been proposed to reject\npredictions on anomalous input. We use outlier detection algorithms to detect\nanomalous input as reliable as specialized methods from the field of OOD. No\nneural network adaptation is required; detection is based on the model's\nsoftmax score. Our approach works unsupervised with an Isolation Forest or with\nsupervised classifiers such as a Gradient Boosting machine.",
          "link": "http://arxiv.org/abs/2108.08218",
          "publishedOn": "2021-08-19T01:35:00.781Z",
          "wordCount": 496,
          "title": "Out-of-Distribution Detection using Outlier Detection Methods. (arXiv:2108.08218v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.03021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1\">Martin Rinard</a>",
          "description": "Researchers have developed neural network verification algorithms motivated\nby the need to characterize the robustness of deep neural networks. The\nverifiers aspire to answer whether a neural network guarantees certain\nproperties with respect to all inputs in a space. However, many verifiers\ninaccurately model floating point arithmetic but do not thoroughly discuss the\nconsequences.\n\nWe show that the negligence of floating point error leads to unsound\nverification that can be systematically exploited in practice. For a pretrained\nneural network, we present a method that efficiently searches inputs as\nwitnesses for the incorrectness of robustness claims made by a complete\nverifier. We also present a method to construct neural network architectures\nand weights that induce wrong results of an incomplete verifier. Our results\nhighlight that, to achieve practically reliable verification of neural\nnetworks, any verification system must accurately (or conservatively) model the\neffects of any floating point computations in the network inference or\nverification system.",
          "link": "http://arxiv.org/abs/2003.03021",
          "publishedOn": "2021-08-19T01:35:00.761Z",
          "wordCount": 631,
          "title": "Exploiting Verified Neural Networks via Floating Point Numerical Error. (arXiv:2003.03021v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08052",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rozen_N/0/1/0/all/0/1\">Noam Rozen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nickel_M/0/1/0/all/0/1\">Maximilian Nickel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lipman_Y/0/1/0/all/0/1\">Yaron Lipman</a>",
          "description": "We are interested in learning generative models for complex geometries\ndescribed via manifolds, such as spheres, tori, and other implicit surfaces.\nCurrent extensions of existing (Euclidean) generative models are restricted to\nspecific geometries and typically suffer from high computational costs. We\nintroduce Moser Flow (MF), a new class of generative models within the family\nof continuous normalizing flows (CNF). MF also produces a CNF via a solution to\nthe change-of-variable formula, however differently from other CNF methods, its\nmodel (learned) density is parameterized as the source (prior) density minus\nthe divergence of a neural network (NN). The divergence is a local, linear\ndifferential operator, easy to approximate and calculate on manifolds.\nTherefore, unlike other CNFs, MF does not require invoking or backpropagating\nthrough an ODE solver during training. Furthermore, representing the model\ndensity explicitly as the divergence of a NN rather than as a solution of an\nODE facilitates learning high fidelity densities. Theoretically, we prove that\nMF constitutes a universal density approximator under suitable assumptions.\nEmpirically, we demonstrate for the first time the use of flow models for\nsampling from general curved surfaces and achieve significant improvements in\ndensity estimation, sample quality, and training complexity over existing CNFs\non challenging synthetic geometries and real-world benchmarks from the earth\nand climate sciences.",
          "link": "http://arxiv.org/abs/2108.08052",
          "publishedOn": "2021-08-19T01:35:00.707Z",
          "wordCount": 648,
          "title": "Moser Flow: Divergence-based Generative Modeling on Manifolds. (arXiv:2108.08052v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1\">Maja J Matari&#x107;</a>",
          "description": "Automated systems that detect the social behavior of deception can enhance\nhuman well-being across medical, social work, and legal domains. Labeled\ndatasets to train supervised deception detection models can rarely be collected\nfor real-world, high-stakes contexts. To address this challenge, we propose the\nfirst unsupervised approach for detecting real-world, high-stakes deception in\nvideos without requiring labels. This paper presents our novel approach for\naffect-aware unsupervised Deep Belief Networks (DBN) to learn discriminative\nrepresentations of deceptive and truthful behavior. Drawing on psychology\ntheories that link affect and deception, we experimented with unimodal and\nmultimodal DBN-based approaches trained on facial valence, facial arousal,\naudio, and visual features. In addition to using facial affect as a feature on\nwhich DBN models are trained, we also introduce a DBN training procedure that\nuses facial affect as an aligner of audio-visual representations. We conducted\nclassification experiments with unsupervised Gaussian Mixture Model clustering\nto evaluate our approaches. Our best unsupervised approach (trained on facial\nvalence and visual features) achieved an AUC of 80%, outperforming human\nability and performing comparably to fully-supervised models. Our results\nmotivate future work on unsupervised, affect-aware computational approaches for\ndetecting deception and other social behaviors in the wild.",
          "link": "http://arxiv.org/abs/2108.07897",
          "publishedOn": "2021-08-19T01:35:00.683Z",
          "wordCount": 637,
          "title": "Affect-Aware Deep Belief Network Representations for Multimodal Unsupervised Deception Detection. (arXiv:2108.07897v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tianhong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hengyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arulkumaran_K/0/1/0/all/0/1\">Kai Arulkumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Guangyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharath_A/0/1/0/all/0/1\">Anil Anthony Bharath</a>",
          "description": "Hindsight experience replay (HER) is a goal relabelling technique typically\nused with off-policy deep reinforcement learning algorithms to solve\ngoal-oriented tasks; it is well suited to robotic manipulation tasks that\ndeliver only sparse rewards. In HER, both trajectories and transitions are\nsampled uniformly for training. However, not all of the agent's experiences\ncontribute equally to training, and so naive uniform sampling may lead to\ninefficient learning. In this paper, we propose diversity-based trajectory and\ngoal selection with HER (DTGSH). Firstly, trajectories are sampled according to\nthe diversity of the goal states as modelled by determinantal point processes\n(DPPs). Secondly, transitions with diverse goal states are selected from the\ntrajectories by using k-DPPs. We evaluate DTGSH on five challenging robotic\nmanipulation tasks in simulated robot environments, where we show that our\nmethod can learn more quickly and reach higher performance than other\nstate-of-the-art approaches on all tasks.",
          "link": "http://arxiv.org/abs/2108.07887",
          "publishedOn": "2021-08-19T01:35:00.640Z",
          "wordCount": 584,
          "title": "Diversity-based Trajectory and Goal Selection with Hindsight Experience Replay. (arXiv:2108.07887v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06412",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vretos_N/0/1/0/all/0/1\">Nicholas Vretos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>",
          "description": "Recently there has been an explosion in the use of Deep Learning (DL) methods\nfor medical image segmentation. However the field's reliability is hindered by\nthe lack of a common base of reference for accuracy/performance evaluation and\nthe fact that previous research uses different datasets for evaluation. In this\npaper, an extensive comparison of DL models for lung and COVID-19 lesion\nsegmentation in Computerized Tomography (CT) scans is presented, which can also\nbe used as a benchmark for testing medical image segmentation models. Four DL\narchitectures (Unet, Linknet, FPN, PSPNet) are combined with 25 randomly\ninitialized and pretrained encoders (variations of VGG, DenseNet, ResNet,\nResNext, DPN, MobileNet, Xception, Inception-v4, EfficientNet), to construct\n200 tested models. Three experimental setups are conducted for lung\nsegmentation, lesion segmentation and lesion segmentation using the original\nlung masks. A public COVID-19 dataset with 100 CT scan images (80 for train, 20\nfor validation) is used for training/validation and a different public dataset\nconsisting of 829 images from 9 CT scan volumes for testing. Multiple findings\nare provided including the best architecture-encoder models for each experiment\nas well as mean Dice results for each experiment, architecture and encoder\nindependently. Finally, the upper bounds improvements when using lung masks as\na preprocessing step or when using pretrained models are quantified. The source\ncode and 600 pretrained models for the three experiments are provided, suitable\nfor fine-tuning in experimental setups without GPU capabilities.",
          "link": "http://arxiv.org/abs/2009.06412",
          "publishedOn": "2021-08-19T01:35:00.632Z",
          "wordCount": 767,
          "title": "Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans. (arXiv:2009.06412v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.08689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1\">Thodoris Lykouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "We initiate the study of multi-stage episodic reinforcement learning under\nadversarial corruptions in both the rewards and the transition probabilities of\nthe underlying system extending recent results for the special case of\nstochastic bandits. We provide a framework which modifies the aggressive\nexploration enjoyed by existing reinforcement learning approaches based on\n\"optimism in the face of uncertainty\", by complementing them with principles\nfrom \"action elimination\". Importantly, our framework circumvents the major\nchallenges posed by naively applying action elimination in the RL setting, as\nformalized by a lower bound we demonstrate. Our framework yields efficient\nalgorithms which (a) attain near-optimal regret in the absence of corruptions\nand (b) adapt to unknown levels corruption, enjoying regret guarantees which\ndegrade gracefully in the total corruption encountered. To showcase the\ngenerality of our approach, we derive results for both tabular settings (where\nstates and actions are finite) as well as linear-function-approximation\nsettings (where the dynamics and rewards admit a linear underlying\nrepresentation). Notably, our work provides the first sublinear regret\nguarantee which accommodates any deviation from purely i.i.d. transitions in\nthe bandit-feedback model for episodic reinforcement learning.",
          "link": "http://arxiv.org/abs/1911.08689",
          "publishedOn": "2021-08-19T01:35:00.625Z",
          "wordCount": 669,
          "title": "Corruption-robust exploration in episodic reinforcement learning. (arXiv:1911.08689v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1\">Martin Rinard</a>",
          "description": "Deep neural networks are an attractive tool for compressing the control\npolicy lookup tables in systems such as the Airborne Collision Avoidance System\n(ACAS). It is vital to ensure the safety of such neural controllers via\nverification techniques. The problem of analyzing ACAS Xu networks has\nmotivated many successful neural network verifiers. These verifiers typically\nanalyze the internal computation of neural networks to decide whether a\nproperty regarding the input/output holds. The intrinsic complexity of neural\nnetwork computation renders such verifiers slow to run and vulnerable to\nfloating-point error.\n\nThis paper revisits the original problem of verifying ACAS Xu networks. The\nnetworks take low-dimensional sensory inputs with training data provided by a\nprecomputed lookup table. We propose to prepend an input quantization layer to\nthe network. Quantization allows efficient verification via input state\nenumeration, whose complexity is bounded by the size of the quantization space.\nQuantization is equivalent to nearest-neighbor interpolation at run time, which\nhas been shown to provide acceptable accuracy for ACAS in simulation. Moreover,\nour technique can deliver exact verification results immune to floating-point\nerror if we directly enumerate the network outputs on the target inference\nimplementation or on an accurate simulation of the target implementation.",
          "link": "http://arxiv.org/abs/2108.07961",
          "publishedOn": "2021-08-19T01:35:00.574Z",
          "wordCount": 634,
          "title": "Verifying Low-dimensional Input Neural Networks via Input Quantization. (arXiv:2108.07961v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1905.11589",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rawlinson_D/0/1/0/all/0/1\">David Rawlinson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ahmed_A/0/1/0/all/0/1\">Abdelrahman Ahmed</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kowadlo_G/0/1/0/all/0/1\">Gideon Kowadlo</a>",
          "description": "We present a recurrent neural network memory that uses sparse coding to\ncreate a combinatoric encoding of sequential inputs. Using several examples, we\nshow that the network can associate distant causes and effects in a discrete\nstochastic process, predict partially-observable higher-order sequences, and\nenable a DQN agent to navigate a maze by giving it memory. The network uses\nonly biologically-plausible, local and immediate credit assignment. Memory\nrequirements are typically one order of magnitude less than existing LSTM, GRU\nand autoregressive feed-forward sequence learning models. The most significant\nlimitation of the memory is generalization to unseen input sequences. We\nexplore this limitation by measuring next-word prediction perplexity on the\nPenn Treebank dataset.",
          "link": "http://arxiv.org/abs/1905.11589",
          "publishedOn": "2021-08-19T01:35:00.470Z",
          "wordCount": 590,
          "title": "Learning distant cause and effect using only local and immediate credit assignment. (arXiv:1905.11589v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Pengcheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tei_K/0/1/0/all/0/1\">Kenji Tei</a>",
          "description": "Users interacting with a UI-embedded machine or system are typically obliged\nto perform their actions in a pre-determined order, to successfully achieve\ncertain functional goals. However, such obligations are often not followed\nstrictly by users, which may lead to the violation to security properties,\nespecially in security-critical systems. In order to improve the security with\nthe awareness of unexpected user behaviors, a system can be redesigned to a\nmore robust one by changing the order of actions in its specification.\nMeanwhile, we anticipate that the functionalities would remain consistent\nfollowing the modifications. In this paper, we propose an efficient algorithm\nto automatically produce specification revisions tackling with attack scenarios\ncaused by the weakened user obligations. By our algorithm, all the revisions\nmaintain the integrity of the functionalities as the original specification,\nwhich are generated using a novel recomposition approach. Then, the qualified\nrevisions that can satisfy the security requirements would be efficiently\nspotted by a hybrid approach combining model checking and machine learning\ntechniques. We evaluate our algorithm by comparing its performance with a\nstate-of-the-art approach regarding their coverage and searching speed of the\ndesirable revisions.",
          "link": "http://arxiv.org/abs/2108.08282",
          "publishedOn": "2021-08-19T01:35:00.454Z",
          "wordCount": 635,
          "title": "OACAL: Finding Module-Consistent Solutions to Weaken User Obligations. (arXiv:2108.08282v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weier Wan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kubendran_R/0/1/0/all/0/1\">Rajkumar Kubendran</a> (2 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_C/0/1/0/all/0/1\">Clemens Schaefer</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Eryilmaz_S/0/1/0/all/0/1\">S. Burc Eryilmaz</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dabin Wu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Deiss_S/0/1/0/all/0/1\">Stephen Deiss</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Raina_P/0/1/0/all/0/1\">Priyanka Raina</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">He Qian</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bin Gao</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Siddharth Joshi</a> (4 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huaqiang Wu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wong_H/0/1/0/all/0/1\">H.-S. Philip Wong</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Cauwenberghs_G/0/1/0/all/0/1\">Gert Cauwenberghs</a> (2) ((1) Stanford University, (2) University of California San Diego, (3) Tsinghua University, (4) University of Notre Dame, (5) University of Pittsburgh)",
          "description": "Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental \"memory wall\": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.",
          "link": "http://arxiv.org/abs/2108.07879",
          "publishedOn": "2021-08-19T01:35:00.445Z",
          "wordCount": 778,
          "title": "Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory. (arXiv:2108.07879v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otness_K/0/1/0/all/0/1\">Karl Otness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gjoka_A/0/1/0/all/0/1\">Arvi Gjoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panozzo_D/0/1/0/all/0/1\">Daniele Panozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peherstorfer_B/0/1/0/all/0/1\">Benjamin Peherstorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_T/0/1/0/all/0/1\">Teseo Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>",
          "description": "Simulating physical systems is a core component of scientific computing,\nencompassing a wide range of physical domains and applications. Recently, there\nhas been a surge in data-driven methods to complement traditional numerical\nsimulations methods, motivated by the opportunity to reduce computational costs\nand/or learn new physical models leveraging access to large collections of\ndata. However, the diversity of problem settings and applications has led to a\nplethora of approaches, each one evaluated on a different setup and with\ndifferent evaluation metrics. We introduce a set of benchmark problems to take\na step towards unified benchmarks and evaluation protocols. We propose four\nrepresentative physical systems, as well as a collection of both widely used\nclassical time integrators and representative data-driven methods\n(kernel-based, MLP, CNN, nearest neighbors). Our framework allows evaluating\nobjectively and systematically the stability, accuracy, and computational\nefficiency of data-driven methods. Additionally, it is configurable to permit\nadjustments for accommodating other learning tasks and for establishing a\nfoundation for future developments in machine learning for scientific\ncomputing.",
          "link": "http://arxiv.org/abs/2108.07799",
          "publishedOn": "2021-08-19T01:35:00.423Z",
          "wordCount": 622,
          "title": "An Extensible Benchmark Suite for Learning to Simulate Physical Systems. (arXiv:2108.07799v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>",
          "description": "Recently, FCNs have attracted widespread attention in the CD field. In\npursuit of better CD performance, it has become a tendency to design deeper and\nmore complicated FCNs, which inevitably brings about huge numbers of parameters\nand an unbearable computational burden. With the goal of designing a quite deep\narchitecture to obtain more precise CD results while simultaneously decreasing\nparameter numbers to improve efficiency, in this work, we present a very deep\nand efficient CD network, entitled EffCDNet. In EffCDNet, to reduce the\nnumerous parameters associated with deep architecture, an efficient convolution\nconsisting of depth-wise convolution and group convolution with a channel\nshuffle mechanism is introduced to replace standard convolutional layers. In\nterms of the specific network architecture, EffCDNet does not use mainstream\nUNet-like architecture, but rather adopts the architecture with a very deep\nencoder and a lightweight decoder. In the very deep encoder, two very deep\nsiamese streams stacked by efficient convolution first extract two highly\nrepresentative and informative feature maps from input image-pairs.\nSubsequently, an efficient ASPP module is designed to capture multi-scale\nchange information. In the lightweight decoder, a recurrent criss-cross\nself-attention (RCCA) module is applied to efficiently utilize non-local\nsimilar feature representations to enhance discriminability for each pixel,\nthus effectively separating the changed and unchanged regions. Moreover, to\ntackle the optimization problem in confused pixels, two novel loss functions\nbased on information entropy are presented. On two challenging CD datasets, our\napproach outperforms other SOTA FCN-based methods, with only benchmark-level\nparameter numbers and quite low computational overhead.",
          "link": "http://arxiv.org/abs/2108.08157",
          "publishedOn": "2021-08-19T01:35:00.250Z",
          "wordCount": 716,
          "title": "Towards Deep and Efficient: A Deep Siamese Self-Attention Fully Efficient Convolutional Network for Change Detection in VHR Images. (arXiv:2108.08157v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1\">Bojia Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shihao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "Adversarial training is one effective approach for training robust deep\nneural networks against adversarial attacks. While being able to bring reliable\nrobustness, adversarial training (AT) methods in general favor high capacity\nmodels, i.e., the larger the model the better the robustness. This tends to\nlimit their effectiveness on small models, which are more preferable in\nscenarios where storage or computing resources are very limited (e.g., mobile\ndevices). In this paper, we leverage the concept of knowledge distillation to\nimprove the robustness of small models by distilling from adversarially trained\nlarge models. We first revisit several state-of-the-art AT methods from a\ndistillation perspective and identify one common technique that can lead to\nimproved robustness: the use of robust soft labels -- predictions of a robust\nmodel. Following this observation, we propose a novel adversarial robustness\ndistillation method called Robust Soft Label Adversarial Distillation (RSLAD)\nto train robust small student models. RSLAD fully exploits the robust soft\nlabels produced by a robust (adversarially-trained) large teacher model to\nguide the student's learning on both natural and adversarial examples in all\nloss terms. We empirically demonstrate the effectiveness of our RSLAD approach\nover existing adversarial training and distillation methods in improving the\nrobustness of small models against state-of-the-art attacks including the\nAutoAttack. We also provide a set of understandings on our RSLAD and the\nimportance of robust soft labels for adversarial robustness distillation.",
          "link": "http://arxiv.org/abs/2108.07969",
          "publishedOn": "2021-08-19T01:35:00.225Z",
          "wordCount": 668,
          "title": "Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better. (arXiv:2108.07969v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1\">Niamh Belton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welaratne_I/0/1/0/all/0/1\">Ivan Welaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahlan_A/0/1/0/all/0/1\">Adil Dahlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearne_R/0/1/0/all/0/1\">Ronan T Hearne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagos_M/0/1/0/all/0/1\">Misgina Tsighe Hagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1\">Aonghus Lawlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1\">Kathleen M. Curran</a>",
          "description": "This work employs a pre-trained, multi-view Convolutional Neural Network\n(CNN) with a spatial attention block to optimise knee injury detection. An\nopen-source Magnetic Resonance Imaging (MRI) data set with image-level labels\nwas leveraged for this analysis. As MRI data is acquired from three planes, we\ncompare our technique using data from a single-plane and multiple planes\n(multi-plane). For multi-plane, we investigate various methods of fusing the\nplanes in the network. This analysis resulted in the novel 'MPFuseNet' network\nand state-of-the-art Area Under the Curve (AUC) scores for detecting Anterior\nCruciate Ligament (ACL) tears and Abnormal MRIs, achieving AUC scores of 0.977\nand 0.957 respectively. We then developed an objective metric, Penalised\nLocalisation Accuracy (PLA), to validate the model's localisation ability. This\nmetric compares binary masks generated from Grad-Cam output and the\nradiologist's annotations on a sample of MRIs. We also extracted explainability\nfeatures in a model-agnostic approach that were then verified as clinically\nrelevant by the radiologist.",
          "link": "http://arxiv.org/abs/2108.08136",
          "publishedOn": "2021-08-19T01:35:00.194Z",
          "wordCount": 623,
          "title": "Optimising Knee Injury Detection with Spatial Attention and Validating Localisation Ability. (arXiv:2108.08136v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olson_M/0/1/0/all/0/1\">Matthew L. Olson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuy-Vy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_G/0/1/0/all/0/1\">Gaurav Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratzlaff_N/0/1/0/all/0/1\">Neale Ratzlaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Weng-Keen Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1\">Minsuk Kahng</a>",
          "description": "Identifying covariate shift is crucial for making machine learning systems\nrobust in the real world and for detecting training data biases that are not\nreflected in test data. However, detecting covariate shift is challenging,\nespecially when the data consists of high-dimensional images, and when multiple\ntypes of localized covariate shift affect different subspaces of the data.\nAlthough automated techniques can be used to detect the existence of covariate\nshift, our goal is to help human users characterize the extent of covariate\nshift in large image datasets with interfaces that seamlessly integrate\ninformation obtained from the detection algorithms. In this paper, we design\nand evaluate a new visual interface that facilitates the comparison of the\nlocal distributions of training and test data. We conduct a quantitative user\nstudy on multi-attribute facial data to compare two different learned\nlow-dimensional latent representations (pretrained ImageNet CNN vs. density\nratio) and two user analytic workflows (nearest-neighbor vs.\ncluster-to-cluster). Our results indicate that the latent representation of our\ndensity ratio model, combined with a nearest-neighbor comparison, is the most\neffective at helping humans identify covariate shift.",
          "link": "http://arxiv.org/abs/2108.08000",
          "publishedOn": "2021-08-19T01:35:00.187Z",
          "wordCount": 622,
          "title": "Contrastive Identification of Covariate Shift in Image Data. (arXiv:2108.08000v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08129",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1\">George Deligiannidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1\">Valentin De Bortoli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "We establish the uniform in time stability, w.r.t. the marginals, of the\nIterative Proportional Fitting Procedure, also known as Sinkhorn algorithm,\nused to solve entropy-regularised Optimal Transport problems. Our result is\nquantitative and stated in terms of the 1-Wasserstein metric. As a corollary we\nestablish a quantitative stability result for Schr\\\"odinger bridges.",
          "link": "http://arxiv.org/abs/2108.08129",
          "publishedOn": "2021-08-19T01:35:00.075Z",
          "wordCount": 499,
          "title": "Quantitative Uniform Stability of the Iterative Proportional Fitting Procedure. (arXiv:2108.08129v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhirong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedov_D/0/1/0/all/0/1\">Denis Sedov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corander_J/0/1/0/all/0/1\">Jukka Corander</a>",
          "description": "Neighbor Embedding (NE) that aims to preserve pairwise similarities between\ndata items has been shown to yield an effective principle for data\nvisualization. However, even the currently best NE methods such as Stochastic\nNeighbor Embedding (SNE) may leave large-scale patterns such as clusters hidden\ndespite of strong signals being present in the data. To address this, we\npropose a new cluster visualization method based on Neighbor Embedding. We\nfirst present a family of Neighbor Embedding methods which generalizes SNE by\nusing non-normalized Kullback-Leibler divergence with a scale parameter. In\nthis family, much better cluster visualizations often appear with a parameter\nvalue different from the one corresponding to SNE. We also develop an efficient\nsoftware which employs asynchronous stochastic block coordinate descent to\noptimize the new family of objective functions. The experimental results\ndemonstrate that our method consistently and substantially improves\nvisualization of data clusters compared with the state-of-the-art NE\napproaches.",
          "link": "http://arxiv.org/abs/2108.08003",
          "publishedOn": "2021-08-19T01:35:00.028Z",
          "wordCount": 572,
          "title": "Stochastic Cluster Embedding. (arXiv:2108.08003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eberle_S/0/1/0/all/0/1\">Simon Eberle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riekert_A/0/1/0/all/0/1\">Adrian Riekert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Georg S. Weiss</a>",
          "description": "The training of artificial neural networks (ANNs) with rectified linear unit\n(ReLU) activation via gradient descent (GD) type optimization schemes is\nnowadays a common industrially relevant procedure. Till this day in the\nscientific literature there is in general no mathematical convergence analysis\nwhich explains the numerical success of GD type optimization schemes in the\ntraining of ANNs with ReLU activation. GD type optimization schemes can be\nregarded as temporal discretization methods for the gradient flow (GF)\ndifferential equations associated to the considered optimization problem and,\nin view of this, it seems to be a natural direction of research to first aim to\ndevelop a mathematical convergence theory for time-continuous GF differential\nequations and, thereafter, to aim to extend such a time-continuous convergence\ntheory to implementable time-discrete GD type optimization methods. In this\narticle we establish two basic results for GF differential equations in the\ntraining of fully-connected feedforward ANNs with one hidden layer and ReLU\nactivation. In the first main result of this article we establish in the\ntraining of such ANNs under the assumption that the probability distribution of\nthe input data of the considered supervised learning problem is absolutely\ncontinuous with a bounded density function that every GF differential equation\nadmits for every initial value a solution which is also unique among a suitable\nclass of solutions. In the second main result of this article we prove in the\ntraining of such ANNs under the assumption that the target function and the\ndensity function of the probability distribution of the input data are\npiecewise polynomial that every non-divergent GF trajectory converges with an\nappropriate rate of convergence to a critical point and that the risk of the\nnon-divergent GF trajectory converges with rate 1 to the risk of the critical\npoint.",
          "link": "http://arxiv.org/abs/2108.08106",
          "publishedOn": "2021-08-19T01:34:59.912Z",
          "wordCount": 764,
          "title": "Existence, uniqueness, and convergence rates for gradient flows in the training of artificial neural networks with ReLU activation. (arXiv:2108.08106v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08095",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shenavarmasouleh_F/0/1/0/all/0/1\">Farzan Shenavarmasouleh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_F/0/1/0/all/0/1\">Farid Ghareh Mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amini_M/0/1/0/all/0/1\">M. Hadi Amini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taha_T/0/1/0/all/0/1\">Thiab Taha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>",
          "description": "Medical Imaging is one of the growing fields in the world of computer vision.\nIn this study, we aim to address the Diabetic Retinopathy (DR) problem as one\nof the open challenges in medical imaging. In this research, we propose a new\nlesion detection architecture, comprising of two sub-modules, which is an\noptimal solution to detect and find not only the type of lesions caused by DR,\ntheir corresponding bounding boxes, and their masks; but also the severity\nlevel of the overall case. Aside from traditional accuracy, we also use two\npopular evaluation criteria to evaluate the outputs of our models, which are\nintersection over union (IOU) and mean average precision (mAP). We hypothesize\nthat this new solution enables specialists to detect lesions with high\nconfidence and estimate the severity of the damage with high accuracy.",
          "link": "http://arxiv.org/abs/2108.08095",
          "publishedOn": "2021-08-19T01:34:59.662Z",
          "wordCount": 620,
          "title": "DRDrV3: Complete Lesion Detection in Fundus Images Using Mask R-CNN, Transfer Learning, and LSTM. (arXiv:2108.08095v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07992",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Le_K/0/1/0/all/0/1\">Khang Le</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pham_T/0/1/0/all/0/1\">Tung Pham</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>",
          "description": "We study the multi-marginal partial optimal transport (POT) problem between\n$m$ discrete (unbalanced) measures with at most $n$ supports. We first prove\nthat we can obtain two equivalence forms of the multimarginal POT problem in\nterms of the multimarginal optimal transport problem via novel extensions of\ncost tensor. The first equivalence form is derived under the assumptions that\nthe total masses of each measure are sufficiently close while the second\nequivalence form does not require any conditions on these masses but at the\nprice of more sophisticated extended cost tensor. Our proof techniques for\nobtaining these equivalence forms rely on novel procedures of moving mass in\ngraph theory to push transportation plan into appropriate regions. Finally,\nbased on the equivalence forms, we develop optimization algorithm, named\nApproxMPOT algorithm, that builds upon the Sinkhorn algorithm for solving the\nentropic regularized multimarginal optimal transport. We demonstrate that the\nApproxMPOT algorithm can approximate the optimal value of multimarginal POT\nproblem with a computational complexity upper bound of the order\n$\\tilde{\\mathcal{O}}(m^3(n+1)^{m}/ \\varepsilon^2)$ where $\\varepsilon > 0$\nstands for the desired tolerance.",
          "link": "http://arxiv.org/abs/2108.07992",
          "publishedOn": "2021-08-19T01:34:59.592Z",
          "wordCount": 645,
          "title": "On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity. (arXiv:2108.07992v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1\">Claudio Gallicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>",
          "description": "Continual Learning (CL) refers to a learning setup where data is non\nstationary and the model has to learn without forgetting existing knowledge.\nThe study of CL for sequential patterns revolves around trained recurrent\nnetworks. In this work, instead, we introduce CL in the context of Echo State\nNetworks (ESNs), where the recurrent component is kept fixed. We provide the\nfirst evaluation of catastrophic forgetting in ESNs and we highlight the\nbenefits in using CL strategies which are not applicable to trained recurrent\nmodels. Our results confirm the ESN as a promising model for CL and open to its\nuse in streaming scenarios.",
          "link": "http://arxiv.org/abs/2105.07674",
          "publishedOn": "2021-08-18T01:55:00.944Z",
          "wordCount": 581,
          "title": "Continual Learning with Echo State Networks. (arXiv:2105.07674v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Yuqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>",
          "description": "We present a task and benchmark dataset for person-centric visual grounding,\nthe problem of linking between people named in a caption and people pictured in\nan image. In contrast to prior work in visual grounding, which is predominantly\nobject-based, our new task masks out the names of people in captions in order\nto encourage methods trained on such image-caption pairs to focus on contextual\ncues (such as rich interactions between multiple people), rather than learning\nassociations between names and appearances. To facilitate this task, we\nintroduce a new dataset, Who's Waldo, mined automatically from image-caption\ndata on Wikimedia Commons. We propose a Transformer-based method that\noutperforms several strong baselines on this task, and are releasing our data\nto the research community to spur work on contextual models that consider both\nvision and language.",
          "link": "http://arxiv.org/abs/2108.07253",
          "publishedOn": "2021-08-18T01:55:00.937Z",
          "wordCount": 609,
          "title": "Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coma_Puig_B/0/1/0/all/0/1\">Bernat Coma-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmona_J/0/1/0/all/0/1\">Josep Carmona</a>",
          "description": "Implementing systems based on Machine Learning to detect fraud and other\nNon-Technical Losses (NTL) is challenging: the data available is biased, and\nthe algorithms currently used are black-boxes that cannot be either easily\ntrusted or understood by stakeholders. This work explains our human-in-the-loop\napproach to mitigate these problems in a real system that uses a supervised\nmodel to detect Non-Technical Losses (NTL) for an international utility company\nfrom Spain. This approach exploits human knowledge (e.g. from the data\nscientists or the company's stakeholders) and the information provided by\nexplanatory methods to guide the system during the training process. This\nsimple, efficient method that can be easily implemented in other industrial\nprojects is tested in a real dataset and the results show that the derived\nprediction model is better in terms of accuracy, interpretability, robustness\nand flexibility.",
          "link": "http://arxiv.org/abs/2009.13437",
          "publishedOn": "2021-08-18T01:55:00.903Z",
          "wordCount": 605,
          "title": "A Human-in-the-Loop Approach based on Explainability to Improve NTL Detection. (arXiv:2009.13437v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Svedin_M/0/1/0/all/0/1\">Martin Svedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1\">Artur Podobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Steven W. D. Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1\">Stefano Markidis</a>",
          "description": "One of the most promising approaches for data analysis and exploration of\nlarge data sets is Machine Learning techniques that are inspired by brain\nmodels. Such methods use alternative learning rules potentially more\nefficiently than established learning rules. In this work, we focus on the\npotential of brain-inspired ML for exploiting High-Performance Computing (HPC)\nresources to solve ML problems: we discuss the BCPNN and an HPC implementation,\ncalled StreamBrain, its computational cost, suitability to HPC systems. As an\nexample, we use StreamBrain to analyze the Higgs Boson dataset from High Energy\nPhysics and discriminate between background and signal classes in collisions of\nhigh-energy particle colliders. Overall, we reach up to 69.15% accuracy and\n76.4% Area Under the Curve (AUC) performance.",
          "link": "http://arxiv.org/abs/2107.06676",
          "publishedOn": "2021-08-18T01:55:00.895Z",
          "wordCount": 620,
          "title": "Higgs Boson Classification: Brain-inspired BCPNN Learning with StreamBrain. (arXiv:2107.06676v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "The past decade has seen the rapid development of Reinforcement Learning,\nwhich acquires impressive performance with numerous training resources.\nHowever, one of the greatest challenges in RL is generalization efficiency\n(i.e., generalization performance in a unit time). This paper proposes a\nframework of Active Reinforcement Learning (ARL) over MDPs to improve\ngeneralization efficiency in a limited resource by instance selection. Given a\nnumber of instances, the algorithm chooses out valuable instances as training\nsets while training the policy, thereby costing fewer resources. Unlike\nexisting approaches, we attempt to actively select and use training data rather\nthan train on all the given data, thereby costing fewer resources. Furthermore,\nwe introduce a general instance evaluation metrics and selection mechanism into\nthe framework. Experiments results reveal that the proposed framework with\nProximal Policy Optimization as policy optimizer can effectively improve\ngeneralization efficiency than unselect-ed and unbiased selected methods.",
          "link": "http://arxiv.org/abs/2108.02323",
          "publishedOn": "2021-08-18T01:55:00.860Z",
          "wordCount": 620,
          "title": "Active Reinforcement Learning over MDPs. (arXiv:2108.02323v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vorbach_C/0/1/0/all/0/1\">Charles Vorbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Imitation learning enables high-fidelity, vision-based learning of policies\nwithin rich, photorealistic environments. However, such techniques often rely\non traditional discrete-time neural models and face difficulties in\ngeneralizing to domain shifts by failing to account for the causal\nrelationships between the agent and the environment. In this paper, we propose\na theoretical and experimental framework for learning causal representations\nusing continuous-time neural networks, specifically over their discrete-time\ncounterparts. We evaluate our method in the context of visual-control learning\nof drones over a series of complex tasks, ranging from short- and long-term\nnavigation, to chasing static and dynamic objects through photorealistic\nenvironments. Our results demonstrate that causal continuous-time deep models\ncan perform robust navigation tasks, where advanced recurrent models fail.\nThese models learn complex causal control representations directly from raw\nvisual inputs and scale to solve a variety of tasks using imitation learning.",
          "link": "http://arxiv.org/abs/2106.08314",
          "publishedOn": "2021-08-18T01:55:00.852Z",
          "wordCount": 614,
          "title": "Causal Navigation by Continuous-time Neural Networks. (arXiv:2106.08314v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>",
          "description": "People navigating in unfamiliar buildings take advantage of myriad visual,\nspatial and semantic cues to efficiently achieve their navigation goals.\nTowards equipping computational agents with similar capabilities, we introduce\nPathdreamer, a visual world model for agents navigating in novel indoor\nenvironments. Given one or more previous visual observations, Pathdreamer\ngenerates plausible high-resolution 360 visual observations (RGB, semantic\nsegmentation and depth) for viewpoints that have not been visited, in buildings\nnot seen during training. In regions of high uncertainty (e.g. predicting\naround corners, imagining the contents of an unseen room), Pathdreamer can\npredict diverse scenes, allowing an agent to sample multiple realistic outcomes\nfor a given trajectory. We demonstrate that Pathdreamer encodes useful and\naccessible visual, spatial and semantic knowledge about human environments by\nusing it in the downstream task of Vision-and-Language Navigation (VLN).\nSpecifically, we show that planning ahead with Pathdreamer brings about half\nthe benefit of looking ahead at actual observations from unobserved parts of\nthe environment. We hope that Pathdreamer will help unlock model-based\napproaches to challenging embodied navigation tasks such as navigating to\nspecified objects and VLN.",
          "link": "http://arxiv.org/abs/2105.08756",
          "publishedOn": "2021-08-18T01:55:00.800Z",
          "wordCount": 657,
          "title": "Pathdreamer: A World Model for Indoor Navigation. (arXiv:2105.08756v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "This paper presents a new vision Transformer, called Swin Transformer, that\ncapably serves as a general-purpose backbone for computer vision. Challenges in\nadapting Transformer from language to vision arise from differences between the\ntwo domains, such as large variations in the scale of visual entities and the\nhigh resolution of pixels in images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose representation is\ncomputed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme\nbrings greater efficiency by limiting self-attention computation to\nnon-overlapping local windows while also allowing for cross-window connection.\nThis hierarchical architecture has the flexibility to model at various scales\nand has linear computational complexity with respect to image size. These\nqualities of Swin Transformer make it compatible with a broad range of vision\ntasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and\ndense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP\non COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its\nperformance surpasses the previous state-of-the-art by a large margin of +2.7\nbox AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the\npotential of Transformer-based models as vision backbones. The hierarchical\ndesign and the shifted window approach also prove beneficial for all-MLP\narchitectures. The code and models are publicly available\nat~\\url{https://github.com/microsoft/Swin-Transformer}.",
          "link": "http://arxiv.org/abs/2103.14030",
          "publishedOn": "2021-08-18T01:55:00.791Z",
          "wordCount": 701,
          "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (arXiv:2103.14030v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qun Li</a>",
          "description": "With the fast development of quantum computing and deep learning, quantum\nneural networks have attracted great attention recently. By leveraging the\npower of quantum computing, deep neural networks can potentially overcome\ncomputational power limitations in classic machine learning. However, when\nmultiple quantum machines wish to train a global model using the local data on\neach machine, it may be very difficult to copy the data into one machine and\ntrain the model. Therefore, a collaborative quantum neural network framework is\nnecessary. In this article, we borrow the core idea of federated learning to\npropose QuantumFed, a quantum federated learning framework to have multiple\nquantum nodes with local quantum data train a mode together. Our experiments\nshow the feasibility and robustness of our framework.",
          "link": "http://arxiv.org/abs/2106.09109",
          "publishedOn": "2021-08-18T01:55:00.785Z",
          "wordCount": 601,
          "title": "QuantumFed: A Federated Learning Framework for Collaborative Quantum Training. (arXiv:2106.09109v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomlinson_K/0/1/0/all/0/1\">Kiran Tomlinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugander_J/0/1/0/all/0/1\">Johan Ugander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1\">Austin R. Benson</a>",
          "description": "Standard methods in preference learning involve estimating the parameters of\ndiscrete choice models from data of selections (choices) made by individuals\nfrom a discrete set of alternatives (the choice set). While there are many\nmodels for individual preferences, existing learning methods overlook how\nchoice set assignment affects the data. Often, the choice set itself is\ninfluenced by an individual's preferences; for instance, a consumer choosing a\nproduct from an online retailer is often presented with options from a\nrecommender system that depend on information about the consumer's preferences.\nIgnoring these assignment mechanisms can mislead choice models into making\nbiased estimates of preferences, a phenomenon that we call choice set\nconfounding; we demonstrate the presence of such confounding in widely-used\nchoice datasets.\n\nTo address this issue, we adapt methods from causal inference to the discrete\nchoice setting. We use covariates of the chooser for inverse probability\nweighting and/or regression controls, accurately recovering individual\npreferences in the presence of choice set confounding under certain\nassumptions. When such covariates are unavailable or inadequate, we develop\nmethods that take advantage of structured choice set assignment to improve\nprediction. We demonstrate the effectiveness of our methods on real-world\nchoice data, showing, for example, that accounting for choice set confounding\nmakes choices observed in hotel booking and commute transportation more\nconsistent with rational utility-maximization.",
          "link": "http://arxiv.org/abs/2105.07959",
          "publishedOn": "2021-08-18T01:55:00.778Z",
          "wordCount": 689,
          "title": "Choice Set Confounding in Discrete Choice. (arXiv:2105.07959v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose a point cloud-applicable explainability approach\nbased on local surrogate model-based method to show which components contribute\nto the classification. Moreover, we propose quantitative fidelity validations\nfor generated explanations that enhance the persuasive power of explainability\nand compare the plausibility of different existing point cloud-applicable\nexplainability methods. Our new explainability approach provides a fairly\naccurate, more semantically coherent and widely applicable explanation for\npoint cloud classification tasks. Our code is available at\nhttps://github.com/Explain3D/LIME-3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-08-18T01:55:00.760Z",
          "wordCount": 597,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhikang T. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masahito Ueda</a>",
          "description": "Despite the empirical success of the deep Q network (DQN) reinforcement\nlearning algorithm and its variants, DQN is still not well understood and it\ndoes not guarantee convergence. In this work, we show that DQN can diverge and\ncease to operate in realistic settings. Although there exist gradient-based\nconvergent methods, we show that they actually have inherent problems in\nlearning behaviour and elucidate why they often fail in practice. To overcome\nthese problems, we propose a convergent DQN algorithm (C-DQN) by carefully\nmodifying DQN, and we show that the algorithm is convergent and can work with\nlarge discount factors (0.9998). It learns robustly in difficult settings and\ncan learn several difficult games in the Atari 2600 benchmark where DQN fail,\nwithin a moderate computational budget. Our codes have been publicly released\nand can be used to reproduce our results.",
          "link": "http://arxiv.org/abs/2106.15419",
          "publishedOn": "2021-08-18T01:55:00.753Z",
          "wordCount": 597,
          "title": "A Convergent and Efficient Deep Q Network Algorithm. (arXiv:2106.15419v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_K/0/1/0/all/0/1\">Kritika Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1\">Andrew Trask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "In recent years, formal methods of privacy protection such as differential\nprivacy (DP), capable of deployment to data-driven tasks such as machine\nlearning (ML), have emerged. Reconciling large-scale ML with the closed-form\nreasoning required for the principled analysis of individual privacy loss\nrequires the introduction of new tools for automatic sensitivity analysis and\nfor tracking an individual's data and their features through the flow of\ncomputation. For this purpose, we introduce a novel \\textit{hybrid} automatic\ndifferentiation (AD) system which combines the efficiency of reverse-mode AD\nwith an ability to obtain a closed-form expression for any given quantity in\nthe computational graph. This enables modelling the sensitivity of arbitrary\ndifferentiable function compositions, such as the training of neural networks\non private data. We demonstrate our approach by analysing the individual DP\nguarantees of statistical database queries. Moreover, we investigate the\napplication of our technique to the training of DP neural networks. Our\napproach can enable the principled reasoning about privacy loss in the setting\nof data processing, and further the development of automatic sensitivity\nanalysis and privacy budgeting systems.",
          "link": "http://arxiv.org/abs/2107.04265",
          "publishedOn": "2021-08-18T01:55:00.747Z",
          "wordCount": 675,
          "title": "Sensitivity analysis in differentially private machine learning using hybrid automatic differentiation. (arXiv:2107.04265v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Norcliffe_A/0/1/0/all/0/1\">Alexander Norcliffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1\">Cristian Bodnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Day_B/0/1/0/all/0/1\">Ben Day</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moss_J/0/1/0/all/0/1\">Jacob Moss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>",
          "description": "Neural Ordinary Differential Equations (NODEs) use a neural network to model\nthe instantaneous rate of change in the state of a system. However, despite\ntheir apparent suitability for dynamics-governed time-series, NODEs present a\nfew disadvantages. First, they are unable to adapt to incoming data points, a\nfundamental requirement for real-time applications imposed by the natural\ndirection of time. Second, time series are often composed of a sparse set of\nmeasurements that could be explained by many possible underlying dynamics.\nNODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are\na family of models providing uncertainty estimation and fast data adaptation\nbut lack an explicit treatment of the flow of time. To address these problems,\nwe introduce Neural ODE Processes (NDPs), a new class of stochastic processes\ndetermined by a distribution over Neural ODEs. By maintaining an adaptive\ndata-dependent distribution over the underlying ODE, we show that our model can\nsuccessfully capture the dynamics of low-dimensional systems from just a few\ndata points. At the same time, we demonstrate that NDPs scale up to challenging\nhigh-dimensional time-series with unknown latent dynamics such as rotating\nMNIST digits.",
          "link": "http://arxiv.org/abs/2103.12413",
          "publishedOn": "2021-08-18T01:55:00.716Z",
          "wordCount": 648,
          "title": "Neural ODE Processes. (arXiv:2103.12413v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saksham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>",
          "description": "Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.",
          "link": "http://arxiv.org/abs/2101.06069",
          "publishedOn": "2021-08-18T01:55:00.708Z",
          "wordCount": 702,
          "title": "Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1\">Takeshi Teshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Causal graphs (CGs) are compact representations of the knowledge of the data\ngenerating processes behind the data distributions. When a CG is available,\ne.g., from the domain knowledge, we can infer the conditional independence (CI)\nrelations that should hold in the data distribution. However, it is not\nstraightforward how to incorporate this knowledge into predictive modeling. In\nthis work, we propose a model-agnostic data augmentation method that allows us\nto exploit the prior knowledge of the CI encoded in a CG for supervised machine\nlearning. We theoretically justify the proposed method by providing an excess\nrisk bound indicating that the proposed method suppresses overfitting by\nreducing the apparent complexity of the predictor hypothesis class. Using\nreal-world data with CGs provided by domain experts, we experimentally show\nthat the proposed method is effective in improving the prediction accuracy,\nespecially in the small-data regime.",
          "link": "http://arxiv.org/abs/2103.00136",
          "publishedOn": "2021-08-18T01:55:00.699Z",
          "wordCount": 637,
          "title": "Incorporating Causal Graphical Prior Knowledge into Predictive Modeling via Simple Data Augmentation. (arXiv:2103.00136v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jacek Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jakub Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraskiewicz_W/0/1/0/all/0/1\">Witold Kraskiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topolewski_M/0/1/0/all/0/1\">Mateusz Topolewski</a>",
          "description": "Various modifications of TRANSFORMER were recently used to solve time-series\nforecasting problem. We propose Query Selector - an efficient, deterministic\nalgorithm for sparse attention matrix. Experiments show it achieves\nstate-of-the art results on ETT, Helpdesk and BPI'12 datasets.",
          "link": "http://arxiv.org/abs/2107.08687",
          "publishedOn": "2021-08-18T01:55:00.690Z",
          "wordCount": 504,
          "title": "Long-term series forecasting with Query Selector -- efficient model of sparse attention. (arXiv:2107.08687v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+E%2E_W/0/1/0/all/0/1\">Wilson E. Marc&#xed;lio-Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eler_D/0/1/0/all/0/1\">Danilo M. Eler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1\">Rog&#xe9;rio E. Garcia</a>",
          "description": "Cluster interpretation after dimensionality reduction (DR) is a ubiquitous\npart of exploring multidimensional datasets. DR results are frequently\nrepresented by scatterplots, where spatial proximity encodes similarity among\ndata samples. In the literature, techniques support the understanding of\nscatterplots' organization by visualizing the importance of the features for\ncluster definition with layout enrichment strategies. However, current\napproaches usually focus on global information, hampering the analysis whenever\nthe focus is to understand the differences among clusters. Thus, this paper\nintroduces a methodology to visually explore DR results and interpret clusters'\nformation based on contrastive analysis. We also introduce a bipartite graph to\nvisually interpret and explore the relationship between the statistical\nvariables employed to understand how the data features influence cluster\nformation. Our approach is demonstrated through case studies, in which we\nexplore two document collections related to news articles and tweets about\nCOVID-19 symptoms. Finally, we evaluate our approach through quantitative\nresults to demonstrate its robustness to support multidimensional analysis.",
          "link": "http://arxiv.org/abs/2101.12044",
          "publishedOn": "2021-08-18T01:55:00.684Z",
          "wordCount": 668,
          "title": "Contrastive analysis for scatterplot-based representations of dimensionality reduction. (arXiv:2101.12044v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1\">Samuel G. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "Automatic augmentation methods have recently become a crucial pillar for\nstrong model performance in vision tasks. While existing automatic augmentation\nmethods need to trade off simplicity, cost and performance, we present a most\nsimple baseline, TrivialAugment, that outperforms previous methods for almost\nfree. TrivialAugment is parameter-free and only applies a single augmentation\nto each image. Thus, TrivialAugment's effectiveness is very unexpected to us\nand we performed very thorough experiments to study its performance. First, we\ncompare TrivialAugment to previous state-of-the-art methods in a variety of\nimage classification scenarios. Then, we perform multiple ablation studies with\ndifferent augmentation spaces, augmentation methods and setups to understand\nthe crucial requirements for its performance. Additionally, we provide a simple\ninterface to facilitate the widespread adoption of automatic augmentation\nmethods, as well as our full code base for reproducibility. Since our work\nreveals a stagnation in many parts of automatic augmentation research, we end\nwith a short proposal of best practices for sustained future progress in\nautomatic augmentation methods.",
          "link": "http://arxiv.org/abs/2103.10158",
          "publishedOn": "2021-08-18T01:55:00.664Z",
          "wordCount": 630,
          "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation. (arXiv:2103.10158v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloem_Reddy_B/0/1/0/all/0/1\">Benjamin Bloem-Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1\">Karen Ullrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1\">Chris J. Maddison</a>",
          "description": "Most data is automatically collected and only ever \"seen\" by algorithms. Yet,\ndata compressors preserve perceptual fidelity rather than just the information\nneeded by algorithms performing downstream tasks. In this paper, we\ncharacterize the bit-rate required to ensure high performance on all predictive\ntasks that are invariant under a set of transformations, such as data\naugmentations. Based on our theory, we design unsupervised objectives for\ntraining neural compressors. Using these objectives, we train a generic image\ncompressor that achieves substantial rate savings (more than $1000\\times$ on\nImageNet) compared to JPEG on 8 datasets, without decreasing downstream\nclassification performance.",
          "link": "http://arxiv.org/abs/2106.10800",
          "publishedOn": "2021-08-18T01:55:00.657Z",
          "wordCount": 572,
          "title": "Lossy Compression for Lossless Prediction. (arXiv:2106.10800v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pavllo_D/0/1/0/all/0/1\">Dario Pavllo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas Kohler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1\">Aurelien Lucchi</a>",
          "description": "Recent advances in differentiable rendering have sparked an interest in\nlearning generative models of textured 3D meshes from image collections. These\nmodels natively disentangle pose and appearance, enable downstream applications\nin computer graphics, and improve the ability of generative models to\nunderstand the concept of image formation. Although there has been prior work\non learning such models from collections of 2D images, these approaches require\na delicate pose estimation step that exploits annotated keypoints, thereby\nrestricting their applicability to a few specific datasets. In this work, we\npropose a GAN framework for generating textured triangle meshes without relying\non such annotations. We show that the performance of our approach is on par\nwith prior work that relies on ground-truth keypoints, and more importantly, we\ndemonstrate the generality of our method by setting new baselines on a larger\nset of categories from ImageNet - for which keypoints are not available -\nwithout any class-specific hyperparameter tuning. We release our code at\nhttps://github.com/dariopavllo/textured-3d-gan",
          "link": "http://arxiv.org/abs/2103.15627",
          "publishedOn": "2021-08-18T01:55:00.648Z",
          "wordCount": 642,
          "title": "Learning Generative Models of Textured 3D Meshes from Real-World Images. (arXiv:2103.15627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.01653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ledent_A/0/1/0/all/0/1\">Antoine Ledent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1\">Rodrigo Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1\">Marius Kloft</a>",
          "description": "We propose orthogonal inductive matrix completion (OMIC), an interpretable\napproach to matrix completion based on a sum of multiple orthonormal side\ninformation terms, together with nuclear-norm regularization. The approach\nallows us to inject prior knowledge about the singular vectors of the ground\ntruth matrix. We optimize the approach by a provably converging algorithm,\nwhich optimizes all components of the model simultaneously. We study the\ngeneralization capabilities of our method in both the distribution-free setting\nand in the case where the sampling distribution admits uniform marginals,\nyielding learning guarantees that improve with the quality of the injected\nknowledge in both cases. As particular cases of our framework, we present\nmodels which can incorporate user and item biases or community information in a\njoint and additive fashion. We analyse the performance of OMIC on several\nsynthetic and real datasets. On synthetic datasets with a sliding scale of user\nbias relevance, we show that OMIC better adapts to different regimes than other\nmethods. On real-life datasets containing user/items recommendations and\nrelevant side information, we find that OMIC surpasses the state-of-the-art,\nwith the added benefit of greater interpretability.",
          "link": "http://arxiv.org/abs/2004.01653",
          "publishedOn": "2021-08-18T01:55:00.640Z",
          "wordCount": 676,
          "title": "Orthogonal Inductive Matrix Completion. (arXiv:2004.01653v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02480",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guitart_A/0/1/0/all/0/1\">Anna Guitart</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rio_A/0/1/0/all/0/1\">Ana Fern&#xe1;ndez del R&#xed;o</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Perianez_A/0/1/0/all/0/1\">&#xc1;frica Peri&#xe1;&#xf1;ez</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bellhouse_L/0/1/0/all/0/1\">Lauren Bellhouse</a>",
          "description": "Every day, 800 women and 6,700 newborns die from complications related to\npregnancy or childbirth. A well-trained midwife can prevent most of these\nmaternal and newborn deaths. Data science models together with logs generated\nby users of online learning applications for midwives can help to improve their\nlearning competencies. The goal is to use these rich behavioral data to push\ndigital learning towards personalized content and to provide an adaptive\nlearning journey. In this work, we evaluate various forecasting methods to\ndetermine the interest of future users on the different kind of contents\navailable in the app, broken down by profession and region.",
          "link": "http://arxiv.org/abs/2107.02480",
          "publishedOn": "2021-08-18T01:55:00.630Z",
          "wordCount": 565,
          "title": "Midwifery Learning and Forecasting: Predicting Content Demand with User-Generated Logs. (arXiv:2107.02480v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Omari_R/0/1/0/all/0/1\">Rollin Omari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKay_R/0/1/0/all/0/1\">R. I. McKay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Raven's Progressive Matrices have been widely used for measuring abstract\nreasoning and intelligence in humans. However for artificial learning systems,\nabstract reasoning remains a challenging problem. In this paper we investigate\nhow neural networks augmented with biologically inspired spiking modules gain a\nsignificant advantage in solving this problem. To illustrate this, we first\ninvestigate the performance of our networks with supervised learning, then with\nunsupervised learning. Experiments on the RAVEN dataset show that the overall\naccuracy of our supervised networks surpass human-level performance, while our\nunsupervised networks significantly outperform existing unsupervised methods.\nFinally, our results from both supervised and unsupervised learning illustrate\nthat, unlike their non-augmented counterparts, networks with spiking modules\nare able to extract and encode temporal features without any explicit\ninstruction, do not heavily rely on training data, and generalise more readily\nto new problems. In summary, the results reported here indicate that artificial\nneural networks with spiking modules are well suited to solving abstract\nreasoning.",
          "link": "http://arxiv.org/abs/2010.06746",
          "publishedOn": "2021-08-18T01:55:00.622Z",
          "wordCount": 672,
          "title": "Analogical and Relational Reasoning with Spiking Neural Networks. (arXiv:2010.06746v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Danfei Xu</a>",
          "description": "Imitation Learning (IL) is an effective framework to learn visuomotor skills\nfrom offline demonstration data. However, IL methods often fail to generalize\nto new scene configurations not covered by training data. On the other hand,\nhumans can manipulate objects in varying conditions. Key to such capability is\nhand-eye coordination, a cognitive ability that enables humans to adaptively\ndirect their movements at task-relevant objects and be invariant to the\nobjects' absolute spatial location. In this work, we present a learnable action\nspace, Hand-eye Action Networks (HAN), that can approximate human's hand-eye\ncoordination behaviors by learning from human teleoperated demonstrations.\nThrough a set of challenging multi-stage manipulation tasks, we show that a\nvisuomotor policy equipped with HAN is able to inherit the key spatial\ninvariance property of hand-eye coordination and achieve zero-shot\ngeneralization to new scene configurations. Additional materials available at\nhttps://sites.google.com/stanford.edu/han",
          "link": "http://arxiv.org/abs/2103.00375",
          "publishedOn": "2021-08-18T01:55:00.604Z",
          "wordCount": 627,
          "title": "Generalization Through Hand-Eye Coordination: An Action Space for Learning Spatially-Invariant Visuomotor Control. (arXiv:2103.00375v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1\">Timothee Masquelier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have attracted enormous research interest due\nto temporal information processing capability, low power consumption, and high\nbiological plausibility. However, the formulation of efficient and\nhigh-performance learning algorithms for SNNs is still challenging. Most\nexisting learning methods learn weights only, and require manual tuning of the\nmembrane-related parameters that determine the dynamics of a single spiking\nneuron. These parameters are typically chosen to be the same for all neurons,\nwhich limits the diversity of neurons and thus the expressiveness of the\nresulting SNNs. In this paper, we take inspiration from the observation that\nmembrane-related parameters are different across brain regions, and propose a\ntraining algorithm that is capable of learning not only the synaptic weights\nbut also the membrane time constants of SNNs. We show that incorporating\nlearnable membrane time constants can make the network less sensitive to\ninitial values and can speed up learning. In addition, we reevaluate the\npooling methods in SNNs and find that max-pooling will not lead to significant\ninformation loss and have the advantage of low computation cost and binary\ncompatibility. We evaluate the proposed method for image classification tasks\non both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and\nneuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment\nresults show that the proposed method outperforms the state-of-the-art accuracy\non nearly all datasets, using fewer time-steps. Our codes are available at\nhttps://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.",
          "link": "http://arxiv.org/abs/2007.05785",
          "publishedOn": "2021-08-18T01:55:00.595Z",
          "wordCount": 750,
          "title": "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks. (arXiv:2007.05785v5 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1\">Suvrit Sra</a>",
          "description": "The label shift problem refers to the supervised learning setting where the\ntrain and test label distributions do not match. Existing work addressing label\nshift usually assumes access to an \\emph{unlabelled} test sample. This sample\nmay be used to estimate the test label distribution, and to then train a\nsuitably re-weighted classifier. While approaches using this idea have proven\neffective, their scope is limited as it is not always feasible to access the\ntarget domain; further, they require repeated retraining if the model is to be\ndeployed in \\emph{multiple} test environments. Can one instead learn a\n\\emph{single} classifier that is robust to arbitrary label shifts from a broad\nfamily? In this paper, we answer this question by proposing a model that\nminimises an objective based on distributionally robust optimisation (DRO). We\nthen design and analyse a gradient descent-proximal mirror ascent algorithm\ntailored for large-scale problems to optimise the proposed objective. %, and\nestablish its convergence. Finally, through experiments on CIFAR-100 and\nImageNet, we show that our technique can significantly improve performance over\na number of baselines in settings where label shift is present.",
          "link": "http://arxiv.org/abs/2010.12230",
          "publishedOn": "2021-08-18T01:55:00.587Z",
          "wordCount": 671,
          "title": "Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04131",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Molnar_C/0/1/0/all/0/1\">Christoph Molnar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1\">Gunnar K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Herbinger_J/0/1/0/all/0/1\">Julia Herbinger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1\">Timo Freiesleben</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dandl_S/0/1/0/all/0/1\">Susanne Dandl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scholbeck_C/0/1/0/all/0/1\">Christian A. Scholbeck</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1\">Giuseppe Casalicchio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1\">Moritz Grosse-Wentrup</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>",
          "description": "An increasing number of model-agnostic interpretation techniques for machine\nlearning (ML) models such as partial dependence plots (PDP), permutation\nfeature importance (PFI) and Shapley values provide insightful model\ninterpretations, but can lead to wrong conclusions if applied incorrectly. We\nhighlight many general pitfalls of ML model interpretation, such as using\ninterpretation techniques in the wrong context, interpreting models that do not\ngeneralize well, ignoring feature dependencies, interactions, uncertainty\nestimates and issues in high-dimensional settings, or making unjustified causal\ninterpretations, and illustrate them with examples. We focus on pitfalls for\nglobal methods that describe the average model behavior, but many pitfalls also\napply to local methods that explain individual predictions. Our paper addresses\nML practitioners by raising awareness of pitfalls and identifying solutions for\ncorrect model interpretation, but also addresses ML researchers by discussing\nopen issues for further research.",
          "link": "http://arxiv.org/abs/2007.04131",
          "publishedOn": "2021-08-18T01:55:00.580Z",
          "wordCount": 603,
          "title": "General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models. (arXiv:2007.04131v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pinyan Lu</a>",
          "description": "Graph matching (GM) has been a building block in many areas including\ncomputer vision and pattern recognition. Despite the recent impressive\nprogress, existing deep GM methods often have difficulty in handling outliers\nin both graphs, which are ubiquitous in practice. We propose a deep\nreinforcement learning (RL) based approach RGM for weighted graph matching,\nwhose sequential node matching scheme naturally fits with the strategy for\nselective inlier matching against outliers, and supports seed graph matching. A\nrevocable action scheme is devised to improve the agent's flexibility against\nthe complex constrained matching task. Moreover, we propose a quadratic\napproximation technique to regularize the affinity matrix, in the presence of\noutliers. As such, the RL agent can finish inlier matching timely when the\nobjective score stop growing, for which otherwise an additional hyperparameter\ni.e. the number of common inliers is needed to avoid matching outliers. In this\npaper, we are focused on learning the back-end solver for the most general form\nof GM: the Lawler's QAP, whose input is the affinity matrix. Our approach can\nalso boost other solvers using the affinity input. Experimental results on both\nsynthetic and real-world datasets showcase its superior performance regarding\nboth matching accuracy and robustness.",
          "link": "http://arxiv.org/abs/2012.08950",
          "publishedOn": "2021-08-18T01:55:00.520Z",
          "wordCount": 686,
          "title": "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching. (arXiv:2012.08950v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abadi_M/0/1/0/all/0/1\">Martin Abadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plotkin_G/0/1/0/all/0/1\">Gordon Plotkin</a>",
          "description": "Describing systems in terms of choices and their resulting costs and rewards\noffers the promise of freeing algorithm designers and programmers from\nspecifying how those choices should be made; in implementations, the choices\ncan be realized by optimization techniques and, increasingly, by\nmachine-learning methods. We study this approach from a programming-language\nperspective. We define two small languages that support decision-making\nabstractions: one with choices and rewards, and the other additionally with\nprobabilities. We give both operational and denotational semantics.\n\nIn the case of the second language we consider three denotational semantics,\nwith varying degrees of correlation between possible program values and\nexpected rewards. The operational semantics combine the usual semantics of\nstandard constructs with optimization over spaces of possible execution\nstrategies. The denotational semantics, which are compositional, rely on the\nselection monad, to handle choice, augmented with an auxiliary monad to handle\nother effects, such as rewards or probability.\n\nWe establish adequacy theorems that the two semantics coincide in all cases.\nWe also prove full abstraction at base types, with varying notions of\nobservation in the probabilistic case corresponding to the various degrees of\ncorrelation. We present axioms for choice combined with rewards and\nprobability, establishing completeness at base types for the case of rewards\nwithout probability.",
          "link": "http://arxiv.org/abs/2007.08926",
          "publishedOn": "2021-08-18T01:55:00.513Z",
          "wordCount": 721,
          "title": "Smart Choices and the Selection Monad. (arXiv:2007.08926v6 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frederic Z. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>",
          "description": "We address the problem of detecting human-object interactions in images using\ngraphical neural networks. Unlike conventional methods, where nodes send scaled\nbut otherwise identical messages to each of their neighbours, we propose to\ncondition messages between pairs of nodes on their spatial relationships,\nresulting in different messages going to neighbours of the same node. To this\nend, we explore various ways of applying spatial conditioning under a\nmulti-branch structure. Through extensive experimentation we demonstrate the\nadvantages of spatial conditioning for the computation of the adjacency\nstructure, messages and the refined graph features. In particular, we\nempirically show that as the quality of the bounding boxes increases, their\ncoarse appearance features contribute relatively less to the disambiguation of\ninteractions compared to the spatial information. Our method achieves an mAP of\n31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming\nstate-of-the-art on fine-tuned detections.",
          "link": "http://arxiv.org/abs/2012.06060",
          "publishedOn": "2021-08-18T01:55:00.506Z",
          "wordCount": 624,
          "title": "Spatially Conditioned Graphs for Detecting Human-Object Interactions. (arXiv:2012.06060v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatzianastasis_M/0/1/0/all/0/1\">Michail Chatzianastasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasoulas_G/0/1/0/all/0/1\">George Dasoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siolas_G/0/1/0/all/0/1\">Georgios Siolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>",
          "description": "Neural Architecture Search (NAS) has recently gained increased attention, as\na class of approaches that automatically searches in an input space of network\narchitectures. A crucial part of the NAS pipeline is the encoding of the\narchitecture that consists of the applied computational blocks, namely the\noperations and the links between them. Most of the existing approaches either\nfail to capture the structural properties of the architectures or use\nhand-engineered vector to encode the operator information. In this paper, we\npropose the replacement of fixed operator encoding with learnable\nrepresentations in the optimization process. This approach, which effectively\ncaptures the relations of different operations, leads to smoother and more\naccurate representations of the architectures and consequently to improved\nperformance of the end task. Our extensive evaluation in ENAS benchmark\ndemonstrates the effectiveness of the proposed operation embeddings to the\ngeneration of highly accurate models, achieving state-of-the-art performance.\nFinally, our method produces top-performing architectures that share similar\noperation and graph patterns, highlighting a strong correlation between the\nstructural properties of the architecture and its performance.",
          "link": "http://arxiv.org/abs/2105.04885",
          "publishedOn": "2021-08-18T01:55:00.497Z",
          "wordCount": 645,
          "title": "Graph-based Neural Architecture Search with Operation Embeddings. (arXiv:2105.04885v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1\">Luciana Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLaren_M/0/1/0/all/0/1\">Mitchell McLaren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brummer_N/0/1/0/all/0/1\">Niko Brummer</a>",
          "description": "In this paper, we address the problem of speaker verification in conditions\nunseen or unknown during development. A standard method for speaker\nverification consists of extracting speaker embeddings with a deep neural\nnetwork and processing them through a backend composed of probabilistic linear\ndiscriminant analysis (PLDA) and global logistic regression score calibration.\nThis method is known to result in systems that work poorly on conditions\ndifferent from those used to train the calibration model. We propose to modify\nthe standard backend, introducing an adaptive calibrator that uses duration and\nother automatically extracted side-information to adapt to the conditions of\nthe inputs. The backend is trained discriminatively to optimize binary\ncross-entropy. When trained on a number of diverse datasets that are labeled\nonly with respect to speaker, the proposed backend consistently and, in some\ncases, dramatically improves calibration, compared to the standard PLDA\napproach, on a number of held-out datasets, some of which are markedly\ndifferent from the training data. Discrimination performance is also\nconsistently improved. We show that joint training of the PLDA and the adaptive\ncalibrator is essential -- the same benefits cannot be achieved when freezing\nPLDA and fine-tuning the calibrator. To our knowledge, the results in this\npaper are the first evidence in the literature that it is possible to develop a\nspeaker verification system with robust out-of-the-box performance on a large\nvariety of conditions.",
          "link": "http://arxiv.org/abs/2102.01760",
          "publishedOn": "2021-08-18T01:55:00.490Z",
          "wordCount": 699,
          "title": "A Speaker Verification Backend with Robust Performance across Conditions. (arXiv:2102.01760v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Abdullah Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Multi-view projection methods have demonstrated their ability to reach\nstate-of-the-art performance on 3D shape recognition. Those methods learn\ndifferent ways to aggregate information from multiple views. However, the\ncamera view-points for those views tend to be heuristically set and fixed for\nall shapes. To circumvent the lack of dynamism of current multi-view methods,\nwe propose to learn those view-points. In particular, we introduce the\nMulti-View Transformation Network (MVTN) that regresses optimal view-points for\n3D shape recognition, building upon advances in differentiable rendering. As a\nresult, MVTN can be trained end-to-end along with any multi-view network for 3D\nshape classification. We integrate MVTN in a novel adaptive multi-view pipeline\nthat can render either 3D meshes or point clouds. MVTN exhibits clear\nperformance gains in the tasks of 3D shape classification and 3D shape\nretrieval without the need for extra training supervision. In these tasks, MVTN\nachieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the\nmost recent and realistic ScanObjectNN dataset (up to 6% improvement).\nInterestingly, we also show that MVTN can provide network robustness against\nrotation and occlusion in the 3D domain. The code is available at\nhttps://github.com/ajhamdi/MVTN .",
          "link": "http://arxiv.org/abs/2011.13244",
          "publishedOn": "2021-08-18T01:55:00.482Z",
          "wordCount": 675,
          "title": "MVTN: Multi-View Transformation Network for 3D Shape Recognition. (arXiv:2011.13244v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jaehui Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jun-Hyuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun-Ho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>",
          "description": "The video-based action recognition task has been extensively studied in\nrecent years. In this paper, we study the structural vulnerability of deep\nlearning-based action recognition models against the adversarial attack using\nthe one frame attack that adds an inconspicuous perturbation to only a single\nframe of a given video clip. Our analysis shows that the models are highly\nvulnerable against the one frame attack due to their structural properties.\nExperiments demonstrate high fooling rates and inconspicuous characteristics of\nthe attack. Furthermore, we show that strong universal one frame perturbations\ncan be obtained under various scenarios. Our work raises the serious issue of\nadversarial vulnerability of the state-of-the-art action recognition models in\nvarious perspectives.",
          "link": "http://arxiv.org/abs/2011.14585",
          "publishedOn": "2021-08-18T01:55:00.371Z",
          "wordCount": 596,
          "title": "Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack. (arXiv:2011.14585v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1\">Stefan Heid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.",
          "link": "http://arxiv.org/abs/2008.01377",
          "publishedOn": "2021-08-18T01:55:00.345Z",
          "wordCount": 718,
          "title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.08085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a> (PANAMA, DANTE), <a href=\"http://arxiv.org/find/cs/1/au:+Blanchard_G/0/1/0/all/0/1\">Gilles Blanchard</a> (LMO), <a href=\"http://arxiv.org/find/cs/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a> (GIPSA-GAIA), <a href=\"http://arxiv.org/find/cs/1/au:+Traonmilin_Y/0/1/0/all/0/1\">Yann Traonmilin</a> (IMB)",
          "description": "We provide statistical learning guarantees for two unsupervised learning\ntasks in the context of compressive statistical learning, a general framework\nfor resource-efficient large-scale learning that we introduced in a companion\npaper.The principle of compressive statistical learning is to compress a\ntraining collection, in one pass, into a low-dimensional sketch (a vector of\nrandom empirical generalized moments) that captures the information relevant to\nthe considered learning task. We explicitly describe and analyze random feature\nfunctions which empirical averages preserve the needed information for\ncompressive clustering and compressive Gaussian mixture modeling with fixed\nknown variance, and establish sufficient sketch sizes given the problem\ndimensions.",
          "link": "http://arxiv.org/abs/2004.08085",
          "publishedOn": "2021-08-18T01:55:00.326Z",
          "wordCount": 629,
          "title": "Statistical Learning Guarantees for Compressive Clustering and Compressive Mixture Modeling. (arXiv:2004.08085v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.08543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beeler_C/0/1/0/all/0/1\">Chris Beeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahorau_U/0/1/0/all/0/1\">Uladzimir Yahorau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coles_R/0/1/0/all/0/1\">Rory Coles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Kyle Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitelam_S/0/1/0/all/0/1\">Stephen Whitelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1\">Isaac Tamblyn</a>",
          "description": "Using a model heat engine, we show that neural network-based reinforcement\nlearning can identify thermodynamic trajectories of maximal efficiency. We\nconsider both gradient and gradient-free reinforcement learning. We use an\nevolutionary learning algorithm to evolve a population of neural networks,\nsubject to a directive to maximize the efficiency of a trajectory composed of a\nset of elementary thermodynamic processes; the resulting networks learn to\ncarry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given\nan additional irreversible process, this evolutionary scheme learns a\npreviously unknown thermodynamic cycle. Gradient-based reinforcement learning\nis able to learn the Stirling cycle, whereas an evolutionary approach achieves\nthe optimal Carnot cycle. Our results show how the reinforcement learning\nstrategies developed for game playing can be applied to solve physical problems\nconditioned upon path-extensive order parameters.",
          "link": "http://arxiv.org/abs/1903.08543",
          "publishedOn": "2021-08-18T01:55:00.306Z",
          "wordCount": 647,
          "title": "Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning. (arXiv:1903.08543v5 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mahdi Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siavoshani_M/0/1/0/all/0/1\">Mahdi Jafari Siavoshani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahangir_A/0/1/0/all/0/1\">Amir Hossein Jahangir</a>",
          "description": "The growing number of Internet users and the prevalence of web applications\nmake it necessary to deal with very complex software and applications in the\nnetwork. This results in an increasing number of new vulnerabilities in the\nsystems, and leading to an increase in cyber threats and, in particular,\nzero-day attacks. The cost of generating appropriate signatures for these\nattacks is a potential motive for using machine learning-based methodologies.\nAlthough there are many studies on using learning-based methods for attack\ndetection, they generally use extracted features and overlook raw contents.\nThis approach can lessen the performance of detection systems against\ncontent-based attacks like SQL injection, Cross-site Scripting (XSS), and\nvarious viruses.\n\nIn this work, we propose a framework, called deep intrusion detection (DID)\nsystem, that uses the pure content of traffic flows in addition to traffic\nmetadata in the learning and detection phases of a passive DNN IDS. To this\nend, we deploy and evaluate an offline IDS following the framework using LSTM\nas a deep learning technique. Due to the inherent nature of deep learning, it\ncan process high dimensional data content and, accordingly, discover the\nsophisticated relations between the auto extracted features of the traffic. To\nevaluate the proposed DID system, we use the CIC-IDS2017 and CSE-CIC-IDS2018\ndatasets. The evaluation metrics, such as precision and recall, reach $0.992$\nand $0.998$ on CIC-IDS2017, and $0.933$ and $0.923$ on CSE-CIC-IDS2018\nrespectively, which show the high performance of the proposed DID method.",
          "link": "http://arxiv.org/abs/2001.05009",
          "publishedOn": "2021-08-18T01:55:00.238Z",
          "wordCount": 710,
          "title": "A Content-Based Deep Intrusion Detection System. (arXiv:2001.05009v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Benlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "3D point cloud understanding has made great progress in recent years.\nHowever, one major bottleneck is the scarcity of annotated real datasets,\nespecially compared to 2D object detection tasks, since a large amount of labor\nis involved in annotating the real scans of a scene. A promising solution to\nthis problem is to make better use of the synthetic dataset, which consists of\nCAD object models, to boost the learning on real datasets. This can be achieved\nby the pre-training and fine-tuning procedure. However, recent work on 3D\npre-training exhibits failure when transfer features learned on synthetic\nobjects to other real-world applications. In this work, we put forward a new\nmethod called RandomRooms to accomplish this objective. In particular, we\npropose to generate random layouts of a scene by making use of the objects in\nthe synthetic CAD dataset and learn the 3D scene representation by applying\nobject-level contrastive learning on two random scenes generated from the same\nset of synthetic objects. The model pre-trained in this way can serve as a\nbetter initialization when later fine-tuning on the 3D object detection task.\nEmpirically, we show consistent improvement in downstream 3D detection tasks on\nseveral base models, especially when less training data are used, which\nstrongly demonstrates the effectiveness and generalization of our method.\nBenefiting from the rich semantic knowledge and diverse objects from synthetic\ndata, our method establishes the new state-of-the-art on widely-used 3D\ndetection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide\na new perspective for bridging object and scene-level 3D understanding.",
          "link": "http://arxiv.org/abs/2108.07794",
          "publishedOn": "2021-08-18T01:55:00.217Z",
          "wordCount": 720,
          "title": "RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection. (arXiv:2108.07794v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qafari_M/0/1/0/all/0/1\">Mahnaz Sadat Qafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1\">Wil van der Aalst</a>",
          "description": "Process mining techniques can help organizations to improve their operational\nprocesses. Organizations can benefit from process mining techniques in finding\nand amending the root causes of performance or compliance problems. Considering\nthe volume of the data and the number of features captured by the information\nsystem of today's companies, the task of discovering the set of features that\nshould be considered in root cause analysis can be quite involving. In this\npaper, we propose a method for finding the set of (aggregated) features with a\npossible effect on the problem.\n\nThe root cause analysis task is usually done by applying a machine learning\ntechnique to the data gathered from the information system supporting the\nprocesses. To prevent mixing up correlation and causation, which may happen\nbecause of interpreting the findings of machine learning techniques as causal,\nwe propose a method for discovering the structural equation model of the\nprocess that can be used for root cause analysis. We have implemented the\nproposed method as a plugin in ProM and we have evaluated it using two real and\nsynthetic event logs. These experiments show the validity and effectiveness of\nthe proposed methods.",
          "link": "http://arxiv.org/abs/2108.07795",
          "publishedOn": "2021-08-18T01:55:00.210Z",
          "wordCount": 633,
          "title": "Feature Recommendation for Structural Equation Model Discovery in Process Mining. (arXiv:2108.07795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Assessing action quality is challenging due to the subtle differences between\nvideos and large variations in scores. Most existing approaches tackle this\nproblem by regressing a quality score from a single video, suffering a lot from\nthe large inter-video score variations. In this paper, we show that the\nrelations among videos can provide important clues for more accurate action\nquality assessment during both training and inference. Specifically, we\nreformulate the problem of action quality assessment as regressing the relative\nscores with reference to another video that has shared attributes (e.g.,\ncategory and difficulty), instead of learning unreferenced scores. Following\nthis formulation, we propose a new Contrastive Regression (CoRe) framework to\nlearn the relative scores by pair-wise comparison, which highlights the\ndifferences between videos and guides the models to learn the key hints for\nassessment. In order to further exploit the relative information between two\nvideos, we devise a group-aware regression tree to convert the conventional\nscore regression into two easier sub-problems: coarse-to-fine classification\nand regression in small intervals. To demonstrate the effectiveness of CoRe, we\nconduct extensive experiments on three mainstream AQA datasets including AQA-7,\nMTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large\nmargin and establishes new state-of-the-art on all three benchmarks.",
          "link": "http://arxiv.org/abs/2108.07797",
          "publishedOn": "2021-08-18T01:55:00.203Z",
          "wordCount": 654,
          "title": "Group-aware Contrastive Regression for Action Quality Assessment. (arXiv:2108.07797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.02326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Charlier_J/0/1/0/all/0/1\">Jeremy Charlier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarenkov_V/0/1/0/all/0/1\">Vladimir Makarenkov</a>",
          "description": "Bootstrap aggregation, known as bagging, is one of the most popular ensemble\nmethods used in machine learning (ML). An ensemble method is a ML method that\ncombines multiple hypotheses to form a single hypothesis used for prediction. A\nbagging algorithm combines multiple classifiers modeled on different\nsub-samples of the same data set to build one large classifier. Banks, and\ntheir retail banking activities, are nowadays using the power of ML algorithms,\nincluding decision trees and random forests, to optimize their processes.\nHowever, banks have to comply with regulators and governance and, hence,\ndelivering effective ML solutions is a challenging task. It starts with the\nbank's validation and governance department, followed by the deployment of the\nsolution in a production environment up to the external validation of the\nnational financial regulator. Each proposed ML model has to be validated and\nclear rules for every algorithm-based decision must be justified. In this\ncontext, we propose XtracTree, an algorithm capable of efficiently converting\nan ML bagging classifier, such as a random forest, into simple \"if-then\" rules\nsatisfying the requirements of model validation. We use a public loan data set\nfrom Kaggle to illustrate the usefulness of our approach. Our experiments\ndemonstrate that using XtracTree, one can convert an ML model into a rule-based\nalgorithm, leading to easier model validation by national financial regulators\nand the bank's validation department. The proposed approach allowed our banking\ninstitution to reduce up to 50% the time of delivery of our AI solutions to the\nend-user.",
          "link": "http://arxiv.org/abs/2004.02326",
          "publishedOn": "2021-08-18T01:55:00.191Z",
          "wordCount": 735,
          "title": "XtracTree: a Simple and Effective Method for Regulator Validation of Bagging Methods Used in Retail Banking. (arXiv:2004.02326v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anshul Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_R/0/1/0/all/0/1\">Roger Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_L/0/1/0/all/0/1\">Lisa Walker</a>",
          "description": "Introduction: When a learner fails to reach a milestone, educators often\nwonder if there had been any warning signs that could have allowed them to\nintervene sooner. Machine learning is used to predict which students are at\nrisk of failing a national certifying exam. Predictions are made well in\nadvance of the exam, such that educators can meaningfully intervene before\nstudents take the exam.\n\nMethods: Using already-collected, first-year student assessment data from\nfour cohorts in a Master of Physician Assistant Studies program, the authors\nimplement an \"adaptive minimum match\" version of the k-nearest neighbors\nalgorithm (AMMKNN), using changing numbers of neighbors to predict each\nstudent's future exam scores on the Physician Assistant National Certifying\nExamination (PANCE). Leave-one-out cross validation (LOOCV) was used to\nevaluate the practical capabilities of this model, before making predictions\nfor new students.\n\nResults: The best predictive model has an accuracy of 93%, sensitivity of\n69%, and specificity of 94%. It generates a predicted PANCE score for each\nstudent, one year before they are scheduled to take the exam. Students can then\nbe prospectively categorized into groups that need extra support, optional\nextra support, or no extra support. The educator then has one year to provide\nthe appropriate customized support to each type of student.\n\nConclusions: Predictive analytics can help health professions educators\nallocate scarce time and resources across their students. Interprofessional\neducators can use the included methods and code to generate predicted test\noutcomes for students. The authors recommend that educators using this or\nsimilar predictive methods act responsibly and transparently.",
          "link": "http://arxiv.org/abs/2108.07709",
          "publishedOn": "2021-08-18T01:55:00.180Z",
          "wordCount": 697,
          "title": "The application of predictive analytics to identify at-risk students in health professions education. (arXiv:2108.07709v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonsior_J/0/1/0/all/0/1\">Julius Gonsior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiele_M/0/1/0/all/0/1\">Maik Thiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehner_W/0/1/0/all/0/1\">Wolfgang Lehner</a>",
          "description": "One of the biggest challenges that complicates applied supervised machine\nlearning is the need for huge amounts of labeled data. Active Learning (AL) is\na well-known standard method for efficiently obtaining labeled data by first\nlabeling the samples that contain the most information based on a query\nstrategy. Although many methods for query strategies have been proposed in the\npast, no clear superior method that works well in general for all domains has\nbeen found yet. Additionally, many strategies are computationally expensive\nwhich further hinders the widespread use of AL for large-scale annotation\nprojects.\n\nWe, therefore, propose ImitAL, a novel query strategy, which encodes AL as a\nlearning-to-rank problem. For training the underlying neural network we chose\nImitation Learning. The required demonstrative expert experience for training\nis generated from purely synthetic data.\n\nTo show the general and superior applicability of \\ImitAL{}, we perform an\nextensive evaluation comparing our strategy on 15 different datasets, from a\nwide range of domains, with 10 different state-of-the-art query strategies. We\nalso show that our approach is more runtime performant than most other\nstrategies, especially on very large datasets.",
          "link": "http://arxiv.org/abs/2108.07670",
          "publishedOn": "2021-08-18T01:55:00.173Z",
          "wordCount": 615,
          "title": "ImitAL: Learning Active Learning Strategies from Synthetic Data. (arXiv:2108.07670v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samsinger_M/0/1/0/all/0/1\">Maximilian Samsinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merkle_F/0/1/0/all/0/1\">Florian Merkle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schottle_P/0/1/0/all/0/1\">Pascal Sch&#xf6;ttle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1\">Tomas Pevny</a>",
          "description": "Adversarial machine learning, i.e., increasing the robustness of machine\nlearning algorithms against so-called adversarial examples, is now an\nestablished field. Yet, newly proposed methods are evaluated and compared under\nunrealistic scenarios where costs for adversary and defender are not considered\nand either all samples are attacked or no sample is attacked. We scrutinize\nthese assumptions and propose the advanced adversarial classification game,\nwhich incorporates all relevant parameters of an adversary and a defender in\nadversarial classification. Especially, we take into account economic factors\non both sides and the fact that all so far proposed countermeasures against\nadversarial examples reduce accuracy on benign samples. Analyzing the scenario\nin detail, where both players have two pure strategies, we identify all best\nresponses and conclude that in practical settings, the most influential factor\nmight be the maximum amount of adversarial examples.",
          "link": "http://arxiv.org/abs/2108.07602",
          "publishedOn": "2021-08-18T01:55:00.143Z",
          "wordCount": 589,
          "title": "When Should You Defend Your Classifier -- A Game-theoretical Analysis of Countermeasures against Adversarial Examples. (arXiv:2108.07602v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1\">Penghua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_H/0/1/0/all/0/1\">Huaiwei Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>",
          "description": "With the renaissance of deep learning, automatic diagnostic systems for\ncomputed tomography (CT) have achieved many successful applications. However,\nthey are mostly attributed to careful expert annotations, which are often\nscarce in practice. This drives our interest to the unsupervised representation\nlearning. Recent studies have shown that self-supervised learning is an\neffective approach for learning representations, but most of them rely on the\nempirical design of transformations and pretext tasks. To avoid the\nsubjectivity associated with these methods, we propose the MVCNet, a novel\nunsupervised three dimensional (3D) representation learning method working in a\ntransformation-free manner. We view each 3D lesion from different orientations\nto collect multiple two dimensional (2D) views. Then, an embedding function is\nlearned by minimizing a contrastive loss so that the 2D views of the same 3D\nlesion are aggregated, and the 2D views of different lesions are separated. We\nevaluate the representations by training a simple classification head upon the\nembedding layer. Experimental results show that MVCNet achieves\nstate-of-the-art accuracies on the LIDC-IDRI (89.55%), LNDb (77.69%) and\nTianChi (79.96%) datasets for unsupervised representation learning. When\nfine-tuned on 10% of the labeled data, the accuracies are comparable to the\nsupervised learning model (89.46% vs. 85.03%, 73.85% vs. 73.44%, 83.56% vs.\n83.34% on the three datasets, respectively), indicating the superiority of\nMVCNet in learning representations with limited annotations. Code is released\nat: https://github.com/penghuazhai/MVCNet.",
          "link": "http://arxiv.org/abs/2108.07662",
          "publishedOn": "2021-08-18T01:55:00.116Z",
          "wordCount": 696,
          "title": "MVCNet: Multiview Contrastive Network for Unsupervised Representation Learning for 3D CT Lesions. (arXiv:2108.07662v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chomiak_K/0/1/0/all/0/1\">Krzysztof Chomiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miktus_M/0/1/0/all/0/1\">Micha&#x142; Miktus</a>",
          "description": "The paper introduces concepts of fairness and explainability (XAI) in\nartificial intelligence, oriented to solve a sophisticated business problems.\nFor fairness, the authors discuss the bias-inducing specifics, as well as\nrelevant mitigation methods, concluding with a set of recipes for introducing\nfairness in data-driven organizations. Additionally, for XAI, the authors audit\nspecific algorithms paired with demonstrational business use-cases, discuss a\nplethora of techniques of explanations quality quantification and provide an\noverview of future research avenues.",
          "link": "http://arxiv.org/abs/2108.07714",
          "publishedOn": "2021-08-18T01:55:00.094Z",
          "wordCount": 516,
          "title": "Harnessing value from data science in business: ensuring explainability and fairness of solutions. (arXiv:2108.07714v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07636",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Prado_E/0/1/0/all/0/1\">Estev&#xe3;o B. Prado</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Parnell_A/0/1/0/all/0/1\">Andrew C. Parnell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+McJames_N/0/1/0/all/0/1\">Nathan McJames</a>, <a href=\"http://arxiv.org/find/stat/1/au:+OShea_A/0/1/0/all/0/1\">Ann O&#x27;Shea</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moral_R/0/1/0/all/0/1\">Rafael A. Moral</a>",
          "description": "We propose a new semi-parametric model based on Bayesian Additive Regression\nTrees (BART). In our approach, the response variable is approximated by a\nlinear predictor and a BART model, where the first component is responsible for\nestimating the main effects and BART accounts for the non-specified\ninteractions and non-linearities. The novelty in our approach lies in the way\nwe change tree generation moves in BART to deal with confounding between the\nparametric and non-parametric components when they have covariates in common.\nThrough synthetic and real-world examples, we demonstrate that the performance\nof the new semi-parametric BART is competitive when compared to regression\nmodels and other tree-based methods. The implementation of the proposed method\nis available at https://github.com/ebprado/SP-BART.",
          "link": "http://arxiv.org/abs/2108.07636",
          "publishedOn": "2021-08-18T01:55:00.088Z",
          "wordCount": 553,
          "title": "Semi-parametric Bayesian Additive Regression Trees. (arXiv:2108.07636v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Helen Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1\">Cooper Raterink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Ivan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1\">Nicholas Frosst</a>",
          "description": "Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.",
          "link": "http://arxiv.org/abs/2108.07790",
          "publishedOn": "2021-08-18T01:55:00.067Z",
          "wordCount": 597,
          "title": "Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07749",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Lin_J/0/1/0/all/0/1\">Joshua Yao-Yu Lin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Pandya_S/0/1/0/all/0/1\">Sneh Pandya</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Pratap_D/0/1/0/all/0/1\">Devanshi Pratap</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kind_M/0/1/0/all/0/1\">Matias Carrasco Kind</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kindratenko_V/0/1/0/all/0/1\">Volodymyr Kindratenko</a>",
          "description": "Supermassive black holes (SMBHs) are ubiquitously found at the centers of\nmost massive galaxies. Measuring SMBH mass is important for understanding the\norigin and evolution of SMBHs. However, traditional methods require\nspectroscopic data which is expensive to gather. We present an algorithm that\nweighs SMBHs using quasar light time series, circumventing the need for\nexpensive spectra. We train, validate, and test neural networks that directly\nlearn from the Sloan Digital Sky Survey (SDSS) Stripe 82 light curves for a\nsample of $38,939$ spectroscopically confirmed quasars to map out the nonlinear\nencoding between SMBH mass and multi-color optical light curves. We find a\n1$\\sigma$ scatter of 0.37 dex between the predicted SMBH mass and the fiducial\nvirial mass estimate based on SDSS single-epoch spectra, which is comparable to\nthe systematic uncertainty in the virial mass estimate. Our results have direct\nimplications for more efficient applications with future observations from the\nVera C. Rubin Observatory. Our code, \\textsf{AGNet}, is publicly available at\n\n{\\color{red} \\url{https://github.com/snehjp2/AGNet}}.",
          "link": "http://arxiv.org/abs/2108.07749",
          "publishedOn": "2021-08-18T01:55:00.057Z",
          "wordCount": 622,
          "title": "AGNet: Weighing Black Holes with Deep Learning. (arXiv:2108.07749v1 [astro-ph.GA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuanchang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yubo Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hai Lin</a>",
          "description": "This paper proposes a novel model for predicting subgraphs in dynamic graphs,\nan extension of traditional link prediction. This proposed end-to-end model\nlearns a mapping from the subgraph structures in the current snapshot to the\nsubgraph structures in the next snapshot directly, i.e., edge existence among\nmultiple nodes in the subgraph. A new mechanism named cross-attention with a\ntwin-tower module is designed to integrate node attribute information and\ntopology information collaboratively for learning subgraph evolution. We\ncompare our model with several state-of-the-art methods for subgraph prediction\nand subgraph pattern prediction in multiple real-world homogeneous and\nheterogeneous dynamic graphs, respectively. Experimental results demonstrate\nthat our model outperforms other models in these two tasks, with a gain\nincrease from 5.02% to 10.88%.",
          "link": "http://arxiv.org/abs/2108.07776",
          "publishedOn": "2021-08-18T01:55:00.045Z",
          "wordCount": 563,
          "title": "SPAN: Subgraph Prediction Attention Network for Dynamic Graphs. (arXiv:2108.07776v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/1902.01687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukai_B/0/1/0/all/0/1\">Ben Boukai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1\">Zuofeng Shang</a>",
          "description": "Deep neural network is a state-of-art method in modern science and\ntechnology. Much statistical literature have been devoted to understanding its\nperformance in nonparametric estimation, whereas the results are suboptimal due\nto a redundant logarithmic sacrifice. In this paper, we show that such\nlog-factors are not necessary. We derive upper bounds for the $L^2$ minimax\nrisk in nonparametric estimation. Sufficient conditions on network\narchitectures are provided such that the upper bounds become optimal (without\nlog-sacrifice). Our proof relies on an explicitly constructed network estimator\nbased on tensor product B-splines. We also derive asymptotic distributions for\nthe constructed network and a relating hypothesis testing procedure. The\ntesting procedure is further proven as minimax optimal under suitable network\narchitectures.",
          "link": "http://arxiv.org/abs/1902.01687",
          "publishedOn": "2021-08-18T01:55:00.038Z",
          "wordCount": 576,
          "title": "Optimal Nonparametric Inference via Deep Neural Network. (arXiv:1902.01687v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jinyung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>",
          "description": "Fruit flies are established model systems for studying olfactory learning as\nthey will readily learn to associate odors with both electric shock or sugar\nrewards. The mechanisms of the insect brain apparently responsible for odor\nlearning form a relatively shallow neuronal architecture. Olfactory inputs are\nreceived by the antennal lobe (AL) of the brain, which produces an encoding of\neach odor mixture across ~50 sub-units known as glomeruli. Each of these\nglomeruli then project its component of this feature vector to several of ~2000\nso-called Kenyon Cells (KCs) in a region of the brain known as the mushroom\nbody (MB). Fly responses to odors are generated by small downstream neuropils\nthat decode the higher-order representation from the MB. Research has shown\nthat there is no recognizable pattern in the glomeruli--KC connections (and\nthus the particular higher-order representations); they are akin to\nfingerprints~-- even isogenic flies have different projections. Leveraging\ninsights from this architecture, we propose KCNet, a single-hidden-layer neural\nnetwork that contains sparse, randomized, binary weights between the input\nlayer and the hidden layer and analytically learned weights between the hidden\nlayer and the output layer. Furthermore, we also propose a dynamic optimization\nalgorithm that enables the KCNet to increase performance beyond its structural\nlimits by searching a more efficient set of inputs. For odorant-perception\ntasks that predict perceptual properties of an odorant, we show that KCNet\noutperforms existing data-driven approaches, such as XGBoost. For\nimage-classification tasks, KCNet achieves reasonable performance on benchmark\ndatasets (MNIST, Fashion-MNIST, and EMNIST) without any data-augmentation\nmethods or convolutional layers and shows particularly fast running time. Thus,\nneural networks inspired by the insect brain can be both economical and perform\nwell.",
          "link": "http://arxiv.org/abs/2108.07554",
          "publishedOn": "2021-08-18T01:55:00.029Z",
          "wordCount": 730,
          "title": "KCNet: An Insect-Inspired Single-Hidden-Layer Neural Network with Randomized Binary Weights for Prediction and Classification Tasks. (arXiv:2108.07554v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odena_A/0/1/0/all/0/1\">Augustus Odena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nye_M/0/1/0/all/0/1\">Maxwell Nye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_E/0/1/0/all/0/1\">Ellen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Carrie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>",
          "description": "This paper explores the limits of the current generation of large language\nmodels for program synthesis in general purpose programming languages. We\nevaluate a collection of such models (with between 244M and 137B parameters) on\ntwo new benchmarks, MBPP and MathQA-Python, in both the few-shot and\nfine-tuning regimes. Our benchmarks are designed to measure the ability of\nthese models to synthesize short Python programs from natural language\ndescriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974\nprogramming tasks, designed to be solvable by entry-level programmers. The\nMathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914\nproblems that evaluate the ability of the models to synthesize code from more\ncomplex text. On both datasets, we find that synthesis performance scales\nlog-linearly with model size. Our largest models, even without finetuning on a\ncode dataset, can synthesize solutions to 59.6 percent of the problems from\nMBPP using few-shot learning with a well-designed prompt. Fine-tuning on a\nheld-out portion of the dataset improves performance by about 10 percentage\npoints across most model sizes. On the MathQA-Python dataset, the largest\nfine-tuned model achieves 83.8 percent accuracy. Going further, we study the\nmodel's ability to engage in dialog about code, incorporating human feedback to\nimprove its solutions. We find that natural language feedback from a human\nhalves the error rate compared to the model's initial prediction. Additionally,\nwe conduct an error analysis to shed light on where these models fall short and\nwhat types of programs are most difficult to generate. Finally, we explore the\nsemantic grounding of these models by fine-tuning them to predict the results\nof program execution. We find that even our best models are generally unable to\npredict the output of a program given a specific input.",
          "link": "http://arxiv.org/abs/2108.07732",
          "publishedOn": "2021-08-18T01:55:00.022Z",
          "wordCount": 739,
          "title": "Program Synthesis with Large Language Models. (arXiv:2108.07732v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1\">Somjit Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baranwal_M/0/1/0/all/0/1\">Mayank Baranwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1\">Harshad Khadilkar</a>",
          "description": "Several real-world scenarios, such as remote control and sensing, are\ncomprised of action and observation delays. The presence of delays degrades the\nperformance of reinforcement learning (RL) algorithms, often to such an extent\nthat algorithms fail to learn anything substantial. This paper formally\ndescribes the notion of Markov Decision Processes (MDPs) with stochastic delays\nand shows that delayed MDPs can be transformed into equivalent standard MDPs\n(without delays) with significantly simplified cost structure. We employ this\nequivalence to derive a model-free Delay-Resolved RL framework and show that\neven a simple RL algorithm built upon this framework achieves near-optimal\nrewards in environments with stochastic delays in actions and observations. The\ndelay-resolved deep Q-network (DRDQN) algorithm is bench-marked on a variety of\nenvironments comprising of multi-step and stochastic delays and results in\nbetter performance, both in terms of achieving near-optimal rewards and\nminimizing the computational overhead thereof, with respect to the currently\nestablished algorithms.",
          "link": "http://arxiv.org/abs/2108.07555",
          "publishedOn": "2021-08-18T01:54:59.999Z",
          "wordCount": 606,
          "title": "Revisiting State Augmentation methods for Reinforcement Learning with Stochastic Delays. (arXiv:2108.07555v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07772",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Petratos_A/0/1/0/all/0/1\">Aidan Petratos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ting_A/0/1/0/all/0/1\">Allen Ting</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padmanabhan_S/0/1/0/all/0/1\">Shankar Padmanabhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1\">Kristina Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hageman_D/0/1/0/all/0/1\">Dylan Hageman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pisel_J/0/1/0/all/0/1\">Jesse R. Pisel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pyrcz_M/0/1/0/all/0/1\">Michael J. Pyrcz</a>",
          "description": "The placement of charging stations in areas with developing charging\ninfrastructure is a critical component of the future success of electric\nvehicles (EVs). In Albany County in New York, the expected rise in the EV\npopulation requires additional charging stations to maintain a sufficient level\nof efficiency across the charging infrastructure. A novel application of\nReinforcement Learning (RL) is able to find optimal locations for new charging\nstations given the predicted charging demand and current charging locations.\nThe most important factors that influence charging demand prediction include\nthe conterminous traffic density, EV registrations, and proximity to certain\ntypes of public buildings. The proposed RL framework can be refined and applied\nto cities across the world to optimize charging station placement.",
          "link": "http://arxiv.org/abs/2108.07772",
          "publishedOn": "2021-08-18T01:54:59.990Z",
          "wordCount": 581,
          "title": "Optimal Placement of Public Electric Vehicle Charging Stations Using Deep Reinforcement Learning. (arXiv:2108.07772v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifah_T/0/1/0/all/0/1\">Tariq Alkhalifah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovcharenko_O/0/1/0/all/0/1\">Oleg Ovcharenko</a>",
          "description": "We propose a direct domain adaptation (DDA) approach to enrich the training\nof supervised neural networks on synthetic data by features from real-world\ndata. The process involves a series of linear operations on the input features\nto the NN model, whether they are from the source or target domains, as\nfollows: 1) A cross-correlation of the input data (i.e. images) with a randomly\npicked sample pixel (or pixels) of all images from that domain or the mean of\nall randomly picked sample pixel (or pixels) of all images. 2) The convolution\nof the resulting data with the mean of the autocorrelated input images from the\nother domain. In the training stage, as expected, the input images are from the\nsource domain, and the mean of auto-correlated images are evaluated from the\ntarget domain. In the inference/application stage, the input images are from\nthe target domain, and the mean of auto-correlated images are evaluated from\nthe source domain. The proposed method only manipulates the data from the\nsource and target domains and does not explicitly interfere with the training\nworkflow and network architecture. An application that includes training a\nconvolutional neural network on the MNIST dataset and testing the network on\nthe MNIST-M dataset achieves a 70% accuracy on the test data. A principal\ncomponent analysis (PCA), as well as t-SNE, show that the input features from\nthe source and target domains, after the proposed direct transformations, share\nsimilar properties along with the principal components as compared to the\noriginal MNIST and MNIST-M input features.",
          "link": "http://arxiv.org/abs/2108.07600",
          "publishedOn": "2021-08-18T01:54:59.979Z",
          "wordCount": 691,
          "title": "Direct domain adaptation through reciprocal linear transformations. (arXiv:2108.07600v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>",
          "description": "The recent breakthroughs and prohibitive complexities of Deep Neural Networks\n(DNNs) have excited extensive interest in domain-specific DNN accelerators,\namong which optical DNN accelerators are particularly promising thanks to their\nunprecedented potential of achieving superior performance-per-watt. However,\nthe development of optical DNN accelerators is much slower than that of\nelectrical DNN accelerators. One key challenge is that while many techniques\nhave been developed to facilitate the development of electrical DNN\naccelerators, techniques that support or expedite optical DNN accelerator\ndesign remain much less explored, limiting both the achievable performance and\nthe innovation development of optical DNN accelerators. To this end, we develop\nthe first-of-its-kind framework dubbed O-HAS, which for the first time\ndemonstrates automated Optical Hardware Accelerator Search for boosting both\nthe acceleration efficiency and development speed of optical DNN accelerators.\nSpecifically, our O-HAS consists of two integrated enablers: (1) an O-Cost\nPredictor, which can accurately yet efficiently predict an optical\naccelerator's energy and latency based on the DNN model parameters and the\noptical accelerator design; and (2) an O-Search Engine, which can automatically\nexplore the large design space of optical DNN accelerators and identify the\noptimal accelerators (i.e., the micro-architectures and\nalgorithm-to-accelerator mapping methods) in order to maximize the target\nacceleration efficiency. Extensive experiments and ablation studies\nconsistently validate the effectiveness of both our O-Cost Predictor and\nO-Search Engine as well as the excellent efficiency of O-HAS generated optical\naccelerators.",
          "link": "http://arxiv.org/abs/2108.07538",
          "publishedOn": "2021-08-18T01:54:59.970Z",
          "wordCount": 686,
          "title": "O-HAS: Optical Hardware Accelerator Search for Boosting Both Acceleration Performance and Development Speed. (arXiv:2108.07538v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kehinde_A/0/1/0/all/0/1\">Adeniyi Jide Kehinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeniyi_A/0/1/0/all/0/1\">Abidemi Emmanuel Adeniyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundokun_R/0/1/0/all/0/1\">Roseline Oluwaseun Ogundokun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_S/0/1/0/all/0/1\">Sanjay Misra</a>",
          "description": "Many researchers have studied student academic performance in supervised and\nunsupervised learning using numerous data mining techniques. Neural networks\noften need a greater collection of observations to achieve enough predictive\nability. Due to the increase in the rate of poor graduates, it is necessary to\ndesign a system that helps to reduce this menace as well as reduce the\nincidence of students having to repeat due to poor performance or having to\ndrop out of school altogether in the middle of the pursuit of their career. It\nis therefore necessary to study each one as well as their advantages and\ndisadvantages, so as to determine which is more efficient in and in what case\none should be preferred over the other. The study aims to develop a system to\npredict student performance with Artificial Neutral Network using the student\ndemographic traits so as to assist the university in selecting candidates\n(students) with a high prediction of success for admission using previous\nacademic records of students granted admissions which will eventually lead to\nquality graduates of the institution. The model was developed based on certain\nselected variables as the input. It achieved an accuracy of over 92.3 percent,\nshowing Artificial Neural Network potential effectiveness as a predictive tool\nand a selection criterion for candidates seeking admission to a university.",
          "link": "http://arxiv.org/abs/2108.07717",
          "publishedOn": "2021-08-18T01:54:59.962Z",
          "wordCount": 684,
          "title": "Prediction of Students performance with Artificial Neural Network using Demographic Traits. (arXiv:2108.07717v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_L/0/1/0/all/0/1\">Leonardo Enzo Brito da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayapati_N/0/1/0/all/0/1\">Nagasharath Rayapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wunsch_D/0/1/0/all/0/1\">Donald C. Wunsch II</a>",
          "description": "In streaming data applications incoming samples are processed and discarded,\ntherefore, intelligent decision-making is crucial for the performance of\nlifelong learning systems. In addition, the order in which samples arrive may\nheavily affect the performance of online (and offline) incremental learners.\nThe recently introduced incremental cluster validity indices (iCVIs) provide\nvaluable aid in addressing such class of problems. Their primary use-case has\nbeen cluster quality monitoring; nonetheless, they have been very recently\nintegrated in a streaming clustering method to assist the clustering task\nitself. In this context, the work presented here introduces the first adaptive\nresonance theory (ART)-based model that uses iCVIs for unsupervised and\nsemi-supervised online learning. Moreover, it shows for the first time how to\nuse iCVIs to regulate ART vigilance via an iCVI-based match tracking mechanism.\nThe model achieves improved accuracy and robustness to ordering effects by\nintegrating an online iCVI framework as module B of a topological adaptive\nresonance theory predictive mapping (TopoARTMAP) -- thereby being named\niCVI-TopoARTMAP -- and by employing iCVI-driven post-processing heuristics at\nthe end of each learning step. The online iCVI framework provides assignments\nof input samples to clusters at each iteration in accordance to any of several\niCVIs. The iCVI-TopoARTMAP maintains useful properties shared by ARTMAP models,\nsuch as stability, immunity to catastrophic forgetting, and the many-to-one\nmapping capability via the map field module. The performance (unsupervised and\nsemi-supervised) and robustness to presentation order (unsupervised) of\niCVI-TopoARTMAP were evaluated via experiments with a synthetic data set and\ndeep embeddings of a real-world face image data set.",
          "link": "http://arxiv.org/abs/2108.07743",
          "publishedOn": "2021-08-18T01:54:59.936Z",
          "wordCount": 701,
          "title": "Incremental cluster validity index-guided online learning for performance and robustness to presentation order. (arXiv:2108.07743v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>",
          "description": "Machine Learning (ML) is about computational methods that enable machines to\nlearn concepts from experiences. In handling a wide variety of experiences\nranging from data instances, knowledge, constraints, to rewards, adversaries,\nand lifelong interplay in an ever-growing spectrum of tasks, contemporary ML/AI\nresearch has resulted in a multitude of learning paradigms and methodologies.\nDespite the continual progresses on all different fronts, the disparate\nnarrowly-focused methods also make standardized, composable, and reusable\ndevelopment of learning solutions difficult, and make it costly if possible to\nbuild AI agents that panoramically learn from all types of experiences. This\npaper presents a standardized ML formalism, in particular a standard equation\nof the learning objective, that offers a unifying understanding of diverse ML\nalgorithms, making them special cases due to different choices of modeling\ncomponents. The framework also provides guidance for mechanic design of new ML\nsolutions, and serves as a promising vehicle towards panoramic learning with\nall experiences.",
          "link": "http://arxiv.org/abs/2108.07783",
          "publishedOn": "2021-08-18T01:54:59.905Z",
          "wordCount": 583,
          "title": "Panoramic Learning with A Standardized Machine Learning Formalism. (arXiv:2108.07783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosenfeld_J/0/1/0/all/0/1\">Jonathan S. Rosenfeld</a>",
          "description": "Running faster will only get you so far -- it is generally advisable to first\nunderstand where the roads lead, then get a car ...\n\nThe renaissance of machine learning (ML) and deep learning (DL) over the last\ndecade is accompanied by an unscalable computational cost, limiting its\nadvancement and weighing on the field in practice. In this thesis we take a\nsystematic approach to address the algorithmic and methodological limitations\nat the root of these costs. We first demonstrate that DL training and pruning\nare predictable and governed by scaling laws -- for state of the art models and\ntasks, spanning image classification and language modeling, as well as for\nstate of the art model compression via iterative pruning. Predictability, via\nthe establishment of these scaling laws, provides the path for principled\ndesign and trade-off reasoning, currently largely lacking in the field. We then\ncontinue to analyze the sources of the scaling laws, offering an\napproximation-theoretic view and showing through the exploration of a noiseless\nrealizable case that DL is in fact dominated by error sources very far from the\nlower error limit. We conclude by building on the gained theoretical\nunderstanding of the scaling laws' origins. We present a conjectural path to\neliminate one of the current dominant error sources -- through a data bandwidth\nlimiting hypothesis and the introduction of Nyquist learners -- which can, in\nprinciple, reach the generalization error lower limit (e.g. 0 in the noiseless\ncase), at finite dataset size.",
          "link": "http://arxiv.org/abs/2108.07686",
          "publishedOn": "2021-08-18T01:54:59.898Z",
          "wordCount": 668,
          "title": "Scaling Laws for Deep Learning. (arXiv:2108.07686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Glimsdal_S/0/1/0/all/0/1\">Sondre Glimsdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1\">Ole-Christoffer Granmo</a>",
          "description": "Using finite-state machines to learn patterns, Tsetlin machines (TMs) have\nobtained competitive accuracy and learning speed across several benchmarks,\nwith frugal memory- and energy footprint. A TM represents patterns as\nconjunctive clauses in propositional logic (AND-rules), each clause voting for\nor against a particular output. While efficient for single-output problems, one\nneeds a separate TM per output for multi-output problems. Employing multiple\nTMs hinders pattern reuse because each TM then operates in a silo. In this\npaper, we introduce clause sharing, merging multiple TMs into a single one.\nEach clause is related to each output by using a weight. A positive weight\nmakes the clause vote for output $1$, while a negative weight makes the clause\nvote for output $0$. The clauses thus coalesce to produce multiple outputs. The\nresulting coalesced Tsetlin Machine (CoTM) simultaneously learns both the\nweights and the composition of each clause by employing interacting Stochastic\nSearching on the Line (SSL) and Tsetlin Automata (TA) teams. Our empirical\nresults on MNIST, Fashion-MNIST, and Kuzushiji-MNIST show that CoTM obtains\nsignificantly higher accuracy than TM on $50$- to $1$K-clause configurations,\nindicating an ability to repurpose clauses. E.g., accuracy goes from $71.99$%\nto $89.66$% on Fashion-MNIST when employing $50$ clauses per class (22 Kb\nmemory). While TM and CoTM accuracy is similar when using more than $1$K\nclauses per class, CoTM reaches peak accuracy $3\\times$ faster on MNIST with\n$8$K clauses. We further investigate robustness towards imbalanced training\ndata. Our evaluations on imbalanced versions of IMDb- and CIFAR10 data show\nthat CoTM is robust towards high degrees of class imbalance. Being able to\nshare clauses, we believe CoTM will enable new TM application domains that\ninvolve multiple outputs, such as learning language models and auto-encoding.",
          "link": "http://arxiv.org/abs/2108.07594",
          "publishedOn": "2021-08-18T01:54:59.890Z",
          "wordCount": 718,
          "title": "Coalesced Multi-Output Tsetlin Machines with Clause Sharing. (arXiv:2108.07594v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Ajay Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkountouna_O/0/1/0/all/0/1\">Olga Gkountouna</a>",
          "description": "We present a demonstration of REACT, a new Real-time Educational AI-powered\nClassroom Tool that employs EDM techniques for supporting the decision-making\nprocess of educators. REACT is a data-driven tool with a user-friendly\ngraphical interface. It analyzes students' performance data and provides\ncontext-based alerts as well as recommendations to educators for course\nplanning. Furthermore, it incorporates model-agnostic explanations for bringing\nexplainability and interpretability in the process of decision making. This\npaper demonstrates a use case scenario of our proposed tool using a real-world\ndataset and presents the design of its architecture and user interface. This\ndemonstration focuses on the agglomerative clustering of students based on\ntheir performance (i.e., incorrect responses and hints used) during an in-class\nactivity. This formation of clusters of students with similar strengths and\nweaknesses may help educators to improve their course planning by identifying\nat-risk students, forming study groups, or encouraging tutoring between\nstudents of different strengths.",
          "link": "http://arxiv.org/abs/2108.07693",
          "publishedOn": "2021-08-18T01:54:59.883Z",
          "wordCount": 596,
          "title": "Demonstrating REACT: a Real-time Educational AI-powered Classroom Tool. (arXiv:2108.07693v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Y/0/1/0/all/0/1\">Yanli Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oesterle_J/0/1/0/all/0/1\">Jonathan Oesterle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Euler_T/0/1/0/all/0/1\">Thomas Euler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berens_P/0/1/0/all/0/1\">Philipp Berens</a>",
          "description": "Spatio-temporal receptive field (STRF) models are frequently used to\napproximate the computation implemented by a sensory neuron. Typically, such\nSTRFs are assumed to be smooth and sparse. Current state-of-the-art approaches\nfor estimating STRFs based on empirical Bayes are often not computationally\nefficient in high-dimensional settings, as encountered in sensory neuroscience.\nHere we pursued an alternative approach and encode prior knowledge for\nestimation of STRFs by choosing a set of basis functions with the desired\nproperties: natural cubic splines. Our method is computationally efficient and\ncan be easily applied to a wide range of existing models. We compared the\nperformance of spline-based methods to non-spline ones on simulated and\nexperimental data, showing that spline-based methods consistently outperform\nthe non-spline versions.",
          "link": "http://arxiv.org/abs/2108.07537",
          "publishedOn": "2021-08-18T01:54:59.861Z",
          "wordCount": 558,
          "title": "Estimating smooth and sparse neural receptive fields with a flexible spline basis. (arXiv:2108.07537v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yifei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongji Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caihua Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1\">Khaled B. Letaief</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>",
          "description": "Graph convolutional networks (GCNs) have recently enabled a popular class of\nalgorithms for collaborative filtering (CF). Nevertheless, the theoretical\nunderpinnings of their empirical successes remain elusive. In this paper, we\nendeavor to obtain a better understanding of GCN-based CF methods via the lens\nof graph signal processing. By identifying the critical role of smoothness, a\nkey concept in graph signal processing, we develop a unified graph\nconvolution-based framework for CF. We prove that many existing CF methods are\nspecial cases of this framework, including the neighborhood-based methods,\nlow-rank matrix factorization, linear auto-encoders, and LightGCN,\ncorresponding to different low-pass filters. Based on our framework, we then\npresent a simple and computationally efficient CF baseline, which we shall\nrefer to as Graph Filter based Collaborative Filtering (GF-CF). Given an\nimplicit feedback matrix, GF-CF can be obtained in a closed form instead of\nexpensive training with back-propagation. Experiments will show that GF-CF\nachieves competitive or better performance against deep learning-based methods\non three well-known datasets, notably with a $70\\%$ performance gain over\nLightGCN on the Amazon-book dataset.",
          "link": "http://arxiv.org/abs/2108.07567",
          "publishedOn": "2021-08-18T01:54:59.852Z",
          "wordCount": 619,
          "title": "How Powerful is Graph Convolution for Recommendation?. (arXiv:2108.07567v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaokang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaibo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1\">Evgeny Kharlamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Graph-based anomaly detection has been widely used for detecting malicious\nactivities in real-world applications. Existing attempts to address this\nproblem have thus far focused on structural feature engineering or learning in\nthe binary classification regime. In this work, we propose to leverage graph\ncontrastive coding and present the supervised GCCAD model for contrasting\nabnormal nodes with normal ones in terms of their distances to the global\ncontext (e.g., the average of all nodes). To handle scenarios with scarce\nlabels, we further enable GCCAD as a self-supervised framework by designing a\ngraph corrupting strategy for generating synthetic node labels. To achieve the\ncontrastive objective, we design a graph neural network encoder that can infer\nand further remove suspicious links during message passing, as well as learn\nthe global context of the input graph. We conduct extensive experiments on four\npublic datasets, demonstrating that 1) GCCAD significantly and consistently\noutperforms various advanced baselines and 2) its self-supervised version\nwithout fine-tuning can achieve comparable performance with its fully\nsupervised version.",
          "link": "http://arxiv.org/abs/2108.07516",
          "publishedOn": "2021-08-18T01:54:59.845Z",
          "wordCount": 610,
          "title": "GCCAD: Graph Contrastive Coding for Anomaly Detection. (arXiv:2108.07516v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>",
          "description": "It's challenging to customize transducer-based automatic speech recognition\n(ASR) system with context information which is dynamic and unavailable during\nmodel training. In this work, we introduce a light-weight contextual spelling\ncorrection model to correct context-related recognition errors in\ntransducer-based ASR systems. We incorporate the context information into the\nspelling correction model with a shared context encoder and use a filtering\nalgorithm to handle large-size context lists. Experiments show that the model\nimproves baseline ASR model performance with about 50% relative word error rate\nreduction, which also significantly outperforms the baseline method such as\ncontextual LM biasing. The model also shows excellent performance for\nout-of-vocabulary terms not seen during training.",
          "link": "http://arxiv.org/abs/2108.07493",
          "publishedOn": "2021-08-18T01:54:59.838Z",
          "wordCount": 566,
          "title": "A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems. (arXiv:2108.07493v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Feng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_A/0/1/0/all/0/1\">Ajith Kumar V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanci Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qikui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ansi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makwana_D/0/1/0/all/0/1\">Dhruv Makwana</a>",
          "description": "To further improve the performance and the self-learning ability of GCNs, in\nthis paper, we propose an efficient self-supervised learning strategy of GCNs,\nnamed randomly removed links with a fixed step at one region (RRLFSOR). In\naddition, we also propose another self-supervised learning strategy of GCNs,\nnamed randomly removing links with a fixed step at some blocks (RRLFSSB), to\nsolve the problem that adjacent nodes have no selected step. Experiments on\ntransductive link prediction tasks show that our strategies outperform the\nbaseline models consistently by up to 21.34% in terms of accuracy on three\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2108.07481",
          "publishedOn": "2021-08-18T01:54:59.830Z",
          "wordCount": 538,
          "title": "RRLFSOR: An Efficient Self-Supervised Learning Strategy of Graph Convolutional Networks. (arXiv:2108.07481v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Puyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yunwen Lei</a>",
          "description": "Randomized coordinate descent (RCD) is a popular optimization algorithm with\nwide applications in solving various machine learning problems, which motivates\na lot of theoretical analysis on its convergence behavior. As a comparison,\nthere is no work studying how the models trained by RCD would generalize to\ntest examples. In this paper, we initialize the generalization analysis of RCD\nby leveraging the powerful tool of algorithmic stability. We establish argument\nstability bounds of RCD for both convex and strongly convex objectives, from\nwhich we develop optimal generalization bounds by showing how to early-stop the\nalgorithm to tradeoff the estimation and optimization. Our analysis shows that\nRCD enjoys better stability as compared to stochastic gradient descent.",
          "link": "http://arxiv.org/abs/2108.07414",
          "publishedOn": "2021-08-18T01:54:59.803Z",
          "wordCount": 544,
          "title": "Stability and Generalization for Randomized Coordinate Descent. (arXiv:2108.07414v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongyoon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghwan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Successful sequential recommendation systems rely on accurately capturing the\nuser's short-term and long-term interest. Although Transformer-based models\nachieved state-of-the-art performance in the sequential recommendation task,\nthey generally require quadratic memory and time complexity to the sequence\nlength, making it difficult to extract the long-term interest of users. On the\nother hand, Multi-Layer Perceptrons (MLP)-based models, renowned for their\nlinear memory and time complexity, have recently shown competitive results\ncompared to Transformer in various tasks. Given the availability of a massive\namount of the user's behavior history, the linear memory and time complexity of\nMLP-based models make them a promising alternative to explore in the sequential\nrecommendation task. To this end, we adopted MLP-based models in sequential\nrecommendation but consistently observed that MLP-based methods obtain lower\nperformance than those of Transformer despite their computational benefits.\nFrom experiments, we observed that introducing explicit high-order interactions\nto MLP layers mitigates such performance gap. In response, we propose the\nMulti-Order Interaction (MOI) layer, which is capable of expressing an\narbitrary order of interactions within the inputs while maintaining the memory\nand time complexity of the MLP layer. By replacing the MLP layer with the MOI\nlayer, our model was able to achieve comparable performance with\nTransformer-based models while retaining the MLP-based models' computational\nbenefits.",
          "link": "http://arxiv.org/abs/2108.07505",
          "publishedOn": "2021-08-18T01:54:59.797Z",
          "wordCount": 658,
          "title": "MOI-Mixer: Improving MLP-Mixer with Multi Order Interactions in Sequential Recommendation. (arXiv:2108.07505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jessie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_B/0/1/0/all/0/1\">Blanca Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_S/0/1/0/all/0/1\">Sebastiano Barbieri</a>",
          "description": "In this study we propose the Learning to Defer with Uncertainty (LDU)\nalgorithm, an approach which considers the model's predictive uncertainty when\nidentifying the patient group to be evaluated by human experts. Our aim is to\nensure patient safety when ML models are deployed in healthcare settings.",
          "link": "http://arxiv.org/abs/2108.07392",
          "publishedOn": "2021-08-18T01:54:59.784Z",
          "wordCount": 484,
          "title": "Incorporating Uncertainty in Learning to Defer Algorithms for Safe Computer-Aided Diagnosis. (arXiv:2108.07392v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sitaula_C/0/1/0/all/0/1\">Chiranjibi Sitaula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jinyuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadarshi_A/0/1/0/all/0/1\">Archana Priyadarshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tracy_M/0/1/0/all/0/1\">Mark Tracy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavehei_O/0/1/0/all/0/1\">Omid Kavehei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinder_M/0/1/0/all/0/1\">Murray Hinder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Withana_A/0/1/0/all/0/1\">Anusha Withana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McEwan_A/0/1/0/all/0/1\">Alistair McEwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzbanrad_F/0/1/0/all/0/1\">Faezeh Marzbanrad</a>",
          "description": "Abdominal auscultation is a convenient, safe and inexpensive method to assess\nbowel conditions, which is essential in neonatal care. It helps early detection\nof neonatal bowel dysfunctions and allows timely intervention. This paper\npresents a neonatal bowel sound detection method to assist the auscultation.\nSpecifically, a Convolutional Neural Network (CNN) is proposed to classify\nperistalsis and non-peristalsis sounds. The classification is then optimized\nusing a Laplace Hidden Semi-Markov Model (HSMM). The proposed method is\nvalidated on abdominal sounds from 49 newborn infants admitted to our tertiary\nNeonatal Intensive Care Unit (NICU). The results show that the method can\neffectively detect bowel sounds with accuracy and area under curve (AUC) score\nbeing 89.81% and 83.96% respectively, outperforming 13 baseline methods.\nFurthermore, the proposed Laplace HSMM refinement strategy is proven capable to\nenhance other bowel sound detection models. The outcomes of this work have the\npotential to facilitate future telehealth applications for neonatal care. The\nsource code of our work can be found at:\nhttps://bitbucket.org/chirudeakin/neonatal-bowel-sound-classification/",
          "link": "http://arxiv.org/abs/2108.07467",
          "publishedOn": "2021-08-18T01:54:59.742Z",
          "wordCount": 637,
          "title": "Neonatal Bowel Sound Detection Using Convolutional Neural Network and Laplace Hidden Semi-Markov Model. (arXiv:2108.07467v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen-Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>",
          "description": "While information delivery in industrial Internet of things demands\nreliability and latency guarantees, the freshness of the controller's available\ninformation, measured by the age of information (AoI), is paramount for\nhigh-performing industrial automation. The problem in this work is cast as a\nsensor's transmit power minimization subject to the peak-AoI requirement and a\nprobabilistic constraint on queuing latency. We further characterize the tail\nbehavior of the latency by a generalized Pareto distribution (GPD) for solving\nthe power allocation problem through Lyapunov optimization. As each sensor\nutilizes its own data to locally train the GPD model, we incorporate federated\nlearning and propose a local-model selection approach which accounts for\ncorrelation among the sensor's training data. Numerical results show the\ntradeoff between the transmit power, peak AoI, and delay's tail distribution.\nFurthermore, we verify the superiority of the proposed correlation-aware\napproach for selecting the local models in federated learning over an existing\nbaseline.",
          "link": "http://arxiv.org/abs/2108.07504",
          "publishedOn": "2021-08-18T01:54:59.732Z",
          "wordCount": 605,
          "title": "Federated Learning with Correlated Data: Taming the Tail for Age-Optimal Industrial IoT. (arXiv:2108.07504v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Angelakis_A/0/1/0/all/0/1\">A. Angelakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulioti_I/0/1/0/all/0/1\">I. Soulioti</a>",
          "description": "We train a machine learning model on a dataset of 2177 individuals using as\nfeatures 26 probe sets and their age in order to classify if someone has acute\nmyeloid leukaemia or is healthy. The dataset is multicentric and consists of\ndata from 27 organisations, 25 cities, 15 countries and 4 continents. The\naccuracy or our model is 99.94\\% and its F1-score 0.9996. To the best of our\nknowledge the performance of our model is the best one in the literature, as\nregards the prediction of AML using similar or not data. Moreover, there has\nnot been any bibliographic reference associated with acute myeloid leukaemia\nfor the 26 probe sets we used as features in our model.",
          "link": "http://arxiv.org/abs/2108.07396",
          "publishedOn": "2021-08-18T01:54:59.695Z",
          "wordCount": 547,
          "title": "Diagnosis of Acute Myeloid Leukaemia Using Machine Learning. (arXiv:2108.07396v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhijian Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaotie Deng</a>",
          "description": "In this paper, we propose a general meta learning approach to computing\napproximate Nash equilibrium for finite $n$-player normal-form games. Unlike\nexisting solutions that approximate or learn a Nash equilibrium from scratch\nfor each of the games, our meta solver directly constructs a mapping from a\ngame utility matrix to a joint strategy profile. The mapping is parameterized\nand learned in a self-supervised fashion by a proposed Nash equilibrium\napproximation metric without ground truth data informing any Nash equilibrium.\nAs such, it can immediately predict the joint strategy profile that\napproximates a Nash equilibrium for any unseen new game under the same game\ndistribution. Moreover, the meta-solver can be further fine-tuned and adaptive\nto a new game if iteration updates are allowed. We theoretically prove that our\nmeta-solver is not affected by the non-smoothness of exact Nash equilibrium\nsolutions, and derive a sample complexity bound to demonstrate its\ngeneralization ability across normal-form games. Experimental results\ndemonstrate its substantial approximation power against other strong baselines\nin both adaptive and non-adaptive cases.",
          "link": "http://arxiv.org/abs/2108.07472",
          "publishedOn": "2021-08-18T01:54:59.657Z",
          "wordCount": 616,
          "title": "Learning to Compute Approximate Nash Equilibrium for Normal-form Games. (arXiv:2108.07472v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Didong Li</a>",
          "description": "Over a complete Riemannian manifold of finite dimension, Greene and Wu\nintroduced a convolution, known as Greene-Wu (GW) convolution. In this paper,\nwe introduce a reformulation of the GW convolution. Using our reformulation,\nmany properties of the GW convolution can be easily derived, including a new\nformula for how the curvature of the space would affect the curvature of the\nfunction through the GW convolution. Also enabled by our new reformulation, an\nimproved method for gradient estimation over Riemannian manifolds is\nintroduced. Theoretically, our gradient estimation method improves the order of\nestimation error from $O \\left( \\left( n + 3 \\right)^{3/2} \\right)$ to $O\n\\left( n^{3/2} \\right)$, where $n$ is the dimension of the manifold.\nEmpirically, our method outperforms the best existing method for gradient\nestimation over Riemannian manifolds, as evidenced by thorough experimental\nevaluations.",
          "link": "http://arxiv.org/abs/2108.07406",
          "publishedOn": "2021-08-18T01:54:59.577Z",
          "wordCount": 578,
          "title": "From the Greene--Wu Convolution to Gradient Estimation over Riemannian Manifolds. (arXiv:2108.07406v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaochao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaolin Zheng</a>",
          "description": "With the increasing demands for privacy protection, privacy-preserving\nmachine learning has been drawing much attention in both academia and industry.\nHowever, most existing methods have their limitations in practical\napplications. On the one hand, although most cryptographic methods are provable\nsecure, they bring heavy computation and communication. On the other hand, the\nsecurity of many relatively efficient private methods (e.g., federated learning\nand split learning) is being questioned, since they are non-provable secure.\nInspired by previous work on privacy-preserving machine learning, we build a\nprivacy-preserving machine learning framework by combining random permutation\nand arithmetic secret sharing via our compute-after-permutation technique.\nSince our method reduces the cost for element-wise function computation, it is\nmore efficient than existing cryptographic methods. Moreover, by adopting\ndistance correlation as a metric for privacy leakage, we demonstrate that our\nmethod is more secure than previous non-provable secure methods. Overall, our\nproposal achieves a good balance between security and efficiency. Experimental\nresults show that our method not only is up to 6x faster and reduces up to 85%\nnetwork traffic compared with state-of-the-art cryptographic methods, but also\nleaks less privacy during the training process compared with non-provable\nsecure methods.",
          "link": "http://arxiv.org/abs/2108.07463",
          "publishedOn": "2021-08-18T01:54:59.550Z",
          "wordCount": 632,
          "title": "Towards Secure and Practical Machine Learning via Secret Sharing and Random Permutation. (arXiv:2108.07463v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yijia Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chang-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Protein is linked to almost every life process. Therefore, analyzing the\nbiological structure and property of protein sequences is critical to the\nexploration of life, as well as disease detection and drug discovery.\nTraditional protein analysis methods tend to be labor-intensive and\ntime-consuming. The emergence of deep learning models makes modeling data\npatterns in large quantities of data possible. Interdisciplinary researchers\nhave begun to leverage deep learning methods to model large biological\ndatasets, e.g. using long short-term memory and convolutional neural network\nfor protein sequence classification. After millions of years of evolution,\nevolutionary information is encoded in protein sequences. Inspired by the\nsimilarity between natural language and protein sequences, we use large-scale\nlanguage models to model evolutionary-scale protein sequences, encoding protein\nbiology information in representation. Significant improvements are observed in\nboth token-level and sequence-level tasks, demonstrating that our large-scale\nmodel can accurately capture evolution information from pretraining on\nevolutionary-scale individual sequences. Our code and model are available at\nhttps://github.com/THUDM/ProteinLM.",
          "link": "http://arxiv.org/abs/2108.07435",
          "publishedOn": "2021-08-18T01:54:59.531Z",
          "wordCount": 615,
          "title": "Modeling Protein Using Large-scale Pretrain Language Model. (arXiv:2108.07435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yu Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1\">Liang Lan</a>",
          "description": "Factorization Machines (FM), a general predictor that can efficiently model\nfeature interactions in linear time, was primarily proposed for collaborative\nrecommendation and have been broadly used for regression, classification and\nranking tasks. Subspace Encoding Factorization Machine (SEFM) has been proposed\nrecently to overcome the expressiveness limitation of Factorization Machines\n(FM) by applying explicit nonlinear feature mapping for both individual\nfeatures and feature interactions through one-hot encoding to each input\nfeature. Despite the effectiveness of SEFM, it increases the memory cost of FM\nby $b$ times, where $b$ is the number of bins when applying one-hot encoding on\neach input feature. To reduce the memory cost of SEFM, we propose a new method\ncalled Binarized FM which constraints the model parameters to be binary values\n(i.e., 1 or $-1$). Then each parameter value can be efficiently stored in one\nbit. Our proposed method can significantly reduce the memory cost of SEFM\nmodel. In addition, we propose a new algorithm to effectively and efficiently\nlearn proposed FM with binary constraints using Straight Through Estimator\n(STE) with Adaptive Gradient Descent (Adagrad). Finally, we evaluate the\nperformance of our proposed method on eight different classification datasets.\nOur experimental results have demonstrated that our proposed method achieves\ncomparable accuracy with SEFM but with much less memory cost.",
          "link": "http://arxiv.org/abs/2108.07421",
          "publishedOn": "2021-08-18T01:54:59.491Z",
          "wordCount": 642,
          "title": "Memory-Efficient Factorization Machines via Binarizing both Data and Model Coefficients. (arXiv:2108.07421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duta_I/0/1/0/all/0/1\">Ionut Cosmin Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "We propose contextual convolution (CoConv) for visual recognition. CoConv is\na direct replacement of the standard convolution, which is the core component\nof convolutional neural networks. CoConv is implicitly equipped with the\ncapability of incorporating contextual information while maintaining a similar\nnumber of parameters and computational cost compared to the standard\nconvolution. CoConv is inspired by neuroscience studies indicating that (i)\nneurons, even from the primary visual cortex (V1 area), are involved in\ndetection of contextual cues and that (ii) the activity of a visual neuron can\nbe influenced by the stimuli placed entirely outside of its theoretical\nreceptive field. On the one hand, we integrate CoConv in the widely-used\nresidual networks and show improved recognition performance over baselines on\nthe core tasks and benchmarks for visual recognition, namely image\nclassification on the ImageNet data set and object detection on the MS COCO\ndata set. On the other hand, we introduce CoConv in the generator of a\nstate-of-the-art Generative Adversarial Network, showing improved generative\nresults on CIFAR-10 and CelebA. Our code is available at\nhttps://github.com/iduta/coconv.",
          "link": "http://arxiv.org/abs/2108.07387",
          "publishedOn": "2021-08-18T01:54:59.481Z",
          "wordCount": 621,
          "title": "Contextual Convolutional Neural Networks. (arXiv:2108.07387v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pastor_E/0/1/0/all/0/1\">Eliana Pastor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfaro_L/0/1/0/all/0/1\">Luca de Alfaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baralis_E/0/1/0/all/0/1\">Elena Baralis</a>",
          "description": "When analyzing the behavior of machine learning algorithms, it is important\nto identify specific data subgroups for which the considered algorithm shows\ndifferent performance with respect to the entire dataset. The intervention of\ndomain experts is normally required to identify relevant attributes that define\nthese subgroups.\n\nWe introduce the notion of divergence to measure this performance difference\nand we exploit it in the context of (i) classification models and (ii) ranking\napplications to automatically detect data subgroups showing a significant\ndeviation in their behavior. Furthermore, we quantify the contribution of all\nattributes in the data subgroup to the divergent behavior by means of Shapley\nvalues, thus allowing the identification of the most impacting attributes.",
          "link": "http://arxiv.org/abs/2108.07450",
          "publishedOn": "2021-08-18T01:54:59.474Z",
          "wordCount": 564,
          "title": "Identifying Biased Subgroups in Ranking and Classification. (arXiv:2108.07450v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+AlQuabeh_H/0/1/0/all/0/1\">Hilal AlQuabeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawazeer_A/0/1/0/all/0/1\">Ameera Bawazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhashmi_A/0/1/0/all/0/1\">Abdulateef Alhashmi</a>",
          "description": "Data labeling in supervised learning is considered an expensive and\ninfeasible tool in some conditions. The self-supervised learning method is\nproposed to tackle the learning effectiveness with fewer labeled data, however,\nthere is a lack of confidence in the size of labeled data needed to achieve\nadequate results. This study aims to draw a baseline on the proportion of the\nlabeled data that models can appreciate to yield competent accuracy when\ncompared to training with additional labels. The study implements the\nkaggle.com' cats-vs-dogs dataset, Mnist and Fashion-Mnist to investigate the\nself-supervised learning task by implementing random rotations augmentation on\nthe original datasets. To reveal the true effectiveness of the pretext process\nin self-supervised learning, the original dataset is divided into smaller\nbatches, and learning is repeated on each batch with and without the pretext\npre-training. Results show that the pretext process in the self-supervised\nlearning improves the accuracy around 15% in the downstream classification task\nwhen compared to the plain supervised learning.",
          "link": "http://arxiv.org/abs/2108.07464",
          "publishedOn": "2021-08-18T01:54:59.466Z",
          "wordCount": 613,
          "title": "Investigating a Baseline Of Self Supervised Learning Towards Reducing Labeling Costs For Image Classification. (arXiv:2108.07464v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Ye Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1\">Diego Klabjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>",
          "description": "Federated learning is a distributed machine learning paradigm where multiple\ndata owners (clients) collaboratively train one machine learning model while\nkeeping data on their own devices. The heterogeneity of client datasets is one\nof the most important challenges of federated learning algorithms. Studies have\nfound performance reduction with standard federated algorithms, such as FedAvg,\non non-IID data. Many existing works on handling non-IID data adopt the same\naggregation framework as FedAvg and focus on improving model updates either on\nthe server side or on clients. In this work, we tackle this challenge in a\ndifferent view by introducing redistribution rounds that delay the aggregation.\nWe perform experiments on multiple tasks and show that the proposed framework\nsignificantly improves the performance on non-IID data.",
          "link": "http://arxiv.org/abs/2108.07433",
          "publishedOn": "2021-08-18T01:54:59.457Z",
          "wordCount": 550,
          "title": "Aggregation Delayed Federated Learning. (arXiv:2108.07433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07339",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fredieu_C/0/1/0/all/0/1\">C. Tanner Fredieu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bui_J/0/1/0/all/0/1\">Justin Bui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martone_A/0/1/0/all/0/1\">Anthony Martone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marks_R/0/1/0/all/0/1\">Robert J. Marks II</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baylis_C/0/1/0/all/0/1\">Charles Baylis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buehrer_R/0/1/0/all/0/1\">R. Michael Buehrer</a>",
          "description": "In this paper, we examine the use of a deep multi-layer perceptron model\narchitecture to classify received signal samples as coming from one of four\ncommon waveforms, Single Carrier (SC), Single-Carrier Frequency Division\nMultiple Access (SC-FDMA), Orthogonal Frequency Division Multiplexing (OFDM),\nand Linear Frequency Modulation (LFM), used in communication and radar\nnetworks. Synchronization of the signals is not needed as we assume there is an\nunknown and uncompensated time and frequency offset. An autoencoder with a deep\nCNN architecture is also examined to create a new fifth classification category\nof an unknown waveform type. This is accomplished by calculating a minimum and\nmaximum threshold values from the root mean square error (RMSE) of the radar\nand communication waveforms. The classifier and autoencoder work together to\nmonitor a spectrum area to identify the common waveforms inside the area of\noperation along with detecting unknown waveforms. Results from testing showed\nthe classifier had 100\\% classification rate above 0 dB with accuracy of 83.2\\%\nand 94.7\\% at -10 dB and -5 dB, respectively, with signal impairments present.\nResults for the anomaly detector showed 85.3\\% accuracy at 0 dB with 100\\% at\nSNR greater than 0 dB with signal impairments present when using a high-value\nFast Fourier Transform (FFT) size. Accurate detection rates decline as\nadditional noise is introduced to the signals, with 78.1\\% at -5 dB and 56.5\\%\nat -10 dB. However, these low rates seen can be potentially mitigated by using\neven higher FFT sizes also shown in our results.",
          "link": "http://arxiv.org/abs/2108.07339",
          "publishedOn": "2021-08-18T01:54:59.435Z",
          "wordCount": 698,
          "title": "Classification of Common Waveforms Including a Watchdog for Unknown Signals. (arXiv:2108.07339v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07453",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yankun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Shiqi Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Hemmings Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sawan_M/0/1/0/all/0/1\">Mohamad Sawan</a>",
          "description": "An accurate seizure prediction system enables early warnings before seizure\nonset of epileptic patients. It is extremely important for drug-refractory\npatients. Conventional seizure prediction works usually rely on features\nextracted from Electroencephalography (EEG) recordings and classification\nalgorithms such as regression or support vector machine (SVM) to locate the\nshort time before seizure onset. However, such methods cannot achieve\nhigh-accuracy prediction due to information loss of the hand-crafted features\nand the limited classification ability of regression and SVM algorithms. We\npropose an end-to-end deep learning solution using a convolutional neural\nnetwork (CNN) in this paper. One and two dimensional kernels are adopted in the\nearly- and late-stage convolution and max-pooling layers, respectively. The\nproposed CNN model is evaluated on Kaggle intracranial and CHB-MIT scalp EEG\ndatasets. Overall sensitivity, false prediction rate, and area under receiver\noperating characteristic curve reaches 93.5%, 0.063/h, 0.981 and 98.8%,\n0.074/h, 0.988 on two datasets respectively. Comparison with state-of-the-art\nworks indicates that the proposed model achieves exceeding prediction\nperformance.",
          "link": "http://arxiv.org/abs/2108.07453",
          "publishedOn": "2021-08-18T01:54:59.425Z",
          "wordCount": 614,
          "title": "An End-to-End Deep Learning Approach for Epileptic Seizure Prediction. (arXiv:2108.07453v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Aritra Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew Lan</a>",
          "description": "Computerized adaptive testing (CAT) refers to a form of tests that are\npersonalized to every student/test taker. CAT methods adaptively select the\nnext most informative question/item for each student given their responses to\nprevious questions, effectively reducing test length. Existing CAT methods use\nitem response theory (IRT) models to relate student ability to their responses\nto questions and static question selection algorithms designed to reduce the\nability estimation error as quickly as possible; therefore, these algorithms\ncannot improve by learning from large-scale student response data. In this\npaper, we propose BOBCAT, a Bilevel Optimization-Based framework for CAT to\ndirectly learn a data-driven question selection algorithm from training data.\nBOBCAT is agnostic to the underlying student response model and is\ncomputationally efficient during the adaptive testing process. Through\nextensive experiments on five real-world student response datasets, we show\nthat BOBCAT outperforms existing CAT methods (sometimes significantly) at\nreducing test length.",
          "link": "http://arxiv.org/abs/2108.07386",
          "publishedOn": "2021-08-18T01:54:59.416Z",
          "wordCount": 581,
          "title": "BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing. (arXiv:2108.07386v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07380",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mukhopadhyay_S/0/1/0/all/0/1\">Subhadeep Mukhopadhyay</a>",
          "description": "We have entered a new era of machine learning (ML), where the most accurate\nalgorithm with superior predictive power may not even be deployable, unless it\nis admissible under the regulatory constraints. This has led to great interest\nin developing fair, transparent and trustworthy ML methods. The purpose of this\narticle is to introduce a new information-theoretic learning framework\n(admissible machine learning) and algorithmic risk-management tools (InfoGram,\nL-features, ALFA-testing) that can guide an analyst to redesign off-the-shelf\nML methods to be regulatory compliant, while maintaining good prediction\naccuracy. We have illustrated our approach using several real-data examples\nfrom financial sectors, biomedical research, marketing campaigns, and the\ncriminal justice system.",
          "link": "http://arxiv.org/abs/2108.07380",
          "publishedOn": "2021-08-18T01:54:59.410Z",
          "wordCount": 554,
          "title": "InfoGram and Admissible Machine Learning. (arXiv:2108.07380v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1\">Albert Bifet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_J/0/1/0/all/0/1\">Jeremy C. Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejdl_W/0/1/0/all/0/1\">Wolfgang Nejdl</a>",
          "description": "As Artificial Intelligence (AI) is used in more applications, the need to\nconsider and mitigate biases from the learned models has followed. Most works\nin developing fair learning algorithms focus on the offline setting. However,\nin many real-world applications data comes in an online fashion and needs to be\nprocessed on the fly. Moreover, in practical application, there is a trade-off\nbetween accuracy and fairness that needs to be accounted for, but current\nmethods often have multiple hyperparameters with non-trivial interaction to\nachieve fairness. In this paper, we propose a flexible ensemble algorithm for\nfair decision-making in the more challenging context of evolving online\nsettings. This algorithm, called FARF (Fair and Adaptive Random Forests), is\nbased on using online component classifiers and updating them according to the\ncurrent distribution, that also accounts for fairness and a single\nhyperparameters that alters fairness-accuracy balance. Experiments on\nreal-world discriminated data streams demonstrate the utility of FARF.",
          "link": "http://arxiv.org/abs/2108.07403",
          "publishedOn": "2021-08-18T01:54:59.402Z",
          "wordCount": 589,
          "title": "FARF: A Fair and Adaptive Random Forests Classifier. (arXiv:2108.07403v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>",
          "description": "We study the problem of learning to cluster data points using an oracle which\ncan answer same-cluster queries. Different from previous approaches, we do not\nassume that the total number of clusters is known at the beginning and do not\nrequire that the true clusters are consistent with a predefined objective\nfunction such as the K-means. These relaxations are critical from the practical\nperspective and, meanwhile, make the problem more challenging. We propose two\nalgorithms with provable theoretical guarantees and verify their effectiveness\nvia an extensive set of experiments on both synthetic and real-world data.",
          "link": "http://arxiv.org/abs/2108.07383",
          "publishedOn": "2021-08-18T01:54:59.380Z",
          "wordCount": 520,
          "title": "Learning to Cluster via Same-Cluster Queries. (arXiv:2108.07383v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Guruprasad Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "In many applications, finding adequate labeled data to train predictive\nmodels is a major challenge. In this work, we propose methods to use\ngroup-level binary labels as weak supervision to train instance-level binary\nclassification models. Aggregate labels are common in several domains where\nannotating on a group-level might be cheaper or might be the only way to\nprovide annotated data without infringing on privacy. We model group-level\nlabels as Class Conditional Noisy (CCN) labels for individual instances and use\nthe noisy labels to regularize predictions of the model trained on the\nstrongly-labeled instances. Our experiments on real-world application of land\ncover mapping shows the utility of the proposed method in leveraging\ngroup-level labels, both in the presence and absence of class imbalance.",
          "link": "http://arxiv.org/abs/2108.07330",
          "publishedOn": "2021-08-18T01:54:59.372Z",
          "wordCount": 554,
          "title": "Weakly Supervised Classification Using Group-Level Labels. (arXiv:2108.07330v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Torfah_H/0/1/0/all/0/1\">Hazem Torfah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shetal Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Supratik Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akshay_S/0/1/0/all/0/1\">S. Akshay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1\">Sanjit A. Seshia</a>",
          "description": "We present a new multi-objective optimization approach for synthesizing\ninterpretations that \"explain\" the behavior of black-box machine learning\nmodels. Constructing human-understandable interpretations for black-box models\noften requires balancing conflicting objectives. A simple interpretation may be\neasier to understand for humans while being less precise in its predictions\nvis-a-vis a complex interpretation. Existing methods for synthesizing\ninterpretations use a single objective function and are often optimized for a\nsingle class of interpretations. In contrast, we provide a more general and\nmulti-objective synthesis framework that allows users to choose (1) the class\nof syntactic templates from which an interpretation should be synthesized, and\n(2) quantitative measures on both the correctness and explainability of an\ninterpretation. For a given black-box, our approach yields a set of\nPareto-optimal interpretations with respect to the correctness and\nexplainability measures. We show that the underlying multi-objective\noptimization problem can be solved via a reduction to quantitative constraint\nsolving, such as weighted maximum satisfiability. To demonstrate the benefits\nof our approach, we have applied it to synthesize interpretations for black-box\nneural-network classifiers. Our experiments show that there often exists a rich\nand varied set of choices for interpretations that are missed by existing\napproaches.",
          "link": "http://arxiv.org/abs/2108.07307",
          "publishedOn": "2021-08-18T01:54:59.364Z",
          "wordCount": 644,
          "title": "Synthesizing Pareto-Optimal Interpretations for Black-Box Models. (arXiv:2108.07307v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1\">William Rudman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillman_N/0/1/0/all/0/1\">Nate Gillman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayne_T/0/1/0/all/0/1\">Taylor Rayne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "The recent success of distributed word representations has led to an\nincreased interest in analyzing the properties of their spatial distribution.\nCurrent metrics suggest that contextualized word embedding models do not\nuniformly utilize all dimensions when embedding tokens in vector space. Here we\nargue that existing metrics are fragile and tend to obfuscate the true spatial\ndistribution of point clouds. To ameliorate this issue, we propose IsoScore: a\nnovel metric which quantifies the degree to which a point cloud uniformly\nutilizes the ambient vector space. We demonstrate that IsoScore has several\ndesirable properties such as mean invariance and direct correspondence to the\nnumber of dimensions used, which are properties that existing scores do not\npossess. Furthermore, IsoScore is conceptually intuitive and computationally\nefficient, making it well suited for analyzing the distribution of point clouds\nin arbitrary vector spaces, not necessarily limited to those of word embeddings\nalone. Additionally, we use IsoScore to demonstrate that a number of recent\nconclusions in the NLP literature that have been derived using brittle metrics\nof spatial distribution, such as average cosine similarity, may be incomplete\nor altogether inaccurate.",
          "link": "http://arxiv.org/abs/2108.07344",
          "publishedOn": "2021-08-18T01:54:59.357Z",
          "wordCount": 620,
          "title": "IsoScore: Measuring the Uniformity of Vector Space Utilization. (arXiv:2108.07344v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gary Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_K/0/1/0/all/0/1\">Karan Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1\">John Duchi</a>",
          "description": "We study the performance of federated learning algorithms and their variants\nin an asymptotic framework. Our starting point is the formulation of federated\nlearning as a multi-criterion objective, where the goal is to minimize each\nclient's loss using information from all of the clients. We propose a linear\nregression model, where, for a given client, we theoretically compare the\nperformance of various algorithms in the high-dimensional asymptotic limit.\nThis asymptotic multi-criterion approach naturally models the high-dimensional,\nmany-device nature of federated learning and suggests that personalization is\ncentral to federated learning. Our theory suggests that Fine-tuned Federated\nAveraging (FTFA), i.e., Federated Averaging followed by local training, and the\nridge regularized variant Ridge-tuned Federated Averaging (RTFA) are\ncompetitive with more sophisticated meta-learning and proximal-regularized\napproaches. In addition to being conceptually simpler, FTFA and RTFA are\ncomputationally more efficient than its competitors. We corroborate our\ntheoretical claims with extensive experiments on federated versions of the\nEMNIST, CIFAR-100, Shakespeare, and Stack Overflow datasets.",
          "link": "http://arxiv.org/abs/2108.07313",
          "publishedOn": "2021-08-18T01:54:59.348Z",
          "wordCount": 612,
          "title": "Fine-tuning is Fine in Federated Learning. (arXiv:2108.07313v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07356",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cutler_J/0/1/0/all/0/1\">Joshua Cutler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Drusvyatskiy_D/0/1/0/all/0/1\">Dmitriy Drusvyatskiy</a>, <a href=\"http://arxiv.org/find/math/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "We consider the problem of minimizing a convex function that is evolving in\ntime according to unknown and possibly stochastic dynamics. Such problems\nabound in the machine learning and signal processing literature, under the\nnames of concept drift and stochastic tracking. We provide novel non-asymptotic\nconvergence guarantees for stochastic algorithms with iterate averaging,\nfocusing on bounds valid both in expectation and with high probability.\nNotably, we show that the tracking efficiency of the proximal stochastic\ngradient method depends only logarithmically on the initialization quality,\nwhen equipped with a step-decay schedule. The results moreover naturally extend\nto settings where the dynamics depend jointly on time and on the decision\nvariable itself, as in the performative prediction framework.",
          "link": "http://arxiv.org/abs/2108.07356",
          "publishedOn": "2021-08-18T01:54:59.316Z",
          "wordCount": 570,
          "title": "Stochastic optimization under time drift: iterate averaging, step decay, and high probability guarantees. (arXiv:2108.07356v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07316",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Constantin_A/0/1/0/all/0/1\">Andrei Constantin</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Harvey_T/0/1/0/all/0/1\">Thomas R. Harvey</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Lukas_A/0/1/0/all/0/1\">Andre Lukas</a>",
          "description": "We use reinforcement learning as a means of constructing string\ncompactifications with prescribed properties. Specifically, we study heterotic\nSO(10) GUT models on Calabi-Yau three-folds with monad bundles, in search of\nphenomenologically promising examples. Due to the vast number of bundles and\nthe sparseness of viable choices, methods based on systematic scanning are not\nsuitable for this class of models. By focusing on two specific manifolds with\nPicard numbers two and three, we show that reinforcement learning can be used\nsuccessfully to explore monad bundles. Training can be accomplished with\nminimal computing resources and leads to highly efficient policy networks. They\nproduce phenomenologically promising states for nearly 100% of episodes and\nwithin a small number of steps. In this way, hundreds of new candidate standard\nmodels are found.",
          "link": "http://arxiv.org/abs/2108.07316",
          "publishedOn": "2021-08-18T01:54:59.307Z",
          "wordCount": 585,
          "title": "Heterotic String Model Building with Monad Bundles and Reinforcement Learning. (arXiv:2108.07316v1 [hep-th])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gavali_S/0/1/0/all/0/1\">Sachin Gavali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowart_J/0/1/0/all/0/1\">Julie Cowart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shanshan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cathy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_T/0/1/0/all/0/1\">Tammy Anderson</a>",
          "description": "In recent years, the US has experienced an opioid epidemic with an\nunprecedented number of drugs overdose deaths. Research finds such overdose\ndeaths are linked to neighborhood-level traits, thus providing opportunity to\nidentify effective interventions. Typically, techniques such as Ordinary Least\nSquares (OLS) or Maximum Likelihood Estimation (MLE) are used to document\nneighborhood-level factors significant in explaining such adverse outcomes.\nThese techniques are, however, less equipped to ascertain non-linear\nrelationships between confounding factors. Hence, in this study we apply\nmachine learning based techniques to identify opioid risks of neighborhoods in\nDelaware and explore the correlation of these factors using Shapley Additive\nexplanations (SHAP). We discovered that the factors related to neighborhoods\nenvironment, followed by education and then crime, were highly correlated with\nhigher opioid risk. We also explored the change in these correlations over the\nyears to understand the changing dynamics of the epidemic. Furthermore, we\ndiscovered that, as the epidemic has shifted from legal (i.e., prescription\nopioids) to illegal (e.g.,heroin and fentanyl) drugs in recent years, the\ncorrelation of environment, crime and health related variables with the opioid\nrisk has increased significantly while the correlation of economic and\nsocio-demographic variables has decreased. The correlation of education related\nfactors has been higher from the start and has increased slightly in recent\nyears suggesting a need for increased awareness about the opioid epidemic.",
          "link": "http://arxiv.org/abs/2108.07301",
          "publishedOn": "2021-08-18T01:54:59.255Z",
          "wordCount": 676,
          "title": "Understanding the factors driving the opioid epidemic using machine learning. (arXiv:2108.07301v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alaka_S/0/1/0/all/0/1\">Souridas Alaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreekumar_R/0/1/0/all/0/1\">Rishikesh Sreekumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>",
          "description": "Data analysis has become a necessity in the modern era of cricket. Everything\nfrom effective team management to match win predictions use some form of\nanalytics. Meaningful data representations are necessary for efficient analysis\nof data. In this study we investigate the use of adaptive (learnable)\nembeddings to represent inter-related features (such as players, teams, etc).\nThe data used for this study is collected from a classical T20 tournament IPL\n(Indian Premier League). To naturally facilitate the learning of meaningful\nrepresentations of features for accurate data analysis, we formulate a deep\nrepresentation learning framework which jointly learns a custom set of\nembeddings (which represents our features of interest) through the minimization\nof a contrastive loss. We base our objective on a set of classes obtained as a\nresult of hierarchical clustering on the overall run rate of an innings. It's\nbeen assessed that the framework ensures greater generality in the obtained\nembeddings, on top of which a task based analysis of overall run rate\nprediction was done to show the reliability of the framework.",
          "link": "http://arxiv.org/abs/2108.07139",
          "publishedOn": "2021-08-17T01:54:53.552Z",
          "wordCount": 618,
          "title": "Efficient Feature Representations for Cricket Data Analysis using Deep Learning based Multi-Modal Fusion Model. (arXiv:2108.07139v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangupantulu_R/0/1/0/all/0/1\">Rohit Gangupantulu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1\">Tyler Cody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_P/0/1/0/all/0/1\">Paul Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">Abdul Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenbeiser_L/0/1/0/all/0/1\">Logan Eisenbeiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radke_D/0/1/0/all/0/1\">Dan Radke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_R/0/1/0/all/0/1\">Ryan Clark</a>",
          "description": "Reinforcement learning (RL) has been applied to attack graphs for penetration\ntesting, however, trained agents do not reflect reality because the attack\ngraphs lack operational nuances typically captured within the intelligence\npreparation of the battlefield (IPB) that include notions of (cyber) terrain.\nIn particular, current practice constructs attack graphs exclusively using the\nCommon Vulnerability Scoring System (CVSS) and its components. We present\nmethods for constructing attack graphs using notions from IPB on cyber terrain\nanalysis of obstacles, avenues of approach, key terrain, observation and fields\nof fire, and cover and concealment. We demonstrate our methods on an example\nwhere firewalls are treated as obstacles and represented in (1) the reward\nspace and (2) the state dynamics. We show that terrain analysis can be used to\nbring realism to attack graphs for RL.",
          "link": "http://arxiv.org/abs/2108.07124",
          "publishedOn": "2021-08-17T01:54:53.463Z",
          "wordCount": 581,
          "title": "Using Cyber Terrain in Reinforcement Learning for Penetration Testing. (arXiv:2108.07124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1\">Mark Sellke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>",
          "description": "We consider incentivized exploration: a version of multi-armed bandits where\nthe choice of arms is controlled by self-interested agents, and the algorithm\ncan only issue recommendations. The algorithm controls the flow of information,\nand the information asymmetry can incentivize the agents to explore. Prior work\nachieves optimal regret rates up to multiplicative factors that become\narbitrarily large depending on the Bayesian priors, and scale exponentially in\nthe number of arms. A more basic problem of sampling each arm once runs into\nsimilar factors.\n\nWe focus on the price of incentives: the loss in performance, broadly\nconstrued, incurred for the sake of incentive-compatibility. We prove that\nThompson Sampling, a standard bandit algorithm, is incentive-compatible if\ninitialized with sufficiently many data points. The performance loss due to\nincentives is therefore limited to the initial rounds when these data points\nare collected. The problem is largely reduced to that of sample complexity: how\nmany rounds are needed? We address this question, providing matching upper and\nlower bounds and instantiating them in various corollaries. Typically, the\noptimal sample complexity is polynomial in the number of arms and exponential\nin the \"strength of beliefs\".",
          "link": "http://arxiv.org/abs/2002.00558",
          "publishedOn": "2021-08-17T01:54:53.457Z",
          "wordCount": 696,
          "title": "The Price of Incentivizing Exploration: A Characterization via Thompson Sampling and Sample Complexity. (arXiv:2002.00558v5 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.01722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>",
          "description": "Video-based person re-identification has drawn massive attention in recent\nyears due to its extensive applications in video surveillance. While deep\nlearning-based methods have led to significant progress, these methods are\nlimited by ineffectively using complementary information, which is blamed on\nnecessary data augmentation in the training process. Data augmentation has been\nwidely used to mitigate the over-fitting trap and improve the ability of\nnetwork representation. However, the previous methods adopt image-based data\naugmentation scheme to individually process the input frames, which corrupts\nthe complementary information between consecutive frames and causes performance\ndegradation. Extensive experiments on three benchmark datasets demonstrate that\nour framework outperforms the most recent state-of-the-art methods. We also\nperform cross-dataset validation to prove the generality of our method.",
          "link": "http://arxiv.org/abs/1905.01722",
          "publishedOn": "2021-08-17T01:54:53.375Z",
          "wordCount": 609,
          "title": "Intra-clip Aggregation for Video Person Re-identification. (arXiv:1905.01722v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>",
          "description": "Real-world recommender system needs to be regularly retrained to keep with\nthe new data. In this work, we consider how to efficiently retrain graph\nconvolution network (GCN) based recommender models, which are state-of-the-art\ntechniques for collaborative recommendation. To pursue high efficiency, we set\nthe target as using only new data for model updating, meanwhile not sacrificing\nthe recommendation accuracy compared with full model retraining. This is\nnon-trivial to achieve, since the interaction data participates in both the\ngraph structure for model construction and the loss function for model\nlearning, whereas the old graph structure is not allowed to use in model\nupdating. Towards the goal, we propose a \\textit{Causal Incremental Graph\nConvolution} approach, which consists of two new operators named\n\\textit{Incremental Graph Convolution} (IGC) and \\textit{Colliding Effect\nDistillation} (CED) to estimate the output of full graph convolution. In\nparticular, we devise simple and effective modules for IGC to ingeniously\ncombine the old representations and the incremental graph and effectively fuse\nthe long-term and short-term preference signals. CED aims to avoid the\nout-of-date issue of inactive nodes that are not in the incremental graph,\nwhich connects the new data with inactive nodes through causal inference. In\nparticular, CED estimates the causal effect of new data on the representation\nof inactive nodes through the control of their collider. Extensive experiments\non three real-world datasets demonstrate both accuracy gains and significant\nspeed-ups over the existing retraining mechanism.",
          "link": "http://arxiv.org/abs/2108.06889",
          "publishedOn": "2021-08-17T01:54:50.403Z",
          "wordCount": 676,
          "title": "Causal Incremental Graph Convolution for Recommender System Retraining. (arXiv:2108.06889v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yutong Li</a>",
          "description": "Building an independent misspelling detector and serve it before correction\ncan bring multiple benefits to speller and other search components, which is\nparticularly true for the most commonly deployed noisy-channel based speller\nsystems. With rapid development of deep learning and substantial advancement in\ncontextual representation learning such as BERTology, building a decent\nmisspelling detector without having to rely on hand-crafted features associated\nwith noisy-channel architecture becomes more-than-ever accessible. However\nBERTolgy models are trained with natural language corpus but Maps Search is\nhighly domain specific, would BERTology continue its success. In this paper we\ndesign 4 stages of models for misspeling detection ranging from the most basic\nLSTM to single-domain augmented fine-tuned BERT. We found for Maps Search in\nour case, other advanced BERTology family model such as RoBERTa does not\nnecessarily outperform BERT, and a classic cross-domain fine-tuned full BERT\neven underperforms a smaller single-domain fine-tuned BERT. We share more\nfindings through comprehensive modeling experiments and analysis, we also\nbriefly cover the data generation algorithm breakthrough.",
          "link": "http://arxiv.org/abs/2108.06842",
          "publishedOn": "2021-08-17T01:54:50.391Z",
          "wordCount": 596,
          "title": "Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations. (arXiv:2108.06842v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06847",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ha_W/0/1/0/all/0/1\">Wooseok Ha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>",
          "description": "Recent deep-learning models have achieved impressive predictive performance\nby learning complex functions of many variables, often at the cost of\ninterpretability. This chapter covers recent work aiming to interpret models by\nattributing importance to features and feature groups for a single prediction.\nImportantly, the proposed attributions assign importance to interactions\nbetween features, in addition to features in isolation. These attributions are\nshown to yield insights across real-world domains, including bio-imaging,\ncosmology image and natural-language processing. We then show how these\nattributions can be used to directly improve the generalization of a neural\nnetwork or to distill it into a simple model. Throughout the chapter, we\nemphasize the use of reality checks to scrutinize the proposed interpretation\ntechniques.",
          "link": "http://arxiv.org/abs/2108.06847",
          "publishedOn": "2021-08-17T01:54:50.306Z",
          "wordCount": 549,
          "title": "Interpreting and improving deep-learning models with reality checks. (arXiv:2108.06847v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lexing Ying</a>",
          "description": "We introduce a class of variational actor-critic algorithms based on a\nvariational formulation over both the value function and the policy. The\nobjective function of the variational formulation consists of two parts: one\nfor maximizing the value function and the other for minimizing the Bellman\nresidual. Besides the vanilla gradient descent with both the value function and\nthe policy updates, we propose two variants, the clipping method and the\nflipping method, in order to speed up the convergence. We also prove that, when\nthe prefactor of the Bellman residual is sufficiently large, the fixed point of\nthe algorithm is close to the optimal policy.",
          "link": "http://arxiv.org/abs/2108.01215",
          "publishedOn": "2021-08-17T01:54:50.255Z",
          "wordCount": 551,
          "title": "Variational Actor-Critic Algorithms. (arXiv:2108.01215v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "We propose Future Discriminators for Generation (FUDGE), a flexible and\nmodular method for controlled text generation. Given a pre-existing model G for\ngenerating text from a distribution of interest, FUDGE enables conditioning on\na desired attribute a (for example, formality) while requiring access only to\nG's output logits. FUDGE learns an attribute predictor operating on a partial\nsequence, and uses this predictor's outputs to adjust G's original\nprobabilities. We show that FUDGE models terms corresponding to a Bayesian\ndecomposition of the conditional distribution of G given attribute a. Moreover,\nFUDGE can easily compose predictors for multiple desired attributes. We\nevaluate FUDGE on three tasks -- couplet completion in poetry, topic control in\nlanguage generation, and formality change in machine translation -- and observe\ngains in all three tasks.",
          "link": "http://arxiv.org/abs/2104.05218",
          "publishedOn": "2021-08-17T01:54:50.230Z",
          "wordCount": 590,
          "title": "FUDGE: Controlled Text Generation With Future Discriminators. (arXiv:2104.05218v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nomura_M/0/1/0/all/0/1\">Masahiro Nomura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1\">Yuta Saito</a>",
          "description": "A typical assumption in supervised machine learning is that the train\n(source) and test (target) datasets follow completely the same distribution.\nThis assumption is, however, often violated in uncertain real-world\napplications, which motivates the study of learning under covariate shift. In\nthis setting, the naive use of adaptive hyperparameter optimization methods\nsuch as Bayesian optimization does not work as desired since it does not\naddress the distributional shift among different datasets. In this work, we\nconsider a novel hyperparameter optimization problem under the multi-source\ncovariate shift whose goal is to find the optimal hyperparameters for a target\ntask of interest using only unlabeled data in a target task and labeled data in\nmultiple source tasks. To conduct efficient hyperparameter optimization for the\ntarget task, it is essential to estimate the target objective using only the\navailable information. To this end, we construct the variance reduced estimator\nthat unbiasedly approximates the target objective with a desirable variance\nproperty. Building on the proposed estimator, we provide a general and\ntractable hyperparameter optimization procedure, which works preferably in our\nsetting with a no-regret guarantee. The experiments demonstrate that the\nproposed framework broadens the applications of automated hyperparameter\noptimization.",
          "link": "http://arxiv.org/abs/2006.10600",
          "publishedOn": "2021-08-17T01:54:50.109Z",
          "wordCount": 653,
          "title": "Efficient Hyperparameter Optimization under Multi-Source Covariate Shift. (arXiv:2006.10600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06552",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Boschini_M/0/1/0/all/0/1\">Matteo Boschini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Buzzega_P/0/1/0/all/0/1\">Pietro Buzzega</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bonicelli_L/0/1/0/all/0/1\">Lorenzo Bonicelli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Porrello_A/0/1/0/all/0/1\">Angelo Porrello</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>",
          "description": "Continual Learning (CL) investigates how to train Deep Networks on a stream\nof tasks without incurring catastrophic forgetting. CL settings proposed in the\nliterature assume that every incoming example is paired with ground-truth\nannotations. However, this clashes with many real-world applications: gathering\nlabeled data, which is in itself tedious and expensive, becomes indeed\ninfeasible when data flow as a stream and must be consumed in real-time. This\nwork explores Weakly Supervised Continual Learning (WSCL): here, only a small\nfraction of labeled input examples are shown to the learner. We assess how\ncurrent CL methods (e.g.: EWC, LwF, iCaRL, ER, GDumb, DER) perform in this\nnovel and challenging scenario, in which overfitting entangles forgetting.\nSubsequently, we design two novel WSCL methods which exploit metric learning\nand consistency regularization to leverage unsupervised data while learning. In\ndoing so, we show that not only our proposals exhibit higher flexibility when\nsupervised information is scarce, but also that less than 25% labels can be\nenough to reach or even outperform SOTA methods trained under full supervision.",
          "link": "http://arxiv.org/abs/2108.06552",
          "publishedOn": "2021-08-17T01:54:50.069Z",
          "wordCount": 605,
          "title": "Weakly Supervised Continual Learning. (arXiv:2108.06552v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kaiming He</a>",
          "description": "This paper does not describe a novel method. Instead, it studies a\nstraightforward, incremental, yet must-know baseline given the recent progress\nin computer vision: self-supervised learning for Vision Transformers (ViT).\nWhile the training recipes for standard convolutional networks have been highly\nmature and robust, the recipes for ViT are yet to be built, especially in the\nself-supervised scenarios where training becomes more challenging. In this\nwork, we go back to basics and investigate the effects of several fundamental\ncomponents for training self-supervised ViT. We observe that instability is a\nmajor issue that degrades accuracy, and it can be hidden by apparently good\nresults. We reveal that these results are indeed partial failure, and they can\nbe improved when training is made more stable. We benchmark ViT results in MoCo\nv3 and several other self-supervised frameworks, with ablations in various\naspects. We discuss the currently positive evidence as well as challenges and\nopen questions. We hope that this work will provide useful data points and\nexperience for future research.",
          "link": "http://arxiv.org/abs/2104.02057",
          "publishedOn": "2021-08-17T01:54:50.042Z",
          "wordCount": 660,
          "title": "An Empirical Study of Training Self-Supervised Vision Transformers. (arXiv:2104.02057v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1\">Ladislav Ramp&#xe1;&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>",
          "description": "Graph neural networks (GNNs) based on message passing between neighboring\nnodes are known to be insufficient for capturing long-range interactions in\ngraphs. In this project we study hierarchical message passing models that\nleverage a multi-resolution representation of a given graph. This facilitates\nlearning of features that span large receptive fields without loss of local\ninformation, an aspect not studied in preceding work on hierarchical GNNs. We\nintroduce Hierarchical Graph Net (HGNet), which for any two connected nodes\nguarantees existence of message-passing paths of at most logarithmic length\nw.r.t. the input graph size. Yet, under mild assumptions, its internal\nhierarchy maintains asymptotic size equivalent to that of the input graph. We\nobserve that our HGNet outperforms conventional stacking of GCN layers\nparticularly in molecular property prediction benchmarks. Finally, we propose\ntwo benchmarking tasks designed to elucidate capability of GNNs to leverage\nlong-range interactions in graphs.",
          "link": "http://arxiv.org/abs/2107.07432",
          "publishedOn": "2021-08-17T01:54:50.036Z",
          "wordCount": 602,
          "title": "Hierarchical graph neural nets can capture long-range interactions. (arXiv:2107.07432v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "The newly emerged transformer technology has a tremendous impact on NLP\nresearch. In the general English domain, transformer-based models have achieved\nstate-of-the-art performances on various NLP benchmarks. In the clinical\ndomain, researchers also have investigated transformer models for clinical\napplications. The goal of this study is to systematically explore three widely\nused transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical\nrelation extraction and develop an open-source package with clinical\npre-trained transformer-based models to facilitate information extraction in\nthe clinical domain. We developed a series of clinical RE models based on three\ntransformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these\nmodels using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2\nchallenges. We compared two classification strategies (binary vs. multi-class\nclassification) and investigated two approaches to generate candidate relations\nin different experimental settings. In this study, we compared three\ntransformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We\ndemonstrated that the RoBERTa-clinical RE model achieved the best performance\non the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2\ndataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our\nresults indicated that the binary classification strategy consistently\noutperformed the multi-class classification strategy for clinical relation\nextraction. Our methods and models are publicly available at\nhttps://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction.\nWe believe this work will improve current practice on clinical relation\nextraction and other related NLP tasks in the biomedical domain.",
          "link": "http://arxiv.org/abs/2107.08957",
          "publishedOn": "2021-08-17T01:54:50.023Z",
          "wordCount": 708,
          "title": "Clinical Relation Extraction Using Transformer-based Models. (arXiv:2107.08957v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1\">Ananth Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1\">Michael Mathioudakis</a>",
          "description": "Machine unlearning is the task of updating machine learning (ML) models after\na subset of the training data they were trained on is deleted. Methods for the\ntask are desired to combine effectiveness and efficiency, i.e., they should\neffectively \"unlearn\" deleted data, but in a way that does not require\nexcessive computation effort (e.g., a full retraining) for a small amount of\ndeletions. Such a combination is typically achieved by tolerating some amount\nof approximation in the unlearning. In addition, laws and regulations in the\nspirit of \"the right to be forgotten\" have given rise to requirements for\ncertifiability, i.e., the ability to demonstrate that the deleted data has\nindeed been unlearned by the ML model.\n\nIn this paper, we present an experimental study of the three state-of-the-art\napproximate unlearning methods for linear models and demonstrate the trade-offs\nbetween efficiency, effectiveness and certifiability offered by each method. In\nimplementing the study, we extend some of the existing works and describe a\ncommon ML pipeline to compare and evaluate the unlearning methods on six\nreal-world datasets and a variety of settings. We provide insights into the\neffect of the quantity and distribution of the deleted data on ML models and\nthe performance of each unlearning method in different settings. We also\npropose a practical online strategy to determine when the accumulated error\nfrom approximate unlearning is large enough to warrant a full retrain of the ML\nmodel.",
          "link": "http://arxiv.org/abs/2106.15093",
          "publishedOn": "2021-08-17T01:54:50.017Z",
          "wordCount": 701,
          "title": "Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menculini_L/0/1/0/all/0/1\">Lorenzo Menculini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marini_A/0/1/0/all/0/1\">Andrea Marini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proietti_M/0/1/0/all/0/1\">Massimiliano Proietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garinei_A/0/1/0/all/0/1\">Alberto Garinei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozza_A/0/1/0/all/0/1\">Alessio Bozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moretti_C/0/1/0/all/0/1\">Cecilia Moretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marconi_M/0/1/0/all/0/1\">Marcello Marconi</a>",
          "description": "Setting sale prices correctly is of great importance for firms, and the study\nand forecast of prices time series is therefore a relevant topic not only from\na data science perspective but also from an economic and applicative one. In\nthis paper we examine different techniques to forecast sale prices applied by\nan Italian food wholesaler, as a step towards the automation of pricing tasks\nusually taken care by human workforce. We consider ARIMA models and compare\nthem to Prophet, a scalable forecasting tool by Facebook based on a generalized\nadditive model, and to deep learning models exploiting Long Short--Term Memory\n(LSTM) and Convolutional Neural Networks (CNNs). ARIMA models are frequently\nused in econometric analyses, providing a good benchmark for the problem under\nstudy. Our results indicate that ARIMA models and LSTM neural networks perform\nsimilarly for the forecasting task under consideration, while the combination\nof CNNs and LSTMs attains the best overall accuracy, but requires more time to\nbe tuned. On the contrary, Prophet is quick and easy to use, but considerably\nless accurate.t overall accuracy, but requires more time to be tuned. On the\ncontrary, Prophet is quick and easy to use, but considerably less accurate.",
          "link": "http://arxiv.org/abs/2107.12770",
          "publishedOn": "2021-08-17T01:54:49.992Z",
          "wordCount": 694,
          "title": "Comparing Prophet and Deep Learning to ARIMA in Forecasting Wholesale Food Prices. (arXiv:2107.12770v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-08-17T01:54:49.957Z",
          "wordCount": 653,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Cross-entropy loss with softmax output is a standard choice to train neural\nnetwork classifiers. We give a new view of neural network classifiers with\nsoftmax and cross-entropy as mutual information evaluators. We show that when\nthe dataset is balanced, training a neural network with cross-entropy maximises\nthe mutual information between inputs and labels through a variational form of\nmutual information. Thereby, we develop a new form of softmax that also\nconverts a classifier to a mutual information evaluator when the dataset is\nimbalanced. Experimental results show that the new form leads to better\nclassification accuracy, in particular for imbalanced datasets.",
          "link": "http://arxiv.org/abs/2106.10471",
          "publishedOn": "2021-08-17T01:54:49.933Z",
          "wordCount": 564,
          "title": "Neural Network Classifier as Mutual Information Evaluator. (arXiv:2106.10471v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_T/0/1/0/all/0/1\">Takeshi Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unno_H/0/1/0/all/0/1\">Hiroshi Unno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekiyama_T/0/1/0/all/0/1\">Taro Sekiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>",
          "description": "Loop-invariant synthesis is the basis of every program verification\nprocedure. Due to its undecidability in general, a tool for invariant synthesis\nnecessarily uses heuristics. Despite the common belief that the design of\nheuristics is vital for the effective performance of a verifier, little work\nhas been performed toward obtaining the optimal heuristics for each\ninvariant-synthesis tool. Instead, developers have hand-tuned the heuristics of\ntools. This study demonstrates that we can effectively and automatically learn\na good heuristic via reinforcement learning for an invariant synthesizer PCSat.\nOur experiment shows that PCSat combined with the heuristic learned by\nreinforcement learning outperforms the state-of-the-art solvers for this task.\nTo the best of our knowledge, this is the first work that investigates learning\nthe heuristics of an invariant synthesis tool.",
          "link": "http://arxiv.org/abs/2107.09766",
          "publishedOn": "2021-08-17T01:54:49.910Z",
          "wordCount": 589,
          "title": "Enhancing Loop-Invariant Synthesis via Reinforcement Learning. (arXiv:2107.09766v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Ding Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_P/0/1/0/all/0/1\">Parth H. Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zizhan Zheng</a>",
          "description": "We consider a set of APs with unknown data rates that cooperatively serve a\nmobile client. The data rate of each link is i.i.d. sampled from a distribution\nthat is unknown a priori. In contrast to traditional link scheduling problems\nunder uncertainty, we assume that in each time step, the device can probe a\nsubset of links before deciding which one to use. We model this problem as a\ncontextual bandit problem with probing (CBwP) and present an efficient\nalgorithm. We further establish the regret of our algorithm for links with\nBernoulli data rates. Our CBwP model is a novel extension of the classic\ncontextual bandit model and can potentially be applied to a large class of\nsequential decision-making problems that involve joint probing and play under\nuncertainty.",
          "link": "http://arxiv.org/abs/2108.03297",
          "publishedOn": "2021-08-17T01:54:49.898Z",
          "wordCount": 587,
          "title": "Joint AP Probing and Scheduling: A Contextual Bandit Approach. (arXiv:2108.03297v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hacohen_G/0/1/0/all/0/1\">Guy Hacohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1\">Daphna Weinshall</a>",
          "description": "Recent work suggests that convolutional neural networks of different\narchitectures learn to classify images in the same order. To understand this\nphenomenon, we revisit the over-parametrized deep linear network model. Our\nasymptotic analysis, assuming that the hidden layers are wide enough, reveals\nthat the convergence rate of this model's parameters is exponentially faster\nalong directions corresponding to the larger principal components of the data,\nat a rate governed by the singular values. We term this convergence pattern the\nPrincipal Components bias (PC-bias). We show how the PC-bias streamlines the\norder of learning of both linear and non-linear networks, more prominently at\nearlier stages of learning. We then compare our results to the simplicity bias,\nshowing that both biases can be seen independently, and affect the order of\nlearning in different ways. Finally, we discuss how the PC-bias may explain\nsome benefits of early stopping and its connection to PCA, and why deep\nnetworks converge more slowly when given random labels.",
          "link": "http://arxiv.org/abs/2105.05553",
          "publishedOn": "2021-08-17T01:54:49.893Z",
          "wordCount": 628,
          "title": "Principal Components Bias in Deep Neural Networks. (arXiv:2105.05553v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiexia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Furong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Juanjuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kejiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>",
          "description": "Accurate traffic state prediction is the foundation of transportation control\nand guidance. It is very challenging due to the complex spatiotemporal\ndependencies in traffic data. Existing works cannot perform well for multi-step\ntraffic prediction that involves long future time period. The spatiotemporal\ninformation dilution becomes serve when the time gap between input step and\npredicted step is large, especially when traffic data is not sufficient or\nnoisy. To address this issue, we propose a multi-spatial graph convolution\nbased Seq2Seq model. Our main novelties are three aspects: (1) We enrich the\nspatiotemporal information of model inputs by fusing multi-view features (time,\nlocation and traffic states) (2) We build multiple kinds of spatial\ncorrelations based on both prior knowledge and data-driven knowledge to improve\nmodel performance especially in insufficient or noisy data cases. (3) A\nspatiotemporal attention mechanism based on reachability knowledge is novelly\ndesigned to produce high-level features fed into decoder of Seq2Seq directly to\nease information dilution. Our model is evaluated on two real world traffic\ndatasets and achieves better performance than other competitors.",
          "link": "http://arxiv.org/abs/2107.01528",
          "publishedOn": "2021-08-17T01:54:49.887Z",
          "wordCount": 653,
          "title": "Incorporating Reachability Knowledge into a Multi-Spatial Graph Convolution Based Seq2Seq Model for Traffic Forecasting. (arXiv:2107.01528v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>",
          "description": "Graph-based multi-view clustering has become an active topic due to the\nefficiency in characterizing both the complex structure and relationship\nbetween multimedia data. However, existing methods have the following\nshortcomings: (1) They are inefficient or even fail for graph learning in large\nscale due to the graph construction and eigen-decomposition. (2) They cannot\nwell exploit both the complementary information and spatial structure embedded\nin graphs of different views. To well exploit complementary information and\ntackle the scalability issue plaguing graph-based multi-view clustering, we\npropose an efficient multiple graph learning model via a small number of anchor\npoints and tensor Schatten p-norm minimization. Specifically, we construct a\nhidden and tractable large graph by anchor graph for each view and well exploit\ncomplementary information embedded in anchor graphs of different views by\ntensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,\nwhich scales linearly with the data size, to solve our proposed model.\nExtensive experimental results on several datasets indicate that our proposed\nmethod outperforms some state-of-the-art multi-view clustering algorithms.",
          "link": "http://arxiv.org/abs/2106.15382",
          "publishedOn": "2021-08-17T01:54:49.871Z",
          "wordCount": 629,
          "title": "Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+John_T/0/1/0/all/0/1\">Thrupthi Ann John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>",
          "description": "As Deep Neural Network models for face processing tasks approach human-like\nperformance, their deployment in critical applications such as law enforcement\nand access control has seen an upswing, where any failure may have far-reaching\nconsequences. We need methods to build trust in deployed systems by making\ntheir working as transparent as possible. Existing visualization algorithms are\ndesigned for object recognition and do not give insightful results when applied\nto the face domain. In this work, we present 'Canonical Saliency Maps', a new\nmethod that highlights relevant facial areas by projecting saliency maps onto a\ncanonical face model. We present two kinds of Canonical Saliency Maps:\nimage-level maps and model-level maps. Image-level maps highlight facial\nfeatures responsible for the decision made by a deep face model on a given\nimage, thus helping to understand how a DNN made a prediction on the image.\nModel-level maps provide an understanding of what the entire DNN model focuses\non in each task and thus can be used to detect biases in the model. Our\nqualitative and quantitative results show the usefulness of the proposed\ncanonical saliency maps, which can be used on any deep face model regardless of\nthe architecture.",
          "link": "http://arxiv.org/abs/2105.01386",
          "publishedOn": "2021-08-17T01:54:49.863Z",
          "wordCount": 682,
          "title": "Canonical Saliency Maps: Decoding Deep Face Models. (arXiv:2105.01386v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with a limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-08-17T01:54:49.858Z",
          "wordCount": 608,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_A/0/1/0/all/0/1\">Austin Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>",
          "description": "Visual engagement in social media platforms comprises interactions with photo\nposts including comments, shares, and likes. In this paper, we leverage such\nvisual engagement clues as supervisory signals for representation learning.\nHowever, learning from engagement signals is non-trivial as it is not clear how\nto bridge the gap between low-level visual information and high-level social\ninteractions. We present VisE, a weakly supervised learning approach, which\nmaps social images to pseudo labels derived by clustered engagement signals. We\nthen study how models trained in this way benefit subjective downstream\ncomputer vision tasks such as emotion recognition or political bias detection.\nThrough extensive studies, we empirically demonstrate the effectiveness of VisE\nacross a diverse set of classification tasks beyond the scope of conventional\nrecognition.",
          "link": "http://arxiv.org/abs/2104.07767",
          "publishedOn": "2021-08-17T01:54:49.852Z",
          "wordCount": 600,
          "title": "Exploring Visual Engagement Signals for Representation Learning. (arXiv:2104.07767v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sourjya Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mustafa Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Anand Raghunathan</a>",
          "description": "Deep Neural Networks (DNNs) have transformed the field of machine learning\nand are widely deployed in many applications involving image, video, speech and\nnatural language processing. The increasing compute demands of DNNs have been\nwidely addressed through Graphics Processing Units (GPUs) and specialized\naccelerators. However, as model sizes grow, these von Neumann architectures\nrequire very high memory bandwidth to keep the processing elements utilized as\na majority of the data resides in the main memory. Processing in memory has\nbeen proposed as a promising solution for the memory wall bottleneck for ML\nworkloads. In this work, we propose a new DRAM-based processing-in-memory (PIM)\nmultiplication primitive coupled with intra-bank accumulation to accelerate\nmatrix vector operations in ML workloads. The proposed multiplication primitive\nadds < 1% area overhead and does not require any change in the DRAM\nperipherals. Therefore, the proposed multiplication can be easily adopted in\ncommodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture,\ndata mapping scheme and dataflow for executing DNNs within DRAM. System\nevaluations performed on networks like AlexNet, VGG16 and ResNet18 show that\nthe proposed architecture, mapping, and data flow can provide up to 19.5x\nspeedup over an NVIDIA Titan Xp GPU highlighting the need to overcome the\nmemory bottleneck in future generations of DNN hardware.",
          "link": "http://arxiv.org/abs/2105.03736",
          "publishedOn": "2021-08-17T01:54:49.846Z",
          "wordCount": 685,
          "title": "PIM-DRAM: Accelerating Machine Learning Workloads using Processing in Commodity DRAM. (arXiv:2105.03736v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "We study how to evaluate the quantitative information content of a region\nwithin an image for a particular label. To this end, we bridge class activation\nmaps with information theory. We develop an informative class activation map\n(infoCAM). Given a classification task, infoCAM depict how to accumulate\ninformation of partial regions to that of the entire image toward a label.\nThus, we can utilise infoCAM to locate the most informative features for a\nlabel. When applied to an image classification task, infoCAM performs better\nthan the traditional classification map in the weakly supervised object\nlocalisation task. We achieve state-of-the-art results on Tiny-ImageNet.",
          "link": "http://arxiv.org/abs/2106.10472",
          "publishedOn": "2021-08-17T01:54:49.831Z",
          "wordCount": 564,
          "title": "Informative Class Activation Maps. (arXiv:2106.10472v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1\">Taeyeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1\">Grzegorz Cielniak</a>",
          "description": "In our previous work, we designed a systematic policy to prioritize sampling\nlocations to lead significant accuracy improvement in spatial interpolation by\nusing the prediction uncertainty of Gaussian Process Regression (GPR) as\n\"attraction force\" to deployed robots in path planning. Although the\nintegration with Traveling Salesman Problem (TSP) solvers was also shown to\nproduce relatively short travel distance, we here hypothesise several factors\nthat could decrease the overall prediction precision as well because\nsub-optimal locations may eventually be included in their paths. To address\nthis issue, in this paper, we first explore \"local planning\" approaches\nadopting various spatial ranges within which next sampling locations are\nprioritized to investigate their effects on the prediction performance as well\nas incurred travel distance. Also, Reinforcement Learning (RL)-based high-level\ncontrollers are trained to adaptively produce blended plans from a particular\nset of local planners to inherit unique strengths from that selection depending\non latest prediction states. Our experiments on use cases of temperature\nmonitoring robots demonstrate that the dynamic mixtures of planners can not\nonly generate sophisticated, informative plans that a single planner could not\ncreate alone but also ensure significantly reduced travel distances at no cost\nof prediction reliability without any assist of additional modules for shortest\npath calculation.",
          "link": "http://arxiv.org/abs/2108.06618",
          "publishedOn": "2021-08-17T01:54:49.825Z",
          "wordCount": 650,
          "title": "Adaptive Selection of Informative Path Planning Strategies via Reinforcement Learning. (arXiv:2108.06618v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03002",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>",
          "description": "More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.",
          "link": "http://arxiv.org/abs/2108.03002",
          "publishedOn": "2021-08-17T01:54:49.820Z",
          "wordCount": 666,
          "title": "Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04150",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yilin Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+DeJong_J/0/1/0/all/0/1\">Jennifer DeJong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Halverson_T/0/1/0/all/0/1\">Tom Halverson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shuman_D/0/1/0/all/0/1\">David I Shuman</a>",
          "description": "Ranked data sets, where m judges/voters specify a preference ranking of n\nobjects/candidates, are increasingly prevalent in contexts such as political\nelections, computer vision, recommender systems, and bioinformatics. The vote\ncounts for each ranking can be viewed as an n! data vector lying on the\npermutahedron, which is a Cayley graph of the symmetric group with vertices\nlabeled by permutations and an edge when two permutations differ by an adjacent\ntransposition. Leveraging combinatorial representation theory and recent\nprogress in signal processing on graphs, we investigate a novel, scalable\ntransform method to interpret and exploit structure in ranked data. We\nrepresent data on the permutahedron using an overcomplete dictionary of atoms,\neach of which captures both smoothness information about the data (typically\nthe focus of spectral graph decomposition methods in graph signal processing)\nand structural information about the data (typically the focus of symmetry\ndecomposition methods from representation theory). These atoms have a more\nnaturally interpretable structure than any known basis for signals on the\npermutahedron, and they form a Parseval frame, ensuring beneficial numerical\nproperties such as energy preservation. We develop specialized algorithms and\nopen software that take advantage of the symmetry and structure of the\npermutahedron to improve the scalability of the proposed method, making it more\napplicable to the high-dimensional ranked data found in applications.",
          "link": "http://arxiv.org/abs/2103.04150",
          "publishedOn": "2021-08-17T01:54:49.814Z",
          "wordCount": 727,
          "title": "Signal Processing on the Permutahedron: Tight Spectral Frames for Ranked Data Analysis. (arXiv:2103.04150v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1\">Jeff Mentch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_J/0/1/0/all/0/1\">Jerry Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Measuring the acoustic characteristics of a space is often done by capturing\nits impulse response (IR), a representation of how a full-range stimulus sound\nexcites it. This work generates an IR from a single image, which can then be\napplied to other signals using convolution, simulating the reverberant\ncharacteristics of the space shown in the image. Recording these IRs is both\ntime-intensive and expensive, and often infeasible for inaccessible locations.\nWe use an end-to-end neural network architecture to generate plausible audio\nimpulse responses from single images of acoustic environments. We evaluate our\nmethod both by comparisons to ground truth data and by human expert evaluation.\nWe demonstrate our approach by generating plausible impulse responses from\ndiverse settings and formats including well known places, musical halls, rooms\nin paintings, images from animations and computer games, synthetic environments\ngenerated from text, panoramic images, and video conference backgrounds.",
          "link": "http://arxiv.org/abs/2103.14201",
          "publishedOn": "2021-08-17T01:54:49.807Z",
          "wordCount": 623,
          "title": "Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis. (arXiv:2103.14201v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kahyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas J. Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McInnes_B/0/1/0/all/0/1\">Bridget McInnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">&#xd6;zlem Uzuner</a>",
          "description": "Methods and Materials: We investigated transferability of neural\nnetwork-based de-identification sys-tems with and without domain\ngeneralization. We used two domain generalization approaches: a novel approach\nJoint-Domain Learning (JDL) as developed in this paper, and a state-of-the-art\ndomain general-ization approach Common-Specific Decomposition (CSD) from the\nliterature. First, we measured trans-ferability from a single external source.\nSecond, we used two external sources and evaluated whether domain\ngeneralization can improve transferability of de-identification models across\ndomains which rep-resent different note types from the same institution. Third,\nusing two external sources with in-domain training data, we studied whether\nexternal source data are useful even in cases where sufficient in-domain\ntraining data are available. Finally, we investigated transferability of the\nde-identification mod-els across institutions. Results and Conclusions: We\nfound transferability from a single external source gave inconsistent re-sults.\nUsing additional external sources consistently yielded an F1-score of\napproximately 80%, but domain generalization was not always helpful to improve\ntransferability. We also found that external sources were useful even in cases\nwhere in-domain training data were available by reducing the amount of needed\nin-domain training data or by improving performance. Transferability across\ninstitutions was differed by note type and annotation label. External sources\nfrom a different institution were also useful to further improve performance.",
          "link": "http://arxiv.org/abs/2102.08517",
          "publishedOn": "2021-08-17T01:54:49.784Z",
          "wordCount": 684,
          "title": "Transferability of Neural Network-based De-identification Systems. (arXiv:2102.08517v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1\">Qinghua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yipeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>",
          "description": "Deep neural networks (DNNs) usually contain massive parameters, but there is\nredundancy such that it is guessed that the DNNs could be trained in\nlow-dimensional subspaces. In this paper, we propose a Dynamic Linear\nDimensionality Reduction (DLDR) based on low-dimensional properties of the\ntraining trajectory. The reduction is efficient, which is supported by\ncomprehensive experiments: optimization in 40 dimensional spaces can achieve\ncomparable performance as regular training over thousands or even millions of\nparameters. Since there are only a few optimization variables, we develop a\nquasi-Newton-based algorithm and also obtain robustness against label noises,\nwhich are two follow-up experiments to show the advantages of finding\nlow-dimensional subspaces.",
          "link": "http://arxiv.org/abs/2103.11154",
          "publishedOn": "2021-08-17T01:54:49.778Z",
          "wordCount": 591,
          "title": "Low Dimensional Landscape Hypothesis is True: DNNs can be Trained in Tiny Subspaces. (arXiv:2103.11154v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "The existence of multiple datasets for sarcasm detection prompts us to apply\ntransfer learning to exploit their commonality. The adversarial neural transfer\n(ANT) framework utilizes multiple loss terms that encourage the source-domain\nand the target-domain feature distributions to be similar while optimizing for\ndomain-specific performance. However, these objectives may be in conflict,\nwhich can lead to optimization difficulties and sometimes diminished transfer.\nWe propose a generalized latent optimization strategy that allows different\nlosses to accommodate each other and improves training dynamics. The proposed\nmethod outperforms transfer learning and meta-learning baselines. In\nparticular, we achieve 10.02% absolute performance gain over the previous state\nof the art on the iSarcasm dataset.",
          "link": "http://arxiv.org/abs/2104.09261",
          "publishedOn": "2021-08-17T01:54:49.766Z",
          "wordCount": 594,
          "title": "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection. (arXiv:2104.09261v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_V/0/1/0/all/0/1\">Vincent Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>",
          "description": "Knowledge graph embedding plays an important role in knowledge\nrepresentation, reasoning, and data mining applications. However, for multiple\ncross-domain knowledge graphs, state-of-the-art embedding models cannot make\nfull use of the data from different knowledge domains while preserving the\nprivacy of exchanged data. In addition, the centralized embedding model may not\nscale to the extensive real-world knowledge graphs. Therefore, we propose a\nnovel decentralized scalable learning framework, \\emph{Federated Knowledge\nGraphs Embedding} (FKGE), where embeddings from different knowledge graphs can\nbe learnt in an asynchronous and peer-to-peer manner while being\nprivacy-preserving. FKGE exploits adversarial generation between pairs of\nknowledge graphs to translate identical entities and relations of different\ndomains into near embedding spaces. In order to protect the privacy of the\ntraining data, FKGE further implements a privacy-preserving neural network\nstructure to guarantee no raw data leakage. We conduct extensive experiments to\nevaluate FKGE on 11 knowledge graphs, demonstrating a significant and\nconsistent improvement in model quality with at most 17.85\\% and 7.90\\%\nincreases in performance on triple classification and link prediction tasks.",
          "link": "http://arxiv.org/abs/2105.07615",
          "publishedOn": "2021-08-17T01:54:49.728Z",
          "wordCount": 639,
          "title": "Differentially Private Federated Knowledge Graphs Embedding. (arXiv:2105.07615v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1\">Vinod Kumar Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sukhdeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>",
          "description": "Handwritten character recognition (HCR) is a challenging learning problem in\npattern recognition, mainly due to similarity in structure of characters,\ndifferent handwriting styles, noisy datasets and a large variety of languages\nand scripts. HCR problem is studied extensively for a few decades but there is\nvery limited research on script independent models. This is because of factors,\nlike, diversity of scripts, focus of the most of conventional research efforts\non handcrafted feature extraction techniques which are language/script specific\nand are not always available, and unavailability of public datasets and codes\nto reproduce the results. On the other hand, deep learning has witnessed huge\nsuccess in different areas of pattern recognition, including HCR, and provides\nend-to-end learning, i.e., automated feature extraction and recognition. In\nthis paper, we have proposed a novel deep learning architecture which exploits\ntransfer learning and image-augmentation for end-to-end learning for script\nindependent handwritten character recognition, called HCR-Net. The network is\nbased on a novel transfer learning approach for HCR, where some of lower layers\nof a pre-trained VGG16 network are utilised. Due to transfer learning and\nimage-augmentation, HCR-Net provides faster training, better performance and\nbetter generalisations. The experimental results on publicly available datasets\nof Bangla, Punjabi, Hindi, English, Swedish, Urdu, Farsi, Tibetan, Kannada,\nMalayalam, Telugu, Marathi, Nepali and Arabic languages prove the efficacy of\nHCR-Net and establishes several new benchmarks. For reproducibility of the\nresults and for the advancements of the HCR research, complete code is publicly\nreleased at \\href{https://github.com/jmdvinodjmd/HCR-Net}{GitHub}.",
          "link": "http://arxiv.org/abs/2108.06663",
          "publishedOn": "2021-08-17T01:54:49.723Z",
          "wordCount": 704,
          "title": "HCR-Net: A deep learning based script independent handwritten character recognition network. (arXiv:2108.06663v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "Data quantity and quality are crucial factors for data-driven learning\nmethods. In some target problem domains, there are not many data samples\navailable, which could significantly hinder the learning process. While data\nfrom similar domains may be leveraged to help through domain adaptation,\nobtaining high-quality labeled data for those source domains themselves could\nbe difficult or costly. To address such challenges on data insufficiency for\nclassification problem in a target domain, we propose a weak adaptation\nlearning (WAL) approach that leverages unlabeled data from a similar source\ndomain, a low-cost weak annotator that produces labels based on task-specific\nheuristics, labeling rules, or other methods (albeit with inaccuracy), and a\nsmall amount of labeled data in the target domain. Our approach first conducts\na theoretical analysis on the error bound of the trained classifier with\nrespect to the data quantity and the performance of the weak annotator, and\nthen introduces a multi-stage weak adaptation learning method to learn an\naccurate classifier by lowering the error bound. Our experiments demonstrate\nthe effectiveness of our approach in learning an accurate classifier with\nlimited labeled data in the target domain and unlabeled data in the source\ndomain.",
          "link": "http://arxiv.org/abs/2102.07358",
          "publishedOn": "2021-08-17T01:54:49.706Z",
          "wordCount": 668,
          "title": "Weak Adaptation Learning -- Addressing Cross-domain Data Insufficiency with Weak Annotator. (arXiv:2102.07358v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lybrand_E/0/1/0/all/0/1\">Eric Lybrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saab_R/0/1/0/all/0/1\">Rayan Saab</a>",
          "description": "We propose a new computationally efficient method for quantizing the weights\nof pre- trained neural networks that is general enough to handle both\nmulti-layer perceptrons and convolutional neural networks. Our method\ndeterministically quantizes layers in an iterative fashion with no complicated\nre-training required. Specifically, we quantize each neuron, or hidden unit,\nusing a greedy path-following algorithm. This simple algorithm is equivalent to\nrunning a dynamical system, which we prove is stable for quantizing a\nsingle-layer neural network (or, alternatively, for quantizing the first layer\nof a multi-layer network) when the training data are Gaussian. We show that\nunder these assumptions, the quantization error decays with the width of the\nlayer, i.e., its level of over-parametrization. We provide numerical\nexperiments, on multi-layer networks, to illustrate the performance of our\nmethods on MNIST and CIFAR10 data, as well as for quantizing the VGG16 network\nusing ImageNet data.",
          "link": "http://arxiv.org/abs/2010.15979",
          "publishedOn": "2021-08-17T01:54:49.694Z",
          "wordCount": 602,
          "title": "A Greedy Algorithm for Quantizing Neural Networks. (arXiv:2010.15979v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1\">Dirk Tasche</a>",
          "description": "For the binary prevalence quantification problem under prior probability\nshift, we determine the asymptotic variance of the maximum likelihood\nestimator. We find that it is a function of the Brier score for the regression\nof the class label against the features under the test data set distribution.\nThis observation suggests that optimising the accuracy of a base classifier on\nthe training data set helps to reduce the variance of the related quantifier on\nthe test data set. Therefore, we also point out training criteria for the base\nclassifier that imply optimisation of both of the Brier scores on the training\nand the test data sets.",
          "link": "http://arxiv.org/abs/2107.08209",
          "publishedOn": "2021-08-17T01:54:49.672Z",
          "wordCount": 576,
          "title": "Minimising quantifier variance under prior probability shift. (arXiv:2107.08209v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Seung Won Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sitao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidayetoglu_M/0/1/0/all/0/1\">Mert Hidayeto&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_E/0/1/0/all/0/1\">Eiman Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>",
          "description": "Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale\ngraph-based recommender systems. Training GCN requires the minibatch generator\ntraversing graphs and sampling the sparsely located neighboring nodes to obtain\ntheir features. Since real-world graphs often exceed the capacity of GPU\nmemory, current GCN training systems keep the feature table in host memory and\nrely on the CPU to collect sparse features before sending them to the GPUs.\nThis approach, however, puts tremendous pressure on host memory bandwidth and\nthe CPU. This is because the CPU needs to (1) read sparse features from memory,\n(2) write features into memory as a dense format, and (3) transfer the features\nfrom memory to the GPUs. In this work, we propose a novel GPU-oriented data\ncommunication approach for GCN training, where GPU threads directly access\nsparse features in host memory through zero-copy accesses without much CPU\nhelp. By removing the CPU gathering stage, our method significantly reduces the\nconsumption of the host resources and data access latency. We further present\ntwo important techniques to achieve high host memory access efficiency by the\nGPU: (1) automatic data access address alignment to maximize PCIe packet\nefficiency, and (2) asynchronous zero-copy access and kernel execution to fully\noverlap data transfer with training. We incorporate our method into PyTorch and\nevaluate its effectiveness using several graphs with sizes up to 111 million\nnodes and 1.6 billion edges. In a multi-GPU training setup, our method is\n65-92% faster than the conventional data transfer method, and can even match\nthe performance of all-in-GPU-memory training for some graphs that fit in GPU\nmemory.",
          "link": "http://arxiv.org/abs/2103.03330",
          "publishedOn": "2021-08-17T01:54:49.644Z",
          "wordCount": 753,
          "title": "Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture. (arXiv:2103.03330v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Sin Kit Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1\">Hye-Young Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>",
          "description": "Federated learning is an emerging privacy-preserving AI technique where\nclients (i.e., organisations or devices) train models locally and formulate a\nglobal model based on the local model updates without transferring local data\nexternally. However, federated learning systems struggle to achieve\ntrustworthiness and embody responsible AI principles. In particular, federated\nlearning systems face accountability and fairness challenges due to\nmulti-stakeholder involvement and heterogeneity in client data distribution. To\nenhance the accountability and fairness of federated learning systems, we\npresent a blockchain-based trustworthy federated learning architecture. We\nfirst design a smart contract-based data-model provenance registry to enable\naccountability. Additionally, we propose a weighted fair data sampler algorithm\nto enhance fairness in training data. We evaluate the proposed approach using a\nCOVID-19 X-ray detection use case. The evaluation results show that the\napproach is feasible to enable accountability and improve fairness. The\nproposed algorithm can achieve better performance than the default federated\nlearning setting in terms of the model's generalisation and accuracy.",
          "link": "http://arxiv.org/abs/2108.06912",
          "publishedOn": "2021-08-17T01:54:49.629Z",
          "wordCount": 640,
          "title": "Blockchain-based Trustworthy Federated Learning Architecture. (arXiv:2108.06912v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_X/0/1/0/all/0/1\">Xin Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>",
          "description": "Unsupervised representation learning has achieved outstanding performances\nusing centralized data available on the Internet. However, the increasing\nawareness of privacy protection limits sharing of decentralized unlabeled image\ndata that grows explosively in multiple parties (e.g., mobile phones and\ncameras). As such, a natural problem is how to leverage these data to learn\nvisual representations for downstream tasks while preserving data privacy. To\naddress this problem, we propose a novel federated unsupervised learning\nframework, FedU. In this framework, each party trains models from unlabeled\ndata independently using contrastive learning with an online network and a\ntarget network. Then, a central server aggregates trained models and updates\nclients' models with the aggregated model. It preserves data privacy as each\nparty only has access to its raw data. Decentralized data among multiple\nparties are normally non-independent and identically distributed (non-IID),\nleading to performance degradation. To tackle this challenge, we propose two\nsimple but effective methods: 1) We design the communication protocol to upload\nonly the encoders of online networks for server aggregation and update them\nwith the aggregated encoder; 2) We introduce a new module to dynamically decide\nhow to update predictors based on the divergence caused by non-IID. The\npredictor is the other component of the online network. Extensive experiments\nand ablations demonstrate the effectiveness and significance of FedU. It\noutperforms training with only one party by over 5% and other methods by over\n14% in linear and semi-supervised evaluation on non-IID data.",
          "link": "http://arxiv.org/abs/2108.06492",
          "publishedOn": "2021-08-17T01:54:49.623Z",
          "wordCount": 696,
          "title": "Collaborative Unsupervised Visual Representation Learning from Decentralized Data. (arXiv:2108.06492v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Charlie Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thekumparampil_K/0/1/0/all/0/1\">Kiran K. Thekumparampil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1\">Giulia Fanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>",
          "description": "A central question in federated learning (FL) is how to design optimization\nalgorithms that minimize the communication cost of training a model over\nheterogeneous data distributed across many clients. A popular technique for\nreducing communication is the use of local steps, where clients take multiple\noptimization steps over local data before communicating with the server (e.g.,\nFedAvg, SCAFFOLD). This contrasts with centralized methods, where clients take\none optimization step per communication round (e.g., Minibatch SGD). A recent\nlower bound on the communication complexity of first-order methods shows that\ncentralized methods are optimal over highly-heterogeneous data, whereas local\nmethods are optimal over purely homogeneous data [Woodworth et al., 2020]. For\nintermediate heterogeneity levels, no algorithm is known to match the lower\nbound. In this paper, we propose a multistage optimization scheme that nearly\nmatches the lower bound across all heterogeneity levels. The idea is to first\nrun a local method up to a heterogeneity-induced error floor; next, we switch\nto a centralized method for the remaining steps. Our analysis may help explain\nempirically-successful stepsize decay methods in FL [Charles et al., 2020;\nReddi et al., 2020]. We demonstrate the scheme's practical utility in image\nclassification tasks.",
          "link": "http://arxiv.org/abs/2108.06869",
          "publishedOn": "2021-08-17T01:54:49.617Z",
          "wordCount": 643,
          "title": "Reducing the Communication Cost of Federated Learning through Multistage Optimization. (arXiv:2108.06869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03932",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ayatollahi_F/0/1/0/all/0/1\">Fazael Ayatollahi</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Shokouhi_S/0/1/0/all/0/1\">Shahriar B. Shokouhi</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Mann_R/0/1/0/all/0/1\">Ritse M. Mann</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a> (2 and 3) ((1) Electrical Engineering Department, Iran University of Science and Technology (IUST), Tehran, Iran, (2) Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, the Netherlands, (3) Department of Radiation Oncology, Netherlands Cancer Institute, Amsterdam, the Netherlands)",
          "description": "Purpose: We propose a deep learning-based computer-aided detection (CADe)\nmethod to detect breast lesions in ultrafast DCE-MRI sequences. This method\nuses both the three-dimensional spatial information and temporal information\nobtained from the early-phase of the dynamic acquisition. Methods: The proposed\nCADe method, based on a modified 3D RetinaNet model, operates on ultrafast T1\nweighted sequences, which are preprocessed for motion compensation, temporal\nnormalization, and are cropped before passing into the model. The model is\noptimized to enable the detection of relatively small breast lesions in a\nscreening setting, focusing on detection of lesions that are harder to\ndifferentiate from confounding structures inside the breast. Results: The\nmethod was developed based on a dataset consisting of 489 ultrafast MRI studies\nobtained from 462 patients containing a total of 572 lesions (365 malignant,\n207 benign) and achieved a detection rate, sensitivity, and detection rate of\nbenign lesions of 0.90 (0.876-0.934), 0.95 (0.934-0.980), and 0.81\n(0.751-0.871) at 4 false positives per normal breast with 10-fold\ncross-testing, respectively. Conclusions: The deep learning architecture used\nfor the proposed CADe application can efficiently detect benign and malignant\nlesions on ultrafast DCE-MRI. Furthermore, utilizing the less visible hard-to\ndetect-lesions in training improves the learning process and, subsequently,\ndetection of malignant breast lesions.",
          "link": "http://arxiv.org/abs/2102.03932",
          "publishedOn": "2021-08-17T01:54:49.611Z",
          "wordCount": 723,
          "title": "Automatic Breast Lesion Detection in Ultrafast DCE-MRI Using Deep Learning. (arXiv:2102.03932v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huaizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shanshan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yong Luo</a>",
          "description": "AI engineering has emerged as a crucial discipline to democratize deep neural\nnetwork (DNN) models among software developers with a diverse background. In\nparticular, altering these DNN models in the deployment stage posits a\ntremendous challenge. In this research, we propose and develop a low-code\nsolution, ModelPS (an acronym for \"Model Photoshop\"), to enable and empower\ncollaborative DNN model editing and intelligent model serving. The ModelPS\nsolution embodies two transformative features: 1) a user-friendly web interface\nfor a developer team to share and edit DNN models pictorially, in a low-code\nfashion, and 2) a model genie engine in the backend to aid developers in\ncustomizing model editing configurations for given deployment requirements or\nconstraints. Our case studies with a wide range of deep learning (DL) models\nshow that the system can tremendously reduce both development and communication\noverheads with improved productivity.",
          "link": "http://arxiv.org/abs/2105.08275",
          "publishedOn": "2021-08-17T01:54:49.605Z",
          "wordCount": 637,
          "title": "ModelPS: An Interactive and Collaborative Platform for Editing Pre-trained Models at Scale. (arXiv:2105.08275v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pantazis_O/0/1/0/all/0/1\">Omiros Pantazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brostow_G/0/1/0/all/0/1\">Gabriel Brostow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_K/0/1/0/all/0/1\">Kate Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>",
          "description": "We address the problem of learning self-supervised representations from\nunlabeled image collections. Unlike existing approaches that attempt to learn\nuseful features by maximizing similarity between augmented versions of each\ninput image or by speculatively picking negative samples, we instead also make\nuse of the natural variation that occurs in image collections that are captured\nusing static monitoring cameras. To achieve this, we exploit readily available\ncontext data that encodes information such as the spatial and temporal\nrelationships between the input images. We are able to learn representations\nthat are surprisingly effective for downstream supervised classification, by\nfirst identifying high probability positive pairs at training time, i.e. those\nimages that are likely to depict the same visual concept. For the critical task\nof global biodiversity monitoring, this results in image features that can be\nadapted to challenging visual species classification tasks with limited human\nsupervision. We present results on four different camera trap image\ncollections, across three different families of self-supervised learning\nmethods, and show that careful image selection at training time results in\nsuperior performance compared to existing baselines such as conventional\nself-supervised training and transfer learning.",
          "link": "http://arxiv.org/abs/2108.06435",
          "publishedOn": "2021-08-17T01:54:49.599Z",
          "wordCount": 633,
          "title": "Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring. (arXiv:2108.06435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_M/0/1/0/all/0/1\">Megan Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jingjing Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acero_A/0/1/0/all/0/1\">Alex Acero</a>",
          "description": "Named entity recognition (NER) is usually developed and tested on text from\nwell-written sources. However, in intelligent voice assistants, where NER is an\nimportant component, input to NER may be noisy because of user or speech\nrecognition error. In applications, entity labels may change frequently, and\nnon-textual properties like topicality or popularity may be needed to choose\namong alternatives.\n\nWe describe a NER system intended to address these problems. We test and\ntrain this system on a proprietary user-derived dataset. We compare with a\nbaseline text-only NER system; the baseline enhanced with external gazetteers;\nand the baseline enhanced with the search and indirect labelling techniques we\ndescribe below. The final configuration gives around 6% reduction in NER error\nrate. We also show that this technique improves related tasks, such as semantic\nparsing, with an improvement of up to 5% in error rate.",
          "link": "http://arxiv.org/abs/2108.06633",
          "publishedOn": "2021-08-17T01:54:49.575Z",
          "wordCount": 609,
          "title": "DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants. (arXiv:2108.06633v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1\">Amartya Sanyal</a>",
          "description": "Deep learning research has recently witnessed an impressively fast-paced\nprogress in a wide range of tasks including computer vision, natural language\nprocessing, and reinforcement learning. The extraordinary performance of these\nsystems often gives the impression that they can be used to revolutionise our\nlives for the better. However, as recent works point out, these systems suffer\nfrom several issues that make them unreliable for use in the real world,\nincluding vulnerability to adversarial attacks (Szegedy et al. [248]), tendency\nto memorise noise (Zhang et al. [292]), being over-confident on incorrect\npredictions (miscalibration) (Guo et al. [99]), and unsuitability for handling\nprivate data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of\nthese issues in detail, investigate their causes, and propose computationally\ncheap algorithms for mitigating them in practice. To do this, we identify\nstructures in deep neural networks that can be exploited to mitigate the above\ncauses of unreliability of deep learning algorithms.",
          "link": "http://arxiv.org/abs/2108.07083",
          "publishedOn": "2021-08-17T01:54:49.570Z",
          "wordCount": 593,
          "title": "Identifying and Exploiting Structures for Reliable Deep Learning. (arXiv:2108.07083v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1\">Mengting Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yusan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>",
          "description": "Modeling inter-dependencies between time-series is the key to achieve high\nperformance in anomaly detection for multivariate time-series data. The\nde-facto solution to model the dependencies is to feed the data into a\nrecurrent neural network (RNN). However, the fully connected network structure\nunderneath the RNN (either GRU or LSTM) assumes a static and complete\ndependency graph between time-series, which may not hold in many real-world\napplications. To alleviate this assumption, we propose a dynamic bipartite\ngraph structure to encode the inter-dependencies between time-series. More\nconcretely, we model time series as one type of nodes, and the time series\nsegments (regarded as event) as another type of nodes, where the edge between\ntwo types of nodes describe a temporal pattern occurred on a specific time\nseries at a certain time. Based on this design, relations between time series\ncan be explicitly modelled via dynamic connections to event nodes, and the\nmultivariate time-series anomaly detection problem can be formulated as a\nself-supervised, edge stream prediction problem in dynamic graphs. We conducted\nextensive experiments to demonstrate the effectiveness of the design.",
          "link": "http://arxiv.org/abs/2108.06783",
          "publishedOn": "2021-08-17T01:54:49.553Z",
          "wordCount": 629,
          "title": "Event2Graph: Event-driven Bipartite Graph for Multivariate Time-series Anomaly Detection. (arXiv:2108.06783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "Since the Lipschitz properties of convolutional neural networks (CNNs) are\nwidely considered to be related to adversarial robustness, we theoretically\ncharacterize the $\\ell_1$ norm and $\\ell_\\infty$ norm of 2D multi-channel\nconvolutional layers and provide efficient methods to compute the exact\n$\\ell_1$ norm and $\\ell_\\infty$ norm. Based on our theorem, we propose a novel\nregularization method termed norm decay, which can effectively reduce the norms\nof convolutional layers and fully-connected layers. Experiments show that\nnorm-regularization methods, including norm decay, weight decay, and singular\nvalue clipping, can improve generalization of CNNs. However, they can slightly\nhurt adversarial robustness. Observing this unexpected phenomenon, we compute\nthe norms of layers in the CNNs trained with three different adversarial\ntraining frameworks and surprisingly find that adversarially robust CNNs have\ncomparable or even larger layer norms than their non-adversarially robust\ncounterparts. Furthermore, we prove that under a mild assumption, adversarially\nrobust classifiers can be achieved using neural networks, and an adversarially\nrobust neural network can have an arbitrarily large Lipschitz constant. For\nthis reason, enforcing small norms on CNN layers may be neither necessary nor\neffective in achieving adversarial robustness. The code is available at\nhttps://github.com/youweiliang/norm_robustness.",
          "link": "http://arxiv.org/abs/2009.08435",
          "publishedOn": "2021-08-17T01:54:49.547Z",
          "wordCount": 710,
          "title": "Large Norms of CNN Layers Do Not Hurt Adversarial Robustness. (arXiv:2009.08435v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.01699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karmakar_S/0/1/0/all/0/1\">Sayar Karmakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Anirbit Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthukumar_R/0/1/0/all/0/1\">Ramchandran Muthukumar</a>",
          "description": "In this work, we study the possibility of defending against \"data-poisoning\"\nattacks while learning a neural net. We focus on the supervised learning setup\nfor a class of finite-sized depth-2 nets - which include the standard single\nfilter convolutional nets. For this setup we attempt to learn the true label\ngenerating weights in the presence of a malicious oracle doing stochastic\nbounded and additive adversarial distortions on the true labels being accessed\nby the algorithm during training. For the non-gradient stochastic algorithm\nthat we instantiate we prove (worst case nearly optimal) trade-offs among the\nmagnitude of the adversarial attack, the accuracy, and the confidence achieved\nby the proposed algorithm. Additionally, our algorithm uses mini-batching and\nwe keep track of how the mini-batch size affects the convergence.",
          "link": "http://arxiv.org/abs/2005.01699",
          "publishedOn": "2021-08-17T01:54:49.531Z",
          "wordCount": 604,
          "title": "Guarantees on learning depth-2 neural networks under a data-poisoning attack. (arXiv:2005.01699v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_V/0/1/0/all/0/1\">Violet Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeNero_J/0/1/0/all/0/1\">John DeNero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "We propose an efficient batching strategy for variable-length decoding on GPU\narchitectures. During decoding, when candidates terminate or are pruned\naccording to heuristics, our streaming approach periodically \"refills\" the\nbatch before proceeding with a selected subset of candidates. We apply our\nmethod to variable-width beam search on a state-of-the-art machine translation\nmodel. Our method decreases runtime by up to 71% compared to a fixed-width beam\nsearch baseline and 17% compared to a variable-width baseline, while matching\nbaselines' BLEU. Finally, experiments show that our method can speed up\ndecoding in other domains, such as semantic and syntactic parsing.",
          "link": "http://arxiv.org/abs/2010.02164",
          "publishedOn": "2021-08-17T01:54:49.526Z",
          "wordCount": 589,
          "title": "A Streaming Approach For Efficient Batched Beam Search. (arXiv:2010.02164v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>",
          "description": "We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter\npruning), which slims down a CNN by reducing the width (number of output\nchannels) of convolutional layers. Inspired by the neurobiology research about\nthe independence of remembering and forgetting, we propose to re-parameterize a\nCNN into the remembering parts and forgetting parts, where the former learn to\nmaintain the performance and the latter learn to prune. Via training with\nregular SGD on the former but a novel update rule with penalty gradients on the\nlatter, we realize structured sparsity. Then we equivalently merge the\nremembering and forgetting parts into the original architecture with narrower\nlayers. In this sense, ResRep can be viewed as a successful application of\nStructural Re-parameterization. Such a methodology distinguishes ResRep from\nthe traditional learning-based pruning paradigm that applies a penalty on\nparameters to produce sparsity, which may suppress the parameters essential for\nthe remembering. ResRep slims down a standard ResNet-50 with 76.15% accuracy on\nImageNet to a narrower one with only 45% FLOPs and no accuracy drop, which is\nthe first to achieve lossless pruning with such a high compression ratio. The\ncode and models are at https://github.com/DingXiaoH/ResRep.",
          "link": "http://arxiv.org/abs/2007.03260",
          "publishedOn": "2021-08-17T01:54:49.520Z",
          "wordCount": 704,
          "title": "ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting. (arXiv:2007.03260v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tantawy_D/0/1/0/all/0/1\">Dina Tantawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahran_M/0/1/0/all/0/1\">Mohamed Zahran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wassal_A/0/1/0/all/0/1\">Amr Wassal</a>",
          "description": "Since its invention, Generative adversarial networks (GANs) have shown\noutstanding results in many applications. Generative Adversarial Networks are\npowerful yet, resource-hungry deep-learning models. Their main difference from\nordinary deep learning models is the nature of their output. For example, GAN\noutput can be a whole image versus other models detecting objects or\nclassifying images. Thus, the architecture and numeric precision of the network\naffect the quality and speed of the solution. Hence, accelerating GANs is\npivotal. Accelerating GANs can be classified into three main tracks: (1) Memory\ncompression, (2) Computation optimization, and (3) Data-flow optimization.\nBecause data transfer is the main source of energy usage, memory compression\nleads to the most savings. Thus, in this paper, we survey memory compression\ntechniques for CNN-Based GANs. Additionally, the paper summarizes opportunities\nand challenges in GANs acceleration and suggests open research problems to be\nfurther investigated.",
          "link": "http://arxiv.org/abs/2108.06626",
          "publishedOn": "2021-08-17T01:54:49.514Z",
          "wordCount": 590,
          "title": "A Survey on GAN Acceleration Using Memory Compression Technique. (arXiv:2108.06626v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1\">Anshul Nasery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_S/0/1/0/all/0/1\">Soumyadeep Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piratla_V/0/1/0/all/0/1\">Vihari Piratla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>",
          "description": "In several real world applications, machine learning models are deployed to\nmake predictions on data whose distribution changes gradually along time,\nleading to a drift between the train and test distributions. Such models are\noften re-trained on new data periodically, and they hence need to generalize to\ndata not too far into the future. In this context, there is much prior work on\nenhancing temporal generalization, e.g. continuous transportation of past data,\nkernel smoothed time-sensitive parameters and more recently, adversarial\nlearning of time-invariant features. However, these methods share several\nlimitations, e.g, poor scalability, training instability, and dependence on\nunlabeled data from the future. Responding to the above limitations, we propose\na simple method that starts with a model with time-sensitive parameters but\nregularizes its temporal complexity using a Gradient Interpolation (GI) loss.\nGI allows the decision boundary to change along time and can still prevent\noverfitting to the limited training time snapshots by allowing task-specific\ncontrol over changes along time. We compare our method to existing baselines on\nmultiple real-world datasets, which show that GI outperforms more complicated\ngenerative and adversarial approaches on the one hand, and simpler gradient\nregularization methods on the other.",
          "link": "http://arxiv.org/abs/2108.06721",
          "publishedOn": "2021-08-17T01:54:49.507Z",
          "wordCount": 641,
          "title": "Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time. (arXiv:2108.06721v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1810.08102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pierrot_T/0/1/0/all/0/1\">Thomas Pierrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrin_N/0/1/0/all/0/1\">Nicolas Perrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1\">Olivier Sigaud</a>",
          "description": "In this paper, we provide an overview of first-order and second-order\nvariants of the gradient descent method that are commonly used in machine\nlearning. We propose a general framework in which 6 of these variants can be\ninterpreted as different instances of the same approach. They are the vanilla\ngradient descent, the classical and generalized Gauss-Newton methods, the\nnatural gradient descent method, the gradient covariance matrix approach, and\nNewton's method. Besides interpreting these methods within a single framework,\nwe explain their specificities and show under which conditions some of them\ncoincide.",
          "link": "http://arxiv.org/abs/1810.08102",
          "publishedOn": "2021-08-17T01:54:49.482Z",
          "wordCount": 581,
          "title": "First-order and second-order variants of the gradient descent in a unified framework. (arXiv:1810.08102v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1\">Armin W. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poldrack_R/0/1/0/all/0/1\">Russell A. Poldrack</a>",
          "description": "In cognitive decoding, researchers aim to characterize a brain region's\nrepresentations by identifying the cognitive states (e.g., accepting/rejecting\na gamble) that can be identified from the region's activity. Deep learning (DL)\nmethods are highly promising for cognitive decoding, with their unmatched\nability to learn versatile representations of complex data. Yet, their\nwidespread application in cognitive decoding is hindered by their general lack\nof interpretability as well as difficulties in applying them to small datasets\nand in ensuring their reproducibility and robustness. We propose to approach\nthese challenges by leveraging recent advances in explainable artificial\nintelligence and transfer learning, while also providing specific\nrecommendations on how to improve the reproducibility and robustness of DL\nmodeling results.",
          "link": "http://arxiv.org/abs/2108.06896",
          "publishedOn": "2021-08-17T01:54:49.462Z",
          "wordCount": 547,
          "title": "Challenges for cognitive decoding using deep learning methods. (arXiv:2108.06896v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "Facial expression recognition (FER) has emerged as an important component of\nhuman-computer interaction systems. Despite recent advancements in FER,\nperformance often drops significantly for non-frontal facial images. We propose\nContrastive Learning of Multi-view facial Expressions (CL-MEx) to exploit\nfacial images captured simultaneously from different angles towards FER. CL-MEx\nis a two-step training framework. In the first step, an encoder network is\npre-trained with the proposed self-supervised contrastive loss, where it learns\nto generate view-invariant embeddings for different views of a subject. The\nmodel is then fine-tuned with labeled data in a supervised setting. We\ndemonstrate the performance of the proposed method on two multi-view FER\ndatasets, KDEF and DDCF, where state-of-the-art performances are achieved.\nFurther experiments show the robustness of our method in dealing with\nchallenging angles and reduced amounts of labeled data.",
          "link": "http://arxiv.org/abs/2108.06723",
          "publishedOn": "2021-08-17T01:54:49.456Z",
          "wordCount": 580,
          "title": "Self-supervised Contrastive Learning of Multi-view Facial Expressions. (arXiv:2108.06723v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piratla_V/0/1/0/all/0/1\">Vihari Piratla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_S/0/1/0/all/0/1\">Soumen Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>",
          "description": "Our goal is to evaluate the accuracy of a black-box classification model, not\nas a single aggregate on a given test data distribution, but as a surface over\na large number of combinations of attributes characterizing multiple test data\ndistributions. Such attributed accuracy measures become important as machine\nlearning models get deployed as a service, where the training data distribution\nis hidden from clients, and different clients may be interested in diverse\nregions of the data distribution. We present Attributed Accuracy Assay (AAA)--a\nGaussian Process (GP)--based probabilistic estimator for such an accuracy\nsurface. Each attribute combination, called an 'arm', is associated with a Beta\ndensity from which the service's accuracy is sampled. We expect the GP to\nsmooth the parameters of the Beta density over related arms to mitigate\nsparsity. We show that obvious application of GPs cannot address the challenge\nof heteroscedastic uncertainty over a huge attribute space that is sparsely and\nunevenly populated. In response, we present two enhancements: pooling sparse\nobservations, and regularizing the scale parameter of the Beta densities. After\nintroducing these innovations, we establish the effectiveness of AAA in terms\nof both its estimation accuracy and exploration efficiency, through extensive\nexperiments and analysis.",
          "link": "http://arxiv.org/abs/2108.06514",
          "publishedOn": "2021-08-17T01:54:49.451Z",
          "wordCount": 633,
          "title": "Active Assessment of Prediction Services as Accuracy Surface Over Attribute Combinations. (arXiv:2108.06514v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Ramneet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Anirban Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangdon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolsky_O/0/1/0/all/0/1\">Oleg Sokolsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>",
          "description": "Deep neural networks (DNNs) are known to produce incorrect predictions with\nvery high confidence on out-of-distribution inputs (OODs). This limitation is\none of the key challenges in the adoption of DNNs in high-assurance systems\nsuch as autonomous driving, air traffic management, and medical diagnosis. This\nchallenge has received significant attention recently, and several techniques\nhave been developed to detect inputs where the model's prediction cannot be\ntrusted. These techniques detect OODs as datapoints with either high epistemic\nuncertainty or high aleatoric uncertainty. We demonstrate the difference in the\ndetection ability of these techniques and propose an ensemble approach for\ndetection of OODs as datapoints with high uncertainty (epistemic or aleatoric).\nWe perform experiments on vision datasets with multiple DNN architectures,\nachieving state-of-the-art results in most cases.",
          "link": "http://arxiv.org/abs/2108.06380",
          "publishedOn": "2021-08-17T01:54:49.446Z",
          "wordCount": 572,
          "title": "Detecting OODs as datapoints with High Uncertainty. (arXiv:2108.06380v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08434",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kotsalis_G/0/1/0/all/0/1\">Georgios Kotsalis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1\">Guanghui Lan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>",
          "description": "The focus of this paper is on stochastic variational inequalities (VI) under\nMarkovian noise. A prominent application of our algorithmic developments is the\nstochastic policy evaluation problem in reinforcement learning. Prior\ninvestigations in the literature focused on temporal difference (TD) learning\nby employing nonsmooth finite time analysis motivated by stochastic subgradient\ndescent leading to certain limitations. These encompass the requirement of\nanalyzing a modified TD algorithm that involves projection to an a-priori\ndefined Euclidean ball, achieving a non-optimal convergence rate and no clear\nway of deriving the beneficial effects of parallel implementation. Our approach\nremedies these shortcomings in the broader context of stochastic VIs and in\nparticular when it comes to stochastic policy evaluation. We developed a\nvariety of simple TD learning type algorithms motivated by its original version\nthat maintain its simplicity, while offering distinct advantages from a\nnon-asymptotic analysis point of view. We first provide an improved analysis of\nthe standard TD algorithm that can benefit from parallel implementation. Then\nwe present versions of a conditional TD algorithm (CTD), that involves periodic\nupdates of the stochastic iterates, which reduce the bias and therefore exhibit\nimproved iteration complexity. This brings us to the fast TD (FTD) algorithm\nwhich combines elements of CTD and the stochastic operator extrapolation method\nof the companion paper. For a novel index resetting policy FTD exhibits the\nbest known convergence rate. We also devised a robust version of the algorithm\nthat is particularly suitable for discounting factors close to 1.",
          "link": "http://arxiv.org/abs/2011.08434",
          "publishedOn": "2021-08-17T01:54:49.418Z",
          "wordCount": 750,
          "title": "Simple and optimal methods for stochastic variational inequalities, II: Markovian noise and policy evaluation in reinforcement learning. (arXiv:2011.08434v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheibani_M/0/1/0/all/0/1\">Mohamadreza Sheibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_G/0/1/0/all/0/1\">Ge Ou</a>",
          "description": "The abundance of training data is not guaranteed in various supervised\nlearning applications. One of these situations is the post-earthquake regional\ndamage assessment of buildings. Querying the damage label of each building\nrequires a thorough inspection by experts, and thus, is an expensive task. A\npractical approach is to sample the most informative buildings in a sequential\nlearning scheme. Active learning methods recommend the most informative cases\nthat are able to maximally reduce the generalization error. The information\ntheoretic measure of mutual information (MI) is one of the most effective\ncriteria to evaluate the effectiveness of the samples in a pool-based sample\nselection scenario. However, the computational complexity of the standard MI\nalgorithm prevents the utilization of this method on large datasets. A local\nkernels strategy was proposed to reduce the computational costs, but the\nadaptability of the kernels to the observed labels was not considered in the\noriginal formulation of this strategy. In this article, an adaptive local\nkernels methodology is developed that allows for the conformability of the\nkernels to the observed output data while enhancing the computational\ncomplexity of the standard MI algorithm. The proposed algorithm is developed to\nwork on a Gaussian process regression (GPR) framework, where the kernel\nhyperparameters are updated after each label query using the maximum likelihood\nestimation. In the sequential learning procedure, the updated hyperparameters\ncan be used in the MI kernel matrices to improve the sample suggestion\nperformance. The advantages are demonstrated on a simulation of the 2018\nAnchorage, AK, earthquake. It is shown that while the proposed algorithm\nenables GPR to reach acceptable performance with fewer training data, the\ncomputational demands remain lower than the standard local kernels strategy.",
          "link": "http://arxiv.org/abs/2105.11492",
          "publishedOn": "2021-08-17T01:54:49.388Z",
          "wordCount": 776,
          "title": "Adaptive Local Kernels Formulation of Mutual Information with Application to Active Post-Seismic Building Damage Inference. (arXiv:2105.11492v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiqiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>",
          "description": "Traditional normalization techniques (e.g., Batch Normalization and Instance\nNormalization) generally and simplistically assume that training and test data\nfollow the same distribution. As distribution shifts are inevitable in\nreal-world applications, well-trained models with previous normalization\nmethods can perform badly in new environments. Can we develop new normalization\nmethods to improve generalization robustness under distribution shifts? In this\npaper, we answer the question by proposing CrossNorm and SelfNorm. CrossNorm\nexchanges channel-wise mean and variance between feature maps to enlarge\ntraining distribution, while SelfNorm uses attention to recalibrate the\nstatistics to bridge gaps between training and test distributions. CrossNorm\nand SelfNorm can complement each other, though exploring different directions\nin statistics usage. Extensive experiments on different fields (vision and\nlanguage), tasks (classification and segmentation), settings (supervised and\nsemi-supervised), and distribution shift types (synthetic and natural) show the\neffectiveness. Code is available at\nhttps://github.com/amazon-research/crossnorm-selfnorm",
          "link": "http://arxiv.org/abs/2102.02811",
          "publishedOn": "2021-08-17T01:54:49.365Z",
          "wordCount": 621,
          "title": "CrossNorm and SelfNorm for Generalization under Distribution Shifts. (arXiv:2102.02811v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Ashwin Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskar_A/0/1/0/all/0/1\">A Baskar</a>",
          "description": "We present a general technique for constructing Graph Neural Networks (GNNs)\ncapable of using multi-relational domain knowledge. The technique is based on\nmode-directed inverse entailment (MDIE) developed in Inductive Logic\nProgramming (ILP). Given a data instance $e$ and background knowledge $B$, MDIE\nidentifies a most-specific logical formula $\\bot_B(e)$ that contains all the\nrelational information in $B$ that is related to $e$. We represent $\\bot_B(e)$\nby a \"bottom-graph\" that can be converted into a form suitable for GNN\nimplementations. This transformation allows a principled way of incorporating\ngeneric background knowledge into GNNs: we use the term `BotGNN' for this form\nof graph neural networks. For several GNN variants, using real-world datasets\nwith substantial background knowledge, we show that BotGNNs perform\nsignificantly better than both GNNs without background knowledge and a recently\nproposed simplified technique for including domain knowledge into GNNs. We also\nprovide experimental evidence comparing BotGNNs favourably to multi-layer\nperceptrons (MLPs) that use features representing a \"propositionalised\" form of\nthe background knowledge; and BotGNNs to a standard ILP based on the use of\nmost-specific clauses. Taken together, these results point to BotGNNs as\ncapable of combining the computational efficacy of GNNs with the\nrepresentational versatility of ILP.",
          "link": "http://arxiv.org/abs/2105.10709",
          "publishedOn": "2021-08-17T01:54:49.359Z",
          "wordCount": 683,
          "title": "Inclusion of Domain-Knowledge into GNNs using Mode-Directed Inverse Entailment. (arXiv:2105.10709v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neu_G/0/1/0/all/0/1\">Gergely Neu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1\">Gintare Karolina Dziugaite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghifam_M/0/1/0/all/0/1\">Mahdi Haghifam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Daniel M. Roy</a>",
          "description": "We study the generalization properties of the popular stochastic optimization\nmethod known as stochastic gradient descent (SGD) for optimizing general\nnon-convex loss functions. Our main contribution is providing upper bounds on\nthe generalization error that depend on local statistics of the stochastic\ngradients evaluated along the path of iterates calculated by SGD. The key\nfactors our bounds depend on are the variance of the gradients (with respect to\nthe data distribution) and the local smoothness of the objective function along\nthe SGD path, and the sensitivity of the loss function to perturbations to the\nfinal output. Our key technical tool is combining the information-theoretic\ngeneralization bounds previously used for analyzing randomized variants of SGD\nwith a perturbation analysis of the iterates.",
          "link": "http://arxiv.org/abs/2102.00931",
          "publishedOn": "2021-08-17T01:54:49.285Z",
          "wordCount": 594,
          "title": "Information-Theoretic Generalization Bounds for Stochastic Gradient Descent. (arXiv:2102.00931v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhizhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1\">Pengfei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>",
          "description": "Community detection, a fundamental task for network analysis, aims to\npartition a network into multiple sub-structures to help reveal their latent\nfunctions. Community detection has been extensively studied in and broadly\napplied to many real-world network problems. Classical approaches to community\ndetection typically utilize probabilistic graphical models and adopt a variety\nof prior knowledge to infer community structures. As the problems that network\nmethods try to solve and the network data to be analyzed become increasingly\nmore sophisticated, new approaches have also been proposed and developed,\nparticularly those that utilize deep learning and convert networked data into\nlow dimensional representation. Despite all the recent advancement, there is\nstill a lack of insightful understanding of the theoretical and methodological\nunderpinning of community detection, which will be critically important for\nfuture development of the area of network analysis. In this paper, we develop\nand present a unified architecture of network community-finding methods to\ncharacterize the state-of-the-art of the field of community detection.\nSpecifically, we provide a comprehensive review of the existing community\ndetection methods and introduce a new taxonomy that divides the existing\nmethods into two categories, namely probabilistic graphical model and deep\nlearning. We then discuss in detail the main idea behind each method in the two\ncategories. Furthermore, to promote future development of community detection,\nwe release several benchmark datasets from several problem domains and\nhighlight their applications to various network analysis tasks. We conclude\nwith discussions of the challenges of the field and suggestions of possible\ndirections for future research.",
          "link": "http://arxiv.org/abs/2101.01669",
          "publishedOn": "2021-08-17T01:54:49.251Z",
          "wordCount": 757,
          "title": "A Survey of Community Detection Approaches: From Statistical Modeling to Deep Learning. (arXiv:2101.01669v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.11897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>",
          "description": "State-of-the-art methods in image-to-image translation are capable of\nlearning a mapping from a source domain to a target domain with unpaired image\ndata. Though the existing methods have achieved promising results, they still\nproduce visual artifacts, being able to translate low-level information but not\nhigh-level semantics of input images. One possible reason is that generators do\nnot have the ability to perceive the most discriminative parts between the\nsource and target domains, thus making the generated images low quality. In\nthis paper, we propose a new Attention-Guided Generative Adversarial Networks\n(AttentionGAN) for the unpaired image-to-image translation task. AttentionGAN\ncan identify the most discriminative foreground objects and minimize the change\nof the background. The attention-guided generators in AttentionGAN are able to\nproduce attention masks, and then fuse the generation output with the attention\nmasks to obtain high-quality target images. Accordingly, we also design a novel\nattention-guided discriminator which only considers attended regions. Extensive\nexperiments are conducted on several generative tasks with eight public\ndatasets, demonstrating that the proposed method is effective to generate\nsharper and more realistic images compared with existing competitive models.\nThe code is available at https://github.com/Ha0Tang/AttentionGAN.",
          "link": "http://arxiv.org/abs/1911.11897",
          "publishedOn": "2021-08-17T01:54:49.238Z",
          "wordCount": 722,
          "title": "AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks. (arXiv:1911.11897v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guo-Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yifan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Knowledge distillation (KD) is a popular method to train efficient networks\n(\"student\") with the help of high-capacity networks (\"teacher\"). Traditional\nmethods use the teacher's soft logits as extra supervision to train the student\nnetwork. In this paper, we argue that it is more advantageous to make the\nstudent mimic the teacher's features in the penultimate layer. Not only the\nstudent can directly learn more effective information from the teacher feature,\nfeature mimicking can also be applied for teachers trained without a softmax\nlayer. Experiments show that it can achieve higher accuracy than traditional\nKD. To further facilitate feature mimicking, we decompose a feature vector into\nthe magnitude and the direction. We argue that the teacher should give more\nfreedom to the student feature's magnitude, and let the student pay more\nattention on mimicking the feature direction. To meet this requirement, we\npropose a loss term based on locality-sensitive hashing (LSH). With the help of\nthis new loss, our method indeed mimics feature directions more accurately,\nrelaxes constraints on feature magnitudes, and achieves state-of-the-art\ndistillation accuracy. We provide theoretical analyses of how LSH facilitates\nfeature direction mimicking, and further extend feature mimicking to\nmulti-label recognition and object detection.",
          "link": "http://arxiv.org/abs/2011.01424",
          "publishedOn": "2021-08-17T01:54:49.222Z",
          "wordCount": 664,
          "title": "Distilling Knowledge by Mimicking Features. (arXiv:2011.01424v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanchirico_M/0/1/0/all/0/1\">Mauro J. Sanchirico III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_X/0/1/0/all/0/1\">Xun Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nataraj_C/0/1/0/all/0/1\">C. Nataraj</a>",
          "description": "Polynomial expansions are important in the analysis of neural network\nnonlinearities. They have been applied thereto addressing well-known\ndifficulties in verification, explainability, and security. Existing approaches\nspan classical Taylor and Chebyshev methods, asymptotics, and many numerical\napproaches. We find that while these individually have useful properties such\nas exact error formulas, adjustable domain, and robustness to undefined\nderivatives, there are no approaches that provide a consistent method yielding\nan expansion with all these properties. To address this, we develop an\nanalytically modified integral transform expansion (AMITE), a novel expansion\nvia integral transforms modified using derived criteria for convergence. We\nshow the general expansion and then demonstrate application for two popular\nactivation functions, hyperbolic tangent and rectified linear units. Compared\nwith existing expansions (i.e., Chebyshev, Taylor, and numerical) employed to\nthis end, AMITE is the first to provide six previously mutually exclusive\ndesired expansion properties such as exact formulas for the coefficients and\nexact expansion errors (Table II). We demonstrate the effectiveness of AMITE in\ntwo case studies. First, a multivariate polynomial form is efficiently\nextracted from a single hidden layer black-box MLP to facilitate equivalence\ntesting from noisy stimulus-response pairs. Second, a variety of FFNN\narchitectures having between 3 and 7 layers are range bounded using Taylor\nmodels improved by the AMITE polynomials and error formulas. AMITE presents a\nnew dimension of expansion methods suitable for analysis/approximation of\nnonlinearities in neural networks, opening new directions and opportunities for\nthe theoretical analysis and systematic testing of neural networks.",
          "link": "http://arxiv.org/abs/2007.06226",
          "publishedOn": "2021-08-17T01:54:49.159Z",
          "wordCount": 752,
          "title": "AMITE: A Novel Polynomial Expansion for Analyzing Neural Network Nonlinearities. (arXiv:2007.06226v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.06814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>",
          "description": "As billions of personal data being shared through social media and network,\nthe data privacy and security have drawn an increasing attention. Several\nattempts have been made to alleviate the leakage of identity information from\nface photos, with the aid of, e.g., image obfuscation techniques. However, most\nof the present results are either perceptually unsatisfactory or ineffective\nagainst face recognition systems. Our goal in this paper is to develop a\ntechnique that can encrypt the personal photos such that they can protect users\nfrom unauthorized face recognition systems but remain visually identical to the\noriginal version for human beings. To achieve this, we propose a targeted\nidentity-protection iterative method (TIP-IM) to generate adversarial identity\nmasks which can be overlaid on facial images, such that the original identities\ncan be concealed without sacrificing the visual quality. Extensive experiments\ndemonstrate that TIP-IM provides 95\\%+ protection success rate against various\nstate-of-the-art face recognition models under practical test scenarios.\nBesides, we also show the practical and effective applicability of our method\non a commercial API service.",
          "link": "http://arxiv.org/abs/2003.06814",
          "publishedOn": "2021-08-17T01:54:49.153Z",
          "wordCount": 659,
          "title": "Towards Face Encryption by Generating Adversarial Identity Masks. (arXiv:2003.06814v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jia Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1\">Andre Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jingxuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuli_M/0/1/0/all/0/1\">Mathieu Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "Neural Architecture Search (NAS) has shifted network design from using human\nintuition to leveraging search algorithms guided by evaluation metrics. We\nstudy channel size optimization in convolutional neural networks (CNN) and\nidentify the role it plays in model accuracy and complexity. Current channel\nsize selection methods are generally limited by discrete sample spaces while\nsuffering from manual iteration and simple heuristics. To solve this, we\nintroduce an efficient dynamic scaling algorithm -- CONet -- that automatically\noptimizes channel sizes across network layers for a given CNN. Two metrics --\n``\\textit{Rank}\" and \"\\textit{Rank Average Slope}\" -- are introduced to\nidentify the information accumulated in training. The algorithm dynamically\nscales channel sizes up or down over a fixed searching phase. We conduct\nexperiments on CIFAR10/100 and ImageNet datasets and show that CONet can find\nefficient and accurate architectures searched in ResNet, DARTS, and DARTS+\nspaces that outperform their baseline models.",
          "link": "http://arxiv.org/abs/2108.06822",
          "publishedOn": "2021-08-17T01:54:49.024Z",
          "wordCount": 609,
          "title": "CONet: Channel Optimization for Convolutional Neural Networks. (arXiv:2108.06822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>",
          "description": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.",
          "link": "http://arxiv.org/abs/2108.03857",
          "publishedOn": "2021-08-17T01:54:48.706Z",
          "wordCount": 613,
          "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04085",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bonneville_C/0/1/0/all/0/1\">Christophe Bonneville</a>, <a href=\"http://arxiv.org/find/math/1/au:+Earls_C/0/1/0/all/0/1\">Christopher J. Earls</a>",
          "description": "Scientific machine learning has been successfully applied to inverse problems\nand PDE discoveries in computational physics. One caveat of current methods\nhowever is the need for large amounts of (clean) data in order to recover full\nsystem responses or underlying physical models. Bayesian methods may be\nparticularly promising to overcome these challenges as they are naturally less\nsensitive to sparse and noisy data. In this paper, we propose to use Bayesian\nneural networks (BNN) in order to: 1) Recover the full system states from\nmeasurement data (e.g. temperature, velocity field, etc.). We use Hamiltonian\nMonte-Carlo to sample the posterior distribution of a deep and dense BNN, and\nshow that it is possible to accurately capture physics of varying complexity\nwithout overfitting. 2) Recover the parameters in the underlying partial\ndifferential equation (PDE) governing the physical system. Using the trained\nBNN as a surrogate of the system response, we generate datasets of derivatives\npotentially comprising the latent PDE of the observed system and perform a\nBayesian linear regression (BLR) between the successive derivatives in space\nand time to recover the original PDE parameters. We take advantage of the\nconfidence intervals on the BNN outputs and introduce the spatial derivative\nvariance into the BLR likelihood to discard the influence of highly uncertain\nsurrogate data points, which allows for more accurate parameter discovery. We\ndemonstrate our approach on a handful of example applied to physics and\nnon-linear dynamics.",
          "link": "http://arxiv.org/abs/2108.04085",
          "publishedOn": "2021-08-17T01:54:48.609Z",
          "wordCount": 696,
          "title": "Bayesian Deep Learning for Partial Differential Equation Parameter Discovery with Sparse and Noisy Data. (arXiv:2108.04085v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rundi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changxi Zheng</a>",
          "description": "Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation --- describing a shape\nas a sequence of computer-aided design (CAD) operations. Unlike meshes and\npoint clouds, CAD models encode the user creation process of 3D shapes, widely\nused in numerous industrial and engineering design tasks. However, the\nsequential and irregular structure of CAD operations poses significant\nchallenges for existing 3D generative models. Drawing an analogy between CAD\noperations and natural language, we propose a CAD generative network based on\nthe Transformer. We demonstrate the performance of our model for both shape\nautoencoding and random shape generation. To train our network, we create a new\nCAD dataset consisting of 178,238 models and their CAD construction sequences.\nWe have made this dataset publicly available to promote future research on this\ntopic.",
          "link": "http://arxiv.org/abs/2105.09492",
          "publishedOn": "2021-08-17T01:54:48.520Z",
          "wordCount": 649,
          "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wycoff_N/0/1/0/all/0/1\">Nathan Wycoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getty_N/0/1/0/all/0/1\">Neil Getty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1\">Rick Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fangfang Xia</a>",
          "description": "The field of neuromorphic computing is in a period of active exploration.\nWhile many tools have been developed to simulate neuronal dynamics or convert\ndeep networks to spiking models, general software libraries for learning rules\nremain underexplored. This is partly due to the diverse, challenging nature of\nefforts to design new learning rules, which range from encoding methods to\ngradient approximations, from population approaches that mimic the Bayesian\nbrain to constrained learning algorithms deployed on memristor crossbars. To\naddress this gap, we present Neko, a modular, extensible library with a focus\non aiding the design of new learning algorithms. We demonstrate the utility of\nNeko in three exemplar cases: online local learning, probabilistic learning,\nand analog on-device learning. Our results show that Neko can replicate the\nstate-of-the-art algorithms and, in one case, lead to significant\noutperformance in accuracy and speed. Further, it offers tools including\ngradient comparison that can help develop new algorithmic variants. Neko is an\nopen source Python library that supports PyTorch and TensorFlow backends.",
          "link": "http://arxiv.org/abs/2105.00324",
          "publishedOn": "2021-08-17T01:54:48.510Z",
          "wordCount": 646,
          "title": "Neko: a Library for Exploring Neuromorphic Learning Rules. (arXiv:2105.00324v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melodia_L/0/1/0/all/0/1\">Luciano Melodia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_R/0/1/0/all/0/1\">Richard Lenz</a>",
          "description": "In this paper, we use topological data analysis techniques to construct a\nsuitable neural network classifier for the task of learning sensor signals of\nentire power plants according to their reference designation system. We use\nrepresentations of persistence diagrams to derive necessary preprocessing steps\nand visualize the large amounts of data. We derive deep architectures with\none-dimensional convolutional layers combined with stacked long short-term\nmemories as residual networks suitable for processing the persistence features.\nWe combine three separate sub-networks, obtaining as input the time series\nitself and a representation of the persistent homology for the zeroth and first\ndimension. We give a mathematical derivation for most of the used\nhyper-parameters. For validation, numerical experiments were performed with\nsensor data from four power plants of the same construction type.",
          "link": "http://arxiv.org/abs/2106.02493",
          "publishedOn": "2021-08-17T01:54:48.493Z",
          "wordCount": 601,
          "title": "Homological Time Series Analysis of Sensor Signals from Power Plants. (arXiv:2106.02493v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huafeng Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chonggang Lu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhimin Hu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaodong Yuan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingshu Zhang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanquan Liu</a> (3) ((1) School of Information, North China University of Technology,(2) Department of Neurology, Kailuan General Hospital, Tangshan,(3) School of Intelligent Systems Engineering, Sun Yat-sen University)",
          "description": "Sleep staging plays an important role on the diagnosis of sleep disorders. In\ngeneral, experts classify sleep stages manually based on polysomnography (PSG),\nwhich is quite time-consuming. Meanwhile, the acquisition process of multiple\nsignals is much complex, which can affect the subject's sleep. Therefore, the\nuse of single-channel electroencephalogram (EEG) for automatic sleep staging\nhas become a popular research topic. In the literature, a large number of sleep\nstaging methods based on single-channel EEG have been proposed with promising\nresults and achieve the preliminary automation of sleep staging. However, the\nperformance for most of these methods in the N1 stage do not satisfy the needs\nof the diagnosis. In this paper, we propose a deep learning model multi scale\ndual attention network(MSDAN) based on raw EEG, which utilizes multi-scale\nconvolution to extract features in different waveforms contained in the EEG\nsignal, connects channel attention and spatial attention mechanisms in series\nto filter and highlight key information, and uses soft thresholding to remove\nredundant information. Experiments were conducted using two datasets with\n5-fold cross-validation and hold-out validation method. The final average\naccuracy, overall accuracy, macro F1 score and Cohen's Kappa coefficient of the\nmodel reach 96.70%, 91.74%, 0.8231 and 0.8723 on the Sleep-EDF dataset, 96.14%,\n90.35%, 0.7945 and 0.8284 on the Sleep-EDFx dataset. Significantly, our model\nperformed superiorly in the N1 stage, with F1 scores of 54.41% and 52.79% on\nthe two datasets respectively. The results show the superiority of our network\nover the existing methods, reaching a new state-of-the-art. In particular, the\nproposed method achieves excellent results in the N1 sleep stage compared to\nother methods.",
          "link": "http://arxiv.org/abs/2107.08442",
          "publishedOn": "2021-08-17T01:54:48.475Z",
          "wordCount": 768,
          "title": "Sleep Staging Based on Multi Scale Dual Attention Network. (arXiv:2107.08442v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13020",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pena_J/0/1/0/all/0/1\">Jose M. Pe&#xf1;a</a>",
          "description": "We present a method for assessing the sensitivity of the true causal effect\nto unmeasured confounding. The method requires the analyst to set two intuitive\nparameters. Otherwise, the method is assumption-free. The method returns an\ninterval that contains the true causal effect, and whose bounds are sharp, i.e.\nattainable. We show experimentally that our bounds can be sharper than those\nobtained by the method of Ding and VanderWeele (2016a) which, moreover,\nrequires to set one more parameter than our method. Finally, we extend our\nmethod to bound the natural direct and indirect effects when there are measured\nmediators and unmeasured exposure-outcome confounding.",
          "link": "http://arxiv.org/abs/2104.13020",
          "publishedOn": "2021-08-17T01:54:48.469Z",
          "wordCount": 560,
          "title": "Simple yet Sharp Sensitivity Analysis for Unmeasured Confounding. (arXiv:2104.13020v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao-Hua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>",
          "description": "While deep reinforcement learning has achieved promising results in\nchallenging decision-making tasks, the main bones of its success --- deep\nneural networks are mostly black-boxes. A feasible way to gain insight into a\nblack-box model is to distill it into an interpretable model such as a decision\ntree, which consists of if-then rules and is easy to grasp and be verified.\nHowever, the traditional model distillation is usually a supervised learning\ntask under a stationary data distribution assumption, which is violated in\nreinforcement learning. Therefore, a typical policy distillation that clones\nmodel behaviors with even a small error could bring a data distribution shift,\nresulting in an unsatisfied distilled policy model with low fidelity or low\nperformance. In this paper, we propose to address this issue by changing the\ndistillation objective from behavior cloning to maximizing an advantage\nevaluation. The novel distillation objective maximizes an approximated\ncumulative reward and focuses more on disastrous behaviors in critical states,\nwhich controls the data shift effect. We evaluate our method on several Gym\ntasks, a commercial fight game, and a self-driving car simulator. The empirical\nresults show that the proposed method can preserve a higher cumulative reward\nthan behavior cloning and learn a more consistent policy to the original one.\nMoreover, by examining the extracted rules from the distilled decision trees,\nwe demonstrate that the proposed method delivers reasonable and robust\ndecisions.",
          "link": "http://arxiv.org/abs/2108.06898",
          "publishedOn": "2021-08-17T01:54:48.452Z",
          "wordCount": 660,
          "title": "Neural-to-Tree Policy Distillation with Policy Improvement Criterion. (arXiv:2108.06898v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08250",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Anderer_M/0/1/0/all/0/1\">Matthias Anderer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>",
          "description": "Hierarchical forecasting with intermittent time series is a challenge in both\nresearch and empirical studies. Vast research focuses on improving the accuracy\nof each hierarchy, especially the intermittent time series at bottom levels. It\nthen reconciles forecasts at each hierarchy to further improve the overall\nperformance. In this paper, we present a hierarchical forecasting approach that\ntreats the bottom level forecasts as mutable to ensure higher forecasting\naccuracy on the upper levels of the hierarchy. We employ a pure deep learning\nforecasting approach N-BEATS for continuous time series on top levels and a\nwidely used tree-based algorithm LightGBM for the bottom level intermittent\ntime series. The hierarchical forecasting with alignment approach is a simple\nyet effective variant of the bottom-up method, which accounts for biases that\nare difficult to observe at the bottom level. It allows suboptimal forecasts at\nthe lower level to retain a higher overall performance. The approach in this\nempirical study was developed by the first author during the M5 Forecasting\nAccuracy competition, ranking second place. The approach is also business\norientated and could be beneficial for business strategic planning.",
          "link": "http://arxiv.org/abs/2103.08250",
          "publishedOn": "2021-08-17T01:54:48.446Z",
          "wordCount": 637,
          "title": "Hierarchical forecasting with a top-down alignment of independent level forecasts. (arXiv:2103.08250v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07736",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jin_D/0/1/0/all/0/1\">Dian Jin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bing_X/0/1/0/all/0/1\">Xin Bing</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqian Zhang</a>",
          "description": "The problem of finding the unique low dimensional decomposition of a given\nmatrix has been a fundamental and recurrent problem in many areas. In this\npaper, we study the problem of seeking a unique decomposition of a low rank\nmatrix $Y\\in \\mathbb{R}^{p\\times n}$ that admits a sparse representation.\nSpecifically, we consider $Y = A X\\in \\mathbb{R}^{p\\times n}$ where the matrix\n$A\\in \\mathbb{R}^{p\\times r}$ has full column rank, with $r < \\min\\{n,p\\}$, and\nthe matrix $X\\in \\mathbb{R}^{r\\times n}$ is element-wise sparse. We prove that\nthis sparse decomposition of $Y$ can be uniquely identified, up to some\nintrinsic signed permutation. Our approach relies on solving a nonconvex\noptimization problem constrained over the unit sphere. Our geometric analysis\nfor the nonconvex optimization landscape shows that any {\\em strict} local\nsolution is close to the ground truth solution, and can be recovered by a\nsimple data-driven initialization followed with any second order descent\nalgorithm. At last, we corroborate these theoretical results with numerical\nexperiments.",
          "link": "http://arxiv.org/abs/2106.07736",
          "publishedOn": "2021-08-17T01:54:48.435Z",
          "wordCount": 620,
          "title": "Unique sparse decomposition of low rank matrices. (arXiv:2106.07736v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1\">Rajarshi Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldsmith_A/0/1/0/all/0/1\">Andrea J. Goldsmith</a>",
          "description": "The communication cost of distributed optimization algorithms is a major\nbottleneck in their scalability. This work considers a parameter-server setting\nin which the worker is constrained to communicate information to the server\nusing only $R$ bits per dimension. We show that $\\mathbf{democratic}$\n$\\mathbf{embeddings}$ from random matrix theory are significantly useful for\ndesigning efficient and optimal vector quantizers that respect this bit budget.\nThe resulting polynomial complexity source coding schemes are used to design\ndistributed optimization algorithms with convergence rates matching the minimax\noptimal lower bounds for (i) Smooth and Strongly-Convex objectives with access\nto an Exact Gradient oracle, as well as (ii) General Convex and Non-Smooth\nobjectives with access to a Noisy Subgradient oracle. We further propose a\nrelaxation of this coding scheme which is nearly minimax optimal. Numerical\nsimulations validate our theoretical claims.",
          "link": "http://arxiv.org/abs/2103.07578",
          "publishedOn": "2021-08-17T01:54:48.424Z",
          "wordCount": 618,
          "title": "Democratic Source Coding: An Optimal Fixed-Length Quantization Scheme for Distributed Optimization Under Communication Constraints. (arXiv:2103.07578v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armenta_M/0/1/0/all/0/1\">Marco Armenta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judge_T/0/1/0/all/0/1\">Thierry Judge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Painchaud_N/0/1/0/all/0/1\">Nathan Painchaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemaire_C/0/1/0/all/0/1\">Carl Lemaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_G/0/1/0/all/0/1\">Gabriel Gibeau Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spino_P/0/1/0/all/0/1\">Philippe Spino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>",
          "description": "In this paper, we explore a process called neural teleportation, a\nmathematical consequence of applying quiver representation theory to neural\nnetworks. Neural teleportation \"teleports\" a network to a new position in the\nweight space and preserves its function. This phenomenon comes directly from\nthe definitions of representation theory applied to neural networks and it\nturns out to be a very simple operation that has remarkable properties. We shed\nlight on surprising and counter-intuitive consequences neural teleportation has\non the loss landscape. In particular, we show that teleportation can be used to\nexplore loss level curves, that it changes the local loss landscape, sharpens\nglobal minima and boosts back-propagated gradients at any moment during the\nlearning process. Our results can be reproduced with the code available here:\nhttps://github.com/vitalab/neuralteleportation",
          "link": "http://arxiv.org/abs/2012.01118",
          "publishedOn": "2021-08-17T01:54:48.406Z",
          "wordCount": 602,
          "title": "Neural Teleportation. (arXiv:2012.01118v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12066",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yanming Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chunyan Wang</a>",
          "description": "The work presented in this paper is to propose a reliable high-quality system\nof Convolutional Neural Network (CNN) for brain tumor segmentation with a low\ncomputation requirement. The system consists of a CNN for the main processing\nfor the segmentation, a pre-CNN block for data reduction and post-CNN\nrefinement block. The unique CNN consists of 7 convolution layers involving\nonly 108 kernels and 20308 trainable parameters. It is custom-designed,\nfollowing the proposed paradigm of ASCNN (application specific CNN), to perform\nmono-modality and cross-modality feature extraction, tumor localization and\npixel classification. Each layer fits the task assigned to it, by means of (i)\nappropriate normalization applied to its input data, (ii) correct convolution\nmodes for the assigned task, and (iii) suitable nonlinear transformation to\noptimize the convolution results. In this specific design context, the number\nof kernels in each of the 7 layers is made to be just-sufficient for its task,\ninstead of exponentially growing over the layers, to increase information\ndensity and to reduce randomness in the processing. The proposed activation\nfunction Full-ReLU helps to halve the number of kernels in convolution layers\nof high-pass filtering without degrading processing quality. A large number of\nexperiments with BRATS2018 dataset have been conducted to measure the\nprocessing quality and reproducibility of the proposed system. The results\ndemonstrate that the system reproduces reliably almost the same output to the\nsame input after retraining. The mean dice scores for enhancing tumor, whole\ntumor and tumor core are 77.2%, 89.2% and 76.3%, respectively. The simple\nstructure and reliable high processing quality of the proposed system will\nfacilitate its implementation and medical applications.",
          "link": "http://arxiv.org/abs/2007.12066",
          "publishedOn": "2021-08-17T01:54:48.400Z",
          "wordCount": 739,
          "title": "A Computation-Efficient CNN System for High-Quality Brain Tumor Segmentation. (arXiv:2007.12066v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11434",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gammelli_D/0/1/0/all/0/1\">Daniele Gammelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_K/0/1/0/all/0/1\">Kaidi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harrison_J/0/1/0/all/0/1\">James Harrison</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_F/0/1/0/all/0/1\">Filipe Rodrigues</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_F/0/1/0/all/0/1\">Francisco C. Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>",
          "description": "Autonomous mobility-on-demand (AMoD) systems represent a rapidly developing\nmode of transportation wherein travel requests are dynamically handled by a\ncoordinated fleet of robotic, self-driving vehicles. Given a graph\nrepresentation of the transportation network - one where, for example, nodes\nrepresent areas of the city, and edges the connectivity between them - we argue\nthat the AMoD control problem is naturally cast as a node-wise decision-making\nproblem. In this paper, we propose a deep reinforcement learning framework to\ncontrol the rebalancing of AMoD systems through graph neural networks.\nCrucially, we demonstrate that graph neural networks enable reinforcement\nlearning agents to recover behavior policies that are significantly more\ntransferable, generalizable, and scalable than policies learned through other\napproaches. Empirically, we show how the learned policies exhibit promising\nzero-shot transfer capabilities when faced with critical portability tasks such\nas inter-city generalization, service area expansion, and adaptation to\npotentially complex urban topologies.",
          "link": "http://arxiv.org/abs/2104.11434",
          "publishedOn": "2021-08-17T01:54:48.394Z",
          "wordCount": 616,
          "title": "Graph Neural Network Reinforcement Learning for Autonomous Mobility-on-Demand Systems. (arXiv:2104.11434v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yantian Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1\">Siddhant Bhambri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1\">Lin Guan</a>",
          "description": "Conventional works that learn grasping affordance from demonstrations need to\nexplicitly predict grasping configurations, such as gripper approaching angles\nor grasping preshapes. Classic motion planners could then sample trajectories\nby using such predicted configurations. In this work, our goal is instead to\nfill the gap between affordance discovery and affordance-based policy learning\nby integrating the two objectives in an end-to-end imitation learning framework\nbased on deep neural networks. From a psychological perspective, there is a\nclose association between attention and affordance. Therefore, with an\nend-to-end neural network, we propose to learn affordance cues as visual\nattention that serves as a useful indicating signal of how a demonstrator\naccomplishes tasks, instead of explicitly modeling affordances. To achieve\nthis, we propose a contrastive learning framework that consists of a Siamese\nencoder and a trajectory decoder. We further introduce a coupled triplet loss\nto encourage the discovered affordance cues to be more affordance-relevant. Our\nexperimental results demonstrate that our model with the coupled triplet loss\nachieves the highest grasping success rate in a simulated robot environment.\nOur project website can be accessed at\nhttps://sites.google.com/asu.edu/affordance-aware-imitation/project.",
          "link": "http://arxiv.org/abs/2104.00878",
          "publishedOn": "2021-08-17T01:54:48.347Z",
          "wordCount": 660,
          "title": "Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping. (arXiv:2104.00878v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jurewicz_M/0/1/0/all/0/1\">Mateusz Jurewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromberg_Derczynski_L/0/1/0/all/0/1\">Leon Str&#xf8;mberg-Derczynski</a>",
          "description": "Machine learning on sets towards sequential output is an important and\nubiquitous task, with applications ranging from language modeling and\nmeta-learning to multi-agent strategy games and power grid optimization.\nCombining elements of representation learning and structured prediction, its\ntwo primary challenges include obtaining a meaningful, permutation invariant\nset representation and subsequently utilizing this representation to output a\ncomplex target permutation. This paper provides a comprehensive introduction to\nthe field as well as an overview of important machine learning methods tackling\nboth of these key challenges, with a detailed qualitative comparison of\nselected model architectures.",
          "link": "http://arxiv.org/abs/2103.09656",
          "publishedOn": "2021-08-17T01:54:48.332Z",
          "wordCount": 593,
          "title": "Set-to-Sequence Methods in Machine Learning: a Review. (arXiv:2103.09656v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1\">Marco Scutari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panero_F/0/1/0/all/0/1\">Francesca Panero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proissl_M/0/1/0/all/0/1\">Manuel Proissl</a>",
          "description": "In this paper we present a general framework for estimating regression models\nsubject to a user-defined level of fairness. We enforce fairness as a model\nselection step in which we choose the value of a ridge penalty to control the\neffect of sensitive attributes. We then estimate the parameters of the model\nconditional on the chosen penalty value. Our proposal is mathematically simple,\nwith a solution that is partly in closed form, and produces estimates of the\nregression coefficients that are intuitive to interpret as a function of the\nlevel of fairness. Furthermore, it is easily extended to generalised linear\nmodels, kernelised regression models and other penalties; and it can\naccommodate multiple definitions of fairness.\n\nWe compare our approach with the regression model from Komiyama et al.\n(2018), which implements a provably-optimal linear regression model; and with\nthe fair models from Zafar et al. (2019). We evaluate these approaches\nempirically on six different data sets, and we find that our proposal provides\nbetter goodness of fit and better predictive accuracy for the same level of\nfairness. In addition, we highlight a source of bias in the original\nexperimental evaluation in Komiyama et al. (2018).",
          "link": "http://arxiv.org/abs/2105.13817",
          "publishedOn": "2021-08-17T01:54:48.302Z",
          "wordCount": 655,
          "title": "Achieving Fairness with a Simple Ridge Penalty. (arXiv:2105.13817v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kataria_A/0/1/0/all/0/1\">Aman Kataria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Smarajit Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karar_V/0/1/0/all/0/1\">Vinod Karar</a>",
          "description": "A head tracker is a crucial part of the head mounted display systems, as it\ntracks the head of the pilot in the plane/cockpit simulator. The operational\nflaws of head trackers are also dependent on different environmental conditions\nlike different lighting conditions and stray light interference. In this\nletter, an optical tracker has been employed to gather the 6-DoF data of head\nmovements under different environmental conditions. Also, the effect of\ndifferent environmental conditions and variation in distance between the\nreceiver and optical transmitter on the 6-DoF data was analyzed.",
          "link": "http://arxiv.org/abs/2108.06606",
          "publishedOn": "2021-08-17T01:54:48.275Z",
          "wordCount": 562,
          "title": "Prediction Analysis of Optical Tracker Parameters using Machine Learning Approaches for efficient Head Tracking. (arXiv:2108.06606v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karpov_N/0/1/0/all/0/1\">Nikolai Karpov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>",
          "description": "Motivated by real-world applications such as fast fashion retailing and\nonline advertising, the Multinomial Logit Bandit (MNL-bandit) is a popular\nmodel in online learning and operations research, and has attracted much\nattention in the past decade. However, it is a bit surprising that pure\nexploration, a basic problem in bandit theory, has not been well studied in\nMNL-bandit so far. In this paper we give efficient algorithms for pure\nexploration in MNL-bandit. Our algorithms achieve instance-sensitive pull\ncomplexities. We also complement the upper bounds by an almost matching lower\nbound.",
          "link": "http://arxiv.org/abs/2012.01499",
          "publishedOn": "2021-08-17T01:54:48.268Z",
          "wordCount": 554,
          "title": "Instance-Sensitive Algorithms for Pure Exploration in Multinomial Logit Bandit. (arXiv:2012.01499v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1\">Doseok Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_L/0/1/0/all/0/1\">Lucas Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattar_M/0/1/0/all/0/1\">Manan Khattar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agwan_U/0/1/0/all/0/1\">Utkarsha Agwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadarajah_S/0/1/0/all/0/1\">Selvaprabuh Nadarajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanos_C/0/1/0/all/0/1\">Costas Spanos</a>",
          "description": "Our team is proposing to run a full-scale energy demand response experiment\nin an office building. Although this is an exciting endeavor which will provide\nvalue to the community, collecting training data for the reinforcement learning\nagent is costly and will be limited. In this work, we examine how offline\ntraining can be leveraged to minimize data costs (accelerate convergence) and\nprogram implementation costs. We present two approaches to doing so:\npretraining our model to warm start the experiment with simulated tasks, and\nusing a planning model trained to simulate the real world's rewards to the\nagent. We present results that demonstrate the utility of offline reinforcement\nlearning to efficient price-setting in the energy demand response problem.",
          "link": "http://arxiv.org/abs/2108.06594",
          "publishedOn": "2021-08-17T01:54:48.263Z",
          "wordCount": 568,
          "title": "Offline-Online Reinforcement Learning for Energy Pricing in Office Demand Response: Lowering Energy and Data Costs. (arXiv:2108.06594v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.01354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kenny Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pogue_A/0/1/0/all/0/1\">Alexandra Pogue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_B/0/1/0/all/0/1\">Brett T. Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agha_mohammadi_A/0/1/0/all/0/1\">Ali-akbar Agha-mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1\">Ankur Mehta</a>",
          "description": "Monocular depth inference has gained tremendous attention from researchers in\nrecent years and remains as a promising replacement for expensive\ntime-of-flight sensors, but issues with scale acquisition and implementation\noverhead still plague these systems. To this end, this work presents an\nunsupervised learning framework that is able to predict at-scale depth maps and\negomotion, in addition to camera intrinsics, from a sequence of monocular\nimages via a single network. Our method incorporates both spatial and temporal\ngeometric constraints to resolve depth and pose scale factors, which are\nenforced within the supervisory reconstruction loss functions at training time.\nOnly unlabeled stereo sequences are required for training the weights of our\nsingle-network architecture, which reduces overall implementation overhead as\ncompared to previous methods. Our results demonstrate strong performance when\ncompared to the current state-of-the-art on multiple sequences of the KITTI\ndriving dataset and can provide faster training times with its reduced network\ncomplexity.",
          "link": "http://arxiv.org/abs/2011.01354",
          "publishedOn": "2021-08-17T01:54:48.240Z",
          "wordCount": 656,
          "title": "Unsupervised Monocular Depth Learning with Integrated Intrinsics and Spatio-Temporal Constraints. (arXiv:2011.01354v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1\">Cheng Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>",
          "description": "This paper tackles the task of category-level pose estimation for garments.\nWith a near infinite degree of freedom, a garment's full configuration (i.e.,\nposes) is often described by the per-vertex 3D locations of its entire 3D\nsurface. However, garments are also commonly subject to extreme cases of\nself-occlusion, especially when folded or crumpled, making it challenging to\nperceive their full 3D surface. To address these challenges, we propose\nGarmentNets, where the key idea is to formulate the deformable object pose\nestimation problem as a shape completion task in the canonical space. This\ncanonical space is defined across garments instances within a category,\ntherefore, specifies the shared category-level pose. By mapping the observed\npartial surface to the canonical space and completing it in this space, the\noutput representation describes the garment's full configuration using a\ncomplete 3D mesh with the per-vertex canonical coordinate label. To properly\nhandle the thin 3D structure presented on garments, we proposed a novel 3D\nshape representation using the generalized winding number field. Experiments\ndemonstrate that GarmentNets is able to generalize to unseen garment instances\nand achieve significantly better performance compared to alternative\napproaches.",
          "link": "http://arxiv.org/abs/2104.05177",
          "publishedOn": "2021-08-17T01:54:48.234Z",
          "wordCount": 663,
          "title": "GarmentNets: Category-Level Pose Estimation for Garments via Canonical Space Shape Completion. (arXiv:2104.05177v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Learning from a limited number of samples is challenging since the learned\nmodel can easily become overfitted based on the biased distribution formed by\nonly a few training examples. In this paper, we calibrate the distribution of\nthese few-sample classes by transferring statistics from the classes with\nsufficient examples, then an adequate number of examples can be sampled from\nthe calibrated distribution to expand the inputs to the classifier. We assume\nevery dimension in the feature representation follows a Gaussian distribution\nso that the mean and the variance of the distribution can borrow from that of\nsimilar classes whose statistics are better estimated with an adequate number\nof samples. Our method can be built on top of off-the-shelf pretrained feature\nextractors and classification models without extra parameters. We show that a\nsimple logistic regression classifier trained using the features sampled from\nour calibrated distribution can outperform the state-of-the-art accuracy on two\ndatasets (~5% improvement on miniImageNet compared to the next best). The\nvisualization of these generated features demonstrates that our calibrated\ndistribution is an accurate estimation.",
          "link": "http://arxiv.org/abs/2101.06395",
          "publishedOn": "2021-08-17T01:54:48.222Z",
          "wordCount": 659,
          "title": "Free Lunch for Few-shot Learning: Distribution Calibration. (arXiv:2101.06395v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1\">Christian Anti&#x107;</a>",
          "description": "Analogy-making is at the core of human and artificial intelligence and\ncreativity. This paper introduces from first principles an abstract algebraic\nframework of analogical proportions of the form `$a$ is to $b$ what $c$ is to\n$d$' in the general setting of universal algebra. This enables us to compare\nmathematical objects possibly across different domains in a uniform way which\nis crucial for AI-systems. The main idea is to define solutions to analogical\nequations in terms of maximal sets of algebraic justifications, which amounts\nto deriving abstract terms of concrete elements from a `known' source domain\nwhich can then be instantiated in an `unknown' target domain to obtain\nanalogous elements. It turns out that our notion of analogical proportions has\nappealing mathematical properties. We compare our framework with two recently\nintroduced frameworks of analogical proportions from the literature in the\nconcrete domains of sets and numbers, and we show that in each case we either\ndisagree with the notion from the literature justified by some counter-example\nor we can show that our model yields strictly more solutions. As we construct\nour model from first principles using only elementary concepts of universal\nalgebra, and since our model questions some basic properties of analogical\nproportions presupposed in the literature, to convince the reader of the\nplausibility of our model we show that it can be naturally embedded into\nfirst-order logic via model-theoretic types, and prove that analogical\nproportions are compatible with structure-preserving mappings from that\nperspective. This provides strong evidence for its applicability. In a broader\nsense, this paper is a first step towards a theory of analogical reasoning and\nlearning systems with potential applications to fundamental AI-problems like\ncommonsense reasoning and computational learning and creativity.",
          "link": "http://arxiv.org/abs/2006.02854",
          "publishedOn": "2021-08-17T01:54:48.215Z",
          "wordCount": 784,
          "title": "Analogical Proportions. (arXiv:2006.02854v7 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Nanyang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qianxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao-Yun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhanxing Zhu</a>",
          "description": "Despite the empirical success in various domains, it has been revealed that\ndeep neural networks are vulnerable to maliciously perturbed input data that\nmuch degrade their performance. This is known as adversarial attacks. To\ncounter adversarial attacks, adversarial training formulated as a form of\nrobust optimization has been demonstrated to be effective. However, conducting\nadversarial training brings much computational overhead compared with standard\ntraining. In order to reduce the computational cost, we propose an annealing\nmechanism, Amata, to reduce the overhead associated with adversarial training.\nThe proposed Amata is provably convergent, well-motivated from the lens of\noptimal control theory and can be combined with existing acceleration methods\nto further enhance performance. It is demonstrated that on standard datasets,\nAmata can achieve similar or better robustness with around 1/3 to 1/2 the\ncomputational time compared with traditional methods. In addition, Amata can be\nincorporated into other adversarial training acceleration algorithms (e.g.\nYOPO, Free, Fast, and ATTA), which leads to further reduction in computational\ntime on large-scale problems.",
          "link": "http://arxiv.org/abs/2012.08112",
          "publishedOn": "2021-08-17T01:54:48.177Z",
          "wordCount": 634,
          "title": "Amata: An Annealing Mechanism for Adversarial Training Acceleration. (arXiv:2012.08112v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_B/0/1/0/all/0/1\">Bissan Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nathwani_J/0/1/0/all/0/1\">Jatin Nathwani</a>",
          "description": "The past decade has seen a rapid penetration of electric vehicles (EV) in the\nmarket, more and more logistics and transportation companies start to deploy\nEVs for service provision. In order to model the operations of a commercial EV\nfleet, we utilize the EV routing problem with time windows (EVRPTW). In this\nresearch, we propose an end-to-end deep reinforcement learning framework to\nsolve the EVRPTW. In particular, we develop an attention model incorporating\nthe pointer network and a graph embedding technique to parameterize a\nstochastic policy for solving the EVRPTW. The model is then trained using\npolicy gradient with rollout baseline. Our numerical studies show that the\nproposed model is able to efficiently solve EVRPTW instances of large sizes\nthat are not solvable with any existing approaches.",
          "link": "http://arxiv.org/abs/2010.02068",
          "publishedOn": "2021-08-17T01:54:48.164Z",
          "wordCount": 620,
          "title": "Deep Reinforcement Learning for Electric Vehicle Routing Problem with Time Windows. (arXiv:2010.02068v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_K/0/1/0/all/0/1\">Kaan Gokcesu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_H/0/1/0/all/0/1\">Hakan Gokcesu</a>",
          "description": "We investigate the problem of online learning, which has gained significant\nattention in recent years due to its applicability in a wide range of fields\nfrom machine learning to game theory. Specifically, we study the online\noptimization of mixable loss functions in a dynamic environment. We introduce\nonline mixture schemes that asymptotically achieves the performance of the best\ndynamic estimation sequence of the switching oracle with optimal regret\nredundancies. The best dynamic estimation sequence that we compete against is\nselected in hindsight with full observation of the loss functions and is\nallowed to select different optimal estimations in different time intervals\n(segments). We propose two mixtures in our work. Firstly, we propose a\ntractable polynomial time complexity algorithm that can achieve the optimal\nredundancy of the intractable brute force approach. Secondly, we propose an\nefficient logarithmic time complexity algorithm that can achieve the optimal\nredundancy up to a constant multiplicity gap. Our results are guaranteed to\nhold in a strong deterministic sense in an individual sequence manner.",
          "link": "http://arxiv.org/abs/2108.06411",
          "publishedOn": "2021-08-17T01:54:48.140Z",
          "wordCount": 605,
          "title": "Optimal and Efficient Algorithms for General Mixable Losses against Switching Oracles. (arXiv:2108.06411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziwei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "In order to model the evolution of user preference, we should learn user/item\nembeddings based on time-ordered item purchasing sequences, which is defined as\nSequential Recommendation (SR) problem. Existing methods leverage sequential\npatterns to model item transitions. However, most of them ignore crucial\ntemporal collaborative signals, which are latent in evolving user-item\ninteractions and coexist with sequential patterns. Therefore, we propose to\nunify sequential patterns and temporal collaborative signals to improve the\nquality of recommendation, which is rather challenging. Firstly, it is hard to\nsimultaneously encode sequential patterns and collaborative signals. Secondly,\nit is non-trivial to express the temporal effects of collaborative signals.\n\nHence, we design a new framework Temporal Graph Sequential Recommender\n(TGSRec) upon our defined continuous-time bi-partite graph. We propose a novel\nTemporal Collaborative Trans-former (TCT) layer in TGSRec, which advances the\nself-attention mechanism by adopting a novel collaborative attention. TCT layer\ncan simultaneously capture collaborative signals from both users and items, as\nwell as considering temporal dynamics inside sequential patterns. We propagate\nthe information learned fromTCTlayerover the temporal graph to unify sequential\npatterns and temporal collaborative signals. Empirical results on five datasets\nshow that TGSRec significantly outperforms other baselines, in average up to\n22.5% and 22.1%absolute improvements in Recall@10and MRR, respectively.",
          "link": "http://arxiv.org/abs/2108.06625",
          "publishedOn": "2021-08-17T01:54:48.134Z",
          "wordCount": 650,
          "title": "Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer. (arXiv:2108.06625v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Boxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Adversarial training is a powerful type of defense against adversarial\nexamples. Previous empirical results suggest that adversarial training requires\nwider networks for better performances. However, it remains elusive how neural\nnetwork width affects model robustness. In this paper, we carefully examine the\nrelationship between network width and model robustness. Specifically, we show\nthat the model robustness is closely related to the tradeoff between natural\naccuracy and perturbation stability, which is controlled by the robust\nregularization parameter $\\lambda$. With the same $\\lambda$, wider networks can\nachieve better natural accuracy but worse perturbation stability, leading to a\npotentially worse overall model robustness. To understand the origin of this\nphenomenon, we further relate the perturbation stability with the network's\nlocal Lipschitzness. By leveraging recent results on neural tangent kernels, we\ntheoretically show that wider networks tend to have worse perturbation\nstability. Our analyses suggest that: 1) the common strategy of first\nfine-tuning $\\lambda$ on small networks and then directly use it for wide model\ntraining could lead to deteriorated model robustness; 2) one needs to properly\nenlarge $\\lambda$ to unleash the robustness potential of wider models fully.\nFinally, we propose a new Width Adjusted Regularization (WAR) method that\nadaptively enlarges $\\lambda$ on wide models and significantly saves the tuning\ntime.",
          "link": "http://arxiv.org/abs/2010.01279",
          "publishedOn": "2021-08-17T01:54:48.128Z",
          "wordCount": 694,
          "title": "Do Wider Neural Networks Really Help Adversarial Robustness?. (arXiv:2010.01279v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_Y/0/1/0/all/0/1\">Yunseok Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1\">Won Joon Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Kook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>",
          "description": "The emergence of quantum computing enables for researchers to apply quantum\ncircuit on many existing studies. Utilizing quantum circuit and quantum\ndifferential programming, many research are conducted such as \\textit{Quantum\nMachine Learning} (QML). In particular, quantum reinforcement learning is a\ngood field to test the possibility of quantum machine learning, and a lot of\nresearch is being done. This work will introduce the concept of quantum\nreinforcement learning using a variational quantum circuit, and confirm its\npossibility through implementation and experimentation. We will first present\nthe background knowledge and working principle of quantum reinforcement\nlearning, and then guide the implementation method using the PennyLane library.\nWe will also discuss the power and possibility of quantum reinforcement\nlearning from the experimental results obtained through this work.",
          "link": "http://arxiv.org/abs/2108.06849",
          "publishedOn": "2021-08-17T01:54:47.822Z",
          "wordCount": 560,
          "title": "Introduction to Quantum Reinforcement Learning: Theory and PennyLane-based Implementation. (arXiv:2108.06849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shao-Qun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Gao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhi-Hua Zhou</a>",
          "description": "Complex-valued neural networks have attracted increasing attention in recent\nyears, while it remains open on the advantages of complex-valued neural\nnetworks in comparison with real-valued networks. This work takes one step on\nthis direction by introducing the \\emph{complex-reaction network} with\nfully-connected feed-forward architecture. We prove the universal approximation\nproperty for complex-reaction networks, and show that a class of radial\nfunctions can be approximated by a complex-reaction network using the\npolynomial number of parameters, whereas real-valued networks need at least\nexponential parameters to reach the same approximation level. For empirical\nrisk minimization, our theoretical result shows that the critical point set of\ncomplex-reaction networks is a proper subset of that of real-valued networks,\nwhich may show some insights on finding the optimal solutions more easily for\ncomplex-reaction networks.",
          "link": "http://arxiv.org/abs/2108.06711",
          "publishedOn": "2021-08-17T01:54:47.750Z",
          "wordCount": 558,
          "title": "Towards Understanding Theoretical Advantages of Complex-Reaction Networks. (arXiv:2108.06711v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karlbauer_M/0/1/0/all/0/1\">Matthias Karlbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menge_T/0/1/0/all/0/1\">Tobias Menge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otte_S/0/1/0/all/0/1\">Sebastian Otte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P.A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholten_T/0/1/0/all/0/1\">Thomas Scholten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wulfmeyer_V/0/1/0/all/0/1\">Volker Wulfmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butz_M/0/1/0/all/0/1\">Martin V. Butz</a>",
          "description": "Knowledge about the hidden factors that determine particular system dynamics\nis crucial for both explaining them and pursuing goal-directed interventions.\nInferring these factors from time series data without supervision remains an\nopen challenge. Here, we focus on spatiotemporal processes, including wave\npropagation and weather dynamics, for which we assume that universal causes\n(e.g. physics) apply throughout space and time. A recently introduced\nDIstributed SpatioTemporal graph Artificial Neural network Architecture\n(DISTANA) is used and enhanced to learn such processes, requiring fewer\nparameters and achieving significantly more accurate predictions compared to\ntemporal convolutional neural networks and other related approaches. We show\nthat DISTANA, when combined with a retrospective latent state inference\nprinciple called active tuning, can reliably derive location-respective hidden\ncausal factors. In a current weather prediction benchmark, DISTANA infers our\nplanet's land-sea mask solely by observing temperature dynamics and, meanwhile,\nuses the self inferred information to improve its own future temperature\npredictions.",
          "link": "http://arxiv.org/abs/2009.09823",
          "publishedOn": "2021-08-17T01:54:47.638Z",
          "wordCount": 641,
          "title": "Latent State Inference in a Spatiotemporal Generative Model. (arXiv:2009.09823v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>",
          "description": "Session-based recommendation systems suggest relevant items to users by\nmodeling user behavior and preferences using short-term anonymous sessions.\nExisting methods leverage Graph Neural Networks (GNNs) that propagate and\naggregate information from neighboring nodes i.e., local message passing. Such\ngraph-based architectures have representational limits, as a single sub-graph\nis susceptible to overfit the sequential dependencies instead of accounting for\ncomplex transitions between items in different sessions. We propose using a\nTransformer in combination with a target attentive GNN, which allows richer\nRepresentation Learning. Our experimental results and ablation show that our\nproposed method is competitive with the existing methods on real-world\nbenchmark datasets, improving on graph-based hypotheses.",
          "link": "http://arxiv.org/abs/2107.01516",
          "publishedOn": "2021-08-17T01:54:47.620Z",
          "wordCount": 570,
          "title": "Improved Representation Learning for Session-based Recommendation. (arXiv:2107.01516v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.09526",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Oglic_D/0/1/0/all/0/1\">Dino Oglic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cvetkovic_Z/0/1/0/all/0/1\">Zoran Cvetkovic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sollich_P/0/1/0/all/0/1\">Peter Sollich</a>",
          "description": "We investigate the potential of stochastic neural networks for learning\neffective waveform-based acoustic models. The waveform-based setting, inherent\nto fully end-to-end speech recognition systems, is motivated by several\ncomparative studies of automatic and human speech recognition that associate\nstandard non-adaptive feature extraction techniques with information loss which\ncan adversely affect robustness. Stochastic neural networks, on the other hand,\nare a class of models capable of incorporating rich regularization mechanisms\ninto the learning process. We consider a deep convolutional neural network that\nfirst decomposes speech into frequency sub-bands via an adaptive parametric\nconvolutional block where filters are specified by cosine modulations of\ncompactly supported windows. The network then employs standard non-parametric\n1D convolutions to extract relevant spectro-temporal patterns while gradually\ncompressing the structured high dimensional representation generated by the\nparametric block. We rely on a probabilistic parametrization of the proposed\nneural architecture and learn the model using stochastic variational inference.\nThis requires evaluation of an analytically intractable integral defining the\nKullback-Leibler divergence term responsible for regularization, for which we\npropose an effective approximation based on the Gauss-Hermite quadrature. Our\nempirical results demonstrate a superior performance of the proposed approach\nover comparable waveform-based baselines and indicate that it could lead to\nrobustness. Moreover, the approach outperforms a recently proposed deep\nconvolutional neural network for learning of robust acoustic models with\nstandard FBANK features.",
          "link": "http://arxiv.org/abs/1906.09526",
          "publishedOn": "2021-08-17T01:54:47.608Z",
          "wordCount": 705,
          "title": "Learning Waveform-Based Acoustic Models using Deep Variational Convolutional Neural Networks. (arXiv:1906.09526v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geissler_F/0/1/0/all/0/1\">Florian Geissler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qutub_S/0/1/0/all/0/1\">Syed Qutub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sayanta Roychowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_A/0/1/0/all/0/1\">Ali Asgari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamasia_A/0/1/0/all/0/1\">Akash Dhamasia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graefe_R/0/1/0/all/0/1\">Ralf Graefe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattabiraman_K/0/1/0/all/0/1\">Karthik Pattabiraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulitsch_M/0/1/0/all/0/1\">Michael Paulitsch</a>",
          "description": "Convolutional neural networks (CNNs) have become an established part of\nnumerous safety-critical computer vision applications, including human robot\ninteractions and automated driving. Real-world implementations will need to\nguarantee their robustness against hardware soft errors corrupting the\nunderlying platform memory. Based on the previously observed efficacy of\nactivation clipping techniques, we build a prototypical safety case for\nclassifier CNNs by demonstrating that range supervision represents a highly\nreliable fault detector and mitigator with respect to relevant bit flips,\nadopting an eight-exponent floating point data representation. We further\nexplore novel, non-uniform range restriction methods that effectively suppress\nthe probability of silent data corruptions and uncorrectable errors. As a\nsafety-relevant end-to-end use case, we showcase the benefit of our approach in\na vehicle classification scenario, using ResNet-50 and the traffic camera data\nset MIOVision. The quantitative evidence provided in this work can be leveraged\nto inspire further and possibly more complex CNN safety arguments.",
          "link": "http://arxiv.org/abs/2108.07019",
          "publishedOn": "2021-08-17T01:54:47.602Z",
          "wordCount": 611,
          "title": "Towards a Safety Case for Hardware Fault Tolerance in Convolutional Neural Networks Using Activation Range Supervision. (arXiv:2108.07019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.13446",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sharma_A/0/1/0/all/0/1\">Ankit Sharma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gupta_G/0/1/0/all/0/1\">Garima Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Prasad_R/0/1/0/all/0/1\">Ranjitha Prasad</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1\">Arnab Chatterjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vig_L/0/1/0/all/0/1\">Lovekesh Vig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shroff_G/0/1/0/all/0/1\">Gautam Shroff</a>",
          "description": "Causal inference (CI) in observational studies has received a lot of\nattention in healthcare, education, ad attribution, policy evaluation, etc.\nConfounding is a typical hazard, where the context affects both, the treatment\nassignment and response. In a multiple treatment scenario, we propose the\nneural network based MultiMBNN, where we overcome confounding by employing\ngeneralized propensity score based matching, and learning balanced\nrepresentations. We benchmark the performance on synthetic and real-world\ndatasets using PEHE, and mean absolute percentage error over ATE as metrics.\nMultiMBNN outperforms the state-of-the-art algorithms for CI such as TARNet and\nPerfect Match (PM).",
          "link": "http://arxiv.org/abs/2004.13446",
          "publishedOn": "2021-08-17T01:54:47.581Z",
          "wordCount": 583,
          "title": "MultiMBNN: Matched and Balanced Causal Inference with Neural Networks. (arXiv:2004.13446v4 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aykas_D/0/1/0/all/0/1\">Dogan Aykas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrkanoon_S/0/1/0/all/0/1\">Siamak Mehrkanoon</a>",
          "description": "Reliable and accurate wind speed prediction has significant impact in many\nindustrial sectors such as economic, business and management among others. This\npaper presents a new model for wind speed prediction based on Graph Attention\nNetworks (GAT). In particular, the proposed model extends GAT architecture by\nequipping it with a learnable adjacency matrix as well as incorporating a new\nattention mechanism with the aim of obtaining attention scores per weather\nvariable. The output of the GAT based model is combined with the LSTM layer in\norder to exploit both the spatial and temporal characteristics of the\nmultivariate multidimensional historical weather data. Real weather data\ncollected from several cities in Denmark and Netherlands are used to conduct\nthe experiments and evaluate the performance of the proposed model. We show\nthat in comparison to previous architectures used for wind speed prediction,\nthe proposed model is able to better learn the complex input-output\nrelationships of the weather data. Furthermore, thanks to the learned attention\nweights, the model provides an additional insights on the most important\nweather variables and cities for the studied prediction task.",
          "link": "http://arxiv.org/abs/2108.07063",
          "publishedOn": "2021-08-17T01:54:47.568Z",
          "wordCount": 615,
          "title": "Multistream Graph Attention Networks for Wind Speed Forecasting. (arXiv:2108.07063v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.11304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faez Ahmed</a>",
          "description": "Deep generative models are proven to be a useful tool for automatic design\nsynthesis and design space exploration. When applied in engineering design,\nexisting generative models face three challenges: 1) generated designs lack\ndiversity and do not cover all areas of the design space, 2) it is difficult to\nexplicitly improve the overall performance or quality of generated designs, and\n3) existing models generally do not generate novel designs, outside the domain\nof the training data. In this paper, we simultaneously address these challenges\nby proposing a new Determinantal Point Processes based loss function for\nprobabilistic modeling of diversity and quality. With this new loss function,\nwe develop a variant of the Generative Adversarial Network, named \"Performance\nAugmented Diverse Generative Adversarial Network\" or PaDGAN, which can generate\nnovel high-quality designs with good coverage of the design space. Using three\nsynthetic examples and one real-world airfoil design example, we demonstrate\nthat PaDGAN can generate diverse and high-quality designs. In comparison to a\nvanilla Generative Adversarial Network, on average, it generates samples with a\n28% higher mean quality score with larger diversity and without the mode\ncollapse issue. Unlike typical generative models that usually generate new\ndesigns by interpolating within the boundary of training data, we show that\nPaDGAN expands the design space boundary outside the training data towards\nhigh-quality regions. The proposed method is broadly applicable to many tasks\nincluding design space exploration, design optimization, and creative solution\nrecommendation.",
          "link": "http://arxiv.org/abs/2002.11304",
          "publishedOn": "2021-08-17T01:54:47.563Z",
          "wordCount": 734,
          "title": "PaDGAN: A Generative Adversarial Network for Performance Augmented Diverse Designs. (arXiv:2002.11304v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1\">Houcemeddine Turki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taieb_M/0/1/0/all/0/1\">Mohamed Ali Hadj Taieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouicha_M/0/1/0/all/0/1\">Mohamed Ben Aouicha</a>",
          "description": "So far, multi-label classification algorithms have been evaluated using\nstatistical methods that do not consider the semantics of the considered\nclasses and that fully depend on abstract computations such as Bayesian\nReasoning. Currently, there are several attempts to develop ontology-based\nmethods for a better assessment of supervised classification algorithms. In\nthis research paper, we define a novel approach that aligns expected labels\nwith predicted labels in multi-label classification using ontology-driven\nfeature-based semantic similarity measures and we use it to develop a method\nfor creating precise confusion matrices for a more effective evaluation of\nmulti-label classification algorithms.",
          "link": "http://arxiv.org/abs/2011.00109",
          "publishedOn": "2021-08-17T01:54:47.556Z",
          "wordCount": 579,
          "title": "Knowledge-Based Construction of Confusion Matrices for Multi-Label Classification Algorithms using Semantic Similarity Measures. (arXiv:2011.00109v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.06022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xudong Li</a>",
          "description": "To discover powerful yet compact models is an important goal of neural\narchitecture search. Previous two-stage one-shot approaches are limited by\nsearch space with a fixed depth. It seems handy to include an additional skip\nconnection in the search space to make depths variable. However, it creates a\nlarge range of perturbation during supernet training and it has difficulty\ngiving a confident ranking for subnetworks. In this paper, we discover that\nskip connections bring about significant feature inconsistency compared with\nother operations, which potentially degrades the supernet performance. Based on\nthis observation, we tackle the problem by imposing an equivariant learnable\nstabilizer to homogenize such disparities. Experiments show that our proposed\nstabilizer helps to improve the supernet's convergence as well as ranking\nperformance. With an evolutionary search backend that incorporates the\nstabilized supernet as an evaluator, we derive a family of state-of-the-art\narchitectures, the SCARLET series of several depths, especially SCARLET-A\nobtains 76.9% top-1 accuracy on ImageNet. Code is available at\nhttps://github.com/xiaomi-automl/ScarletNAS.",
          "link": "http://arxiv.org/abs/1908.06022",
          "publishedOn": "2021-08-17T01:54:47.550Z",
          "wordCount": 688,
          "title": "SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search. (arXiv:1908.06022v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiawei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jian Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yanru Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jieli Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiange Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wei-Wei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1\">Yufang Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_G/0/1/0/all/0/1\">Guang Ning</a>",
          "description": "Diabetes prediction is an important data science application in the social\nhealthcare domain. There exist two main challenges in the diabetes prediction\ntask: data heterogeneity since demographic and metabolic data are of different\ntypes, data insufficiency since the number of diabetes cases in a single\nmedical center is usually limited. To tackle the above challenges, we employ\ngradient boosting decision trees (GBDT) to handle data heterogeneity and\nintroduce multi-task learning (MTL) to solve data insufficiency. To this end,\nTask-wise Split Gradient Boosting Trees (TSGB) is proposed for the multi-center\ndiabetes prediction task. Specifically, we firstly introduce task gain to\nevaluate each task separately during tree construction, with a theoretical\nanalysis of GBDT's learning objective. Secondly, we reveal a problem when\ndirectly applying GBDT in MTL, i.e., the negative task gain problem. Finally,\nwe propose a novel split method for GBDT in MTL based on the task gain\nstatistics, named task-wise split, as an alternative to standard feature-wise\nsplit to overcome the mentioned negative task gain problem. Extensive\nexperiments on a large-scale real-world diabetes dataset and a commonly used\nbenchmark dataset demonstrate TSGB achieves superior performance against\nseveral state-of-the-art methods. Detailed case studies further support our\nanalysis of negative task gain problems and provide insightful findings. The\nproposed TSGB method has been deployed as an online diabetes risk assessment\nsoftware for early diagnosis.",
          "link": "http://arxiv.org/abs/2108.07107",
          "publishedOn": "2021-08-17T01:54:47.545Z",
          "wordCount": 705,
          "title": "Task-wise Split Gradient Boosting Trees for Multi-center Diabetes Prediction. (arXiv:2108.07107v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasu_M/0/1/0/all/0/1\">Malolan Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abreu_N/0/1/0/all/0/1\">Nelson Abreu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasquez_R/0/1/0/all/0/1\">Raysa V&#xe1;squez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">Christian L&#xf3;pez</a>",
          "description": "Vehicle counting systems can help with vehicle analysis and traffic incident\ndetection. Unfortunately, most existing methods require some level of human\ninput to identify the Region of interest (ROI), movements of interest, or to\nestablish a reference point or line to count vehicles from traffic cameras.\nThis work introduces a method to count vehicles from traffic videos that\nautomatically identifies the ROI for the camera, as well as the driving\ntrajectories of the vehicles. This makes the method feasible to use with\nPan-Tilt-Zoom cameras, which are frequently used in developing countries.\nPreliminary results indicate that the proposed method achieves an average\nintersection over the union of 57.05% for the ROI and a mean absolute error of\njust 17.44% at counting vehicles of the traffic video cameras tested.",
          "link": "http://arxiv.org/abs/2108.07135",
          "publishedOn": "2021-08-17T01:54:47.522Z",
          "wordCount": 578,
          "title": "Vehicle-counting with Automatic Region-of-Interest and Driving-Trajectory detection. (arXiv:2108.07135v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09264",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Wen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hung_K/0/1/0/all/0/1\">Kuo-Hsuan Hung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">You-Jin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_A/0/1/0/all/0/1\">Alexander Chao-Fu Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_Y/0/1/0/all/0/1\">Ya-Hsin Lai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kai-Chun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_S/0/1/0/all/0/1\">Sze-Wei Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Syu-Siang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>",
          "description": "In this paper, we present a deep learning-based speech signal-processing\nmobile application, CITISEN, which can perform three functions: speech\nenhancement (SE), acoustic scene conversion (ASC), and model adaptation (MA).\nFor SE, CITISEN can effectively reduce noise components from speech signals and\naccordingly enhance their clarity and intelligibility. For ASC, CITISEN can\nconvert the current background sound to a different background sound. Finally,\nfor MA, CITISEN can effectively adapt an SE model, with a few audio files, when\nit encounters unknown speakers or noise types; the adapted SE model is used to\nenhance the upcoming noisy utterances. Experimental results confirmed the\neffectiveness of CITISEN in performing these three functions via objective\nevaluation and subjective listening tests. The promising results reveal that\nthe developed CITISEN mobile application can potentially be used as a front-end\nprocessor for various speech-related services such as voice communication,\nassistive hearing devices, and virtual reality headsets.",
          "link": "http://arxiv.org/abs/2008.09264",
          "publishedOn": "2021-08-17T01:54:47.510Z",
          "wordCount": 621,
          "title": "CITISEN: A Deep Learning-Based Speech Signal-Processing Mobile Application. (arXiv:2008.09264v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06890",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Ji-Hoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Hoon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Hyun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_H/0/1/0/all/0/1\">Hong-Gyu Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Few-shot speaker adaptation is a specific Text-to-Speech (TTS) system that\naims to reproduce a novel speaker's voice with a few training data. While\nnumerous attempts have been made to the few-shot speaker adaptation system,\nthere is still a gap in terms of speaker similarity to the target speaker\ndepending on the amount of data. To bridge the gap, we propose GC-TTS which\nachieves high-quality speaker adaptation with significantly improved speaker\nsimilarity. Specifically, we leverage two geometric constraints to learn\ndiscriminative speaker representations. Here, a TTS model is pre-trained for\nbase speakers with a sufficient amount of data, and then fine-tuned for novel\nspeakers on a few minutes of data with two geometric constraints. Two geometric\nconstraints enable the model to extract discriminative speaker embeddings from\nlimited data, which leads to the synthesis of intelligible speech. We discuss\nand verify the effectiveness of GC-TTS by comparing it with popular and\nessential methods. The experimental results demonstrate that GC-TTS generates\nhigh-quality speech from only a few minutes of training data, outperforming\nstandard techniques in terms of speaker similarity to the target speaker.",
          "link": "http://arxiv.org/abs/2108.06890",
          "publishedOn": "2021-08-17T01:54:47.501Z",
          "wordCount": 639,
          "title": "GC-TTS: Few-shot Speaker Adaptation with Geometric Constraints. (arXiv:2108.06890v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/1911.09721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Avishek Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maity_R/0/1/0/all/0/1\">Raj Kumar Maity</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadhe_S/0/1/0/all/0/1\">Swanand Kadhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1\">Arya Mazumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>",
          "description": "We develop a communication-efficient distributed learning algorithm that is\nrobust against Byzantine worker machines. We propose and analyze a distributed\ngradient-descent algorithm that performs a simple thresholding based on\ngradient norms to mitigate Byzantine failures. We show the (statistical)\nerror-rate of our algorithm matches that of Yin et al.~\\cite{dong}, which uses\nmore complicated schemes (coordinate-wise median, trimmed mean). Furthermore,\nfor communication efficiency, we consider a generic class of\n$\\delta$-approximate compressors from Karimireddi et al.~\\cite{errorfeed} that\nencompasses sign-based compressors and top-$k$ sparsification. Our algorithm\nuses compressed gradients and gradient norms for aggregation and Byzantine\nremoval respectively. We establish the statistical error rate for non-convex\nsmooth loss functions. We show that, in certain range of the compression factor\n$\\delta$, the (order-wise) rate of convergence is not affected by the\ncompression operation. Moreover, we analyze the compressed gradient descent\nalgorithm with error feedback (proposed in \\cite{errorfeed}) in a distributed\nsetting and in the presence of Byzantine worker machines. We show that\nexploiting error feedback improves the statistical error rate. Finally, we\nexperimentally validate our results and show good performance in convergence\nfor convex (least-square regression) and non-convex (neural network training)\nproblems.",
          "link": "http://arxiv.org/abs/1911.09721",
          "publishedOn": "2021-08-17T01:54:47.485Z",
          "wordCount": 692,
          "title": "Communication-Efficient and Byzantine-Robust Distributed Learning with Error Feedback. (arXiv:1911.09721v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Alvaro Almeida Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_A/0/1/0/all/0/1\">Ant&#xf4;nio J. Silva Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubelli_J/0/1/0/all/0/1\">Jorge P. Zubelli</a>",
          "description": "We recover the gradient of a given function defined on interior points of a\nsubmanifold with boundary of the Euclidean space based on a (normally\ndistributed) random sample of function evaluations at points in the manifold.\nThis approach is based on the estimates of the Laplace-Beltrami operator\nproposed in the theory of Diffusion-Maps. Analytical convergence results of the\nresulting expansion are proved, and an efficient algorithm is proposed to deal\nwith non-convex optimization problems defined on Euclidean submanifolds. We\ntest and validate our methodology as a post-processing tool in Cryogenic\nelectron microscopy (Cryo-EM). We also apply the method to the classical sphere\npacking problem.",
          "link": "http://arxiv.org/abs/2108.06988",
          "publishedOn": "2021-08-17T01:54:47.466Z",
          "wordCount": 549,
          "title": "A diffusion-map-based algorithm for gradient computation on manifolds and applications. (arXiv:2108.06988v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vieillard_N/0/1/0/all/0/1\">Nino Vieillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrychowicz_M/0/1/0/all/0/1\">Marcin Andrychowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raichuk_A/0/1/0/all/0/1\">Anton Raichuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1\">Olivier Pietquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1\">Matthieu Geist</a>",
          "description": "The $Q$-function is a central quantity in many Reinforcement Learning (RL)\nalgorithms for which RL agents behave following a (soft)-greedy policy w.r.t.\nto $Q$. It is a powerful tool that allows action selection without a model of\nthe environment and even without explicitly modeling the policy. Yet, this\nscheme can only be used in discrete action tasks, with small numbers of\nactions, as the softmax cannot be computed exactly otherwise. Especially the\nusage of function approximation, to deal with continuous action spaces in\nmodern actor-critic architectures, intrinsically prevents the exact computation\nof a softmax. We propose to alleviate this issue by parametrizing the\n$Q$-function implicitly, as the sum of a log-policy and of a value function. We\nuse the resulting parametrization to derive a practical off-policy deep RL\nalgorithm, suitable for large action spaces, and that enforces the softmax\nrelation between the policy and the $Q$-value. We provide a theoretical\nanalysis of our algorithm: from an Approximate Dynamic Programming perspective,\nwe show its equivalence to a regularized version of value iteration, accounting\nfor both entropy and Kullback-Leibler regularization, and that enjoys\nbeneficial error propagation results. We then evaluate our algorithm on classic\ncontrol tasks, where its results compete with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.07041",
          "publishedOn": "2021-08-17T01:54:47.442Z",
          "wordCount": 628,
          "title": "Implicitly Regularized RL with Implicit Q-Values. (arXiv:2108.07041v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1\">Shubham Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pahwa_K/0/1/0/all/0/1\">Khushbu Pahwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>",
          "description": "Structure learning offers an expressive, versatile and explainable approach\nto causal and mechanistic modeling of complex biological data. We present\nwiseR, an open source application for learning, evaluating and deploying robust\ncausal graphical models using graph neural networks and Bayesian networks. We\ndemonstrate the utility of this application through application on for\nbiomarker discovery in a COVID-19 clinical dataset.",
          "link": "http://arxiv.org/abs/2108.07046",
          "publishedOn": "2021-08-17T01:54:47.436Z",
          "wordCount": 538,
          "title": "WiseR: An end-to-end structure learning and deployment framework for causal graphical models. (arXiv:2108.07046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.01777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dat Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Stephen S. Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>",
          "description": "We propose a novel interpretable deep neural network for text classification,\ncalled ProtoryNet, based on a new concept of prototype trajectories. Motivated\nby the prototype theory in modern linguistics, ProtoryNet makes a prediction by\nfinding the most similar prototype for each sentence in a text sequence and\nfeeding an RNN backbone with the proximity of each sentence to the\ncorresponding active prototype. The RNN backbone then captures the temporal\npattern of the prototypes, which we refer to as prototype trajectories.\nPrototype trajectories enable intuitive and fine-grained interpretation of the\nreasoning process of the RNN model, in resemblance to how humans analyze texts.\nWe also design a prototype pruning procedure to reduce the total number of\nprototypes used by the model for better interpretability. Experiments on\nmultiple public data sets show that ProtoryNet is more accurate than the\nbaseline prototype-based deep neural net and reduces the performance gap\ncompared to state-of-the-art black-box models. In addition, after prototype\npruning, the resulting ProtoryNet models only need less than or around 20\nprototypes for all datasets, which significantly benefits interpretability.\nFurthermore, we report a survey result indicating that human users find\nProtoryNet more intuitive and easier to understand than other prototype-based\nmethods.",
          "link": "http://arxiv.org/abs/2007.01777",
          "publishedOn": "2021-08-17T01:54:47.430Z",
          "wordCount": 658,
          "title": "Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jihoon Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuu_C/0/1/0/all/0/1\">Cheng-hsin Wuu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsuan-ru Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>",
          "description": "We contribute HAA500, a manually annotated human-centric atomic action\ndataset for action recognition on 500 classes with over 591K labeled frames. To\nminimize ambiguities in action classification, HAA500 consists of highly\ndiversified classes of fine-grained atomic actions, where only consistent\nactions fall under the same label, e.g., \"Baseball Pitching\" vs \"Free Throw in\nBasketball\". Thus HAA500 is different from existing atomic action datasets,\nwhere coarse-grained atomic actions were labeled with coarse action-verbs such\nas \"Throw\". HAA500 has been carefully curated to capture the precise movement\nof human figures with little class-irrelevant motions or spatio-temporal label\nnoises. The advantages of HAA500 are fourfold: 1) human-centric actions with a\nhigh average of 69.7% detectable joints for the relevant human poses; 2) high\nscalability since adding a new class can be done under 20-60 minutes; 3)\ncurated videos capturing essential elements of an atomic action without\nirrelevant frames; 4) fine-grained atomic action classes. Our extensive\nexperiments including cross-data validation using datasets collected in the\nwild demonstrate the clear benefits of human-centric and atomic characteristics\nof HAA500, which enable training even a baseline deep learning model to improve\nprediction by attending to atomic human poses. We detail the HAA500 dataset\nstatistics and collection methodology and compare quantitatively with existing\naction recognition datasets.",
          "link": "http://arxiv.org/abs/2009.05224",
          "publishedOn": "2021-08-17T01:54:47.423Z",
          "wordCount": 685,
          "title": "HAA500: Human-Centric Atomic Action Dataset with Curated Videos. (arXiv:2009.05224v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goswami_S/0/1/0/all/0/1\">Somdatta Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Minglang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Karniadakis</a>",
          "description": "Failure trajectories, identifying the probable failure zones, and damage\nstatistics are some of the key quantities of relevance in brittle fracture\napplications. High-fidelity numerical solvers that reliably estimate these\nrelevant quantities exist but they are computationally demanding requiring a\nhigh resolution of the crack. Moreover, independent intensive simulations need\nto be carried out even for a small change in domain parameters and/or material\nproperties. Therefore, fast and generalizable surrogate models are needed to\nalleviate the computational burden but the discontinuous nature of fracture\nmechanics presents a major challenge to developing such models. We propose a\nphysics-informed variational formulation of DeepONet (V-DeepONet) for brittle\nfracture analysis. V-DeepONet is trained to map the initial configuration of\nthe defect to the relevant fields of interests (e.g., damage and displacement\nfields). Once the network is trained, the entire global solution can be rapidly\nobtained for any initial crack configuration and loading steps on that domain.\nWhile the original DeepONet is solely data-driven, we take a different path to\ntrain the V-DeepONet by imposing the governing equations in variational form\nand we also use some labelled data. We demonstrate the effectiveness of\nV-DeepOnet through two benchmarks of brittle fracture, and we verify its\naccuracy using results from high-fidelity solvers. Encoding the physical laws\nand also some data to train the network renders the surrogate model capable of\naccurately performing both interpolation and extrapolation tasks, considering\nthat fracture modeling is very sensitive to fluctuations. The proposed hybrid\ntraining of V-DeepONet is superior to state-of-the-art methods and can be\napplied to a wide array of dynamical systems with complex responses.",
          "link": "http://arxiv.org/abs/2108.06905",
          "publishedOn": "2021-08-17T01:54:47.416Z",
          "wordCount": 706,
          "title": "A physics-informed variational DeepONet for predicting the crack path in brittle materials. (arXiv:2108.06905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.04720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wengong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swanson_K/0/1/0/all/0/1\">Kyle Swanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>",
          "description": "Generative models in molecular design tend to be richly parameterized,\ndata-hungry neural models, as they must create complex structured objects as\noutputs. Estimating such models from data may be challenging due to the lack of\nsufficient training data. In this paper, we propose a surprisingly effective\nself-training approach for iteratively creating additional molecular targets.\nWe first pre-train the generative model together with a simple property\npredictor. The property predictor is then used as a likelihood model for\nfiltering candidate structures from the generative model. Additional targets\nare iteratively produced and used in the course of stochastic EM iterations to\nmaximize the log-likelihood that the candidate structures are accepted. A\nsimple rejection (re-weighting) sampler suffices to draw posterior samples\nsince the generative model is already reasonable after pre-training. We\ndemonstrate significant gains over strong baselines for both unconditional and\nconditional molecular design. In particular, our approach outperforms the\nprevious state-of-the-art in conditional molecular design by over 10% in\nabsolute gain. Finally, we show that our approach is useful in other domains as\nwell, such as program synthesis.",
          "link": "http://arxiv.org/abs/2002.04720",
          "publishedOn": "2021-08-17T01:54:47.399Z",
          "wordCount": 666,
          "title": "Improving Molecular Design by Stochastic Iterative Target Augmentation. (arXiv:2002.04720v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cairoli_F/0/1/0/all/0/1\">Francesca Cairoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1\">Luca Bortolussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paoletti_N/0/1/0/all/0/1\">Nicola Paoletti</a>",
          "description": "We consider the problem of predictive monitoring (PM), i.e., predicting at\nruntime future violations of a system from the current state. We work under the\nmost realistic settings where only partial and noisy observations of the state\nare available at runtime. Such settings directly affect the accuracy and\nreliability of the reachability predictions, jeopardizing the safety of the\nsystem. In this work, we present a learning-based method for PM that produces\naccurate and reliable reachability predictions despite partial observability\n(PO). We build on Neural Predictive Monitoring (NPM), a PM method that uses\ndeep neural networks for approximating hybrid systems reachability, and extend\nit to the PO case. We propose and compare two solutions, an end-to-end\napproach, which directly operates on the rough observations, and a two-step\napproach, which introduces an intermediate state estimation step. Both\nsolutions rely on conformal prediction to provide 1) probabilistic guarantees\nin the form of prediction regions and 2) sound estimates of predictive\nuncertainty. We use the latter to identify unreliable (and likely erroneous)\npredictions and to retrain and improve the monitors on these uncertain inputs\n(i.e., active learning). Our method results in highly accurate reachability\npredictions and error detection, as well as tight prediction regions with\nguaranteed coverage.",
          "link": "http://arxiv.org/abs/2108.07134",
          "publishedOn": "2021-08-17T01:54:47.393Z",
          "wordCount": 627,
          "title": "Neural Predictive Monitoring under Partial Observabilit. (arXiv:2108.07134v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samiei_S/0/1/0/all/0/1\">Samaneh Samiei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_N/0/1/0/all/0/1\">Nasser Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansari_B/0/1/0/all/0/1\">Behnaz Ansari</a>",
          "description": "Electromyography (EMG) refers to a biomedical signal indicating neuromuscular\nactivity and muscle morphology. Experts accurately diagnose neuromuscular\ndisorders using this time series. Modern data analysis techniques have recently\nled to introducing novel approaches for mapping time series data to graphs and\ncomplex networks with applications in diverse fields, including medicine. The\nresulting networks develop a completely different visual acuity that can be\nused to complement physician findings of time series. This can lead to a more\nenriched analysis, reduced error, more accurate diagnosis of the disease, and\nincreased accuracy and speed of the treatment process. The mapping process may\ncause the loss of essential data from the time series and not retain all the\ntime series features. As a result, achieving an approach that can provide a\ngood representation of the time series while maintaining essential features is\ncrucial. This paper proposes a new approach to network development named\nGraphTS to overcome the limited accuracy of existing methods through EMG time\nseries using the visibility graph method. For this purpose, EMG signals are\npre-processed and mapped to a complex network by a standard visibility graph\nalgorithm. The resulting networks can differentiate between healthy and patient\nsamples. In the next step, the properties of the developed networks are given\nin the form of a feature matrix as input to classifiers after extracting\noptimal features. Performance evaluation of the proposed approach with deep\nneural network shows 99.30% accuracy for training data and 99.18% for test\ndata. Therefore, in addition to enriched network representation and covering\nthe features of time series for healthy, myopathy, and neuropathy EMG, the\nproposed technique improves accuracy, precision, recall, and F-score.",
          "link": "http://arxiv.org/abs/2108.06920",
          "publishedOn": "2021-08-17T01:54:47.386Z",
          "wordCount": 737,
          "title": "A complex network approach to time series analysis with application in diagnosis of neuromuscular disorders. (arXiv:2108.06920v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1\">Niamh Belton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1\">Aonghus Lawlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1\">Kathleen M. Curran</a>",
          "description": "Noisy data present in medical imaging datasets can often aid the development\nof robust models that are equipped to handle real-world data. However, if the\nbad data contains insufficient anatomical information, it can have a severe\nnegative effect on the model's performance. We propose a novel methodology\nusing a semi-supervised Siamese network to identify bad data. This method\nrequires only a small pool of 'reference' medical images to be reviewed by a\nnon-expert human to ensure the major anatomical structures are present in the\nField of View. The model trains on this reference set and identifies bad data\nby using the Siamese network to compute the distance between the reference set\nand all other medical images in the dataset. This methodology achieves an Area\nUnder the Curve (AUC) of 0.989 for identifying bad data. Code will be available\nat https://git.io/JYFuV.",
          "link": "http://arxiv.org/abs/2108.07130",
          "publishedOn": "2021-08-17T01:54:47.379Z",
          "wordCount": 594,
          "title": "Semi-Supervised Siamese Network for Identifying Bad Data in Medical Imaging Datasets. (arXiv:2108.07130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eikeland_O/0/1/0/all/0/1\">Odin Foldvik Eikeland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmstrand_I/0/1/0/all/0/1\">Inga Sets&#xe5; Holmstrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakkejord_S/0/1/0/all/0/1\">Sigurd Bakkejord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiesa_M/0/1/0/all/0/1\">Matteo Chiesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Filippo Maria Bianchi</a>",
          "description": "Unscheduled power disturbances cause severe consequences both for customers\nand grid operators. To defend against such events, it is necessary to identify\nthe causes of interruptions in the power distribution network. In this work, we\nfocus on the power grid of a Norwegian community in the Arctic that experiences\nseveral faults whose sources are unknown. First, we construct a data set\nconsisting of relevant meteorological data and information about the current\npower quality logged by power-quality meters. Then, we adopt machine-learning\ntechniques to predict the occurrence of faults. Experimental results show that\nboth linear and non-linear classifiers achieve good classification performance.\nThis indicates that the considered power-quality and weather variables explain\nwell the power disturbances. Interpreting the decision process of the\nclassifiers provides valuable insights to understand the main causes of\ndisturbances. Traditional features selection methods can only indicate which\nare the variables that, on average, mostly explain the fault occurrences in the\ndataset. Besides providing such a global interpretation, it is also important\nto identify the specific set of variables that explain each individual fault.\nTo address this challenge, we adopt a recent technique to interpret the\ndecision process of a deep learning model, called Integrated Gradients. The\nproposed approach allows to gain detailed insights on the occurrence of a\nspecific fault, which are valuable for the distribution system operators to\nimplement strategies to prevent and mitigate power disturbances.",
          "link": "http://arxiv.org/abs/2108.07060",
          "publishedOn": "2021-08-17T01:54:47.373Z",
          "wordCount": 672,
          "title": "Detecting and interpreting faults in vulnerable power grids with machine learning. (arXiv:2108.07060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_Y/0/1/0/all/0/1\">Yuya Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_K/0/1/0/all/0/1\">Kei Harada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamasaki_S/0/1/0/all/0/1\">Shohei Yamasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onizuka_M/0/1/0/all/0/1\">Makoto Onizuka</a>",
          "description": "Urban air pollution is a major environmental problem affecting human health\nand quality of life. Monitoring stations have been established to continuously\nobtain air quality information, but they do not cover all areas. Thus, there\nare numerous methods for spatially fine-grained air quality inference. Since\nexisting methods aim to infer air quality of locations only in monitored\ncities, they do not assume inferring air quality in unmonitored cities. In this\npaper, we first study the air quality inference in unmonitored cities. To\naccurately infer air quality in unmonitored cities, we propose a neural\nnetwork-based approach AIREX. The novelty of AIREX is employing a\nmixture-of-experts approach, which is a machine learning technique based on the\ndivide-and-conquer principle, to learn correlations of air quality between\nmultiple cities. To further boost the performance, it employs attention\nmechanisms to compute impacts of air quality inference from the monitored\ncities to the locations in the unmonitored city. We show, through experiments\non a real-world air quality dataset, that AIREX achieves higher accuracy than\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.07120",
          "publishedOn": "2021-08-17T01:54:47.354Z",
          "wordCount": 609,
          "title": "AIREX: Neural Network-based Approach for Air Quality Inference in Unmonitored Cities. (arXiv:2108.07120v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1\">Taeyeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyenson_B/0/1/0/all/0/1\">Benjamin Pyenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebig_J/0/1/0/all/0/1\">Juergen Liebig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>",
          "description": "Biology is both an important application area and a source of motivation for\ndevelopment of advanced machine learning techniques. Although much attention\nhas been paid to large and complex data sets resulting from high-throughput\nsequencing, advances in high-quality video recording technology have begun to\ngenerate similarly rich data sets requiring sophisticated techniques from both\ncomputer vision and time-series analysis. Moreover, just as studying gene\nexpression patterns in one organism can reveal general principles that apply to\nother organisms, the study of complex social interactions in an experimentally\ntractable model system, such as a laboratory ant colony, can provide general\nprinciples about the dynamics of other social groups. Here, we focus on one\nsuch example from the study of reproductive regulation in small laboratory\ncolonies of more than 50 Harpegnathos ants. These ants can be artificially\ninduced to begin a ~20 day process of hierarchy reformation. Although the\nconclusion of this process is conspicuous to a human observer, it remains\nunclear which behaviors during the transient period are contributing to the\nprocess. To address this issue, we explore the potential application of\nOne-class Classification (OC) to the detection of abnormal states in ant\ncolonies for which behavioral data is only available for the normal societal\nconditions during training. Specifically, we build upon the Deep Support Vector\nData Description (DSVDD) and introduce the Inner-Outlier Generator (IO-GEN)\nthat synthesizes fake \"inner outlier\" observations during training that are\nnear the center of the DSVDD data description. We show that IO-GEN increases\nthe reliability of the final OC classifier relative to other DSVDD baselines.\nThis method can be used to screen video frames for which additional human\nobservation is needed.",
          "link": "http://arxiv.org/abs/2009.08626",
          "publishedOn": "2021-08-17T01:54:47.345Z",
          "wordCount": 801,
          "title": "Identification of Abnormal States in Videos of Ants Undergoing Social Phase Change. (arXiv:2009.08626v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.14220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mostaani_A/0/1/0/all/0/1\">Arsham Mostaani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thang X. Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzinotas_S/0/1/0/all/0/1\">Symeon Chatzinotas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottersten_B/0/1/0/all/0/1\">Bj&#xf6;rn Ottersten</a>",
          "description": "A collaborative task is assigned to a multiagent system (MAS) in which agents\nare allowed to communicate. The MAS runs over an underlying Markov decision\nprocess and its task is to maximize the averaged sum of discounted one-stage\nrewards. Although knowing the global state of the environment is necessary for\nthe optimal action selection of the MAS, agents are limited to individual\nobservations. Inter-agent communication can tackle the issue of local\nobservability, however, the limited rate of inter-agent communication prevents\nthe agents from acquiring the precise global state information. To overcome\nthis challenge, agents need to communicate an abstract version of their\nobservations to each other such that the MAS compromises the minimum possible\nsum of rewards. We show that this problem is equivalent to a form of\nrate-distortion problem, which we call task-based information compression\n(TBIC). We introduce state aggregation for information compression (SAIC) to\nsolve the TBIC problem. SAIC is shown to achieve near-optimal performance in\nterms of the achieved sum of discounted rewards. The proposed algorithm is\napplied to a rendezvous problem and its performance is compared with several\nbenchmarks. Numerical experiments confirm the superiority of the proposed\nalgorithm.",
          "link": "http://arxiv.org/abs/2005.14220",
          "publishedOn": "2021-08-17T01:54:47.328Z",
          "wordCount": 691,
          "title": "Task-Based Information Compression for Multi-Agent Communication Problems with Channel Rate Constraints. (arXiv:2005.14220v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1801.04062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belghazi_M/0/1/0/all/0/1\">Mohamed Ishmael Belghazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1\">Aristide Baratin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1\">Sai Rajeswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozair_S/0/1/0/all/0/1\">Sherjil Ozair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1\">R Devon Hjelm</a>",
          "description": "We argue that the estimation of mutual information between high dimensional\ncontinuous random variables can be achieved by gradient descent over neural\nnetworks. We present a Mutual Information Neural Estimator (MINE) that is\nlinearly scalable in dimensionality as well as in sample size, trainable\nthrough back-prop, and strongly consistent. We present a handful of\napplications on which MINE can be used to minimize or maximize mutual\ninformation. We apply MINE to improve adversarially trained generative models.\nWe also use MINE to implement Information Bottleneck, applying it to supervised\nclassification; our results demonstrate substantial improvement in flexibility\nand performance in these settings.",
          "link": "http://arxiv.org/abs/1801.04062",
          "publishedOn": "2021-08-17T01:54:47.320Z",
          "wordCount": 607,
          "title": "MINE: Mutual Information Neural Estimation. (arXiv:1801.04062v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "With the tremendous advances in the architecture and scale of convolutional\nneural networks (CNNs) over the past few decades, they can easily reach or even\nexceed the performance of humans in certain tasks. However, a recently\ndiscovered shortcoming of CNNs is that they are vulnerable to adversarial\nattacks. Although the adversarial robustness of CNNs can be improved by\nadversarial training, there is a trade-off between standard accuracy and\nadversarial robustness. From the neural architecture perspective, this paper\naims to improve the adversarial robustness of the backbone CNNs that have a\nsatisfactory accuracy. Under a minimal computational overhead, the introduction\nof a dilation architecture is expected to be friendly with the standard\nperformance of the backbone CNN while pursuing adversarial robustness.\nTheoretical analyses on the standard and adversarial error bounds naturally\nmotivate the proposed neural architecture dilation algorithm. Experimental\nresults on real-world datasets and benchmark neural networks demonstrate the\neffectiveness of the proposed algorithm to balance the accuracy and adversarial\nrobustness.",
          "link": "http://arxiv.org/abs/2108.06885",
          "publishedOn": "2021-08-17T01:54:47.312Z",
          "wordCount": 614,
          "title": "Neural Architecture Dilation for Adversarial Robustness. (arXiv:2108.06885v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07028",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rahmani_M/0/1/0/all/0/1\">Mostafa Rahmani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shafipour_R/0/1/0/all/0/1\">Rasoul Shafipour</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "In contrast to image/text data whose order can be used to perform non-local\nfeature aggregation in a straightforward way using the pooling layers, graphs\nlack the tensor representation and mostly the element-wise max/mean function is\nutilized to aggregate the locally extracted feature vectors. In this paper, we\npresent a novel approach for global feature aggregation in Graph Neural\nNetworks (GNNs) which utilizes a Latent Fixed Data Structure (LFDS) to\naggregate the extracted feature vectors. The locally extracted feature vectors\nare sorted/distributed on the LFDS and a latent neural network (CNN/GNN) is\nutilized to perform feature aggregation on the LFDS. The proposed approach is\nused to design several novel global feature aggregation methods based on the\nchoice of the LFDS. We introduce multiple LFDSs including loop, 3D tensor\n(image), sequence, data driven graphs and an algorithm which sorts/distributes\nthe extracted local feature vectors on the LFDS. While the computational\ncomplexity of the proposed methods are linear with the order of input graphs,\nthey achieve competitive or better results.",
          "link": "http://arxiv.org/abs/2108.07028",
          "publishedOn": "2021-08-17T01:54:47.295Z",
          "wordCount": 615,
          "title": "Non-Local Feature Aggregation on Graphs via Latent Fixed Data Structures. (arXiv:2108.07028v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Freitas_S/0/1/0/all/0/1\">Scott Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Srijan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>",
          "description": "Network robustness plays a crucial role in our understanding of complex\ninterconnected systems such as transportation, communication, and computer\nnetworks. While significant research has been conducted in the area of network\nrobustness, no comprehensive open-source toolbox currently exists to assist\nresearchers and practitioners in this important topic. This lack of available\ntools hinders reproducibility and examination of existing work, development of\nnew research, and dissemination of new ideas. We contribute TIGER, an\nopen-sourced Python toolbox to address these challenges. TIGER contains 22\ngraph robustness measures with both original and fast approximate versions; 17\nfailure and attack strategies; 15 heuristic and optimization-based defense\ntechniques; and 4 simulation tools. By democratizing the tools required to\nstudy network robustness, our goal is to assist researchers and practitioners\nin analyzing their own networks; and facilitate the development of new research\nin the field. TIGER has been integrated into the Nvidia Data Science Teaching\nKit available to educators across the world; and Georgia Tech's Data and Visual\nAnalytics class with over 1,000 students. TIGER is open sourced at:\nhttps://github.com/safreita1/TIGER",
          "link": "http://arxiv.org/abs/2006.05648",
          "publishedOn": "2021-08-17T01:54:47.289Z",
          "wordCount": 655,
          "title": "Evaluating Graph Vulnerability and Robustness using TIGER. (arXiv:2006.05648v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellani_A/0/1/0/all/0/1\">Andrea Castellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1\">Sebastian Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Detecting drifts in data is essential for machine learning applications, as\nchanges in the statistics of processed data typically has a profound influence\non the performance of trained models. Most of the available drift detection\nmethods require access to true labels during inference time. In a real-world\nscenario, true labels usually available only during model training. In this\nwork, we propose a novel task-sensitive drift detection framework, which is\nable to detect drifts without access to true labels during inference. It\nutilizes metric learning of a constrained low-dimensional embedding\nrepresentation of the input data, which is best suited for the classification\ntask. It is able to detect real drift, where the drift affects the\nclassification performance, while it properly ignores virtual drift, where the\nclassification performance is not affected by the drift. In the proposed\nframework, the actual method to detect a change in the statistics of incoming\ndata samples can be chosen freely. We also propose the two change detection\nmethods, which are based on the exponential moving average and a modified\n$z$-score, respectively. We evaluate the performance of the proposed framework\nwith a novel metric, which accumulates the standard metrics of detection\naccuracy, false positive rate and detection delay into one value. Experimental\nevaluation on nine benchmarks datasets, with different types of drift,\ndemonstrates that the proposed framework can reliably detect drifts, and\noutperforms state-of-the-art unsupervised drift detection approaches.",
          "link": "http://arxiv.org/abs/2108.06980",
          "publishedOn": "2021-08-17T01:54:47.282Z",
          "wordCount": 673,
          "title": "Task-Sensitive Concept Drift Detector with Metric Learning. (arXiv:2108.06980v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.09843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Weijun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaohu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Rongxing Lu</a>",
          "description": "Although federated learning improves privacy of training data by exchanging\nlocal gradients or parameters rather than raw data, the adversary still can\nleverage local gradients and parameters to obtain local training data by\nlaunching reconstruction and membership inference attacks. To defend such\nprivacy attacks, many noises perturbation methods (like differential privacy or\nCountSketch matrix) have been widely designed. However, the strong defence\nability and high learning accuracy of these schemes cannot be ensured at the\nsame time, which will impede the wide application of FL in practice (especially\nfor medical or financial institutions that require both high accuracy and\nstrong privacy guarantee). To overcome this issue, in this paper, we propose\n\\emph{an efficient model perturbation method for federated learning} to defend\nreconstruction and membership inference attacks launched by curious clients. On\nthe one hand, similar to the differential privacy, our method also selects\nrandom numbers as perturbed noises added to the global model parameters, and\nthus it is very efficient and easy to be integrated in practice. Meanwhile, the\nrandom selected noises are positive real numbers and the corresponding value\ncan be arbitrarily large, and thus the strong defence ability can be ensured.\nOn the other hand, unlike differential privacy or other perturbation methods\nthat cannot eliminate the added noises, our method allows the server to recover\nthe true gradients by eliminating the added noises. Therefore, our method does\nnot hinder learning accuracy at all.",
          "link": "http://arxiv.org/abs/2002.09843",
          "publishedOn": "2021-08-17T01:54:47.276Z",
          "wordCount": 750,
          "title": "An Accuracy-Lossless Perturbation Method for Defending Privacy Attacks in Federated Learning. (arXiv:2002.09843v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cengguang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_D/0/1/0/all/0/1\">Di Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>",
          "description": "Vertical federated learning (VFL) leverages various privacy-preserving\nalgorithms, e.g., homomorphic encryption or secret sharing based SecureBoost,\nto ensure data privacy. However, these algorithms all require a semi-honest\nsecure definition, which raises concerns in real-world applications. In this\npaper, we present Aegis, a trusted, automatic, and accurate verification\nframework to verify the security of VFL jobs. Aegis is separated from local\nparties to ensure the security of the framework. Furthermore, it automatically\nadapts to evolving VFL algorithms by defining the VFL job as a finite state\nmachine to uniformly verify different algorithms and reproduce the entire job\nto provide more accurate verification. We implement and evaluate Aegis with\ndifferent threat models on financial and medical datasets. Evaluation results\nshow that: 1) Aegis can detect 95% threat models, and 2) it provides\nfine-grained verification results within 84% of the total VFL job time.",
          "link": "http://arxiv.org/abs/2108.06958",
          "publishedOn": "2021-08-17T01:54:47.269Z",
          "wordCount": 616,
          "title": "Aegis: A Trusted, Automatic and Accurate Verification Framework for Vertical Federated Learning. (arXiv:2108.06958v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06430",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bradford_E/0/1/0/all/0/1\">E. Bradford</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Imsland_L/0/1/0/all/0/1\">L. Imsland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reble_M/0/1/0/all/0/1\">M. Reble</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rio_Chanona_E/0/1/0/all/0/1\">E.A. del Rio-Chanona</a>",
          "description": "Nonlinear model predictive control (NMPC) is an efficient approach for the\ncontrol of nonlinear multivariable dynamic systems with constraints, which\nhowever requires an accurate plant model. Plant models can often be determined\nfrom first principles, parts of the model are however difficult to derive using\nphysical laws alone. In this paper a hybrid Gaussian process (GP) first\nprinciples modeling scheme is proposed to overcome this issue, which exploits\nGPs to model the parts of the dynamic system that are difficult to describe\nusing first principles. GPs not only give accurate predictions, but also\nquantify the residual uncertainty of this model. It is vital to account for\nthis uncertainty in the control algorithm, to prevent constraint violations and\nperformance deterioration. Monte Carlo samples of the GPs are generated offline\nto tighten constraints of the NMPC to ensure joint probabilistic constraint\nsatisfaction online. Advantages of our method include fast online evaluation\ntimes, possibility to account for online learning alleviating conservativeness,\nand exploiting the flexibility of GPs and the data efficiency of first\nprinciple models. The algorithm is verified on a case study involving a\nchallenging semi-batch bioreactor.",
          "link": "http://arxiv.org/abs/2108.06430",
          "publishedOn": "2021-08-17T01:54:47.251Z",
          "wordCount": 673,
          "title": "Hybrid Gaussian Process Modeling Applied to Economic Stochastic Model Predictive Control of Batch Processes. (arXiv:2108.06430v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>",
          "description": "Federated learning (FL) emerged as a promising learning paradigm to enable a\nmultitude of participants to construct a joint ML model without exposing their\nprivate training data. Existing FL designs have been shown to exhibit\nvulnerabilities which can be exploited by adversaries both within and outside\nof the system to compromise data privacy. However, most current works conduct\nattacks by leveraging gradients on a small batch of data, which is less\npractical in FL. In this work, we consider a more practical and interesting\nscenario in which participants share their epoch-averaged gradients (share\ngradients after at least 1 epoch of local training) rather than per-example or\nsmall batch-averaged gradients as in previous works. We perform the first\nsystematic evaluation of attribute reconstruction attack (ARA) launched by the\nmalicious server in the FL system, and empirically demonstrate that the shared\nepoch-averaged local model gradients can reveal sensitive attributes of local\ntraining data of any victim participant. To achieve this goal, we develop a\nmore effective and efficient gradient matching based method called cos-matching\nto reconstruct the training data attributes. We evaluate our attacks on a\nvariety of real-world datasets, scenarios, assumptions. Our experiments show\nthat our proposed method achieves better attribute attack performance than most\nexisting baselines.",
          "link": "http://arxiv.org/abs/2108.06910",
          "publishedOn": "2021-08-17T01:54:47.245Z",
          "wordCount": 643,
          "title": "A Novel Attribute Reconstruction Attack in Federated Learning. (arXiv:2108.06910v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changliu Liu</a>",
          "description": "Neural networks (NNs) are widely used for classification tasks for their\nremarkable performance. However, the robustness and accuracy of NNs heavily\ndepend on the training data. In many applications, massive training data is\nusually not available. To address the challenge, this paper proposes an\niterative adversarial data augmentation (IADA) framework to learn neural\nnetwork models from an insufficient amount of training data. The method uses\nformal verification to identify the most \"confusing\" input samples, and\nleverages human guidance to safely and iteratively augment the training data\nwith these samples. The proposed framework is applied to an artificial 2D\ndataset, the MNIST dataset, and a human motion dataset. By applying IADA to\nfully-connected NN classifiers, we show that our training method can improve\nthe robustness and accuracy of the learned model. By comparing to regular\nsupervised training, on the MNIST dataset, the average perturbation bound\nimproved 107.4%. The classification accuracy improved 1.77%, 3.76%, 10.85% on\nthe 2D dataset, the MNIST dataset, and the human motion dataset respectively.",
          "link": "http://arxiv.org/abs/2108.06871",
          "publishedOn": "2021-08-17T01:54:47.239Z",
          "wordCount": 619,
          "title": "IADA: Iterative Adversarial Data Augmentation Using Formal Verification and Expert Guidance. (arXiv:2108.06871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1\">Constantinos Daskalakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishelson_M/0/1/0/all/0/1\">Maxwell Fishelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1\">Noah Golowich</a>",
          "description": "We show that Optimistic Hedge -- a common variant of\nmultiplicative-weights-updates with recency bias -- attains ${\\rm poly}(\\log\nT)$ regret in multi-player general-sum games. In particular, when every player\nof the game uses Optimistic Hedge to iteratively update her strategy in\nresponse to the history of play so far, then after $T$ rounds of interaction,\neach player experiences total regret that is ${\\rm poly}(\\log T)$. Our bound\nimproves, exponentially, the $O({T}^{1/2})$ regret attainable by standard\nno-regret learners in games, the $O(T^{1/4})$ regret attainable by no-regret\nlearners with recency bias (Syrgkanis et al., 2015), and the ${O}(T^{1/6})$\nbound that was recently shown for Optimistic Hedge in the special case of\ntwo-player games (Chen & Pen, 2020). A corollary of our bound is that\nOptimistic Hedge converges to coarse correlated equilibrium in general games at\na rate of $\\tilde{O}\\left(\\frac 1T\\right)$.",
          "link": "http://arxiv.org/abs/2108.06924",
          "publishedOn": "2021-08-17T01:54:47.233Z",
          "wordCount": 565,
          "title": "Near-Optimal No-Regret Learning in General Games. (arXiv:2108.06924v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "We introduce a novel dataset for architectural style classification,\nconsisting of 9,485 images of church buildings. Both images and style labels\nwere sourced from Wikipedia. The dataset can serve as a benchmark for various\nresearch fields, as it combines numerous real-world challenges: fine-grained\ndistinctions between classes based on subtle visual features, a comparatively\nsmall sample size, a highly imbalanced class distribution, a high variance of\nviewpoints, and a hierarchical organization of labels, where only some images\nare labeled at the most precise level. In addition, we provide 631 bounding box\nannotations of characteristic visual features for 139 churches from four major\ncategories. These annotations can, for example, be useful for research on\nfine-grained classification, where additional expert knowledge about\ndistinctive object parts is often available. Images and annotations are\navailable at: https://doi.org/10.5281/zenodo.5166987",
          "link": "http://arxiv.org/abs/2108.06959",
          "publishedOn": "2021-08-17T01:54:47.227Z",
          "wordCount": 580,
          "title": "WikiChurches: A Fine-Grained Dataset of Architectural Styles with Real-World Challenges. (arXiv:2108.06959v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06717",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Oh_Y/0/1/0/all/0/1\">YongKyung Oh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kwak_J/0/1/0/all/0/1\">JiIn Kwak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1\">JuYoung Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kim_S/0/1/0/all/0/1\">Sungil Kim</a>",
          "description": "Considering how congestion will propagate in the near future, understanding\ntraffic congestion propagation has become crucial in GPS navigation systems for\nproviding users with a more accurate estimated time of arrival (ETA). However,\nproviding the exact ETA during congestion is a challenge owing to the complex\npropagation process between roads and high uncertainty regarding the future\nbehavior of the process. Recent studies have focused on finding frequent\ncongestion propagation patterns and determining the propagation probabilities.\nBy contrast, this study proposes a novel time delay estimation method for\ntraffic congestion propagation between roads using lag-specific transfer\nentropy (TE). Nonlinear normalization with a sliding window is used to\neffectively reveal the causal relationship between the source and target time\nseries in calculating the TE. Moreover, Markov bootstrap techniques were\nadopted to quantify the uncertainty in the time delay estimator. To the best of\nour knowledge, the time delay estimation method presented in this article is\nthe first to determine the time delay between roads for any congestion\npropagation pattern. The proposed method was validated using simulated data as\nwell as real user trajectory data obtained from a major GPS navigation system\napplied in South Korea.",
          "link": "http://arxiv.org/abs/2108.06717",
          "publishedOn": "2021-08-17T01:54:47.209Z",
          "wordCount": 638,
          "title": "Time Delay Estimation of Traffic Congestion Propagation based on Transfer Entropy. (arXiv:2108.06717v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuyun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yufei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper aims to explain adversarial attacks in terms of how adversarial\nperturbations contribute to the attacking task. We estimate attributions of\ndifferent image regions to the decrease of the attacking cost based on the\nShapley value. We define and quantify interactions among adversarial\nperturbation pixels, and decompose the entire perturbation map into relatively\nindependent perturbation components. The decomposition of the perturbation map\nshows that adversarially-trained DNNs have more perturbation components in the\nforeground than normally-trained DNNs. Moreover, compared to the\nnormally-trained DNN, the adversarially-trained DNN have more components which\nmainly decrease the score of the true category. Above analyses provide new\ninsights into the understanding of adversarial attacks.",
          "link": "http://arxiv.org/abs/2108.06895",
          "publishedOn": "2021-08-17T01:54:47.203Z",
          "wordCount": 555,
          "title": "Interpreting Attributions and Interactions of Adversarial Attacks. (arXiv:2108.06895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karpov_N/0/1/0/all/0/1\">Nikolai Karpov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>",
          "description": "We study Thompson Sampling algorithms for stochastic multi-armed bandits in\nthe batched setting, in which we want to minimize the regret over a sequence of\narm pulls using a small number of policy changes (or, batches). We propose two\nalgorithms and demonstrate their effectiveness by experiments on both synthetic\nand real datasets. We also analyze the proposed algorithms from the theoretical\naspect and obtain almost tight regret-batches tradeoffs for the two-arm case.",
          "link": "http://arxiv.org/abs/2108.06812",
          "publishedOn": "2021-08-17T01:54:47.176Z",
          "wordCount": 496,
          "title": "Batched Thompson Sampling for Multi-Armed Bandits. (arXiv:2108.06812v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_C/0/1/0/all/0/1\">Chayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noman_N/0/1/0/all/0/1\">Nasimul Noman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_M/0/1/0/all/0/1\">Mohsen Zamani</a>",
          "description": "Actor-critic (AC) algorithms are known for their efficacy and high\nperformance in solving reinforcement learning problems, but they also suffer\nfrom low sampling efficiency. An AC based policy optimization process is\niterative and needs to frequently access the agent-environment system to\nevaluate and update the policy by rolling out the policy, collecting rewards\nand states (i.e. samples), and learning from them. It ultimately requires a\nhuge number of samples to learn an optimal policy. To improve sampling\nefficiency, we propose a strategy to optimize the training dataset that\ncontains significantly less samples collected from the AC process. The dataset\noptimization is made of a best episode only operation, a policy\nparameter-fitness model, and a genetic algorithm module. The optimal policy\nnetwork trained by the optimized training dataset exhibits superior performance\ncompared to many contemporary AC algorithms in controlling autonomous dynamical\nsystems. Evaluation on standard benchmarks show that the method improves\nsampling efficiency, ensures faster convergence to optima, and is more\ndata-efficient than its counterparts.",
          "link": "http://arxiv.org/abs/2108.06911",
          "publishedOn": "2021-08-17T01:54:47.138Z",
          "wordCount": 599,
          "title": "Optimal Actor-Critic Policy with Optimized Training Datasets. (arXiv:2108.06911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zeyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_P/0/1/0/all/0/1\">Pierce KH Chow</a>",
          "description": "Accurate automatic liver and tumor segmentation plays a vital role in\ntreatment planning and disease monitoring. Recently, deep convolutional neural\nnetwork (DCNNs) has obtained tremendous success in 2D and 3D medical image\nsegmentation. However, 2D DCNNs cannot fully leverage the inter-slice\ninformation, while 3D DCNNs are computationally expensive and memory intensive.\nTo address these issues, we first propose a novel dense-sparse training flow\nfrom a data perspective, in which, densely adjacent slices and sparsely\nadjacent slices are extracted as inputs for regularizing DCNNs, thereby\nimproving the model performance. Moreover, we design a 2.5D light-weight\nnnU-Net from a network perspective, in which, depthwise separable convolutions\nare adopted to improve the efficiency. Extensive experiments on the LiTS\ndataset have demonstrated the superiority of the proposed method.",
          "link": "http://arxiv.org/abs/2108.06761",
          "publishedOn": "2021-08-17T01:54:47.128Z",
          "wordCount": 599,
          "title": "Multi-Slice Dense-Sparse Learning for Efficient Liver and Tumor Segmentation. (arXiv:2108.06761v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barba_O/0/1/0/all/0/1\">Ocean M. Barba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calbay_F/0/1/0/all/0/1\">Franz Arvin T. Calbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francisco_A/0/1/0/all/0/1\">Angelica Jane S. Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1\">Angel Luis D. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponay_C/0/1/0/all/0/1\">Charmaine S. Ponay</a>",
          "description": "Social media has played a huge part on how people get informed and\ncommunicate with one another. It has helped people express their needs due to\ndistress especially during disasters. Because posts made through it are\npublicly accessible by default, Twitter is among the most helpful social media\nsites in times of disaster. With this, the study aims to assess the needs\nexpressed during calamities by Filipinos on Twitter. Data were gathered and\nclassified as either disaster-related or unrelated with the use of Na\\\"ive\nBayes classifier. After this, the disaster-related tweets were clustered per\ndisaster type using Incremental Clustering Algorithm, and then sub-clustered\nbased on the location and time of the tweet using Density-based Spatiotemporal\nClustering Algorithm. Lastly, using Support Vector Machines, the tweets were\nclassified according to the expressed need, such as shelter, rescue, relief,\ncash, prayer, and others. After conducting the study, results showed that the\nIncremental Clustering Algorithm and Density-Based Spatiotemporal Clustering\nAlgorithm were able to cluster the tweets with f-measure scores of 47.20% and\n82.28% respectively. Also, the Na\\\"ive Bayes and Support Vector Machines were\nable to classify with an average f-measure score of 97% and an average accuracy\nof 77.57% respectively.",
          "link": "http://arxiv.org/abs/2108.06853",
          "publishedOn": "2021-08-17T01:54:47.103Z",
          "wordCount": 661,
          "title": "Clustering Filipino Disaster-Related Tweets Using Incremental and Density-Based Spatiotemporal Algorithm with Support Vector Machines for Needs Assessment 2. (arXiv:2108.06853v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saini_A/0/1/0/all/0/1\">Aditya Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1\">Ranjitha Prasad</a>",
          "description": "Owing to tremendous performance improvements in data-intensive domains,\nmachine learning (ML) has garnered immense interest in the research community.\nHowever, these ML models turn out to be black boxes, which are tough to\ninterpret, resulting in a direct decrease in productivity. Local Interpretable\nModel-Agnostic Explanations (LIME) is a popular technique for explaining the\nprediction of a single instance. Although LIME is simple and versatile, it\nsuffers from instability in the generated explanations. In this paper, we\npropose a Gaussian Process (GP) based variation of locally interpretable\nmodels. We employ a smart sampling strategy based on the acquisition functions\nin Bayesian optimization. Further, we employ the automatic relevance\ndetermination based covariance function in GP, with separate length-scale\nparameters for each feature, where the reciprocal of lengthscale parameters\nserve as feature explanations. We illustrate the performance of the proposed\ntechnique on two real-world datasets, and demonstrate the superior stability of\nthe proposed technique. Furthermore, we demonstrate that the proposed technique\nis able to generate faithful explanations using much fewer samples as compared\nto LIME.",
          "link": "http://arxiv.org/abs/2108.06907",
          "publishedOn": "2021-08-17T01:54:47.094Z",
          "wordCount": 598,
          "title": "Locally Interpretable Model Agnostic Explanations using Gaussian Processes. (arXiv:2108.06907v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lima_M/0/1/0/all/0/1\">Markus V. S. Lima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaves_G/0/1/0/all/0/1\">Gabriel S. Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_T/0/1/0/all/0/1\">Tadeu N. Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diniz_P/0/1/0/all/0/1\">Paulo S. R. Diniz</a>",
          "description": "Adaptive filters exploiting sparsity have been a very active research field,\namong which the algorithms that follow the \"proportional-update principle\", the\nso-called proportionate-type algorithms, are very popular. Indeed, there are\nhundreds of works on proportionate-type algorithms and, therefore, their\nadvantages are widely known. This paper addresses the unexplored drawbacks and\nlimitations of using proportional updates and their practical impacts. Our\nfindings include the theoretical justification for the poor performance of\nthese algorithms in several sparse scenarios, and also when dealing with\nnon-stationary and compressible systems. Simulation results corroborating the\ntheory are presented.",
          "link": "http://arxiv.org/abs/2108.06846",
          "publishedOn": "2021-08-17T01:54:47.087Z",
          "wordCount": 529,
          "title": "Do Proportionate Algorithms Exploit Sparsity?. (arXiv:2108.06846v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>",
          "description": "In recent years, Neural Machine Translator (NMT) has shown promise in\nautomatically editing source code. Typical NMT based code editor only considers\nthe code that needs to be changed as input and suggests developers with a\nranked list of patched code to choose from - where the correct one may not\nalways be at the top of the list. While NMT based code editing systems generate\na broad spectrum of plausible patches, the correct one depends on the\ndevelopers' requirement and often on the context where the patch is applied.\nThus, if developers provide some hints, using natural language, or providing\npatch context, NMT models can benefit from them. As a proof of concept, in this\nresearch, we leverage three modalities of information: edit location, edit code\ncontext, commit messages (as a proxy of developers' hint in natural language)\nto automatically generate edits with NMT models. To that end, we build MODIT, a\nmulti-modal NMT based code editing engine. With in-depth investigation and\nanalysis, we show that developers' hint as an input modality can narrow the\nsearch space for patches and outperform state-of-the-art models to generate\ncorrectly patched code in top-1 position.",
          "link": "http://arxiv.org/abs/2108.06645",
          "publishedOn": "2021-08-17T01:54:47.080Z",
          "wordCount": 636,
          "title": "On Multi-Modal Learning of Editing Source Code. (arXiv:2108.06645v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagus_B/0/1/0/all/0/1\">Benedikt Bagus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gepperth_A/0/1/0/all/0/1\">Alexander Gepperth</a>",
          "description": "Continual learning (CL) is a major challenge of machine learning (ML) and\ndescribes the ability to learn several tasks sequentially without catastrophic\nforgetting (CF). Recent works indicate that CL is a complex topic, even more so\nwhen real-world scenarios with multiple constraints are involved. Several\nsolution classes have been proposed, of which so-called replay-based approaches\nseem very promising due to their simplicity and robustness. Such approaches\nstore a subset of past samples in a dedicated memory for later processing:\nwhile this does not solve all problems, good results have been obtained. In\nthis article, we empirically investigate replay-based approaches of continual\nlearning and assess their potential for applications. Selected recent\napproaches as well as own proposals are compared on a common set of benchmarks,\nwith a particular focus on assessing the performance of different sample\nselection strategies. We find that the impact of sample selection increases\nwhen a smaller number of samples is stored. Nevertheless, performance varies\nstrongly between different replay approaches. Surprisingly, we find that the\nmost naive rehearsal-based approaches that we propose here can outperform\nrecent state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.06758",
          "publishedOn": "2021-08-17T01:54:47.073Z",
          "wordCount": 615,
          "title": "An Investigation of Replay-based Approaches for Continual Learning. (arXiv:2108.06758v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_J/0/1/0/all/0/1\">Jessica Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narisetty_C/0/1/0/all/0/1\">Chaitanya Narisetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>",
          "description": "We motivate and propose a suite of simple but effective improvements for\nconcept-to-text generation called SAPPHIRE: Set Augmentation and Post-hoc\nPHrase Infilling and REcombination. We demonstrate their effectiveness on\ngenerative commonsense reasoning, a.k.a. the CommonGen task, through\nexperiments using both BART and T5 models. Through extensive automatic and\nhuman evaluation, we show that SAPPHIRE noticeably improves model performance.\nAn in-depth qualitative analysis illustrates that SAPPHIRE effectively\naddresses many issues of the baseline model generations, including lack of\ncommonsense, insufficient specificity, and poor fluency.",
          "link": "http://arxiv.org/abs/2108.06643",
          "publishedOn": "2021-08-17T01:54:47.068Z",
          "wordCount": 532,
          "title": "SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation. (arXiv:2108.06643v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cahall_D/0/1/0/all/0/1\">Daniel E. Cahall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal C. Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathallah_Shaykh_H/0/1/0/all/0/1\">Hassan M. Fathallah-Shaykh</a>",
          "description": "Magnetic resonance imaging (MRI) is routinely used for brain tumor diagnosis,\ntreatment planning, and post-treatment surveillance. Recently, various models\nbased on deep neural networks have been proposed for the pixel-level\nsegmentation of tumors in brain MRIs. However, the structural variations,\nspatial dissimilarities, and intensity inhomogeneity in MRIs make segmentation\na challenging task. We propose a new end-to-end brain tumor segmentation\narchitecture based on U-Net that integrates Inception modules and dilated\nconvolutions into its contracting and expanding paths. This allows us to\nextract local structural as well as global contextual information. We performed\nsegmentation of glioma sub-regions, including tumor core, enhancing tumor, and\nwhole tumor using Brain Tumor Segmentation (BraTS) 2018 dataset. Our proposed\nmodel performed significantly better than the state-of-the-art U-Net-based\nmodel ($p<0.05$) for tumor core and whole tumor segmentation.",
          "link": "http://arxiv.org/abs/2108.06772",
          "publishedOn": "2021-08-17T01:54:47.050Z",
          "wordCount": 572,
          "title": "Dilated Inception U-Net (DIU-Net) for Brain Tumor Segmentation. (arXiv:2108.06772v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>",
          "description": "Recent works have theoretically and empirically shown that deep neural\nnetworks (DNNs) have an inherent vulnerability to small perturbations. Applying\nthe Deep k-Nearest Neighbors (DkNN) classifier, we observe a dramatically\nincreasing robustness-accuracy trade-off as the layer goes deeper. In this\nwork, we propose a Deep Adversarially-Enhanced k-Nearest Neighbors (DAEkNN)\nmethod which achieves higher robustness than DkNN and mitigates the\nrobustness-accuracy trade-off in deep layers through two key elements. First,\nDAEkNN is based on an adversarially trained model. Second, DAEkNN makes\npredictions by leveraging a weighted combination of benign and adversarial\ntraining data. Empirically, we find that DAEkNN improves both the robustness\nand the robustness-accuracy trade-off on MNIST and CIFAR-10 datasets.",
          "link": "http://arxiv.org/abs/2108.06797",
          "publishedOn": "2021-08-17T01:54:47.042Z",
          "wordCount": 539,
          "title": "Deep Adversarially-Enhanced k-Nearest Neighbors. (arXiv:2108.06797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yunhui Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Graph structured data have enabled several successful applications such as\nrecommendation systems and traffic prediction, given the rich node features and\nedges information. However, these high-dimensional features and high-order\nadjacency information are usually heterogeneous and held by different data\nholders in practice. Given such vertical data partition (e.g., one data holder\nwill only own either the node features or edge information), different data\nholders have to develop efficient joint training protocols rather than directly\ntransfer data to each other due to privacy concerns. In this paper, we focus on\nthe edge privacy, and consider a training scenario where Bob with node features\nwill first send training node features to Alice who owns the adjacency\ninformation. Alice will then train a graph neural network (GNN) with the joint\ninformation and release an inference API. During inference, Bob is able to\nprovide test node features and query the API to obtain the predictions for test\nnodes. Under this setting, we first propose a privacy attack LinkTeller via\ninfluence analysis to infer the private edge information held by Alice via\ndesigning adversarial queries for Bob. We then empirically show that LinkTeller\nis able to recover a significant amount of private edges, outperforming\nexisting baselines. To further evaluate the privacy leakage, we adapt an\nexisting algorithm for differentially private graph convolutional network (DP\nGCN) training and propose a new DP GCN mechanism LapGraph. We show that these\nDP GCN mechanisms are not always resilient against LinkTeller empirically under\nmild privacy guarantees ($\\varepsilon>5$). Our studies will shed light on\nfuture research towards designing more resilient privacy-preserving GCN models;\nin the meantime, provide an in-depth understanding of the tradeoff between GCN\nmodel utility and robustness against potential privacy attacks.",
          "link": "http://arxiv.org/abs/2108.06504",
          "publishedOn": "2021-08-17T01:54:47.034Z",
          "wordCount": 719,
          "title": "LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis. (arXiv:2108.06504v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Haitao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Initialization plays a critical role in the training of deep neural networks\n(DNN). Existing initialization strategies mainly focus on stabilizing the\ntraining process to mitigate gradient vanish/explosion problems. However, these\ninitialization methods are lacking in consideration about how to enhance\ngeneralization ability. The Information Bottleneck (IB) theory is a well-known\nunderstanding framework to provide an explanation about the generalization of\nDNN. Guided by the insights provided by IB theory, we design two criteria for\nbetter initializing DNN. And we further design a neuron campaign initialization\nalgorithm to efficiently select a good initialization for a neural network on a\ngiven dataset. The experiments on MNIST dataset show that our method can lead\nto a better generalization performance with faster convergence.",
          "link": "http://arxiv.org/abs/2108.06530",
          "publishedOn": "2021-08-17T01:54:47.028Z",
          "wordCount": 568,
          "title": "Neuron Campaign for Initialization Guided by Information Bottleneck Theory. (arXiv:2108.06530v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Despite the impressive clustering performance and efficiency in\ncharacterizing both the relationship between data and cluster structure,\nexisting graph-based multi-view clustering methods still have the following\ndrawbacks. They suffer from the expensive time burden due to both the\nconstruction of graphs and eigen-decomposition of Laplacian matrix, and fail to\nexplore the cluster structure of large-scale data. Moreover, they require a\npost-processing to get the final clustering, resulting in suboptimal\nperformance. Furthermore, rank of the learned view-consensus graph cannot\napproximate the target rank. In this paper, drawing the inspiration from the\nbipartite graph, we propose an effective and efficient graph learning model for\nmulti-view clustering. Specifically, our method exploits the view-similar\nbetween graphs of different views by the minimization of tensor Schatten\np-norm, which well characterizes both the spatial structure and complementary\ninformation embedded in graphs of different views. We learn view-consensus\ngraph with adaptively weighted strategy and connectivity constraint such that\nthe connected components indicates clusters directly. Our proposed algorithm is\ntime-economical and obtains the stable results and scales well with the data\nsize. Extensive experimental results indicate that our method is superior to\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.06734",
          "publishedOn": "2021-08-17T01:54:47.021Z",
          "wordCount": 616,
          "title": "Effective and Efficient Graph Learning for Multi-view Clustering. (arXiv:2108.06734v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yanwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xun Yu Zhou</a>",
          "description": "We propose a unified framework to study policy evaluation (PE) and the\nassociated temporal difference (TD) methods for reinforcement learning in\ncontinuous time and space. We show that PE is equivalent to maintaining the\nmartingale condition of a process. From this perspective, we find that the\nmean--square TD error approximates the quadratic variation of the martingale\nand thus is not a suitable objective for PE. We present two methods to use the\nmartingale characterization for designing PE algorithms. The first one\nminimizes a \"martingale loss function\", whose solution is proved to be the best\napproximation of the true value function in the mean--square sense. This method\ninterprets the classical gradient Monte-Carlo algorithm. The second method is\nbased on a system of equations called the \"martingale orthogonality conditions\"\nwith \"test functions\". Solving these equations in different ways recovers\nvarious classical TD algorithms, such as TD($\\lambda$), LSTD, and GTD.\nDifferent choices of test functions determine in what sense the resulting\nsolutions approximate the true value function. Moreover, we prove that any\nconvergent time-discretized algorithm converges to its continuous-time\ncounterpart as the mesh size goes to zero. We demonstrate the theoretical\nresults and corresponding algorithms with numerical experiments and\napplications.",
          "link": "http://arxiv.org/abs/2108.06655",
          "publishedOn": "2021-08-17T01:54:47.001Z",
          "wordCount": 644,
          "title": "Policy Evaluation and Temporal-Difference Learning in Continuous Time and Space: A Martingale Approach. (arXiv:2108.06655v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yitian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kunlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kunjin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>",
          "description": "We took part in the city brain challenge competition and achieved the 8th\nplace. In this competition, the players are provided with a real-world\ncity-scale road network and its traffic demand derived from real traffic data.\nThe players are asked to coordinate the traffic signals with a self-designed\nagent to maximize the number of vehicles served while maintaining an acceptable\ndelay. In this abstract paper, we present an overall analysis and our detailed\nsolution to this competition. Our approach is mainly based on the adaptation of\nthe deep Q-network (DQN) for real-time traffic signal control. From our\nperspective, the major challenge of this competition is how to extend the\nclassical DQN framework to traffic signals control in real-world complex road\nnetwork and traffic flow situation. After trying and implementing several\nclassical reward functions, we finally chose to apply our newly-designed reward\nin our agent. By applying our newly-proposed reward function and carefully\ntuning the control scheme, an agent based on a single DQN model can rank among\nthe top 15 teams. We hope this paper could serve, to some extent, as a baseline\nsolution to traffic signal control of real-world road network and inspire\nfurther attempts and researches.",
          "link": "http://arxiv.org/abs/2108.06491",
          "publishedOn": "2021-08-17T01:54:46.995Z",
          "wordCount": 644,
          "title": "DQN Control Solution for KDD Cup 2021 City Brain Challenge. (arXiv:2108.06491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_M/0/1/0/all/0/1\">Mostafa Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "This paper studies the subspace clustering problem in which data points\ncollected from high-dimensional ambient space lie in a union of linear\nsubspaces. Subspace clustering becomes challenging when the dimension of\nintersection between subspaces is large and most of the self-representation\nbased methods are sensitive to the intersection between the span of clusters.\nIn sharp contrast to the self-representation based methods, a recently proposed\nclustering method termed Innovation Pursuit, computed a set of optimal\ndirections (directions of innovation) to build the adjacency matrix. This paper\nfocuses on the Innovation Pursuit Algorithm to shed light on its impressive\nperformance when the subspaces are heavily intersected. It is shown that in\ncontrast to most of the existing methods which require the subspaces to be\nsufficiently incoherent with each other, Innovation Pursuit only requires the\ninnovative components of the subspaces to be sufficiently incoherent with each\nother. These new sufficient conditions allow the clusters to be strongly close\nto each other. Motivated by the presented theoretical analysis, a simple yet\neffective projection based technique is proposed which we show with both\nnumerical and theoretical results that it can boost the performance of\nInnovation Pursuit.",
          "link": "http://arxiv.org/abs/2108.06888",
          "publishedOn": "2021-08-17T01:54:46.988Z",
          "wordCount": 615,
          "title": "Provable Data Clustering via Innovation Search. (arXiv:2108.06888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratik Kumar</a>",
          "description": "With surge in online platforms, there has been an upsurge in the user\nengagement on these platforms via comments and reactions. A large portion of\nsuch textual comments are abusive, rude and offensive to the audience. With\nmachine learning systems in-place to check such comments coming onto platform,\nbiases present in the training data gets passed onto the classifier leading to\ndiscrimination against a set of classes, religion and gender. In this work, we\nevaluate different classifiers and feature to estimate the bias in these\nclassifiers along with their performance on downstream task of toxicity\nclassification. Results show that improvement in performance of automatic toxic\ncomment detection models is positively correlated to mitigating biases in these\nmodels. In our work, LSTM with attention mechanism proved to be a better\nmodelling strategy than a CNN model. Further analysis shows that fasttext\nembeddings is marginally preferable than glove embeddings on training models\nfor toxicity comment detection. Deeper analysis reveals the findings that such\nautomatic models are particularly biased to specific identity groups even\nthough the model has a high AUC score. Finally, in effort to mitigate bias in\ntoxicity detection models, a multi-task setup trained with auxiliary task of\ntoxicity sub-types proved to be useful leading to upto 0.26% (6% relative) gain\nin AUC scores.",
          "link": "http://arxiv.org/abs/2108.06487",
          "publishedOn": "2021-08-17T01:54:46.981Z",
          "wordCount": 659,
          "title": "Investigating Bias In Automatic Toxic Comment Detection: An Empirical Study. (arXiv:2108.06487v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossen_M/0/1/0/all/0/1\">Md Imran Hossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Ashraful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anowar_F/0/1/0/all/0/1\">Farzana Anowar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_E/0/1/0/all/0/1\">Eshtiak Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammad Masudur Rahman</a>",
          "description": "Due to the variety of cyber-attacks or threats, the cybersecurity community\nhas been enhancing the traditional security control mechanisms to an advanced\nlevel so that automated tools can encounter potential security threats. Very\nrecently a term, Cyber Threat Intelligence (CTI) has been represented as one of\nthe proactive and robust mechanisms because of its automated cybersecurity\nthreat prediction based on data. In general, CTI collects and analyses data\nfrom various sources e.g. online security forums, social media where cyber\nenthusiasts, analysts, even cybercriminals discuss cyber or computer security\nrelated topics and discovers potential threats based on the analysis. As the\nmanual analysis of every such discussion i.e. posts on online platforms is\ntime-consuming, inefficient, and susceptible to errors, CTI as an automated\ntool can perform uniquely to detect cyber threats. In this paper, our goal is\nto identify and explore relevant CTI from hacker forums by using different\nsupervised and unsupervised learning techniques. To this end, we collect data\nfrom a real hacker forum and constructed two datasets: a binary dataset and a\nmulti-class dataset. Our binary dataset contains two classes one containing\ncybersecurity-relevant posts and another one containing posts that are not\nrelated to security. This dataset is constructed using simple keyword search\ntechnique. Using a similar approach, we further categorize posts from\nsecurity-relevant posts into five different threat categories. We then applied\nseveral machine learning classifiers along with deep neural network-based\nclassifiers and use them on the datasets to compare their performances. We also\ntested the classifiers on a leaked dataset with labels named nulled.io as our\nground truth. We further explore the datasets using unsupervised techniques\ni.e. Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization\n(NMF).",
          "link": "http://arxiv.org/abs/2108.06862",
          "publishedOn": "2021-08-17T01:54:46.973Z",
          "wordCount": 730,
          "title": "Generating Cyber Threat Intelligence to Discover Potential Security Threats Using Classification and Topic Modeling. (arXiv:2108.06862v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_M/0/1/0/all/0/1\">Mohammad Reza Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarei_A/0/1/0/all/0/1\">Ariyan Zarei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Hoshin V. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnard_K/0/1/0/all/0/1\">Kobus Barnard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrangi_A/0/1/0/all/0/1\">Ali Behrangi</a>",
          "description": "Accurate and timely estimation of precipitation is critical for issuing\nhazard warnings (e.g., for flash floods or landslides). Current remotely sensed\nprecipitation products have a few hours of latency, associated with the\nacquisition and processing of satellite data. By applying a robust nowcasting\nsystem to these products, it is (in principle) possible to reduce this latency\nand improve their applicability, value, and impact. However, the development of\nsuch a system is complicated by the chaotic nature of the atmosphere, and the\nconsequent rapid changes that can occur in the structures of precipitation\nsystems In this work, we develop two approaches (hereafter referred to as\nNowcasting-Nets) that use Recurrent and Convolutional deep neural network\nstructures to address the challenge of precipitation nowcasting. A total of\nfive models are trained using Global Precipitation Measurement (GPM) Integrated\nMulti-satellitE Retrievals for GPM (IMERG) precipitation data over the Eastern\nContiguous United States (CONUS) and then tested against independent data for\nthe Eastern and Western CONUS. The models were designed to provide forecasts\nwith a lead time of up to 1.5 hours and, by using a feedback loop approach, the\nability of the models to extend the forecast time to 4.5 hours was also\ninvestigated. Model performance was compared against the Random Forest (RF) and\nLinear Regression (LR) machine learning methods, and also against a persistence\nbenchmark (BM) that used the most recent observation as the forecast.\nIndependent IMERG observations were used as a reference, and experiments were\nconducted to examine both overall statistics and case studies involving\nspecific precipitation events. Overall, the forecasts provided by the\nNowcasting-Net models are superior, with the Convolutional Nowcasting Network\nwith Residual Head (CNC-R) achieving 25%, 28%, and 46% improvement in the test\n...",
          "link": "http://arxiv.org/abs/2108.06868",
          "publishedOn": "2021-08-17T01:54:46.954Z",
          "wordCount": 746,
          "title": "Nowcasting-Nets: Deep Neural Network Structures for Precipitation Nowcasting Using IMERG. (arXiv:2108.06868v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Khay Boon Hong</a>",
          "description": "We use deep sparsely connected neural networks to measure the complexity of a\nfunction class in $L^2(\\mathbb R^d)$ by restricting connectivity and memory\nrequirement for storing the neural networks. We also introduce representation\nsystem - a countable collection of functions to guide neural networks, since\napproximation theory with representation system has been well developed in\nMathematics. We then prove the fundamental bound theorem, implying a quantity\nintrinsic to the function class itself can give information about the\napproximation ability of neural networks and representation system. We also\nprovides a method for transferring existing theories about approximation by\nrepresentation systems to that of neural networks, greatly amplifying the\npractical values of neural networks. Finally, we use neural networks to\napproximate B-spline functions, which are used to generate the B-spline curves.\nThen, we analyse the complexity of a class called $\\beta$ cartoon-like\nfunctions using rate-distortion theory and wedgelets construction.",
          "link": "http://arxiv.org/abs/2108.06467",
          "publishedOn": "2021-08-17T01:54:46.947Z",
          "wordCount": 592,
          "title": "Optimal Approximation with Sparse Neural Networks and Applications. (arXiv:2108.06467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenggang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kai Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Liting Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changliu Liu</a>",
          "description": "Microscopic epidemic models are powerful tools for government policy makers\nto predict and simulate epidemic outbreaks, which can capture the impact of\nindividual behaviors on the macroscopic phenomenon. However, existing models\nonly consider simple rule-based individual behaviors, limiting their\napplicability. This paper proposes a deep-reinforcement-learning-powered\nmicroscopic model named Microscopic Pandemic Simulator (MPS). By replacing\nrule-based agents with rational agents whose behaviors are driven to maximize\nrewards, the MPS provides a better approximation of real world dynamics. To\nefficiently simulate with massive amounts of agents in MPS, we propose Scalable\nMillion-Agent DQN (SMADQN). The MPS allows us to efficiently evaluate the\nimpact of different government strategies. This paper first calibrates the MPS\nagainst real-world data in Allegheny, US, then demonstratively evaluates two\ngovernment strategies: information disclosure and quarantine. The results\nvalidate the effectiveness of the proposed method. As a broad impact, this\npaper provides novel insights for the application of DRL in large scale\nagent-based networks such as economic and social networks.",
          "link": "http://arxiv.org/abs/2108.06589",
          "publishedOn": "2021-08-17T01:54:46.936Z",
          "wordCount": 611,
          "title": "A Microscopic Pandemic Simulator for Pandemic Prediction Using Scalable Million-Agent Reinforcement Learning. (arXiv:2108.06589v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarna_A/0/1/0/all/0/1\">Aaron Sarna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1\">Dilip Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maschinot_A/0/1/0/all/0/1\">Aaron Maschinot</a>",
          "description": "Disentangled visual representations have largely been studied with generative\nmodels such as Variational AutoEncoders (VAEs). While prior work has focused on\ngenerative methods for disentangled representation learning, these approaches\ndo not scale to large datasets due to current limitations of generative models.\nInstead, we explore regularization methods with contrastive learning, which\ncould result in disentangled representations that are powerful enough for large\nscale datasets and downstream applications. However, we find that unsupervised\ndisentanglement is difficult to achieve due to optimization and initialization\nsensitivity, with trade-offs in task performance. We evaluate disentanglement\nwith downstream tasks, analyze the benefits and disadvantages of each\nregularization used, and discuss future directions.",
          "link": "http://arxiv.org/abs/2108.06613",
          "publishedOn": "2021-08-17T01:54:46.925Z",
          "wordCount": 559,
          "title": "Unsupervised Disentanglement without Autoencoding: Pitfalls and Future Directions. (arXiv:2108.06613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1\">Sara Abdali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilescu_M/0/1/0/all/0/1\">M. Alex O. Vasilescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1\">Evangelos E. Papalexakis</a>",
          "description": "Generative neural network architectures such as GANs, may be used to generate\nsynthetic instances to compensate for the lack of real data. However, they may\nbe employed to create media that may cause social, political or economical\nupheaval. One emerging media is \"Deepfake\".Techniques that can discriminate\nbetween such media is indispensable. In this paper, we propose a modified\nmultilinear (tensor) method, a combination of linear and multilinear\nregressions for representing fake and real data. We test our approach by\nrepresenting Deepfakes with our modified multilinear (tensor) approach and\nperform SVM classification with encouraging results.",
          "link": "http://arxiv.org/abs/2108.06702",
          "publishedOn": "2021-08-17T01:54:46.919Z",
          "wordCount": 532,
          "title": "Deepfake Representation with Multilinear Regression. (arXiv:2108.06702v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Christopher Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jai Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_M/0/1/0/all/0/1\">Milind Maiti</a>",
          "description": "Dropout Regularization, serving to reduce variance, is nearly ubiquitous in\nDeep Learning models. We explore the relationship between the dropout rate and\nmodel complexity by training 2,000 neural networks configured with random\ncombinations of the dropout rate and the number of hidden units in each dense\nlayer, on each of the three data sets we selected. The generated figures, with\nbinary cross entropy loss and binary accuracy on the z-axis, question the\ncommon assumption that adding depth to a dense layer while increasing the\ndropout rate will certainly enhance performance. We also discover a complex\ncorrelation between the two hyperparameters that we proceed to quantify by\nbuilding additional machine learning and Deep Learning models which predict the\noptimal dropout rate given some hidden units in each dense layer. Linear\nregression and polynomial logistic regression require the use of arbitrary\nthresholds to select the cost data points included in the regression and to\nassign the cost data points a binary classification, respectively. These\nmachine learning models have mediocre performance because their naive nature\nprevented the modeling of complex decision boundaries. Turning to Deep Learning\nmodels, we build neural networks that predict the optimal dropout rate given\nthe number of hidden units in each dense layer, the desired cost, and the\ndesired accuracy of the model. Though, this attempt encounters a mathematical\nerror that can be attributed to the failure of the vertical line test. The\nultimate Deep Learning model is a neural network whose decision boundary\nrepresents the 2,000 previously generated data points. This final model leads\nus to devise a promising method for tuning hyperparameters to minimize\ncomputational expense yet maximize performance. The strategy can be applied to\nany model hyperparameters, with the prospect of more efficient tuning in\nindustrial models.",
          "link": "http://arxiv.org/abs/2108.06628",
          "publishedOn": "2021-08-17T01:54:46.813Z",
          "wordCount": 726,
          "title": "Investigating the Relationship Between Dropout Regularization and Model Complexity in Neural Networks. (arXiv:2108.06628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sehun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_H/0/1/0/all/0/1\">Hyunjun Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>",
          "description": "Most recent studies on detecting and localizing temporal anomalies have\nmainly employed deep neural networks to learn the normal patterns of temporal\ndata in an unsupervised manner. Unlike them, the goal of our work is to fully\nutilize instance-level (or weak) anomaly labels, which only indicate whether\nany anomalous events occurred or not in each instance of temporal data. In this\npaper, we present WETAS, a novel framework that effectively identifies\nanomalous temporal segments (i.e., consecutive time points) in an input\ninstance. WETAS learns discriminative features from the instance-level labels\nso that it infers the sequential order of normal and anomalous segments within\neach instance, which can be used as a rough segmentation mask. Based on the\ndynamic time warping (DTW) alignment between the input instance and its\nsegmentation mask, WETAS obtains the result of temporal segmentation, and\nsimultaneously, it further enhances itself by using the mask as additional\nsupervision. Our experiments show that WETAS considerably outperforms other\nbaselines in terms of the localization of temporal anomalies, and also it\nprovides more informative results than point-level detection methods.",
          "link": "http://arxiv.org/abs/2108.06816",
          "publishedOn": "2021-08-17T01:54:46.791Z",
          "wordCount": 636,
          "title": "Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping. (arXiv:2108.06816v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06460",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hong_K/0/1/0/all/0/1\">Kai Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Cailian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yancheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>",
          "description": "This work presents an unsupervised deep learning scheme that exploiting\nhigh-dimensional assisted score-based generative model for color image\nrestoration tasks. Considering that the sample number and internal dimension in\nscore-based generative model have key influence on estimating the gradients of\ndata distribution, two different high-dimensional ways are proposed: The\nchannel-copy transformation increases the sample number and the pixel-scale\ntransformation decreases feasible space dimension. Subsequently, a set of\nhigh-dimensional tensors represented by these transformations are used to train\nthe network through denoising score matching. Then, sampling is performed by\nannealing Langevin dynamics and alternative data-consistency update.\nFurthermore, to alleviate the difficulty of learning high-dimensional\nrepresentation, a progressive strategy is proposed to leverage the performance.\nThe proposed unsupervised learning and iterative restoration algo-rithm, which\ninvolves a pre-trained generative network to obtain prior, has transparent and\nclear interpretation compared to other data-driven approaches. Experimental\nresults on demosaicking and inpainting conveyed the remarkable performance and\ndiversity of our proposed method.",
          "link": "http://arxiv.org/abs/2108.06460",
          "publishedOn": "2021-08-17T01:54:46.784Z",
          "wordCount": 614,
          "title": "High-dimensional Assisted Generative Model for Color Image Restoration. (arXiv:2108.06460v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06624",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bhat_H/0/1/0/all/0/1\">Harish S. Bhat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reeves_M/0/1/0/all/0/1\">Majerle E. Reeves</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldman_Mellor_S/0/1/0/all/0/1\">Sidra Goldman-Mellor</a>",
          "description": "When faced with severely imbalanced binary classification problems, we often\ntrain models on bootstrapped data in which the number of instances of each\nclass occur in a more favorable ratio, e.g., one. We view algorithmic inequity\nthrough the lens of imbalanced classification: in order to balance the\nperformance of a classifier across groups, we can bootstrap to achieve training\nsets that are balanced with respect to both labels and group identity. For an\nexample problem with severe class imbalance---prediction of suicide death from\nadministrative patient records---we illustrate how an equity-directed bootstrap\ncan bring test set sensitivities and specificities much closer to satisfying\nthe equal odds criterion. In the context of na\\\"ive Bayes and logistic\nregression, we analyze the equity-directed bootstrap, demonstrating that it\nworks by bringing odds ratios close to one, and linking it to methods involving\nintercept adjustment, thresholding, and weighting.",
          "link": "http://arxiv.org/abs/2108.06624",
          "publishedOn": "2021-08-17T01:54:46.778Z",
          "wordCount": 576,
          "title": "Equity-Directed Bootstrapping: Examples and Analysis. (arXiv:2108.06624v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wanye_F/0/1/0/all/0/1\">Frank Wanye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleyzer_V/0/1/0/all/0/1\">Vitaliy Gleyzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_E/0/1/0/all/0/1\">Edward Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wu-chun Feng</a>",
          "description": "Community detection is a well-studied problem with applications in domains\nranging from computer networking to bioinformatics. While there are many\nalgorithms that perform community detection, the more accurate and\nstatistically robust algorithms tend to be slow and hard to parallelize. One\nway to speed up such algorithms is through data reduction. However, this\napproach has not been thoroughly studied, and the quality of results obtained\nwith this approach varies with the graph it is applied to. In this manuscript,\nwe present an approach based on topology-guided sampling for accelerating\nstochastic block partitioning - a community detection algorithm that works well\non graphs with complex and heterogeneous community structure. We also introduce\na degree-based thresholding scheme that improves the efficacy of our approach\nat the expense of speedup. Finally, we perform a series of experiments on\nsynthetically generated graphs to determine how various graph parameters affect\nthe quality of results and speedup obtained with our approach, and we validate\nour approach on real-world data. Our results show that our approach can lead to\na speedup of up to 15X over stochastic block partitioning without sampling\nwhile maintaining result quality and can even lead to improvements of over 150%\nin result quality in terms of F1 score on certain kinds of graphs.",
          "link": "http://arxiv.org/abs/2108.06651",
          "publishedOn": "2021-08-17T01:54:46.771Z",
          "wordCount": 648,
          "title": "Topology-Guided Sampling for Fast and Accurate Community Detection. (arXiv:2108.06651v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qinyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yiou Xiao</a>",
          "description": "As a special field in deep learning, Graph Neural Networks (GNNs) focus on\nextracting intrinsic network features and have drawn unprecedented popularity\nin both academia and industry. Most of the state-of-the-art GNN models offer\nexpressive, robust, scalable and inductive solutions empowering social network\nrecommender systems with rich network features that are computationally\ndifficult to leverage with graph traversal based methods.\n\nMost recent GNNs follow an encoder-decoder paradigm to encode high\ndimensional heterogeneous information from a subgraph onto one low dimensional\nembedding space. However, one single embedding space usually fails to capture\nall aspects of graph signals. In this work, we propose boosting-based meta\nlearner for GNNs, which automatically learns multiple projections and the\ncorresponding embedding spaces that captures different aspects of the graph\nsignals. As a result, similarities between sub-graphs are quantified by\nembedding proximity on multiple embedding spaces. AdaGNN performs exceptionally\nwell for applications with rich and diverse node neighborhood information.\nMoreover, AdaGNN is compatible with any inductive GNNs for both node-level and\nedge-level tasks.",
          "link": "http://arxiv.org/abs/2108.06452",
          "publishedOn": "2021-08-17T01:54:46.765Z",
          "wordCount": 604,
          "title": "AdaGNN: A multi-modal latent representation meta-learner for GNNs based on AdaBoosting. (arXiv:2108.06452v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06764",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Ruinong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>",
          "description": "In order to reduce the negative impact of the uncertainty of load and\nrenewable energies outputs on microgrid operation, an optimal scheduling model\nis proposed for isolated microgrids by using automated reinforcement\nlearning-based multi-period forecasting of renewable power generations and\nloads. Firstly, a prioritized experience replay automated reinforcement\nlearning (PER-AutoRL) is designed to simplify the deployment of deep\nreinforcement learning (DRL)-based forecasting model in a customized manner,\nthe single-step multi-period forecasting method based on PER-AutoRL is proposed\nfor the first time to address the error accumulation issue suffered by existing\nmulti-step forecasting methods, then the prediction values obtained by the\nproposed forecasting method are revised via the error distribution to improve\nthe prediction accuracy; secondly, a scheduling model considering demand\nresponse is constructed to minimize the total microgrid operating costs, where\nthe revised forecasting values are used as the dispatch basis, and a spinning\nreserve chance constraint is set according to the error distribution; finally,\nby transforming the original scheduling model into a readily solvable mixed\ninteger linear programming via the sequence operation theory (SOT), the\ntransformed model is solved by using CPLEX solver. The simulation results show\nthat compared with the traditional scheduling model without forecasting, this\napproach manages to significantly reduce the system operating costs by\nimproving the prediction accuracy.",
          "link": "http://arxiv.org/abs/2108.06764",
          "publishedOn": "2021-08-17T01:54:46.748Z",
          "wordCount": 670,
          "title": "Optimal Scheduling of Isolated Microgrids Using Automated Reinforcement Learning-based Multi-period Forecasting. (arXiv:2108.06764v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06558",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Papapicco_D/0/1/0/all/0/1\">Davide Papapicco</a>, <a href=\"http://arxiv.org/find/math/1/au:+Demo_N/0/1/0/all/0/1\">Nicola Demo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Girfoglio_M/0/1/0/all/0/1\">Michele Girfoglio</a>, <a href=\"http://arxiv.org/find/math/1/au:+Stabile_G/0/1/0/all/0/1\">Giovanni Stabile</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rozza_G/0/1/0/all/0/1\">Gianluigi Rozza</a>",
          "description": "Models with dominant advection always posed a difficult challenge for\nprojection-based reduced order modelling. Many methodologies that have recently\nbeen proposed are based on the pre-processing of the full-order solutions to\naccelerate the Kolmogorov N-width decay thereby obtaining smaller linear\nsubspaces with improved accuracy. These methods however must rely on the\nknowledge of the characteristic speeds in phase space of the solution, limiting\ntheir range of applicability to problems with explicit functional form for the\nadvection field. In this work we approach the problem of automatically\ndetecting the correct pre-processing transformation in a statistical learning\nframework by implementing a deep-learning architecture. The purely data-driven\nmethod allowed us to generalise the existing approaches of linear subspace\nmanipulation to non-linear hyperbolic problems with unknown advection fields.\nThe proposed algorithm has been validated against simple test cases to\nbenchmark its performances and later successfully applied to a multiphase\nsimulation.",
          "link": "http://arxiv.org/abs/2108.06558",
          "publishedOn": "2021-08-17T01:54:46.742Z",
          "wordCount": 598,
          "title": "The Neural Network shifted-Proper Orthogonal Decomposition: a Machine Learning Approach for Non-linear Reduction of Hyperbolic Equations. (arXiv:2108.06558v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sasso_R/0/1/0/all/0/1\">Remo Sasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatelli_M/0/1/0/all/0/1\">Matthia Sabatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1\">Marco A. Wiering</a>",
          "description": "Reinforcement learning (RL) is well known for requiring large amounts of data\nin order for RL agents to learn to perform complex tasks. Recent progress in\nmodel-based RL allows agents to be much more data-efficient, as it enables them\nto learn behaviors of visual environments in imagination by leveraging an\ninternal World Model of the environment. Improved sample efficiency can also be\nachieved by reusing knowledge from previously learned tasks, but transfer\nlearning is still a challenging topic in RL. Parameter-based transfer learning\nis generally done using an all-or-nothing approach, where the network's\nparameters are either fully transferred or randomly initialized. In this work\nwe present a simple alternative approach: fractional transfer learning. The\nidea is to transfer fractions of knowledge, opposed to discarding potentially\nuseful knowledge as is commonly done with random initialization. Using the\nWorld Model-based Dreamer algorithm, we identify which type of components this\napproach is applicable to, and perform experiments in a new multi-source\ntransfer learning setting. The results show that fractional transfer learning\noften leads to substantially improved performance and faster learning compared\nto learning from scratch and random initialization.",
          "link": "http://arxiv.org/abs/2108.06526",
          "publishedOn": "2021-08-17T01:54:46.735Z",
          "wordCount": 628,
          "title": "Fractional Transfer Learning for Deep Model-Based Reinforcement Learning. (arXiv:2108.06526v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06394",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Shneider_C/0/1/0/all/0/1\">Carl Shneider</a> (1), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hu_A/0/1/0/all/0/1\">Andong Hu</a> (1), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tiwari_A/0/1/0/all/0/1\">Ajay K. Tiwari</a> (1), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bobra_M/0/1/0/all/0/1\">Monica G. Bobra</a> (2), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Battams_K/0/1/0/all/0/1\">Karl Battams</a> (5), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Teunissen_J/0/1/0/all/0/1\">Jannis Teunissen</a> (1), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Camporeale_E/0/1/0/all/0/1\">Enrico Camporeale</a> (3 and 4) ((1) Multiscale Dynamics Group, Center for Mathematics and Computer Science (CWI), Amsterdam, The Netherlands, (2) W.W. Hansen Experimental Physics Laboratory, Stanford University, Stanford, CA, USA, (3) CIRES, University of Colorado, Boulder, CO, USA, (4) NOAA, Space Weather Prediction Center, Boulder, CO, USA, (5) US Naval Research Laboratory, Washington DC, USA)",
          "description": "We present a Python tool to generate a standard dataset from solar images\nthat allows for user-defined selection criteria and a range of pre-processing\nsteps. Our Python tool works with all image products from both the Solar and\nHeliospheric Observatory (SoHO) and Solar Dynamics Observatory (SDO) missions.\nWe discuss a dataset produced from the SoHO mission's multi-spectral images\nwhich is free of missing or corrupt data as well as planetary transits in\ncoronagraph images, and is temporally synced making it ready for input to a\nmachine learning system. Machine-learning-ready images are a valuable resource\nfor the community because they can be used, for example, for forecasting space\nweather parameters. We illustrate the use of this data with a 3-5 day-ahead\nforecast of the north-south component of the interplanetary magnetic field\n(IMF) observed at Lagrange point one (L1). For this use case, we apply a deep\nconvolutional neural network (CNN) to a subset of the full SoHO dataset and\ncompare with baseline results from a Gaussian Naive Bayes classifier.",
          "link": "http://arxiv.org/abs/2108.06394",
          "publishedOn": "2021-08-17T01:54:46.729Z",
          "wordCount": 698,
          "title": "A Machine-Learning-Ready Dataset Prepared from the Solar and Heliospheric Observatory Mission. (arXiv:2108.06394v1 [astro-ph.SR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Caleb Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_E/0/1/0/all/0/1\">Ethan X. Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Bregman proximal point algorithm (BPPA), as one of the centerpieces in the\noptimization toolbox, has been witnessing emerging applications. With simple\nand easy to implement update rule, the algorithm bears several compelling\nintuitions for empirical successes, yet rigorous justifications are still\nlargely unexplored. We study the computational properties of BPPA through\nclassification tasks with separable data, and demonstrate provable algorithmic\nregularization effects associated with BPPA. We show that BPPA attains\nnon-trivial margin, which closely depends on the condition number of the\ndistance generating function inducing the Bregman divergence. We further\ndemonstrate that the dependence on the condition number is tight for a class of\nproblems, thus showing the importance of divergence in affecting the quality of\nthe obtained solutions. In addition, we extend our findings to mirror descent\n(MD), for which we establish similar connections between the margin and Bregman\ndivergence. We demonstrate through a concrete example, and show BPPA/MD\nconverges in direction to the maximal margin solution with respect to the\nMahalanobis distance. Our theoretical findings are among the first to\ndemonstrate the benign learning properties BPPA/MD, and also provide\ncorroborations for a careful choice of divergence in the algorithmic design.",
          "link": "http://arxiv.org/abs/2108.06808",
          "publishedOn": "2021-08-17T01:54:46.722Z",
          "wordCount": 633,
          "title": "Implicit Regularization of Bregman Proximal Point Algorithm and Mirror Descent on Separable Data. (arXiv:2108.06808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_N/0/1/0/all/0/1\">Nanda K. Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parhi_K/0/1/0/all/0/1\">Keshab K. Parhi</a>",
          "description": "The time required for training the neural networks increases with size,\ncomplexity, and depth. Training model parameters by backpropagation inherently\ncreates feedback loops. These loops hinder efficient pipelining and scheduling\nof the tasks within the layer and between consecutive layers. Prior approaches,\nsuch as PipeDream, have exploited the use of delayed gradient to achieve\ninter-layer pipelining. However, these approaches treat the entire\nbackpropagation as a single task; this leads to an increase in computation time\nand processor underutilization. This paper presents novel optimization\napproaches where the gradient computations with respect to the weights and the\nactivation functions are considered independently; therefore, these can be\ncomputed in parallel. This is referred to as intra-layer optimization.\nAdditionally, the gradient computation with respect to the activation function\nis further divided into two parts and distributed to two consecutive layers.\nThis leads to balanced scheduling where the computation time of each layer is\nthe same. This is referred to as inter-layer optimization. The proposed system,\nreferred to as LayerPipe, reduces the number of clock cycles required for\ntraining while maximizing processor utilization with minimal inter-processor\ncommunication overhead. LayerPipe achieves an average speedup of 25% and\nupwards of 80% with 7 to 9 processors with less communication overhead when\ncompared to PipeDream.",
          "link": "http://arxiv.org/abs/2108.06629",
          "publishedOn": "2021-08-17T01:54:46.715Z",
          "wordCount": 681,
          "title": "LayerPipe: Accelerating Deep Neural Network Training by Intra-Layer and Inter-Layer Gradient Pipelining and Multiprocessor Scheduling. (arXiv:2108.06629v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Suyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1\">Luis Nunes Vicente</a>",
          "description": "In machine learning (ML) applications, unfair predictions may discriminate\nagainst a minority group. Most existing approaches for fair machine learning\n(FML) treat fairness as a constraint or a penalization term in the optimization\nof a ML model, which does not lead to the discovery of the complete landscape\nof the trade-offs among learning accuracy and fairness metrics, and does not\nintegrate fairness in a meaningful way.\n\nRecently, we have introduced a new paradigm for FML based on Stochastic\nMulti-Objective Optimization (SMOO), where accuracy and fairness metrics stand\nas conflicting objectives to be optimized simultaneously. The entire trade-offs\nrange is defined as the Pareto front of the SMOO problem, which can then be\nefficiently computed using stochastic-gradient type algorithms. SMOO also\nallows defining and computing new meaningful predictors for FML, a novel one\nbeing the Sharpe predictor that we introduce and explore in this paper, and\nwhich gives the highest ratio of accuracy-to-unfairness. Inspired from SMOO in\nfinance, the Sharpe predictor for FML provides the highest prediction return\n(accuracy) per unit of prediction risk (unfairness).",
          "link": "http://arxiv.org/abs/2108.06415",
          "publishedOn": "2021-08-17T01:54:46.698Z",
          "wordCount": 601,
          "title": "The Sharpe predictor for fairness in machine learning. (arXiv:2108.06415v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_S/0/1/0/all/0/1\">Sumit Kumar Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1\">Jeetu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Aditya Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishabh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunturi_V/0/1/0/all/0/1\">Venkata M. V. Gunturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C. Krishnan</a>",
          "description": "Interpolation in Spatio-temporal data has applications in various domains\nsuch as climate, transportation, and mining. Spatio-Temporal interpolation is\nhighly challenging due to the complex spatial and temporal relationships.\nHowever, traditional techniques such as Kriging suffer from high running time\nand poor performance on data that exhibit high variance across space and time\ndimensions. To this end, we propose a novel deep neural network called as Deep\nGeospatial Interpolation Network(DGIN), which incorporates both spatial and\ntemporal relationships and has significantly lower training time. DGIN consists\nof three major components: Spatial Encoder to capture the spatial dependencies,\nSequential module to incorporate the temporal dynamics, and an Attention block\nto learn the importance of the temporal neighborhood around the gap. We\nevaluate DGIN on the MODIS reflectance dataset from two different regions. Our\nexperimental results indicate that DGIN has two advantages: (a) it outperforms\nalternative approaches (has lower MSE with p-value < 0.01) and, (b) it has\nsignificantly low execution time than Kriging.",
          "link": "http://arxiv.org/abs/2108.06670",
          "publishedOn": "2021-08-17T01:54:46.692Z",
          "wordCount": 593,
          "title": "Deep Geospatial Interpolation Networks. (arXiv:2108.06670v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1\">Reza Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhier_L/0/1/0/all/0/1\">Lucas Rouhier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>",
          "description": "Labeling vertebral discs from MRI scans is important for the proper diagnosis\nof spinal related diseases, including multiple sclerosis, amyotrophic lateral\nsclerosis, degenerative cervical myelopathy and cancer. Automatic labeling of\nthe vertebral discs in MRI data is a difficult task because of the similarity\nbetween discs and bone area, the variability in the geometry of the spine and\nsurrounding tissues across individuals, and the variability across scans\n(manufacturers, pulse sequence, image contrast, resolution and artefacts). In\nprevious studies, vertebral disc labeling is often done after a disc detection\nstep and mostly fails when the localization algorithm misses discs or has false\npositive detection. In this work, we aim to mitigate this problem by\nreformulating the semantic vertebral disc labeling using the pose estimation\ntechnique. To do so, we propose a stacked hourglass network with multi-level\nattention mechanism to jointly learn intervertebral disc position and their\nskeleton structure. The proposed deep learning model takes into account the\nstrength of semantic segmentation and pose estimation technique to handle the\nmissing area and false positive detection. To further improve the performance\nof the proposed method, we propose a skeleton-based search space to reduce\nfalse positive detection. The proposed method evaluated on spine generic public\nmulti-center dataset and demonstrated better performance comparing to previous\nwork, on both T1w and T2w contrasts. The method is implemented in ivadomed\n(https://ivadomed.org).",
          "link": "http://arxiv.org/abs/2108.06554",
          "publishedOn": "2021-08-17T01:54:46.686Z",
          "wordCount": 685,
          "title": "Stacked Hourglass Network with a Multi-level Attention Mechanism: Where to Look for Intervertebral Disc Labeling. (arXiv:2108.06554v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>",
          "description": "Person re-identification (ReID) aims to re-identify a person from\nnon-overlapping camera views. Since person ReID data contains sensitive\npersonal information, researchers have adopted federated learning, an emerging\ndistributed training method, to mitigate the privacy leakage risks. However,\nexisting studies rely on data labels that are laborious and time-consuming to\nobtain. We present FedUReID, a federated unsupervised person ReID system to\nlearn person ReID models without any labels while preserving privacy. FedUReID\nenables in-situ model training on edges with unlabeled data. A cloud server\naggregates models from edges instead of centralizing raw data to preserve data\nprivacy. Moreover, to tackle the problem that edges vary in data volumes and\ndistributions, we personalize training in edges with joint optimization of\ncloud and edge. Specifically, we propose personalized epoch to reassign\ncomputation throughout training, personalized clustering to iteratively predict\nsuitable labels for unlabeled data, and personalized update to adapt the server\naggregated model to each edge. Extensive experiments on eight person ReID\ndatasets demonstrate that FedUReID not only achieves higher accuracy but also\nreduces computation cost by 29%. Our FedUReID system with the joint\noptimization will shed light on implementing federated learning to more\nmultimedia tasks without data labels.",
          "link": "http://arxiv.org/abs/2108.06493",
          "publishedOn": "2021-08-17T01:54:46.653Z",
          "wordCount": 653,
          "title": "Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised Person Re-identification. (arXiv:2108.06493v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Sheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Ju Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jiang Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Deyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weihua Zhuang</a>",
          "description": "Federated meta-learning (FML) has emerged as a promising paradigm to cope\nwith the data limitation and heterogeneity challenges in today's edge learning\narena. However, its performance is often limited by slow convergence and\ncorresponding low communication efficiency. Besides, since the wireless\nbandwidth and IoT devices' energy capacity are usually insufficient, it is\ncrucial to control the resource allocation and energy consumption when\ndeploying FML in realistic wireless networks. To overcome these challenges, in\nthis paper, we first rigorously analyze each device's contribution to the\nglobal loss reduction in each round and develop an FML algorithm (called NUFM)\nwith a non-uniform device selection scheme to accelerate the convergence. After\nthat, we formulate a resource allocation problem integrating NUFM in\nmulti-access wireless systems to jointly improve the convergence rate and\nminimize the wall-clock time along with energy cost. By deconstructing the\noriginal problem step by step, we devise a joint device selection and resource\nallocation strategy (called URAL) to solve the problem and provide theoretical\nguarantees. Further, we show that the computational complexity of NUFM can be\nreduced from $O(d^2)$ to $O(d)$ (with $d$ being the model dimension) via\ncombining two first-order approximation techniques. Extensive simulation\nresults demonstrate the effectiveness and superiority of the proposed methods\nby comparing with the existing baselines.",
          "link": "http://arxiv.org/abs/2108.06453",
          "publishedOn": "2021-08-17T01:54:46.631Z",
          "wordCount": 642,
          "title": "Efficient Federated Meta-Learning over Multi-Access Wireless Networks. (arXiv:2108.06453v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Runzhe Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Lin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>",
          "description": "How to explore efficiently is a central problem in multi-armed bandits. In\nthis paper, we introduce the metadata-based multi-task bandit problem, where\nthe agent needs to solve a large number of related multi-armed bandit tasks and\ncan leverage some task-specific features (i.e., metadata) to share knowledge\nacross tasks. As a general framework, we propose to capture task relations\nthrough the lens of Bayesian hierarchical models, upon which a Thompson\nsampling algorithm is designed to efficiently learn task relations, share\ninformation, and minimize the cumulative regrets. Two concrete examples for\nGaussian bandits and Bernoulli bandits are carefully analyzed. The Bayes regret\nfor Gaussian bandits clearly demonstrates the benefits of information sharing\nwith our algorithm. The proposed method is further supported by extensive\nexperiments.",
          "link": "http://arxiv.org/abs/2108.06422",
          "publishedOn": "2021-08-17T01:54:46.603Z",
          "wordCount": 552,
          "title": "Metadata-based Multi-Task Bandits with Bayesian Hierarchical Models. (arXiv:2108.06422v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutin_M/0/1/0/all/0/1\">Mireille Boutin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coupkova_E/0/1/0/all/0/1\">Evzenie Coupkova</a>",
          "description": "The generalization error of a classifier is related to the complexity of the\nset of functions among which the classifier is chosen. Roughly speaking, the\nmore complex the family, the greater the potential disparity between the\ntraining error and the population error of the classifier. This principle is\nembodied in layman's terms by Occam's razor principle, which suggests favoring\nlow-complexity hypotheses over complex ones. We study a family of\nlow-complexity classifiers consisting of thresholding the one-dimensional\nfeature obtained by projecting the data on a random line after embedding it\ninto a higher dimensional space parametrized by monomials of order up to k.\nMore specifically, the extended data is projected n-times and the best\nclassifier among those n (based on its performance on training data) is chosen.\nWe obtain a bound on the generalization error of these low-complexity\nclassifiers. The bound is less than that of any classifier with a non-trivial\nVC dimension, and thus less than that of a linear classifier. We also show\nthat, given full knowledge of the class conditional densities, the error of the\nclassifiers would converge to the optimal (Bayes) error as k and n go to\ninfinity; if only a training dataset is given, we show that the classifiers\nwill perfectly classify all the training points as k and n go to infinity.",
          "link": "http://arxiv.org/abs/2108.06339",
          "publishedOn": "2021-08-17T01:54:46.593Z",
          "wordCount": 657,
          "title": "Asymptotic optimality and minimal complexity of classification by random projection. (arXiv:2108.06339v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_Cardenas_C/0/1/0/all/0/1\">Carlos H. Mendoza-Cardenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockmeier_A/0/1/0/all/0/1\">Austin J. Brockmeier</a>",
          "description": "Seizure detection algorithms must discriminate abnormal neuronal activity\nassociated with a seizure from normal neural activity in a variety of\nconditions. Our approach is to seek spatiotemporal waveforms with distinct\nmorphology in electrocorticographic (ECoG) recordings of epileptic patients\nthat are indicative of a subsequent seizure (preictal) versus non-seizure\nsegments (interictal). To find these waveforms we apply a shift-invariant\nk-means algorithm to segments of spatially filtered signals to learn codebooks\nof prototypical waveforms. The frequency of the cluster labels from the\ncodebooks is then used to train a binary classifier that predicts the class\n(preictal or interictal) of a test ECoG segment. We use the Matthews\ncorrelation coefficient to evaluate the performance of the classifier and the\nquality of the codebooks. We found that our method finds recurrent\nnon-sinusoidal waveforms that could be used to build interpretable features for\nseizure prediction and that are also physiologically meaningful.",
          "link": "http://arxiv.org/abs/2108.03177",
          "publishedOn": "2021-08-16T01:57:42.793Z",
          "wordCount": 621,
          "title": "Shift-invariant waveform learning on epileptic ECoG. (arXiv:2108.03177v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tailor_S/0/1/0/all/0/1\">Shyam A. Tailor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_R/0/1/0/all/0/1\">Ren&#xe9; de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azevedo_T/0/1/0/all/0/1\">Tiago Azevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1\">Matthew Mattina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_P/0/1/0/all/0/1\">Partha Maji</a>",
          "description": "In recent years graph neural network (GNN)-based approaches have become a\npopular strategy for processing point cloud data, regularly achieving\nstate-of-the-art performance on a variety of tasks. To date, the research\ncommunity has primarily focused on improving model expressiveness, with\nsecondary thought given to how to design models that can run efficiently on\nresource constrained mobile devices including smartphones or mixed reality\nheadsets. In this work we make a step towards improving the efficiency of these\nmodels by making the observation that these GNN models are heavily limited by\nthe representational power of their first, feature extracting, layer. We find\nthat it is possible to radically simplify these models so long as the feature\nextraction layer is retained with minimal degradation to model performance;\nfurther, we discover that it is possible to improve performance overall on\nModelNet40 and S3DIS by improving the design of the feature extractor. Our\napproach reduces memory consumption by 20$\\times$ and latency by up to\n9.9$\\times$ for graph layers in models such as DGCNN; overall, we achieve\nspeed-ups of up to 4.5$\\times$ and peak memory reductions of 72.5%.",
          "link": "http://arxiv.org/abs/2108.06317",
          "publishedOn": "2021-08-16T00:47:33.903Z",
          "wordCount": 645,
          "title": "Towards Efficient Point Cloud Graph Neural Networks Through Architectural Simplification. (arXiv:2108.06317v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berghoff_C/0/1/0/all/0/1\">Christian Berghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielik_P/0/1/0/all/0/1\">Pavol Bielik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neu_M/0/1/0/all/0/1\">Matthias Neu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsankov_P/0/1/0/all/0/1\">Petar Tsankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twickel_A/0/1/0/all/0/1\">Arndt von Twickel</a>",
          "description": "In the last years, AI systems, in particular neural networks, have seen a\ntremendous increase in performance, and they are now used in a broad range of\napplications. Unlike classical symbolic AI systems, neural networks are trained\nusing large data sets and their inner structure containing possibly billions of\nparameters does not lend itself to human interpretation. As a consequence, it\nis so far not feasible to provide broad guarantees for the correct behaviour of\nneural networks during operation if they process input data that significantly\ndiffer from those seen during training. However, many applications of AI\nsystems are security- or safety-critical, and hence require obtaining\nstatements on the robustness of the systems when facing unexpected events,\nwhether they occur naturally or are induced by an attacker in a targeted way.\nAs a step towards developing robust AI systems for such applications, this\npaper presents how the robustness of AI systems can be practically examined and\nwhich methods and metrics can be used to do so. The robustness testing\nmethodology is described and analysed for the example use case of traffic sign\nrecognition in autonomous driving.",
          "link": "http://arxiv.org/abs/2108.06159",
          "publishedOn": "2021-08-16T00:47:33.897Z",
          "wordCount": 686,
          "title": "Robustness testing of AI systems: A case study for traffic sign recognition. (arXiv:2108.06159v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jasmine Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_V/0/1/0/all/0/1\">Viranga Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magana_A/0/1/0/all/0/1\">Alejandra J. Magana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newell_B/0/1/0/all/0/1\">Brittany Newell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Kocsis_J/0/1/0/all/0/1\">Jin Wei-Kocsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seah_Y/0/1/0/all/0/1\">Ying Ying Seah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Greg J. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Charles Xie</a>",
          "description": "Computer-aided design (CAD) programs are essential to engineering as they\nallow for better designs through low-cost iterations. While CAD programs are\ntypically taught to undergraduate students as a job skill, such software can\nalso help students learn engineering concepts. A current limitation of CAD\nprograms (even those that are specifically designed for educational purposes)\nis that they are not capable of providing automated real-time help to students.\nTo encourage CAD programs to build in assistance to students, we used data\ngenerated from students using a free, open source CAD software called Aladdin\nto demonstrate how student data combined with machine learning techniques can\npredict how well a particular student will perform in a design task. We\nchallenged students to design a house that consumed zero net energy as part of\nan introductory engineering technology undergraduate course. Using data from\n128 students, along with the scikit-learn Python machine learning library, we\ntested our models using both total counts of design actions and sequences of\ndesign actions as inputs. We found that our models using early design sequence\nactions are particularly valuable for prediction. Our logistic regression model\nachieved a >60% chance of predicting if a student would succeed in designing a\nzero net energy house. Our results suggest that it would be feasible for\nAladdin to provide useful feedback to students when they are approximately\nhalfway through their design. Further improvements to these models could lead\nto earlier predictions and thus provide students feedback sooner to enhance\ntheir learning.",
          "link": "http://arxiv.org/abs/2108.05955",
          "publishedOn": "2021-08-16T00:47:33.487Z",
          "wordCount": 703,
          "title": "Using Machine Learning to Predict Engineering Technology Students' Success with Computer Aided Design. (arXiv:2108.05955v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06338",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_L/0/1/0/all/0/1\">Lei Deng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_Y/0/1/0/all/0/1\">Yibiao Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_X/0/1/0/all/0/1\">Xuejun Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>",
          "description": "Accumulated clinical studies show that microbes living in humans interact\nclosely with human hosts, and get involved in modulating drug efficacy and drug\ntoxicity. Microbes have become novel targets for the development of\nantibacterial agents. Therefore, screening of microbe-drug associations can\nbenefit greatly drug research and development. With the increase of microbial\ngenomic and pharmacological datasets, we are greatly motivated to develop an\neffective computational method to identify new microbe-drug associations. In\nthis paper, we proposed a novel method, Graph2MDA, to predict microbe-drug\nassociations by using variational graph autoencoder (VGAE). We constructed\nmulti-modal attributed graphs based on multiple features of microbes and drugs,\nsuch as molecular structures, microbe genetic sequences, and function\nannotations. Taking as input the multi-modal attribute graphs, VGAE was trained\nto learn the informative and interpretable latent representations of each node\nand the whole graph, and then a deep neural network classifier was used to\npredict microbe-drug associations. The hyperparameter analysis and model\nablation studies showed the sensitivity and robustness of our model. We\nevaluated our method on three independent datasets and the experimental results\nshowed that our proposed method outperformed six existing state-of-the-art\nmethods. We also explored the meaningness of the learned latent representations\nof drugs and found that the drugs show obvious clustering patterns that are\nsignificantly consistent with drug ATC classification. Moreover, we conducted\ncase studies on two microbes and two drugs and found 75\\%-95\\% predicted\nassociations have been reported in PubMed literature. Our extensive performance\nevaluations validated the effectiveness of our proposed method.\\",
          "link": "http://arxiv.org/abs/2108.06338",
          "publishedOn": "2021-08-16T00:47:33.474Z",
          "wordCount": 690,
          "title": "Graph2MDA: a multi-modal variational graph embedding model for predicting microbe-drug associations. (arXiv:2108.06338v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caled_D/0/1/0/all/0/1\">Danielle Caled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_P/0/1/0/all/0/1\">Paula Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">M&#xe1;rio J. Silva</a>",
          "description": "This paper presents and characterizes MIND, a new Portuguese corpus comprised\nof different types of articles collected from online mainstream and alternative\nmedia sources, over a 10-month period. The articles in the corpus are organized\ninto five collections: facts, opinions, entertainment, satires, and conspiracy\ntheories. Throughout this paper, we explain how the data collection process was\nconducted, and present a set of linguistic metrics that allow us to perform a\npreliminary characterization of the texts included in the corpus. Also, we\ndeliver an analysis of the most frequent topics in the corpus, and discuss the\nmain differences and similarities among the collections considered. Finally, we\nenumerate some tasks and applications that could benefit from this corpus, in\nparticular the ones (in)directly related to misinformation detection. Overall,\nour contribution of a corpus and initial analysis are designed to support\nfuture exploratory news studies, and provide a better insight into\nmisinformation.",
          "link": "http://arxiv.org/abs/2108.06249",
          "publishedOn": "2021-08-16T00:47:33.469Z",
          "wordCount": 577,
          "title": "MIND - Mainstream and Independent News Documents Corpus. (arXiv:2108.06249v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.15658",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Behboodi_A/0/1/0/all/0/1\">Arash Behboodi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schnoor_E/0/1/0/all/0/1\">Ekkehard Schnoor</a>",
          "description": "Various iterative reconstruction algorithms for inverse problems can be\nunfolded as neural networks. Empirically, this approach has often led to\nimproved results, but theoretical guarantees are still scarce. While some\nprogress on generalization properties of neural networks have been made, great\nchallenges remain. In this chapter, we discuss and combine these topics to\npresent a generalization error analysis for a class of neural networks suitable\nfor sparse reconstruction from few linear measurements. The hypothesis class\nconsidered is inspired by the classical iterative soft-thresholding algorithm\n(ISTA). The neural networks in this class are obtained by unfolding iterations\nof ISTA and learning some of the weights. Based on training samples, we aim at\nlearning the optimal network parameters via empirical risk minimization and\nthereby the optimal network that reconstructs signals from their compressive\nlinear measurements. In particular, we may learn a sparsity basis that is\nshared by all of the iterations/layers and thereby obtain a new approach for\ndictionary learning. For this class of networks, we present a generalization\nbound, which is based on bounding the Rademacher complexity of hypothesis\nclasses consisting of such deep networks via Dudley's integral. Remarkably,\nunder realistic conditions, the generalization error scales only\nlogarithmically in the number of layers, and at most linear in number of\nmeasurements.",
          "link": "http://arxiv.org/abs/2010.15658",
          "publishedOn": "2021-08-16T00:47:33.435Z",
          "wordCount": 698,
          "title": "Compressive Sensing and Neural Networks from a Statistical Learning Perspective. (arXiv:2010.15658v4 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amir_G/0/1/0/all/0/1\">Guy Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schapira_M/0/1/0/all/0/1\">Michael Schapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Guy Katz</a>",
          "description": "Deep neural networks (DNNs) have gained significant popularity in recent\nyears, becoming the state of the art in a variety of domains. In particular,\ndeep reinforcement learning (DRL) has recently been employed to train DNNs that\nrealize control policies for various types of real-world systems. In this work,\nwe present the whiRL 2.0 tool, which implements a new approach for verifying\ncomplex properties of interest for DRL systems. To demonstrate the benefits of\nwhiRL 2.0, we apply it to case studies from the communication networks domain\nthat have recently been used to motivate formal verification of DRL systems,\nand which exhibit characteristics that are conducive for scalable verification.\nWe propose techniques for performing k-induction and semi-automated invariant\ninference on such systems, and leverage these techniques for proving safety and\nliveness properties that were previously impossible to verify due to the\nscalability barriers of prior approaches. Furthermore, we show how our proposed\ntechniques provide insights into the inner workings and the generalizability of\nDRL systems. whiRL 2.0 is publicly available online.",
          "link": "http://arxiv.org/abs/2105.11931",
          "publishedOn": "2021-08-16T00:47:33.423Z",
          "wordCount": 646,
          "title": "Towards Scalable Verification of Deep Reinforcement Learning. (arXiv:2105.11931v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.15050",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_J/0/1/0/all/0/1\">Junhua Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Traverso_A/0/1/0/all/0/1\">Alberto Traverso</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhovannik_I/0/1/0/all/0/1\">Ivan Zhovannik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dekker_A/0/1/0/all/0/1\">Andre Dekker</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wee_L/0/1/0/all/0/1\">Leonard Wee</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bermejo_I/0/1/0/all/0/1\">Inigo Bermejo</a>",
          "description": "Radiomics is an active area of research in medical image analysis, the low\nreproducibility of radiomics has limited its applicability to clinical\npractice. This issue is especially prominent when radiomic features are\ncalculated from noisy images, such as low dose computed tomography (CT) scans.\nIn this article, we investigate the possibility of improving the\nreproducibility of radiomic features calculated on noisy CTs by using\ngenerative models for denoising.One traditional denoising method - non-local\nmeans - and two generative models - encoder-decoder networks (EDN) and\nconditional generative adversarial networks (CGANs) - were selected as the test\nmodels. We added noise to the sinograms of full dose CTs to mimic low dose CTs\nwith two different levels of noise: low-noise CT and high-noise CT. Models were\ntrained on high-noise CTs and used to denoise low-noise CTs without\nre-training. We also test the performance of our model in real data, using\ndataset of same-day repeat low dose CTs to assess the reproducibility of\nradiomic features in denoised images. The EDN and the CGAN improved the\nconcordance correlation coefficients (CCC) of radiomic features for low-noise\nimages from 0.87 to 0.92 and for high-noise images from 0.68 to 0.92\nrespectively. Moreover, the EDN and the CGAN improved the test-retest\nreliability of radiomic features (mean CCC increased from 0.89 to 0.94) based\non real low dose CTs. The results show that denoising using EDN and CGANs can\nimprove the reproducibility of radiomic features calculated on noisy CTs.\nMoreover, images with different noise levels can be denoised to improve the\nreproducibility using these models without re-training, as long as the noise\nintensity is equal or lower than that in high-noise CTs. To the authors'\nknowledge, this is the first effort to improve the reproducibility of radiomic\nfeatures calculated on low dose CT scans.",
          "link": "http://arxiv.org/abs/2104.15050",
          "publishedOn": "2021-08-16T00:47:33.418Z",
          "wordCount": 779,
          "title": "Generative Models Improve Radiomics Reproducibility in Low Dose CTs: A Simulation Study. (arXiv:2104.15050v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hyeon_Woo_N/0/1/0/all/0/1\">Nam Hyeon-Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Bin_M/0/1/0/all/0/1\">Moon Ye-Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>",
          "description": "To overcome the burdens on frequent model uploads and downloads during\nfederated learning (FL), we propose a communication-efficient\nre-parameterization, FedPara. Our method re-parameterizes the model's layers\nusing low-rank matrices or tensors followed by the Hadamard product. Different\nfrom the conventional low-rank parameterization, our method is not limited to\nlow-rank constraints. Thereby, our FedPara has a larger capacity than the\nlow-rank one, even with the same number of parameters. It can achieve\ncomparable performance to the original models while requiring 2.8 to 10.1 times\nlower communication costs than the original models, which is not achievable by\nthe traditional low-rank parameterization. Moreover, the efficiency can be\nfurther improved by combining our method and other efficient FL techniques\nbecause our method is compatible with others. We also extend our method to a\npersonalized FL application, pFedPara, which separates parameters into global\nand local ones. We show that pFedPara outperforms competing personalized FL\nmethods with more than three times fewer parameters.",
          "link": "http://arxiv.org/abs/2108.06098",
          "publishedOn": "2021-08-16T00:47:33.384Z",
          "wordCount": 594,
          "title": "FedPara: Low-rank Hadamard Product Parameterization for Efficient Federated Learning. (arXiv:2108.06098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Seehwan Yoo</a>",
          "description": "This paper proposes a novel split learning framework with multiple\nend-systems in order to realize privacypreserving deep neural network\ncomputation. In conventional split learning frameworks, deep neural network\ncomputation is separated into multiple computing systems for hiding entire\nnetwork architectures. In our proposed framework, multiple computing\nend-systems are sharing one centralized server in split learning computation,\nwhere the multiple end-systems are with input and first hidden layers and the\ncentralized server is with the other hidden layers and output layer. This\nframework, which is called as spatio-temporal split learning, is spatially\nseparated for gathering data from multiple end-systems and also temporally\nseparated due to the nature of split learning. Our performance evaluation\nverifies that our proposed framework shows nearoptimal accuracy while\npreserving data privacy.",
          "link": "http://arxiv.org/abs/2108.06309",
          "publishedOn": "2021-08-16T00:47:33.370Z",
          "wordCount": 557,
          "title": "Spatio-Temporal Split Learning. (arXiv:2108.06309v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.13472",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vowels_M/0/1/0/all/0/1\">Matthew James Vowels</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Camgoz_N/0/1/0/all/0/1\">Necati Cihan Camgoz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "Undertaking causal inference with observational data is incredibly useful\nacross a wide range of tasks including the development of medical treatments,\nadvertisements and marketing, and policy making. There are two significant\nchallenges associated with undertaking causal inference using observational\ndata: treatment assignment heterogeneity (i.e., differences between the treated\nand untreated groups), and an absence of counterfactual data (i.e., not knowing\nwhat would have happened if an individual who did get treatment, were instead\nto have not been treated). We address these two challenges by combining\nstructured inference and targeted learning. In terms of structure, we factorize\nthe joint distribution into risk, confounding, instrumental, and miscellaneous\nfactors, and in terms of targeted learning, we apply a regularizer derived from\nthe influence curve in order to reduce residual bias. An ablation study is\nundertaken, and an evaluation on benchmark datasets demonstrates that TVAE has\ncompetitive and state of the art performance.",
          "link": "http://arxiv.org/abs/2009.13472",
          "publishedOn": "2021-08-16T00:47:33.364Z",
          "wordCount": 615,
          "title": "Targeted VAE: Variational and Targeted Learning for Causal Inference. (arXiv:2009.13472v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Termritthikun_C/0/1/0/all/0/1\">Chakkrit Termritthikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamtsho_Y/0/1/0/all/0/1\">Yeshi Jamtsho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ieamsaard_J/0/1/0/all/0/1\">Jirarat Ieamsaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muneesawang_P/0/1/0/all/0/1\">Paisarn Muneesawang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Ivan Lee</a>",
          "description": "The goals of this research were to search for Convolutional Neural Network\n(CNN) architectures, suitable for an on-device processor with limited computing\nresources, performing at substantially lower Network Architecture Search (NAS)\ncosts. A new algorithm entitled an Early Exit Population Initialisation (EE-PI)\nfor Evolutionary Algorithm (EA) was developed to achieve both goals. The EE-PI\nreduces the total number of parameters in the search process by filtering the\nmodels with fewer parameters than the maximum threshold. It will look for a new\nmodel to replace those models with parameters more than the threshold. Thereby,\nreducing the number of parameters, memory usage for model storage and\nprocessing time while maintaining the same performance or accuracy. The search\ntime was reduced to 0.52 GPU day. This is a huge and significant achievement\ncompared to the NAS of 4 GPU days achieved using NSGA-Net, 3,150 GPU days by\nthe AmoebaNet model, and the 2,000 GPU days by the NASNet model. As well, Early\nExit Evolutionary Algorithm networks (EEEA-Nets) yield network architectures\nwith minimal error and computational cost suitable for a given dataset as a\nclass of network algorithms. Using EEEA-Net on CIFAR-10, CIFAR-100, and\nImageNet datasets, our experiments showed that EEEA-Net achieved the lowest\nerror rate among state-of-the-art NAS models, with 2.46% for CIFAR-10, 15.02%\nfor CIFAR-100, and 23.8% for ImageNet dataset. Further, we implemented this\nimage recognition architecture for other tasks, such as object detection,\nsemantic segmentation, and keypoint detection tasks, and, in our experiments,\nEEEA-Net-C2 outperformed MobileNet-V3 on all of these various tasks. (The\nalgorithm code is available at https://github.com/chakkritte/EEEA-Net).",
          "link": "http://arxiv.org/abs/2108.06156",
          "publishedOn": "2021-08-16T00:47:33.357Z",
          "wordCount": 754,
          "title": "EEEA-Net: An Early Exit Evolutionary Neural Architecture Search. (arXiv:2108.06156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolova_M/0/1/0/all/0/1\">Marina Sokolova</a>",
          "description": "Reddit.com is a popular social media platform among young people. Reddit\nusers share their stories to seek support from other users, especially during\nthe Covid-19 pandemic. Messages posted on Reddit and their content have\nprovided researchers with opportunity to analyze public concerns. In this\nstudy, we analyzed sentiments of COVID-related messages posted on r/Depression.\nOur study poses the following questions: a) What are the common topics that the\nReddit users discuss? b) Can we use these topics to classify sentiments of the\nposts? c) What matters concern people more during the pandemic?\n\nKey Words: Sentiment Classification, Depression, COVID-19, Reddit, LDA, BERT",
          "link": "http://arxiv.org/abs/2108.06215",
          "publishedOn": "2021-08-16T00:47:33.351Z",
          "wordCount": 598,
          "title": "Sentiment Analysis of the COVID-related r/Depression Posts. (arXiv:2108.06215v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yicheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1\">Bingchen Fan</a>",
          "description": "A combinatorial cost function for hierarchical clustering was introduced by\nDasgupta \\cite{dasgupta2016cost}. It has been generalized by Cohen-Addad et al.\n\\cite{cohen2019hierarchical} to a general form named admissible function. In\nthis paper, we investigate hierarchical clustering from the\n\\emph{information-theoretic} perspective and formulate a new objective\nfunction. We also establish the relationship between these two perspectives. In\nalgorithmic aspect, we get rid of the traditional top-down and bottom-up\nframeworks, and propose a new one to stratify the \\emph{sparsest} level of a\ncluster tree recursively in guide with our objective function. For practical\nuse, our resulting cluster tree is not binary. Our algorithm called HCSE\noutputs a $k$-level cluster tree by a novel and interpretable mechanism to\nchoose $k$ automatically without any hyper-parameter. Our experimental results\non synthetic datasets show that HCSE has a great advantage in finding the\nintrinsic number of hierarchies, and the results on real datasets show that\nHCSE also achieves competitive costs over the popular algorithms LOUVAIN and\nHLP.",
          "link": "http://arxiv.org/abs/2108.06036",
          "publishedOn": "2021-08-16T00:47:33.345Z",
          "wordCount": 583,
          "title": "An Information-theoretic Perspective of Hierarchical Clustering. (arXiv:2108.06036v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tizikara_D/0/1/0/all/0/1\">Dativa K. Tizikara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serugunda_J/0/1/0/all/0/1\">Jonathan Serugunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katumba_A/0/1/0/all/0/1\">Andrew Katumba</a>",
          "description": "Future communication systems are faced with increased demand for high\ncapacity, dynamic bandwidth, reliability and heterogeneous traffic. To meet\nthese requirements, networks have become more complex and thus require new\ndesign methods and monitoring techniques, as they evolve towards becoming\nautonomous. Machine learning has come to the forefront in recent years as a\npromising technology to aid in this evolution. Optical fiber communications can\nalready provide the high capacity required for most applications, however,\nthere is a need for increased scalability and adaptability to changing user\ndemands and link conditions. Accurate performance monitoring is an integral\npart of this transformation. In this paper we review optical performance\nmonitoring techniques where machine learning algorithms have been applied.\nMoreover, since alot of OPM depends on knowledge of the signal type, we also\nreview work for modulation format recognition and bitrate identification. We\nadditionally briefly introduce a neuromorphic approach to OPM as an emerging\ntechnique that has only recently been applied to this domain.",
          "link": "http://arxiv.org/abs/2107.07338",
          "publishedOn": "2021-08-16T00:47:33.154Z",
          "wordCount": 622,
          "title": "An Overview of Machine Learning-aided Optical Performance Monitoring Techniques. (arXiv:2107.07338v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>",
          "description": "How to discriminatively vectorize graphs is a fundamental challenge that\nattracts increasing attentions in recent years. Inspired by the recent success\nof unsupervised contrastive learning, we aim to learn graph-level\nrepresentation in an unsupervised manner. Specifically, we propose a novel\nunsupervised graph learning paradigm called Iterative Graph Self-Distillation\n(IGSD) which iteratively performs the teacher-student distillation with graph\naugmentations. Different from conventional knowledge distillation, IGSD\nconstructs the teacher with an exponential moving average of the student model\nand distills the knowledge of itself. The intuition behind IGSD is to predict\nthe teacher network representation of the graph pairs under different augmented\nviews. As a natural extension, we also apply IGSD to semi-supervised scenarios\nby jointly regularizing the network with both supervised and unsupervised\ncontrastive loss. Finally, we show that finetuning the IGSD-trained models with\nself-training can further improve the graph representation power. Empirically,\nwe achieve significant and consistent performance gain on various graph\ndatasets in both unsupervised and semi-supervised settings, which well\nvalidates the superiority of IGSD.",
          "link": "http://arxiv.org/abs/2010.12609",
          "publishedOn": "2021-08-16T00:47:33.131Z",
          "wordCount": 633,
          "title": "Iterative Graph Self-Distillation. (arXiv:2010.12609v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16336",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1\">Emily M. Goren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.",
          "link": "http://arxiv.org/abs/2103.16336",
          "publishedOn": "2021-08-16T00:47:33.051Z",
          "wordCount": 680,
          "title": "Fast model-based clustering of partial records. (arXiv:2103.16336v5 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "Motivated by the success of masked language modeling~(MLM) in pre-training\nnatural language processing models, we propose w2v-BERT that explores MLM for\nself-supervised speech representation learning. w2v-BERT is a framework that\ncombines contrastive learning and MLM, where the former trains the model to\ndiscretize input continuous speech signals into a finite set of discriminative\nspeech tokens, and the latter trains the model to learn contextualized speech\nrepresentations via solving a masked prediction task consuming the discretized\ntokens. In contrast to existing MLM-based speech pre-training frameworks such\nas HuBERT, which relies on an iterative re-clustering and re-training process,\nor vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can\nbe optimized in an end-to-end fashion by solving the two self-supervised\ntasks~(the contrastive task and MLM) simultaneously. Our experiments show that\nw2v-BERT achieves competitive results compared to current state-of-the-art\npre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k\ncorpus as the unsupervised data. In particular, when compared to published\nmodels such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\\%\nto~10\\% relative WER reduction on the test-clean and test-other subsets. When\napplied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our\ninternal conformer-based wav2vec~2.0 by more than~30\\% relatively.",
          "link": "http://arxiv.org/abs/2108.06209",
          "publishedOn": "2021-08-16T00:47:33.004Z",
          "wordCount": 649,
          "title": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training. (arXiv:2108.06209v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dietrich_A/0/1/0/all/0/1\">Anastasia Dietrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gressmann_F/0/1/0/all/0/1\">Frithjof Gressmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_D/0/1/0/all/0/1\">Douglas Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelombiev_I/0/1/0/all/0/1\">Ivan Chelombiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justus_D/0/1/0/all/0/1\">Daniel Justus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>",
          "description": "Identifying algorithms for computational efficient unsupervised training of\nlarge language models is an important and active area of research. In this\nwork, we develop and study a straightforward, dynamic always-sparse\npre-training approach for BERT language modeling task, which leverages periodic\ncompression steps based on magnitude pruning followed by random parameter\nre-allocation. This approach enables us to achieve Pareto improvements in terms\nof the number of floating-point operations (FLOPs) over statically sparse and\ndense models across a broad spectrum of network sizes. Furthermore, we\ndemonstrate that training remains FLOP-efficient when using coarse-grained\nblock sparsity, making it particularly promising for efficient execution on\nmodern hardware accelerators.",
          "link": "http://arxiv.org/abs/2108.06277",
          "publishedOn": "2021-08-16T00:47:32.983Z",
          "wordCount": 541,
          "title": "Towards Structured Dynamic Sparse Pre-Training of BERT. (arXiv:2108.06277v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_P/0/1/0/all/0/1\">Prithiviraj Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhkaran Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achankuju_J/0/1/0/all/0/1\">Josemon Achankuju</a>",
          "description": "We present VESPA, an intentionally simple yet novel zero-shot system for\nlayout, locale, and domain agnostic document extraction. In spite of the\navailability of large corpora of documents, the lack of labeled and validated\ndatasets makes it a challenge to discriminatively train document extraction\nmodels for enterprises. We show that this problem can be addressed by simply\ntransferring the information extraction (IE) task to a natural language\nQuestion-Answering (QA) task without engineering task-specific architectures.\nWe demonstrate the effectiveness of our system by evaluating on a closed corpus\nof real-world retail and tax invoices with multiple complex layouts, domains,\nand geographies. The empirical evaluation shows that our system outperforms 4\nprominent commercial invoice solutions that use discriminatively trained models\nwith architectures specifically crafted for invoice extraction. We extracted 6\nfields with zero upfront human annotation or training with an Avg. F1 of 87.50.",
          "link": "http://arxiv.org/abs/2108.06069",
          "publishedOn": "2021-08-16T00:47:32.977Z",
          "wordCount": 585,
          "title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA Ensemble. (arXiv:2108.06069v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gnecco_G/0/1/0/all/0/1\">Giorgio Gnecco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacigalupo_A/0/1/0/all/0/1\">Andrea Bacigalupo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fantoni_F/0/1/0/all/0/1\">Francesca Fantoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvi_D/0/1/0/all/0/1\">Daniela Selvi</a>",
          "description": "A promising technique for the spectral design of acoustic metamaterials is\nbased on the formulation of suitable constrained nonlinear optimization\nproblems. Unfortunately, the straightforward application of classical\ngradient-based iterative optimization algorithms to the numerical solution of\nsuch problems is typically highly demanding, due to the complexity of the\nunderlying physical models. Nevertheless, supervised machine learning\ntechniques can reduce such a computational effort, e.g., by replacing the\noriginal objective functions of such optimization problems with more-easily\ncomputable approximations. In this framework, the present article describes the\napplication of a related unsupervised machine learning technique, namely,\nprincipal component analysis, to approximate the gradient of the objective\nfunction of a band gap optimization problem for an acoustic metamaterial, with\nthe aim of making the successive application of a gradient-based iterative\noptimization algorithm faster. Numerical results show the effectiveness of the\nproposed method.",
          "link": "http://arxiv.org/abs/2104.02588",
          "publishedOn": "2021-08-16T00:47:32.962Z",
          "wordCount": 662,
          "title": "Principal Component Analysis Applied to Gradient Fields in Band Gap Optimization Problems for Metamaterials. (arXiv:2104.02588v6 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets, often with better accuracy and fewer\nparameters. Our model eliminates the requirement for class token and positional\nembeddings through a novel sequence pooling strategy and the use of\nconvolution/s. It is flexible in terms of model size, and can have as little as\n0.28M parameters while achieving good results. Our model can reach 98.00%\naccuracy when training from scratch on CIFAR-10, which is a significant\nimprovement over previous Transformer based models. It also outperforms many\nmodern CNN based approaches, such as ResNet, and even some recent NAS-based\napproaches, such as Proxyless-NAS. Our simple and compact design democratizes\ntransformers by making them accessible to those with limited computing\nresources and/or dealing with small datasets. Our method also works on larger\ndatasets, such as ImageNet (82.71% accuracy with 29% parameters of ViT), and\nNLP tasks as well. Our code and pre-trained models are publicly available at\nhttps://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-08-16T00:47:32.955Z",
          "wordCount": 760,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1\">Hassan Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_W/0/1/0/all/0/1\">Wei-Hong Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>",
          "description": "We present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Specifically, our\nVideo-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts\nmultimodal representations that are rich enough to benefit a variety of\ndownstream tasks. We train VATT end-to-end from scratch using multimodal\ncontrastive losses and evaluate its performance by the downstream tasks of\nvideo action recognition, audio event classification, image classification, and\ntext-to-video retrieval. Furthermore, we study a modality-agnostic\nsingle-backbone Transformer by sharing weights among the three modalities. We\nshow that the convolution-free VATT outperforms state-of-the-art ConvNet-based\narchitectures in the downstream tasks. Especially, VATT's vision Transformer\nachieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and\n41.1% on Moments in Time, new records while avoiding supervised pre-training.\nTransferring to image classification leads to 78.7% top-1 accuracy on ImageNet\ncompared to 64.7% by training the same Transformer from scratch, showing the\ngeneralizability of our model despite the domain gap between videos and images.\nVATT's audio Transformer also sets a new record on waveform-based audio event\nrecognition by achieving the mAP of 39.4% on AudioSet without any supervised\npre-training. VATT's source code is publicly available.",
          "link": "http://arxiv.org/abs/2104.11178",
          "publishedOn": "2021-08-16T00:47:32.949Z",
          "wordCount": 689,
          "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text. (arXiv:2104.11178v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Faccio_F/0/1/0/all/0/1\">Francesco Faccio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_L/0/1/0/all/0/1\">Louis Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "Traditional off-policy actor-critic Reinforcement Learning (RL) algorithms\nlearn value functions of a single target policy. However, when value functions\nare updated to track the learned policy, they forget potentially useful\ninformation about old policies. We introduce a class of value functions called\nParameter-Based Value Functions (PBVFs) whose inputs include the policy\nparameters. They can generalize across different policies. PBVFs can evaluate\nthe performance of any policy given a state, a state-action pair, or a\ndistribution over the RL agent's initial states. First we show how PBVFs yield\nnovel off-policy policy gradient theorems. Then we derive off-policy\nactor-critic algorithms based on PBVFs trained by Monte Carlo or Temporal\nDifference methods. We show how learned PBVFs can zero-shot learn new policies\nthat outperform any policy seen during training. Finally our algorithms are\nevaluated on a selection of discrete and continuous control tasks using shallow\npolicies and deep neural networks. Their performance is comparable to\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2006.09226",
          "publishedOn": "2021-08-16T00:47:32.942Z",
          "wordCount": 635,
          "title": "Parameter-Based Value Functions. (arXiv:2006.09226v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12756",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Beck_E/0/1/0/all/0/1\">Edgar Beck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bockelmann_C/0/1/0/all/0/1\">Carsten Bockelmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dekorsy_A/0/1/0/all/0/1\">Armin Dekorsy</a>",
          "description": "Following the great success of Machine Learning (ML), especially Deep Neural\nNetworks (DNNs), in many research domains in 2010s, several ML-based approaches\nwere proposed for detection in large inverse linear problems, e.g., massive\nMIMO systems. The main motivation behind is that the complexity of Maximum\nA-Posteriori (MAP) detection grows exponentially with system dimensions.\nInstead of using DNNs, essentially being a black-box, we take a slightly\ndifferent approach and introduce a probabilistic Continuous relaxation of\ndisCrete variables to MAP detection. Enabling close approximation and\ncontinuous optimization, we derive an iterative detection algorithm: Concrete\nMAP Detection (CMD). Furthermore, extending CMD by the idea of deep unfolding\ninto CMDNet, we allow for (online) optimization of a small number of parameters\nto different working points while limiting complexity. In contrast to recent\nDNN-based approaches, we select the optimization criterion and output of CMDNet\nbased on information theory and are thus able to learn approximate\nprobabilities of the individual optimal detector. This is crucial for soft\ndecoding in today's communication systems. Numerical simulation results in MIMO\nsystems reveal CMDNet to feature a promising accuracy complexity trade-off\ncompared to State of the Art. Notably, we demonstrate CMDNet's soft outputs to\nbe reliable for decoders.",
          "link": "http://arxiv.org/abs/2102.12756",
          "publishedOn": "2021-08-16T00:47:32.934Z",
          "wordCount": 692,
          "title": "CMDNet: Learning a Probabilistic Relaxation of Discrete Variables for Soft Detection with Low Complexity. (arXiv:2102.12756v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brunke_L/0/1/0/all/0/1\">Lukas Brunke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greeff_M/0/1/0/all/0/1\">Melissa Greeff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_A/0/1/0/all/0/1\">Adam W. Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhaocong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Siqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panerati_J/0/1/0/all/0/1\">Jacopo Panerati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoellig_A/0/1/0/all/0/1\">Angela P. Schoellig</a> (University of Toronto Institute for Aerospace Studies, University of Toronto Robotics Institute, Vector Institute for Artificial Intelligence)",
          "description": "The last half-decade has seen a steep rise in the number of contributions on\nsafe learning methods for real-world robotic deployments from both the control\nand reinforcement learning communities. This article provides a concise but\nholistic review of the recent advances made in using machine learning to\nachieve safe decision making under uncertainties, with a focus on unifying the\nlanguage and frameworks used in control theory and reinforcement learning\nresearch. Our review includes: learning-based control approaches that safely\nimprove performance by learning the uncertain dynamics, reinforcement learning\napproaches that encourage safety or robustness, and methods that can formally\ncertify the safety of a learned control policy. As data- and learning-based\nrobot control methods continue to gain traction, researchers must understand\nwhen and how to best leverage them in real-world scenarios where safety is\nimperative, such as when operating in close proximity to humans. We highlight\nsome of the open challenges that will drive the field of robot learning in the\ncoming years, and emphasize the need for realistic physics-based benchmarks to\nfacilitate fair comparisons between control and reinforcement learning\napproaches.",
          "link": "http://arxiv.org/abs/2108.06266",
          "publishedOn": "2021-08-16T00:47:32.898Z",
          "wordCount": 655,
          "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning. (arXiv:2108.06266v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Savkli_C/0/1/0/all/0/1\">Cetin Savkli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_C/0/1/0/all/0/1\">Catherine Schwartz</a>",
          "description": "We present a new subspace-based method to construct probabilistic models for\nhigh-dimensional data and highlight its use in anomaly detection. The approach\nis based on a statistical estimation of probability density using densities of\nrandom subspaces combined with geometric averaging. In selecting random\nsubspaces, equal representation of each attribute is used to ensure correct\nstatistical limits. Gaussian mixture models (GMMs) are used to create the\nprobability densities for each subspace with techniques included to mitigate\nsingularities allowing for the ability to handle both numerical and categorial\nattributes. The number of components for each GMM is determined automatically\nthrough Bayesian information criterion to prevent overfitting. The proposed\nalgorithm attains competitive AUC scores compared with prominent algorithms\nagainst benchmark anomaly detection datasets with the added benefits of being\nsimple, scalable, and interpretable.",
          "link": "http://arxiv.org/abs/2108.06283",
          "publishedOn": "2021-08-16T00:47:32.888Z",
          "wordCount": 581,
          "title": "Random Subspace Mixture Models for Interpretable Anomaly Detection. (arXiv:2108.06283v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_E/0/1/0/all/0/1\">Erzhuo Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Fine-grained population distribution data is of great importance for many\napplications, e.g., urban planning, traffic scheduling, epidemic modeling, and\nrisk control. However, due to the limitations of data collection, including\ninfrastructure density, user privacy, and business security, such fine-grained\ndata is hard to collect and usually, only coarse-grained data is available.\nThus, obtaining fine-grained population distribution from coarse-grained\ndistribution becomes an important problem. To complete this task, existing\nmethods mainly rely on sufficient fine-grained ground truth for training, which\nis not often available. This limits the applications of these methods and\nbrings the necessity to transfer knowledge from data-sufficient cities to\ndata-scarce cities.\n\nIn knowledge transfer scenario, we employ single reference fine-grained\nground truth in the target city as the ground truth to inform the large-scale\nurban structure and support the knowledge transfer in the target city. By this\napproach, we transform the fine-grained population mapping problem into a\none-shot transfer learning problem for population mapping task.\n\nIn this paper, we propose a one-shot transfer learning framework, PSRNet, to\ntransfer spatial-temporal knowledge across cities in fine-grained population\nmapping task from the view of network structure, data, and optimization.\nExperiments on real-life datasets of 4 cities demonstrate that PSRNet has\nsignificant advantages over 8 baselines by reducing RMSE and MAE for more than\n25%. Our code and datasets are released in Github.",
          "link": "http://arxiv.org/abs/2108.06228",
          "publishedOn": "2021-08-16T00:47:32.879Z",
          "wordCount": 653,
          "title": "One-shot Transfer Learning for Population Mapping. (arXiv:2108.06228v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malekmohammadi_S/0/1/0/all/0/1\">Saber Malekmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaloudegi_K/0/1/0/all/0/1\">Kiarash Shaloudegi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zeou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaoliang Yu</a>",
          "description": "Over the past few years, the federated learning ($\\texttt{FL}$) community has\nwitnessed a proliferation of new $\\texttt{FL}$ algorithms. However, our\nunderstating of the theory of $\\texttt{FL}$ is still fragmented, and a\nthorough, formal comparison of these algorithms remains elusive. Motivated by\nthis gap, we show that many of the existing $\\texttt{FL}$ algorithms can be\nunderstood from an operator splitting point of view. This unification allows us\nto compare different algorithms with ease, to refine previous convergence\nresults and to uncover new algorithmic variants. In particular, our analysis\nreveals the vital role played by the step size in $\\texttt{FL}$ algorithms. The\nunification also leads to a streamlined and economic way to accelerate\n$\\texttt{FL}$ algorithms, without incurring any communication overhead. We\nperform numerical experiments on both convex and nonconvex models to validate\nour findings.",
          "link": "http://arxiv.org/abs/2108.05974",
          "publishedOn": "2021-08-16T00:47:32.874Z",
          "wordCount": 565,
          "title": "An Operator Splitting View of Federated Learning. (arXiv:2108.05974v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.13268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjal Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatziafratis_V/0/1/0/all/0/1\">Vaggos Chatziafratis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayaraghavan_A/0/1/0/all/0/1\">Aravindan Vijayaraghavan</a>",
          "description": "Many machine learning systems are vulnerable to small perturbations made to\ninputs either at test time or at training time. This has received much recent\ninterest on the empirical front due to applications where reliability and\nsecurity are critical. However, theoretical understanding of algorithms that\nare robust to adversarial perturbations is limited.\n\nIn this work we focus on Principal Component Analysis (PCA), a ubiquitous\nalgorithmic primitive in machine learning. We formulate a natural robust\nvariant of PCA where the goal is to find a low dimensional subspace to\nrepresent the given data with minimum projection error, that is in addition\nrobust to small perturbations measured in $\\ell_q$ norm (say $q=\\infty$).\nUnlike PCA which is solvable in polynomial time, our formulation is\ncomputationally intractable to optimize as it captures a variant of the\nwell-studied sparse PCA objective as a special case. We show the following\nresults:\n\n-Polynomial time algorithm that is constant factor competitive in the\nworst-case with respect to the best subspace, in terms of the projection error\nand the robustness criterion.\n\n-We show that our algorithmic techniques can also be made robust to\nadversarial training-time perturbations, in addition to yielding\nrepresentations that are robust to adversarial perturbations at test time.\nSpecifically, we design algorithms for a strong notion of training-time\nperturbations, where every point is adversarially perturbed up to a specified\namount.\n\n-We illustrate the broad applicability of our algorithmic techniques in\naddressing robustness to adversarial perturbations, both at training time and\ntest time. In particular, our adversarially robust PCA primitive leads to\ncomputationally efficient and robust algorithms for both unsupervised and\nsupervised learning problems such as clustering and learning adversarially\nrobust classifiers.",
          "link": "http://arxiv.org/abs/1911.13268",
          "publishedOn": "2021-08-16T00:47:32.869Z",
          "wordCount": 760,
          "title": "Adversarially Robust Low Dimensional Representations. (arXiv:1911.13268v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_V/0/1/0/all/0/1\">Vasileios Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "Data-driven learning algorithms are employed in many online applications, in\nwhich data become available over time, like network monitoring, stock price\nprediction, job applications, etc. The underlying data distribution might\nevolve over time calling for model adaptation as new instances arrive and old\ninstances become obsolete. In such dynamic environments, the so-called data\nstreams, fairness-aware learning cannot be considered as a one-off requirement,\nbut rather it should comprise a continual requirement over the stream. Recent\nfairness-aware stream classifiers ignore the problem of class imbalance, which\nmanifests in many real-life applications, and mitigate discrimination mainly\nbecause they \"reject\" minority instances at large due to their inability to\neffectively learn all classes.\n\nIn this work, we propose \\ours, an online fairness-aware approach that\nmaintains a valid and fair classifier over the stream. \\ours~is an online\nboosting approach that changes the training distribution in an online fashion\nby monitoring stream's class imbalance and tweaks its decision boundary to\nmitigate discriminatory outcomes over the stream. Experiments on 8 real-world\nand 1 synthetic datasets from different domains with varying class imbalance\ndemonstrate the superiority of our method over state-of-the-art fairness-aware\nstream approaches with a range (relative) increase [11.2\\%-14.2\\%] in balanced\naccuracy, [22.6\\%-31.8\\%] in gmean, [42.5\\%-49.6\\%] in recall, [14.3\\%-25.7\\%]\nin kappa and [89.4\\%-96.6\\%] in statistical parity (fairness).",
          "link": "http://arxiv.org/abs/2108.06231",
          "publishedOn": "2021-08-16T00:47:32.851Z",
          "wordCount": 648,
          "title": "Online Fairness-Aware Learning with Imbalanced Data Streams. (arXiv:2108.06231v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parasrampuria_R/0/1/0/all/0/1\">Rohan Parasrampuria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Suchandra Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Dhrubasish Sarkar</a>",
          "description": "In today's tech-savvy world every industry is trying to formulate methods for\nrecommending products by combining several techniques and algorithms to form a\npool that would bring forward the most enhanced models for making the\npredictions. Building on these lines is our paper focused on the application of\nsentiment analysis for recommendation in the insurance domain. We tried\nbuilding the following Machine Learning models namely, Logistic Regression,\nMultinomial Naive Bayes, and the mighty Random Forest for analyzing the\npolarity of a given feedback line given by a customer. Then we used this\npolarity along with other attributes like Age, Gender, Locality, Income, and\nthe list of other products already purchased by our existing customers as input\nfor our recommendation model. Then we matched the polarity score along with the\nuser's profiles and generated the list of insurance products to be recommended\nin descending order. Despite our model's simplicity and the lack of the key\ndata sets, the results seemed very logical and realistic. So, by developing the\nmodel with more enhanced methods and with access to better and true data\ngathered from an insurance industry may be the sector could be very well\nbenefitted from the amalgamation of sentiment analysis with a recommendation.",
          "link": "http://arxiv.org/abs/2108.06210",
          "publishedOn": "2021-08-16T00:47:32.845Z",
          "wordCount": 642,
          "title": "Recommending Insurance products by using Users' Sentiments. (arXiv:2108.06210v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2005.08859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Filom_K/0/1/0/all/0/1\">Khashayar Filom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kording_K/0/1/0/all/0/1\">Konrad Paul Kording</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhoodi_R/0/1/0/all/0/1\">Roozbeh Farhoodi</a>",
          "description": "Neural networks are versatile tools for computation, having the ability to\napproximate a broad range of functions. An important problem in the theory of\ndeep neural networks is expressivity; that is, we want to understand the\nfunctions that are computable by a given network. We study real infinitely\ndifferentiable (smooth) hierarchical functions implemented by feedforward\nneural networks via composing simpler functions in two cases:\n\n1) each constituent function of the composition has fewer inputs than the\nresulting function;\n\n2) constituent functions are in the more specific yet prevalent form of a\nnon-linear univariate function (e.g. tanh) applied to a linear multivariate\nfunction.\n\nWe establish that in each of these regimes there exist non-trivial algebraic\npartial differential equations (PDEs), which are satisfied by the computed\nfunctions. These PDEs are purely in terms of the partial derivatives and are\ndependent only on the topology of the network. For compositions of polynomial\nfunctions, the algebraic PDEs yield non-trivial equations (of degrees dependent\nonly on the architecture) in the ambient polynomial space that are satisfied on\nthe associated functional varieties. Conversely, we conjecture that such PDE\nconstraints, once accompanied by appropriate non-singularity conditions and\nperhaps certain inequalities involving partial derivatives, guarantee that the\nsmooth function under consideration can be represented by the network. The\nconjecture is verified in numerous examples including the case of tree\narchitectures which are of neuroscientific interest. Our approach is a step\ntoward formulating an algebraic description of functional spaces associated\nwith specific neural networks, and may provide new, useful tools for\nconstructing neural networks.",
          "link": "http://arxiv.org/abs/2005.08859",
          "publishedOn": "2021-08-16T00:47:32.836Z",
          "wordCount": 737,
          "title": "PDE constraints on smooth hierarchical functions computed by neural networks. (arXiv:2005.08859v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12758",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Dhadphale_J/0/1/0/all/0/1\">Jayesh Dhadphale</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Unni_V/0/1/0/all/0/1\">Vishnu R. Unni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saha_A/0/1/0/all/0/1\">Abhishek Saha</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sujith_R/0/1/0/all/0/1\">R. I. Sujith</a>",
          "description": "In reacting flow systems, thermoacoustic instability characterized by high\namplitude pressure fluctuations, is driven by a positive coupling between the\nunsteady heat release rate and the acoustic field of the combustor. When the\nunderlying flow is turbulent, as a control parameter of the system is varied\nand the system approach thermoacoustic instability, the acoustic pressure\noscillations synchronize with heat release rate oscillations. Consequently,\nduring the onset of thermoacoustic instability in turbulent combustors, the\nsystem dynamics transition from chaotic oscillations to periodic oscillations\nvia a state of intermittency. Thermoacoustic systems are traditionally modeled\nby coupling the model for the unsteady heat source and the acoustic subsystem,\neach estimated independently. The response of the unsteady heat source, the\nflame, to acoustic fluctuations are characterized by introducing external\nunsteady forcing. This necessitates a powerful excitation module to obtain the\nnonlinear response of the flame to acoustic perturbations. Instead of\ncharacterizing individual subsystems, we introduce a neural ordinary\ndifferential equation (neural ODE) framework to model the thermoacoustic system\nas a whole. The neural ODE model for the thermoacoustic system uses time series\nof the heat release rate and the pressure fluctuations, measured simultaneously\nwithout introducing any external perturbations, to model their coupled\ninteraction. Further, we use the parameters of neural ODE to define an anomaly\nmeasure that represents the proximity of system dynamics to limit cycle\noscillations and thus provide an early warning signal for the onset of\nthermoacoustic instability.",
          "link": "http://arxiv.org/abs/2106.12758",
          "publishedOn": "2021-08-16T00:47:32.821Z",
          "wordCount": 699,
          "title": "Neural ODE to model and prognose thermoacoustic instability. (arXiv:2106.12758v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01779",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Lu_Y/0/1/0/all/0/1\">Ying Lu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_Y/0/1/0/all/0/1\">Yue-Min Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhou_P/0/1/0/all/0/1\">Peng-Fei Zhou</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1\">Shi-Ju Ran</a>",
          "description": "State preparation is of fundamental importance in quantum physics, which can\nbe realized by constructing the quantum circuit as a unitary that transforms\nthe initial state to the target, or implementing a quantum control protocol to\nevolve to the target state with a designed Hamiltonian. In this work, we study\nthe latter on quantum many-body systems by the time evolution with fixed\ncouplings and variational magnetic fields. In specific, we consider to prepare\nthe ground states of the Hamiltonians containing certain interactions that are\nmissing in the Hamiltonians for the time evolution. An optimization method is\nproposed to optimize the magnetic fields by \"fine-graining\" the discretization\nof time, in order to gain high precision and stability. The back propagation\ntechnique is utilized to obtain the gradients of the fields against the\nlogarithmic fidelity. Our method is tested on preparing the ground state of\nHeisenberg chain with the time evolution by the XY and Ising interactions, and\nits performance surpasses two baseline methods that use local and global\noptimization strategies, respectively. Our work can be applied and generalized\nto other quantum models such as those defined on higher dimensional lattices.\nIt enlightens to reduce the complexity of the required interactions for\nimplementing quantum control or other tasks in quantum information and\ncomputation by means of optimizing the magnetic fields.",
          "link": "http://arxiv.org/abs/2106.01779",
          "publishedOn": "2021-08-16T00:47:32.816Z",
          "wordCount": 690,
          "title": "Preparation of Many-body Ground States by Time Evolution with Variational Microscopic Magnetic Fields and Incomplete Interactions. (arXiv:2106.01779v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.04840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjal Awasthi</a>",
          "description": "We study active learning of homogeneous $s$-sparse halfspaces in\n$\\mathbb{R}^d$ under the setting where the unlabeled data distribution is\nisotropic log-concave and each label is flipped with probability at most $\\eta$\nfor a parameter $\\eta \\in \\big[0, \\frac12\\big)$, known as the bounded noise.\nEven in the presence of mild label noise, i.e. $\\eta$ is a small constant, this\nis a challenging problem and only recently have label complexity bounds of the\nform $\\tilde{O}\\big(s \\cdot \\mathrm{polylog}(d, \\frac{1}{\\epsilon})\\big)$ been\nestablished in [Zhang, 2018] for computationally efficient algorithms. In\ncontrast, under high levels of label noise, the label complexity bounds\nachieved by computationally efficient algorithms are much worse: the best known\nresult of [Awasthi et al., 2016] provides a computationally efficient algorithm\nwith label complexity $\\tilde{O}\\big((\\frac{s \\ln\nd}{\\epsilon})^{2^{\\mathrm{poly}(1/(1-2\\eta))}} \\big)$, which is label-efficient\nonly when the noise rate $\\eta$ is a fixed constant. In this work, we\nsubstantially improve on it by designing a polynomial time algorithm for active\nlearning of $s$-sparse halfspaces, with a label complexity of\n$\\tilde{O}\\big(\\frac{s}{(1-2\\eta)^4} \\mathrm{polylog} (d, \\frac 1 \\epsilon)\n\\big)$. This is the first efficient algorithm with label complexity polynomial\nin $\\frac{1}{1-2\\eta}$ in this setting, which is label-efficient even for\n$\\eta$ arbitrarily close to $\\frac12$. Our active learning algorithm and its\ntheoretical guarantees also immediately translate to new state-of-the-art label\nand sample complexity results for full-dimensional active and passive halfspace\nlearning under arbitrary bounded noise. The key insight of our algorithm and\nanalysis is a new interpretation of online learning regret inequalities, which\nmay be of independent interest.",
          "link": "http://arxiv.org/abs/2002.04840",
          "publishedOn": "2021-08-16T00:47:32.801Z",
          "wordCount": 730,
          "title": "Efficient active learning of sparse halfspaces with arbitrary bounded noise. (arXiv:2002.04840v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kucherenko_T/0/1/0/all/0/1\">Taras Kucherenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_R/0/1/0/all/0/1\">Rajmund Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonell_P/0/1/0/all/0/1\">Patrik Jonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_M/0/1/0/all/0/1\">Michael Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>",
          "description": "We propose a new framework for gesture generation, aiming to allow\ndata-driven approaches to produce more semantically rich gestures. Our approach\nfirst predicts whether to gesture, followed by a prediction of the gesture\nproperties. Those properties are then used as conditioning for a modern\nprobabilistic gesture-generation model capable of high-quality output. This\nempowers the approach to generate gestures that are both diverse and\nrepresentational. Follow-ups and more information can be found on the project\npage: https://svito-zar.github.io/speech2properties2gestures/ .",
          "link": "http://arxiv.org/abs/2106.14736",
          "publishedOn": "2021-08-16T00:47:32.776Z",
          "wordCount": 587,
          "title": "Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for Generating Representational Gestures from Speech. (arXiv:2106.14736v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1\">Pavan Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1\">Alexander Lerch</a>",
          "description": "Several automatic approaches for objective music performance assessment (MPA)\nhave been proposed in the past, however, existing systems are not yet capable\nof reliably predicting ratings with the same accuracy as professional judges.\nThis study investigates contrastive learning as a potential method to improve\nexisting MPA systems. Contrastive learning is a widely used technique in\nrepresentation learning to learn a structured latent space capable of\nseparately clustering multiple classes. It has been shown to produce state of\nthe art results for image-based classification problems. We introduce a\nweighted contrastive loss suitable for regression tasks applied to a\nconvolutional neural network and show that contrastive loss results in\nperformance gains in regression tasks for MPA. Our results show that\ncontrastive-based methods are able to match and exceed SoTA performance for MPA\nregression tasks by creating better class clusters within the latent space of\nthe neural networks.",
          "link": "http://arxiv.org/abs/2108.01711",
          "publishedOn": "2021-08-16T00:47:32.771Z",
          "wordCount": 595,
          "title": "Improving Music Performance Assessment with Contrastive Learning. (arXiv:2108.01711v1 [cs.SD] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">Jan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhulai_S/0/1/0/all/0/1\">Sandjai Bhulai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1\">Mark Hoogendoorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_R/0/1/0/all/0/1\">Rob van der Mei</a>",
          "description": "Over the past decade, the advent of cybercrime has accelarated the research\non cybersecurity. However, the deployment of intrusion detection methods falls\nshort. One of the reasons for this is the lack of realistic evaluation\ndatasets, which makes it a challenge to develop techniques and compare them.\nThis is caused by the large amounts of effort it takes for a cyber analyst to\nclassify network connections. This has raised the need for methods (i) that can\nlearn from small sets of labeled data, (ii) that can make predictions on large\nsets of unlabeled data, and (iii) that request the label of only specially\nselected unlabeled data instances. Hence, Active Learning (AL) methods are of\ninterest. These approaches choose speci?fic unlabeled instances by a query\nfunction that are expected to improve overall classi?cation performance. The\nresulting query observations are labeled by a human expert and added to the\nlabeled set.\n\nIn this paper, we propose a new hybrid AL method called Jasmine. Firstly, it\ndetermines how suitable each observation is for querying, i.e., how likely it\nis to enhance classi?cation. These properties are the uncertainty score and\nanomaly score. Secondly, Jasmine introduces dynamic updating. This allows the\nmodel to adjust the balance between querying uncertain, anomalous and randomly\nselected observations. To this end, Jasmine is able to learn the best query\nstrategy during the labeling process. This is in contrast to the other AL\nmethods in cybersecurity that all have static, predetermined query functions.\nWe show that dynamic updating, and therefore Jasmine, is able to consistently\nobtain good and more robust results than querying only uncertainties, only\nanomalies or a ?fixed combination of the two.",
          "link": "http://arxiv.org/abs/2108.06238",
          "publishedOn": "2021-08-16T00:47:32.766Z",
          "wordCount": 718,
          "title": "Jasmine: A New Active Learning Approach to Combat Cybercrime. (arXiv:2108.06238v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06201",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Loecher_M/0/1/0/all/0/1\">Markus Loecher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>",
          "description": "Tree-based algorithms such as random forests and gradient boosted trees\ncontinue to be among the most popular and powerful machine learning models used\nacross multiple disciplines. The conventional wisdom of estimating the impact\nof a feature in tree based models is to measure the \\textit{node-wise reduction\nof a loss function}, which (i) yields only global importance measures and (ii)\nis known to suffer from severe biases. Conditional feature contributions (CFCs)\nprovide \\textit{local}, case-by-case explanations of a prediction by following\nthe decision path and attributing changes in the expected output of the model\nto each feature along the path. However, Lundberg et al. pointed out a\npotential bias of CFCs which depends on the distance from the root of a tree.\nThe by now immensely popular alternative, SHapley Additive exPlanation (SHAP)\nvalues appear to mitigate this bias but are computationally much more\nexpensive. Here we contribute a thorough comparison of the explanations\ncomputed by both methods on a set of 164 publicly available classification\nproblems in order to provide data-driven algorithm recommendations to current\nresearchers. For random forests, we find extremely high similarities and\ncorrelations of both local and global SHAP values and CFC scores, leading to\nvery similar rankings and interpretations. Analogous conclusions hold for the\nfidelity of using global feature importance scores as a proxy for the\npredictive power associated with each feature.",
          "link": "http://arxiv.org/abs/2108.06201",
          "publishedOn": "2021-08-16T00:47:32.747Z",
          "wordCount": 664,
          "title": "Data-driven advice for interpreting local and global model predictions in bioinformatics problems. (arXiv:2108.06201v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_R/0/1/0/all/0/1\">Rohan Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Maheep Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurya_C/0/1/0/all/0/1\">Chandresh Kumar Maurya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1\">Shitala Prasad</a>",
          "description": "Intelligent recommendation and reminder systems are the need of the\nfast-pacing life. Current intelligent systems such as Siri, Google Assistant,\nMicrosoft Cortona, etc., have limited capability. For example, if you want to\nwake up at 6 am because you have an upcoming trip, you have to set the alarm\nmanually. Besides, these systems do not recommend or remind what else to carry,\nsuch as carrying an umbrella during a likely rain. The present work proposes a\nsystem that takes an email as input and returns a recommendation-cumreminder\nlist. As a first step, we parse the emails, recognize the entities using named\nentity recognition (NER). In the second step, information retrieval over the\nweb is done to identify nearby places, climatic conditions, etc. Imperative\nsentences from the reviews of all places are extracted and passed to the object\nextraction module. The main challenge lies in extracting the objects (items) of\ninterest from the review. To solve it, a modified Machine Reading\nComprehension-NER (MRC-NER) model is trained to tag objects of interest by\nformulating annotation rules as a query. The objects so found are recommended\nto the user one day in advance. The final reminder list of objects is pruned by\nour proposed model for tracking objects kept during the \"packing activity.\"\nEventually, when the user leaves for the event/trip, an alert is sent\ncontaining the reminding list items. Our approach achieves superior performance\ncompared to several baselines by as much as 30% on recall and 10% on precision.",
          "link": "http://arxiv.org/abs/2108.06206",
          "publishedOn": "2021-08-16T00:47:32.726Z",
          "wordCount": 676,
          "title": "An Intelligent Recommendation-cum-Reminder System. (arXiv:2108.06206v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sherman_U/0/1/0/all/0/1\">Uri Sherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>",
          "description": "We study a variant of online convex optimization where the player is\npermitted to switch decisions at most $S$ times in expectation throughout $T$\nrounds. Similar problems have been addressed in prior work for the discrete\ndecision set setting, and more recently in the continuous setting but only with\nan adaptive adversary. In this work, we aim to fill the gap and present\ncomputationally efficient algorithms in the more prevalent oblivious setting,\nestablishing a regret bound of $O(T/S)$ for general convex losses and\n$\\widetilde O(T/S^2)$ for strongly convex losses. In addition, for stochastic\ni.i.d.~losses, we present a simple algorithm that performs $\\log T$ switches\nwith only a multiplicative $\\log T$ factor overhead in its regret in both the\ngeneral and strongly convex settings. Finally, we complement our algorithms\nwith lower bounds that match our upper bounds in some of the cases we consider.",
          "link": "http://arxiv.org/abs/2102.03803",
          "publishedOn": "2021-08-16T00:47:32.714Z",
          "wordCount": 617,
          "title": "Lazy OCO: Online Convex Optimization on a Switching Budget. (arXiv:2102.03803v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gerhard_Young_G/0/1/0/all/0/1\">Greyson Gerhard-Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantha_R/0/1/0/all/0/1\">Raviteja Anantha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chappidi_S/0/1/0/all/0/1\">Srinivas Chappidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmeister_B/0/1/0/all/0/1\">Bj&#xf6;rn Hoffmeister</a>",
          "description": "Recent work building open-domain chatbots has demonstrated that increasing\nmodel size improves performance. On the other hand, latency and connectivity\nconsiderations dictate the move of digital assistants on the device. Giving a\ndigital assistant like Siri, Alexa, or Google Assistant the ability to discuss\njust about anything leads to the need for reducing the chatbot model size such\nthat it fits on the user's device. We demonstrate that low parameter models can\nsimultaneously retain their general knowledge conversational abilities while\nimproving in a specific domain. Additionally, we propose a generic framework\nthat accounts for variety in question types, tracks reference throughout\nmulti-turn conversations, and removes inconsistent and potentially toxic\nresponses. Our framework seamlessly transitions between chatting and performing\ntransactional tasks, which will ultimately make interactions with digital\nassistants more human-like. We evaluate our framework on 1 internal and 4\npublic benchmark datasets using both automatic (Perplexity) and human (SSA -\nSensibleness and Specificity Average) evaluation metrics and establish\ncomparable performance while reducing model parameters by 90%.",
          "link": "http://arxiv.org/abs/2108.06329",
          "publishedOn": "2021-08-16T00:47:32.703Z",
          "wordCount": 601,
          "title": "Low-Resource Adaptation of Open-Domain Generative Chatbots. (arXiv:2108.06329v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Real world traffic sign recognition is an important step towards building\nautonomous vehicles, most of which highly dependent on Deep Neural Networks\n(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to\nadversarial examples. Many attack methods have been proposed to understand and\ngenerate adversarial examples, such as gradient based attack, score based\nattack, decision based attack, and transfer based attacks. However, most of\nthese algorithms are ineffective in real-world road sign attack, because (1)\niteratively learning perturbations for each frame is not realistic for a fast\nmoving car and (2) most optimization algorithms traverse all pixels equally\nwithout considering their diverse contribution. To alleviate these problems,\nthis paper proposes the targeted attention attack (TAA) method for real world\nroad sign attack. Specifically, we have made the following contributions: (1)\nwe leverage the soft attention map to highlight those important pixels and skip\nthose zero-contributed areas - this also helps to generate natural\nperturbations, (2) we design an efficient universal attack that optimizes a\nsingle perturbation/noise based on a set of training images under the guidance\nof the pre-trained attention map, (3) we design a simple objective function\nthat can be easily optimized, (4) we evaluate the effectiveness of TAA on real\nworld data sets. Experimental results validate that the TAA method improves the\nattack successful rate (nearly 10%) and reduces the perturbation loss (about a\nquarter) compared with the popular RP2 method. Additionally, our TAA also\nprovides good properties, e.g., transferability and generalization capability.\nWe provide code and data to ensure the reproducibility:\nhttps://github.com/AdvAttack/RoadSignAttack.",
          "link": "http://arxiv.org/abs/2010.04331",
          "publishedOn": "2021-08-16T00:47:32.693Z",
          "wordCount": 756,
          "title": "Targeted Physical-World Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12698",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Moreno_E/0/1/0/all/0/1\">Eric A. Moreno</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Vlimant_J/0/1/0/all/0/1\">Jean-Roch Vlimant</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Spiropulu_M/0/1/0/all/0/1\">Maria Spiropulu</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Borzyszkowski_B/0/1/0/all/0/1\">Bartlomiej Borzyszkowski</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>",
          "description": "We present an application of anomaly detection techniques based on deep\nrecurrent autoencoders to the problem of detecting gravitational wave signals\nin laser interferometers. Trained on noise data, this class of algorithms could\ndetect signals using an unsupervised strategy, i.e., without targeting a\nspecific kind of source. We develop a custom architecture to analyze the data\nfrom two interferometers. We compare the obtained performance to that obtained\nwith other autoencoder architectures and with a convolutional classifier. The\nunsupervised nature of the proposed strategy comes with a cost in terms of\naccuracy, when compared to more traditional supervised techniques. On the other\nhand, there is a qualitative gain in generalizing the experimental sensitivity\nbeyond the ensemble of pre-computed signal templates. The recurrent autoencoder\noutperforms other autoencoders based on different architectures. The class of\nrecurrent autoencoders presented in this paper could complement the search\nstrategy employed for gravitational wave detection and extend the reach of the\nongoing detection campaigns.",
          "link": "http://arxiv.org/abs/2107.12698",
          "publishedOn": "2021-08-16T00:47:32.675Z",
          "wordCount": 638,
          "title": "Source-Agnostic Gravitational-Wave Detection with Recurrent Autoencoders. (arXiv:2107.12698v2 [gr-qc] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1\">Hrishav Bakul Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mg_T/0/1/0/all/0/1\">Theint Haythi Mg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_P/0/1/0/all/0/1\">Pradip Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_C/0/1/0/all/0/1\">Chayan Sarkar</a>",
          "description": "Robots in our daily surroundings are increasing day by day. Their usability\nand acceptability largely depend on their explicit and implicit interaction\ncapability with fellow human beings. As a result, social behavior is one of the\nmost sought-after qualities that a robot can possess. However, there is no\nspecific aspect and/or feature that defines socially acceptable behavior and it\nlargely depends on the situation, application, and society. In this article, we\ninvestigate one such social behavior for collocated robots. Imagine a group of\npeople is interacting with each other and we want to join the group. We as\nhuman beings do it in a socially acceptable manner, i.e., within the group, we\ndo position ourselves in such a way that we can participate in the group\nactivity without disturbing/obstructing anybody. To possess such a quality,\nfirst, a robot needs to determine the formation of the group and then determine\na position for itself, which we humans do implicitly. The theory of f-formation\ncan be utilized for this purpose. As the types of formations can be very\ndiverse, detecting the social groups is not a trivial task. In this article, we\nprovide a comprehensive survey of the existing work on social interaction and\ngroup detection using f-formation for robotics and other applications. We also\nput forward a novel holistic survey framework combining all the possible\nconcerns and modules relevant to this problem. We define taxonomies based on\nmethods, camera views, datasets, detection capabilities and scale, evaluation\napproaches, and application areas. We discuss certain open challenges and\nlimitations in current literature along with possible future research\ndirections based on this framework. In particular, we discuss the existing\nmethods/techniques and their relative merits and demerits, applications, and\nprovide a set of unsolved but relevant problems in this domain.",
          "link": "http://arxiv.org/abs/2108.06181",
          "publishedOn": "2021-08-16T00:47:32.668Z",
          "wordCount": 790,
          "title": "Detecting socially interacting groups using f-formation: A survey of taxonomy, methods, datasets, applications, challenges, and future research directions. (arXiv:2108.06181v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08871",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Haan_K/0/1/0/all/0/1\">Kevin de Haan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yijie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuckerman_J/0/1/0/all/0/1\">Jonathan E. Zuckerman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisk_A/0/1/0/all/0/1\">Anthony E. Sisk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_M/0/1/0/all/0/1\">Miguel F. P. Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jen_K/0/1/0/all/0/1\">Kuang-Yu Jen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nobori_A/0/1/0/all/0/1\">Alexander Nobori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liou_S/0/1/0/all/0/1\">Sofia Liou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Sarah Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riahi_R/0/1/0/all/0/1\">Rana Riahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivenson_Y/0/1/0/all/0/1\">Yair Rivenson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wallace_W/0/1/0/all/0/1\">W. Dean Wallace</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>",
          "description": "Pathology is practiced by visual inspection of histochemically stained\nslides. Most commonly, the hematoxylin and eosin (H&E) stain is used in the\ndiagnostic workflow and it is the gold standard for cancer diagnosis. However,\nin many cases, especially for non-neoplastic diseases, additional \"special\nstains\" are used to provide different levels of contrast and color to tissue\ncomponents and allow pathologists to get a clearer diagnostic picture. In this\nstudy, we demonstrate the utility of supervised learning-based computational\nstain transformation from H&E to different special stains (Masson's Trichrome,\nperiodic acid-Schiff and Jones silver stain) using tissue sections from kidney\nneedle core biopsies. Based on evaluation by three renal pathologists, followed\nby adjudication by a fourth renal pathologist, we show that the generation of\nvirtual special stains from existing H&E images improves the diagnosis in\nseveral non-neoplastic kidney diseases sampled from 58 unique subjects. A\nsecond study performed by three pathologists found that the quality of the\nspecial stains generated by the stain transformation network was statistically\nequivalent to those generated through standard histochemical staining. As the\ntransformation of H&E images into special stains can be achieved within 1 min\nor less per patient core specimen slide, this stain-to-stain transformation\nframework can improve the quality of the preliminary diagnosis when additional\nspecial stains are needed, along with significant savings in time and cost,\nreducing the burden on healthcare system and patients.",
          "link": "http://arxiv.org/abs/2008.08871",
          "publishedOn": "2021-08-16T00:47:32.660Z",
          "wordCount": 763,
          "title": "Deep learning-based transformation of the H&E stain into special stains. (arXiv:2008.08871v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asikis_T/0/1/0/all/0/1\">Thomas Asikis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bottcher_L/0/1/0/all/0/1\">Lucas B&#xf6;ttcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antulov_Fantulin_N/0/1/0/all/0/1\">Nino Antulov-Fantulin</a>",
          "description": "We study the ability of neural networks to steer or control trajectories of\ncontinuous time non-linear dynamical systems on graphs, which we represent with\nneural ordinary differential equations (neural ODEs). To do so, we introduce a\nneural-ODE control (NODEC) framework and find that it can learn control signals\nthat drive graph dynamical systems into desired target states. While we use\nloss functions that do not constrain the control energy, our results show that\nNODEC produces low energy control signals. Finally, we showcase the performance\nand versatility of NODEC by using it to control a system of more than one\nthousand coupled, non-linear ODEs.",
          "link": "http://arxiv.org/abs/2006.09773",
          "publishedOn": "2021-08-16T00:47:32.648Z",
          "wordCount": 619,
          "title": "Neural Ordinary Differential Equation Control of Dynamics on Graphs. (arXiv:2006.09773v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao-Jung Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulicny_M/0/1/0/all/0/1\">Matej Ulicny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzke_M/0/1/0/all/0/1\">Michael Manzke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahyot_R/0/1/0/all/0/1\">Rozenn Dahyot</a>",
          "description": "Localization of street objects from images has gained a lot of attention in\nrecent years. We propose an approach to improve asset geolocation from street\nview imagery by enhancing the quality of the metadata associated with the\nimages using Structure from Motion. The predicted object geolocation is further\nrefined by imposing contextual geographic information extracted from\nOpenStreetMap. Our pipeline is validated experimentally against the state of\nthe art approaches for geotagging traffic lights.",
          "link": "http://arxiv.org/abs/2108.06302",
          "publishedOn": "2021-08-16T00:47:32.641Z",
          "wordCount": 501,
          "title": "Context Aware Object Geotagging. (arXiv:2108.06302v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhongyi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haoliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>",
          "description": "Unsupervised domain adaptation (UDA) enables a learning machine to adapt from\na labeled source domain to an unlabeled domain under the distribution shift.\nThanks to the strong representation ability of deep neural networks, recent\nremarkable achievements in UDA resort to learning domain-invariant features.\nIntuitively, the hope is that a good feature representation, together with the\nhypothesis learned from the source domain, can generalize well to the target\ndomain. However, the learning processes of domain-invariant features and source\nhypothesis inevitably involve domain-specific information that would degrade\nthe generalizability of UDA models on the target domain. In this paper,\nmotivated by the lottery ticket hypothesis that only partial parameters are\nessential for generalization, we find that only partial parameters are\nessential for learning domain-invariant information and generalizing well in\nUDA. Such parameters are termed transferable parameters. In contrast, the other\nparameters tend to fit domain-specific details and often fail to generalize,\nwhich we term as untransferable parameters. Driven by this insight, we propose\nTransferable Parameter Learning (TransPar) to reduce the side effect brought by\ndomain-specific information in the learning process and thus enhance the\nmemorization of domain-invariant information. Specifically, according to the\ndistribution discrepancy degree, we divide all parameters into transferable and\nuntransferable ones in each training iteration. We then perform separate\nupdates rules for the two types of parameters. Extensive experiments on image\nclassification and regression tasks (keypoint detection) show that TransPar\noutperforms prior arts by non-trivial margins. Moreover, experiments\ndemonstrate that TransPar can be integrated into the most popular deep UDA\nnetworks and be easily extended to handle any data distribution shift\nscenarios.",
          "link": "http://arxiv.org/abs/2108.06129",
          "publishedOn": "2021-08-16T00:47:32.636Z",
          "wordCount": 703,
          "title": "Learning Transferable Parameters for Unsupervised Domain Adaptation. (arXiv:2108.06129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ronge_R/0/1/0/all/0/1\">Raphael Ronge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nho_K/0/1/0/all/0/1\">Kwangsik Nho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>",
          "description": "The current state-of-the-art deep neural networks (DNNs) for Alzheimer's\nDisease diagnosis use different biomarker combinations to classify patients,\nbut do not allow extracting knowledge about the interactions of biomarkers.\nHowever, to improve our understanding of the disease, it is paramount to\nextract such knowledge from the learned model. In this paper, we propose a Deep\nFactorization Machine model that combines the ability of DNNs to learn complex\nrelationships and the ease of interpretability of a linear model. The proposed\nmodel has three parts: (i) an embedding layer to deal with sparse categorical\ndata, (ii) a Factorization Machine to efficiently learn pairwise interactions,\nand (iii) a DNN to implicitly model higher order interactions. In our\nexperiments on data from the Alzheimer's Disease Neuroimaging Initiative, we\ndemonstrate that our proposed model classifies cognitive normal, mild cognitive\nimpaired, and demented patients more accurately than competing models. In\naddition, we show that valuable knowledge about the interactions among\nbiomarkers can be obtained.",
          "link": "http://arxiv.org/abs/2108.05916",
          "publishedOn": "2021-08-16T00:47:32.620Z",
          "wordCount": 608,
          "title": "Alzheimer's Disease Diagnosis via Deep Factorization Machine Models. (arXiv:2108.05916v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06264",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bienkiewicz_M/0/1/0/all/0/1\">M. M. N. Bie&#x144;kiewicz</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Smykovskyi_A/0/1/0/all/0/1\">A. Smykovskyi</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Olugbade_T/0/1/0/all/0/1\">T. Olugbade</a> (2), <a href=\"http://arxiv.org/find/q-bio/1/au:+Janaqi_S/0/1/0/all/0/1\">S. Janaqi</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Camurri_A/0/1/0/all/0/1\">A. Camurri</a> (3), <a href=\"http://arxiv.org/find/q-bio/1/au:+Bianchi_Berthouze_N/0/1/0/all/0/1\">N. Bianchi-Berthouze</a> (2), <a href=\"http://arxiv.org/find/q-bio/1/au:+Bjorkman_M/0/1/0/all/0/1\">M. Bj&#xf6;rkman</a> (4), <a href=\"http://arxiv.org/find/q-bio/1/au:+Bardy_B/0/1/0/all/0/1\">B. G. Bardy</a> (1) ((1) EuroMov Digital Health in Motion Univ. Montpellier IMT Mines Ales France, (2) UCL, University College of London UK, (3) UNIGE InfoMus Casa Paganini Italy, (4) KTH Royal Institute of Technology Sweden)",
          "description": "Our daily human life is filled with a myriad of joint action moments, be it\nchildren playing, adults working together (i.e., team sports), or strangers\nnavigating through a crowd. Joint action brings individuals (and embodiment of\ntheir emotions) together, in space and in time. Yet little is known about how\nindividual emotions propagate through embodied presence in a group, and how\njoint action changes individual emotion. In fact, the multi-agent component is\nlargely missing from neuroscience-based approaches to emotion, and reversely\njoint action research has not found a way yet to include emotion as one of the\nkey parameters to model socio-motor interaction. In this review, we first\nidentify the gap and then stockpile evidence showing strong entanglement\nbetween emotion and acting together from various branches of sciences. We\npropose an integrative approach to bridge the gap, highlight five research\navenues to do so in behavioral neuroscience and digital sciences, and address\nsome of the key challenges in the area faced by modern societies.",
          "link": "http://arxiv.org/abs/2108.06264",
          "publishedOn": "2021-08-16T00:47:32.610Z",
          "wordCount": 677,
          "title": "Bridging the gap between emotion and joint action. (arXiv:2108.06264v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paudel_A/0/1/0/all/0/1\">Abhishek Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakal_R/0/1/0/all/0/1\">Roshan Dhakal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_S/0/1/0/all/0/1\">Sakshat Bhattarai</a>",
          "description": "We present our approach to improve room classification task on floor plan\nmaps of buildings by representing floor plans as undirected graphs and\nleveraging graph neural networks to predict the room categories. Rooms in the\nfloor plans are represented as nodes in the graph with edges representing their\nadjacency in the map. We experiment with House-GAN dataset that consists of\nfloor plan maps in vector format and train multilayer perceptron and graph\nneural networks. Our results show that graph neural networks, specifically\nGraphSAGE and Topology Adaptive GCN were able to achieve accuracy of 80% and\n81% respectively outperforming baseline multilayer perceptron by more than 15%\nmargin.",
          "link": "http://arxiv.org/abs/2108.05947",
          "publishedOn": "2021-08-16T00:47:32.604Z",
          "wordCount": 542,
          "title": "Room Classification on Floor Plan Graphs using Graph Neural Networks. (arXiv:2108.05947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1906.04591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Frances J. Ferri</a>",
          "description": "Acoustic scene classification (ASC) has been approached in the last years\nusing deep learning techniques such as convolutional neural networks or\nrecurrent neural networks. Many state-of-the-art solutions are based on image\nclassification frameworks and, as such, a 2D representation of the audio signal\nis considered for training these networks. Finding the most suitable audio\nrepresentation is still a research area of interest. In this paper, different\nlog-Mel representations and combinations are analyzed. Experiments show that\nthe best results are obtained using the harmonic and percussive components plus\nthe difference between left and right stereo channels, (L-R). On the other\nhand, it is a common strategy to ensemble different models in order to increase\nthe final accuracy. Even though averaging different model predictions is a\ncommon choice, an exhaustive analysis of different ensemble techniques has not\nbeen presented in ASC problems. In this paper, geometric and arithmetic mean\nplus the Ordered Weighted Averaging (OWA) operator are studied as aggregation\noperators for the output of the different models of the ensemble. Finally, the\nwork carried out in this paper is highly oriented towards real-time\nimplementations. In this context, as the number of applications for audio\nclassification on edge devices is increasing exponentially, we also analyze\ndifferent network depths and efficient solutions for aggregating ensemble\npredictions.",
          "link": "http://arxiv.org/abs/1906.04591",
          "publishedOn": "2021-08-16T00:47:32.598Z",
          "wordCount": 716,
          "title": "CNN depth analysis with different channel inputs for Acoustic Scene Classification. (arXiv:1906.04591v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>",
          "description": "Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, most existing\nlearning-based approaches usually suffer from limited manually annotated\nmedical data, which poses a major practical problem for accurate and robust\nmedical image segmentation. In addition, most existing semi-supervised\napproaches are usually not robust compared with the supervised counterparts,\nand also lack explicit modeling of geometric structure and semantic\ninformation, both of which limit the segmentation accuracy. In this work, we\npresent SimCVD, a simple contrastive distillation framework that significantly\nadvances state-of-the-art voxel-wise representation learning. We first describe\nan unsupervised training strategy, which takes two views of an input volume and\npredicts their signed distance maps of object boundaries in a contrastive\nobjective, with only two independent dropout as mask. This simple approach\nworks surprisingly well, performing on the same level as previous fully\nsupervised methods with much less labeled data. We hypothesize that dropout can\nbe viewed as a minimal form of data augmentation and makes the network robust\nto representation collapse. Then, we propose to perform structural distillation\nby distilling pair-wise similarities. We evaluate SimCVD on two popular\ndatasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\ndataset. The results on the LA dataset demonstrate that, in two types of\nlabeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\nprevious best results. Our method can be trained in an end-to-end fashion,\nshowing the promise of utilizing SimCVD as a general framework for downstream\ntasks, such as medical image synthesis and registration.",
          "link": "http://arxiv.org/abs/2108.06227",
          "publishedOn": "2021-08-16T00:47:32.582Z",
          "wordCount": 724,
          "title": "SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_D/0/1/0/all/0/1\">Daoming Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Hugh Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_L/0/1/0/all/0/1\">Levent Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>",
          "description": "Human-robot interactive decision-making is increasingly becoming ubiquitous,\nand trust is an influential factor in determining the reliance on autonomy.\nHowever, it is not reasonable to trust systems that are beyond our\ncomprehension, and typical machine learning and data-driven decision-making are\nblack-box paradigms that impede interpretability. Therefore, it is critical to\nestablish computational trustworthy decision-making mechanisms enhanced by\ninterpretability-aware strategies. To this end, we propose a Trustworthy\nDecision-Making (TDM) framework, which integrates symbolic planning into\nsequential decision-making. The framework learns interpretable subtasks that\nresult in a complex, higher-level composite task that can be formally evaluated\nusing the proposed trust metric. TDM enables the subtask-level interpretability\nby design and converges to an optimal symbolic plan from the learned subtasks.\nMoreover, a TDM-based algorithm is introduced to demonstrate the unification of\nsymbolic planning with other sequential-decision making algorithms, reaping the\nbenefits of both. Experimental results validate the effectiveness of\ntrust-score-based planning while improving the interpretability of subtasks.",
          "link": "http://arxiv.org/abs/2108.06080",
          "publishedOn": "2021-08-16T00:47:32.576Z",
          "wordCount": 599,
          "title": "TDM: Trustworthy Decision-Making via Interpretability Enhancement. (arXiv:2108.06080v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.11855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>",
          "description": "The transformer self-attention network has been extensively used in research\ndomains such as computer vision, image processing, and natural language\nprocessing. The transformer, however, has not been actively used in graph\nneural networks, where constructing an advanced aggregation function is\nessential. To this end, we present an effective model, named UGformer, which --\nby leveraging a transformer self-attention mechanism followed by a recurrent\ntransition -- induces an advanced aggregation function to learn graph\nrepresentations. Experimental results show that UGformer achieves\nstate-of-the-art accuracies on well-known benchmark datasets for graph\nclassification.",
          "link": "http://arxiv.org/abs/1909.11855",
          "publishedOn": "2021-08-16T00:47:32.549Z",
          "wordCount": 643,
          "title": "Universal Graph Transformer Self-Attention Networks. (arXiv:1909.11855v10 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_M/0/1/0/all/0/1\">Mohit Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1\">Daniel Sheldon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>",
          "description": "A key challenge in scaling Gaussian Process (GP) regression to massive\ndatasets is that exact inference requires computation with a dense n x n kernel\nmatrix, where n is the number of data points. Significant work focuses on\napproximating the kernel matrix via interpolation using a smaller set of m\ninducing points. Structured kernel interpolation (SKI) is among the most\nscalable methods: by placing inducing points on a dense grid and using\nstructured matrix algebra, SKI achieves per-iteration time of O(n + m log m)\nfor approximate inference. This linear scaling in n enables inference for very\nlarge data sets; however the cost is per-iteration, which remains a limitation\nfor extremely large n. We show that the SKI per-iteration time can be reduced\nto O(m log m) after a single O(n) time precomputation step by reframing SKI as\nsolving a natural Bayesian linear regression problem with a fixed set of m\ncompact basis functions. With per-iteration complexity independent of the\ndataset size n for a fixed grid, our method scales to truly massive data sets.\nWe demonstrate speedups in practice for a wide range of m and n and apply the\nmethod to GP inference on a three-dimensional weather radar dataset with over\n100 million points.",
          "link": "http://arxiv.org/abs/2101.11751",
          "publishedOn": "2021-08-16T00:47:32.535Z",
          "wordCount": 670,
          "title": "Faster Kernel Interpolation for Gaussian Processes. (arXiv:2101.11751v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05385",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ward_O/0/1/0/all/0/1\">Owen G. Ward</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Davison_A/0/1/0/all/0/1\">Andrew Davison</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zheng_T/0/1/0/all/0/1\">Tian Zheng</a>",
          "description": "Embedding nodes of a large network into a metric (e.g., Euclidean) space has\nbecome an area of active research in statistical machine learning, which has\nfound applications in natural and social sciences. Generally, a representation\nof a network object is learned in a Euclidean geometry and is then used for\nsubsequent tasks regarding the nodes and/or edges of the network, such as\ncommunity detection, node classification and link prediction. Network embedding\nalgorithms have been proposed in multiple disciplines, often with\ndomain-specific notations and details. In addition, different measures and\ntools have been adopted to evaluate and compare the methods proposed under\ndifferent settings, often dependent of the downstream tasks. As a result, it is\nchallenging to study these algorithms in the literature systematically.\nMotivated by the recently proposed Veridical Data Science (VDS) framework, we\npropose a framework for network embedding algorithms and discuss how the\nprinciples of predictability, computability and stability apply in this\ncontext. The utilization of this framework in network embedding holds the\npotential to motivate and point to new directions for future research.",
          "link": "http://arxiv.org/abs/2007.05385",
          "publishedOn": "2021-08-16T00:47:32.530Z",
          "wordCount": 631,
          "title": "Next Waves in Veridical Network Embedding. (arXiv:2007.05385v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qibiao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Recent studies have shown that Graph Convolutional Networks (GCNs) are\nvulnerable to adversarial attacks on the graph structure. Although multiple\nworks have been proposed to improve their robustness against such structural\nadversarial attacks, the reasons for the success of the attacks remain unclear.\nIn this work, we theoretically and empirically demonstrate that structural\nadversarial examples can be attributed to the non-robust aggregation scheme\n(i.e., the weighted mean) of GCNs. Specifically, our analysis takes advantage\nof the breakdown point which can quantitatively measure the robustness of\naggregation schemes. The key insight is that weighted mean, as the basic design\nof GCNs, has a low breakdown point and its output can be dramatically changed\nby injecting a single edge. We show that adopting the aggregation scheme with a\nhigh breakdown point (e.g., median or trimmed mean) could significantly enhance\nthe robustness of GCNs against structural attacks. Extensive experiments on\nfour real-world datasets demonstrate that such a simple but effective method\nachieves the best robustness performance compared to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.06280",
          "publishedOn": "2021-08-16T00:47:32.511Z",
          "wordCount": 608,
          "title": "Understanding Structural Vulnerability in Graph Convolutional Networks. (arXiv:2108.06280v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.07221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>",
          "description": "Weakly-supervised learning (WSL) has recently triggered substantial interest\nas it mitigates the lack of pixel-wise annotations.\n\nGiven global image labels, WSL methods yield pixel-level predictions\n(segmentations), which enable to interpret class predictions. Despite their\nrecent success, mostly with natural images, such methods can face important\nchallenges when the foreground and background regions have similar visual cues,\nyielding high false-positive rates in segmentations, as is the case in\nchallenging histology images. WSL training is commonly driven by standard\nclassification losses, which implicitly maximize model confidence, and locate\nthe discriminative regions linked to classification decisions. Therefore, they\nlack mechanisms for modeling explicitly non-discriminative regions and reducing\nfalse-positive rates. We propose novel regularization terms, which enable the\nmodel to seek both non-discriminative and discriminative regions, while\ndiscouraging unbalanced segmentations. We introduce high uncertainty as a\ncriterion to localize non-discriminative regions that do not affect classifier\ndecision, and describe it with original Kullback-Leibler (KL) divergence losses\nevaluating the deviation of posterior predictions from the uniform\ndistribution. Our KL terms encourage high uncertainty of the model when the\nlatter inputs the latent non-discriminative regions. Our loss integrates: (i) a\ncross-entropy seeking a foreground, where model confidence about class\nprediction is high; (ii) a KL regularizer seeking a background, where model\nuncertainty is high; and (iii) log-barrier terms discouraging unbalanced\nsegmentations. Comprehensive experiments and ablation studies over the public\nGlaS colon cancer data and a Camelyon16 patch-based benchmark for breast cancer\nshow substantial improvements over state-of-the-art WSL methods, and confirm\nthe effect of our new regularizers.",
          "link": "http://arxiv.org/abs/2011.07221",
          "publishedOn": "2021-08-16T00:47:32.501Z",
          "wordCount": 741,
          "title": "Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty. (arXiv:2011.07221v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.04872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hufei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chenghao Wei</a>",
          "description": "To accelerate the existing Broad Learning System (BLS) for new added nodes in\n[7], we extend the inverse Cholesky factorization in [10] to deduce an\nefficient inverse Cholesky factorization for a Hermitian matrix partitioned\ninto 2 * 2 blocks, which is utilized to develop the proposed BLS algorithm 1.\nThe proposed BLS algorithm 1 compute the ridge solution (i.e, the output\nweights) from the inverse Cholesky factor of the Hermitian matrix in the ridge\ninverse, and update the inverse Cholesky factor efficiently. From the proposed\nBLS algorithm 1, we deduce the proposed ridge inverse, which can be obtained\nfrom the generalized inverse in [7] by just change one matrix in the equation\nto compute the newly added sub-matrix. We also modify the proposed algorithm 1\ninto the proposed algorithm 2, which is equivalent to the existing BLS\nalgorithm [7] in terms of numerical computations. The proposed algorithms 1 and\n2 can reduce the computational complexity, since usually the Hermitian matrix\nin the ridge inverse is smaller than the ridge inverse. With respect to the\nexisting BLS algorithm, the proposed algorithms 1 and 2 usually require about\n13 and 2 3 of complexities, respectively, while in numerical experiments they\nachieve the speedups (in each additional training time) of 2.40 - 2.91 and 1.36\n- 1.60, respectively. Numerical experiments also show that the proposed\nalgorithm 1 and the standard ridge solution always bear the same testing\naccuracy, and usually so do the proposed algorithm 2 and the existing BLS\nalgorithm. The existing BLS assumes the ridge parameter lamda->0, since it is\nbased on the generalized inverse with the ridge regression approximation. When\nthe assumption of lamda-> 0 is not satisfied, the standard ridge solution\nobviously achieves a better testing accuracy than the existing BLS algorithm in\nnumerical experiments.",
          "link": "http://arxiv.org/abs/1911.04872",
          "publishedOn": "2021-08-16T00:47:32.494Z",
          "wordCount": 785,
          "title": "Efficient Ridge Solution for the Incremental Broad Learning System on Added Nodes by Inverse Cholesky Factorization of a Partitioned Matrix. (arXiv:1911.04872v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.14129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhuoyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xingzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1\">Rui Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee Keong Kwoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chi Xu</a>",
          "description": "Latent factor models play a dominant role among recommendation techniques.\nHowever, most of the existing latent factor models assume both historical\ninteractions and embedding dimensions are independent of each other, and thus\nregrettably ignore the high-order interaction information among historical\ninteractions and embedding dimensions. In this paper, we propose a novel latent\nfactor model called COMET (COnvolutional diMEnsion inTeraction), which\nsimultaneously model the high-order interaction patterns among historical\ninteractions and embedding dimensions. To be specific, COMET stacks the\nembeddings of historical interactions horizontally at first, which results in\ntwo \"embedding maps\". In this way, internal interactions and dimensional\ninteractions can be exploited by convolutional neural networks with kernels of\ndifferent sizes simultaneously. A fully-connected multi-layer perceptron is\nthen applied to obtain two interaction vectors. Lastly, the representations of\nusers and items are enriched by the learnt interaction vectors, which can\nfurther be used to produce the final prediction. Extensive experiments and\nablation studies on various public implicit feedback datasets clearly\ndemonstrate the effectiveness and the rationality of our proposed method.",
          "link": "http://arxiv.org/abs/2007.14129",
          "publishedOn": "2021-08-16T00:47:32.486Z",
          "wordCount": 646,
          "title": "COMET: Convolutional Dimension Interaction for Collaborative Filtering. (arXiv:2007.14129v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dohare_S/0/1/0/all/0/1\">Shibhansh Dohare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">A. Rupam Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "The Backprop algorithm for learning in neural networks utilizes two\nmechanisms: first, stochastic gradient descent and second, initialization with\nsmall random weights, where the latter is essential to the effectiveness of the\nformer. We show that in continual learning setups, Backprop performs well\ninitially, but over time its performance degrades. Stochastic gradient descent\nalone is insufficient to learn continually; the initial randomness enables only\ninitial learning but not continual learning. To the best of our knowledge, ours\nis the first result showing this degradation in Backprop's ability to learn. To\naddress this issue, we propose an algorithm that continually injects random\nfeatures alongside gradient descent using a new generate-and-test process. We\ncall this the Continual Backprop algorithm. We show that, unlike Backprop,\nContinual Backprop is able to continually adapt in both supervised and\nreinforcement learning problems. We expect that as continual learning becomes\nmore common in future applications, a method like Continual Backprop will be\nessential where the advantages of random initialization are present throughout\nlearning.",
          "link": "http://arxiv.org/abs/2108.06325",
          "publishedOn": "2021-08-16T00:47:32.444Z",
          "wordCount": 595,
          "title": "Continual Backprop: Stochastic Gradient Descent with Persistent Randomness. (arXiv:2108.06325v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1\">Avi Schwarzschild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgnia_E/0/1/0/all/0/1\">Eitan Borgnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arjun Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Arpit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emam_Z/0/1/0/all/0/1\">Zeyad Emam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "We describe new datasets for studying generalization from easy to hard\nexamples.",
          "link": "http://arxiv.org/abs/2108.06011",
          "publishedOn": "2021-08-16T00:47:32.439Z",
          "wordCount": 456,
          "title": "Datasets for Studying Generalization from Easy to Hard Examples. (arXiv:2108.06011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06094",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1\">Yang Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1\">Xuekui Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Esfahani_F/0/1/0/all/0/1\">Fatemeh Esfahani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Srinivasan_V/0/1/0/all/0/1\">Venkatesh Srinivasan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thomo_A/0/1/0/all/0/1\">Alex Thomo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xing_L/0/1/0/all/0/1\">Li Xing</a>",
          "description": "Mining dense subgraphs where vertices connect closely with each other is a\ncommon task when analyzing graphs. A very popular notion in subgraph analysis\nis core decomposition. Recently, Esfahani et al. presented a probabilistic core\ndecomposition algorithm based on graph peeling and Central Limit Theorem (CLT)\nthat is capable of handling very large graphs. Their proposed peeling algorithm\n(PA) starts from the lowest degree vertices and recursively deletes these\nvertices, assigning core numbers, and updating the degree of neighbour vertices\nuntil it reached the maximum core. However, in many applications, particularly\nin biology, more valuable information can be obtained from dense\nsub-communities and we are not interested in small cores where vertices do not\ninteract much with others. To make the previous PA focus more on dense\nsubgraphs, we propose a multi-stage graph peeling algorithm (M-PA) that has a\ntwo-stage data screening procedure added before the previous PA. After removing\nvertices from the graph based on the user-defined thresholds, we can reduce the\ngraph complexity largely and without affecting the vertices in subgraphs that\nwe are interested in. We show that M-PA is more efficient than the previous PA\nand with the properly set filtering threshold, can produce very similar if not\nidentical dense subgraphs to the previous PA (in terms of graph density and\nclustering coefficient).",
          "link": "http://arxiv.org/abs/2108.06094",
          "publishedOn": "2021-08-16T00:47:32.433Z",
          "wordCount": 655,
          "title": "Multi-Stage Graph Peeling Algorithm for Probabilistic Core Decomposition. (arXiv:2108.06094v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1906.05029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robberechts_P/0/1/0/all/0/1\">Pieter Robberechts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haaren_J/0/1/0/all/0/1\">Jan Van Haaren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jesse Davis</a>",
          "description": "In-game win probability models, which provide a sports team's likelihood of\nwinning at each point in a game based on historical observations, are becoming\nincreasingly popular. In baseball, basketball and American football, they have\nbecome important tools to enhance fan experience, to evaluate in-game\ndecision-making, and to inform coaching decisions. While equally relevant in\nsoccer, the adoption of these models is held back by technical challenges\narising from the low-scoring nature of the sport.\n\nIn this paper, we introduce an in-game win probability model for soccer that\naddresses the shortcomings of existing models. First, we demonstrate that\nin-game win probability models for other sports struggle to provide accurate\nestimates for soccer, especially towards the end of a game. Second, we\nintroduce a novel Bayesian statistical framework that estimates running win,\ntie and loss probabilities by leveraging a set of contextual game state\nfeatures. An empirical evaluation on eight seasons of data for the top-five\nsoccer leagues demonstrates that our framework provides well-calibrated\nprobabilities. Furthermore, two use cases show its ability to enhance fan\nexperience and to evaluate performance in crucial game situations.",
          "link": "http://arxiv.org/abs/1906.05029",
          "publishedOn": "2021-08-16T00:47:32.427Z",
          "wordCount": 667,
          "title": "A Bayesian Approach to In-Game Win Probability in Soccer. (arXiv:1906.05029v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stolfi_P/0/1/0/all/0/1\">Paola Stolfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastropietro_A/0/1/0/all/0/1\">Andrea Mastropietro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasculli_G/0/1/0/all/0/1\">Giuseppe Pasculli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tieri_P/0/1/0/all/0/1\">Paolo Tieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vergni_D/0/1/0/all/0/1\">Davide Vergni</a>",
          "description": "Positive-Unlabelled (PU) learning is the machine learning setting in which\nonly a set of positive instances are labelled, while the rest of the data set\nis unlabelled. The unlabelled instances may be either unspecified positive\nsamples or true negative samples. Over the years, many solutions have been\nproposed to deal with PU learning. Some techniques consider the unlabelled\nsamples as negative ones, reducing the problem to a binary classification with\na noisy negative set, while others aim to detect sets of possible negative\nexamples to later apply a supervised machine learning strategy (two-step\ntechniques). The approach proposed in this work falls in the latter category\nand works in a semi-supervised fashion: motivated and inspired by previous\nworks, a Markov diffusion process with restart is used to assign pseudo-labels\nto unlabelled instances. Afterward, a machine learning model, exploiting the\nnewly assigned classes, is trained. The principal aim of the algorithm is to\nidentify a set of instances which are likely to contain positive instances that\nwere originally unlabelled.",
          "link": "http://arxiv.org/abs/2108.06158",
          "publishedOn": "2021-08-16T00:47:32.420Z",
          "wordCount": 598,
          "title": "Adaptive Positive-Unlabelled Learning via Markov Diffusion. (arXiv:2108.06158v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Floryan_D/0/1/0/all/0/1\">Daniel Floryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_M/0/1/0/all/0/1\">Michael D. Graham</a>",
          "description": "We introduce a method for learning minimal-dimensional dynamical models from\nhigh-dimensional time series data that lie on a low-dimensional manifold, as\narises for many processes. For an arbitrary manifold, there is no smooth global\ncoordinate representation, so following the formalism of differential topology\nwe represent the manifold as an atlas of charts. We first partition the data\ninto overlapping regions. Then undercomplete autoencoders are used to find\nlow-dimensional coordinate representations for each region. We then use the\ndata to learn dynamical models in each region, which together yield a global\nlow-dimensional dynamical model. We apply this method to examples ranging from\nsimple periodic dynamics to complex, nominally high-dimensional non-periodic\nbursting dynamics of the Kuramoto-Sivashinsky equation. We demonstrate that it:\n(1) can yield dynamical models of the lowest possible dimension, where previous\nmethods generally cannot; (2) exhibits computational benefits including\nscalability, parallelizability, and adaptivity; and (3) separates state space\ninto regions of distinct behaviours.",
          "link": "http://arxiv.org/abs/2108.05928",
          "publishedOn": "2021-08-16T00:47:32.411Z",
          "wordCount": 595,
          "title": "Charts and atlases for nonlinear data-driven models of dynamics on manifolds. (arXiv:2108.05928v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadri_N/0/1/0/all/0/1\">Nima Sadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bihan Liu</a>",
          "description": "Creating abstractive summaries from meeting transcripts has proven to be\nchallenging due to the limited amount of labeled data available for training\nneural network models. Moreover, Transformer-based architectures have proven to\nbeat state-of-the-art models in summarizing news data. In this paper, we\nutilize a Transformer-based Pointer Generator Network to generate abstract\nsummaries for meeting transcripts. This model uses 2 LSTMs as an encoder and a\ndecoder, a Pointer network which copies words from the inputted text, and a\nGenerator network to produce out-of-vocabulary words (hence making the summary\nabstractive). Moreover, a coverage mechanism is used to avoid repetition of\nwords in the generated summary. First, we show that training the model on a\nnews summary dataset and using zero-shot learning to test it on the meeting\ndataset proves to produce better results than training it on the AMI meeting\ndataset. Second, we show that training this model first on out-of-domain data,\nsuch as the CNN-Dailymail dataset, followed by a fine-tuning stage on the AMI\nmeeting dataset is able to improve the performance of the model significantly.\nWe test our model on a testing set from the AMI dataset and report the ROUGE-2\nscore of the generated summary to compare with previous literature. We also\nreport the Factual score of our summaries since it is a better benchmark for\nabstractive summaries since the ROUGE-2 score is limited to measuring\nword-overlaps. We show that our improved model is able to improve on previous\nmodels by at least 5 ROUGE-2 scores, which is a substantial improvement. Also,\na qualitative analysis of the summaries generated by our model shows that these\nsummaries and human-readable and indeed capture most of the important\ninformation from the transcripts.",
          "link": "http://arxiv.org/abs/2108.06310",
          "publishedOn": "2021-08-16T00:47:32.363Z",
          "wordCount": 712,
          "title": "MeetSum: Transforming Meeting Transcript Summarization using Transformers!. (arXiv:2108.06310v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feiyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Min Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dapeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "The delayed feedback problem is one of the imperative challenges in online\nadvertising, which is caused by the highly diversified feedback delay of a\nconversion varying from a few minutes to several days. It is hard to design an\nappropriate online learning system under these non-identical delay for\ndifferent types of ads and users. In this paper, we propose to tackle the\ndelayed feedback problem in online advertising by \"Following the Prophet\" (FTP\nfor short). The key insight is that, if the feedback came instantly for all the\nlogged samples, we could get a model without delayed feedback, namely the\n\"prophet\". Although the prophet cannot be obtained during online learning, we\nshow that we could predict the prophet's predictions by an aggregation policy\non top of a set of multi-task predictions, where each task captures the\nfeedback patterns of different periods. We propose the objective and\noptimization approach for the policy, and use the logged data to imitate the\nprophet. Extensive experiments on three real-world advertising datasets show\nthat our method outperforms the previous state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2108.06167",
          "publishedOn": "2021-08-16T00:47:32.350Z",
          "wordCount": 664,
          "title": "Follow the Prophet: Accurate Online Conversion Rate Prediction in the Face of Delayed Feedback. (arXiv:2108.06167v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">James Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Min Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Hanqi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingquan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherubin_L/0/1/0/all/0/1\">Laurent Ch&#xe9;rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanZwieten_J/0/1/0/all/0/1\">James VanZwieten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yufei Tang</a>",
          "description": "Ocean current, fluid mechanics, and many other spatio-temporal physical\ndynamical systems are essential components of the universe. One key\ncharacteristic of such systems is that certain physics laws -- represented as\nordinary/partial differential equations (ODEs/PDEs) -- largely dominate the\nwhole process, irrespective of time or location. Physics-informed learning has\nrecently emerged to learn physics for accurate prediction, but they often lack\na mechanism to leverage localized spatial and temporal correlation or rely on\nhard-coded physics parameters. In this paper, we advocate a physics-coupled\nneural network model to learn parameters governing the physics of the system,\nand further couple the learned physics to assist the learning of recurring\ndynamics. A spatio-temporal physics-coupled neural network (ST-PCNN) model is\nproposed to achieve three goals: (1) learning the underlying physics\nparameters, (2) transition of local information between spatio-temporal\nregions, and (3) forecasting future values for the dynamical system. The\nphysics-coupled learning ensures that the proposed model can be tremendously\nimproved by using learned physics parameters, and can achieve good long-range\nforecasting (e.g., more than 30-steps). Experiments, using simulated and\nfield-collected ocean current data, validate that ST-PCNN outperforms existing\nphysics-informed models.",
          "link": "http://arxiv.org/abs/2108.05940",
          "publishedOn": "2021-08-16T00:47:32.323Z",
          "wordCount": 624,
          "title": "ST-PCNN: Spatio-Temporal Physics-Coupled Neural Networks for Dynamics Forecasting. (arXiv:2108.05940v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davydov_V/0/1/0/all/0/1\">Vasilii Davydov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1\">Konstantin Yakovlev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr I. Panov</a>",
          "description": "In this paper, we consider the problem of multi-agent navigation in partially\nobservable grid environments. This problem is challenging for centralized\nplanning approaches as they, typically, rely on the full knowledge of the\nenvironment. We suggest utilizing the reinforcement learning approach when the\nagents, first, learn the policies that map observations to actions and then\nfollow these policies to reach their goals. To tackle the challenge associated\nwith learning cooperative behavior, i.e. in many cases agents need to yield to\neach other to accomplish a mission, we use a mixing Q-network that complements\nlearning individual policies. In the experimental evaluation, we show that such\napproach leads to plausible results and scales well to large number of agents.",
          "link": "http://arxiv.org/abs/2108.06148",
          "publishedOn": "2021-08-16T00:47:32.300Z",
          "wordCount": 575,
          "title": "Q-Mixing Network for Multi-Agent Pathfinding in Partially Observable Grid Environments. (arXiv:2108.06148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1\">Amir Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novin_R/0/1/0/all/0/1\">Roya Sabbagh Novin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merryweather_A/0/1/0/all/0/1\">Andrew Merryweather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1\">Tucker Hermans</a>",
          "description": "Ergonomics and human comfort are essential concerns in physical human-robot\ninteraction applications, and common practical methods either fail in\nestimating the correct posture due to occlusion or suffer from less accurate\nergonomics models in their postural optimization methods. Instead, we propose a\nnovel framework for posture estimation, assessment, and optimization for\nergonomically intelligent physical human-robot interaction. We show that we can\nestimate human posture solely from the trajectory of the interacting robot. We\npropose DULA, a differentiable ergonomics model, and use it in gradient-free\npostural optimization for physical human-robot interaction tasks such as\nco-manipulation and teleoperation. We evaluate our framework through human and\nsimulation experiments.",
          "link": "http://arxiv.org/abs/2108.05971",
          "publishedOn": "2021-08-16T00:47:32.295Z",
          "wordCount": 550,
          "title": "Ergonomically Intelligent Physical Human-Robot Interaction: Postural Estimation, Assessment, and Optimization. (arXiv:2108.05971v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Anh Tran</a>",
          "description": "Bayesian optimization (BO) is a flexible and powerful framework that is\nsuitable for computationally expensive simulation-based applications and\nguarantees statistical convergence to the global optimum. While remaining as\none of the most popular optimization methods, its capability is hindered by the\nsize of data, the dimensionality of the considered problem, and the nature of\nsequential optimization. These scalability issues are intertwined with each\nother and must be tackled simultaneously. In this work, we propose the\nScalable$^3$-BO framework, which employs sparse GP as the underlying surrogate\nmodel to scope with Big Data and is equipped with a random embedding to\nefficiently optimize high-dimensional problems with low effective\ndimensionality. The Scalable$^3$-BO framework is further leveraged with\nasynchronous parallelization feature, which fully exploits the computational\nresource on HPC within a computational budget. As a result, the proposed\nScalable$^3$-BO framework is scalable in three independent perspectives: with\nrespect to data size, dimensionality, and computational resource on HPC. The\ngoal of this work is to push the frontiers of BO beyond its well-known\nscalability issues and minimize the wall-clock waiting time for optimizing\nhigh-dimensional computationally expensive applications. We demonstrate the\ncapability of Scalable$^3$-BO with 1 million data points, 10,000-dimensional\nproblems, with 20 concurrent workers in an HPC environment.",
          "link": "http://arxiv.org/abs/2108.05969",
          "publishedOn": "2021-08-16T00:47:32.289Z",
          "wordCount": 665,
          "title": "Scalable3-BO: Big Data meets HPC - A scalable asynchronous parallel high-dimensional Bayesian optimization framework on supercomputers. (arXiv:2108.05969v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2001.03040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianfeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper establishes the optimal approximation error characterization of\ndeep rectified linear unit (ReLU) networks for smooth functions in terms of\nboth width and depth simultaneously. To that end, we first prove that\nmultivariate polynomials can be approximated by deep ReLU networks of width\n$\\mathcal{O}(N)$ and depth $\\mathcal{O}(L)$ with an approximation error\n$\\mathcal{O}(N^{-L})$. Through local Taylor expansions and their deep ReLU\nnetwork approximations, we show that deep ReLU networks of width\n$\\mathcal{O}(N\\ln N)$ and depth $\\mathcal{O}(L\\ln L)$ can approximate $f\\in\nC^s([0,1]^d)$ with a nearly optimal approximation error\n$\\mathcal{O}(\\|f\\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our estimate is\nnon-asymptotic in the sense that it is valid for arbitrary width and depth\nspecified by $N\\in\\mathbb{N}^+$ and $L\\in\\mathbb{N}^+$, respectively.",
          "link": "http://arxiv.org/abs/2001.03040",
          "publishedOn": "2021-08-16T00:47:32.283Z",
          "wordCount": 621,
          "title": "Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.08919",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alyaev_S/0/1/0/all/0/1\">Sergey Alyaev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shahriari_M/0/1/0/all/0/1\">Mostafa Shahriari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pardo_D/0/1/0/all/0/1\">David Pardo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Omella_A/0/1/0/all/0/1\">Angel Javier Omella</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Larsen_D/0/1/0/all/0/1\">David Larsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahani_N/0/1/0/all/0/1\">Nazanin Jahani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suter_E/0/1/0/all/0/1\">Erich Suter</a>",
          "description": "Modern geosteering is heavily dependent on real-time interpretation of deep\nelectromagnetic (EM) measurements. We present a methodology to construct a deep\nneural network (DNN) model trained to reproduce a full set of extra-deep EM\nlogs consisting of 22 measurements per logging position. The model is trained\nin a 1D layered environment consisting of up to seven layers with different\nresistivity values. A commercial simulator provided by a tool vendor is used to\ngenerate a training dataset. The dataset size is limited because the simulator\nprovided by the vendor is optimized for sequential execution. Therefore, we\ndesign a training dataset that embraces the geological rules and geosteering\nspecifics supported by the forward model. We use this dataset to produce an EM\nsimulator based on a DNN without access to the proprietary information about\nthe EM tool configuration or the original simulator source code. Despite\nemploying a relatively small training set size, the resulting DNN forward model\nis quite accurate for the considered examples: a multi-layer synthetic case and\na section of a published historical operation from the Goliat Field. The\nobserved average evaluation time of 0.15 ms per logging position makes it also\nsuitable for future use as part of evaluation-hungry statistical and/or\nMonte-Carlo inversion algorithms within geosteering workflows.",
          "link": "http://arxiv.org/abs/2005.08919",
          "publishedOn": "2021-08-16T00:47:32.263Z",
          "wordCount": 697,
          "title": "Modeling extra-deep electromagnetic logs using a deep neural network. (arXiv:2005.08919v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Divya Shyam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Atul Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1\">D. Roy Mahapatra</a>",
          "description": "This paper reports a reduced-order modeling framework of bladed disks on a\nrotating shaft to simulate the vibration signature of faults like cracks in\ndifferent components aiming towards simulated data-driven machine learning. We\nhave employed lumped and one-dimensional analytical models of the subcomponents\nfor better insight into the complex dynamic response. The framework seeks to\naddress some of the challenges encountered in analyzing and optimizing fault\ndetection and identification schemes for health monitoring of rotating\nturbomachinery, including aero-engines. We model the bladed disks and shafts by\ncombining lumped elements and one-dimensional finite elements, leading to a\ncoupled system. The simulation results are in good agreement with previously\npublished data. We model the cracks in a blade analytically with their\neffective reduced stiffness approximation. Multiple types of faults are\nmodeled, including cracks in the blades of single and two-stage bladed disks,\nFan Blade Off (FBO), and Foreign Object Damage (FOD). We have applied\naero-engine operational loading conditions to simulate realistic scenarios of\nonline health monitoring. The proposed reduced-order simulation framework will\nhave applications in probabilistic signal modeling, machine learning toward\nfault signature identification, and parameter estimation with measured\nvibration signals.",
          "link": "http://arxiv.org/abs/2108.06265",
          "publishedOn": "2021-08-16T00:47:32.248Z",
          "wordCount": 648,
          "title": "A reduced-order modeling framework for simulating signatures of faults in a bladed disk. (arXiv:2108.06265v1 [cs.CE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Conglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>",
          "description": "Recent works have demonstrated great success in training high-capacity\nautoregressive language models (GPT, GPT-2, GPT-3) on a huge amount of\nunlabeled text corpus for text generation. Despite showing great results, this\ngenerates two training efficiency challenges. First, training large corpora can\nbe extremely timing consuming, and how to present training samples to the model\nto improve the token-wise convergence speed remains a challenging and open\nquestion. Second, many of these large models have to be trained with hundreds\nor even thousands of processors using data-parallelism with a very large batch\nsize. Despite of its better compute efficiency, it has been observed that\nlarge-batch training often runs into training instability issue or converges to\nsolutions with bad generalization performance. To overcome these two\nchallenges, we present a study of a curriculum learning based approach, which\nhelps improves the pre-training convergence speed of autoregressive models.\nMore importantly, we find that curriculum learning, as a regularization method,\nexerts a gradient variance reduction effect and enables to train autoregressive\nmodels with much larger batch sizes and learning rates without training\ninstability, further improving the training speed. Our evaluations demonstrate\nthat curriculum learning enables training GPT-2 models (with up to 1.5B\nparameters) with 8x larger batch size and 4x larger learning rate, whereas the\nbaseline approach struggles with training divergence. To achieve the same\nvalidation perplexity targets during pre-training, curriculum learning reduces\nthe required number of tokens and wall clock time by up to 59% and 54%,\nrespectively. To achieve the same or better zero-shot WikiText-103/LAMBADA\nevaluation results at the end of pre-training, curriculum learning reduces the\nrequired number of tokens and wall clock time by up to 13% and 61%,\nrespectively.",
          "link": "http://arxiv.org/abs/2108.06084",
          "publishedOn": "2021-08-16T00:47:32.242Z",
          "wordCount": 721,
          "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. (arXiv:2108.06084v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>",
          "description": "Both latent semantic analysis (LSA) and correspondence analysis (CA) use a\nsingular value decomposition (SVD) for dimensionality reduction. In this\narticle, LSA and CA are compared from a theoretical point of view and applied\nin both a toy example and an authorship attribution example. In text mining\ninterest goes out to the relationships among documents and terms: for example,\nwhat terms are more often used in what documents. However, the LSA solution\ndisplays a mix of marginal effects and these relationships. It appears that CA\nhas more attractive properties than LSA. One such property is that, in CA, the\neffect of the margins is effectively eliminated, so that the CA solution is\noptimally suited to focus on the relationships among documents and terms. Three\nmechanisms are distinguished to weight documents and terms, and a unifying\nframework is proposed that includes these three mechanisms and includes both CA\nand LSA as special cases. In the authorship attribution example, the national\nanthem of the Netherlands, the application of the discussed methods is\nillustrated.",
          "link": "http://arxiv.org/abs/2108.06197",
          "publishedOn": "2021-08-16T00:47:32.236Z",
          "wordCount": 621,
          "title": "A Comparison of Latent Semantic Analysis and Correspondence Analysis for Text Mining. (arXiv:2108.06197v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jeongwhan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jinsung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>",
          "description": "Collaborative filtering (CF) is a long-standing problem of recommender\nsystems. Many novel methods have been proposed, ranging from classical matrix\nfactorization to recent graph convolutional network-based approaches. After\nrecent fierce debates, researchers started to focus on linear graph\nconvolutional networks (GCNs) with a layer combination, which show\nstate-of-the-art accuracy in many datasets. In this work, we extend them based\non neural ordinary differential equations (NODEs), because the linear GCN\nconcept can be interpreted as a differential equation, and present the method\nof Learnable-Time ODE-based Collaborative Filtering (LT-OCF). The main novelty\nin our method is that after redesigning linear GCNs on top of the NODE regime,\ni) we learn the optimal architecture rather than relying on manually designed\nones, ii) we learn smooth ODE solutions that are considered suitable for CF,\nand iii) we test with various ODE solvers that internally build a diverse set\nof neural network connections. We also present a novel training method\nspecialized to our method. In our experiments with three benchmark datasets,\nGowalla, Yelp2018, and Amazon-Book, our method consistently shows better\naccuracy than existing methods, e.g., a recall of 0.0411 by LightGCN vs. 0.0442\nby LT-OCF and an NDCG of 0.0315 by LightGCN vs. 0.0341 by LT-OCF in\nAmazon-Book. One more important discovery in our experiments that is worth\nmentioning is that our best accuracy was achieved by dense connections rather\nthan linear connections.",
          "link": "http://arxiv.org/abs/2108.06208",
          "publishedOn": "2021-08-16T00:47:32.170Z",
          "wordCount": 673,
          "title": "LT-OCF: Learnable-Time ODE-based Collaborative Filtering. (arXiv:2108.06208v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storchan_V/0/1/0/all/0/1\">Victor Storchan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurshan_E/0/1/0/all/0/1\">Eren Kurshan</a>",
          "description": "We review practical challenges in building and deploying ethical AI at the\nscale of contemporary industrial and societal uses. Apart from the purely\ntechnical concerns that are the usual focus of academic research, the\noperational challenges of inconsistent regulatory pressures, conflicting\nbusiness goals, data quality issues, development processes, systems integration\npractices, and the scale of deployment all conspire to create new ethical\nrisks. Such ethical concerns arising from these practical considerations are\nnot adequately addressed by existing research results. We argue that a holistic\nconsideration of ethics in the development and deployment of AI systems is\nnecessary for building ethical AI in practice, and exhort researchers to\nconsider the full operational contexts of AI systems when assessing ethical\nrisks.",
          "link": "http://arxiv.org/abs/2108.06217",
          "publishedOn": "2021-08-16T00:47:32.143Z",
          "wordCount": 586,
          "title": "Beyond Fairness Metrics: Roadblocks and Challenges for Ethical AI in Practice. (arXiv:2108.06217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitin Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Hima Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_S/0/1/0/all/0/1\">Shazia Afzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panwar_N/0/1/0/all/0/1\">Naveen Panwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_R/0/1/0/all/0/1\">Ruhi Sharma Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttula_S/0/1/0/all/0/1\">Shanmukha Guttula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Abhinav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagalapatti_L/0/1/0/all/0/1\">Lokesh Nagalapatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sameep Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hans_S/0/1/0/all/0/1\">Sandeep Hans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohia_P/0/1/0/all/0/1\">Pranay Lohia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1\">Aniya Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_D/0/1/0/all/0/1\">Diptikalyan Saha</a>",
          "description": "The quality of training data has a huge impact on the efficiency, accuracy\nand complexity of machine learning tasks. Various tools and techniques are\navailable that assess data quality with respect to general cleaning and\nprofiling checks. However these techniques are not applicable to detect data\nissues in the context of machine learning tasks, like noisy labels, existence\nof overlapping classes etc. We attempt to re-look at the data quality issues in\nthe context of building a machine learning pipeline and build a tool that can\ndetect, explain and remediate issues in the data, and systematically and\nautomatically capture all the changes applied to the data. We introduce the\nData Quality Toolkit for machine learning as a library of some key quality\nmetrics and relevant remediation techniques to analyze and enhance the\nreadiness of structured training datasets for machine learning projects. The\ntoolkit can reduce the turn-around times of data preparation pipelines and\nstreamline the data quality assessment process. Our toolkit is publicly\navailable via IBM API Hub [1] platform, any developer can assess the data\nquality using the IBM's Data Quality for AI apis [2]. Detailed tutorials are\nalso available on IBM Learning Path [3].",
          "link": "http://arxiv.org/abs/2108.05935",
          "publishedOn": "2021-08-16T00:47:32.102Z",
          "wordCount": 657,
          "title": "Data Quality Toolkit: Automatic assessment of data quality and remediation for machine learning datasets. (arXiv:2108.05935v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}